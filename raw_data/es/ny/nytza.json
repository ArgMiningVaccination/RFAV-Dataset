{"title": "PDF", "author": "PDF", "url": "http://matema.ujaen.es/jnavas/web_recursos/archivos/weka%20master%20recursos%20naturales/apuntesAD.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "T\u00c9CNICAS DE AN\u00c1LISIS DE DATOS APLICACIONES PR\u00c1CTICAS UTILIZANDO MICROSOFT EXCEL Y WEKA Jos\u00e9 Manuel Molina L\u00f3pez Jes\u00fas Garc\u00eda Herrero 2006 PR\u00d3LOGO Estos apuntes pretenden dar una visi\u00f3n gener al de las t\u00e9cnicas de an\u00e1lisis de datos y de las aplicaciones que las implementan, permitiendo entender los conceptos y algoritmos sobre los que se basan las t\u00e9cnicas as\u00ed como el resultado de su aplicaci\u00f3n sobre diversas fuentes de ficheros. Estos apuntes son una recolecci\u00f3n de informaci\u00f3n de muy variadas fuentes, p\u00e1ginas de intenet, art\u00edculos etc.. todas ellas aparecen citadas. De entre todas ellas cabe resaltar el trabajo fin de ca rrera de David S\u00e1nchez titulado \"Data Mining mediante Sistemas Clasificadores Gen\u00e9ticos. An\u00e1lisis comparativo con las t\u00e9cnicas cl\u00e1sicas implementadas en WEKA\", en la titulaci\u00f3n de Ingenier\u00eda Inform\u00e1tica (Julio 2003) donde se rea liza un gran esfuerzo por explicar el funcionamiento interno de la herramient a WEKA y de d\u00f3nde se ha extra\u00eddo la informaci\u00f3n acerca de la s clases y el c\u00f3digo que implementa los algoritmos para estos apuntes. As\u00ed tambi\u00e9n resulta ne cesario resaltar la tesis doctoral de F\u00e9lix Chamorro, ya que el cap\u00edtulo 2 (e l estado del arte) se pormenorizan todas las t\u00e9cnicas de an\u00e1lisis de datos y que ha sido utilizado para la elaboraci\u00f3n de estos apuntes. Esperamos que estos apuntes sean de utilidad para los alumnos que se acerquen al an\u00e1lisis de datos y en par ticular para aquellos que tengan inter\u00e9s en aplicar los conocimientos te\u00f3ric os en el campo de la pr\u00e1ctica. Jos\u00e9 Manuel Molina L\u00f3pez Jes\u00fas Garc\u00eda Herrero \u00cdndice T\u00e9cnicas de An\u00e1lisis de Datos i \u00cdndice CAP\u00cdTULO 1. INTRODUCCI\u00d3N 1 1.1. KDD Y MINER\u00cdA DE DATOS 1 1.1.2. EL PROCESO DE KDD 3 1.1.3. MINER\u00cdA DE DATOS 5 1.1.4. TECNOLOG\u00cdAS DE APOYO 6 1.1.5. \u00c1REAS DE APLICACI\u00d3N 9 1.1.6. TENDENCIAS DE LA MINER\u00cdA DE DATOS 13 1.2. MINER\u00cdA DE DATOS Y ALMACENAMIENTO DE DATOS 14 1.2.1. ARQUITECTURA , MODELADO , DISE\u00d1O , Y ASPECTOS DE LA ADMINISTRACI\u00d3N 14 1.2.2. DATA MINING Y FUNCIONES DE BASES DE DATOS 16 1.2.3. DATA WAREHOUSE 17 1.2.4. DATA WAREHOUSE Y DATA MINING 21 1.3. HERRAMIENTAS COMERCIALES DE AN\u00c1LISIS DE DATOS 22 1.4. ARQUITECTURA SOFTWARE PARA DATA MINING 33 1.4.2. ARQUITECTURA FUNCIONAL 35 1.4.3. ARQUITECTURA DEL SISTEMA 36 1.4.4. EL DATA MINING EN LA ARQUITECTURA DEL SISTEMA 38 CAP\u00cdTULO 2. AN\u00c1LISIS ESTAD\u00cdSTICO MEDIANTE EXCEL 41 2.1. AN\u00c1LISIS DE UNA VARIABLE . ESTAD\u00cdSTICA DESCRIPTIVA E INFERENCIA 43 \u00cdndice T\u00e9cnicas de An\u00e1lisis de Datos ii 2.2. T\u00c9CNICAS DE EVALUACI\u00d3N DE HIP\u00d3TESIS 57 2.2.1. AN\u00c1LISIS DE RELACIONES ENTRE ATRIBUTOS 57 2.2.2. RELACI\u00d3N ENTRE VARIABLES NOMINALES -NOMINALES 57 2.2.3. RELACIONES NUM\u00c9RICAS -NOMINALES 59 2.2.3.1. Comparaci\u00f3n de dos medias 60 2.2.3.2. An\u00e1lisis de la varianza 61 2.2.4. RELACIONES NUM\u00c9RICAS -NUM\u00c9RICAS : 64 2.2.4.1. Regres i\u00f3n lineal 64 2.2.5. EVALUACI\u00d3N DEL MODELO DE REGRESI\u00d3N 65 2.2.5.1. Medidas de Calidad 65 2.2.5.2. Test de Hip\u00f3tesis sobre modelo de regresi\u00f3n 66 2.3. EJEMPLOS DE APLICACI\u00d3N DE T\u00c9CNICAS DE EVALUACI\u00d3N DE HIP\u00d3TESIS 67 2.3.1. EJEMPLOS DE VALIDACI\u00d3N DE HIP\u00d3TESIS 67 2.4. T\u00c9CNICAS CL\u00c1SICAS DE CLASIFICACI\u00d3N Y PREDICCI\u00d3N 76 2.4.1. CLASIFICACI\u00d3N BAYESIANA : 80 2.4.2. REGRESI\u00d3N LINEAL 90 CAP\u00cdTULO 3. T\u00c9CNICAS DE MINER\u00cdA DE DATOS BASADAS EN APRENDIZAJE AUTOM\u00c1TICO 96 3.1. T\u00c9CNICAS DE MINER\u00cdA DE DATOS 96 3.2. CLUSTERING . (\"SEGMENTACI\u00d3N \") 98 CONCEPTUAL (COBWEB) 100 3.2.3. CLUSTERING PROBABIL\u00cdSTICO (EM) 104 3.3. REGLAS DE ASOCIACI\u00d3N 107 3.4. LA PREDICCI\u00d3N 110 3.4.1. REGRESI\u00d3N NO LINEAL . 110 3.4.2. \u00c1RBOLES DE PREDICCI\u00d3N 111 3.4.3. ESTIMADOR DE N\u00daCLEOS 115 3.5. LA CLASIFICACI\u00d3N 120 3.5.1. TABLA DE DECISI\u00d3N 121 3.5.2. \u00c1RBOLES DE DECISI\u00d3N 123 3.5.3. REGLAS DE CLASIFICACI\u00d3N 135 \u00cdndice T\u00e9cnicas de An\u00e1lisis de Datos iii 3.5.4. CLASIFICACI\u00d3N BAYESIANA 140 3.5.5. APRENDIZAJE BASADO EN EJEMPLARES 145 3.5.6. REDES DE NEURONAS 153 3.5.7. L\u00d3GICA BORROSA (\"FUZZY LOGIC \") 157 3.5.8. T\u00c9CNICAS GEN\u00c9TICAS : ALGORITMOS GEN\u00c9TICOS (\"GENETIC ALGORITHMS \") 157 CAP\u00cdTULO 4. T\u00c9CNICAS DE AN\u00c1LISIS DE DATOS EN WEKA 159 INTRODUCCI\u00d3N 159 PREPARACI\u00d3N DE LOS DATOS 160 MUESTRA DE DATOS 160 OBJETIVOS DEL AN\u00c1LISIS 161 EJECUCI\u00d3N DE WEKA 162 PREPROCESADO DE LOS DATOS 164 CARACTER\u00cdSTICAS DE LOS ATRIBUTOS 165 TRABAJO CON FILTROS . PREPARACI\u00d3N DE FICHEROS DE MUESTRA 167 Filtros de atributos 168 Filtros de instancias 172 VISUALIZACI\u00d3N 173 REPRESENTACI\u00d3N 2D DE LOS DATOS 173 FILTRADO \"GR\u00c1FICO \" DE LOS DATOS 177 ASOCIACI\u00d3N 178 AGRUPAMIENTO 183 AGRUPAMIENTO NUM\u00c9RICO 184 AGRUPAMIENTO SIMB\u00d3LICO 189 CLASIFICACI\u00d3N 191 MODOS DE EVALUACI\u00d3N DEL CLASIFICADOR 192 SELECCI\u00d3N Y CONFIGURACI\u00d3N DE CLASIFICADORES 195 PREDICCI\u00d3N NUM\u00c9RICA 203 APRENDIZAJE DEL MODELO Y APLICACI\u00d3N A NUEVOS DATOS . 209 SELECCI\u00d3N DE ATRIBUTOS 211 \u00cdndice T\u00e9cnicas de An\u00e1lisis de Datos iv CAP\u00cdTULO 5. IMPLEMENTACI\u00d3N DE LAS T\u00c9CNICAS DE AN\u00c1LISIS DE DATOS EN WEKA 215 5.1. UTILIZACI\u00d3N DE LAS CLASES DE WEKA EN PROGRAMAS INDEPENDIENTES 215 5.2. TABLA DE DECISI\u00d3N EN WEKA 215 5.3. ID3 EN WEKA 216 5.4. C4.5 EN WEKA (J48) 216 5.5. \u00c1RBOL DE DECISI\u00d3N DE UN SOLO NIVEL EN WEKA 219 5.6. 1R EN WEKA 220 5.7. PRISM EN WEKA 221 5.8. PART EN 221 5.14. REGRESI\u00d3N LINEAL EN WEKA 228 5.15. REGRESI\u00d3N PONDERADA LOCALMENTE EN WEKA 230 5.16. M5 EN WEKA 231 5.17. KERNEL EN WEKA 235 5.21. ASOCIACI\u00d3N A PRIORI EN WEKA 236 CAP\u00cdTULO 6. EJEMPLOS SOBRE CASOS DE ESTUDIO 239 \u00cdndice T\u00e9cnicas de An\u00e1lisis de Datos v BIBLIOGRAF\u00cdA 240 Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 1 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Cap\u00edtulo 1. Introducci\u00f3n En este texto se estudia uno de los campos que m\u00e1s se est\u00e1n estudiando en estos d\u00edas: La extracci\u00f3n de conocimiento a partir de fuentes masivas de datos. Para ello se emplean las denominadas t\u00e9cnicas de miner\u00eda de datos, que son algoritmos capaces de obtener relaciones entre distintos atributos o conceptos para ayudar, por ejemplo, a la toma de decisiones. Adem\u00e1s de las t\u00e9cnicas estad\u00edsticas se estudian las t\u00e9cnicas de Miner\u00eda de Datos [Data Mining] basadas en t\u00e9cnic as de aprendizaje autom\u00e1tico que se implementan en una herramienta de miner\u00eda de datos de libre distribuci\u00f3n: WEKA. Esta herramienta permite, a partir de ficheros de texto en un formato determinado, utilizar distintos tipos de t\u00e9cnicas para extraer informaci\u00f3n. A continuaci\u00f3n se definen los conceptos fundamentales emplea dos en el texto: KDD y, sobretodo, miner\u00eda de datos, as\u00ed como sus principales caracter\u00edsticas. Posteriormente se comenta la estructura del proyecto. 1.1. KDD y Miner\u00eda de Datos Hoy en d\u00eda, la canti dad de datos que ha sido alma cenada en las bases de datos excede nuestra habilidad para reducir y analizar los datos sin el uso de t\u00e9cnicas de an\u00e1lisis automatizadas. Muchas bases de datos comerciales transaccionales y cient\u00edficas crecen a una proporci\u00f3n fenomenal. KDD [Knowledge Discovery in Databases] [PSF91] es el proceso completo de extracci\u00f3n de informaci\u00f3n, que se encarga adem\u00e1s de la preparaci\u00f3n de los datos y de la interpretaci\u00f3n de los resultados obtenidos. KDD se ha definido como \"el proceso no trivia l de identificaci\u00f3n en los datos de patrones v\u00e1lidos, nuevos, potencialmente \u00fatiles, y finalmente comprensibles\" [FAYY96]. Se trata de interpretar grandes cantidades de datos y encontrar relaciones o patrones. Para conseguirlo har\u00e1n falta t\u00e9cnicas de aprendizaje autom \u00e1tico [Machine Learning] [MBK98], estad\u00edstica [MIT97, t\u00e9cnicas de representaci\u00f3n del conocim iento, razonamiento basado en casos [CBR, Case Based Reasoning], ra zonamiento aproximado, adquisici\u00f3n de conocimiento, redes de neuronas y visua lizaci\u00f3n de datos. Tareas comunes en KDD son la inducci\u00f3n de reglas, los probl emas de clasificaci\u00f3n y clustering, el reconocimiento de patrones, el model ado predictivo, la detecci\u00f3n de dependencias, etc. KDD es un campo creciente: hay muc has metodolog\u00edas del descubrimiento del conocimiento en uso y bajo desarrollo . Algunas de estas t\u00e9cnicas son gen\u00e9ricas, mientras otros son de dominio espec\u00edfico. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 2 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Los datos recogen un conjunto de hechos (una base de datos) y los patrones son expresiones que describen un subconjunto de los datos (un modelo aplicable a ese subconjunto). KDD involu cra un proceso iterativo e interactivo de b\u00fasqueda de modelos, patrones o par \u00e1metros. Los patrones descubiertos han de ser v\u00e1lidos, novedosos para el sist ema (para el usuar io siempre que sea posible) y potenc ialmente \u00fatiles. Se han de definir medidas cuantitativas para los patrones obtenidos (precisi\u00f3n, utilidad, beneficio obtenido...) . Se debe establecer al guna medida de inter\u00e9s [interestingness] que considere la validez , utilidad y simplicidad de los patrones obtenidos mediante algun a de las t\u00e9cnicas de Miner\u00ed a de Datos. El objetivo final de todo esto es incorporar el conocimiento obtenido en alg\u00fan sistema real, tomar decisiones a partir de los resultados alcanzados o, simp lemente, registrar la informaci\u00f3n conseguida y suminist r\u00e1rsela a quien est\u00e9 interesado. Ha llegado un momento en el que disp onemos de tanta informaci\u00f3n que nos vemos incapaces de sacarle provecho. Lo s datos tal cual se almacenan [raw data] no suelen proporcionar beneficios directos. Su va lor real reside en la informaci\u00f3n que podamos extraer de el los: informaci\u00f3n que nos ayude a tomar decisiones o a mejorar nuestra comprens i\u00f3n de los fen\u00f3menos que nos rodean. Se requiere de grandes cantidades de datos que proporcionen informaci\u00f3n suficiente para derivar un conocimi ento adicional. Dado que se requieren grandes cantidades de datos, es esencial el proceso de la eficiencia. La exactitud es requerida para asegurar que el descubrimient o del conocimiento es v\u00e1lido. Los resultados deber\u00e1n se r presentados de una manera entendible para el ser humano. Una de las prem isas mayores de KDD es que el conocimiento es descubierto usando t\u00e9 cnicas de aprendizaje inteligente que van examinando los datos a trav\u00e9s de procesos autom atizados. Para que una t\u00e9cnica sea considerada \u00fatil para el descubrimiento del co nocimiento, \u00e9ste debe ser interesante; es decir, debe tener un valor potencial para el usuario. KDD proporciona la capacidad para descubr ir informaci\u00f3n nueva y significativa usando los datos existentes . KDD se knowledge discovery: An overview\" Ad vances in Knowledge Discovery and Data Mining (AAAI / MIT Press, 1996) y se puede resumir en la Figura 1. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 3 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 1.1: Esquema del proceso de KDD KDD r\u00e1pidamente excede la capacidad humana para analizar grandes cantidades de datos. La cantidad de da tos que requieren procesamiento y an\u00e1lisis en grandes bases de datos ex ceden las capacidades humanas y la dificultad de transformar los datos c on precisi\u00f3n es un co nocimiento que va m\u00e1s all\u00e1 de los l\u00edmites de las bases de dat os tradicionales. Por consiguiente, la utilizaci\u00f3n plena de los datos almacenado s depende del uso de t\u00e9cnicas del descubrimiento del conocimiento. La utilidad de aplicaciones futuras en KDD es de largo al cance. KDD puede usarse como un medio de recuperaci\u00f3 n de informaci\u00f3n, de la misma manera que los agentes inteligentes realizan la recuperaci\u00f3n de informaci\u00f3n en el Web. Nuevos modelos o tendencias en los datos podr\u00e1n descubrirse usando estas t\u00e9cnicas. KDD tambi\u00e9n puede usarse como una base para las interfaces inteligentes del ma\u00f1ana, agregando un componente del descubrimiento del conocimiento a un sistema de bases de datos o integrando KDD con las hojas de c\u00e1lculo y visualizaciones. 1.1.2. El proceso de KDD El proceso de KDD se inicia con la ident ificaci\u00f3n de los datos. Para ello hay que imaginar qu\u00e9 datos se necesitan, d\u00f3nde se pueden encontrar y c\u00f3mo conseguirlos. Una vez que se dispone de datos, se deben se leccionar aquellos que sean \u00fatiles para los objetivos propuesto s. Se preparan, poni\u00e9ndolos en un formato adecuado. Una vez se tienen los datos adecuados se procede a la miner\u00eda de datos, proceso en el que se seleccionar\u00e1n la s herramientas y t\u00e9cnicas adecuadas para lograr los objetivos pretendidos. Y tras este pr oceso llega el an\u00e1lisis de resultados, con lo que se obti ene el conocimiento pretendido. En la figura 1.2 se muestra la met odolog\u00eda que debe seguirse para obtener conocimiento a partir de los datos que se encuentran en la base de datos. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 4 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 1.2: Metodolog\u00eda para el descubrimiento de conocimiento en bases de datos. KDD es un proceso interactivo e iter ativo, que involucra numerosos pasos e incluye muchas decisiones que deben ser tomadas por el usuario, y se estructura en las siguientes etapas [FAYY96]: Comprensi\u00f3n del dominio de la aplicac i\u00f3n, del conocimiento relevante y de los objetivos del usuario final. Creaci\u00f3n del conjunto de datos: consiste en la selecci\u00f3n del conjunto de datos, o del subconjunto de variabl es o muestra de datos, sobre los cuales se va a realizar el descubrimiento. Limpieza y preprocesamiento de los datos: Se compone de las operaciones, tales como: recolecci\u00f3n de la informaci\u00f3n necesaria sobre la cual se va a realizar el proceso, decidir las estrategi as sobre la forma en que se van a manejar los cam pos de los datos no disponibles, estimaci\u00f3n del tiempo de la informaci\u00f3n y sus posibles cambios. Reducci\u00f3n de los datos y proyecci\u00f3n: Encontrar las caracter\u00edsticas m\u00e1s significativas para representar lo s datos, dependiendo del objetivo del proceso. En este paso se pueden ut ilizar m\u00e9todos de transformaci\u00f3n para reducir el n\u00famero efectivo de variables a ser consideradas o para encontrar otras representaciones de los datos. Elegir la tarea de Miner\u00eda de Datos: De cidir si el objetivo del proceso de KDD es: Regresi\u00f3n, Clasificac i\u00f3n, Agrupamiento, etc. Elecci\u00f3n del algoritmo(s) de Miner\u00eda de Datos: Selecci\u00f3n del m\u00e9todo(s) a ser utilizado para buscar los patrones en los datos. Incluye adem\u00e1s la decisi\u00f3n sobre que modelos y par\u00e1metros pueden ser los m\u00e1s apropiados. Miner\u00eda de Datos: Consiste en la b\u00fasqueda de los patrones de inter\u00e9s en una determinada forma de representac i\u00f3n o sobre un conjunto de Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 5 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda representaciones, utilizando para ello m\u00e9 todos de clasificaci\u00f3n, reglas o \u00e1rboles, regresi\u00f3n, agrupaci\u00f3n, etc. Interpretaci\u00f3n de los patrones encontrados. Dependiendo de los resultados, a veces se hace necesar io regresar a uno de los pasos anteriores. Consolidaci\u00f3n del conocimiento descubierto: consiste en la incorporaci\u00f3n de este conocimiento al funcionamiento del sistema, o simplemente documentaci\u00f3n e informaci\u00f3n a las partes interesadas. El proceso de KDD puede involucrar vari as iteraciones y puede contener ciclos entre dos de cualquiera de los pasos. La mayor\u00eda de los trabajos que se han realizado sobre KDD se c entran en la etapa de miner\u00eda . Sin embargo, los otros pasos se consideran impor tantes para el \u00e9xito del KDD. Por eso aunque la Miner\u00eda de Datos es una parte del pr oceso completo de KDD [FAYY96], en buena parte de la literatura los t\u00e9rminos Miner\u00eda de Dato s y KDD se identifican como si fueran lo mismo. En la figura 1.3 se muestra el esfuer zo que requiere cada fa se del proceso de KDD. 0%10%20%30%40%50%60%70%Esfuerzo (%) Entendimiento del DominioPreparaci\u00f3n de los DatosData Mining Interpretaci\u00f3n y Consolidaci\u00f3n del Conocimiento Fase Figura 1.3: Esfuerzo requerido por cada fase del proceso de KDD. Como se observa en la figura 1.3, gran parte del esfuerzo del proceso de KDD recae sobre la fase de preparaci\u00f3n de lo s datos, fase crucial para tener \u00e9xito como ya se coment\u00f3 anteriormente. 1.1.3. Miner\u00eda de Datos Miner\u00eda de Datos es un t\u00e9rmino gen \u00e9rico que engloba resultados de investigaci\u00f3n, t\u00e9cnicas y herramientas usadas para extraer informaci\u00f3n \u00fatil de grandes bases de datos. Si bien Miner\u00eda de Datos es una parte del proceso completo de KDD, en buena parte de la lit eratura los t\u00e9rminos Miner\u00eda de Datos y KDD se identifican como si fueran lo mismo. Concretamente, el t\u00e9rmino Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 6 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Miner\u00eda de Datos es us ado com\u00fanmente por los es tad\u00edsticos, analistas de datos, y por la comunidad de administ radores de sistemas inform\u00e1ticos como todo el proceso del descubr imiento, mientras que el t\u00e9rmino KDD es utilizado m\u00e1s por los especialistas en Inteligencia Artificial. El an\u00e1lisis de la informaci\u00f3n recopil ada (por ejemplo, en un experimento cient\u00edfico) es habitual que sea un proces o completamente m anual (basado por lo general en t\u00e9cnicas est ad\u00edsticas). Sin embargo, cu ando la cantidad de datos de los que disponemos aumenta la resolu ci\u00f3n manual del pr oblema se hace intratable. Aqu\u00ed es donde entra en juego el conjunto de t\u00e9cnicas de an\u00e1lisis autom\u00e1tico al que nos referimos al habl ar de Miner\u00eda de Datos o KDD. Hasta ahora, los mayores \u00e9xitos en Mi ner\u00eda de Datos se pueden atribuir directa o indirectamente a avances en bases de datos (un campo en el que los ordenadores superan a los humanos). No obstante, muchos problemas de representaci\u00f3n del conocimiento y de reducci\u00f3n de la complejidad de la b\u00fasqueda necesaria (usando conocimiento a priori) est\u00e1n a\u00fan por resolver. Ah\u00ed reside el inter\u00e9s que ha despertado el tema entre investigadores de todo el mundo. A continuaci\u00f3n se presentan varias def iniciones de Miner\u00eda de Datos (MD): \"MD es la extracci\u00f3n no trivial de informaci\u00f3n impl\u00edcita, desconocida previamente, y potencialmente \u00fatil desde los datos\" [PSF91]. \"MD es el proceso de extracci\u00f3n y refinamiento de conocimiento \u00fatil desde grandes bases de datos\" [SLK96]. \"MD es el proceso de extra cci\u00f3n de informaci\u00f3n previamente desconocida, v\u00e1lida y procesable desde grandes bases de datos para luego ser utilizada en la toma de decisiones\" [CHSVZ]. \"MD es la exploraci\u00f3n y an\u00e1lisis, a trav\u00e9s de medios autom\u00e1ticos y semiautom\u00e1ticos, de grandes cantidades de datos con el fin de descubrir patrones y reglas signif icativos\" [BERR97]. \"MD es el proceso de planteamient o de distintas cons ultas y extracci\u00f3n de informaci\u00f3n \u00fatil, patrones y t endencias previamente desconocidas desde grandes cantidades de datos posiblemente almacenados en bases de datos\" [THUR99]. \"MD es el proceso de descubrir modelos en los datos\" [WF00]. 1.1.4. Tecnolog\u00edas de Apoyo Para el estudio de la Miner\u00eda de Datos se ha tomado la perspectiva orientada a datos, por dos razones. Pr imero porque la mayor\u00eda de los trabajos en Miner\u00eda de Datos est\u00e1n enfocados hacia el data warehouse que proporciona el apoyo a la Miner\u00eda de Datos organizando y es tructurando los datos. Adem\u00e1s, otras tecnolog\u00edas de apoyo a la miner\u00eda datos han sido utilizadas desde hace tiempo Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 7 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda y la integraci\u00f3n de estas tecnolog\u00edas con la administr aci\u00f3n de datos ha contribuido mucho a mejora r la Miner\u00eda de Datos. Las m\u00e1s importantes entre estas tecnol og\u00edas son los m\u00e9todos estad\u00edsticos [DEGR86] y el aprendizaje autom\u00e1tico [MIT97]. Los m\u00e9todos estad\u00edsticos han producido varios paquetes estad\u00edsticos [THUR99] para computar sumas, promedios, y distribuciones, que han ido in tegr\u00e1ndose con las bases de datos a explorar. El aprendizaje autom\u00e1tico c onsiste en la obtenci\u00f3n de reglas de aprendizaje y modelos de los datos, para lo cual a menudo se necesita la ayuda de la estad\u00edstica. Por esta ra z\u00f3n, los m\u00e9todos estad\u00edsticos y el aprendizaje autom\u00e1tico son los dos co mponentes m\u00e1s importantes de la Miner\u00eda de Datos. Adem\u00e1s existen otras tecnolog\u00edas, entre las que se incluyen visualizaci\u00f3n, procesami ento paralelo, y apoyo a la toma de decisiones. Las t\u00e9cnicas de visualizaci\u00f3n ayudan a presen tar los datos para facilitar la Miner\u00eda de Datos. Las t\u00e9cnicas procesamiento paralelo ayudan a mejorar el rendimiento de la Miner\u00eda de Datos. Los sistemas de apoyo a la toma de decisiones ayudan a discriminar los resultados y proporcionan los resultados esenciales para llevar a cabo las funciones de direcci\u00f3n. Razonamiento estad\u00edstico Las t\u00e9cnicas y m\u00e9todos estad\u00edsticas del razonamiento han sido utilizados durante varias d\u00e9cadas, siendo los \u00fanicos medios de anal izar los datos en el pasado. Numerosos paquetes [THUR99] est\u00e1n ahora disponibles para computar promedios, sumas, y diferent es distribuciones para diferentes aplicaciones. Por ejemplo, la oficin a del censo usa an\u00e1lisis y m\u00e9todos estad\u00edsticos para analizar la poblaci\u00f3n en un pa\u00eds. M\u00e1s recientemente, las t\u00e9cnicas estad\u00edsticas del razonamient o est\u00e1n jugando un papel importante en la Miner\u00eda de Datos. Algunos paquetes estad\u00edsticos que han sido utilizados durante mucho tiempo, se han integrado con las diferentes bases de datos, y se est\u00e1n comercializ\u00e1ndose en la actual idad como productos para la Miner\u00eda de Datos. La estad\u00edstica juega un impor tante papel en el an\u00e1lisis de los datos, e incluso tambi\u00e9n en el aprendizaje autom\u00e1tico. Debido a esto, no se puede estudiar la Miner\u00eda de Datos sin un buen conocimiento de la estad\u00edstica. Visualizaci\u00f3n Las tecnolog\u00edas de la visualizaci\u00f3n muestran gr\u00e1ficamente los datos en las bases de datos. Se ha investigado much o sobre la visualizaci\u00f3n y el campo ha adelantado un gran trecho sobre todo con la incorporaci\u00f3n de la inform\u00e1tica multimedia. Por ejemplo, los datos en las bases de datos ser\u00e1n filas y filas de valores num\u00e9ricos, y las herramientas de visualizaci\u00f3n toman estos datos y trazan con ellos alg\u00fan tipo de gr\u00e1fico. Los modelos de visualizaci\u00f3n pueden ser bidimensionales, tridimensionales o in cluso multidimensio nales. Se han desarrollado varias herramientas de visual izaci\u00f3n para integrarse con las bases de datos, y algunos trabajos sobre este tema est\u00e1n recogidos en [VIS95]. As\u00ed, las herramientas de visualizaci\u00f3n ay udan de forma interactiva a la Miner\u00eda de Datos, aunque hay pocos tr abajos sobre la integrac i\u00f3n de las herramientas Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 8 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda de Miner\u00eda de Datos y de visualizaci\u00f3 n. Algunas ideas preliminares se presentaron en el IEEE Databases and Visualization Workshop de 1995 (v\u00e9ase, por ejemplo, [VIS95]). Sin em bargo, se han realizado m\u00e1s progresos que se pueden encontrar en [VIS97], aunque queda todav\u00eda mucho trabajo por hacer en este tema. Procesamiento paralelo El procesamiento paralelo es una t\u00e9cn ica que ha sido utilizado durante mucho tiempo. El \u00e1rea se ha des arrollado significativamente, desde sistemas con un \u00fanico procesador hasta sistemas mu ltiprocesador. Los sistemas de multiprocesamiento pueden estar formados por sistemas distribuidos o por sistemas centralizados de multiproces adores con memoria compartida, o con multiprocesadores sin memoria compartida. Hay muchos trabajos sobre la utilizaci\u00f3n de las arquitecturas paralelas pa ra el procesamiento de las bases de datos (v\u00e9ase, por ejemplo, [IEEE89] ). A pesar de haberse realizado considerable trabajo sobre el tema, es tos sistemas no fueron comercializados hasta el desarrollo del data warehouse , ya que muchos de los data warehouses emplean el procesamiento paralelo para ac elerar el proceso de las consultas. En un sistema de bases de datos paralelas, se ejecutan varias operaciones y funciones en paralelo. A pes ar de que la investigaci\u00f3n en sistemas de bases de datos en paralelo empez\u00f3 en los a\u00f1os setenta, estos sistemas se han empezado a utilizar para las aplicaciones comerciales recientemente, debido en parte a la explosi\u00f3n del data warehouse y de las tecnolog\u00edas de Miner\u00eda de Datos d\u00f3nde el rendimiento de los algoritmos de consulta es cr\u00edtico. Para escalar las t\u00e9cnicas de Miner\u00eda de Datos se necesita hardware y software apropiado, por lo que los fabricantes de bases de datos est\u00e1n empleando ordenadores con procesamiento paralel o para llevar a cabo la Miner\u00eda de Datos. Apoyo a la toma de decisiones Los sistemas de apoyo a la toma de decis iones son las herramientas que usan los directivos para tomar decisiones efic aces, y se basan en la teor\u00eda de la decisi\u00f3n. Se puede consider ar a las herramientas de Miner\u00eda de Datos como tipos especiales de herramientas de apo yo a la toma de decisiones. Las herramientas de apoyo a la toma de decisiones pertenecen a una amplia categor\u00eda (v\u00e9ase, por ejemplo, [DECI]). En general, las herramientas de apoyo a la toma de decisiones podr\u00edan utilizarse tambi\u00e9n como herramientas par a eliminar los resultados innecesarios e irrelevantes obtenidos de la Mi ner\u00eda de Datos. Tambi\u00e9n pueden ser consideradas de este tipo, herramientas tales como las hojas de c\u00e1lculo, sistemas expertos, sistem as de hipertexto, sistemas de gesti\u00f3n de informaci\u00f3n de web, y cualquier otro sistema que ayude a analistas y gestores a manejar eficazmente grandes cant idades de datos e informa ci\u00f3n. Recientemente ha aparecido un \u00e1rea nueva ll amada gesti\u00f3n del conocimiento. La gesti\u00f3n del conocimiento trata de manejar eficazmente los datos, la informaci\u00f3n, y el conocimiento de una organizaci\u00f3n [MORE98a]. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 9 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Se puede pensar que el apoyo a la toma de decisiones es una tecnolog\u00eda que se solapa con la Miner\u00eda de Datos, almacenamiento de datos, gesti\u00f3n del conocimiento, aprendizaje autom\u00e1tico, es tad\u00edstica, y otra s tecnolog\u00edas que ayudan gestionar el conocimiento de una organizaci\u00f3n y los datos. Aprendizaje autom\u00e1tico El aprendizaje autom\u00e1t ico, en muchos casos, consiste fundamentalmente en el aprendizaje de reglas a partir de los datos [MIT97], y por eso muchas de las t\u00e9cnicas de aprendizaje autom\u00e1tico son ut ilizadas en la actualidad en la Miner\u00eda de Datos. El aprendizaje autom\u00e1tic o aparece continuamente en la realizaci\u00f3n de aprendizaje computacional desde la exper iencia. Como Mitchell describe en su excelente texto sobre aprendizaje au tom\u00e1tico [MIT97], el aprendizaje autom\u00e1tico consiste en aprender de las ex periencias del pasado con respecto a alguna medida de rendimient o. Por ejemplo, en las aplicaciones de los juegos de computadora, el aprendi zaje autom\u00e1tico podr\u00eda ser aprender a jugar un juego de ajedrez, desde las experiencia s del pasado que podr \u00edan ser juegos que el ordenador juega contra s\u00ed mismo, con respecto a alguna medida de rendimiento, como ganar un cierto n\u00famero de partidas. Se han desarrollado distintas t\u00e9cnicas en el aprendizaje autom\u00e1t ico, incluyendo el aprendizaje conceptual donde se apr ende los conceptos desde diferentes ejemplos de entrenamiento, las redes de neuronas, los algoritmos gen\u00e9ticos, los \u00e1rboles de decisi\u00f3n, y la programaci\u00f3n de la l \u00f3gica inductiva. Se han realizado diferentes est udios te\u00f3ricos sobre el aprendizaje autom\u00e1tico, que intentan determinar la complejidad y c apacidad de las diferentes t\u00e9cnicas de aprendizaje autom\u00e1tico [MIT97]. Los investigadores del aprendizaje aut om\u00e1tico han agr upado las t\u00e9cnicas en tres categor\u00edas [THUR99]. La primera es el aprendizaje activo que se ocupa de la interacci\u00f3n y realizaci\u00f3n de las consul tas durante el aprendizaje, la segunda es el aprendizaje desde el conocimiento anterior, y la tercera es el aprendizaje incremental. Hay alguna superposici\u00f3n entre los tres m\u00e9todos. Durante un seminario sobre aprendizaje autom\u00e1ti co [DARP98] fueron estudiados los problemas y desaf\u00edos en aprendizaje autom\u00e1tico y sus relaciones con la Miner\u00eda de Datos. Hay todav\u00eda mucha in vestigaci\u00f3n que realiz ar en este \u00e1rea, sobre todo en la integraci\u00f3n del apr endizaje autom\u00e1tico con las diferentes t\u00e9cnicas de gesti\u00f3n de datos. Tal invest igaci\u00f3n mejorar\u00e1 significativamente el \u00e1rea de Miner\u00eda de Datos. Algunos de lo s algoritmos m\u00e1s conocidos de aprendizaje autom\u00e1tico se enc uentran en [QUIN93, MBK98]. 1.1.5. \u00c1reas de Aplicaci\u00f3n En este punto se presentan las principa les \u00e1reas y sectores empresariales en las que se puede aplicar la miner\u00eda de datos. Marketing Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 10 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Actualmente con la generaci\u00f3n de los puntos de ventas informatizados y conectados a un ordenador ce ntral, y el constante uso de las tarjetas de cr\u00e9ditos se genera gran cantidad de info rmaci\u00f3n que hay que analizar. Con ello se puede emplear la miner\u00eda de datos para: Identificar patrones de compra de los clientes: Determinar c\u00f3mo compran, a partir de sus principales ca racter\u00edsticas, conocer el grado de inter\u00e9s sobre tipos de productos, si compran determinados productos en determinados momentos,... Segmentaci\u00f3n de clientes: Consiste en la agrupaci\u00f3n de los clientes con caracter\u00edsticas similares, por ejem plo demogr\u00e1ficas. Es una importante herramienta en la estrategia de mark eting que permite realizar ofertas acordes a diferentes tipos de co mportamiento de los consumidores. Predecir respuestas a campa\u00f1as de mailing : Estas campa\u00f1as son caras y pueden llegar a ser molestas para los clientes a los que no le interesan el tipo de producto pr omocionado por lo que es importante limitarlas a los individuos con una alta probabili dad de interesarse por el producto. Est\u00e1 por ello muy relacionada con la segmentaci\u00f3n de clientes. An\u00e1lisis de cestas de la compra [market-basket analysis]: Consiste en descubrir relaciones entre productos, esto es, determinar qu\u00e9 productos suelen comprarse junto con otros, con el fin de distribuirlos adecuadamente. Compa\u00f1\u00edas de Seguros En el sector de las compa\u00f1\u00edas de seguros y la salud privada, se pueden emplear las t\u00e9cnicas de miner\u00eda de datos, por ejemplo para: An\u00e1lisis de procedimientos m\u00e9di cos solicitados conjuntamente. Predecir qu\u00e9 clientes compran nuevas p\u00f3lizas. Identificar patrones de comporta miento para clientes con riesgo. Identificar compor tamiento fraudulento. Banca En el sector bancario la informaci \u00f3n que puede almacenarse es, adem\u00e1s de las cuentas de los clientes, la relativa a la utilizaci\u00f3n de las tarjetas de cr\u00e9dito, que puede permitir conocer h\u00e1bitos y patrones de comportamiento de los usuarios. Esta informaci\u00f3n puede aplicarse para: Detectar patrones de uso fraudul ento de tarjetas de cr\u00e9dito. Identificar clientes leales: Es im portante para las compa\u00f1\u00edas de cualquier sector mantener los clientes. Y es que hay estudios que demuestran que Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 11 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda es cuatro veces m\u00e1s caros obtener nuevos clientes que mantener los existentes. Predecir clientes con probabilid ad de cambiar su afiliaci\u00f3n. Determinar gasto en tarjeta de cr\u00e9dito por grupos. Encontrar correlaciones entre indicadores financieros. Identificar reglas de mercado de valores a partir de hist\u00f3ricos: Telecomunicaciones En el sector de las telecomunicaci ones se puede almacenar informaci\u00f3n interesante sobre las llamadas realizadas, tal como el destino, la duraci\u00f3n, la fecha,... en que se realiza la llamada, por ejemplo para: Detecci\u00f3n de fraude telef\u00f3nico: Mediant e por ejemplo el agrupamiento o clustering se pueden detectar patrones en los datos que permitan detectar fraudes. Medicina Tambi\u00e9n en el campo m\u00e9dico se alma cena gran cantidad de informaci\u00f3n, sobre los pacientes, tal como enfermedades pasadas, tratamientos impuestos, pruebas realizadas, evoluci\u00f3n,... Se pueden emplear t\u00e9cnicas de miner\u00eda de datos con esta informaci\u00f3n, por ejemplo, para: Identificaci\u00f3n de terapias m\u00e9dica s satisfactorias para diferentes enfermedades. Asociaci\u00f3n de s\u00edntomas y clasific aci\u00f3n diferencial de patolog\u00edas. Estudio de factores ( gen\u00e9ticos, precedentes, h\u00e1bitos, alimenticios,...) de riesgo para la salud en distintas patolog\u00edas. Segmentaci\u00f3n de pacientes para una atenci\u00f3n m\u00e1s inteligente seg\u00fan su grupo. Estudios epidemiol\u00f3gicos, an\u00e1lisis de rendimientos de campa\u00f1as de informaci\u00f3n, prevenci\u00f3n, sustituci\u00f3n de f\u00e1rmacos,... Identificaci\u00f3n de terapias m\u00e9di cas y tratamientos err\u00f3neos para determinadas enfermedades. Industria farmac\u00e9utica Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 12 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda En el sector qu\u00edmico y farmac\u00e9utico se almacenan gran cantidad de informaci\u00f3n: Bases de datos de dominio p\u00fablic o conteniendo informaci\u00f3n sobre estructuras y propiedades de componentes qu\u00edmicos. Resultados de universidades y l aboratorios publicadas en revistas t\u00e9cnicas. Datos generados en la realizaci\u00f3n de los experimentos. Datos propios de la empresa. Los datos son almacenados en diferentes categor\u00edas y a cada categor\u00eda se le aplica un diferente trato. Se podr\u00edan realizar, entre otras, las siguientes operaciones con la informaci\u00f3n obtenida: Clustering de mol\u00e9culas: Consiste en el agrupamiento de mol\u00e9culas que presentan un cierto nivel de similitud, con lo que se pueden descubrir importantes propiedades qu\u00edmicas. B\u00fasqueda de todas las mol\u00e9culas que contienen un patr\u00f3n espec\u00edfico: Se podr\u00eda introducir una subestruct ura (un patr\u00f3n), devolviendo el sistema todas las mol\u00e9culas que son similares a dicha estructura. B\u00fasqueda de todas las mol\u00e9culas que vincula un camino espec\u00edfico hacia una mol\u00e9cula objetivo: Rea lizar una b\u00fasqueda exhaustiva puede ser impracticable, por lo que se p ueden usar restricciones en el espacio de b\u00fasqueda. Predicci\u00f3n de resultado de experiment os de una nueva mol\u00e9cula a partir de los datos almacenados: A trav\u00e9s de determinadas t\u00e9cnicas de inteligencia artificial es posible predecir los resultados a nuevos experimentos a partir de los datos, c on el consiguiente ahorro de tiempo y dinero. Biolog\u00eda Con la finalizaci\u00f3n en los pr\u00f3ximos a\u00f1os del Proyecto Genoma Humano y el almacenamiento de toda la informaci\u00f3n que est\u00e1 generando en bases de datos accesibles por Internet, el siguiente reto consiste en descubrir c\u00f3mo funcionan nuestros genes y su influencia en la sal ud. Existen nuevas tecnolog\u00edas (chips de ADN, prote\u00f3mica, gen\u00f3mic a funcional, variablidad gen\u00e9tica individual) que est\u00e1n posibilitando el desa rrollo de una \"nueva biolog\u00eda\" que permite extraer conocimiento biom\u00e9dicos a partir de bas es de datos experimentales en el entorno de un ordenador b\u00e1sicam ente mediante t\u00e9cnicas de miner\u00eda de datos y visualizaci\u00f3n. Estos trabajos form an parte de los desarrollos de la Bioinform\u00e1tica. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 13 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 1.1.6. Tendencias de la Miner\u00eda de Datos El inter\u00e9s que despierta la Miner\u00eda de Da tos para el an\u00e1lisis de la informaci\u00f3n especialmente en el \u00e1rea comercial ha ce que se busquen nuevas aplicaciones basadas en esta tecnolog\u00eda. Algunas de las principales nuevas aplicaciones basadas en la Miner\u00eda de Datos se presentan a continuaci\u00f3n. Miner\u00eda de Textos La Miner\u00eda de Textos [Text Mining] surge ante el problema cada vez m\u00e1s apremiante de extraer informaci\u00f3n aut om\u00e1ticamente a partir de masas de textos. Se trata as\u00ed de extraer informa ci\u00f3n de datos no estructurados: texto plano. Existen varias aproximaciones a la representaci\u00f3n de la informaci\u00f3n no estructurada [HH96]: \"Bag of Words\": Cada palabra constitu ye una posici\u00f3n de un vector y el valor corresponde con el n\u00famero de veces que ha aparecido. N-gramas o frases: Permite tener en cuenta el orden de las palabras. Trata mejor frases negativas \" ... excepto ... \", \"... pero no .... \", que tomar\u00edan en otro caso las palabras que le siguen como relevantes. Representaci\u00f3n relacional (primer orden): Permite detectar patrones m\u00e1s complejos (si la palabra X est\u00e1 a la izqui erda de la palabra Y en la misma frase...). Categor\u00edas de conceptos. Casi todos se enfrentan con el \"v ocabulary problem\" [FUR87]: Tienen problemas con la sinonimia, la polisemia, los lemas, etc. Un ejemplo de aplicaci\u00f3n basada en Miner\u00eda de Textos es la generaci\u00f3n autom\u00e1tica de \u00edndices en documentos. Ot ras m\u00e1s complicadas consistir\u00edan en escanear completamente un texto y mo strar un mapa en el que las partes m\u00e1s relacionadas, o los documentos m\u00e1s relacionados se coloquen cerca unos de otros. En este caso se tratar\u00eda de ana lizar las palabras en el contexto en que se encuentren. En cualquier caso, aunque a\u00fan no se ha avanzado mucho en el \u00e1rea de Miner\u00eda de Textos, ya hay productos co merciales que emplean esta tecnolog\u00eda con diferentes prop\u00f3sitos. Miner\u00eda de datos Web La Miner\u00eda de datos Web [Web Mining] es una tecnolog\u00eda usada para descubrir conocimiento interesante en todos los as pectos relacionados a la Web. Es uno de los mayores retos. El enorme volum en de datos en la Web generado por la explosi\u00f3n de usuarios y el desarrollo de librer\u00edas digitales hace que la extracci\u00f3n de la informaci\u00f3n \u00fatil sea un gran problema. Cuando el usuario Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 14 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda navega por la web se encuentra frec uentemente saturado por los datos. La integraci\u00f3n de herramientas de miner\u00eda de datos puede ayudar a la extracci\u00f3n de la informaci\u00f3n \u00fatil. La Miner\u00eda de datos Web se puede clasif icar en tres grupos distintos no disjuntos, dependiendo del tipo de informaci\u00f3n que se quiera extraer, o de los objetivos [KB00]: Miner\u00eda del Contenido de la Web [Web Content Mining]: Extraer informaci\u00f3n del contenido de los documentos en la web. Se puede clasificar a su vez en: o Text Mining: Si los documentos son textuales (planos). o Hypertext Mining: Si los doc umentos contienen enlaces a s\u00ed mismos o a otros documentos o Markup Mining: Si los documentos son semiestructurados (con marcas). o Multimedia Mining: Para im\u00e1genes, audio, v\u00eddeo,... Miner\u00eda de la Estructura de la W eb [Web Structure Mining]: Se intenta descubrir un modelo a partir de la tipolog\u00eda de enlaces de la red. Este modelo puede ser \u00fatil para clas ificar o agrupar documentos. Miner\u00eda del Uso de la Web [Web Usage Mining]: Se intenta extraer informaci\u00f3n (h\u00e1bitos, preferencias, etc. de los usuarios o contenidos y relevancia de documentos) a partir de las sesiones y comportamiento de los usuarios navegantes 1.2. Miner\u00eda de Datos y Almacenamiento de Datos Como se ha enfatizado repetidamente, los datos son cr\u00edticos para hacer data mining. Por consiguiente, se necesit an sistemas de bases de datos para manejar los datos a los que aplicar dat a mining eficazmente. Estos sistemas podr\u00edan ser sistemas de data warehous e o sistemas de bases de datos. 1.2.1. Arquitectura, Modelado, Dise\u00f1o, y Aspectos de la Administraci\u00f3n Las t\u00e9cnicas de data mining exist en desde hace alg\u00fan tiempo. \u00bfPor qu\u00e9 entonces data mining se ha hecho tan popular ahora? La principal raz\u00f3n es que ahora con los sistemas de bases de datos se pueden representar, almacenar y recuperar los datos, y reforzar caracter\u00eds ticas como la integridad y seguridad. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 15 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Ahora que se tienen los datos guarda dos en las bases de datos y quiz\u00e1s normalizados y estructurados, \u00bfC\u00f3mo se puede hacer data mining? Un enfoque es reforzar un SGBD con una he rramienta de data mining. Se puede comprar un SGBD comercial y una herra mienta de data mining comercial que tenga construidas las interfaces par a el SGBD y se puede aplicar la herramienta a los datos administrados por el SGBD. A pesar de que este enfoque tiene ventajas y prom ueve las arquitecturas abiertas, hay algunos inconvenientes. Podr\u00eda haber algunos problemas de rendimiento cuando se usa un SGBD de prop\u00f3sito general para data mining. El otro enfoque es una integraci\u00f3n fuer te del SGBD con las herramientas de data mining. El n\u00facleo de la base de datos tiene las herramientas de data mining incorporadas dentro de \u00e9l. Se puede decir que este tipo de SGBD es un Mining SGBD (SGBD de data mining). Seg \u00fan esto las diferentes funciones del SGBD como el procesamiento de consul tas y la gesti\u00f3n del almacenamiento son influenciadas por las t\u00e9 cnicas de data mining. Por ejemplo, los algoritmos de optimizaci\u00f3n pueden ser modificados po r las t\u00e9cnicas de data mining. Se ha investigado mucho sobre la integraci \u00f3n de data mining y el n\u00facleo del SGBD (v\u00e9ase [TSUR98]). Mining SGBD tambi\u00e9n significar\u00eda la elim inaci\u00f3n de funciones innecesarias de un SGBD y el protagonismo de las caract er\u00edsticas clave. Por ejemplo, el procesamiento de transacciones es una funci\u00f3n soportada por la mayor\u00eda de los SGBD comerciales. Sin embargo, data mining normalment e no se dirige a los datos transaccionales sino a los dat os de apoyo a la toma de decisiones. Estos datos no pueden ser datos q ue se actualicen a menudo por transacciones. As\u00ed que, podr\u00edan eliminar se funciones como la gesti\u00f3n de transacciones en un Mining SGBD, y se podr\u00eda dar m\u00e1s importancia a las caracter\u00edsticas adicionales que proporcion en integridad y calidad a los datos. En el general, en el caso de un Mining SGBD, la agregaci\u00f3n de una herramienta de data mining influir\u00e1 sobre las diferentes funciones del SGBD como: el procesamiento de consultas, la gesti\u00f3n del almacenamiento, la gesti\u00f3n de transacciones, la gesti\u00f3n de metadata (di ccionario de datos), la gesti\u00f3n de la seguridad y de la integridad. El tipo de modelado de los datos us ado puede tener alg\u00fan impacto en data mining. Muchos de los datos que ser\u00e1n utilizados se guardan en bases de datos relacionales. Sin embargo, ac tualmente cada vez m\u00e1s se guardan los datos en bases de datos no relacionales tales como ba ses de datos orientadas a objetos, bases de datos objeto-relaciona les y bases de datos multimedia. Hay poca informaci\u00f3n sobre data minig en ba ses de datos orientadas a objetos, aunque si hay algunos trabaj os sobre data mining en las bases de datos multimedia. En las bases de datos orient adas a objetos prim ero se extraen las relaciones entre los objetos y se guardan en una base de datos relacional, y despu\u00e9s las herramientas de data mining se aplican a la base de datos relacional. El dise\u00f1o de la base de datos juega un papel fundamental en la aplicaci\u00f3n de data mining. Por ejemplo, en el caso de data warehousing, se han propuesto diferentes enfoques en el modelo y s ubsiguiente dise\u00f1o del almac\u00e9n. \u00c9stos Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 16 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda incluyen modelos multidimensionales de datos y modelos del procesamiento anal\u00edtico en l\u00ednea. Se han propuesto va rios esquemas como el esquema en estrella para el almacenamiento de los datos. Como se ha mencionado, la organizaci\u00f3n eficaz de los datos es cr\u00edti ca para data mining. Por consiguiente tambi\u00e9n, tales modelos y esquemas son importantes para data mining La administraci\u00f3n de las bases de datos tambi\u00e9n resulta influida por la realizaci\u00f3n de data mining. Si se int egra data mining un SGBD, aparecen las siguientes cuestiones \u00bfCon q u\u00e9 frecuencia ser\u00e1 aplica do data mining a la base de datos? \u00bfPuede ser usado data mining pa ra analizar la auditoria de datos? \u00bfComo influir\u00e1 en data mining la act ualizaci\u00f3n frecuente de los datos? \u00c9stas interesantes preguntas tendr\u00e1n res puestas cuando se obtenga m\u00e1s informaci\u00f3n sobre la integraci\u00f3n de data mining con las funciones del SGBD. 1.2.2. Data mining y Funciones de Bases de datos En el caso de integraci\u00f3n fuerte entre el SGBD y data mi ning hay un fuerte impacto sobre las diferentes funci ones del sistema de bases de datos. Por ejemplo, en el procesamiento de consul tas. Se han realizado trabajos para examinar lenguajes de consultas como SQL y determinar si se necesitan extensiones para soportar data mining (v \u00e9ase por ejemplo [ACM96a]). Si hay estructuras adicionales y consultas que son complejas, entonces el optimizador de consultas tiene que ser adaptado para manejar esos casos. Estrechamente relacionado con la optimizaci\u00f3n de cons ultas esta la eficiencia de las estructuras de almacenamiento, \u00edndices, y m\u00e9todos de acceso. Pueden ser necesarios mecanismos especiales para apoyar data mining en el procesamiento de consultas. En el caso de gesti\u00f3n de transacciones, la realizaci\u00f3n de data mining puede tener poco impacto, puesto que data mining se hace normalmente en los datos de apoyo a la toma de decisiones y no en los datos transaccionales. Sin embargo hay casos d\u00f3nde se analizan los datos transaccionales para anomal\u00edas como en los casos de tarjetas de cr\u00e9dito y de tarjetas de tel\u00e9fono. A veces las compa\u00f1\u00edas de tarjetas de cr\u00e9d ito o de tel\u00e9fono han notificado sobre usos an\u00f3malos de tarjetas de cr\u00e9dito o de tel\u00e9fono. Esto normalmente se hace analizando los datos transaccionales. Tambi \u00e9n se podr\u00eda aplicar data mining a estos datos. En el caso de metadata, se podr\u00eda aplic ar data mining a me tadata para extraer la informaci\u00f3n \u00fatil en casos d\u00f3nde lo s datos no sean analizables. \u00c9sta puede ser la situaci\u00f3n para datos no estr ucturados cuyo metadata deba ser estructurado. Por otro lado, los met adata podr\u00edan ser un recurso muy \u00fatil para una herramienta de data mining. Metadata podr\u00eda dar informaci\u00f3n adicional para ayudar con el proceso de data mining. La seguridad, integridad, ca lidad del datos, y toleranc ia a fallos son influidas por data mining. En el caso de s eguridad, data mining podr\u00eda suponer una amenaza importante para la seguridad y privacidad. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 17 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Por otro lado data mining pueden usarse para descubrir las intrusiones as\u00ed como para analizar la auditoria de datos. En el caso de auditoria, la cantidad de datos sobre los que se apl ica data mining es grand e. Se pueden aplicar las herramientas de data mining a los datos para descubrir los modelos anormales. Por ejemplo, si un empleado hace un exce sivo n\u00famero de viajes a un pa\u00eds determinado y este hecho es conocid o, proponiendo al gunas preguntas. La siguiente pregunta a realizar es si el empleado tiene asociaciones con ciertas personas de ese pa\u00eds. Si la respuesta es positiva, entonces la conducta del empleado se marca. Como ya se ha mencionado data mining tiene muchas aplicaciones en el descubrimiento de la intrusi\u00f3n y analiz ando amenazas a las bases de datos. Se puede usar data mining para descubrir modelos de intrusiones y amenazas. \u00c9sta es un \u00e1rea emergente y se llama Informaci\u00f3n de Confianza. No s\u00f3lo es importante tener datos de calidad, tambi \u00e9n es importante recuperarse de fallos maliciosos o de otro tipo, y proteger los datos de amenaz as o intrusiones. Aunque la investigaci\u00f3n en esta \u00e1r ea simplemente est\u00e1 empezando, se esperan grandes progresos. En el caso de calidad e in tegridad de los datos, se podr\u00edan aplicar las t\u00e9cnicas de data mining para descubrir datos malo s y mejorar la calidad de los datos. Data mining tambi\u00e9n pueden usarse par a analizar la seguridad de los datos para varios sistemas como sistemas de control de circulaci\u00f3n a\u00e9rea, sistemas nuclear, y sistemas de armamento. 1.2.3. DATA WAREHOUSE Un data warehouse es un tipo especial de base de dat os. Al parecer, el t\u00e9rmino se origin\u00f3 a finales de lo s ochenta [DEVL88], [INMO88], aunque el concepto es m\u00e1s antiguo. La referenc ia [INMO93] define un data warehouse como \"un almac\u00e9n de datos orientado a un tema, integrado, no vol\u00e1til y variante en el tiempo, que soporta decisiones de administraci\u00f3n\" (donde el t\u00e9rmino no vol\u00e1til significa que una vez que los datos han sido insertados, no pueden ser cambiados, aunque s\u00ed pueden ser borrados). Los data warehouses surgieron por dos razones: primero, la necesida d de proporcionar una fuente \u00fanica de datos limpia y consistente para prop\u00f3s itos de apoyo para la toma de decisiones; segundo, la necesidad de ha cerlo sin afectar a los sistemas operacionales. Por definici\u00f3n, las cargas de trabajo del data warehouse est\u00e1n destinadas para el apoyo a la toma de decisiones y por lo tanto, tienen consultas intensivas (con actividades ocasionales de inserci\u00f3n por lotes); asimismo, los propios data warehouses tienden a ser bastante grandes (a menudo mayores que 500GB y con una tasa de crecimiento de ha sta el 50 por ciento anual). Por consecuencia, es dif\u00edcil -aunque no im posible- perfeccionar el rendimiento. Tambi\u00e9n puede ser un problema la esca labilidad. Contribuy en a ese problema (a) los errores de dise\u00f1o de la base de datos, (b) el uso ineficiente de los operadores relacionales, (e) la debilidad en la implementac i\u00f3n del modelo relacional del DBMS, (d) la fa lta de escalabili dad del propio DBMS y (e) los errores de dise\u00f1o arquitect\u00f3nico que li mitan la capacidad e imposibilitan la escalabilidad de la plataforma. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 18 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda DATA MARTS Los usuarios a menudo realizaban amplias operaciones de informes y an\u00e1lisis de datos sobre un subconjunto rela tivamente peque\u00f1o de todo el data warehouse. Asimismo, era muy probable qu e los usuarios repitieran las mismas operaciones sobre el mismo subconj unto de datos cada vez que era actualizado. Adem\u00e1s, algunas de esas actividades -por ej emplo, an\u00e1lisis de pron\u00f3sticos, simulaci\u00f3n, modelado de dat os de negocios del tipo \"qu\u00e9 pasar\u00eda si...\"- involucraban la creaci\u00f3n de nuevos esquemas y datos con actualizaciones posteriores a esos nuevos datos. La ejecuci\u00f3n repetida de tale s operaciones sobre el mismo subconjunto de todo el almac\u00e9n no era muy eficiente; por lo tanto, pareci\u00f3 buena idea construir alg\u00fan tipo de \"almac\u00e9n\" lim itado de prop\u00f3sito general q ue estuviera hecho a la medida de ese prop\u00f3sito. Adem\u00e1s, en al gunos casos ser\u00eda posible extraer y preparar los datos requeridos directamente a partir de las fuentes locales, lo que proporcionaba un acceso m\u00e1s r\u00e1pido a los datos que si tuvieran que ser sincronizados con los dem\u00e1s datos ca rgados en todo el data warehouse. Dichas consideraciones condujeron al concepto de data marts. De hecho, hay alguna controversia s obre la definici\u00f3n precisa del t\u00e9rmino data mart. Se puede definir como \"un almac\u00e9n de datos especializado, orientado a un tema, integrado, vol\u00e1til y variante en el tiempo para apoyar un subconjunto espec\u00edfico de decisiones de administraci \u00f3n\". La principal diferencia entre un data mart y un data warehouse es que el data mart es especializado y vol\u00e1til. Especializado quiere decir que contiene datos par a dar apoyo (solamente) a un \u00e1rea espec\u00edfica de an\u00e1lisis de negocios; por vol\u00e1til se entiende que los usuarios pueden actualizar los datos e incluso, posiblemente, crear nuevos datos (es decir, nuevas tablas) para alg\u00fan prop\u00f3sito. Hay tres enfoques principales par a la creaci\u00f3n de un data mart: Los datos pueden ser simplemente ex tra\u00eddos del data warehouse; se sigue un enfoque de \"divide y vencer\u00e1s\" sobre la carga de trabajo general de apoyo para la toma de decis iones, a fin de lograr un mejor rendimiento y escalabilidad. Por lo general, los datos extra\u00eddos son cargados en una base de datos que tiene un esquema f\u00edsico que se parece mucho al subconjunto aplicab le del data warehouse; sin embargo, puede ser simplificado de alguna manera gracias a la naturaleza especializada del data mart. A pesar del hecho de que el data warehouse pretende proporcionar un \"punto de control \u00fanico\", un data mart puede ser creado en forma independiente (es decir, no por medio de la extracci\u00f3n a partir del data warehouse). Dicho enfoque puede se r adecuado si el data warehouse es inaccesible por alguna causa: ra zones financieras, operacionales o incluso pol\u00edticas (o puede ser que ni si quiera exista todav\u00eda el data warehouse). Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 19 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Algunas instalaciones han segui do un enfoque de \"primero el data mart\", donde los data marts son creados conforme van siendo necesarios y el data warehouse general es creado, finalmente, como una consolidaci\u00f3n de los diversos data marts. Los \u00faltimos dos enfoques sufren posible s problemas de desacople sem\u00e1ntico. Los data marts independientes son par ticularmente susceptibles a tales problemas, debido a que no hay forma obvia de verificar los desacoples sem\u00e1nticos cuando las bases de datos son dise\u00f1adas en forma independiente. Por lo general, la consolidaci\u00f3n de data marts en data wa rehouses falla, a menos que (a) se construya primero un esquema l\u00f3gico \u00fanico para el data warehouse y (b) los esquemas para los data marts individuales se deriven despu\u00e9s a partir del esquema del data warehouse. Un aspecto importante en el dise\u00f1o de data marts: es la granularidad de la base de datos. Donde granularidad se refiere al nivel m\u00e1s bajo de agregaci\u00f3n de datos que se mantendr\u00e1 en la base de da tos. Ahora bien, la mayor\u00eda de las aplicaciones de apoyo para la toma de dec isiones requerir\u00e1n tarde o temprano acceso a datos detallados y por lo tanto, la decisi\u00f3n ser\u00e1 f\u00e1cil para el data warehouse. Para un data mart puede ser m\u00e1s dif\u00edcil. La extracci\u00f3n de grandes cantidades de datos detallados del data wa rehouse, y su almacenamiento en el data mart, puede ser muy ineficiente si es e nivel de detalle no se necesita con mucha frecuencia. Por otro lado, en algunas ocasiones es dif\u00edcil establecer definitivamente cu\u00e1l es el nivel m\u00e1 s bajo de agregaci\u00f3n que en realidad se necesita. En dichos casos, los datos detallados pueden ser accedidos directamente desde el data warehouse cuando se necesiten, manteniendo en el data mart los datos que de al guna manera ya fueron agregados. APLICACIONES DE LOS DATA WAREHOUSE La explotaci\u00f3n del Data Warehouse pu ede realizarse mediante diversas t\u00e9cnicas: Query & Reporting On-line analytical processing (OLAP) Executive Information System (EIS) Decision Support Systems (DSS) Visualizaci\u00f3n de la informaci\u00f3n Data Mining \u00f3 Mi ner\u00eda de Datos, etc. Se llaman sistemas OLAP (On Line Analyt ical Processing) a aquellos sistemas que deben: Soportar requerimientos complejos de an\u00e1lisis Analizar datos desde diferentes perspectivas Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 20 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Soportar an\u00e1lisis complejos contra un volumen ingente de datos La funcionalidad de los sistemas OLAP se caracteriza por ser un an\u00e1lisis multidimensional de datos mediante navega ci\u00f3n del usuario por los mismos de modo asistido. Existen dos arquitecturas diferent es para los sistemas OLAP: OLAP multidimensional (MD-OLAP) y OLAP relacionales (ROLAP). La arquitectura MD-OLAP usa bases de datos multidimensionales, la arquitectura ROLAP implanta OLAP s obre bases de datos relacionales La arquitectura MD-OLAP requiere unos c\u00e1 lculos intensivos de compilaci\u00f3n. Lee de datos precompilados, y tie ne capacidades limitadas de crear agregaciones din\u00e1micamente o de hallar ra tios que no se hayan precalculado y almacenado previamente. La arquitectura ROLAP, accede a los datos almacenados en un Data Warehouse para proporcionar los an\u00e1lisis OLAP. La premisa de los sistemas ROLAP es que las capacidades OLAP se s oportan mejor contra las bases de datos relacionales. Los usuarios finales ejecutan sus an\u00e1lisis multidimensionales a trav\u00e9s del motor ROLAP, que transforma din\u00e1micamente su s consultas a consultas SQL. Se ejecutan estas consultas SQL en las bases de datos relacionales, y sus resultados se relacionan mediant e tablas cruzadas y conjuntos multidimensionales para devolver los resu ltados a los usuario s. ROLAP es una arquitectura flexible y general, que crece para dar soporte a amplios requerimientos OLAP. El MOLAP es una soluci\u00f3n particular, adecuada para soluciones departamentales con unos vol\u00famenes de informaci\u00f3n y n\u00famero de dimensiones m\u00e1s modestos. Una cuesti\u00f3n t\u00edpica de un sistema OLAP o DSS podr\u00eda ser: \"\u00bfCompraron m\u00e1s monovol\u00famenes en 1998 los habitantes de l norte de Espa\u00f1a, o los del sur?\" Sin embargo, un sistema data mining en este escenario podr\u00eda ser interrogado as\u00ed: \"Quiero un modelo que identifique las caracter\u00edsticas predictivas m\u00e1s importantes de las personas que compran monovolumenes...\" QUERY & REPORTING Las consultas o informes libres trabajan t anto sobre el detalle como sobre las agregaciones de la informaci\u00f3n. Realizar este tipo de explotaci\u00f3n en un almac\u00e9n de datos supone una optimizaci\u00f3n del tradiciona l entorno de informes (repor ting), dado que el Data Warehouse mantiene una estructura y una tecnolog\u00eda mucho m\u00e1s apropiada para este tipo de solicitudes. Los sistemas de \"Query & Reporting\", no basados en almacenes de datos se caracterizan por la complejidad de las consultas, los alt\u00edsimos tiempos de Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 21 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda respuesta y la interferencia con otros procesos inform\u00e1ticos que compartan su entorno. 1.2.4. DATA WAREHOUSE Y DATA MINING Data warehouse almacena los datos de las bases de datos heterog\u00e9neas para que los usuarios consulten s\u00f3lo un \u00fani co aspecto. Las respuestas que un usuario consigue a una consulta dependen de los vol\u00famenes del data warehouse. El data warehous e en general no intenta extraer la informaci\u00f3n de los datos almacenados. Data warehouse estructura y organiza los datos para suportar funciones de administraci\u00f3n, dat a mining intenta extraer la informaci\u00f3n \u00fatil, as\u00ed como predecir las tendencias de los datos. La Figur a 3 10 ilustra la relaci\u00f3n entre el data warehouse y data mining. Observe que no es necesario construir un data warehouse para hacer data mining, ya que tambi\u00e9n puede aplicarse data mining a las bases de da tos. Sin embargo, un data warehouse estructura los datos de tal manera que fa cilita data mining, por lo que en muchos casos es muy deseable tener un almac\u00e9n del datos para llevar a cabo data mining.. \u00bfD\u00f3nde acaba data warehouse y donde em pieza data mining? \u00bfHay una diferencia clara entre data warehouse y data mining? La respues ta es subjetiva. Hay ciertas preguntas que los data war ehouse pueden contes tar. Adem\u00e1s, los data warehouse disponen de capacida des para el apoyo a la toma de decisiones. Algunos data warehouse llev an a cabo predicciones y tendencias. En este caso los data warehouse lle van a cabo algunas de las funciones de data mining. En el general, en el caso de un data warehouse la respuesta est\u00e1 en la base de datos. El data warehouse tiene que disponer de optimizaci\u00f3n de consultas y t\u00e9cnicas de acceso par a obtener respuestas. Por ejemplo, considere preguntas como \u00bf\"Cu\u00e1ntos aut om\u00f3viles rojos compraron los m\u00e9dicos en 1990 en Nueva York \"? La respuesta est\u00e1 en la base de datos. Sin embargo, para una pregunta como \" \u00bfC u\u00e1ntos autom\u00f3viles rojos comprar\u00e1n los m\u00e9dicos en 2005 en Nueva York \"? la respuesta no puede estar en la base de datos. Bas\u00e1ndose en los patrones de comp ra de los m\u00e9dicos en Nueva York y sus proyecciones del sueldo, se podr\u00eda predecir la respuesta a esta pregunta. Esencialmente, un warehouse organiza los datos eficazmente para realizar data mining sobre ellos. La pregunta es entonces \u00bfEs imprescindible tener un warehouse para hacer data mining? La re spuesta es que es muy interesante tener un warehouse, pero esto no signifi ca que sea impre scindible. Podr\u00eda usarse un buen SGBD para gestionar una base de datos eficazmente. Tambi\u00e9n, a menudo con un warehouse no se tienen datos transaccionales. Por lo tanto, los datos no pueden ser actuales, y los resultados obtenidos desde data mining tampoco lo ser\u00e1n. Si se necesita la informaci\u00f3n actualizada, entonces se podr\u00eda hacer data mining sobre una base de datos administrada por un SGBD que tambi\u00e9n tenga cara cter\u00edsticas de procesamiento de transacciones. Hacer data mining sobre datos que se actualizan a menudo es un desaf\u00edo. T\u00edpicamente data mining se ha usado sobre los datos de apoyo a la toma de decisiones. Por consiguiente hay varios problemas que necesitan ser investigados extensamente, antes de que se pueda llevar a cabo lo que se conoce como data mining en tiempo real. De momento al menos, es cr\u00edtico Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 22 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda disponer de un buen data warehouse para llevar a cabo un buen data mining para funciones de apoyo a la toma de decisiones. Observe que tambi\u00e9n se podr\u00eda tener una herramienta integrada para llevar a cabo las funciones de data warehouse y data mining. Una herramienta de este tipo ser\u00e1 conocida como data warehouse miner. 1.3. Herramientas Comerciales de An\u00e1lisis de Datos KnowledgeSeeker de Angoss Softwa re International, Toronto, Canada Puntos Clave: Herramienta interactiva de clasificaci\u00f3n. Basada en los algoritmos de \u00e1rbol es de decisi\u00f3n CHAID y XAID. Se ejecuta sobre plataformas Windows y UNIX Ventajas: Representaci\u00f3n flexible de \u00e1rboles de decisi\u00f3n. Provee caracter\u00edsticas para permitir la identificaci\u00f3n de la relevancia de los resultados en los negocios. El API permite usar los resultados del an\u00e1lisis en aplicaciones personalizadas. Aspectos a tener en cuenta: Solo soporta \u00e1rboles de decisi\u00f3n Poco soporte para la transformaci\u00f3n de datos. El soporte para predicci\u00f3n se limit a a la exportaci\u00f3n de las reglas generadas. Cuando usarla: Si se necesita una herramient a que permita adelantar una visi\u00f3n instant\u00e1nea general de sus datos. Si necesita una herramienta interactiva para explorar sus datos. No est\u00e1 indicada si se necesit a una herramienta que soporte predicci\u00f3n desde dentro de sus datos. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 23 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda DataCruncher de DataMind, San Mateo, CA, USA Puntos Clave: Herramienta de Data Mining para clasificaci\u00f3n y clustering Basada en Tecnolog\u00eda de agentes de redes (ANT Agent Network Technology) La aplicaci\u00f3n servidor se ejec uta sobre UNIX y Windows NT; la aplicaci\u00f3n cliente en todas las plataformas Windows. Ventajas: F\u00e1cil de usar, ya que los modelos necesitan pocas adaptaciones. Agent Network Technology puede ser utilizada para clasificaci\u00f3n, predicci\u00f3n y clustering no supervisado. Resultados vers\u00e1tiles, que permit en una minuciosa valoraci\u00f3n de los modelos y de sus resultados Aspectos a tener en cuenta: Se necesita familiarizarse con la tecnolog\u00eda para comprender los resultados. Est\u00e1 basada en una t\u00e9cnica propietaria Tiene soporte limitado para la transformaci\u00f3n de datos. Cuando usarla: Si se necesita una herramienta client e-servidor con una interface f\u00e1cil de usar. Si se necesita valorar para cada caso la bondad de la predicci\u00f3n de los modelos. Si quiere invertir alg\u00fan esfuerzo en hacer un completo uso del an\u00e1lisis de resultados. Intelligent Miner de IBM, Armonk, NY, USA Puntos Clave: Soporta m\u00faltiples operaciones de data minino en un entrono cliente- servidor Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 24 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Utiliza redes de neuronas, \u00e1rboles de inducci\u00f3n y varias t\u00e9cnicas estad\u00edsticas. Trabaja sobre clientes Windows, OS /2 y X-Windows, y servidores AIX (incluyendoSP2), OS/400 y OS/390. Ventajas: Buen soporte para an\u00e1lisis de asociaciones y clustering (incluyendo visualizaci\u00f3n de clustering), adem\u00e1s de clasificaci\u00f3n y predicci\u00f3n. Optimizada para data minino en grandes bases de datos(del orden de gigabytes) ya que se aprovecha de la plataforma de procesamiento paralelo PS2 de IBM. Tiene un entorno de trabajo int egrado con caracter\u00edsticas muy interesantes tanto para usuarios expertos como no especialistas. Aspectos a tener en cuenta: Algunos problemas que ten\u00eda han si do resueltos con la nueva interface que ha sido desarrollada completamente en Java. Solo trabaja sobre plataf ormas IBM, y el acceso a los datos se limita a las bases de datos DB2 y a ficheros planos. Inicialmente la mayor\u00eda de los proy ectos requerir\u00e1n entr adas importantes desde los servicios de soporte y consultor\u00eda de IBM Cuando usarla: Deber\u00eda ir a una tienda de IBM para obs ervar la funcionalidad del data mining integrado en su entorno de soporte a las decisiones Para grandes proyectos de data mi ning, en particular cuando los datos est\u00e1n contenidos en DB2. Si se desan utilizar varias operaciones de data mining, tales como clasificaci\u00f3n, clustering y an\u00e1lisis de asociaciones. Para realizar an\u00e1lisis de cesta de la compra con varios gigabytes de datos. Si interesa utilizar los servicios de consultor\u00eda de IBM. Clamentine de Integral Solutions, Basingstoks, UK Puntos Clave: Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 25 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Herramienta con un entrono de trabajo que soporta todo el proceso de data mining Ofrece \u00e1rboles de decisi\u00f3n, redes de neuronas, generaci\u00f3n de reglas de asociaci\u00f3n y caracter\u00edsticas de visualizaci\u00f3n. Se ejecuta sobre VM S, UNIX o Windows NT. Ventajas: Interface gr\u00e1fica intuitiva para programaci\u00f3n visual. Las t\u00e9cnicas de data mining pueden complementarse combin\u00e1ndose entre si. Visi\u00f3n interactiva de las relaciones ent re las variables a trav\u00e9s de grafos de red. Aspectos a tener en cuenta: No soporta Windows nativo. Es necesario familiarizarse con la herramienta para conseguir una \u00f3ptima utilizaci\u00f3n de su s funcionalidades. No est\u00e1 optimizada para arquitecturas en paralelo. Cuando usarla: Si se necesita una herramienta que c ubra por completo el rango de los procesos de data mining. Si se desean combinar herramient as y modelos para construir los procesos de data mining que exijan tales requisitos. Si se desea desarrollar el modelo en C. Si se necesitan grandes capacidades anal\u00edticas y de gesti\u00f3n de datos sin requerir un extenso an\u00e1lisis de datos ni experiencia en tecnolog\u00edas inform\u00e1ticas. Alice de Isoft SA, Gif sur Yvette, Francia. Puntos Clave: Herramienta de escritorio para data minino interactivo. Se basa en tecnolog\u00eda de \u00e1rboles de decisi\u00f3n. Se ejecuta sobre plataformas Windows. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 26 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Ventajas: La representaci\u00f3n altamente intera ctiva permite guiar el an\u00e1lisis. La opci\u00f3n de generar gr\u00e1ficos provee una visi\u00f3n general de los datos en todas las etapas del proceso de Data Mining. Se trata de una herramienta econ\u00f3m ica valida para usuarios que comienzan a realizar data mining. Aspectos a tener en cuenta: No tiene opciones para desarrollar modelos. Peque\u00f1o soporte para transformaci\u00f3n de datos. No genera conjuntos de reglas opt imizadas desde los \u00e1rboles de decisi\u00f3n. Cuando usarla: Si se desea usar data mining para buscar patrones y relaciones en los datos. Si se quiere tener la posibilidad de dirigir el an\u00e1lisis interactivamente. Si no se es un experto en data mining y se desea realizar el an\u00e1lisis. Si se quiere entender los patr ones que se encuentran en la base de datos y no se desea construir modelos predictivos. Decisi\u00f3n Series, de NeoVista Software Cupertino CA, USA. Puntos Clave: Herramientas para m\u00faltiples oper aciones de data mining para el desarrollo de modelos basados en servidores. Proporciones algoritmos de redes de neuronas, \u00e1rboles y reglas de inducci\u00f3n, clustering y an\u00e1lisis de asociaciones. Trabaja sobre sistemas UNIX mono o multi-procesadores de HP y Sun. Accede s\u00f3lo a ficheros planos , aunque posiblemente las \u00faltimas versiones ya trabajaran contra bases de datos relacionales. Ventajas: Soporta un gran rango de operaci ones y algoritmos de data mining, la mayor\u00eda de los cuales han sido altam ente optimizados para obtener altos rendimientos. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 27 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Est\u00e1 optimizado para plataformas que trabajan en paralelo con grandes conjuntos de datos. Ofrece una considerable flexibilidad para construir modelos de alto rendimiento para aplicaciones de usuario final embebidas. Aspectos a tener en cuenta: Las herramientas de desarrollo gr\u00e1fico son bastante b\u00e1sicas. Poco soporte para la exploraci\u00f3n de datos. La mayor\u00eda de los clientes necesitaran un considerable soporte de consultas para generar aplicaciones y ejecutarlas. Es necesario tener conocimientos de an\u00e1lisis de dat os y de utilizaci\u00f3n de UNIX para desarrollar las aplicaciones. Cuando usarla: Si se desean construir aplicaciones con alto rendimiento de modelos de data mining embebidos que utilizan ent ornos con multiprocesadores. Si se quiere tener un absoluto cont rol sobre todos los elementos de los procesos de construcci\u00f3n de modelos. Si se necesitan combinar operac iones y tecnicas de data mining alternativas en aplicaciones complejas. Si se quiere trabajar con una so luci\u00f3n que puede comunicar una aplicaci\u00f3n data minino para enlazar con sus necesidades. Pilot Discovery Server de Pilo t Software, Cambridge MA, USA. Puntos Clave: Herramienta para clasificaci\u00f3n y predicci\u00f3n. Basada en la tecnolog\u00eda de \u00e1rboles de decisi\u00f3n CART. Trabaja sobre UNIX y Windows NT Ventajas: Buena representaci\u00f3n del an\u00e1lisis de resultados Es f\u00e1cil de usar y de entender. Muy integrada con sistemas gestores de bases de datos relacionales. Aspectos a tener en cuenta: Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 28 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Solamente indicada para clientes de los programas para soporte a la toma de decisiones de Pilot. Solamente cubre un especifico sect or del espectro del data mining. S\u00f3lo trabaja con datos almacenados en bases de datos relacionales. Cuando usarla: Si se desea optimizar las campa\u00f1as de marketing. Si se necesita interpretar f\u00e1cilmente los resultados sin realizar un gran refinamiento de los modelos. Solo si se est\u00e1n utilizando los pr ogramas para soporte a la toma de decisiones de Pilot. No est\u00e1 indicada si se quieren resolver los problemas utilizando diferentes t\u00e9cnicas. SAS Solution for Data Mining de SAS Institute, Cary, NC, USA Puntos Clave: Un gran n\u00famero de herramientas de selecci\u00f3n, exploraci\u00f3n y an\u00e1lisis de datos para entornos cliente-servidor. Las opciones de data mining incl uyen: aplicaciones de redes de neuronas, de \u00e1rboles de decisi\u00f3n y herramientas de estad\u00edstica. Aplicaciones portables para un gr an n\u00famero de entornos PC, UNIX y mainframes. Ventajas: SAS ofrece data warehouse y an\u00e1lisis de datos. Conjuntos extensibles de herramient as de manipulaci\u00f3n y visualizaci\u00f3n de datos. SAS tiene una gran experiencia en herramientas estad\u00edsticas y de an\u00e1lisis de datos. Aspectos a tener en cuenta: La oferta para hacer data mining es una mezcolanza de todas las t\u00e9cnicas SAS existentes. Integraci\u00f3n con la programaci\u00f3n en 4GL. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 29 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda No soporta el an\u00e1lisis de asociaciones. Cuando usarla: Si ya se utiliza SAS para almacenar , administrar y analizar los datos. Si se va a utilizar SAS para la construcci\u00f3n del data warehouse. Si es necesaria una alta funcionalidad en la manipulaci\u00f3n de datos. Si se es experto en estad\u00edstica y se quieren utilizar las funciones estad\u00edsticas de SAS. MineSet, de Silicon Graphi cs, Mountain View, CA, USA Puntos Clave: Paquete de herramientas para Data mining y visualizaci\u00f3n. Proporciona algoritmos par a la generaci\u00f3n de reglas para clasificaci\u00f3n y asociaciones. Trabaja sobre plataformas SGI bajo IRIS. Ventajas: Ofrece herramientas de visualizaci\u00f3n para los datos y los modelos generados. Suporta muchas operaciones de data mining. El gestor de herramientas act\u00faa como un punto central de control y permite el acceso y transformaci\u00f3n de los datos. Aspectos a considerar: Requiere un servidor SGI. La gran cantidad de opciones y par\u00e1 metros puede provocar confusi\u00f3n en usuarios noveles. Las herramientas de visualizaci \u00f3n necesitan mucha preparaci\u00f3n y personalizaci\u00f3n de los datos par a producir buenos resultados. Cuando usarla: Si se quieren detectar patrones por visualizaci\u00f3n. Si se quieren construir aplicaciones que representen los resultados de data mining a trav\u00e9s de visualizaci\u00f3n. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 30 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Si se dispone de equipos de Silicon Graphics Esta indicada para VARs que quieran desarrollar soluciones personalizadas de data mining usando MineSet. SPSS, de SPSS, Chicago IL, USA Puntos Clave: Herramientas de escritorio para clasif icaci\u00f3n y predicci\u00f3n, clustering, y un gran rango de operaciones estad\u00edsticas. Proporciona una herramienta de redes de neuronas adem\u00e1s de productos de an\u00e1lisis estad\u00edstico. SPSS para Windows y Neural Connec tion son productos que trabajan en modo monopuesto en plataformas Windows. Ventajas: Las funciones de an\u00e1lisis estad\u00edstic o complejo son accesibles a trav\u00e9s de una interface de usuario muy bien dise\u00f1ada. Neural Connection ofrece un amplio rango de opciones y funciones a trav\u00e9s un entorno de desarrollo muy f\u00e1cil de usar. El lenguaje de scripts permite una gr an personalizaci\u00f3n del entorno y el desarrollo de aplicaciones estad\u00edsticas aisladas. Aspectos a considerar: Para analistas de datos y estad\u00edsticos, m\u00e1s que para usuarios finales. SPSS CHAID carece de la funcionalidad de otros productos de escritorio de \u00e1rboles de decisi\u00f3n. Neural Connection es un producto aisl ado: la base de la integraci\u00f3n con SPSS es a trav\u00e9s de transferencia de datos, que se limita a la importaci\u00f3n de 32.000 registros. Cuando usarla: Si se necesita un an\u00e1lisis complejo combinando estad\u00edstica con \u00e1rboles de decisi\u00f3n y redes de neuronas. Si se disponen de grandes conocimientos estad\u00edsticos y se quiere utilizar data mining basado en IA. Si se necesita verificaci\u00f3n estad\u00edstica de los resultados encontrados. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 31 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Si es preciso construir aplicac iones de an\u00e1lisis departamental para escritorio. Si tiene un presupuesto ajustado. Syllogic Data Mining Tool, de Sy llogic, Houten, The Netherlands Puntos Clave: Herramienta con entorno de trabajo mu lti-estrat\u00e9gico con interface visual. Soporta an\u00e1lisis de \u00e1rboles de decis i\u00f3n, clasificaci\u00f3n k-vecino m\u00e1s pr\u00f3ximo, y an\u00e1lisis de clusteri ng y asociaciones por k-means. Trabaja sobre Windows NT y en es taciones UNIX con uno o varios procesadores Ventajas: La interface visual permite a los usuarios construir proyectos de data mining enlazando objetos. La versi\u00f3n est\u00e1 optimizada para entornos masivam ente paralelos y validos para grandes bases de datos. La empresa tambi\u00e9n ofrece un gran n\u00famero de servicios de consultar\u00eda en las \u00e1reas de datawarehousing y data mining. Aspectos a considerar: La interface y la presentaci\u00f3n de resultados necesita algunos refinamientos para ser utilizada por usuarios finales. DMT/MP no soportan el mism o rango de operaciones que DMT Cuando usarla: Si se necesita servicio de consultor\u00eda a la vez que se desarrolla el proyecto de data mining con un entorno de datawarehousing. Si se necesita utilizar gran n\u00fam ero de operaciones de data mining. Si se quiere utilizar una herramient a similar en el escritorio y en el entorno MP. Darwin de Thinking Machines, Bedford MA, USA Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 32 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Puntos Clave: Herramientas de desarrollo de data mining de tipo cliente-servidor para la construcci\u00f3n de modelos de clasificaci\u00f3n y predicci\u00f3n. La construcci\u00f3n de modelos utiliz a algoritmos de redes de neuronas, \u00e1rboles de inducci\u00f3n y k-vecino m\u00e1s pr\u00f3ximo. Trabaja sobre plataformas Sun de Solaris, AIX de IBM y SP2, con clientes Motif. Tambi\u00e9n existen ve rsiones cliente que trabajan sobre Windows. Ventajas: Ofrecen buena cobertura al proceso completo de descubrimiento del conocimiento. Pone el \u00e9nfasis en el desarrollo de modelos predictivos de alto rendimiento. Proporciona escalabilidad par a soportar paralelizaci\u00f3n. Aspectos a considerar: Mejor para analistas de datos y desarrolladores de aplicaciones que para los usuarios de negocio. Es preciso familiarizarse con las diferentes opciones de Darwin para cada tipo de modelo si se quiere obtener el mejor resultado de la herramienta. No soporta an\u00e1lisis no supervisa do de clustering o de asociaciones. Cuando usarla: En la construcci\u00f3n de aplicaciones de data mining para gesti\u00f3n de relaciones entre clientes. Si se necesita una herramienta que ponga mucho \u00e9nfasis en modelado por clasificaci\u00f3n y predictivos. Si se dispone de una gran comp leja base de datos que precise la potencia de una plataforma con multiprocesadores. Si se necesita observar la creaci\u00f3n de los modelos de data mining, Darwin proporciona m\u00faltiples al goritmos y varias opciones de refinamiento. Si se quiere usar las herramientas de data mining para auxi liar la gesti\u00f3n de redes Thinking Machina ti ene objetivos muy expl\u00edc itos en este sector y ya colabora con Cabletron. Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 33 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 1.4. Arquitectura Software para Data Mining Anteriormente se han discutido diferent es tecnolog\u00edas para data mining. Se necesita el apoyo arquitect\u00f3nico para inte grar estas tecnolog\u00edas. La Figura 1.4 muestra una pir\u00e1mide que presenta la estructura de c\u00f3mo las diferentes tecnolog\u00edas encajan entre si. Como se mu estra en esta figura, en el nivel m\u00e1s bajo se encuentra las comunicaciones y sistemas. A conti nuaci\u00f3n aparece el soporte del middleware. Esto va seguido por la gesti\u00f3n de la bases de datos y el data warehouse. Despu\u00e9s aparecen la s diferentes tecnolog\u00edas de data mining. Finalmente, se tienen los sistemas de apoyo a la toma de decisiones que usan los resultados de data mining y ayudan a que los usuarios tomen las decisiones eficazmente. Estos usuarios pueden ser administradores, analistas, programadores, y cualquier otro usuar io del sistema de informaci\u00f3n. Cuando se construyen sistemas, las diferentes tecnolog\u00edas involucradas pueden no encajar exactamente en la pir\u00e1 mide tal como se ha mostrado. Por ejemplo, se podr\u00eda saltar la fase de data warehouse y se podr\u00eda ir directamente a la herramienta de data mining. Uno de los problemas importantes, en este punto, son las interfaces entre los diferent es sistemas. En la actualidad no se tiene bien definida cual quiera de las interfaces no rmales excepto en el caso de algunos de los lenguajes est\u00e1ndar de definici\u00f3n de in terfaz que surgen de los diferentes grupos como el Object Management Group. Si n embargo, cuando estas tecnolog\u00edas vayan madurando, se ir\u00e1n desarrollando los est\u00e1ndares para las interfaces. Figura 1.4: Pir\u00e1mide para Data mining Ya se ha estudiado c\u00f3mo las diferent es tecnolog\u00edas trabajan juntas. Por ejemplo, una posibilidad es la mostrada en la Figur a 1.5 donde se integran m\u00faltiples bases de datos a trav\u00e9s de alg\u00fan middleware y como consecuencia forman un data warehouse que se explora a continuaci\u00f3n. Los componentes de data mining tambi\u00e9n se integran en este escenario para aplicar data mining a Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 34 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda las bases de datos directam ente. Algunos de estos probl emas se discutir\u00e1n en la secci\u00f3n de la arquitectura del sistema. Figura 1.5: Arquitectura de data mining La figura 1.6 ilustra una vista tridimensiona l de las tecnolog\u00edas de data mining. En el centro se encuentra la tecnolog\u00ed a para la integraci\u00f3n. \u00c9sta es la tecnolog\u00eda del middleware tal como la ges ti\u00f3n distribuida orie ntada al objeto y tambi\u00e9n la tecnolog\u00eda web para la int egraci\u00f3n y acceso a trav\u00e9s de web. Figura 1.6: Visi\u00f3n en tres dimensiones Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 35 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda En una primera dimensi\u00f3n tenemos t odas las tecnolog\u00edas b\u00e1sicas de datos como multimedia, bases de datos relacionales y orientadas a objetos, y bases de datos distribuidas, heterog\u00e9neas y de he rencia. En la segunda dimensi\u00f3n tenemos las tecnolog\u00edas para realizar da ta mining. Aqu\u00ed se ha incluido el warehousing as\u00ed como el aprendizaje autom \u00e1tico, tal como la programaci\u00f3n de la l\u00f3gica inductiva, y el razonamient o estad\u00edstico. La tercera dimensi\u00f3n comprende tecnolog\u00edas como el proces amiento paralelo, la visualizaci\u00f3n, gesti\u00f3n de metadatos (diccionario de dat os), y el acceso seguro que son importantes para llevar a cabo data mining. 1.4.2. Arquitectura Funcional A continuaci\u00f3n se describen los compo nentes funcionales de data mining. Anteriormente se discutieron los com ponentes funcionales de un sistema de gesti\u00f3n de bases de datos. En adici\u00f3n, se mostro una arquitectura en la que la herramienta de data mining er a uno de los m\u00f3dulos de l SGBD. Un SGBD con estas caracter\u00edsticas ser\u00e1 un SGBD Mining. Un SGBD Mining se puede organizar de varias maneras. Un enfoque alte rnativo se ilustra en Figura 4. En este enfoque se considera data mining como una extensi\u00f3n del procesador de consultas. Es decir, podr\u00edan extender se los m\u00f3dulos del procesador de consultas como el optimizador de cons ultas para ocuparse de data mining. Esto es una vista de alto nivel como se ilustra en la Figura 1.7. Observe que en este diagrama se ha omitido al gestor de la s transacciones, ya que data mining se usa principalmente en el proces amiento anal\u00edtico en l\u00ednea (OLTP). Figura 1.7: Data mining como parte del procesador de consultas La pregunta es: \u00bfCu\u00e1les son los componentes de la herramienta de data mining? Como se ilustra en la Figur a 1.8, una herramienta de data mining podr\u00eda tener los siguientes componentes : un componente de aprendizaje de experiencia que usa varios conjunt os de entrenamiento y aprende varias estrategias, un componente analizador de datos que analiza los datos en base a lo que tiene que aprender, y un compo nente productor de resultados que realiza la clasificaci\u00f3n, el clustering, y otras tareas como las asociaciones. Hay Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 36 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda interacci\u00f3n entre los tres component es. Por ejemplo, el componente que produce los resultados entrega los re sultados obtenidos al componente de entrenamiento para ver si este co mponente tiene que ser adaptado. El componente de entrenamiento da la in formaci\u00f3n al componente analizador de datos. El componente de analizador de dat os da la informaci\u00f3n al componente productor de los resultados. Figura 1.8: Las Funciones de data mining Observe que no se han incluido componente s tales como el preprocesador de datos y el podador (refinador) de los resu ltados en los m\u00f3dulos de data mining. Estos componentes tambi\u00e9n son necesario s para completar el proceso entero. El preprocesador de datos formatea los datos. De alguna forma el data warehouse puede hacer esta funci\u00f3n. El componente de poda o recorte de resultados puede extraer s\u00f3lo la informaci\u00f3n \u00fatil. Esto podr\u00eda llevarse a cabo por un sistema de apoyo a la toma de decisiones. Todos estos pasos se integrar\u00e1n en el proceso de data mining. 1.4.3. Arquitectura del Sistema Algunas de las arquitecturas que se ha n discutido anteriormente as\u00ed como la observada en la Figura 1.5 pueden cons iderarse como una arquitectura del sistema para data mining. Una arquitectura del sistema consiste en componentes como los middleware y otro s componentes del sistema como el sistema de bases de datos y el sistem a de data warehouse para data mining. Los middleware que se ilustran en Figura 1.5 podr\u00ed an basarse en diferentes tecnolog\u00edas. Un sistema middleware mu y popular es el qu e se basa en una arquitectura cliente-servidor. En efecto, muchos de los sistemas de bases de datos se basan en la arquitectura cliente-servidor. Middlewar e tambi\u00e9n incluye de facto est\u00e1ndares como el Open DataBase Connectivity Connectivity (ODBC) de Microsoft o sistemas distribuidos basados en objetos. En [THUR97] se proporciona una discusi \u00f3n detallada de tecnolog\u00edas cliente- servidor. En particular se discute el par adigma de cliente-servidor as\u00ed como una apreciaci\u00f3n global de ODBC y los sistemas de gesti\u00f3 n distribuida de objetos como el Object Manegement Group's (O MG) Common Object Request Broquer Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 37 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Architecture (CORBA). Aqu\u00ed se discute data mining con respecto al paradigma del cliente-servidor. La mayor\u00eda de los vendedores de sistem as de bases de datos han migrado a una arquitectura llamada arquitectura de c liente-servidor. Con este enfoque, m\u00faltiples clientes acceden a los diferent es servidores de las bases de datos a trav\u00e9s de alguna red. Una visi\u00f3n de al to nivel de la comunicaci\u00f3n cliente- servidor de se ilustra en la Figura 1.9. El objetivo \u00faltimo es comunicar m\u00faltiples clientes vendedores con m\u00faltiples serv idores vendedores de una manera transparente. Figura 1.9: La Arquitectura cliente-servidor de Basada en la Interoperabilidad En orden a facilitar la comunicaci\u00f3n ent re m\u00faltiples clientes y servidores, se han propuesto varios est\u00e1ndares. Un ejem plo es la Organizaci\u00f3n Internacional de Est\u00e1ndares (ISO), el est\u00e1ndar Remote Database Access (RDA). Esta norma provee una interfaz gen\u00e9rica para la comunicaci\u00f3n entre un cliente y un servidor. Microsoft ODBC tambi\u00e9n ha aumentado su popularidad para la comunicaci\u00f3n de los clientes con los se rvidores. El CORBA de OMG mantiene las especificaciones para las comunicaci ones cliente-servid or basadas en la tecnolog\u00eda orientada a objetos. Aqu\u00ed, una posibilidad es enc apsular las bases de datos servidoras como objetos y dist ribuir las peticiones apropiadas de los clientes y acceder los serv idores a trav\u00e9s de un Obje ct Request Broker (ORB). Otros est\u00e1ndares incluyen el DRDA de IB M (Distribuited Relational Database Access - el Acceso de la base de datos relacional Distribuida ) y el SQL Access Group (ahora parte del Open Group); Call Level Interface la Interfaz de Nivel de Llamada (CLI). Se han publicado varios lib ros sobre computaci\u00f3n cliente- servidor y administraci\u00f3n de datos. Dos buenas referencias son [ORFA94] y [ORFA96]. Tambi\u00e9n se estudian en detal le algunos de estos problemas en [THUR97]. Un sistema de middleware que est\u00e1 aum entando su popularidad para conectar sistemas heterog\u00e9neos es el CORBA de OMG. Como se declara en [OMG95], hay tres componentes principales en COR BA. Uno es el mo delo orientado a objetos, el segundo es Object Request Broker el Corredor de Demanda de Objeto (ORB) a trav\u00e9s del cual los clientes y servidores se comunican entre s\u00ed, y el tercero es Interface Definition Language el Lenguaje de Definici\u00f3n de Interfaces (IDL) qu\u00e9 espec\u00edfica las interfac es para la comunicaci\u00f3n cliente- servidor. La Figura 1.10 ilustra la comuni caci\u00f3n cliente-servidor a trav\u00e9s de Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 38 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ORB. Aqu\u00ed, los clientes y servidores est\u00e1n encapsulados como objetos. Los dos objetos comunican entonces entre s\u00ed. La comunicaci\u00f3n se hace mediante ORB. Adem\u00e1s, las interfaces deben ajustarse a IDL. Figura 1.10: La interoperabilidad a trav\u00e9s del ORB 1.4.4. El Data Mining en la Arquitectura del Sistema Considere la arquitectura de la Figura 8. En este ejemplo, la herramienta de data mining podr\u00eda usarse como un serv idor, los sistemas de administraci\u00f3n de bases de datos podr\u00edan ser otro servidor , mientras el data warehouse ser\u00eda un tercer servidor. El cliente emite las peticiones al sistema de base de datos, al warehouse, y al componente de data mining como se ilustra en la figura 1.11. Figura 1.11: Data mining basado en Cliente-Servidor Tambi\u00e9n se podr\u00eda usar un ORB para data mi ning. En este caso la herramienta de data mining se encapsula como un obj eto. El sistema de bases de datos y warehouse tambi\u00e9n son objetos. Esto se ilustra en la Figura 1.12. El desaf\u00edo aqu\u00ed es definir IDLs para varios objetos. Obs\u00e9rvese que la tecnolog\u00eda cliente-serv idor no desarrolla algoritmos para la administraci\u00f3n de datos, para warehousin g, o para la realizaci\u00f3n de data Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 39 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda mining. Esto significa que todav\u00eda se neces itan los algoritmos para realizar data mining, warehousing, y administraci\u00f3n de la base de datos. La tecnolog\u00eda cliente-servidor y, en particular, la te cnolog\u00eda de administraci\u00f3n distribuida de objetos como CORBA, es la que facilita la \u00ednteroperaci\u00f3n entre los diferentes componentes. Por ejemplo, el sistem a data mining, el sistema de base de datos, y warehose comunican entre s\u00ed y con los clientes a trav\u00e9s del ORB. Figura 1.12: Data mining mediante ORB La arquitectura a tres niveles se ha hecho muy popular (vea la discusi\u00f3n en [THUR971). En esta arquitectura, el c liente es un cliente ligero y realiza un procesamiento m\u00ednimo, el servidor hac e las funciones de adm inistraci\u00f3n de la base de datos, y el nivel intermedio lleva a cabo varias funciones de proceso de negocio. En el caso de data mining, se podr\u00eda utilizar tambi\u00e9n una arquitectura de tres niveles donde la herramienta de data mining se pone en el nivel intermedio. La herramienta de data mi ning podr\u00eda desarrollarse como una colecci\u00f3n de componentes. Estos component es podr\u00edan estar basados en la tecnolog\u00eda orientada al objeto. Desarro llando los m\u00f3dulos de data mining como una colecci\u00f3n de componentes, se podr\u00edan desarrollar herramientas gen\u00e9ricas y entonces se podr\u00eda personalizarlas para las aplicaciones especializadas. Otra ventaja de desarrollar un sistema de data mining como una colecci\u00f3n de componentes es que se podr\u00edan co mprar los componentes a vendedores diferentes y despu\u00e9s ensamblarlos par a formar un sistema. Adem\u00e1s, los componentes podr\u00edan ser reutilizados. Por ahora asumiremos que los m\u00f3dulos son el integrador de los dat os fuente, la herramienta de data mining, el podador (discriminador) de los resultados, y el generador de informes. Entonces cada uno de estos m\u00f3dulos puede encapsularse como un objeto y se podr\u00eda usar ORB's para integrar estos objetos dife rentes. Como resultado, se puede usar un enfoque plug-and-play en el desarrollo de herramientas de data mining. Tambi\u00e9n se podr\u00eda descomponer la he rramienta de data mining en m\u00faltiples m\u00f3dulos y encapsular estos m\u00f3dulos como objetos. Por ejemplo, considere los m\u00f3dulos de la herramienta de data mining ilustrados en la Figura 5. Estos Cap\u00edtulo 1 Introducci\u00f3n T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 40 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda m\u00f3dulos son parte del m\u00f3dulo de la herramienta de data mining y pueden ser encapsulados como objetos e integrados a trav\u00e9s de un ORB. Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 41 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Cap\u00edtulo 2. An\u00e1lisis Estad\u00edstico mediante Excel Introducci\u00f3n. M\u00e9todos cl\u00e1sicos de an\u00e1lisis de datos Descripci\u00f3n de datos. Estad\u00edsticos de una variable Generalizaci\u00f3n. Distribuciones de probabili dad e intervalos de confianza Contrastes de hip\u00f3tesis. Tipos Relaciones entre atributos Nominales- Num\u00e9ricos: Tests de comparaci\u00f3n de medias (muestras dependientes e independientes) y an\u00e1lisis de varianza. Nominales-Nominales: Tablas de Contingencia. Tests de independencia y comparaci\u00f3n de proporciones. Num\u00e9ricos - Num\u00e9ricos: An\u00e1lisis de Regresi\u00f3n Aplicaci\u00f3n de t\u00e9cnicas estad\u00edsticas a la clasificaci\u00f3n. T\u00e9cnicas cl\u00e1sicas de clasificaci\u00f3n y predicci\u00f3n Clasificaci\u00f3n mediante regresi\u00f3n num\u00e9rica Clasificador bayesiano Evaluaci\u00f3n de Hip\u00f3tesis Objetivo: se pretende validar o rechazar ideas preconcebidas a partir del an\u00e1lisis de los datos disponible s, generalizando la s conclusiones Pasos: 1. Generaci\u00f3n de hip\u00f3tesis 2. Determinar qu\u00e9 datos son neces arios. Recolectar y preparar 3. Evaluaci\u00f3n de hip\u00f3tesis para aceptar o rechazar Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 42 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Variables (Atributos) Unidades (Ejemplos) Tiempo Matriz de datos v1v2 vM 1 2 n t1 Tipos de variables nominales o categ\u00f3ricas (incluyendo ordinales) num\u00e9ricas Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 43 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 2.1. An\u00e1lisis de una variable. Estad\u00edstica Descriptiva e Inferencia Estad\u00edsticos: resumen (describen) toda la informaci\u00f3n contenida en una muestra de datos : Variables continuas medidas centrales (me dia, moda, mediana) medidas de dispersi\u00f3n (rango, varianza, desviaci\u00f3n est\u00e1ndar, percentiles) medidas de forma (histograma) Variables nominales frecuencias relativas (probabilidades), moda media y varianza de probabilidad estimada Muestra : yi; i =1...n; toma valo res en un rango continuo/discreto Estad\u00edsticos de variable continua Media (esperanza) muestral: promedio de todos los valores ===n iiyny y media 11)( Moda : valor que aparece m\u00e1s veces Mediana : valor que deja el mismo n\u00famero de casos a ambos lados ( ) ( )i i i y N y Ny y mediana = =k j y casos \u00ba y casos \u00ba| )( equivale a ordenar el vector de datos y tomar el valor central menos sensible frente a valores extremos poco probables Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 44 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Recorrido (rango): max(yi)-min(yi) Varianza: promedio de desviaciones con respecto a valor medio = = = =n iin ii yn ynyyny Var 12 2 12 11) (11)( Desviaci\u00f3n est\u00e1ndar (t\u00edpica): ra\u00edz cuadrada de la varianza )( )( y Var y desvy= = media, sigma -4-202468101214 0 1 02 03 04 0 muestravalorDatos valor medio valor medio+sigma valor medio - sigma Histograma Estimaci\u00f3n de la distribuci\u00f3n de densidad de probabilidad: frecuencia relativa de valores de yi por unidad de intervalo la suma total de frecuencias absolutas es el n\u00famero de datos la suma de frecuencias relativas es 1 Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 45 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Histograma acumulado Suma de frecuencias relativas de casos inferiores al valor en abscisas (acumulaci\u00f3n de histograma normalizado): Estimaci\u00f3n de Prob(Y<=yi) en el extremo superior debe ser 1 Ejemplo: histograma de variable uniforme intervalos N\u00ba de casos en intervalo histograma normal 020406080100120140 -3 -2,4 -1,8 -1,2 -0,6 0 0,6 1,2 1,8 3 yfrecuencia absoluta acumulado 00,10,20,30,40,50,60,70,80,91 -3 -2,4 -1,8 -1,2 -0,6 0 0,6 1,2 1,8 2,4 3 intervalos Valores acumulados Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 46 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Cuantiles del histograma Cuantil: valores que dividen el reco rrido de datos en k partes de la misma frecuencia (percentiles: 100 par tes, cuartiles: 4 partes, etc.) Ejemplo: cuartiles histograma 020406080100120140 0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1histograma acumulado 00,20,40,60,811,2 00 , 5 1acumuladoCap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 47 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Percentiles e histograma acumulado Percentil p: valor que deja debajo al p% de los individuos, y al (100-p)% por encima: se entra en eje vertical del histograma acumulado - percentil 50: median a (por definici\u00f3n) - percentiles 25, 75: cuartiles. Abarcan al 50% de los individuos (recorrido inter-cuart\u00edlico) - con distribuci\u00f3n normal tipificada - -0,6 1,2 1,8 2,4 3 Cuartil 1 frecuencia 020406080 0123456789 1 0 calificaci\u00f3nalumnosCalificaci\u00f3n 2,8 0,6 5 3,1 3,94,9 10 6,55 ...porcentaje cuartiles 0,25 1,4 0,5 2,725 0,75 4 17 7 Cuartil 2 Cuartil 3 Recorrido inter-cuart\u00edlico: [1.4, 4]: contiene 50% datos Cuartil 4 Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 48 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Estad\u00edsticos de variable nominal yi nominal: toma valores de un conj unto discreto (categor\u00edas): {vi1, ..., viki} Distribuci\u00f3n de frecuencias ik jjki ki n nnn pnn pnn p 11 21 1 )%/(100)%/(100)%/(100 # Moda : valor que aparece m\u00e1s veces )(maxjn j Ejemplo variable nominal y num\u00e9rica Edad Sexo 23 M 25 M 18 H 37 M45 H 62 H 43 M 40 H 60 M54 H 28 H 18 H 54 M 29 H42 M 26 M 32 M 41 M 37 M36 H 53 H 21 M 24 H 21 H 45 M 64 H22 M 61 M 37 M 66 M0102030405060 HM sexoporcentaje 020406080100120 18 25 35 45 55 65 edadporcentajefrecuencia acumuladaCap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 49 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Media y varianza de frecuencias estimadas C\u00e1lculo de cada frecuencia para una categor\u00eda dada: m casos de n p=m/n puede verse como asignar: vi=1 c ada ejemplo en la categor\u00eda vi=0 en el resto ==n iivnp 11 Varianza de p: 1() 1( ) (1)(2 1 p pp p pvnp Var pn ii = = = = caso m\u00e1xima varianza: p=0.5 Generalizaci\u00f3n de la muestra a la poblaci\u00f3n Los estad\u00edsticos resumen (describen) toda la informaci\u00f3n contenida en una muestra (estad\u00edstica descriptiva) Para generalizar las conclusiones, es deseable formular razonamientos sobre la poblaci\u00f3n que genera la muestra Paso de los estad\u00edsticos (y i) a los estimadores (Yi) Uso de distribuciones te\u00f3ricas de probabilidad para caracterizar los estimadores Cuantificaci\u00f3n de la probabilid ad de los resultados (nunca se garantiza con certeza absoluta) Puede hacerse an\u00e1lisis contrari o: deducci\u00f3n de propiedades de la muestra a partir de la poblac i\u00f3n (inter\u00e9s te\u00f3rico) Distribuciones de probabilidad Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 50 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Modelo que representa la tendenci a de un histograma con muchos datos y cajas peque\u00f1as Funci\u00f3n distribuci\u00f3n de probabilidad de X: FX(x) << = x xXPxFX ); ( )( Funci\u00f3n densidad de probabilidad de X: fX(x) = =<< = b aXx dxxf xFxdxx dFxf )( ) (;)( )(;)()( Distribuci\u00f3n Normal Curva de gran inter\u00e9s por explic ar datos en muchas situaciones Aplicada por primera vez como distribuc i\u00f3n por A. Quetelet (1830) =2 21exp 21)( z zf distribuci\u00f3n sim\u00e9trica: co incide media y mediana en 0 se dispone del valor de la distribuci\u00f3n de probabilidad: \u00e1rea bajo la curva de fZ(z) para cualquier valor: Tipificar o estandarizar variables: Se mide el des plazamiento respecto a la media en unidades de desviaci\u00f3n t\u00edpica: ii iyyz= Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 51 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Distribuci\u00f3n Normal e Intervalos de Confianza Ej.: se sabe conocen par\u00e1metros de (intervalos z0.025*20=[75.8, 154.2] Inferencia Objetivo : dado un estad\u00edsticos de una muestr a sacada al azar, razonar acerca del verdadero par \u00e1metro de la poblaci\u00f3n Se basa en la estimaci\u00f3n de par\u00e1me tros y contraste de hip\u00f3tesis con c\u00e1lculo de probabilidades muestra aleatoria y repr esentativa (estratificaci\u00f3n) 2 0,977249938 2,5 0,99379032 3 0,998650033 - - - - - 0 1 3 0 zf(z) z0F(z 0) Una cola (unilateral) -3 -1f(z) F(z 0) -2 0 zf(z) F(z 0) Sim\u00e9trico dos colas (bilateral) Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 52 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Paso de la poblaci\u00f3n a una muestra aleatoria Dada una poblaci\u00f3n con media y varianza: Se toma una muestra aleatoria (n casos) de la poblaci\u00f3n: yi, i=1,...,n C\u00f3mo se distribuyen los estad\u00edsticos de la muestra? A su vez son VAs Distribuci\u00f3n de la media distribuci\u00f3n sigue? Teorema del L\u00edmite Central: \"Una muestra suficientemente grande de una poblaci\u00f3n con distribuci\u00f3n arbitraria tendr\u00e1 estad\u00edstico media con distribuci\u00f3n normal\" Consecuencia: intervalo de confianza de la media a partir de dist. Normal Ynz Yy 1\u00b1 = Mayor \"Normalidad\": tama\u00f1o de las m uestras, distribuci\u00f3n pob. parecida a normal Ejemplo l\u00edmite central Poblaci\u00f3n: 1000 individuos, 400 mujeres, 600 hombres 49.0) 1( ;4.0 = = = P P P Muestras de 10 individuos Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 53 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 155.0) 1( 101;4.0 )(;10 1 = ==== = P PPpEy p pii Intervalo de confianza al 95% (con distribuci\u00f3n normal): Influye: intervalo de confianza (z): \"garant\u00eda\" de no equivocarnos tama\u00f1o de muestra (n) variabilidad de poblaci\u00f3n (p) 155.0)P1(P 101;4.0P)p(E;y101p p10 1ii 60 , 70 , 80 , 9 1 Si las muestras fueran de 50 individuos: 069.0) 1( 501;50150 1 = == = P Py p pii ]54.0,26.0[ 96.1 = \u00b1p P Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 54 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Ejemplo de aplicac i\u00f3n para decisi\u00f3n Para determinar el intervalo de conf ianza del estimador al 95% se aplica el argumento del muestreo \"dado la vuelta\": py pP EjzyY 96.1 :2/ \u00b1=\u00b1 Ejemplo: Un supermercado se plant ea extender su horario a s\u00e1bado por la tarde. Necesita un m\u00ednimo del 10% de sus clientes para cubrir costes. Con una muestra de 1500 personas se obtiene que hay un 8% de clientes interesados \u00bfQu\u00e9 hacer? %]37.9 %,63.6[1500/)08.01(*08.0 96.1 08.0 96.1p Pp = \u00b1 = \u00b1= Con una confianza del 95% podemos decir que los clientes dispuestos a comprar el s\u00e1bado por la tarde no contiene al deseado 10%. Contrastes de hip\u00f3tesis Contrastar es medir la probabili dad de que el estad\u00ed stico obtenido en una muestra sea fruto del azar Formulaci\u00f3n del modelo e hip\u00f3tesis : se conoce la distribuci\u00f3n del estad\u00edstico bajo condiciones hip\u00f3tesis Hip\u00f3tesis nula ( H0): es lo que dudamos y quer emos contrastar: Ej: \u00bfEl porcentaje total es 10%?, la medi a de los ingresos es superior a 5? Bajo H0, el estad\u00edstico sigue el modelo, y la diferencia observada es \u00fanicamente fruto del azar Hip\u00f3tesis alternativas: alternativas que permiten rechazar la hip\u00f3tesis nula: prob. distinta de 10% , media menor a 5, etc. Rechazar hip\u00f3tesis H0: hay evidencia para negar H0 No rechazable: no hay evidencia estad\u00edstica para hacerlo (no implica demostrar su veracidad) Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 55 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Contrastes con normal y varianza conocida Contraste de dos colas (bilateral ): deja la mitad a cada lado, a/2 Ej: Hip\u00f3tesis Contraste de una cola (unilateral): deja a un solo lado a Ej: Hip\u00f3tesis nula 0.1 0.087 Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 56 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Regi\u00f3n cr\u00edtica: z>1.65 Contraste con varianza estimada La variable (yi-y)/s no es exacta mente la normal tipificada (s es estimada): Distribuci\u00f3n t-Student : par\u00e1metro grados de libertad:n-1 se ensanchan los intervalos de confianza (s\u00f3lo si pocos datos) ,\u00b5 conocida estad\u00edstico )1,0( /N ny \u00b5 Int. confianza n zy /2/\u00b1 , \u00b5 conocida estad\u00edstico )1,0( /1 nt ny \u00b5 Int. confianza n tyn/1,2/ \u00b1 Ejemplo de Intervalos con t-Student Los valores del pH de una piscina Utilizando normal y t-Student, hallar: Intervalo de confianza 95% para media poblacional Intervalo de confianza 65% para media poblacional -5 -4 -3 -2 -1 0 1 2 3 4 500.050.10.150.20.250.30.350.4 Student (N=9)Student (N=50)Student (N=100) 10% 1,38 1,30 1,29 1,2820% 0,88 0,85 0,85 0,84Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 57 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Contrastar hip\u00f3tesis nula de que la media poblacional es 6,8 con niveles de significaci\u00f3n a =0,05 y T\u00e9cnicas de Evaluaci\u00f3n de hip\u00f3tesis 2.2.1. An\u00e1lisis de relaciones entre atributos El objetivo del an\u00e1lisis entre los atributos que definen los datos es ver el tipo de interrelaci\u00f3n o dependencia que existe entre los valores de dichos atributos. Este an\u00e1lisis se lleva a cabo haciendo uso de los datos disponibles para tener \"evidencia estad\u00edstica\" que permita valid ar o refutar hip\u00f3t esis que pretendan explicar las relaciones. La herramienta o t\u00e9cnica que permite lleva r a cabo este tipo de an\u00e1lisis es el denominado tests de hip\u00f3tesis, que se define de manera distinta en funci\u00f3n del tipo de atributos con los que este mos trabajando. De esta manera en funci\u00f3n del tipo de atributo tenemos: Nominales-nominales : En este caso los dos atributos toman valores de un conjunto de posibles valores (por ejempl o: Norte, Sur, Este y Oeste). La relaci\u00f3n entre las variables se obti ene mediante las tablas de contingencia. Nominales-num\u00e9ricos : En este caso uno de los atributos toma valores de un conjunto de posibles valores y otro to ma valores num\u00e9ricos. La relaci\u00f3n entre los atributos se obtiene mediant e la comparaci\u00f3n de medias y el an\u00e1lisis de varianza. Num\u00e9ricos-num\u00e9ricos : En caso los dos atributos toman valores num\u00e9ricos. La relaci\u00f3n entre los dos atributos se obtiene mediante el an\u00e1lisis de regresi\u00f3n y covarianza. En la secci\u00f3n \u00a1Error! No se encuentra el origen de la referencia. se contemplan m\u00e1s casos de contrastes de hip\u00f3tesis. 2.2.2. Relaci\u00f3n entre variables nominales-nominales El objetivo es analizar la interrelaci\u00f3n (dependencia) entre los valores de variables nominales. En este caso la herramienta de an\u00e1lisis para dos variables es la denominada tabla de contingencia . En esta tabla se calcula la Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 58 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda distribuci\u00f3n de los casos (las frecuenc ias de aparici\u00f3n) par a las distintas combinaciones de valores de las dos variables, como se oberva en la figura siguiente. Variable 2 totales 1 valor 1 valor 2 ... valor p2 valor 1 n 11 n 12 ... n 1p2 t1 valor 2 n 21 n ... t'p2 t Figura 1: Tabla de contingencia. A partir de la tabla de contingencia podemos calcular las probabilidades marginales de los valores de la vari able 1 como Pi=ti /t, que representa la probabilidad de que la variable 1 tome el valor i. Del mismo modo podemos calcular las probabilidades para la variable 2 como Pj=t'j/t. A partir de las probabilidades margi nales podemos calcular los casos \"esperados\", bajo la hip\u00f3tesis a cues tionar de independencia entre variables. Para calcular el valor esperado se mult iplica el n\u00famero total de casos por la probabilidad de que la variable 1 tome el valor i y la variable 2 tome el valor j, es decir Eij=t(ti/t)(t'j/t)= tit'j/t. Obs\u00e9rvese que \u00fanicame nte bajo la hip\u00f3tesis de independencia podemos calc ular la probabilidad conj unta como un producto de probabilidades. La t\u00e9cnica de an\u00e1lisis estad\u00edstico que se aplica para la relaci\u00f3n entre dos variables nominales es el contraste Chi-2. Las caracter\u00edsticas de este test son: Es aplicable en an\u00e1lisis bi-variable (normalmente clase vs atributo) Determina si es rechazable la hip\u00f3tesis de que dos variables son independientes Bajo hip\u00f3tesis H0 se determinan los casos en el supuesto de variables independientes. Los va lores esperados se determinan con probabilidades marginales de las categor\u00edas: Eij=tPi Pj (valores esperados) El estad\u00edstico Chi-cuadrado mide la diferencia entre los valores esperados y los valores observados, por lo que su expresi\u00f3n es: == =1 12 12 2/) (p ip jij ij ij E OE (1) La expresi\u00f3n anterior, 2, bajo las condiciones de H0 sigue una distribuci\u00f3n conocida denominada distribuc i\u00f3n Chi-cuadrado, caracterizada por Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 59 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda el par\u00e1metro grados de libertad que es el (n\u00ba de filas-1)(n\u00ba de columnas -1) . Cuando no se cumple la hip\u00f3tesis H0 las variables son dependientes. Por lo tanto se formula un test de hip\u00f3tesis para determinar el valor de Chi-cuadrado para esa hip\u00f3tesis. La distribuci\u00f3n Chi-Cuadradado est\u00e1 tabulada: probabilidad chi2 supera estad\u00edsticovalor estad grados de libertad 56789 el test lo que calcula es la probabilidad de que la diferencia entre el valor observado y el valor esperado supere un cierto umbral. Figura 2: Representaci\u00f3n Gr\u00e1fica del test Chi-Cuadrado. 2.2.3. Relaciones num\u00e9ricas-nominales Las t\u00e9cnicas para establecer posibles relaciones entre dos variables una de ellas num\u00e9rica y la otra nominal (o ent re dos nominales si trabajamos con proporciones) se utiliza la t\u00e9cnica de la comparaci\u00f3n de medias y proporciones. Esta t\u00e9cnica mide la relaci\u00f3n entre variables num\u00e9ricas y nominales, o nominales y nominales (proporciones), determinando si es rechazable la hip\u00f3tesis de que las diferencias de medi as o proporciones condicionadas a las etiquetas de la variable nominal son debid as al azar. Es decir que se calcula el impacto de la variable nomi nal sobre la continua. Existen dos tipos de an\u00e1lisis s eg\u00fan si tenemos dos medias o proporciones o un n\u00famero mayor de dos. Si tenemos dos medias o proporciones se calcula la significatividad de la difere ncia. Si tenemos m\u00e1s de dos valores distintos se realiza un an\u00e1lisis de varianza. 2Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 60 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 2.2.3.1. Comparaci\u00f3n de dos medias En este caso tenemos dos subpobl aciones, una para cada grupo, cada una con su media y varianza. Las hip\u00f3tesis que podemos establecer son: H0: la diferencia de medias en la poblaci\u00f3n es nula D=0 Hip\u00f3tesis alternativa A: las medias son distintas: D!=0 Hip\u00f3tesis alternativa B: la media de 1 es mayor que 2: D>0 Hip\u00f3tesis alternativa C: la media de 1 es mayor que 2: D<0 Como vemos, no hay una \u00fanica posibil idad de hip\u00f3tesis alternativa sino varias, con diferentes intervalos de re chazo en funci\u00f3n de la informaci\u00f3n que tengamos a priori. Adem\u00e1s, para la com paraci\u00f3n de las variables num\u00e9ricas de dos clases, las situaciones posibles q ue podemos encontrar nos dentro de la muestra total son: Muestras independientes: conjuntos distintos Muestras dependientes: es decir las muestras pertenecen al mismo conjunto, con dos variables a comparar en cada ejemplo Cuando el n\u00famero de muestras es muy elevado para cada grupo, las muestras siguen una distribuci\u00f3n normal po r lo que las hip\u00f3tes is anteriormente expuestas se eval\u00faan m ediante los valores de una gaussiana est\u00e1ndar. De esta manera se calcular\u00eda la media de la diferencia y su varianza y se aplicar\u00eda al c\u00e1lculo de probabilidades de una gaussiana est\u00e1ndar. En el caso de la hip\u00f3tesis A se utilizar\u00edan las dos cola s de la gaussiana y en el caso de la hip\u00f3tesis B utilizar\u00edamos una \u00fanica cola , como se observa en la siguiente figura. Figura 3: Representaci\u00f3n Gr\u00e1fica de compraci\u00f3n de dos medias /2=0.025 /2=0.025 z=1.96 z=+1.96 - 3=0.05 z=1.65 Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 61 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Cuando las muestras s on peque\u00f1as no es v\u00e1lida la hip\u00f3tesis de normalidad de los estad\u00edsticos de medias y el test se realiza considerando una distribuci\u00f3n t-Student: /2,GL yt \u00b1 ( 2 ) El proceso para el c\u00e1lculo cuando la s muestras son independientes (test no pareado) es: En cada muestra (tama\u00f1os n1, n2) obtenemos las medias y varianzas: 2 1 2 1 , ,,y y yy (3) Se calcula la diferencia: 2 1y y d = ( 4 ) Varianza de la diferencia: 22 2 12 1 2 n ny y d + = ( 5 ) Los grados de libertad de la t-Student se eval\u00faan la El proceso de c\u00e1lculo cuando las m uestras dependientes (test pareado), se fundamenta en que se dispone de la di ferencia en cada uno de los ejemplos y no en que tenemos dos variables (eje mplo: cambio en el tiempo de una variable para ejem plos d1, d2, ..., dn): di=d1i- todo es equivalente al caso anter ior pero lo c\u00e1lculos son: nddndnddn iin ii1;) (11;1 12 2 1= = = = = (6) 2.2.3.2. An\u00e1lisis de la varianza Esta t\u00e9cnica tambi\u00e9n mide la relaci\u00f3n entre variables num\u00e9ricas y nominales, pero en este caso se descompone la variabilidad del resultado en varios componentes: Efectos de factores repres entados por otras variables Efectos de error experimental La t\u00e9cnica del an\u00e1lisis de la varianza simple (ANOVA) considera un solo factor con varios niveles nominales. Para cada nivel se tiene una serie de Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 62 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda observaciones y el modelo: Yij=ui+uij, representa ruido con la misma varianza por nivel, donde i var\u00eda entre 1 y el n\u00famero de niveles (variable nominal) y j var\u00eda entre 1 y el n\u00famero de datos por ni vel. Adem\u00e1s de esta t\u00e9cnica existe la t\u00e9cnica MANOVA que es un modelo multif actorial de la varianza. En este modelo se definen I niveles, cada uno de ellos representado por un conjunto de muestras, como se puede observar en la siguiente figura, y donde cada nivel est\u00e1 represntado por una media y una varianza. Figura 4: Niveles de la t\u00e9cnica MANOVA. Figura 5: Represntaci\u00f3n Gr\u00e1fica de los Niveles de la t\u00e9cnica MANOVA. El an\u00e1lisis MANOVA eval\u00faa las siguientes variables: N\u00famero total de elementos: Factor B 1 2 ... ECap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 63 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ==I iin n 1 (7) Media por nivel: ==I iij iin jijI iin jiji i + = = == == (10) Y realiza una estimaci\u00f3n de varianzas de la siguiente manera Varianza inter-grupo ( between ) (I-1 grados de libertad): 2 1) (11YYnISI ii i = = ( 1 1 ) Varianza intra-grupo ( within ) w YYInSi === 1 2 jiji === ( 1 3 ) La hip\u00f3tesis que planteamos o la pregunta que queremos responder es: \u00bfEs significativamente mayor que la un idad la relaci\u00f3n entre la varianza intergrupo e intragrupo, f=Sb/Sw?. Por lo tanto debemos realizar un contraste de hip\u00f3tesis de cociente de varianzas maestrales, que sigue una distribuci\u00f3n F de Fisher-Snedecor : F(x, I-1,n-I), como se ve en la figura siguiente. Figura 6: Representaci\u00f3n de la F-Fisher-Snedecor. F Rango: [0,20] Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 64 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Este test permite rechazar o no la hip\u00f3tesis de que el cociente entre varianzas estimadas se deba al azar. Por lo tanto 2.2.4. Relaciones num\u00e9ricas-num\u00e9ricas: 2.2.4.1. Regresi\u00f3n lineal La regresi\u00f3n lineal permite identificar relaciones entre variables num\u00e9ricas y construir modelos de regresi\u00f3n: 1 va riable salida y m\u00faltiples entradas num\u00e9ricas. Se consideran relaciones de una variable de sa lida (dependiente) con m\u00faltiples variables de entrada (indepe ndientes). Este problema se puede representar de la siguiente manera: Dada la muestra de datos: )} , ( ),...,, (),, {(2 2 1 1 n ny X y X y XG G G donde s dimensione I con vectores:XG , se busca estimar una funci\u00f3n que mejor \"explique\" los datos: )g(y :(.) X XR R gI G G = ( 1 4 ) El procedimiento de resoluci\u00f3n para estimar dicha funci\u00f3n es el procedimiento de m\u00ednimos cuadrados que es tima el vector de coeficientes que minimiza error: t It ItI ppp i i x x X a aa AX A xa a Xgy 1 010 \"G \"GGG G = == + = = = (15) El objetivo es que dadas N muestr as, el procedimiento debe determinar coeficientes que minimicen el error de predicci\u00f3n global 2 1] )([ = =n jj jy XgG ( 1 6 ) Este es un problema cl\u00e1sico de mini mizaci\u00f3n de funci\u00f3n cuadr\u00e1tica cuya soluci\u00f3n es \u00fanica. La formulaci\u00f3n gen\u00e9ric a matricial del problema se puede expresar como: Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 65 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda AHA x xx xx x XgXg yy g yy y N # #G* 111 ) ()( ; 12 2 11 1 1 1 1 1 = = = = = (17) Por lo que la soluci\u00f3n de m\u00ednimos cuadrados es: 1ttAH HH y=G G 2.2.5. Evaluaci\u00f3n del modelo de regresi\u00f3n La evaluaci\u00f3n del modelo rea liza el an\u00e1lisis de validez del modelo asumido, es decir se van a calcular una serie de medida s de \"parecido\" entre la variable de salida estimada mediant e la funci\u00f3n y los valores de la variable de salida real, ide esta manera analizarem os la nfluencia de las va riables de entrada en el c\u00e1lculo de la variable de salida (si ex iste o no una relaci \u00f3n lineal entre las variables de entrada que permita determi nar la variable de salida). Estas medidas son: el Factor de Correlaci\u00f3n (que muestra si existe la relaci\u00f3n lineal), el error de predicci\u00f3n (diferencia entre la predicha y la real) y el error en coeficientes. 2.2.5.1. Medidas de Calidad El factor de correlaci\u00f3n se eval\u00faa como: () () = == == jjn jj Varyy 1,1; ; )()(),() )( (1),( (18) En general, se puede hacer factores de correlaci\u00f3n entre cualquier par de variables num\u00e9ricas: indica el grado de re laci\u00f3n lineal existent e. Para ello se calcula la matriz de covarianzas (o la de correlaciones que es la misma pero normalizada) de 1 1var cov , cov , cov , var 1 cov , varnt i X i IIx xx xx xx xCXn xx x\u00b5 = = = \" G # #% \" (19) donde ==n iiX n 11GG\u00b5 Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 66 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda El error de predicci\u00f3n se eval\u00faa como: () ( )2 12 1 = == =n jjn jj jyy Error bajo la hip\u00f3tesis de que los datos yi tengan la misma varianza sy, sean independientes, y que el modelo lineal sea adecuado el error puede calcularse como: 2)1(y n Error . El error en coeficientes se eval\u00faa a partir de la expresi\u00f3n que permite encontrar los coeficientes yt t At tH HH yH HH A G GG G1 1] [ ; ] [ = = . La relaci\u00f3n entre los errores en predicci\u00f3n y en coeficientes estimados se eval\u00faa: 1 2 222 ] [10 = = HH Ct y AAA A I \"\"#% ## #\"\" (20) Por lo que el error en los coeficient es depende de el error en y, sy2 y el recorrido de datos X, es decir la matriz H. 2.2.5.2. Test de Hip\u00f3tesis sobre modelo de regresi\u00f3n Estos valores permiten analizar la \"cali dad\" del modelo mediante los test de hip\u00f3tesis: hip\u00f3tesis de significatividad de par\u00e1metros (gaussiana o t-Student) y la hip\u00f3tesis de ausencia de relaci\u00f3n (F de Fisher-Snedecor). Para evaluar la significatividad de par\u00e1metros, partimos de varianzas de par\u00e1metros {s2A1,...s2AF} y los propios valores estimados, y nos preguntamos si son significativ os los par\u00e1metros: ? ,..., 11 FAF AA A . Este test puede resolverse mediante una gaussiana est\u00e1nda r si tenemos gran cantidad de datos, o bien, si hay pocos datos: en vez de estad\u00edstica normal, una t-Student con n-F-1 grados de libertad. Tambi\u00e9n podemos extender el modelo y analizarlo: ej: dependencia cuadr\u00e1tica, ver si son significativos nuevos t\u00e9rminos Para analizar la validez del modelo debemos realizar un an\u00e1lisis de la varianza que permite rechazar o no la hi p\u00f3tesis de que no existe relaci\u00f3n entre variables (relaci\u00f3n debida al azar, correlaci\u00f3n nula). Para ello a partir del valor: () ()()2 12 12 1 = = = + N jjN jjN jj yy sigue una distribuci\u00f3n: F de Snedecor: F(n1, n2), donde los grados de libertad son: I, n-I-1 Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 67 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 2.3. Ejemplos de aplicaci\u00f3n de t\u00e9cnicas de evaluaci\u00f3n de hip\u00f3tesis Con un objetivo meramente ilustra tivo, en esta secci\u00f3n se sugieren algunas aplicaciones de las t\u00e9cnicas de contraste de hip\u00f3tesis y miner\u00eda de datos presentadas en otras secciones. So n ejemplos que se relacionar\u00edan con el objetivo final de este proyecto de anal izar y describir relaciones de inter\u00e9s y modelos subyacentes en datos del dominio del tr\u00e1fico a\u00e9reo. Hay que tener en cuenta, que son ejemplos sugeridos que quedar\u00edan sujetos a su validaci\u00f3n mediante la generaci\u00f3n de los datos apropiados, sujeto a una metodolog\u00eda apropiada de preparaci\u00f3n, inte rpretaci\u00f3n y validaci\u00f3n. 2.3.1. Ejemplos de Validaci\u00f3n de Hip\u00f3tesis Para ilustrar la t\u00e9cnica de contrast e de hip\u00f3tesis para independencia entre variables de tipo nominal, supongamos que partimos de los datos de la tabla siguiente: En esta tabla se representan dos variabl es nominales: retardo y tipo de avi\u00f3n. La variable retardo puede tomar 4 valores: nulo, medio, alto y muy alto. La variable tipo de avi\u00f3n puede tomar 3 valore s: Ligero, Mediano y Pesado. En la tabla aparecen el n\u00famero de aviones de cada tipo en funci\u00f3n del retardo que sufren. Es decir, aparece la distribuci \u00f3n observada para el n\u00famero de aviones de cada tipo que sufre una determinada categor\u00eda de retardo. Si en la tabla anterior consideramos \u00fanicamente los valores totales de las variables tipo de avi\u00f3n y retardo, podem os calcular la pr obabilidad de cada categor\u00eda dividiendo del total marginal por el n\u00famero total de casos. Adem\u00e1s, en el caso hipot\u00e9tico de que fueran la s dos variables independientes, la probabilidad conjunta de cada casilla ser\u00eda el producto de estas probabilidades, y multiplicada por el n\u00famero total de ca sos tendr\u00edamos el valor esperado en cada casilla. Eij= t(ti/t)(t'j/t) As\u00ed, por ejemplo, para la combinaci\u00f3n av i\u00f3n ligero y retardo nulo, tendr\u00edamos: 74.51 934934117 934413 , = == = nulo retardo ligero tipoN Repitiendo el mismo proceso para el resto de casillas, tenemos: Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 68 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Por lo tanto a partir de dichos datos pode mos plantearnos la hip\u00f3tesis nula H0: las variables retardo y categor\u00eda son i ndependientes. Calculando el estad\u00edstico que acumula las desviaciones cuadr\u00e1ticas divididas por los valores esperados tenemos: == =1 12 12 2/) (p ip jij ij ij E O E (22) y evaluamos la probabilidad del estad\u00edstic o mediante la funci\u00f3n Chi-cuadrado. Tomando 3x2 grados de liber tad, tenemos que el valor de corte al 95% para rechazar ser\u00eda de 12.59 (ver siguiente Figura). Figura 7: Test Chi-Cuadrado. Sin embargo, con los valores observados, tenemos que la desviaci\u00f3n es 44,91, que para una distribuci\u00f3n Chi-cuadrado de 6 grados de libertad tiene una probabilidad de aparecer de 4, 87e-8, lo que nos permite rechazar con mucha evidencia la hip\u00f3tesis de independencia y concluir una clara dependencia entre las variables. El ejemplo siguiente aplica la misma t\u00e9cnica para determinar la interdependencia entre la intenci\u00f3n de voto y el sexo en una poblaci\u00f3n dada: 2Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 69 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Relaciones num\u00e9ricas-num\u00e9r icas: regresi\u00f3n lineal Permite identificar relaciones en tre variables num\u00e9ricas y construir modelos de regresi\u00f3n: 1 variable sa lida y m\u00faltiples entradas num\u00e9ricas Se consideran relaciones de una variable de salida (dependiente) con m\u00faltiples variables de entrada (independientes) Ejemplo: regresi\u00f3n lineal de 1 variable A\u00f1o Renta Consumo consumo Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 70 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda a1 a0 0,927041871 -133,296932Estimaci\u00f3n Lineal nta a a ConsumoE Re*1 0+ = dependencia consumo 02000400060008000100001200014000160001800020000 0 5000 10000 15000 20000 rentaconsumosConsumo consumo E Ejemplo: regresi\u00f3n lineal de 2 variables x1 x2 y Valor Superficie Antig \u00fcedad Valor predicho 310 20 106.287 Euro s 109.180 Euros 333 12 107.784 Euro s 112.283 Euros 356 33 113.024 Euro s 108.993 Euros 379 43 112.275 Euro s 108.128 Euros 402 53 104.042 Euro s 107.262 Euros 425 23 126.497 Euro s 115.215 Euros 448 99 94.311 Euro s 99.800 Euros 471 34 106.961 Euro s 115.469 Euros 494 23 122.006 Euro s 119.233 Euros 517 55 126.497 Euro s 113.518 Euros 540 22 111.527 Euro s 122.132 Euros Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 71 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda a2 a1 a0 -220,444829 58,2271936 95538,7217Estimaci\u00f3n Lineal Antig\u00fcedad*2a Superficie*1a0a Valor + + = 020000400006000080000100000120000140000valor (euros) 10 20 30 40 50 60 70 80 90 100 110 310 333 356 379 402 425 448 471 494 517 540 antig\u00fcedad (a)superficie (m2)valores predichos 020000400006000080000100000120000140000 10 30 50 70 90 310 356 402 448 494 540 Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 72 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Evaluaci\u00f3n del modelo de regresi\u00f3n An\u00e1lisis de validez del modelo asumido: Medidas de \"parecido\" entre variable de salida estimada y real, influencia de variables de entrada - Factor de Correlaci\u00f3n - Error de predicci\u00f3n - Error en coeficientes An\u00e1lisis de \"calidad\" del modelo - Hip\u00f3tesis de significativi dad de par\u00e1metros: t-Student - Hip\u00f3tesis de ausencia de relaci\u00f3n: F de Fisher-Snedecor Factor de correlaci\u00f3n Factor de correlaci\u00f3n entre datos y predicciones: () () = == == = = jjn jj Varyy 1,1; ; )()(),() )( (1),( En general, se puede hacer factores de correlaci\u00f3n entre cualquier par de variables num\u00e9ricas: indica el grado de relaci\u00f3n lineal existente Matriz de Covarianza Muestra de vectores aleatorios: Matriz de covarianzas: Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 73 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ==n iiX n 11GG\u00b5 = = = ) var( ), cov() var( ), cov(), cov( ), cov( ) var( ))((1 11 2 12 1 2 1 1 1 I In it i i X x xxx xxxx xx x X XnC \"# % #\" GGGG G \u00b5 \u00b5 La matriz de correlaciones es similar, normalizada Error de Predicci\u00f3n () ( )2 12 1 = == =n jjn jj jyy Error bajo la hip\u00f3tesis de que los datos yi tengan la misma varianza sy, sean independientes, y que el modelo lineal sea adecuado: 2)1(y n Error Error en coeficientes? Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 74 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda yt t At tH HH yH HH A G GG G1 1] [ ; ] [ = = relaci\u00f3n errores en predicci\u00f3n y en coeficientes estimados: 1 2 222 ] [10 = = HH Ct y AAA A I \"\"#% ## #\"\" El error en los coeficientes depende de error en y, sy2 recorrido de datos X: matriz H Significatividad de par\u00e1metros Dadas las varianzas de par\u00e1metros {s2A1,...s2AF} y los propios valores estimados, An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 75 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ? ,..., 11 FAF AA A Si hay pocos datos: en vez de estad\u00ed stica normal, t-Student con n-F-1 grados de libertad Posibilidad de extender el m odelo y analizarlo: ej: dependencia cuadr\u00e1tica, ver si son significativos nuevos t\u00e9rminos Validez del modelo: an\u00e1lisis de varianza Permite rechazar o no la hip\u00f3tesis de que no existe relaci\u00f3n entre variables (relaci\u00f3n debida al azar, correlaci\u00f3n nula) () ()()2 12 12 1 = = = + = N jjN grados de libertad: I, n-I-1 -4 -3 -2 -1 0 1 x N(0,1) /2 Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 76 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 2.4. T\u00e9cnicas cl\u00e1sicas de clasificaci\u00f3n y predicci\u00f3n Modelado de datos con atributos num\u00e9ricos para su aplicaci\u00f3n a Clasificaci\u00f3n. Generalizaci\u00f3n Datos representados como nMX X X XX XGGGGGG - Tama\u00f1o: ==M jjn n 1 Para cada clase, Ci, hay ni pat rones, cada uno con I atributos: para cada clase Ci: } ...,, {)( )( 1i ni iX XGG i i Iji j i j n j xx X ,...,1 ; )()( 1 )(= =#G )g(C } ,...,{ :(.)1 X XC C C R gMI G G = = Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 77 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Funci\u00f3n discriminante de cada clase: Propiedad deseable para el dise\u00f1o de gi(.): sobre el conjunto de entrenamiento, cada patr\u00f3n de la clase Ci tiene un valor m\u00e1ximo con el discriminante gi(.): ii j k M ki j i n j Xg Xg ,...,1 )}, ({ max ) ()( Fronteras de decisi\u00f3n )(1XgG )(2XgG )(XgMGXGMax(.) CCap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 78 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda lineales Xgij :)(G 051015202530 0 5 10 15 20 25 30 + + 5 + + +++ ++++ + 123g13g12 g23 g12Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 79 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Clasificaci\u00f3n con Regresi\u00f3n Lineal: 1 Para cada clase se define la funci\u00f3n de pertenencia gi: =ii iCXCXXg GGG ;0;1)( gi: () () () ()it i it i i tI ntti nti i i yH HH A XXXX y IiG G G##GG##G ## G1 )()1( 1)()( 1 ] [ ; 1111 H 0011 = = = Hay que \"aprender\" M funciones gi Otra opci\u00f3n: para cada par gij: += ji ijCXCXXg GGG ;1;1)( Funciones lineales para todos los pares: Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 80 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas ## G1 )()( 1)()( 1 ] [ ; 1111 H 1111 = = ++ = Hay que \"aprender\" M(M-1)/2 pares gij fronteras posibles 2.4.1. Clasificaci\u00f3n bayesiana aplicaci\u00f3n de modelos estad\u00edsticos Clasificaci\u00f3n con modelo de estructura probabil\u00edstica conocida Clases: {C1, ..., CM}. Se conoce a priori: - Probabilidades de clase: P(Ci) - Distribuciones de probabilidad condicionadas (par\u00e1metros constantes) )(), ,..., ()| ,..., ( )| ,...,( 1 11 1 ii I Ii I I I i I X CPCx X x XPCx X x XP Cx xF = = G - densidad Ii I X i I Xx xCx xFCx xf =...)| ,...,()| ,...,( 11 1G G Ej.: distribuci\u00f3n normal multivariada Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 81 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Par\u00e1metros: vector de medias y matriz covarianzas () = = = 22 1 11 2/ 2 11 21 ;) ( ) (21exp 21)( F n FF x xx xxxx xx x nt n SxS x Sxf \u00b5\u00b5 =2166 21;530S \u00b5G Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 82 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Teorema de Bayes ap licado a clasificaci\u00f3n )()()|()|( XfCpCXfXCPi i i GGG = Probabilidad a posteriori: es la probabilidad de que el patr\u00f3n tenga clase Ci: )|( X CPiG Probabilidad a priori: P(Ci) es la probabilidad total de cada clase Verosimilitud: )|(iCXfG : es la distribuci\u00f3n de Ci aplicada a Densidad total: ) () |( ...)()|( )(1 1 M M CP CXf CPCXf XfG GG ++ = Criterio de clasificaci\u00f3n MAP: {} { } )()|( )|( )(i i i CpCXf im\u00e1ximo XCP im\u00e1ximo X ClaseG G de Ci: propor cional a su prob a posteriori: )()|( )(i i i CpCXf XgGG = Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 83 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda - la clase es la de aquella que maximiza el discriminante Clasificaci\u00f3n bayesiana y distrib. normal Distribuciones condicionales gaussi anas. Para cada clase Ci hay una funci\u00f3n discriminante de par\u00e1m etros mij, 2)(log))|()( log()(ijF iij j Fi i ini i i i xCPCxfCP xg \u00b5 = = =G G Par\u00e1metros de distribuci\u00f3n condicionada a cada clase Regiones de decisi\u00f3n: - Funciones Si son iguales, y diagonales: regiones lineales (caso particular) Resumen clasificador bayesiano num\u00e9rico Algoritmo: Estimar par\u00e1metros de cada clase Ci (entrenamiento) Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 84 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda i ii ni i C X XC i, } ...,,{:)( )( 1 \u00b5GG i ii xnC 12) (1\u00b5G Estimar probabilidad de cada clase == =M iii i n nNnCP 1; )( Obtener regiones de decisi\u00f3n: gij(.) Clasificaci\u00f3n Bayesiana con Atributos Nominales Atributos nominales con valores discretos - Ai={V1,...,Vni}: atributo con ni valores posibles - Pasamos de densidades a probabi lidades: probabilidad a priori: p(Ai=Vj|Ck)? - Estimaci\u00f3n \"contando\" el n\u00famero de casos: kj i k C clase de e de \u00baV Acon C clase de e (\"Naive Bayes\"): la probabilidad conjunta de varios at ributos se pone como producto )| (*...*| (*)| ( )|() ,..., , ( 2 2 1 12 2 1 1 k I I k k k iI I i CV Ap CV Ap CV Ap CXpV A V AV A X = = = == = = = Clasificaci\u00f3n: )()(*)| (*...*)| (*)| ()()(*)|()|( 2 2 1 1 ik k F F k kik k i i k XpCp CV Ap CVAp CVApXpCp CXpXCp = = == = Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 85 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Ejemplo con SI pocospXSIp iiii == = = = ==== = = = == Atributos sin valores Si el ejemplo a clasificar no tiene un atributo, simplemente se omite. - Ej.: ( salario=poco, cliente=si, edad=?, hijos=3 ) SALARIO CLIENTE EDAD HIJOS CR\u00c9DITO Poco S\u00ed Joven Uno NO Mucho Si Joven Uno SI Mucho Si Joven Uno SI Poco Si Joven Uno NO Mucho Si Joven Dos SI Poco Si Joven Dos NO Mucho Si Adulto Dos SI Mucho Si Adulto Dos SI Poco No Adulto Dos NO Mucho Si Adulto Dos SI Medio No Adulto Tres NO Mucho Si Adulto Dos SI Medio Si Adulto Dos SI Medio No Adulto Tres NO Medio No Adulto Dos SI Mucho No Mayor Tres NO Poco No Mayor Tres SI Poco No Mayor Tres SI Mucho No Mayor Tres NO Mucho No Mayor Tres SI p(SI) = 12/20 p(NO) = 8/20 SalarioCr\u00e9dito No 2/8 8/12 Medio 2/8 2/12 ClienteCr\u00e9dito No S\u00ed S\u00ed 3/8 8/12 No 5/8 4/12 Edad Cr\u00e9dito No S\u00ed Joven 3/8 3/12 Adulto 3/8 6/12 Mayor 2/8 3/12 Hijos Cr\u00e9dito No S\u00ed Uno 2/8 2/12 Dos 2/8 7/12 Tres 4/8 3/12 Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 86 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda )(/ 0375.0)(/20/8*8/4*8/3*8/4)(/)(*)| pocospXSIp iiii == = = ==== = = == Si hay faltas en la muestra de entrenamiento, no cuentan en la estimaci\u00f3n de probabilidades de ese atributo Faltas en atributo EDAD Atributos no representados. Ley m Problema: con muestra poco repres entativa, puede ocurrir que en alguna clase, un valor de atri buto no aparezca: p(Ai=Vj|Ck)=0 SALARIO CLIENTE EDAD HIJOS CR\u00c9DITO Poco S\u00ed Joven Uno NO Mucho Si Joven Uno SI Mucho Si Joven Uno SI Poco Si ? Uno NO Mucho Si ? Dos SI Poco Si ? Dos NO Mucho Si ? Dos SI Mucho Si Adulto Dos SI Poco No Adulto Dos NO Mucho Si Adulto Dos SI Medio No Adulto Tres NO Mucho Si Adulto Dos SI Medio Si Adulto Dos SI Medio No Adulto Tres NO Medio No Adulto Dos SI Mucho No Mayor Tres NO Poco No Mayor Tres SI Poco No Mayor Tres SI Mucho No Mayor Tres NO Mucho No Mayor Tres SI SalarioCr\u00e9dito No S\u00ed Poco 4/8 2/12 Mucho 2/8 8/12 Medio 2/8 2/12 ClienteCr\u00e9dito No S\u00ed S\u00ed 3/8 8/12 No 5/8 4/12 Edad Cr\u00e9dito No S\u00ed Cr\u00e9dito Uno 2/8 2/12 Dos 2/8 7/12 Tres 4/8 3/12 Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 87 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda - Cualquier ejemplo X con Ai=Vj generar\u00e1 P(Ck|X)=0, independientemente de lo s otros atributos! Se suele modificar la estimaci\u00f3n de las probabilidades a priori con un factor que elimina los ceros. - Ej.: P(Edad|Cr\u00e9dito=NO)= 82: ,83: - A veces simplemente se inicializ an las cuentas a 1 en vez de 0: ++ ++ ++ 3812: ,3813: ,3813: (\"Naive Bayes\") )C| V A(p*...*)C|V A(p*)C|V A(p)C|X(p k F F k 2 2 k 1 1k i = = == - Atributos discretos: probabilidade s a priori con cada clase Ck kj i k C clase de e de \u00baV Acon C clase de e de \u00ba )| ( jemplos njemplos n CV Apk j i= = = - Atributos continuos: densidades de clase Ck: normales de par\u00e1metros mk, sk Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 88 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u00b5 = = 2 ik2 ikj ikkj A kj i ) (salario=700, cliente=si, edad=adulto, hijos=3) SALARIO CLIENTE EDAD HIJOS CR\u00c9DITO 525 S\u00ed Joven 1 NO 2000 Si Joven 1 SI 2500 Si Joven 1 SI 470 Si Joven 1 NO 3000 Si Joven 2 SI 510 Si Joven 2 NO 2800 Si Adulto 2 SI 2700 Si Adulto 2 SI 550 No Adulto 2 NO 2600 Si Adulto 2 SI 1100 No Adulto 3 NO 2300 Si Adulto 2 SI 1200 Si Adulto 2 SI 900 No Adulto 3 NO 800 No Adulto 2 SI 800 No Mayor 3 NO 1300 No Mayor 3 SI 1100 No Mayor 3 SI 1000 No Mayor 3 NO 4000 No Mayor 3 SI p(SI) = 12/20 p(NO) = 8/20 Hijos 2.25 2.08 Desv Est\u00e1ndar 0.89 0.67 Edad Cr\u00e9dito No S\u00ed 3/8 3/12 Adulto 3/8 6/12 Mayor 2/8 3/12 ClienteCr\u00e9dito No S\u00ed S\u00ed 3/8 8/12 No 5/8 4/12 SalarioCr\u00e9dito No S\u00ed Media 732 2192 Desv Est\u00e1ndar 249 942 Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 89 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas SI sfXSIp ii H Siii H Si == = = = = == == = = = = == Clasificaci\u00f3n con costes MAP proporciona clasificaci\u00f3n con m\u00ednima prob. de Error - Coste de decisi\u00f3n : prob. Error total= Con frecuencia los costes son as im\u00e9tricos, y unos errores son m\u00e1s graves que otros. Matriz de costes Costes de cada decisi\u00f3n. Criterio de m\u00ednimo coste medio: dada una decisi\u00f3n, promedio los costes de cada equivocaci\u00f3n y su coste: )|( )|( )|( cos)|( )|( )|( cos)|( )|( )|( cos 2 23 1 13 33 32 1 12 23 31 2 21 1 XCpc XCpc XDteXCpc XCpc XDteXCpc XCpc XDte G + =+ =+ = 000 32 3123 2113 12 c cc cc c Clase real Clasificado comoCap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 90 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Ejemplo de clasificaci\u00f3n con costes Clasificaci\u00f3n de setas con dos atributos, (X, Y) y tres categor\u00edas: Venenosa, Mal sabor, comestible : {V, MS, C } 2.4.2. Regresi\u00f3n Lineal La regresi\u00f3n lineal [DOB90] es la forma m\u00e1s simple de regresi\u00f3n, ya que en ella se modelan los datos usando una l\u00ednea recta. Se caracteriza, por tanto, por la utilizaci\u00f3n de dos variables, una aleatoria, y (llamada variable respuesta), que es funci\u00f3n lineal de otra variable aleatoria, x (llamada variable predictora), form\u00e1ndose la ecuaci\u00f3n 2.13. -30 -20 -10 0 10 20 30 40 50-50-40-30-20-100102030 -30 -20 -10 0 10 20 30 40 50-50-40-30-20-100102030 0 1 110 0 11000 10000 Clase Clasificado V MS C V MS C [] [] [] = = = = :71 4040 71;55 :71 CC V ttt \u00b5\u00b5\u00b5 VC MS VC MS M\u00ednimo error M\u00ednimo coste Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 91 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda bxay += Ec. 2.13 En esta ecuaci\u00f3n la variaci\u00f3n de y se asume que es constante, y a y b son los coeficientes de regresi\u00f3n que especifican la intersecci\u00f3n con el eje de ordenadas, y la pendiente de la recta, respectivamente. Estos coeficientes se calculan utilizando el m\u00e9todo de los m\u00ednimos cuadrados [PTVF96] que minimizan el error entre los datos reales y la estimaci\u00f3n de la l\u00ednea. Dados s ejemplos de datos en forma de puntos (x 1, y1), (x2, x2),..., (xs, ys), entonces los coeficientes de la regresi\u00f3n pueden estimarse seg\u00fan el m\u00e9todo de los m\u00ednimos cuadrados con las ecuaciones 2.14 y 2.15. 2 xxy SSb= Ec. 2.14 bx-ya= Ec. 2.15 En la ecuaci\u00f3n 2.14, Sxy es la covarianza de x e y, y Sx2 la varianza de x. Tambi\u00e9n es necesario saber cu\u00e1n buena es la recta de regresi\u00f3n construida. Para ello, se emplea el coeficiente de regresi\u00f3n (ecuaci\u00f3n 2.16), que es una medida del ajuste de la muestra. 2 y2 x2 xy 2 SSSR= Ec. 2.16 El valor de R2 debe estar entre 0 y 1. Si se acerca a 0 la recta de regresi\u00f3n no tiene un buen ajuste, mientras que si se acerca a 1 el ajuste es \"perfecto\". Los coeficientes a y b a menudo proporcionan buenas aproximaciones a otras ecuaciones de regresi\u00f3n complicadas. En el ejemplo siguiente, para una muestra de 35 marcas de cerveza, se estudia la relaci\u00f3n entre el grado de alcohol de las cervezas y su contenido cal\u00f3rico. y se representa un peque\u00f1o conjunto de datos. Figura 2.1: Regresi\u00f3n lineal simple. Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 92 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda El eje vertical muestra el n\u00famero de calor\u00edas (por cada tercio de litro) y el horizontal el contenido de alcohol (expresado en porcentaje). La nube de puntos es la representaci\u00f3n de los datos de la muestra, y la recta es el resultado de la regresi\u00f3n lineal aplicando el ajuste de los m\u00ednimos cuadrados. En los siguientes apartados se mostrar\u00e1n dos tipos de regresiones que ampl\u00edan la regresi\u00f3n lineal simple. Regresi\u00f3n Lineal M\u00faltiple La regresi\u00f3n Lineal M\u00faltiple [PTVF96] es una extensi\u00f3n de regresi\u00f3n lineal que involucra m\u00e1s de una variable predictora, y permite que la variable respuesta y sea planteada como una funci\u00f3n lineal de un vector multidimensional. El modelo de regresi\u00f3n m\u00faltiple para n variables predictoras ser\u00eda como el que se muestra en la ecuaci\u00f3n 2.17. nn 22 11 0 xb xbxbby ++ + = ... Ec. 2.17 Para encontrar los coeficientes bi se plantea el modelo en t\u00e9rminos de matrices, como se muestra en la ecuaci\u00f3n 2.18. = mn m11n 211n 11 n21 bbb B# Ec. 2.18 En la matriz Z, las filas representan los m ejemplos disponibles para calcular la regresi\u00f3n, y las columnas los n atributos que formar\u00e1n parte de la regresi\u00f3n. De esta forma, zij ser\u00e1 el valor que toma en el ejemplo i el atributo j. El vector Y est\u00e1 formado por los valores de la variable dependiente para cada uno de los ejemplos, y el vector B es el que se desea calcular, ya que se corresponde con los par\u00e1metros desconocidos necesarios para construir la regresi\u00f3n lineal m\u00faltiple. Representando con XT la matriz traspuesta de X y con X-1 la inversa de la matriz X, se calcular\u00e1 el vector B mediante la ecuaci\u00f3n 2.19. () YZZZ BT1T= Ec. 2.19 Para determinar si la recta de regresi\u00f3n lineal m\u00faltiple est\u00e1 bien ajustada, se emplea el mismo concepto que en el caso de la regresi\u00f3n lineal simple: el coeficiente de regresi\u00f3n. En este caso, utilizar\u00e1 la ecuaci\u00f3n 2.20. ) ()==m 1i2 iT T 2 yyZB-Y ZB-Y1 R Ec. 2.20 Al igual que en el caso de la regresi\u00f3n simple, el valor de R2 debe estar entre 0 y 1, siendo 1 el indicador de ajuste perfecto. Una vez explicado el modo b\u00e1sico por el que se puede obtener una recta de regresi\u00f3n m\u00faltiple para un conjunto de ejemplos de entrenamiento, a continuaci\u00f3n se muestra, en la figura 2.11, un ejemplo concreto en el que se muestra el proceso. Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 93 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 2.2: Ejemplo de obtenci\u00f3n de una Regresi\u00f3n Lineal M\u00faltiple. Tal y como se muestra en la figura 2.11, en un primer momento se obtienen, a partir de los ejemplos de entrenamiento, las matrices Z e Y, siendo el objetivo la matriz B. En el segundo paso se calcula los valores de dicha matriz, que ser\u00e1n los coeficientes en la regresi\u00f3n. Por \u00faltimo, en un tercer paso se comprueba si la recta generada tiene un buen ajuste o no. En este caso, como se muestra en la misma figura, el ajuste es magn\u00edfico, dado que el valor de R 2 es muy cercano a 1. Por \u00faltimo, en este ejemplo no se ha considerado el t\u00e9rmino independiente, pero para que se obtuviese bastar\u00eda con a\u00f1adir una nueva columna a la matriz Z con todos los valores a 1. Selecci\u00f3n de Variables Adem\u00e1s del proceso anterior para la generaci\u00f3n de la regresi\u00f3n lineal, se suele realizar un procedimiento estad\u00edstico que seleccione las mejores variables predictoras, ya que no todas tienen la misma importancia, y reducir su n\u00famero har\u00e1 que computacionalmente mejore el tiempo de respuesta del modelo. Los procesos que se siguen para la selecci\u00f3n de variables predictoras son b\u00e1sicamente dos: eliminaci\u00f3n hacia atr\u00e1s [backward elimination], consistente en obtener la regresi\u00f3n lineal para todos los par\u00e1metros e ir eliminando uno a uno los menos importantes; y selecci\u00f3n hacia delante [fordward selection], que consiste en generar una regresi\u00f3n lineal simple (con el mejor par\u00e1metro, esto es, el m\u00e1s correlacionado con la variable a predecir) e ir a\u00f1adiendo par\u00e1metros al modelo. Hay un gran n\u00famero de estad\u00edsticos que permiten seleccionar los par\u00e1metros, y a modo de ejemplo se comentar\u00e1 el basado en el criterio Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 94 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda de informaci\u00f3n Akaike [AKA73], que se basa en la teor\u00eda de la informaci\u00f3n y cuya formulaci\u00f3n se muestra en la ecuaci\u00f3n 2.21. ()2pLlog2 AIC + \u00d7= Ec. 2.21 En esta ecuaci\u00f3n L es la verosimilitud [likelihood] y p el n\u00famero de variables predictorias. Aplicado a la regresi\u00f3n, el resultado ser\u00eda el que se muestra en las ecuaciones 2.22 y 2.23. ( 2.23 En la ecuaci\u00f3n 2.22, m es el n\u00famero de ejemplos disponibles, y MSE es el error cuadr\u00e1tico medio [mean squared error] del modelo, tal y como se define en la ecuaci\u00f3n 2.23. En esta ecuaci\u00f3n yi es el valor de la clase para el ejemplo i e iy el valor que la regresi\u00f3n lineal da al ejemplo i. En la pr\u00e1ctica algunas herramientas no utilizan exactamente la ecuaci\u00f3n 2.22, sino una aproximaci\u00f3n de dicha ecuaci\u00f3n. Regresi\u00f3n Lineal Ponderada Localmente Otro m\u00e9todo de predicci\u00f3n num\u00e9rica es la regresi\u00f3n lineal ponderada localmente [Locally weighted linear regresi\u00f3n]. Con este m\u00e9todo se generan modelos locales durante el proceso de predicci\u00f3n dando m\u00e1s peso a aquellos ejemplares de entrenamiento m\u00e1s cercanos al que hay que predecir. Dicho de otro modo, la construcci\u00f3n del clasificador consiste en el almacenamiento de los ejemplos de entrenamiento, mientras que el proceso de validaci\u00f3n o de clasificaci\u00f3n de un ejemplo de test consiste en la generaci\u00f3n de una regresi\u00f3n lineal espec\u00edfica, esto es, una regresi\u00f3n lineal en la que se da m\u00e1s peso a aquellos ejemplos de entrenamiento cercanos al ejemplo a clasificar. De esta forma, este tipo de regresi\u00f3n est\u00e1 \u00edntimamente relacionado con los algoritmos basados en ejemplares. Para utilizar este tipo de regresi\u00f3n es necesario decidir un esquema de ponderaci\u00f3n para los ejemplos de entrenamiento, esto es, decidir cu\u00e1nto peso se le va a dar a cada ejemplo de entrenamiento para la clasificaci\u00f3n de un ejemplo de test. Una medida usual es ponderar el ejemplo de entrenamiento con la inversa de la distancia eucl\u00eddea entre dicho ejemplo y el de test, tal y como se muestra en ecuaci\u00f3n 2.24. ijid11+= Ec. 2.24 En esta ecuaci\u00f3n i es el peso que se le otorgar\u00e1 al ejemplo de entrenamiento i para clasificar al ejemplo j, y dij ser\u00e1 la distancia eucl\u00eddea de i con respecto a j. M\u00e1s cr\u00edtico que la elecci\u00f3n del m\u00e9todo para ponderar es el \"par\u00e1metro de suavizado\" que se utilizar\u00e1 para escalar la funci\u00f3n de distancia, esto es, la distancia ser\u00e1 multiplicada por la inversa de este par\u00e1metro. Si este par\u00e1metro es muy peque\u00f1o s\u00f3lo los ejemplos muy cercanos recibir\u00e1n un gran peso, mientras que si es demasiado grande los ejemplos muy lejanos podr\u00edan tener peso. Un modo de asignar un valor a este par\u00e1metro es d\u00e1ndole el valor de la distancia del k-\u00e9simo vecino m\u00e1s cercano al Cap\u00edtulo 2 An\u00e1lisis Estad\u00edstico Mediante Excel T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 95 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ejemplo a clasificar. El valor de k depender\u00e1 del ruido de los datos. Cuanto m\u00e1s ruido, m\u00e1s grande deber\u00e1 ser k. Una ventaja de este m\u00e9todo de estimaci\u00f3n es que es capaz de aproximar funciones no lineales. Adem\u00e1s, se puede actualizar el clasificador (modelo incremental), dado que \u00fanicamente ser\u00eda necesario a\u00f1adirlo al conjunto de entrenamiento. Sin embargo, como el resto de algoritmos basado en ejemplares, es lento. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 96 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Cap\u00edtulo 3. T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico 3.1. T\u00e9cnicas de Miner\u00eda de Datos Como ya se ha comentado, las t\u00e9cnicas de Miner\u00eda de Datos (una etapa dentro del proceso completo de KDD [FAYY96]) intentan obtener patrones o modelos a partir de los datos recopilados. Decidir si los modelos obtenidos son \u00fatiles o no suele requerir una valoraci\u00f3n subjetiva por parte del usuario. Las t\u00e9cnicas de Miner\u00eda de Datos se clasifican en dos grandes categor\u00edas: supervisadas o predictivas y no supervisadas o descriptivas [WI98]. Num\u00e9rico Clustering Conceptual Probabilistico No supervisadas Asociaci\u00f3n A Priori T\u00e9cnicas Regresi\u00f3n Predicci\u00f3n \u00c1rboles de Predicci\u00f3n Estimador de N\u00facleos Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 97 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Supervisadas Tabla de Decisi\u00f3n \u00c1rboles de Decisi\u00f3n Inducci\u00f3n de Reglas Bayesiana Clasificaci\u00f3n Basado en Ejemplares Redes de Neuronas L\u00f3gica Borrosa T\u00e9cnicas Gen\u00e9ticas Figura 3.1: T\u00e9cnicas de la Miner\u00eda de Datos Una t\u00e9cnica constituye el enfoque conceptual para extraer la informaci\u00f3n de los datos, y, en general es implementada por varios algoritmos. Cada algoritmo representa, en la pr\u00e1ctica, la manera de desarrollar una determinada t\u00e9cnica paso a paso, de forma que es preciso un entendimiento de alto nivel de los algoritmos para saber cual es la t\u00e9cnica m\u00e1s apropiada para cada problema. Asimismo es preciso entender los par\u00e1metros y las caracter\u00edsticas de los algoritmos para preparar los datos a analizar. Las predicciones se utilizan para prever el comportamiento futuro de alg\u00fan tipo de entidad mientras que una descripci\u00f3n puede ayudar a su comprensi\u00f3n. De hecho, los modelos predictivos pueden ser descriptivos (hasta donde sean comprensibles por personas) y los modelos descriptivos pueden emplearse para realizar predicciones. De esta forma, hay algoritmos o t\u00e9cnicas que pueden servir para distintos prop\u00f3sitos, por lo que la figura anterior \u00fanicamente representa para qu\u00e9 prop\u00f3sito son m\u00e1s utilizadas las t\u00e9cnicas. Por ejemplo, las redes de neuronas pueden servir para predicci\u00f3n, clasificaci\u00f3n e incluso para aprendizaje no supervisado. El aprendizaje inductivo no supervisado estudia el aprendizaje sin la ayuda del maestr o; es decir, se aborda el aprendizaje sin supervisi\u00f3n, que trata de ordenar los ejemplos en una jerarqu\u00eda seg\u00fan las regularidades en la distribuci\u00f3n de los pares atributo-valor sin la gu\u00eda del atributo especial clase. \u00c9ste es el proceder de los sistemas que realizan clustering conceptual y de los que se dice tambi\u00e9n que adquieren nuevos conceptos. Otra posibilidad contemplada para estos sistemas es la de sintetizar conocimiento cualitativo o cuant itativo, objetivo de los sistemas que llevan a cabo tareas de descubrimiento. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 98 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda En el aprendizaje inductivo supervisado existe un atributo especial, normalmente denominado clase, presente en todos los ejemplos que especifica si el ejemplo pertenece o no a un cierto concepto, que ser\u00e1 el objetivo del aprendizaje. El atributo clase normalmente toma los valores + y -, que significan la pertenencia o no del ejemplo al concepto que se trata de aprender; es decir, que el ejemplo ejemplifica positivamente al concepto -pertenece al conc epto- o bien lo ejemplifica negativamente -que no pertenece al concepto. Mediante una generalizaci\u00f3n del papel del atributo clase, cualquier atributo puede desempe\u00f1ar ese papel, convirti\u00e9ndose la clasificaci\u00f3n de los ejemplos seg\u00fan los valores del atributo en cuesti\u00f3n, en el objeto del aprendizaje. Expresado en una forma breve, el objetivo del aprendizaje supervisado es: a partir de un conjunto de ejemplos, denominados de entrenamiento, de un cierto dominio D de ellos, construir criterios para determinar el valor del atributo clase en un ejemplo cualquiera del dominio. Esos criterios est\u00e1n basados en los valores de uno o varios de los otros pares (atributo; valor) que intervienen en la definici\u00f3n de los ejemplos. Es sencillo transmitir esa idea al caso en el que el atributo que juega el papel de la clase sea uno cualquiera o con m\u00e1s de dos valores. Dentro de este tipo de aprendizaje se pueden distinguir dos grandes grupos de t\u00e9cnicas: la predicci\u00f3n y la clasificaci\u00f3n [WK91]. A continuaci\u00f3n se presentan las principales t\u00e9cnicas (supervisadas y no supervisadas) de miner\u00eda de datos 3.2. Clustering. (\"Segmentaci\u00f3n\") Tambi\u00e9n llamada agrupamiento, permite la identificaci\u00f3n de tipolog\u00edas o grupos donde los elementos guardan gran similitud entre s\u00ed y muchas diferencias con los de otros grupos. As\u00ed se puede segmentar el colectivo de clientes, el conjunto de valores e \u00edndices financieros, el espectro de observaciones astron\u00f3micas, el conjunto de zonas forestales, el conjunto de empleados y de sucu rsales u oficinas, etc. La segmentaci\u00f3n est\u00e1 teniendo mucho inter\u00e9s desde hace ya tiempo dadas las importantes ventajas que aporta al permitir el tratamiento de grandes colectivos de forma pseudoparticularizada, en el m\u00e1s id\u00f3neo punto de equilibrio entre el tratamiento individualizado y aquel totalmente masificado. Las herramientas de segmentaci\u00f3n se basan en t\u00e9cnicas de car\u00e1cter estad\u00edstico, de empleo de algoritmos matem\u00e1ticos, de generaci\u00f3n de reglas y de redes neuronales para el tratamiento de registros. Para otro tipo de elementos a agrupar o segmentar, como texto y documentos, se usan t\u00e9cnicas de reconocimiento de conceptos. Esta t\u00e9cnica suele servir de punto de partida para despu\u00e9s hacer un an\u00e1lisis de clasificaci\u00f3n sobre los clusters . La principal caracter\u00edstica de esta t\u00e9cnica es la utilizaci\u00f3n de una medida de similaridad que, en general, est\u00e1 basada en los atributos que describen a los objetos, y se define usualmente por proximidad en un espacio multidimensional. Para datos num\u00e9ricos, suele ser preciso preparar los datos antes de realizar data mining sobre ellos, de manera que en primer lugar se someten a un proceso de estandarizaci\u00f3n. Una de las t\u00e9cnicas empleadas para conseguir la normalizaci\u00f3n de los datos es utilizar la medida z ( z-score ) que elimina las unidades de los datos. Esta medida, z, es la que se muestra en la ecuaci\u00f3n 2.1, donde \u00b5 f es la media de la variable f y f la desviaci\u00f3n t\u00edpica de la misma. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 99 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ff if if\u00b5 x z = Ec. 2.1 Entre las medidas de similaridad destaca la distancia eucl\u00eddea, ecuaci\u00f3n 2.2. () = =n 1l2 jl il j i x x x d(x ), Ec. 2.2 Hay varios algoritmos de clustering . A continuaci\u00f3n se exponen los m\u00e1s conocidos. 3.2.1. Clustering Num\u00e9rico (k-medias) Uno de los algoritmos m\u00e1s utilizados para hacer clustering es el k-medias (k- means) [MAC67], que se caracteriza por su sencillez. En primer lugar se debe especificar por adelantado cuantos clusters se van a crear, \u00e9ste es el par\u00e1metro k, para lo cual se seleccionan k elementos aleatoriamente, que representaran el centro o media de cada cluster. A continuaci\u00f3n cada una de las instancias, ejemplos, es asignada al centro del cluster m\u00e1s cercano de acuerdo con la distancia Euclidea que le separa de \u00e9l. Para cada uno de los clusters as\u00ed construidos se calcula el centroide de todas sus instancias. Estos centroides son tomados como los nuevos centros de sus respectivos clusters. Finalmente se repite el proceso completo con los nuevos centros de los clusters. La iteraci\u00f3n contin\u00faa hasta que se repite la asignaci\u00f3n de los mismos ejemplos a los mismos clusters, ya que los puntos centrales de los clusters se han estabilizado y permanecer\u00e1n invariables despu\u00e9s de cada iteraci\u00f3n. El algoritmo de k-medias es el siguiente: 1. Elegir k ejemplos que act\u00faan como semillas ( k n\u00famero de clusters). 2. Para cada ejemplo, a\u00f1adir ejemplo a la clase m\u00e1s similar. 3. Calcular el centroide de cada clase, que pasan a ser las nuevas semillas 4. Si no se llega a un criterio de convergencia (por ejemplo, dos iteraciones no cambian las clasificaciones de los ejemplos), volver a 2. Figura 3.2: Pseudoc\u00f3digo del algoritmo de k-medias. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 100 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Para obtener los centroides, se calcula la media [mean] o la moda [mode] seg\u00fan se trate de atributos num\u00e9ricos o simb\u00f3licos. A continuaci\u00f3n, en la figura 2.3, se muestra un ejemplo de clustering con el algoritmo k-medias. En este caso se parte de un total de nueve ejemplos o instancias, se configura el algoritmo para que obtenga 3 clusters, y se inicializan aleatoriamente los centroides de los clusters a un ejemplo determinado. Una vez inicializados los datos, se comienza el bucle del algoritmo. En cada una de las gr\u00e1ficas inferiores se muestra un paso por el algoritmo. Cada uno de los ejemplos se representa con un tono de color diferente que indica la pertenencia del ejemplo a un cluster determinado, mientras que los centroides siguen mostr\u00e1ndose como c\u00edrculos de mayor tama\u00f1o y sin relleno. Por ultimo el proceso de clustering finaliza en el paso 3, ya que en la siguiente pasada del algoritmo (realmente har\u00eda cuatro pasadas, si se configurara as\u00ed) ning\u00fan ejemplo cambiar\u00eda de cluster. Figura 3.3: Ejemplo de clustering con k-medias. 3.2.2. Clustering Conceptual (COBWEB) El algoritmo de k-medias se encuentra con un problema cuando los atributos no son num\u00e9ricos, ya que en ese caso la distancia entre ejemplares no est\u00e1 tan clara. Para resolver este problema Michalski [MS83] presenta la noci\u00f3n de clustering conceptual, que utiliza para justificar la necesidad de un clustering cualitativo frente al clustering cuantitativo, basado en la vecindad entre los elementos de la poblaci\u00f3n. En este tipo de clustering una partici\u00f3n de los datos es buena si cada clase tiene una Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 101 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda buena interpretaci\u00f3n conceptual (modelo cognitivo de jerarqu\u00edas). Una de las principales motivaciones de la categorizaci\u00f3n de un conjunto de ejemplos, que b\u00e1sicamente supone la formaci\u00f3n de conceptos, es la predicci\u00f3n de caracter\u00edsticas de las categor\u00edas que heredar\u00e1n sus subcategor\u00edas. Esta conjetura es la base de COBWEB [FIS87]. A semejanza de los humanos, COBWEB forma los conceptos por agrupaci\u00f3n de ejemplos con atributos similares. Representa los clusters como una distribuci\u00f3n de probabilidad sobre el espacio de los valores de los atributos, generando un \u00e1rbol de clasificaci\u00f3n jer\u00e1rquica en el que los nodos intermedios definen subconceptos. El objetivo de COBWEB es hallar un conjunto de clases o clusters (subconjuntos de ejemplos) que maximice la utilidad de la categor\u00eda (partici\u00f3n del conjunto de ejemplos cuyos miembros son clases). La descripci\u00f3n probabil\u00edstica se basa en dos conceptos: Predicibilidad: Probabilidad condicional de que un suceso tenga un cierto atributo dada la clase, P(A i=Vij|Ck). El mayor de estos valores corresponde al valor del atributo m\u00e1s predecible y es el de los miembros de la clase (alta similaridad entre los elementos de la clase). Previsibilidad: Probabilidad condicional de que un ejemplo sea una instancia de una cierta clase, dado el valor de un atributo particular, P(C k|Ai=Vij). Un valor alto indica que pocos ejemplos de las otras clases comparten este valor del atributo, y el valor del atributo de mayor probabilidad es el de los miembros de la clase (baja similaridad interclase). Estas dos medidas, combinadas mediante el teorema de Bayes, proporcionan una funci\u00f3n que eval\u00faa la utilidad de una categor\u00eda (CU), que se muestra en la ecuaci\u00f3n 2.3. () i ij2 k ij i k = = == Ec. 2.3 En esta ecuaci\u00f3n n es el n\u00famero de clases y las sumas se extienden a todos los atributos Ai y sus valores Vij en cada una de las n clases Ck. La divisi\u00f3n por n sirve para incentivar tener clusters con m\u00e1s de un elemento. La utilidad de la categor\u00eda mide el valor esperado de valores de atributos que pueden ser adivinados a partir de la partici\u00f3n sobre los valores que se pueden adivinar sin esa partici\u00f3n. Si la partici\u00f3n no ayuda en esto, entonces no es una buena partici\u00f3n. El \u00e1rbol resultante de este algoritmo cabe denominarse organizaci\u00f3n probabil\u00edstica o jer\u00e1rquica de conceptos. En la figura 2.4 se muestra un ejemplo de \u00e1rbol que se podr\u00eda generar mediante COBWEB. En la construcci\u00f3n del \u00e1rbol, incrementalmente se incorpora cada ejemplo al mismo, donde cada nodo es un concepto probabil\u00edstico que representa una clase de objetos. COBWEB desciende por el \u00e1rbol buscando el mejor lugar o nodo para cada ejemplo. Esto se basa en medir en cu\u00e1l se tiene la mayor ganancia de utilidad de categor\u00eda. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 102 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.4: Ejemplo de \u00e1rbol generado por COBWEB. Sin embargo, no se puede garantizar que se genere este \u00e1rbol, dado que el algoritmo es sensible al orden en que se introduzcan los ejemplos. En cuanto a las etiquetas de los nodos, \u00e9stas fueron puestas a posteriori, coherentes con los valores de los atributos que determinan el nodo. Cuando COBWEB incorpora un nuevo ejemplo en el nodo de clasificaci\u00f3n, desciende a lo largo del camino apropiado, actualizando las cuentas de cada nodo, y llevando a cabo por medio de los diferentes operadores, una de las siguientes acciones: Incorporaci\u00f3n: A\u00f1adir un nuevo ejemplo a un nodo ya existente. Creaci\u00f3n de una nueva disyunci\u00f3n: Crear una nueva clase. Uni\u00f3n: Combinar dos clases en una sola. Divisi\u00f3n: Dividir una clase existente en varias clases. La b\u00fasqueda, que se realiza en el espacio de conceptos, es por medio de un heur\u00edstico basado en el m\u00e9todo de escalada gracias a los operadores de uni\u00f3n y divisi\u00f3n. En la figura 2.5 se muestra el resultado de aplicar cada una de estas operaciones. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 103 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.5: Operaciones de COBWEB. 1. Nuevo Ejemplo: Lee un ejemplo e. Si no hay m\u00e1s ejemplos, terminar. 2. Actualiza ra\u00edz. Actualiza el c\u00e1lculo de la ra\u00edz. 3. Si la ra\u00edz es hoja, entonces: Expandir en dos nodos hijos y acomodar en cada uno de ellos un ejemplo; volver a 1. 4. Avanzar hasta el siguiente nivel: Aplicar la funci\u00f3n de evaluaci\u00f3n a varias opciones para determinar, mediante la f\u00f3rmula de utilidad de una categor\u00eda, el mejor (m\u00e1xima CU) lugar donde incorporar el ejemplo en el nivel siguiente de la jerarqu\u00eda. En las opciones que se evaluar\u00e1n se considerar\u00e1 \u00fanicamente el nodo actual y sus hijos y se elegir\u00e1 la mejor opci\u00f3n de las siguientes: a. A\u00f1adir e a un nodo que existe (al mejor hijo) y, si esta opci\u00f3n resulta ganadora, comenzar de nuevo el proceso de avance hacia el siguiente nivel en ese nodo hijo. b. Crear un nuevo nodo conteniendo \u00fanicamente a e y, si esta opci\u00f3n resulta ganadora, volver a 1. c. Juntar los dos mejores nodos hijos con eincorporado al nuevo nodo combinado y, si esta opci\u00f3n resulta ganadora, comenzar el nuevo proceso de avanzar hacia el siguiente nivel en ese nuevo nodo. d. Dividir el mejor nodo, reemplazando este nodo con sus hijos y, si esta opci\u00f3n resulta ganadora, aplicar la funci\u00f3n de evaluaci\u00f3n para incorporar een los nodos originados por la divisi\u00f3n. Figura 3.6: Algoritmo de COBWEB. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 104 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda El algoritmo se puede extender a valores num\u00e9ricos usando distribuciones gaussianas, ecuaci\u00f3n 2.4. De esta forma, el sumatorio de probabilidades es ahora como se muestra en la ecuaci\u00f3n 2.5. ()() 222\u00b5x e ij2 i21dxxf VAP Ec. 2.5 Por lo que la ecuaci\u00f3n de la utilidad de la categor\u00eda quedar\u00eda como se muestra en la ecuaci\u00f3n 2.6. () = = ii ikn 1kk1 1 21CPk1CU - Ec. 2.6 3.2.3. Clustering Probabil\u00edstico (EM) Los algoritmos de clustering estudiados hasta el momento presentan ciertos defectos entre los que destacan la dependencia que tiene el resultado del orden de los ejemplos y la tendencia de estos algoritmos al sobreajuste [overfitting]. Una aproximaci\u00f3n estad\u00edstica al problema del cl ustering resuelve estos problemas. Desde este punto de vista, lo que se busca es el grupo de clusters m\u00e1s probables dados los datos. Ahora los ejemplos tienen ciertas probabilidades de pertenecer a un cluster. La base de este tipo de clustering se encuentra en un modelo estad\u00edstico llamado mezcla de distribuciones [finite mixtures]. Cada distribuci\u00f3n representa la probabilidad de que un objeto tenga un conjunto particular de pares atributo-valor, si se supiera que es miembro de ese cluster. Se tienen k distribuciones de probabilidad que representan los k clusters. La mezcla m\u00e1s sencilla se tiene cuando los atributos son num\u00e9ricos con distribuciones gaussianas. Cada distribuci\u00f3n (normal) se caracteriza por dos par\u00e1metros: la media ( \u00b5) y la varianza ( 2). Adem\u00e1s, cada distribuci\u00f3n tendr\u00e1 cierta probabilidad de aparici\u00f3n p, que vendr\u00e1 determinada por la proporci\u00f3n de ejemplos que pertenecen a dicho cluster respecto del n\u00famero total de ejemplos. En ese caso, si hay k clusters, habr\u00e1 que calcular un total de 3k-1 par\u00e1metros: las k medias, k varianzas y k-1 probabilidades de la distribuci\u00f3n dado que la suma de probabilidades debe ser 1, con lo que conocidas k-1 se puede determinar la k-\u00e9sima . Si se conociera el cluster al que pertenece, en un principio, cada uno de los ejemplos de entrenamiento ser\u00eda muy sencillo obtener los 3k-1 par\u00e1metros necesarios para definir totalmente las distribuciones de dichos clusters, ya que simplemente se aplicar\u00edan las ecuaciones de la media y de la varianza para cada uno de los clusters. Adem\u00e1s, para calcular la probabilidad de cada una de las distribuciones \u00fanicamente se dividir\u00eda el n\u00famero de ejemplos de entrenamiento que pertenecen al cluster en cuesti\u00f3n entre el n\u00famero total de ejemplos de entrenamiento. Una vez obtenidos estos par\u00e1metros, si se deseara calcular la probabilidad de pertenencia de un determinado ejemplo de test a cada cluster, simplemente se aplicar\u00eda el teorema de Bayes, ecuaci\u00f3n 2.54 a cada problema concreto, con lo que quedar\u00eda la ecuaci\u00f3n 2.7. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 105 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda P(x)p\u00b5f(x; P(x)A)P(A)|P(xx)|P(AA A A ),= = Ec. 2.7 En esta ecuaci\u00f3n A es un cluster del sistema, x el ejemplo de test, pA la probabilidad del cluster A y f(x;\u00b5 A,A) la funci\u00f3n de la distribuci\u00f3n normal del cluster A, que se expresa con la ecuaci\u00f3n 2.4. Sin embargo, el problema es que no se sabe de qu\u00e9 distribuci\u00f3n viene cada dato y se desconocen los par\u00e1metros de las distribuciones. Por ello se adopta el procedimiento empleado por el algoritmo de clustering k-medias, y se itera. El algoritmo EM ( Expectation Maximization ) empieza adivinando los par\u00e1metros de las distribuciones (dicho de otro modo, se empieza adivinando las probabilidades de que un objeto pertenezca a una clase) y, a continuaci\u00f3n, los utiliza para calcular las probabilidades de que cada objeto pertenezca a un cluster y usa esas probabilidades para re-estimar los par\u00e1metros de las probabilidades, hasta converger. Este algoritmo recibe su nombre de los dos pasos en los que se basa cada iteraci\u00f3n: el c\u00e1lculo de las probabilidades de los grupos o los valores esperados de los grupos, mediante la ecuaci\u00f3n 2.7, denominado expectation ; y el c\u00e1lculo de los valores de los par\u00e1metros de las distribuciones, denominado maximization , en el que se maximiza la verosimilitud de las distribuciones dados los datos. Para estimar los par\u00e1metros de las distribuciones se tiene que considerar que se conocen \u00fanicamente las probabilidades de pertenencia a cada cluster, y no los clusters en s\u00ed. Estas probabilidades act\u00faan como pesos, con lo que el c\u00e1lculo de la media y la varianza se realiza con las ecuaciones 2.8 y 2.9 respectivamente. = == N 1i iN 1i ii Awxw\u00b5 Ec. 2.8 ( ) = == N 1i iN 1i ii 2 Aw\u00b5xw Ec. 2.9 Donde N es el n\u00famero total de ejemplos del conjunto de entrenamiento y wi es la probabilidad de que el ejemplo i pertenezca al cluster A. La cuesti\u00f3n es determinar cu\u00e1ndo se finaliza el procedimiento, es decir en que momento se dejan de realizar iteraciones. En el algoritmo k-medias se finalizaba cuando ning\u00fan ejemplo de entrenamiento cambiaba de cluster en una iteraci\u00f3n, alcanz\u00e1ndose as\u00ed un \"punto fijo\" [fixed point]. En el algoritmo EM es un poco m\u00e1s complicado, dado que el algoritmo tiende a converger pero nunca se llega a ning\u00fan punto fijo. Sin embargo, se puede ver cu\u00e1nto se acerca calculando la verosimilitud [likelihood] general de los datos con esos par\u00e1metros, multiplicando las probabilidades de los ejemplos, tal y como se muestra en la ecuaci\u00f3n 2.10. () =N 1icluster ji jsjxPp | Ec. 2.10 En esta ecuaci\u00f3n j representa cada uno de los clusters del sistema, y pj la probabilidad de dicho cluster. La verosimilitud es una medida de lo \"bueno\" que es el clustering, y se incrementa con cada iteraci\u00f3n del algoritmo EM. Se seguir\u00e1 iterando hasta que dicha medida se incremente un valor despreciable. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 106 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Aunque EM garantiza la convergencia, \u00e9sta puede ser a un m\u00e1ximo local, por lo que se recomienda repetir el proceso varias veces, con diferentes par\u00e1metros iniciales para las distribuciones. Tras estas repeticiones, se pueden comparar las medidas de verosimilitud obtenidas y escoger la mayor de todas ellas. En la figura 2.7 se muestra un ejemplo de clustering probabil\u00edstico con el algoritmo EM. Figura 3.7: Ejemplo de clustering con EM. En este experimento se introducen un total de doscientos ejemplos que constituyen dos distribuciones desconocidas para el algoritmo. Lo \u00fanico que conoce el algoritmo es que hay dos clusters, dado que este dato se introduce como par\u00e1metro de entrada. En la iteraci\u00f3n 0 se inicializan los par\u00e1metros de los clusters a 0 (media, desviaci\u00f3n t\u00edpica y probabilidad). En las siguientes iteraciones estos par\u00e1metros van tomando forma hasta finalizar en la iteraci\u00f3n 11, iteraci\u00f3n en la que finaliza el proceso, por el incremento de la medida de verosimilitud , tan s\u00f3lo del orden de 10 -4. Extensiones al algoritmo EM Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 107 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda El modelo puede extenderse desde un atributo num\u00e9rico como se ha visto hasta el momento, hasta m\u00faltiples atributos, asumiendo independencia entre atributos. Las probabilidades de cada atributo se multiplican entre s\u00ed para obtener una probabilidad conjunta para la instancia, tal y como se hace en el algoritmo naive Bayesiano. Tambi\u00e9n puede haber atributos correlacionados, en cuyo caso se puede modelar con una distribuci\u00f3n normal bivariable, en donde se utiliza una matriz de covarianza. En este caso el n\u00famero de par\u00e1metros crece seg\u00fan el cuadrado del n\u00famero de atributos que se consideren correlacionados entre s\u00ed, ya que se debe construir una matriz de covarianza. Esta escalabilidad en el n\u00famero de par\u00e1metros tiene serias consecuencias de sobreajuste. En el caso de un atributo nominal con v posibles valores, se caracteriza mediante v valores num\u00e9ricos que representan la probabilidad de cada valor. Se necesitar\u00e1n otros kv valores num\u00e9ricos, que ser\u00e1n las probabilidades condicionadas de cada posible valor del atributo con respecto a cada cluster. En cuanto a los valores desconocidos, se puede optar por varias soluciones: ignorarlo en el productorio de probabilidades; a\u00f1adir un nuevo valor a los posibles, s\u00f3lo en el caso de atributos nominales; o tomar la media o la moda del atributo, seg\u00fan se trate de atributos num\u00e9ricos o nominales. Por \u00faltimo, aunque se puede especificar el n\u00famero de clusters, tambi\u00e9n es posible dejar que sea el algoritmo el que determine autom\u00e1ticamente cu\u00e1l es el n\u00famero de clusters mediante validaci\u00f3n cruzada. 3.3. Reglas de Asociaci\u00f3n Este tipo de t\u00e9cnicas se emplea para establecer las posibles relaciones o correlaciones entre distintas acciones o sucesos aparentemente independientes; pudiendo reconocer como la ocurrencia de un suceso o acci\u00f3n puede inducir o generar la aparici\u00f3n de otros [AIS93b]. Son utilizadas cuando el objetivo es realizar an\u00e1lisis exploratorios , buscando relaciones dentro del conjunto de datos. Las asociaciones identificadas pueden usarse para predecir comportamientos, y permiten descubrir correlaciones y co-ocurrencias de eventos [AS94, AS94a, AS94b]. Debido a sus caracter\u00edsticas, estas t\u00e9cnicas tienen una gran aplicaci\u00f3n pr\u00e1ctica en muchos campos como, por ejemplo, el comercial ya que son especialmente interesantes a la hora de comprender los h\u00e1bitos de compra de los clientes y constituyen un pilar b\u00e1sico en la concepci\u00f3n de las ofertas y ventas cruzada, as\u00ed como del \"merchandising\" [RMS98]. En otros entornos como el sanitario, estas herramientas se emplean para identificar factores de riesgo en la aparici\u00f3n o complicaci\u00f3n de enfermedades. Para su utilizaci\u00f3n es necesario disponer de informaci\u00f3n de cada uno de los sucesos llevados a cabo por un mismo individuo o cliente en un determinado per\u00edodo temporal. Por lo general esta forma de extracci\u00f3n de conocimiento se fundamenta en t\u00e9cnicas estad\u00edsticas [CHY96], como los an\u00e1lisis de correlaci\u00f3n y de variaci\u00f3n [BMS97]. Uno de los algoritmos mas utilizado es el algoritmo A priori, que se presenta a continuaci\u00f3n. Algoritmo A Priori La generaci\u00f3n de reglas de asociaci\u00f3n se logra bas\u00e1ndose en un procedimiento de covering . Las reglas de asociaci\u00f3n son parecidas, en su forma, a las reglas de clasificaci\u00f3n, si bien en su lado derecho puede aparecer cualquier par o Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 108 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda pares atributo-valor . De manera que para encontrar ese tipo de reglas es preciso considerar cada posible combinaci\u00f3n de pares atributo-valor del lado derecho. Para evaluar las reglas se emplean la medida del soporte [support], ecuaci\u00f3n 2.11, que indica el n\u00famero de casos, ejemplos, que cubre la regla y la confianza [confidence], ecuaci\u00f3n 2.12, que indica el n\u00famero de casos que predice la regla correctamente, y que viene expresado como el cociente entre el n\u00famero de casos en que se cumple la regla y el n\u00famero de casos en que se aplica, ya que se cumplen las premisas. () ( )BAP B A soporte = ) ()APBAPA|BP B A confianza= = Ec. 2.12 Las reglas que interesan son \u00fanicamente aquellas que tienen su valor de soporte muy alto, por lo que se buscan, independientemente de en qu\u00e9 lado aparezcan, pares atributo-valor que cubran una gran cantidad de ejemplos. A cada par atributo-valor se le denomina item, mientras que a un conjunto de items se les denomina item-sets . Por supuesto, para la formaci\u00f3n de item-sets no se pueden unir items referidos al mismo atributo pero con distinto valor, dado que eso nunca se podr\u00eda producir en un ejemplo. Se buscan item-sets con un m\u00e1ximo soporte, para lo que se comienza con item-sets con un \u00fanico item. Se eliminan los item-sets cuyo valor de soporte sea inferior al m\u00ednimo establecido, y se combinan el resto formando item-sets con dos items . A su vez se eliminan aquellos nuevos item-sets que no cumplan con la condici\u00f3n del soporte, y al resto se le a\u00f1adir\u00e1 un nuevo item, formando item-sets con tres items . El proceso continuar\u00e1 hasta que ya no se puedan formar item-sets con un item m\u00e1s. Adem\u00e1s, para generar los item-sets de un determinado nivel, s\u00f3lo es necesario emplear los item-sets del nivel inferior (con n-1 coincidencias, siendo n el n\u00famero de items del nivel). Una vez se han obtenido todos los item-sets , se pasar\u00e1 a la generaci\u00f3n de reglas. Se tomar\u00e1 cada item-set y se formar\u00e1n reglas que cumplan con la condici\u00f3n de confianza . Debe tenerse en cuenta que un item-set puede dar lugar a m\u00e1s de una regla de asociaci\u00f3n, al igual que un item-set tambi\u00e9n puede no dar lugar a ninguna regla. Un ejemplo t\u00edpico de reglas de asociaci\u00f3n es el an\u00e1lisis de la cesta de la compra [market-basket analysis]. B\u00e1sicamente consiste en encontrar asociaciones entre los productos que habitualmente compran los clientes, para utilizarlas en el desarrollo de las estrategias mercadot\u00e9cnicas. En la figura 2.8 se muestra un ejemplo sencillo de obtenci\u00f3n de reglas de asociaci\u00f3n aplicado a este campo. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 109 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.8: Ejemplo de obtenci\u00f3n de reglas de asociaci\u00f3n A Priori. En esta imagen se muestra c\u00f3mo se forman los item-sets a partir de los item- sets del nivel inferior, y c\u00f3mo posteriormente se obtienen las reglas de asociaci\u00f3n a partir de los item-sets seleccionados. Las reglas en negrita son las que se obtendr\u00edan, dado que cumplen con la confianza m\u00ednima requerida. El proceso de obtenci\u00f3n de las reglas de asociaci\u00f3n que se coment\u00f3 anteriormente se basa en el algoritmo que se muestran en la figura 2.9 (A priori, Agrawal et al. 94). 1. Genera todos los items-sets con un elemento. Usa \u00e9stos para generar los de dos elementos y as\u00ed sucesivamente. Se toman todos los posibles pares que cumplen con las medidas m\u00ednimas del soporte. Esto permite ir eliminando posibles combinaciones ya que no todas se tienen que considerar. 2. Genera las reglas revisando que cumplan con el criterio m\u00ednimo de confianza. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 110 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.9: Algoritmo de obtenci\u00f3n de reglas de asociaci\u00f3n A Priori. Una observaci\u00f3n interesante es que si una conjunci\u00f3n de consecuentes de una regla cumple con los niveles m\u00ednimos de soporte y confianza, sus subconjuntos (consecuentes) tambi\u00e9n los cumplen. Por el contrario, si alg\u00fan item no los cumple, no tiene caso considerar sus superconjuntos. Esto da una forma de ir construyendo reglas, con un solo consecuente, y a partir de ellas construir de dos consecuentes y as\u00ed sucesivamente. 3.4. La predicci\u00f3n Es el proceso que intenta determinar los valores de una o varias variables, a partir de un conjunto de datos. La predicci\u00f3n de valores continuos puede planificarse por las t\u00e9cnicas estad\u00edsticas de regresi\u00f3n [JAM85, DEV95, AGR96]. Por ejemplo, para predecir el sueldo de un graduado de la universidad con 10 a\u00f1os de experiencia de trabajo, o las ventas potenciales de un nuevo producto dado su precio. Se pueden resolver muchos problemas por medio de la regresi\u00f3n lineal, y puede conseguirse todav\u00eda m\u00e1s aplicando las transformaciones a las variables para que un problema no lineal pueda convertirse a uno lineal. A continuaci\u00f3n se presenta una introducci\u00f3n intuitiva de las ideas de regresi\u00f3n lineal, m\u00faltiple, y no lineal, as\u00ed como la generalizaci\u00f3n a los modelos lineales. M\u00e1s adelante, dentro de la clasificaci\u00f3n, se estudiar\u00e1n varias t\u00e9cnicas de data mining que pueden servir para predicci\u00f3n num\u00e9rica. De entre todas ellas las m\u00e1s importantes se presentaran en la clasificaci\u00f3n bayesiana, la basada en ejemplares y las redes de neuronas. A continuaci\u00f3n se mostrar\u00e1n un conjunto de t\u00e9cnicas que espec\u00edficamente sirven para la predicci\u00f3n. 3.4.1. Regresi\u00f3n no lineal. En muchas ocasiones los datos no muestran una dependencia lineal [FRI91]. Esto es lo que sucede si, por ejemplo, la variable respuesta depende de las variables independientes seg\u00fan una funci\u00f3n polin\u00f3mica, dando lugar a una regresi\u00f3n polin\u00f3mica que puede planearse agregando las condiciones polin\u00f3micas al modelo lineal b\u00e1sico. De est\u00e1 forma y aplicando ciertas transformaciones a las variables, se puede convertir el modelo no lineal en uno lineal que puede resolverse entonces por el m\u00e9todo de m\u00ednimos cuadrados. Por ejemplo consid\u00e9rese una relaci\u00f3n polin\u00f3mica c\u00fabica dada por: y = a + b 1x + b 2 x2 + b 3 x3. Ec. 2.25 Para convertir esta ecuaci\u00f3n a la forma lineal, se definen las nuevas variables: x1= x x 2 = x2 x 3 =x3 Ec. 2.26 Con lo que la ecuaci\u00f3n anterior puede convertirse entonces a la forma lineal aplicando los cambios de variables, y resultando la ecuaci\u00f3n 2.27, que es resoluble por el m\u00e9todo de m\u00ednimos cuadrados Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 111 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda y = a + b 1 x1 + b 2 x 2 + b 3 x 3 Ec. 2.27 . No obstante, algunos modelos son especialmente no lineales como, por ejemplo, la suma de t\u00e9rminos exponenciales y no pueden convertirse a un modelo lineal. Para estos casos, puede ser posible obtener las estimaciones del m\u00ednimo cuadrado a trav\u00e9s de c\u00e1lculos extensos en formulas m\u00e1s complejas. Los modelos lineales generalizados representan el fundamento te\u00f3rico en que la regresi\u00f3n lineal puede aplicarse para modelar las categor\u00edas de las variables dependientes. En los modelos lineales generalizados, la variaci\u00f3n de la variable y es una funci\u00f3n del valor medio de y, distinto a la regresi\u00f3n lineal d\u00f3nde la variaci\u00f3n de y es constante. Los tipos comunes de modelos lineales generalizados incluyen regresi\u00f3n log\u00edstica y regresi\u00f3n del Poisson. La regresi\u00f3n log\u00edstica modela la probabilidad de alg\u00fan evento que ocurre como una funci\u00f3n lineal de un conjunto de variables independientes. Frecuentemente los datos exhiben una distribuci\u00f3n de Poisson y se modelan normalmente usando la regresi\u00f3n del Poisson. Los modelos lineales logar\u00edtmicos [PEA 88] aproximan las distribuciones de probabilidad multidimensionales discretas, y pueden usarse para estimar el valor de probabilidad asociado con los datos de las c\u00e9lulas c\u00fabicas. Por ejemplo, suponiendo que se tienen los datos para los atributos ciudad, art\u00edculo, a\u00f1o, y ventas. En el m\u00e9todo logar\u00edtmico lineal, todos los atributos deben ser categor\u00edas; por lo que los atributos estimados continuos (como las ventas) deben ser previamente discretizados. 3.4.2. \u00c1rboles de Predicci\u00f3n Los \u00e1rboles de predicci\u00f3n num\u00e9rica son similares a los \u00e1rboles de decisi\u00f3n, que se estudiar\u00e1n m\u00e1s adelante, excepto en que la clase a predecir es continua. En este caso, cada nodo hoja almacena un valor de clase consistente en la media de las instancias que se clasifican con esa hoja, en cuyo caso estamos hablando de un \u00e1rbol de regresi\u00f3n , o bien un modelo lineal que predice el valor de la clase, en cuyo caso se habla de \u00e1rbol de modelos . En el caso del algoritmo M5 [WF00], se trata de obtener un \u00e1rbol de modelos, si bien se puede utilizar para obtener un \u00e1rbol de regresi\u00f3n, por ser \u00e9ste un caso espec\u00edfico de \u00e1rbol de modelos. Mientras que en el caso de los \u00e1rboles de decisi\u00f3n se emplea la entrop\u00eda de clases para definir el atributo con el que dividir, en el caso de la predicci\u00f3n num\u00e9rica se emplea la varianza del error en cada hoja. Una vez construido el \u00e1rbol que clasifica las instancias se realiza la poda del mismo, tras lo cual, se obtiene para cada nodo hoja una constante en el caso de los \u00e1rboles de regresi\u00f3n o un plano de regresi\u00f3n en el caso de \u00e1rboles de modelos. En \u00e9ste \u00faltimo caso, los atributos que formar\u00e1n parte de la regresi\u00f3n ser\u00e1n aquellos que participaban en el sub\u00e1rbol que ha sido podado. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 112 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Al construir un \u00e1rbol de modelos y definir, para cada hoja, un modelo lineal con los atributos del sub\u00e1rbol podado suele ser beneficioso, sobre todo cuando se tiene un peque\u00f1o conjunto de entrenamiento, realizar un proceso de suavizado [smoothing] que compense las discontinuidades que ocurren entre modelos lineales adyacentes. Este proceso consiste en: cuando se predice el valor de una instancia de test con el modelo lineal del nodo hoja correspondiente, este valor obtenido se filtra hacia atr\u00e1s hasta el nodo hoja, suavizando dicho valor al combinarlo con el modelo lineal de cada nodo interior por el que pasa. Un modelo que se suele utilizar es el que se muestra en la ecuaci\u00f3n 2.28. knkq npp'++= Ec. 2.28 En esta ecuaci\u00f3n, p es la predicci\u00f3n que llega al nodo (desde abajo), p' es la predicci\u00f3n filtrada hacia el nivel superior, q el valor obtenido por el modelo lineal de este nodo, n es el n\u00famero de ejemplos que alcanzan el nodo inferior y k el factor de suavizado. Para construir el \u00e1rbol se emplea como heur\u00edstica el minimizar la variaci\u00f3n interna de los valores de la clase dentro de cada subconjunto. Se trata de seleccionar aquel atributo que maximice la reducci\u00f3n de la desviaci\u00f3n est\u00e1ndar de error (SDR, [standard deviation reduction]) con la f\u00f3rmula que se muestra la ecuaci\u00f3n 2.29. = iii) SD(EEESD(E) SDR Ec. 2.29 En esta ecuaci\u00f3n E es el conjunto de ejemplos en el nodo a dividir, Ej es cada uno de los conjuntos de ejemplos que resultan en la divisi\u00f3n en el nodo seg\u00fan el atributo considerado, |E| es el n\u00famero de ejemplos del conjunto E y SD(E) la desviaci\u00f3n t\u00edpica de los valores de la clase en E. El proceso de divisi\u00f3n puede finalizar porque la desviaci\u00f3n t\u00edpica es una peque\u00f1a fracci\u00f3n (por ejemplo, el 5%) de la desviaci\u00f3n t\u00edpica del conjunto original de instancias o porque hay pocas instancias (por ejemplo, 2). En la figura 2.12 se muestra un ejemplo de generaci\u00f3n del \u00e1rbol de predicci\u00f3n con el algoritmo M5. Para ello se muestra en primer lugar los ejemplos de entrenamiento, en los que se trata de predecir los puntos que un jugador de baloncesto anotar\u00eda en un partido. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 113 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.10: Ejemplo de generaci\u00f3n del \u00e1rbol de predicci\u00f3n con M5. En cada nodo del \u00e1rbol se muestra la desviaci\u00f3n t\u00edpica de los ejemplos de entrenamiento que inciden en el nodo ( SD(E) ) y la desviaci\u00f3n est\u00e1ndar del error para el atributo y el punto de corte que lo maximiza, por lo que es el seleccionado. Para obtener el atributo y el punto de corte se debe calcular la desviaci\u00f3n est\u00e1ndar del error para cada posible punto de corte. En este caso, la finalizaci\u00f3n de la construcci\u00f3n del \u00e1rbol ocurre porque no se puede seguir subdividiendo, ya que en cada hoja hay dos ejemplos (n\u00famero m\u00ednimo permitido). Por \u00faltimo, tras generar el \u00e1rbol, en cada hoja se a\u00f1ade la media de los valores de la clase de los ejemplos que se clasifican a trav\u00e9s de dicha hoja. Una vez se ha construido el \u00e1rbol se va definiendo, para cada nodo interior (no para las hojas para emplear el proceso de suavizado ) un modelo lineal, concretamente una regresi\u00f3n lineal m\u00faltiple, tal y como se mostr\u00f3 anteriormente. \u00danicamente se emplean para realizar esta regresi\u00f3n aquellos atributos que se utilizan en el sub\u00e1rbol del nodo en cuesti\u00f3n. A continuaci\u00f3n se pasa al proceso de poda, en el que se estima, para cada nodo, el error esperado en el conjunto de test. Para ello, lo primero que se hace es calcular la desviaci\u00f3n de las predicciones del nodo con los valores reales de la clase para los ejemplos de entrenamiento que se clasifican por el mismo nodo. Sin embargo, dado que el \u00e1rbol se ha construido con estos ejemplos, el error puede infravalorarse, con lo que se compensa con el factor v)(nv)(n + , donde n es el n\u00famero de ejemplos de entrenamiento que se clasifican por el nodo actual y v es el n\u00famero de par\u00e1metros del modelo lineal. De esta forma, la estimaci\u00f3n del error en un conjunto I de ejemplos se realizar\u00eda con la ecuaci\u00f3n 2.30. ny-y v-nvnMAEv-nvne(I)Iii i \u00d7+= \u00d7+= Ec. 2.30 En la ecuaci\u00f3n 2.30, MAE es el error medio absoluto [mean absolute error] del modelo, donde yi es el valor de la clase para el ejemplo i y iy la predicci\u00f3n del modelo para el mismo ejemplo. Para podar el \u00e1rbol, se comienza por las hojas del mismo y se va comparando el error estimado para el nodo con el error estimado para los hijos del mismo, para lo cu\u00e1l se emplea la ecuaci\u00f3n 2.31. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 114 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda nde(d)|i|e(i)) e(sub\u00e1rbol+= Ec. 2.31 En la ecuaci\u00f3n 2.31, e(i) y e(d) son los errores estimados para los nodos hijo izquierdo y derecho, |x| el n\u00famero de ejemplos que se clasifica por el nodo x y n el n\u00famero de ejemplos que se clasifica por el nodo padre. Comparando el error estimado para el nodo con el error estimado para el sub\u00e1rbol, se decide podar si no es menor el error para el sub\u00e1rbol. El proceso explicado hasta el momento sirve para el caso de que los atributos sean num\u00e9ricos pero, si los atributos son nomi nales ser\u00e1 preciso modificar el proceso: en primer lugar, se calcula el promedio de la clase en los ejemplos de entrenamiento para cada posible valor del atributo nominal, y se ordenan dichos valores de acuerdo a este promedio. Entonces, un atributo nominal con k posibles valores se transforma en k-1 atributos binarios. El i-\u00e9simo atributo binario tendr\u00e1, para un ejemplo dado, un 0 si el valor del atributo nominal es uno de los primeros i valores del orden establecido y un 1 en caso contrario. Con este proceso se logra tratar los atributos nominales como num\u00e9ricos. Tambi\u00e9n es necesario determinar c\u00f3mo se actuar\u00e1 frente a los atributos para los que faltan valores. En este caso, se modifica ligeramente la ecuaci\u00f3n 2.29 para llegar hasta la ecuaci\u00f3n 2.32. = iii) SD(EEESD(E)|E|cSDR Ec. 2.32 En esta ecuaci\u00f3n c es el n\u00famero de ejemplos con el atributo conocido. Una vez explicadas las caracter\u00edsticas de los \u00e1rboles de predicci\u00f3n num\u00e9rica, se pasa a mostrar el algoritmo M5, cuyo pseudoc\u00f3digo se recoge en la figura 2.13. M5 (ejemplos) { SD = sd(ejemplos) Para cada atributo nominal con k-valores convertir en k-1 atributos binarios ra\u00edz = nuevo nodo ra\u00edz.ejemplos = ejemplos Dividir(ra\u00edz) Podar(ra\u00edz) Dibujar(ra\u00edz) } Dividir(nodo) { Si O sd(nodo.ejemplos)<=0.05*SD Entonces nodo.tipo = HOJA Si no nodo.tipo = INTERIOR Para cada atributo Para cada posible punto de divisi\u00f3n del atributo calcular el SDR del atributo nodo.atributo = atributo con mayor SDR Dividir(nodo.izquierda) Dividir(nodo.derecha) } Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 115 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Podar(nodo) { Si nodo = INTERIOR Podar(nodo.hijoizquierdo) Podar(nodo.hijoderecho) nodo.modelo = RegresionLinear(nodo) Si ErrorSubarbol(nodo) > Error(nodo) = HOJA } ErrorSubarbol(nodo) { l = nodo.izquierda r = nodo.derecha Si nodo = INTERIOR Entonces ErrorSubarbol = (tama\u00f1o(l.ejemplos)*ErrorSubarbol(l) + tama\u00f1o(r.ejemplos)*ErrorSubarbol(r))tama\u00f1o(nodo.ejemplos) Si no ErrorSubarbol = error(nodo) } Figura 3.11: Pseudoc\u00f3digo del algoritmo M5. La funci\u00f3n RegresionLinear generar\u00e1 la regresi\u00f3n correspondiente al nodo en el que nos encontramos. La funci\u00f3n error evaluar\u00e1 el error del nodo mediante la ecuaci\u00f3n 2.31. 3.4.3. Estimador de N\u00facleos Los estimadores n\u00facleo [kernel density] son estimadores no param\u00e9tricos. De entre los que destaca el conocido histograma, por ser uno de los m\u00e1s antiguos y m\u00e1s utilizado, que tiene ciertas deficiencias relacionadas con la continuidad que llevaron a desarrollar otras t\u00e9cnicas. El estimador de n\u00facleos fue propuesto por Rosenblatt en 1956 y Parzen en 1962 [DFL96]. La idea en la que se basan los estimadores de densidad de n\u00facleo es la siguiente. Si X es una variable aleatoria con funci\u00f3n de distribuci\u00f3n F y densidad f, entonces en cada punto de continuidad x de f se confirma la ecuaci\u00f3n 2.33. () ()() hxFhxF2h1lim f(x)0h + = Ec. 2.33 Dada una muestra X1,...,X n proveniente de la distribuci\u00f3n F, para cada h fijo, F(x+h)-F(x-h) se puede estimar por la proporci\u00f3n de observaciones que est\u00e1n dentro del intervalo (x-h, x+h) . Por lo tanto, tomando h peque\u00f1o, un estimador natural de la densidad es el que se muestra en la ecuaci\u00f3n 2.34, donde #A es el n\u00famero de elementos del conjunto A. () {} hx h,-x XX2hn1(x)fi i hn, + = : # Ec. 2.34 Otra manera de expresar este estimador es considerando la funci\u00f3n de peso w definida como se muestra en la ecuaci\u00f3n 2.35, de manera que el estimador de la densidad f en el punto x se puede expresar como se expresa en la ecuaci\u00f3n 2.36. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 116 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Ec. 2.35 = = hX-xwh1 n1(x)fin 1ihn, Ec. 2.36 Pero este estimador no es una funci\u00f3n continua, ya que tiene saltos en los puntos Xi\u00b1h y su derivada es 0 en todos los otros puntos. Por ello se ha sugerido reemplazar a la funci\u00f3n w por funciones m\u00e1s suaves K, llamadas n\u00facleos, lo que da origen a los estimadores de n\u00facleos. El estimador de n\u00facleos de una funci\u00f3n de densidad f calculado a partir de una muestra aleatoria X1,...,X n de dicha densidad se define seg\u00fan la ecuaci\u00f3n 2.37. = = hX-xKnh1(x)fin 1ihn, Ec. 2.37 En la ecuaci\u00f3n 2.37, la funci\u00f3n K se elige generalmente entre las funciones de densidad conocidas, por ejemplo gaussiana, que se muestra en la ecuaci\u00f3n 2.38, donde es la desviaci\u00f3n t\u00edpica de la distribuci\u00f3n y \u00b5 la media. () 22 2\u00b5x e 21f(x) = Ec. 2.38 El otro par\u00e1metro de la ecuaci\u00f3n 2.37 es h, llamado ventana, par\u00e1metro de suavizado o ancho de banda, el cual determina las propiedades estad\u00edsticas del estimador: el sesgo crece y la varianza decrece con h [HALI94]. Es decir que si h es grande, los estimadores est\u00e1n sobresuavizados y son sesgados, y si h es peque\u00f1o, los estimadores resultantes est\u00e1n subsuavizados, lo que equivale a decir que su varianza es grande. Figura 3.12: Importancia del par\u00e1metro \"tama\u00f1o de ventana\" en el estimador de n\u00facleos. A pesar de que la elecci\u00f3n del n\u00facleo K determina la forma de la densidad estimada, la literatura sugiere que esta elecci\u00f3n no es cr\u00edtica, al menos entre las alternativas usuales [DEA97]. M\u00e1s importante es la elecci\u00f3n del tama\u00f1o de ventana. En la figura 2.14 se muestra c\u00f3mo un valor peque\u00f1o para este factor hace que la Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 117 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda funci\u00f3n de distribuci\u00f3n generada est\u00e9 subsuavizada. Mientras, al emplear un h demasiado grande provoca el sobresuavizado de la funci\u00f3n de distribuci\u00f3n. Por \u00faltimo, empleando el h \u00f3ptimo se obtiene la funci\u00f3n de distribuci\u00f3n adecuada. Para determinar un ancho de banda con el cual comenzar, una alternativa es calcular el ancho de banda \u00f3ptimo si se supone que la densidad tiene una forma espec\u00edfica. La ventana \u00f3ptima en el sentido de minimizar el error medio cuadr\u00e1tico integrado, definido como la esperanza de la integral del error cuadr\u00e1tico sobre toda la densidad, fue calculada por Bowman [BOW85], y Silverman [SIL86] y depende de la verdadera densidad f y del n\u00facleo K. Al suponer que ambos, la densidad y el n\u00facleo son normales, la ventana \u00f3ptima resulta ser la que se muestra en la ecuaci\u00f3n 2.39. -1/5n 1.06 h* = Ec. 2.39 En la ecuaci\u00f3n 2.39 es la desviaci\u00f3n t\u00edpica de la densidad. La utilizaci\u00f3n de esta h ser\u00e1 adecuada si la poblaci\u00f3n se asemeja en su distribuci\u00f3n a la de la normal; sin embargo si trabajamos con poblaciones multimodales se producir\u00e1 una sobresuavizaci\u00f3n de la estimaci\u00f3n. Por ello el mismo autor sugiere utilizar medidas robustas de dispersi\u00f3n en lugar de , con lo cual el ancho de banda \u00f3ptimo se obtiene como se muestra en la ecuaci\u00f3n 2.40. ( )-1/5n IQR 0.75 , min 1.06 h*= Ec. 2.40 En la ecuaci\u00f3n 2.40 IQR es el rango intercuart\u00edlico, esto es, la diferencia entre los percentiles 75 y 25 [DEA97]. Una vez definidos todos los par\u00e1metros a tener en cuenta para emplear un estimador de n\u00facleos, hay que definir c\u00f3mo se obtiene, a partir del mismo, el valor de la variable a predecir, y, en funci\u00f3n del valor de la variable dependiente, x. Esto se realiza mediante el estimador de Nadaray a-Watson, que se muestra en 2.41 En la ecuaci\u00f3n 2.41 x es el valor del atributo dependiente a partir del cual se debe obtener el valor de la variable independiente y; Yi es el valor del atributo independiente para el ejemplo de entrenamiento i. Una vez completada la explicaci\u00f3n de c\u00f3mo aplicar los estimadores de n\u00facleos para predecir el valor de una clase num\u00e9rica, se muestra, en la figura 2.15, un ejemplo de su utilizaci\u00f3n basado en los ejemplos de la tabla 2.1 (apartado 2.5), tomando la variable temperatura como predictora y la variable humedad como dependiente o a predecir. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 118 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.13: Ejemplo de predicci\u00f3n con un estimador de n\u00facleos. En primer lugar se definen los par\u00e1metros que se van a emplear para el estimador de n\u00facleos: la funci\u00f3n n\u00facleo y el par\u00e1metro de suavizado. Posteriormente se puede realizar la predicci\u00f3n, que en este caso consiste en predecir el valor del atributo humedad sabiendo que la temperatura es igual a 77. Despu\u00e9s de completar el proceso se determina que el valor de la humedad es igual a 82.97 . Aplicaci\u00f3n a problemas multivariantes Hasta el momento se han explicado las bases sobre las que se sustentan los estimadores de n\u00facleos, pero en los problemas reales no es una \u00fanica variable la que debe tratarse, sino que han de tenerse en cuenta un n\u00famero indeterminado de variables. Por ello, es necesario ampliar el modelo explicado para permitir la introducci\u00f3n de d variables. As\u00ed, supongamos n ejemplos X i, siendo Xi un vector d- dimensional. El estimador de n\u00facleos de la funci\u00f3n de densidad f calculado a partir de la muestra aleatoria X1,...,X n de dicha densidad se define como se muestra en la ecuaci\u00f3n 2.42. ()()i1n 1iHn, X-x HKHn1(x)f = = Ec. 2.42 Tal y como puede verse, la ecuaci\u00f3n 2.42 es una mera ampliaci\u00f3n de la ecuaci\u00f3n 2.37: en este caso H no es ya un \u00fanico valor num\u00e9rico, sino una matriz sim\u00e9trica y definida positiva de orden dd\u00d7, denominada matriz de anchos de ventana. Por su parte K es generalmente una funci\u00f3n de densidad multivariante. Por ejemplo, la funci\u00f3n gaussiana normalizada en este caso pasar\u00eda a ser la que se muestra en la ecuaci\u00f3n 2.43. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 119 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ()2xx dT e 21f(x)= 2 Ec. 2.43 De nuevo, es m\u00e1s importante definir una correcta matriz H que la funci\u00f3n n\u00facleo elegida. Tambi\u00e9n el estimador de Nadaraya-Watson, que se muestra en la ecuaci\u00f3n 2.44, es una ampliaci\u00f3n del = == = Ec. 2.44 Tal y como se ve en la ecuaci\u00f3n 2.44, el cambio radica en que se tiene una matriz de anchos de ventana en lugar de un \u00fanico valor de ancho de ventana. Aplicaci\u00f3n a problemas de clasificaci\u00f3n Si bien los estimadores de n\u00facleo son dise\u00f1ados para la predicci\u00f3n num\u00e9rica, tambi\u00e9n pueden utilizarse para la clasificaci\u00f3n. En este caso, se dispone de un conjunto de c clases a las que puede pertenecer un ejemplo determinado. Y estos ejemplos se componen de d variables o atributos. Se puede estimar la densidad de la clase j mediante la ecuaci\u00f3n 2.45, en la que n j es el n\u00famero de ejemplos de entrenamiento que pertenecen a la clase j, Yij ser\u00e1 1 en caso de que el ejemplo i pertenezca a la clase j y 0 en otro caso, K vuelve a ser la funci\u00f3n n\u00facleo y h el ancho de ventana. En este caso se ha realizado la simplificaci\u00f3n del modelo multivariante, empleando en lugar de una matriz de anchos de ventana un \u00fanico valor escalar porque es el modelo que se utiliza en la implementaci\u00f3n que realiza WEKA de los estimadores de n\u00facleo. = = hX-xKhYn1(x)fin 1id-j i jj Ec. 2.45 La probabilidad a priori de que un ejemplo pertenezca a la clase j es igual a nn Pj j= . Se puede estimar la probabilidad a posteriori , definida mediante qj(x), de que el ejemplo pertenezca a j, tal y como se muestra en la ecuaci\u00f3n 2.46. (x)q hXxKhhXxKhY c 1kkkjj jj j = = = = = = Ec. 2.46 De esta forma, el estimador en este caso es id\u00e9ntico al estimador de Nadayara-Watson representado en las ecuaciones 2.41 y 2.44. Por \u00faltimo, se muestra un ejemplo de la aplicaci\u00f3n de un estimador de n\u00facleos a un problema de clasificaci\u00f3n: se trata del problema planteado en la tabla 2.1 (apartado 2.5), y m\u00e1s concretamente se trata de predecir el valor de la clase jugar a partir \u00fanicamente del atributo num\u00e9rico temperatura . Este ejemplo se muestra en la figura 2.16. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 120 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.14: Ejemplo de clasificaci\u00f3n mediante un estimador de n\u00facleos. Al igual que para el problema de predi cci\u00f3n, en primer lugar se definen los par\u00e1metros del estimador de n\u00facleos para, posteriormente, estimar la clase a la que pertenece el ejemplo de test. En este caso se trata de predecir si se puede jugar o no al tenis teniendo en cuenta que la temperatura es igual a 77. Y la conclusi\u00f3n a la que se llega utilizando el estimador de n\u00facleos es que s\u00ed se puede jugar. 3.5. La clasificaci\u00f3n La clasificaci\u00f3n es el proceso de dividir un conjunto de datos en grupos mutuamente excluyentes [WK91, LAN96, MIT97], de tal forma que cada miembro de un grupo est\u00e9 lo mas cerca posible de otros y grupos diferentes est\u00e9n lo m\u00e1s lejos posible de otros, donde la distancia se mide con respecto a las variables especificadas, que se quieren predecir. Tabla2.1. Ejemplo de problema de clasificaci\u00f3n. Ejemplo Vista Temperatura Humedad Viento Jugar 1 Soleado Alta (85) Alta (85) No No 2 Soleado Alta (80) Alta (90) S\u00ed No 3 Nublado Alta (83) Alta (86) No S\u00ed 4 Lluvioso Media (70) Alta (96) No S\u00ed Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 121 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 5 Lluvioso Baja (68) Normal (80) No S\u00ed 6 Lluvioso Baja (65) Normal (70) S\u00ed No 7 Nublado Baja (64) Normal (65) S\u00ed S\u00ed 8 Soleado Media (72) Alta (95) No No 9 Soleado Baja (69) Normal (70) No S\u00ed 10 Lluvioso Media (75) Normal (80) No S\u00ed 11 Soleado Media (75) Normal (70) S\u00ed S\u00ed 12 Nublado Media (72) Alta (90) S\u00ed S\u00ed 13 Nublado Alta (81) Normal (75) No S\u00ed 14 Lluvioso Media (71) Alta (91) S\u00ed No El ejemplo empleado tiene dos atributos, temperatura y humedad , que pueden emplearse como simb\u00f3licos o num\u00e9ricos. Entre par\u00e9ntesis se presentan sus valores num\u00e9ricos. En los siguientes apartados se presentan y explican las principales t\u00e9cnicas de clasificaci\u00f3n. Adem\u00e1s, se mostrar\u00e1n ejemplos que permiten observar el funcionamiento del algoritmo, para lo que se utilizar\u00e1 la tabla 2.1, que presenta un sencillo problema de clasificaci\u00f3n consistente en, a partir de los atributos que modelan el tiempo (vista, temperatura, humedad y viento), determinar si se puede o no jugar al tenis. 3.5.1. Tabla de Decisi\u00f3n La tabla de decisi\u00f3n constituye la forma m\u00e1s simple y rudimentaria de representar la salida de un algoritmo de aprendizaje, que es justamente representarlo como la entrada. Estos algoritmos consisten en seleccionar subconjuntos de atributos y calcular su precisi\u00f3n [accuracy] para predecir o clasificar los ejemplos. Una vez seleccionado el mejor de los subconjuntos, la tabla de decisi\u00f3n estar\u00e1 formada por los atributos seleccionados (m\u00e1s la clase), en la que se insertar\u00e1n todos los ejemplos de entrenamiento \u00fanicamente con el subconjunto de atributos elegido. Si hay dos ejemplos con exactamente los mismos pares atributo-valor para todos los atributos del subconjunto, la clase que se elija ser\u00e1 la media de los ejemplos (en el caso de una clase num\u00e9rica) o la que mayor probabilidad de aparici\u00f3n tenga (en el caso de una clase simb\u00f3lica). Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 122 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda La precisi\u00f3n de un subconjunto S de atributos para todos los ejemplos de entrenamientos se calcular\u00e1 mediante la ecuaci\u00f3n 2.47 para el caso de que la clase sea simb\u00f3lica o mediante la ecuaci\u00f3n 2.48 en el caso de que la clase sea num\u00e9rica: totales ejemplosos clasificad bien ejemplosS) precisi\u00f3n( = Ec. 2.47 RMSE S) precisi\u00f3n(Ii2 i i = = Ec. 2.48 Donde, en la ecuaci\u00f3n 2.48, RMSE es la del error cuadr\u00e1tico medio [root mean squared error], n es el n\u00famero de ejemplos totales, yi el valor de la clase para el ejemplo i y iy el valor predicho por el modelo para el ejemplo i. Como ejemplo de tabla de decisi\u00f3n, simplemente se puede utilizar la propia tabla 2.1, dado que si se comenzase a combinar atributos y a probar la precisi\u00f3n de dicha combinaci\u00f3n, se obtendr\u00eda como resultado que los cuatro atributos deben emplearse, con lo que la tabla de salida ser\u00eda la misma. Esto no tiene por qu\u00e9 ser as\u00ed, ya que en otros problemas no ser\u00e1n necesarios todos los atributos para generar la tabla de decisi\u00f3n, como ocurre en el ejemplo de la tabla 2.2 donde se dispone de un conjunto de entrenamiento en el que aparecen los atributos sexo, y tipo (tipo de profesor) y la clase a determinar es si el tipo de contrato es o no fijo. Tabla2.2. Determinaci\u00f3n del tipo de contrato. Atributos Clase Ejemplo N\u00ba Sexo Tipo Fijo 1 Hombre Asociado No 2 Mujer Catedr\u00e1tico Si 3 Hombre Titular Si 4 Mujer Asociado No 5 Hombre Catedr\u00e1tico Si 6 Mujer Asociado No 7 Hombre Ayudante No 8 Mujer Titular Si Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 123 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 9 Hombre Asociado No 10 Mujer Ayudante No 11 Hombre Asociado No Si se toma como primer subconjunto el formado por el atributo sexo, y se eliminan las repeticiones resulta la tabla 2.3 Tabla2.3. Subconjunto 1. Ejemplo N\u00ba Sexo Fijo 1 Hombre No 2 Mujer Si 3 Hombre Si 4 Mujer No Con lo que se pone de manifiesto que la probabilidad de clasificar bien es del 50%. Si por el contrario se elimina el atributo Sexo, quedar\u00e1 la tabla 2.4. Tabla2.4. Subconjunto 2. Ejemplo N\u00ba Tipo Fijo 1 Asociado No 2 Catedr\u00e1tico Si 3 Titular Si 7 Ayudante No Que tiene una precisi\u00f3n de aciertos del 100%, por lo que se deduce que \u00e9sta \u00faltima tabla es la que se debe tomar como tabla de decisi\u00f3n. El resultado es l\u00f3gico ya que el atributo sexo es irrelevante a la hora de determinar si el contrato es o no fijo. 3.5.2. \u00c1rboles de Decisi\u00f3n El aprendizaje de \u00e1rboles de decisi\u00f3n est\u00e1 englobado como una metodolog\u00eda del aprendizaje supervisado. La representaci\u00f3n que se utiliza para las descripciones del concepto adquirido es el \u00e1rbol de decisi\u00f3n, que consiste en una representaci\u00f3n del conocimiento relativamente simple y que es una de las causas por la que los procedimientos utilizados en su aprendizaje son m\u00e1s sencillos que los de sistemas que Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 124 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda utilizan lenguajes de representaci\u00f3n m\u00e1s potentes, como redes sem\u00e1nticas, representaciones en l\u00f3gica de primer orden etc. No obstante, la potencia expresiva de los \u00e1rboles de decisi\u00f3n es tambi\u00e9n menor que la de esos otros sistemas. El aprendizaje de \u00e1rboles de decisi\u00f3n suele ser m\u00e1s robusto frente al ruido y conceptualmente sencillo, aunque los sistemas que han resultado del perfeccionamiento y de la evoluci\u00f3n de los m\u00e1s antiguos se complican con los procesos que incorporan para ganar fiabilidad. La mayor\u00eda de los sistemas de aprendizaje de \u00e1rboles suelen ser no incrementales, pero existe alguna excepci\u00f3n [UTG88]. El primer sistema que constru\u00eda \u00e1rboles de decisi\u00f3n fue CLS de Hunt, desarrollado en 1959 y depurado a lo largo de los a\u00f1os sesenta. CLS es un sistema desarrollado por psic\u00f3logos como un modelo del proceso cognitivo de formaci\u00f3n de conceptos sencillos. Su contribuci\u00f3n fundamental fue la propia metodolog\u00eda pero no resultaba computacionalmente eficiente debido al m\u00e9todo que empleaba en la extensi\u00f3n de los nodos. Se guiaba por una estrategia similar al minimax con una funci\u00f3n que integraba diferentes costes. En 1979 Quinlan desarrolla el sistema ID3 [QUIN79], que \u00e9l denominar\u00eda simplemente herramienta porque la consideraba experimental. Conceptualmente es fiel a la metodolog\u00eda de CLS pero le aventaja en el m\u00e9todo de expansi\u00f3n de los nodos, basado en una funci\u00f3n que utiliza la medida de la informaci\u00f3n de Shannon. La versi\u00f3n definitiva, presentada por su autor Quinlan como un sistema de aprendizaje, es el sistema C4.5 que expone con cierto detalle en la obra C4.5: Programs for Machine Learning [QUIN93]. La evoluci\u00f3n -comercial- de ese sistema es otro denominado C5 del mismo autor, del que se puede obtener una versi\u00f3n de demostraci\u00f3n restringida en cuanto a capacidades; por ejemplo, el n\u00famero m\u00e1ximo de ejemplos de entrenamiento. Representaci\u00f3n de un \u00e1rbol de decisi\u00f3n Un \u00e1rbol de decisi\u00f3n [MUR98] puede interpretarse esencialmente como una serie de reglas compactadas para su representaci\u00f3n en forma de \u00e1rbol. Dado un conjunto de ejemplos, estructurados como vectores de pares ordenados atributo-valor, de acuerdo con el formato general en el aprendizaje inductivo a partir de ejemplos, el concepto que estos sistemas adquieren durante el proceso de aprendizaje consiste en un \u00e1rbol. Cada eje est\u00e1 etiquetado con un par atributo-valor y las hojas con una clase, de forma que la trayectoria que determinan desde la ra\u00edz los pares de un ejemplo de entrenamiento alcanzan una hoja etiquetada -nor malmente- con la clase del ejemplo. La clasificaci\u00f3n de un ejemplo nuevo del que se desconoce su clase se hace con la misma t\u00e9cnica, solamente que en ese caso al atributo clase, cuyo valor se desconoce, se le asigna de acuerdo con la etiqueta de la hoja a la que se accede con ese ejemplo. Problemas apropiados para este tipo de aprendizaje Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 125 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Las caracter\u00edsticas de los problemas apropiados para resolver mediante este aprendizaje dependen del sistema de aprendizaje espec\u00edfico utilizado, pero hay una serie de ellas generales y comunes a la mayor\u00eda y que se describen a continuaci\u00f3n: - Que la representaci\u00f3n de los ejempl os sea mediante vectores de pares atributo-valor, especialmente cuando los valores son disjuntos y en un n\u00famero peque\u00f1o. Los sistemas actuales est\u00e1n preparados para tratar atributos con valores continuos, valores desconocidos e incluso valores con una distribuci\u00f3n de probabilidad. - Que el atributo que hace el papel de la clase sea de tipo discreto y con un n\u00famero peque\u00f1o de valores, sin embargo existen sistemas que adquieren como concepto aprendido funciones con valores continuos. - Que las descripciones del concepto adquirido deban ser expresadas en forma normal disyuntiva. - Que posiblemente existan errores de cl asificaci\u00f3n en el conjunto de ejemplos de entrenamiento, as\u00ed como valores desconocidos en algunos de los atributos en algunos ejemplos Estos sistemas, por lo general, son robustos frente a los errores del tipo mencionado. A continuaci\u00f3n se presentan tres algoritmos de \u00e1rboles de decisi\u00f3n, los dos primeros dise\u00f1ados por Quinlan [QUIN86, QUIN93], los sistemas ID3 y C4.5; y el tercero un \u00e1rbol de decisi\u00f3n muy sencillo, con un \u00fanico nivel de decisi\u00f3n. El sistema ID3 El sistema ID3 [QUIN86] es un algoritmo simple y, sin embargo, potente, cuya misi\u00f3n es la elaboraci\u00f3n de un \u00e1rbol de decisi\u00f3n. El procedimiento para generar un \u00e1rbol de decisi\u00f3n consiste, como se com ent\u00f3 anteriormente en seleccionar un atributo como ra\u00edz del \u00e1rbol y crear una rama con cada uno de los posibles valores de dicho atributo. Con cada rama resultante (nuevo nodo del \u00e1rbol), se realiza el mismo proceso, esto es, se selecciona otro atributo y se genera una nueva rama para cada posible valor del atributo. Este procedimiento contin\u00faa hasta que los ejemplos se clasifiquen a trav\u00e9s de uno de los caminos del \u00e1rbol. El nodo final de cada camino ser\u00e1 un nodo hoja, al que se le asignar\u00e1 la clase correspondiente. As\u00ed, el objetivo de los \u00e1rboles de decisi\u00f3n es obtener reglas o relaciones que permitan clasificar a partir de los atributos. En cada nodo del \u00e1rbol de decisi\u00f3n se debe seleccionar un atributo para seguir dividiendo, y el criterio que se toma para elegirlo es: se selecciona el atributo que mejor separe (ordene) los ejemplos de acuerdo a las clases. Para ello se emplea la entrop\u00eda, que es una medida de c\u00f3mo est\u00e1 ordenado el universo. La teor\u00eda de la informaci\u00f3n (basada en la entrop\u00eda) calcula el n\u00famero de bits (informaci\u00f3n, preguntas sobre atributos) que hace falta suministrar para conocer la clase a la que pertenece un ejemplo. Cuanto menor sea el valor de la ent rop\u00eda, menor ser\u00e1 la incertidumbre y m\u00e1s \u00fatil ser\u00e1 el atributo para la clasificaci\u00f3n. La definici\u00f3n de entrop\u00eda que da Shannon en su Teor\u00eda de la Informaci\u00f3n (1948) es: Dado un conjunto de eventos A={A 1, A2,..., A n}, con probabilidades {p1, p2,..., p n}, la informaci\u00f3n en el conocimiento de un suceso Ai Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 126 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda (bits) se define en la ecuaci\u00f3n 2.49, mientras que la informaci\u00f3n media de A (bits) se muestra en la ecuaci\u00f3n 2.50. ()i 2 i2 i p logp1log)I(A = = Ec. 2.49 = 1ii I(A) Ec. 2.50 Si aplicamos la entrop\u00eda a los problemas de clasificaci\u00f3n se puede medir lo que se discrimina (se gana por usar) un atributo Ai empleando para ello la ecuaci\u00f3n 2.51, en la que se define la ganancia de informaci\u00f3n. )I(AI) G(Ai i = Ec. 2.51 Siendo I la informaci\u00f3n antes de utilizar el atributo e I(Ai) la informaci\u00f3n despu\u00e9s de utilizarlo. Se definen ambas en las ecuaciones 2.52 y ijnnlognnI Ec. 2.53 En estas ecuaciones nc ser\u00e1 el n\u00famero de clases y nc el n\u00famero de ejemplares de la clase c, siendo n el n\u00famero total de ejemplos. Ser\u00e1 nv(A i) el n\u00famero de valores del atributo Ai, nij el n\u00famero de ejemplos con el valor j en Ai y nijk el n\u00famero de ejemplos con valor j en Ai y que pertenecen a la clase k. Una vez explicada la heur\u00edstica empleada para seleccionar el mejor atributo en un nodo del \u00e1rbol de decisi\u00f3n, se muestra el algoritmo ID3: 1. Seleccionar el atributo Ai que maximice la ganancia G(Ai). 2. Crear un nodo para ese atributo con tantos sucesores como valores tenga. 3. Introducir los ejemplos en los sucesores seg\u00fan el valor que tenga el atributo Ai. 4. Por cada sucesor: a. Si s\u00f3lo hay ejemplos de una clase, Ck, entonces etiquetarlo con Ck. b. Si no, llamar a ID3 con una tabla formada por los ejemplos de ese nodo, eliminando la columna del atributo Ai. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 127 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.15: Pseudoc\u00f3digo del algoritmo ID3. Por \u00faltimo, en la figura 2.18 se representa el proceso de generaci\u00f3n del \u00e1rbol de decisi\u00f3n para el problema planteado en la tabla 2.1. Figura 3.16: Ejemplo de clasificaci\u00f3n con ID3. En la figura 2.18 se muestra el \u00e1rbol de decisi\u00f3n que se generar\u00eda con el algoritmo ID3. Adem\u00e1s, para el primer nodo del \u00e1rbol se muestra c\u00f3mo se llega a decidir que el mejor atributo para dicho nodo es vista. Se generan nodos para cada valor del atributo y, en el caso de vista = Nublado se llega a un nodo hoja ya que todos los ejemplos de entrenamiento que llegan a dicho nodo son de clase S\u00ed. Sin embargo, para los otros dos casos se repite el proceso de elecci\u00f3n con el resto de atributos y con los ejemplos de entrenamiento que se clasifican a trav\u00e9s de ese nodo. El sistema C4.5 El ID3 es capaz de tratar con atributos cuyos valores sean discretos o continuos. En el primer caso, el \u00e1rbol de decisi\u00f3n generado tendr\u00e1 tantas ramas como valores posibles tome el atributo. Si los valores del atributo son continuos, el ID3 no clasifica correctamente los ejemplos dados. Por ello, Quinlan [QUIN93] propuso el C4.5, como extensi\u00f3n del ID3, que permite: 1. Empleo del concepto raz\u00f3n de ganancia (GR, [Gain Ratio]) 2. Construir \u00e1rboles de decisi\u00f3n cuando algunos de los ejemplos presentan valores desconocidos para algunos de los atributos. 3. Trabajar con atributos que presenten valores continuos. 4. La poda de los \u00e1rboles de decisi\u00f3n [QUIN87, QR89]. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 128 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 5. Obtenci\u00f3n de Reglas de Clasificaci\u00f3n. Raz\u00f3n de Ganancia El test basado en el criterio de maximizar la ganancia tiene como sesgo la elecci\u00f3n de atributos con muchos valores. Esto es debido a que cuanto m\u00e1s fina sea la participaci\u00f3n producida por los valores del atributo, normalmente, la incertidumbre o entrop\u00eda en cada nuevo nodo ser\u00e1 menor, y por lo tanto tambi\u00e9n ser\u00e1 menor la media de la entrop\u00eda a ese nivel. C4.5 modifica el criterio de selecci\u00f3n del atributo empleando en lugar de la ganancia la raz\u00f3n de ganancia , cuya definici\u00f3n se muestra en la ecuaci\u00f3n 2.54. = = =) nv(A 1jij 2iji )A I(Divisi\u00f3n)(AG)(AGR Ec. 2.54 Al t\u00e9rmino I(Divisi\u00f3n A i) se le denomina informaci\u00f3n de ruptura. En esta medida cuando nij tiende a n, el denominador se hace 0. Esto es un problema aunque seg\u00fan Quinlan, la raz\u00f3n de ganancia elimina el sesgo. Valores Desconocidos El sistema C4.5 admite ejemplos c on atributos desconocidos tanto en el proceso de aprendizaje como en el de validaci\u00f3n. Para calcular, durante el proceso de aprendizaje, la raz\u00f3n de ganancia de un atributo con valores desconocidos, se redefinen sus dos t\u00e9rminos, la ganancia, ecuaci\u00f3n 2.55, y la informaci\u00f3n de ruptura, En estas ecuaciones, nic es el n\u00famero de ejemplos con el atributo i conocido, y nid el n\u00famero de ejemplos con valor desconocido en el mismo atributo. Adem\u00e1s, para el c\u00e1lculo de las entrop\u00eda I(Ai) se tendr\u00e1n en cuenta \u00fanicamente los ejemplos en los que el atributo Ai tenga un valor definido. No se toma el valor desconocido como significativo, sino que se supone una distribuci\u00f3n probabil\u00edstica del atributo de acuerdo con los valores de los ejemplos en la muestra de entrenamiento. Cuando se entrena, los casos con valores desconocidos se distribuyen con pesos de acuerdo a la frecuencia de aparici\u00f3n de cada posible valor del atributo en el resto de ejemplos de entrenamiento. El peso ij con que un ejemplo i se distribuir\u00eda desde un nodo etiquetado con el atributo A hacia el hijo con valor j en Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 129 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda dicho atributo se calcula mediante la ecuaci\u00f3n 2.57, en la que i es el peso del ejemplo i al llegar al nodo, esto es, antes de distribuirse, y p(A=j) la suma de pesos de todos los ejemplos del nodo con valor j en el atributo A entre la suma total de pesos de todos los ejemplos del nodo ( ). j) p(AjA i i ij=== = Ec. 2.57 En cuanto a la clasificaci\u00f3n de un ejemplo de test, si se alcanza un nodo con un atributo que el ejemplo no tiene (desconocido), se distribuye el ejemplo (divide) en tantos casos como valores tenga el atributo, y se da un peso a cada resultado con el mismo criterio que en el caso del entrenamiento: la frecuencia de aparici\u00f3n de cada posible valor del atributo en los ejemplos de entrenamiento. El resultado de esta t\u00e9cnica es una clasificaci\u00f3n con probabilidades, correspondientes a la distribuci\u00f3n de ejemplos en cada nodo hoja. Atributos Continuos El tratamiento que realiza C4.5 de los atributos continuos est\u00e1 basado en la ganancia de informaci\u00f3n, al igual que ocurre con los atributos discretos. Si un atributo continuo A i presenta los valores ordenados v1, v2,..., v n, se comprueba cu\u00e1l de los valores z i =(v i + v i+1)/2 ; 1 j < n , supone una ruptura del intervalo [v1, vn] en dos subintervalos [v1, zj] y (zj, vn] con mayor ganancia de informaci\u00f3n. El atributo continuo, ahora con dos \u00fanicos valores posibles, entrar\u00e1 en competencia con el resto de los atributos disponibles para expandir el nodo. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 130 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.17: Ejemplo de tratamiento de atri butos continuos con C4.5. Para mejorar la eficiencia del algoritmo no se consideran todos los posibles puntos de corte, sino que se tienen en cuenta las siguientes reglas: 1. Cada subintervalo debe tener un n\u00famero m\u00ednimo de ejemplos (por ejemplo, 2). 2. No se divide el intervalo si el siguiente ejemplo pertenece a la misma clase que el actual. 3. No se divide el intervalo si el siguiente ejemplo tiene el mismo valor que el actual. 4. Se unen subintervalos adyacentes si tienen la misma clase mayoritaria. Como se ve en el ejemplo de la figura 2.19, aplicando las reglas anteriores s\u00f3lo es preciso probar dos puntos de corte ( 66,5 y 77,5), mientras que si no se empleara ninguna de las mejoras que se comentaron anteriormente se deber\u00edan haber probado un total de trece puntos. Como se ve en la figura 2.19, finalmente se tomar\u00eda como punto de ruptura el 77,5, dado que obtiene una mejor ganancia. Una vez seleccionado el punto de corte, este atributo num\u00e9rico competir\u00eda con el resto de atributos. Si bien aqu\u00ed se ha empleado la ganancia, realmente se emplear\u00eda la raz\u00f3n de ganancia, pero no afecta a la elecci\u00f3n del punto de corte. Cabe mencionar que ese atributo no deja de estar disponible en niveles inferiores como en el caso de los discretos, aunque con sus valores restringidos al intervalo que domina el camino. Poda del \u00e1rbol de decisi\u00f3n El \u00e1rbol de decisi\u00f3n ha sido construido a partir de un conjunto de ejemplos, por tanto, reflejar\u00e1 correctamente todo el grupo de casos. Sin embargo, como esos ejemplos pueden ser muy diferentes entre s\u00ed, el \u00e1rbol resultante puede llegar a ser bastante complejo, con trayectorias largas y muy desiguales. Para facilitar la comprensi\u00f3n del \u00e1rbol puede realizarse una poda del mismo. C4.5 efect\u00faa la poda despu\u00e9s de haber desarrollado el \u00e1rbol completo ( post-poda ), a diferencia de otros sistemas que realizan la construcci\u00f3n del \u00e1rbol y la poda a la vez (pre-poda); es decir, estiman la necesidad de seguir desarrollando un nodo aunque no posea el car\u00e1cter de hoja. En C4.5 el proceso de podado comienza en los nodos hoja y recursivamente contin\u00faa hasta llegar al nodo ra\u00edz. Se consideran dos operaciones de poda en C4.5: reemplazo de sub-\u00e1rbol por elevaci\u00f3n de sub-\u00e1rbol (subtree raising ). En la figura 2.20 se muestra en lo que consiste cada tipo de poda. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 131 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.18: Tipos de operaciones de poda en C4.5. En esta figura tenemos el \u00e1rbol original antes del podado (a), y las dos posibles acciones de podado a realizar sobre el nodo interno C. En (b) se realiza subtree replacement , en cuyo caso el nodo C es reemplazado por uno de sus sub\u00e1rboles. Por \u00faltimo, en (c) se realiza subtree raising : El nodo B es sustituido por el sub\u00e1rbol con ra\u00edz C. En este \u00faltimo caso hay que tener en cuenta que habr\u00e1 que reclasificar de nuevo los ejemplos a partir del nodo C. Adem\u00e1s, subtree raising es muy costoso computacionalmente hablando, por lo que se suele restringir su uso al camino m\u00e1s largo a partir del nodo (hasta la hoja) que estamos podando. Como se coment\u00f3 anteriormente, el proceso de podado comienza en las hojas y contin\u00faa hacia la ra\u00edz pero, la cuesti\u00f3n es c\u00f3mo decidir reemplazar un nodo interno por una hoja (replacement ) o reemplazar un nodo interno por uno de sus nodos hijo (raising ). Lo que se hace es comparar el error estimado de clasificaci\u00f3n en el nodo en el que nos encontramos y compararlo con el error en cada uno de sus hijos y en su padre para realizar alguna de las operaciones o ninguna. En la figura 2.21 se muestra el pseudoc\u00f3digo del proceso de podado que se emplea en C4.5. Podar (ra\u00edz) { Si ra\u00edz No es HOJA Entonces Para cada hijo H de ra\u00edz Hacer Podar (H) Obtener Brazo m\u00e1s largo (B) de ra\u00edz // raising ErrorBrazo = <= Error\u00c1rbol Entonces // raising ra\u00edz = B Podar (ra\u00edz) } EstimarErrorArbol (ra\u00edz, ejemplos) { Si ra\u00edz es HOJA Entonces Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 132 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda EstimarError (ra\u00edz, ejemplos) Si no Distribuir los ejemplos (ej[]) en los brazos Para cada brazo (B) error = error + EstimarErrorArbol (B, ej[B]) } Figura 3.19: Pseudoc\u00f3digo del algoritmo de podado en C4.5. De esta forma, el subtree raising se emplea \u00fanicamente para el sub\u00e1rbol m\u00e1s largo. Adem\u00e1s, para estimar su error se emplean los ejemplos de entrenamiento, pero los del nodo origen, ya que si se eleva deber\u00e1 clasificarlos \u00e9l. En cuanto a la funci\u00f3n EstimarError , es la funci\u00f3n que estima el error de clasificaci\u00f3n de una hoja del \u00e1rbol. As\u00ed, para tomar la decisi\u00f3n debemos estimar el error de clasificaci\u00f3n en un nodo determinado para un conjunto de test independiente. Habr\u00e1 que estimarlo tanto para los nodos hoja como para los internos (suma de errores de clasificaci\u00f3n de sus hijos). No se puede tomar como dato el error de clasificaci\u00f3n en el conjunto de entrenamiento dado que, l\u00f3gicamente, el error se subestimar\u00eda. Una t\u00e9cnica para estimar el error de clasificaci\u00f3n es la denominada reduced- error pruning , que consiste en dividir el conjunto de entrenamiento en n subconjuntos n-1 de los cu\u00e1les servir\u00e1n realmente para el entrenamiento del sistema y 1 para la estimaci\u00f3n del error. Sin embargo, el problema es que la construcci\u00f3n del clasificador se lleva a cabo con menos ejemplos. Esta no es la t\u00e9cnica empleada en C4.5. La t\u00e9cnica empleada en C4.5 consiste en estimar el error de clasificaci\u00f3n bas\u00e1ndose en los propios ejemplos de entrenamiento. Para ello, en el nodo donde queramos estimar el error de clasificaci\u00f3n, se toma la clase mayoritaria de sus ejemplos como clase representante. Esto implica que habr\u00e1 E errores de clasificaci\u00f3n de un total de N ejemplos que se clasifican a trav\u00e9s de dicho nodo. El error observado ser\u00e1 f=E/N , siendo q la probabilidad de error de clasificaci\u00f3n del nodo y p=1-q la probabilidad de \u00e9xito. Se supone que la funci\u00f3n f sigue una distribuci\u00f3n binomial de par\u00e1metro q. Y lo que se desea obtener es el error e, que ser\u00e1 la probabilidad del extremo superior con un intervalo [f-z, f+z] de confianza c. Dado que se trata de una distribuci\u00f3n binomial, se obtendr\u00e1 e mediante las ecuaciones 2.58 y 2.59. czq)/N-q(1q-fP = Nfz2Nzf e222 2 2 Ec. 2.59 Como factor c (factor de confianza) se suele emplear en C4.5 el 25%, dado que es el que mejores resultados suele dar y que corresponde a un z=0.69 . Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 133 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Obtenci\u00f3n de Reglas de Clasificaci\u00f3n Cualquier \u00e1rbol de decisi\u00f3n se puede convertir en reglas de clasificaci\u00f3n, entendiendo como tal una estructura del tipo Si <Condici\u00f3n> Entonces <Clase> . El algoritmo de generaci\u00f3n de reglas consiste b\u00e1sicamente en, por cada rama del \u00e1rbol de decisi\u00f3n, las preguntas y sus valores estar\u00e1n en la parte izquierda de las reglas y la etiqueta del nodo hoja correspondiente en la parte derecha (clasificaci\u00f3n). Sin embargo, este procedimiento generar\u00eda un sistema de reglas con mayor complejidad de la necesaria. Por ello, el sistema C4.5 [QUIN93] realiza un podado de las reglas obtenidas. En la figura 2.22 se muestra el algoritmo completo de obtenci\u00f3n de reglas. ObtenerReglas (\u00e1rbol) { Convertir el \u00e1rbol de decisi\u00f3n (\u00e1rbol) a un conjunto de reglas, R error = error de clasificaci\u00f3n con R Para cada regla Ri de R Hacer Para cada precondici\u00f3n pj de Ri Hacer nuevoError = error al eliminar pj de Ri Si nuevoError <= error Entonces Eliminar pj de Ri error = nuevoError Si Ri no tiene precondiciones Entonces Eliminar Ri } Figura 3.20: Pseudoc\u00f3digo del algoritmo de obtenci\u00f3n de reglas de C4.5. En cuanto a la estimaci\u00f3n del error, se realiza del mismo modo que para realizar el podado del \u00e1rbol de decisi\u00f3n. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 134 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Decision Stump (\u00c1rbol de un solo nivel ) Todav\u00eda existe un algoritmo m\u00e1s sencillo que genera un \u00e1rbol de decisi\u00f3n de un \u00fanico nivel. Se trata de un algoritmo, [decision stump], que utiliza un \u00fanico atributo para construir el \u00e1rbol de decisi\u00f3n. La elecci\u00f3n del \u00fanico atributo que formar\u00e1 parte del \u00e1rbol se realizar\u00e1 bas\u00e1ndose en la ganancia de informaci\u00f3n, y a pesar de su simplicidad, en algunos problemas puede llegar a conseguir resultados interesantes. No tiene opciones de configuraci\u00f3n, pero la implementaci\u00f3n es muy completa, dado que admite tanto atributos num\u00e9ricos como simb\u00f3licos y clases de ambos tipos tambi\u00e9n. El \u00e1rbol de decisi\u00f3n tendr\u00e1 tres ramas: una de ellas ser\u00e1 para el caso de que el atributo sea desconocido, y las otras dos ser\u00e1n para el caso de que el valor del atributo del ejemplo de test sea igual a un valor concreto del atributo o distinto a dicho valor, en caso de los atributos simb\u00f3licos, o que el valor del ejemplo de test sea mayor o menor a un determinado valor en el caso de atributos num\u00e9ricos. En el caso de los atributos simb\u00f3licos se considera cada valor posible del mismo y se calcula la ganancia de informaci\u00f3n con el atributo igual al valor, distinto al valor y valores desconocidos del atributo. En el caso de atributos simb\u00f3licos se busca el mejor punto de ruptura, tal y como se vio en el sistema C4.5. Deben tenerse en cuenta cuatro posibles casos al calcular la ganancia de informaci\u00f3n: que sea un atributo simb\u00f3lico y la clase sea simb\u00f3lica o que la clase sea num\u00e9rica, o que sea un atributo num\u00e9rico y la clase sea simb\u00f3lica o que la clase sea num\u00e9rica. A continuaci\u00f3n se comenta cada caso por separado. Atributo Simb\u00f3lico y Clase Simb\u00f3lica Se toma cada vez un valor v x del atributo simb\u00f3lico Ai como base y se consideran \u00fanicamente tres posibles ramas en la construcci\u00f3n del \u00e1rbol: que el atributo Ai sea igual a vx, que el atributo Ai sea distinto a vx o que el valor del atributo Ai sea desconocido. Con ello, se calcula la entrop\u00eda del atributo tomando como base el valor escogido tal y como se muestra en la ecuaci\u00f3n 2.60, en la que el valor de j en el sumatorio va desde 1 a 3 porque los valores del atributo se restringen a tres: igual a v x , distinto de vx o valor desconocido. En cuanto a los par\u00e1metros, nij es el n\u00famero de ejemplos con valor j en el atributo i, n el n\u00famero total de ejemplos y nijk el n\u00famero de ejemplos con valor j en el atributo i y que pertenece a la clase k. () nI nlogn )(AI3 ij nlogn I Ec. 2.60 Atributo Num\u00e9rico y Clase Simb\u00f3lica Se ordenan los ejemplos seg\u00fan el atributo Ai y se considera cada valor vx del atributo como posible punto de corte. Se consideran entonces como posibles valores del atributo el rango menor o igual a v x, mayor a vx y valor desconocido. Se calcula la entrop\u00eda del rango tomando como base esos tres posibles valores restringidos del atributo. Atributo Simb\u00f3lico y Clase Num\u00e9rica Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 135 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Se vuelve a tomar como base cada vez cada valor del atributo, tal y como se hac\u00eda en el caso Atributo Simb\u00f3lico y Clase Simb\u00f3lica , pero en este caso se calcula la varianza de la clase para los valores del atributo mediante la ecuaci\u00f3n 2.61, donde Sj es la suma de los valores de la clase de los ejemplos con valor j en el atributo i, SSj es la suma de los valores de la clase al cuadrado y Wj es la suma de los pesos de los ejemplos (n\u00famero de ejemplos si no se incluyen pesos) con valor j en el atributo. = =3 1j jj j ivWS-SS )(A Varianza x Ec. 2.61 Atributo Num\u00e9rico y Clase Num\u00e9rica Se considera cada valor del atributo como punto de corte tal y como se hac\u00eda en el caso Atributo Num\u00e9rico y Clase Simb\u00f3lica . Posteriormente, se calcula la varianza tal y como se muestra en la ecuaci\u00f3n 2.61. En cualquiera de los cuatro casos que se han comentado, lo que se busca es el valor m\u00ednimo de la ecuaci\u00f3n calculada, ya sea la entrop\u00eda o la varianza. De esta forma se obtiene el atributo que ser\u00e1 ra\u00edz del \u00e1rbol de decisi\u00f3n y sus tres ramas. Lo \u00fanico que se har\u00e1 por \u00faltimo es construir dicho \u00e1rbol: cada rama finaliza en un nodo hoja con el valor de la clase, que ser\u00e1 la media o la moda de los ejemplos que se clasifican por ese camino, seg\u00fan se trate de una clase num\u00e9rica o simb\u00f3lica. 3.5.3. Reglas de Clasificaci\u00f3n Las t\u00e9cnicas de Inducci\u00f3n de Reglas [QUIN87, QUIN93] surgieron hace m\u00e1s de dos d\u00e9cadas y permiten la generaci\u00f3n y contraste de \u00e1rboles de decisi\u00f3n, o reglas y patrones a partir de los datos de entrada. La informaci\u00f3n de entrada ser\u00e1 un conjunto de casos donde se ha asociado una clasificaci\u00f3n o evaluaci\u00f3n a un conjunto de variables o atributos. Con esa informaci\u00f3n estas t\u00e9cnicas obtienen el \u00e1rbol de decisi\u00f3n o conjunto de reglas que soportan la evaluaci\u00f3n o clasificaci\u00f3n [CN89, HMM86]. En los casos en que la informaci\u00f3n de entrada posee alg\u00fan tipo de \"ruido\" o defecto (insuficientes atributos o datos, atributos irrelevantes o errores u omisiones en los datos) estas t\u00e9cnicas pueden habilitar m\u00e9todos estad\u00edsticos de tipo probabil\u00edstico para generar \u00e1rboles de decisi\u00f3n recortados o podados. Tambi\u00e9n en estos casos pueden identificar los atributos irrelevantes, la falta de atributos discriminantes o detectar \"gaps\" o huecos de conocimiento. Esta t\u00e9cnica suele llevar asociada una alta interacci\u00f3n con el analista de forma que \u00e9ste pueda intervenir en cada paso de la construcci\u00f3n de las reglas, bien para aceptarlas, bien para modificarlas [MM95]. La inducci\u00f3n de reglas se puede lograr fundamentalmente mediante dos caminos: Generando un \u00e1rbol de decisi\u00f3n y extrayendo de \u00e9l las reglas [QUIN93], como puede hacer el sistema C4.5 o bien mediante una estrategia de covering , consistente en tener en cuenta cada vez una clase y buscar las reglas necesarias para cubrir [cover] todos los ejemplos de esa clase; cuando se obtiene una regla se eliminan todos los ejemplos que cubre y se contin\u00faa buscando m\u00e1s reglas hasta que no haya m\u00e1s ejemplos de la clase. A continuaci\u00f3n se muestran una t\u00e9cnica de inducci\u00f3n de reglas basada en \u00e1rboles de decisi\u00f3n, otra basada en covering y una m\u00e1s que mezcla las dos estrategias. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 136 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Algoritmo 1R El m\u00e1s simple algoritmo de reglas de clasificaci\u00f3n para un conjunto de ejemplos es el 1R [HOL93]. Este algoritmo genera un \u00e1rbol de decisi\u00f3n de un nivel expresado mediante reglas. Consiste en seleccionar un atributo (nodo ra\u00edz) del cual nace una rama por cada valor, que va a parar a un nodo hoja con la clase m\u00e1s probable de los ejemplos de entrenamiento que se clasifican a trav\u00e9s suyo. Este algoritmo se muestra en la figura 2.23. 1R (ejemplos) { Para cada atributo (A) Para cada valor del atributo (Ai) Contar el n\u00famero de apariciones de cada clase con Ai Obtener la clase m\u00e1s frecuente (Cj) Crear una regla del tipo Ai -> Cj Calcular el error de las reglas del atributo A Escoger las reglas con menor error } Figura 3.21: Pseudoc\u00f3digo del algoritmo 1R. La clase debe ser simb\u00f3lica, mientras los atributos pueden ser simb\u00f3licos o num\u00e9ricos. Tambi\u00e9n admite valores desconocidos, que se toman como otro valor m\u00e1s del atributo. En cuanto al error de las reglas de un atributo, consiste en la proporci\u00f3n entre los ejemplos que cumplen la regla y los ejemplos que cumplen la premisa de la regla. En el caso de los atributos num\u00e9ricos, se generan una serie de puntos de ruptura [breakpoint], que discretizar\u00e1n dicho atributo formando conjuntos. Para ello, se ordenan los ejemplos por el atributo num\u00e9rico y se recorren. Se van contando las apariciones de cada clase hasta un n\u00famero m que indica el m\u00ednimo n\u00famero de ejemplos que pueden pertenecer a un conjunto, para evitar conjuntos demasiado peque\u00f1os. Por \u00faltimo, se unen a este conjunto ejemplos con la clase m\u00e1s frecuente y ejemplos con el mismo valor en el atributo. La sencillez de este algoritmo es un poco insultante. Su autor llega a decir [HOL93; pag 64] : \"Program 1R is ordinary in most respects.\" Tanto es as\u00ed que 1R no tiene ning\u00fan elemento de sofistificaci\u00f3n y genera para cada atributo un \u00e1rbol de profundidad 1, donde una rama est\u00e1 etiquetada por missing si es que aparecen valores desconocidos ( missing values) en ese atributo en el conjunto de entrenamiento; el resto de las ramas tienen como etiqueta un intervalo construido de una manera muy simple, como se ha explicado antes, o un valor nominal, seg\u00fan el tipo de atributo del que se trate. Lo sorprendente de este sistema es su rendimiento. En [HOL93] se describen rendimientos que en media est\u00e1n por debajo de los de C4.5 en 5,7 puntos porcentuales de aciertos de clasificaci\u00f3n. Para la realizaci\u00f3n de las pruebas, Holte, elige un conjunto de 16 problemas del almac\u00e9n de la U.C.I. [Blake, Keog, Merz, 98] que desde entonces han gozado de cierto reconocimiento como conjunto de pruebas; en alguno de estos problemas introduce algunas modificaciones que tambi\u00e9n se han hecho est\u00e1ndar. El mecanismo de estimaci\u00f3n consiste en separar el subconjunto de entrenamiento original en subconjuntos de entrenamiento y test en Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 137 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda proporci\u00f3n 2/3 y 1/3 respectivamente y repetir el experimento 25 veces. Aunque la diferencia de 5,7 es algo elevada, en realidad en 14 de los 16 problemas la diferencia es solo de 3,1 puntos. En la tabla 2.5 se presenta un ejemplo de 1R, basado en los ejemplos de la tabla 2.1. Tabla2.5. Resultados del algoritmo 1R. atributo reglas errores error total vista Soleado \u00c6 no Nublado \u00c6 si Lluvioso \u00c6 si 2/5 0/4 2/5 4/14 temperatura Alta \u00c6 no Media \u00c6 si Baja \u00c6 si 2/4 2/6 1/4 5/14 humedad Alta \u00c6 no Normal \u00c6 si 3/7 1/7 4/14 viento Falso \u00c6 si Cierto \u00c6 no 2/8 3/6 5/14 Para clasificar seg\u00fan la clase jugar, 1R considera cuatro conjuntos de reglas, uno por cada atributo, que son las mostradas en la tabla anterior, en las que adem\u00e1s aparecen los errores que se cometen. De esta forma se concluye que como los errores m\u00ednimos corresponden a las reglas generadas por los atributos vista y humedad, cualquiera de ellas es valida, de manera que arbitrariamente se puede elegir cualquiera de estos dos conjuntos de reglas como generador de 1R. Algoritmo PRISM PRISM [CEN87] es un algoritmo b\u00e1sico de aprendizaje de reglas que asume que no hay ruido en los datos. Sea t el n\u00famero de ejemplos cubiertos por la regla y p el n\u00famero de ejemplos positivos cubiertos por la regla. Lo que hace PRISM es a\u00f1adir condiciones a reglas que maximicen la relaci\u00f3n p/t (relaci\u00f3n entre los ejemplos positivos cubiertos y ejemplos cubiertos en total). En la figura 2.24 se muestra el algoritmo de PRISM. PRISM (ejemplos) { Para cada clase (C) E = ejemplos Mientras E tenga ejemplos de C Crea una regla R con parte izquierda vac\u00eda y clase C Hasta R perfecta Hacer Para cada atributo A no incluido en R y cada valor v de A Considera a\u00f1adir la condici\u00f3n A=v a la parte izquierda de R Selecciona el par A=v que maximice p/t (en caso de empates, escoge la que tenga p mayor) A\u00f1adir A=v a R Elimina de E los ejemplos cubiertos por R Figura 3.22: Pseudoc\u00f3digo del algoritmo PRISM. Este algoritmo va eliminando los ejemplos que va cubriendo cada regla, por lo que las reglas tienen que interpretarse en orden. Se habla entonces de listas de reglas [decision list]. En la figura 2.25 se muestra un ejemplo de c\u00f3mo act\u00faa el algoritmo. Concretamente se trata de la aplicaci\u00f3n del mismo sobre el ejemplo de la tabla 2.1. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 138 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.23: Ejemplo de PRISM. En la figura 2.25 se muestra c\u00f3mo el algoritmo toma en primer lugar la clase S\u00ed. Partiendo de todos los ejemplos de entrenamiento (un total de catorce) calcula el cociente p/t para cada par atributo-valor y escoge el mayor. En este caso, dado que la condici\u00f3n escogida hace la regla perfecta ( p/t = 1 ), se eliminan los cuatro ejemplos que cubre dicha regla y se busca una nueva regla. En la segunda regla se obtiene en un primer momento una condici\u00f3n que no hace perfecta la regla, por lo que se contin\u00faa buscando con otra condici\u00f3n. Finalmente, se muestra la lista de decisi\u00f3n completa que genera el algoritmo. Algoritmo PART Uno de los sistemas m\u00e1s importantes de aprendizaje de reglas es el proporcionado por C4.5 [QUI93], explicado anteriormente. Este sistema, al igual que otros sistemas de inducci\u00f3n de reglas, realiza dos fases: primero, genera un conjunto de reglas de clasificaci\u00f3n y despu\u00e9s refina estas reglas para mejorarlas, realizando as\u00ed una proceso de optimizaci\u00f3n global de dichas reglas. Este proceso de optimizaci\u00f3n global es siempre muy complejo y costoso computacionalmente hablando. Por otro lado, el algoritmo PART [FRWI98] es un sistema que obtiene reglas sin dicha optimizaci\u00f3n global. Recibe el nombre PART por su modo de actuaci\u00f3n: obtaining rules from PARTial decision trees , y fue desarrollado por el grupo neozeland\u00e9s que construy\u00f3 el entorno WEKA [WF98]. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 139 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda El sistema se basa en las dos estrategias b\u00e1sicas para la inducci\u00f3n de reglas: el covering y la generaci\u00f3n de reglas a partir de \u00e1rboles de decisi\u00f3n. Adopta la estrategia del covering (con lo que se obtiene una lista de decisi\u00f3n) dado que genera una regla, elimina los ejemplares que dicha regla cubre y contin\u00faa generando reglas hasta que no queden ejemplos por clasificar. Sin embargo, el proceso de generaci\u00f3n de cada regla no es el usual. En este caso, para crear una regla, se genera un \u00e1rbol de decisi\u00f3n podado, se obtiene la hoja que clasifique el mayor n\u00famero de ejemplos, que se transforma en la regla, y posteriormente se elimina el \u00e1rbol. Uniendo estas dos estrategias se consigue mayor flexibilidad y velocidad. Adem\u00e1s, no se genera un \u00e1rbol completo, sino un \u00e1rbol parcial [partial decisi\u00f3n tree]. Un \u00e1rbol parcial es un \u00e1rbol de decisi\u00f3n que contiene brazos con sub\u00e1rboles no definidos. Para generar este \u00e1rbol se integran los procesos de construcci\u00f3n y podado hasta que se encuentra un sub\u00e1rbol estable que no puede simplificarse m\u00e1s, en cuyo caso se para el proceso y se genera la regla a partir de dicho sub\u00e1rbol. Este proceso se muestra en la figura 2.26. Expandir (ejemplos) { elegir el mejor atributo para dividir en subconjuntos Mientras (subconjuntos No expandidos) Y (todos los subconjuntos expandidos son HOJA) Expandir (subconjunto) Si (todos los subconjuntos expandidos son HOJA) Y (errorSub\u00e1rbol >= errorNodo) deshacer la expansi\u00f3n del nodo y nodo es HOJA Figura 3.24: Pseudoc\u00f3digo de expansi\u00f3n de PART. El proceso de elecci\u00f3n del mejor atributo se hace como en el sistema C4.5, esto es, bas\u00e1ndose en la raz\u00f3n de ganancia. La expansi\u00f3n de los subconjuntos generados se realiza en orden, comenzando por el que tiene menor entrop\u00eda y finalizando por el que tiene mayor. La raz\u00f3n de realizarlo as\u00ed es porque si un subconjunto tiene menor entrop\u00eda hay m\u00e1s probabilidades de que se genere un sub\u00e1rbol menor y consecuentemente se cree una regla m\u00e1s general. El proceso contin\u00faa recursivamente expandiendo los subconjuntos hasta que se obtienen hojas , momento en el que se realizar\u00e1 una vuelta atr\u00e1s [backtracking]. Cuando se realiza dicha vuelta atr\u00e1s y los hijos del nodo en cuesti\u00f3n son hojas , comienza el podado tal y como se realiza en C4.5 (comparando el error esperado del sub\u00e1rbol con el del nodo), pero \u00fanicamente se realiza la funci\u00f3n de reemplazamiento del nodo por hoja [subtree replacement]. Si se realiza el podado se realiza otra vuelta atr\u00e1s hacia el nodo padre , que sigue explorando el resto de sus hijos, pero si no se puede realizar el podado el padre no continuar\u00e1 con la exploraci\u00f3n del resto de nodos hijos (ver segunda condici\u00f3n del bucle \" mientras\" en la figura 2.26). En este momento finalizar\u00e1 el proceso de expansi\u00f3n y generaci\u00f3n del \u00e1rbol de decisi\u00f3n. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 140 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.25: Ejemplo de generaci\u00f3n de \u00e1rbol parcial con PART. En la figura 2.27 se presenta un ejemplo de generaci\u00f3n de un \u00e1rbol parcial donde, junto a cada brazo de un nodo, se muestra el orden de exploraci\u00f3n (orden ascendente seg\u00fan el valor de la entrop\u00eda). Los nodos con relleno gris claro son los que a\u00fan no se han explorado y los nodos con relleno gris oscuro los nodos hoja. Las flechas ascendentes representan el proceso de backtracking . Por \u00faltimo, en el paso 5, cuando el nodo 4 es explorado y los nodos 9 y 10 pasan a ser hoja, el nodo padre intenta realizar el proceso de podado, pero no se realiza el reemplazo (representado con el 4 en negrita), con lo que el proceso, al volver al nodo 1, finaliza sin explorar el nodo 2. Una vez generado el \u00e1rbol parcial se extrae una regla del mismo. Cada hoja se corresponde con una posible regla, y lo que se busca es la mejor hoja. Si bien se pueden considerar otras heur\u00edsticas, en el algoritmo PART se considera mejor hoja aquella que cubre un mayor n\u00famero de ejemplos. Se podr\u00eda haber optado, por ejemplo, por considerar mejor aquella que tiene un menor error esperado, pero tener una regla muy precisa no significa lograr un conjunto de reglas muy preciso. Por \u00faltimo, PART permite que haya atributos con valores desconocidos tanto en el proceso de aprendizaje como en el de validaci\u00f3n y atributos num\u00e9ricos, trat\u00e1ndolos exactamente como el sistema C4.5. 3.5.4. Clasificaci\u00f3n Bayesiana Los clasificadores Bayesianos [DH73] son clasificadores estad\u00edsticos, que pueden predecir tanto las probabilidades del n\u00famero de miembros de clase, como la probabilidad de que una muestra dada pertenezca a una clase particular. La clasificaci\u00f3n Bayesiana se basa en el teorema de Bayes, y los clasificadores Bayesianos han demostrado una alta exactitud y velocidad cuando se han aplicado a grandes bases de datos Diferentes estudios comparando los algoritmos de clasificaci\u00f3n han determinado que un clasificador Bayesiano sencillo conocido como el clasificador \" naive Bayesiano\" [JOH97] es comparable en rendimiento a un \u00e1rbol de decisi\u00f3n y a clasificadores de redes de neuronas. A continuaci\u00f3n se explica los fundamentos de los clasificadores bayesianos y, m\u00e1s concretamente, del clasificador naive Bayesiano. Tras esta explicaci\u00f3n se comentar\u00e1 otro clasificador que, si bien no es un clasificador bayesiano, esta relacionado con \u00e9l, dado que se trata tambi\u00e9n de un clasificador basado en la estad\u00edstica. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 141 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Clasificador Naive Bayesiano Lo que normalmente se quiere saber en aprendizaje es cu\u00e1l es la mejor hip\u00f3tesis (m\u00e1s probable) dados los datos. Si denotamos P(D) como la probabilidad a priori de los datos (i.e., cuales datos son m\u00e1s probables que otros), P(D|h) la probabilidad de los datos dada una hip\u00f3tesis, lo que queremos estimar es: P(h|D) , la probabilidad posterior de h dados los datos. Esto se puede estimar con el teorema de Bayes, ecuaci\u00f3n 2.62. ()( )() ()DPhPh|DPD|hP = Ec. 2.62 Para estimar la hip\u00f3tesis m\u00e1s probable (MAP, [maximum a posteriori hip\u00f3tesis]) se busca el mayor P(h|D) como Ec. 2.63 Ya que P(D) es una constante independiente de h. Si se asume que todas las hip\u00f3tesis son igualmente probables, entonces resulta la hip\u00f3tesis de m\u00e1xima verosimilitud (ML, [maximum [ingenuo] Bayesiano se utiliza cuando se quiere clasificar un ejemplo descrito por un conjunto de atributos ( ai's) en un conjunto finito de clases (V). Clasificar un nuevo ejemplo de acuerdo con el valor m\u00e1s probable dados los valores de sus atributos. Si se aplica 2.64 al problema de la clasificaci\u00f3n se 1 Vvn 1 j Vv MAP vPv|a,...,aP argmaxa,...,aPvPv|a,...,aPargmaxa,...,a|vP argmax v == Ec. 2.65 Adem\u00e1s, el clasificador naive Bayesiano asume que los valores de los atributos son condicionalmente independientes dado el valor de la clase, por lo que se hace cierta la ecuaci\u00f3n 2.66 y con ella la 2.67. v|a,...,aP Ec. 2.66 Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 142 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ( )() ( Ec. 2.67 Los clasificadores naive Bayesianos asumen que el efecto de un valor del atributo en una clase dada es independiente de los valores de los otros atributos. Esta suposici\u00f3n se llama \"independencia condicional de clase\". \u00c9sta simplifica los c\u00e1lculos involucrados y, en este sentido, es considerado \"ingenuo\" [naive]. Esta asunci\u00f3n es una simplificaci\u00f3n de la realidad. A pesar del nombre del clasificador y de la simplificaci\u00f3n realizada, el naive Bayesiano funciona muy bien, sobre todo cuando se filtra el conjunto de atributos seleccionado para eliminar redundancia, con lo que se elimina tambi\u00e9n dependencia entre datos. En la figura 2.28 se muestra un ejemplo de aprendizaje con el clasificador naive Bayesiano , as\u00ed como una muestra de c\u00f3mo se clasificar\u00eda un ejemplo de test. Como ejemplo se emplear\u00e1 el de la tabla 2.1. Figura 3.26: Ejemplo de aprendizaje y clasificaci\u00f3n con naive Bayesiano. En este ejemplo se observa que en la fase de aprendizaje se obtienen todas las probabilidades condicionadas P(a i|vj) y las probabilidades P(v j). En la clasificaci\u00f3n se realiza el productorio y se escoge como clase del ejemplo de entrenamiento la que obtenga un mayor valor. Algo que puede ocurrir durante el entrenamiento con este clasificador es que para cada valor de cada atributo no se encuentren ejemplos para todas las clases. Sup\u00f3ngase que para el atributo a i y el valor j de dicho atributo no hay ning\u00fan ejemplo de entrenamiento con clase k. En este caso, P(a ij|k)=0 . Esto hace que si se intenta clasificar cualquier ejemplo con el par atributo-valor aij, la probabilidad asociada para la clase k ser\u00e1 siempre 0, ya que hay que realizar el productorio de las probabilidades condicionadas para todos los atributos de la instancia. Para resolver este problema se parte de que las probabilidades se contabilizan a partir de las frecuencias de aparici\u00f3n de cada evento o, en nuestro caso, las frecuencias de aparici\u00f3n de cada terna atributo-valor-clase. El estimador de Laplace , consiste en Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 143 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda comenzar a contabilizar la frecuencia de aparici\u00f3n de cada terna a partir del 1 y no del 0, con lo que ninguna probabilidad condicionada ser\u00e1 igual a 0. Una ventaja de este clasificador es la cuesti\u00f3n de los valores perdidos o desconocidos: en el clasificador naive Bayesiano si se intenta clasificar un ejemplo con un atributo sin valor simplemente el atributo en cuesti\u00f3n no entra en el productorio que sirve para calcular las probabilidades. Respecto a los atributos num\u00e9ricos, se suele suponer que siguen una distribuci\u00f3n Normal o Gaussiana. Para estos atributos se calcula la media \u00b5 y la desviaci\u00f3n t\u00edpica obteniendo los dos par\u00e1metros de la distribuci\u00f3n N(\u00b5, ), que sigue la expresi\u00f3n de la ecuaci\u00f3n 2.68, donde el par\u00e1metro x ser\u00e1 el valor del atributo num\u00e9rico en el ejemplo que se quiere clasificar. ()() 22 2\u00b5x e 21xf = Ec. 2.68 Votaci\u00f3n por intervalos de caracter\u00edsticas Este algoritmo es una t\u00e9cnica basada en la proyecci\u00f3n de caracter\u00edsticas. Se le denomina \"votaci\u00f3n por intervalos de caracter\u00eds ticas\" (VFI, [Voting Feature Interval]) porque se construyen intervalos para cada caracter\u00edstica [feature] o atributo en la fase de aprendizaje y el intervalo correspondiente en cada caracter\u00edstica \"vota\" para cada clase en la fase de clasificaci\u00f3n. Al igual que en el clasificador naive Bayesiano, cada caracter\u00edstica es tratada de forma individual e independiente del resto. Se dise\u00f1a un sistema de votaci\u00f3n para combinar las clasificaciones individuales de cada atributo por separado. Mientras que en el clasificador naive Bayesiano cada caracter\u00edstica participa en la clasificaci\u00f3n asignando una probabilidad para cada clase y la probabilidad final para cada clase consiste en el producto de cada probabilidad dada por cada caracter\u00edstica, en el algoritmo VFI cada caracter\u00edstica distribuye sus votos para cada clase y el voto final de cada clase es la suma de los votos obtenidos por cada caracter\u00edstica. Una ventaja de estos clasificadores, al igual que ocurr\u00eda con el clasificador naive Bayesiano, es el tratamiento de los valores desconocidos tanto en el proceso de aprendizaje como en el de clasificaci\u00f3n: simplemente se ignoran, dado que se considera cada atributo como independiente del resto. En la fase de aprendizaje del algoritmo VFI se construyen intervalos para cada atributo contabilizando, para cada clase, el n\u00famero de ejemplos de entrenamiento que aparecen en dicho intervalo. En la fase de clasificaci\u00f3n, cada atributo del ejemplo de test a\u00f1ade votos para cada clase dependiendo del intervalo en el que se encuentre y el conteo de la fase de aprendizaje para dicho intervalo en cada clase. En la figura 2.29 se muestra este algoritmo. Aprendizaje (ejemplos) { Para cada atributo (A) Hacer Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 144 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Si A es NUM\u00c9RICO Entonces Obtener m\u00ednimo y m\u00e1ximo de A para cada clase en ejemplos Ordenar los valores obtenidos (I intervalos) Si no /* es SIMB\u00d3LICO */ Obtener los valores que recibe A para cada clase en ejemplos Los valores obtenidos son puntos (I intervalos) Para cada intervalo I Hacer Para cada clase C Hacer contadores [A, I, C] = 0 Para cada ejemplo E Hacer Si A es conocido Entonces Si A es SIMB\u00d3LICO Entonces contadores [A, E.A, E.C] += 1 Si no /* */ Obtener intervalo I de E.A Si E.A = extremo inferior de intervalo I I, C] = 1 */ } clasificar (ejemplo E) { Para cada atributo (A) Hacer Si E.A es conocido Entonces Si A es SIMB\u00d3LICO Para cada clase C Hacer voto[A, C] = contadores[A, E.A, C] Si no /* */ Obtener intervalo I de E.A Si E.A = l\u00edmite inferior de I Entonces Para cada clase C Hacer voto[A, C] = 0.5*contadores[A,I,C] + 0.5*contadores[A,I-1,C] Si no Para cada Hacer C] Figura 3.27: Pseudoc\u00f3digo del algoritmo VFI. En la figura 2.30 se presenta un ejemplo de entrenamiento y clasificaci\u00f3n con el algoritmo VFI, en el que se muestra una tabla con los ejemplos de entrenamiento y c\u00f3mo el proceso de aprendizaje consiste en el establecimiento de intervalos para cada atributo con el conteo de ejemplos que se encuentran en cada intervalo. Se muestra entre par\u00e9ntesis el n\u00famero de ejemplos que se encuentran en la clase e intervalo concreto, mientras que fuera de los par\u00e9ntesis se encuentra el valor normalizado. Para el atributo simb\u00f3lico simplemente se toma como intervalo (punto) cada valor de dicho atributo y se cuenta el n\u00famero de ejemplos que tienen un valor determinado en el atributo para la clase del ejemplo en cuesti\u00f3n. En el caso del atributo num\u00e9rico, se obtiene el m\u00e1ximo y el m\u00ednimo valor del atributo para cada clase que en este caso son 4 y 7 para la clase A, y 1 y 5 para la clase B. Se ordenan los valores form\u00e1ndose un Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 145 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda total de cinco intervalos y se cuenta el n\u00famero de ejemplos que se encuentran en un intervalo determinado para su clase, teniendo en cuenta que si se encuentra en el punto compartido por dos intervalos se contabiliza la mitad para cada uno de ellos. Tambi\u00e9n se muestra un ejemplo de clasificaci\u00f3 n: en primer lugar, se obtienen los votos que cada atributo por separado concede a cada clase, que ser\u00e1 el valor normalizado del intervalo (o punto si se trata de atributos simb\u00f3licos) en el que se encuentre el valor del atributo, y posteriormente se suman los votos (que se muestra entre par\u00e9ntesis) y se normaliza. La clase con mayor porcentaje de votos (en el ejemplo la clase A) gana . Figura 3.28: Ejemplo de aprendizaje y clasificaci\u00f3n con VFI. 3.5.5. Aprendizaje Basado en Ejemplares El aprendizaje basado en ejemplares o instancias [BRIS96] tiene como principio de funcionamiento, en sus m\u00faltiples variantes, el almacenamiento de ejemplos: en unos casos todos los ejemplos de entrenamiento, en otros solo los m\u00e1s representativos, en otros los incorrectamente clasificados cuando se clasifican por primera vez, etc. La clasificaci\u00f3n posterior se realiza por medio de una funci\u00f3n que mide la proximidad o parecido. Dado un ejemplo para clasificar se le clasifica de acuerdo al ejemplo o ejemplos m\u00e1s pr\u00f3ximos. El bias (sesgo) que rige este m\u00e9todo es la proximidad; es decir, la generalizaci\u00f3n se gu\u00eda por la proximidad de un ejemplo a otros. Algunos autores consideran este bias m\u00e1s apropiado para el aprendizaje de conceptos naturales que el correspondiente al proceso inductivo (Bareiss et al. en [KODR90]), por otra parte tambi\u00e9n se ha estudiado la relaci\u00f3n entre este m\u00e9todo y los que generan reglas (Clark, 1990). Se han enumerado ventajas e inconvenientes del aprendizaje basado en ejemplares [BRIS96], pero se suele considerar no adecuado para el tratamiento de Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 146 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda atributos no num\u00e9ricos y valores desconoc idos. Las mismas medidas de proximidad sobre atributos simb\u00f3licos suelen proporci onar resultados muy dispares en problemas diferentes. A continuaci\u00f3n se muestran dos t\u00e9cnicas de aprendizaje basado en ejemplares: el m\u00e9todo de los k-vecinos m\u00e1s pr\u00f3ximos y el k estrella. Algoritmo de los k-vecinos m\u00e1s pr\u00f3ximos El m\u00e9todo de los k-vecinos m\u00e1s pr\u00f3ximos [MITC97] (KNN, [k-Nearest Neighbor]) est\u00e1 considerado como un buen representante de este tipo de aprendizaje, y es de gran sencillez conceptual. Se suele denominar m\u00e9todo porque es el esqueleto de un algoritmo que admite el intercambio de la funci\u00f3n de proximidad dando lugar a m\u00faltiples variantes. La funci\u00f3n de proximidad puede decidir la clasificaci\u00f3n de un nuevo ejemplo atendiendo a la clasificaci\u00f3n del ejemplo o de la mayor\u00eda de los k ejemplos m\u00e1s cercanos. Admite tambi\u00e9n funciones de proximidad que consideren el peso o coste de los atributos que intervienen, lo que permite, entre otras cosas, eliminar los atributos irrelevantes. U na funci\u00f3n de proximidad cl\u00e1sica entre dos instancias x i y xj , si suponemos que un ejemplo viene representado por una n-tupla de la forma ( a1(x), a 2(x), ... , a n(x)) en la que ar(x) es el valor de la instancia para el atributo ar, es la distancia eucl\u00eddea, que se muestra en la ecuaci\u00f3n 2.69. () = 1l2 jl il )x,d(x Ec. 2.69 En la figura 2.31 se muestra un ejemplo del algoritmo KNN para un sistema de dos atributos, represent\u00e1ndose por ello en un plano. En este ejemplo se ve c\u00f3mo el proceso de aprendizaje consiste en el almacenamiento de todos los ejemplos de entrenamiento. Se han representado los ejemplos de acuerdo a los valores de sus dos atributos y la clase a la que pertenecen (las clases son + y -). La clasificaci\u00f3n consiste en la b\u00fasqueda de los k ejemplos (en este caso 3) m\u00e1s cercanos al ejemplo a clasificar. Concretamente, el ejemplo a se clasificar\u00eda como -, y el ejemplo b como +. Figura 3.29: Ejemplo de Aprendizaje y Clasificaci\u00f3n con KNN. Dado que el algoritmo k-NN permite que los atributos de los ejemplares sean simb\u00f3licos y num\u00e9ricos, as\u00ed como que haya atributos sin valor [missing values] el Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 147 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda algoritmo para el c\u00e1lculo de la distancia entre ejemplares se complica ligeramente. En la figura 2.32 se muestra el algoritmo que calcula la distancia entre dos ejemplares cualesquiera. Distancia (E1, E2) { dst = 0 n = 0 Para cada atributo A Hacer { dif = Diferencia(E1.A, E2.A) dst = dst + dif * dif n = n + 1 } dst = dst / n Devolver dst } Diferencia (A1, A2) { Si A1.nominal Entonces { Si SinValor(A1) O SinValor(A2) O A1 <> A2 Entonces Devolver 1 Si no Devolver 0 } Si no { Si SinValor(A1) O SinValor(A2) Entonces { Si SinValor(A1) Y SinValor(A2) Entonces Devolver 1 Si SinValor(A1) Entonces dif = A2 Si no Entonces dif = A1 Si dif < 0.5 Entonces Devolver 1 - dif Si no Devolver dif } Si no Devolver abs(A1 - A2) } } Figura 3.30: Pseudoc\u00f3digo del algoritmo empleado para definir la distancia entre dos ejemplos. Adem\u00e1s de los distintos tipos de atributos hay que tener en cuenta tambi\u00e9n, en el caso de los atributos num\u00e9ricos, los rangos en los que se mueven sus valores. Para evitar que atributos con valores muy altos tengan mucho mayor peso que atributos con valores bajos, se normalizar\u00e1n dichos valores con la ecuaci\u00f3n 2.70. l ll il min Maxminx Ec. 2.70 En esta ecuaci\u00f3n xif ser\u00e1 el valor i del atributo f, siendo min f el m\u00ednimo valor del atributo f y Max f el m\u00e1ximo. Por otro lado, el algoritmo permite dar mayor preferencia a aquellos ejemplares m\u00e1s cercanos al que deseamos clasificar. En ese caso, en lugar de emplear directamente la distancia entre ejemplares, se utilizar\u00e1 la ecuaci\u00f3n 2.71. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 148 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda )x,d(x11 j i + Ec. 2.71 Algoritmo k-estrella El algoritmo K* [CLTR95] es una t\u00e9cnica de data mining basada en ejemplares en la que la medida de la distancia entre ejemplares se basa en la teor\u00eda de la informaci\u00f3n. Una forma intuitiva de verlo es que la distancia entre dos ejemplares se define como la complejidad de transformar un ejemplar en el otro. El c\u00e1lculo de la complejidad se basa en primer lugar en definir un conjunto de transformaciones T={t 1, t2, ..., t n , } para pasar de un ejemplo (valor de atributo) a a uno b. La transformaci\u00f3n es la de parada y es la transformaci\u00f3n identidad ( (a)=a ). El conjunto P es el conjunto de todas las posibles secuencias de transformaciones descritos en T* que terminan en , y (a)t es una de estas secuencias concretas sobre el ejemplo a. Esta secuencia de transformaciones tendr\u00e1 una probabilidad determinada )tp(, defini\u00e9ndose la funci\u00f3n de probabilidad P*(b|a) como la probabilidad de pasar del ejemplo a al ejemplo b a trav\u00e9s de cualquier secuencia de transformaciones, tal y como se muestra en la ecuaci\u00f3n 2.72. = = b(a)t:Pt)tp( a)|(b*P Ec. 2.72 Esta funci\u00f3n de probabilidad cumplir\u00e1 las propiedades que se muestran en 2.73. 1a)|(b*P b= ; 1a)|(b*P0 Ec. 2.73 La funci\u00f3n de distancia K* se define entonces tomando logaritmos, tal y como se muestra en la ecuaci\u00f3n 2.74. a)|(b*Plog a)|(b*K2 = Ec. 2.74 Realmente K* no es una funci\u00f3n de distancia dado que, por ejemplo K*(a|a) generalmente no ser\u00e1 exactamente 0, adem\u00e1s de que el operador | no es sim\u00e9trico, esto es, K*(a|b) no es igual que K*(b|a) . Sin embargo, esto no interfiere en el algoritmo K*. Adem\u00e1s, la funci\u00f3n K* cumple las propiedades que se muestran en la ecuaci\u00f3n 2.75. 0a)|(b*K ; a)|(c*Ka)|(b*Kb)|(c*K + Ec. 2.75 Una vez explicado c\u00f3mo se obtiene la funci\u00f3n K* y cuales son sus propiedades, se presenta a continuaci\u00f3n la expresi\u00f3n concreta de la funci\u00f3n P*, de la que se obtiene K*, para los tipos de atributos admitidos por el algoritmo: num\u00e9ricos y simb\u00f3licos. Probabilidad de transformaci\u00f3n para los atributos permitidos Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 149 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda En cuanto a los atributos num\u00e9ricos, la s transformaciones consideradas ser\u00e1n restar del valor a un n\u00famero n o sumar al valor a un n\u00famero n, siendo n un n\u00famero m\u00ednimo. La probabilidad de pasar de un ejemplo con valor a a uno con valor b vendr\u00e1 determinada \u00fanicamente por el valor absoluto de la diferencia entre a y b, que se denominar\u00e1 x. Se escribir\u00e1 la funci\u00f3n de probabilidad como una funci\u00f3n de densidad, tal y como se muestra en la ecuaci\u00f3n 2.76, donde x0 ser\u00e1 una medida de longitud de la escala, por ejemplo, la media esperada para x sobre la distribuci\u00f3n P*. Es necesario elegir un x0 razonable. Posteriormente se mostrar\u00e1 un m\u00e9todo para elegir este factor. Para los simb\u00f3licos, se considerar\u00e1n las probabilidades de aparici\u00f3n de cada uno de los valores de dicho atributo. dx e2x1(x)*P0xx 0 = Ec. 2.76 Si el atributo tiene un total de n posibles valores, y la probabilidad de aparici\u00f3n del valor i del atributo es pi (obtenido a partir de las apariciones en los ejemplos de entrenamiento), se define la probabilidad de transformaci\u00f3n de un ejemplo con valor i a uno con valor j como se muestra en la ecuaci\u00f3n 2.77. () Ec. 2.77 En esta ecuaci\u00f3n s es la probabilidad del s\u00edmbolo de parada ( ). De esta forma, se define la probabilidad de cambiar de valor como la probabilidad de que no se pare la transformaci\u00f3n multiplicado por la probabilidad del valor de destino, mientras la probabilidad de continuar con el mismo valor es la probabilidad del s\u00edmbolo de parada m\u00e1s la probabilidad de que se contin\u00fae transformando multiplicado por la probabilidad del valor de destino. Tambi\u00e9n es importante, al igual que con el factor x 0, definir correctamente la probabilidad s. Y como ya se coment\u00f3 con x0, posteriormente se comentar\u00e1 un m\u00e9todo para obtenerlo. Tambi\u00e9n deben tenerse en cuenta la posibilidad de los atributos con valores desconocidos. Cuando los valores desconocidos aparecen en los ejemplos de entrenamiento se propone como soluci\u00f3n el considerar que el atributo desconocido se determina a trav\u00e9s del resto de ejemplares de entrenamiento. Esto se muestra en la ecuaci\u00f3n 2.78, donde n es el n\u00famero de ejemplos de entrenamiento. ==n 1b na)|(b*Pa)|(?*P Ec. 2.78 Combinaci\u00f3n de atributos Ya se han definido las funciones de probabilidad para los tipos de atributos permitidos. Pero los ejemplos reales tienen m\u00e1s de un atributo, por lo que es necesario combinar los resultados obtenidos para cada atributo. Y para combinarlos, y definir as\u00ed la distancia entre dos ejemplos, se entiende la probabilidad de transformaci\u00f3n de un ejemplar en otro como la probabilidad de trans formar el primer atributo del primer ejemplo en el del segundo, seguido de la transformaci\u00f3n del segundo atributo del primer ejemplo en el del segundo, etc. De esta forma, la probabilidad de transformar Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 150 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda un ejemplo en otro viene determinado por la multiplicaci\u00f3n de las probabilidades de transformaci\u00f3n de cada atributo de forma individual, tal y como se muestra en la ecuaci\u00f3n 2.79. En esta ecuaci\u00f3n m ser\u00e1 el n\u00famero de atributo de los ejemplos. Y con esta definici\u00f3n la distancia entre dos ejemplos se define como la suma de distancias entre cada atributo de los ejemplos. ==m 1i1i 2i 1 2 )v|(v*P )E|(E*P Ec. 2.79 Selecci\u00f3n de los par\u00e1metros aleatorios Para cada atributo debe determinarse el valor para los par\u00e1metros s o x0 seg\u00fan se trate de un atributo simb\u00f3lico o num\u00e9rico respectivamente. Y el valor de este atributo es muy importante. Por ejemplo, si a s se le asigna un valor muy bajo las probabilidades de transformaci\u00f3n ser\u00e1n muy altas, mientras que si s se acerca a 0 las probabilidades de transformaci\u00f3n ser\u00e1n muy bajas. Y lo mismo ocurrir\u00eda con el par\u00e1metro x 0. En ambos casos se puede observar c\u00f3mo var\u00eda la funci\u00f3n de probabilidad P* seg\u00fan se var\u00eda el n\u00famero de ejemplos incluidos partiendo desde 1 (vecino m\u00e1s cercano) hasta n (todos los ejemplares con el mismo peso). Se puede calcular para cualquier funci\u00f3n de probabilidad el n\u00famero efectivo de ejemplos como se muestra en la ecuaci\u00f3n 2.80, en la que n es el n\u00famero de ejemplos de entrenamiento y n 0 es el n\u00famero de ejemplos con la distancia m\u00ednima al ejemplo a (para el atributo considerado). El algoritmo K* escoger\u00e1 para x0 (o s) un n\u00famero () ()n a|b*Pa|b*P n2n 1b2n 1b 0 == Ec. 2.80 Por conveniencia se expresa el valor escogido como un par\u00e1metro de mezclado [blending] b, que var\u00eda entre b=0% (n0) y b=100% (n). La configuraci\u00f3n de este par\u00e1metro se puede ver como una esfera de influencia que determina cuantos vecinos de a deben considerarse importantes. Para obtener el valor correcto para el par\u00e1metro x0 (o s) se realiza un proceso iterativo en el que se obtienen las esferas de influencia m\u00e1xima ( x0 o s igual a 0) y m\u00ednima ( x0 o s igual a 1), y se aproximan los valores para que dicha esfera se acerque a la necesaria para cumplir con el par\u00e1metro de mezclado. En la figura 2.33 se presenta un ejemplo pr\u00e1ctico de c\u00f3mo obtener los valores para los par\u00e1metros x0 o s. Se va a utilizar para ello el problema que se present\u00f3 en la tabla 2.1, y m\u00e1s concretamente el atributo Vista con el valor igual a Lluvioso , de dicho problema. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 151 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.31: Ejemplo de obtenci\u00f3n del par\u00e1metros de un atributo simb\u00f3lico con el algoritmo K*. En la figura 2.33 se muestra c\u00f3mo el objetivo es conseguir un valor para s tal que se obtenga una esfera de influencia de 6,8 ejemplos. Los par\u00e1metros de configuraci\u00f3n necesarios para el funcionamiento del sistema son: el par\u00e1metro de mezclado b, en este caso igual a 20%; una constante denominada EPSILON , en este caso igual a 0,01, que determina entre otras cosas cu\u00e1ndo se considera alcanzada la esfera de influencia deseada. En cuanto a la nomenclatura empleada, n ser\u00e1 el n\u00famero total de ejemplos de entrenamiento, nv el n\u00famero de valores que puede adquirir el atributo, y se han empleado abreviaturas para denominar los valores del atributo: lluv por lluvioso, nub por nublado y sol por soleado. Tal y como puede observarse en la figura 2.33, las ecuaciones empleadas para el c\u00e1lculo de la esfera y de P* no son exactamente las definidas en las ecuaciones definidas anteriormente. Sin embargo, en el ejemplo se han empleado las implementadas en la herramienta WEKA por los creadores del algoritmo. En cuanto al Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 152 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ejemplo en s\u00ed, se muestra c\u00f3mo son necesarias 8 iteraciones para llegar a conseguir el objetivo planteado, siendo el resultado de dicho proceso, el valor de s, igual a 0,75341 . Clasificaci\u00f3n de un ejemplo Se calcula la probabilidad de que un ejemplo a pertenezca a la clase c sumando la probabilidad de a a cada ejemplo que es miembro de c, tal y como se muestra en 2.81. () () = cba|b*P a|c*P Ec. 2.81 Se calcula la probabilidad de pertenencia a cada clase y se escoge la que mayor resultado haya obtenido como predicci\u00f3n para el ejemplo. Figura 3.32: Ejemplo de clasificaci\u00f3n con K*. Una vez definido el modo en que se clasifica un determinado ejemplo de test mediante el algoritmo K*, en la figura 2.34 se muestra un ejemplo concreto en el que Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 153 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda se emplea dicho algoritmo. En el ejemplo se clasifica un ejemplo de test tomando como ejemplos de entrenamiento los que se mostraron en la tabla 2.1, tomando los atributos Temperatura y Humedad como num\u00e9ricos. El proceso que se sigue para determinar a qu\u00e9 clase pertenece un ejemplo de test determinado es el siguiente: en primer lugar, habr\u00eda que calcular los par\u00e1metros x0 y s que a\u00fan no se conocen para los pares atributo-valor del ejemplo de test. Posteriormente se aplican las ecuaciones, que de nuevo no son exactamente las definidas anteriormente: se han empleado las que los autores del algoritmo implementan en la herramienta WEKA. Una vez obtenidas las probabilidades, se normalizan y se escoge la mayor de las obtenidas. En este caso hay m\u00e1s de un 99% de probabilidad a favor de la clase no. Esto se debe a que el ejemplo 14 (el \u00faltimo) es casi id\u00e9ntico al ejemplo de test por clasificar. En este ejemplo no se detallan todas las operaciones realizadas, sino un ejemplo de cada tipo: un ejemplo de la obtenci\u00f3n de P* para un atributo simb\u00f3lico, otro de la obtenci\u00f3n de P* para un atributo num\u00e9rico y otro para la obtenci\u00f3n de la probabilidad de transformaci\u00f3n del ejemplo de test en un ejemplo de entrenamiento. 3.5.6. Redes de Neuronas Las redes de neuronas constituyen una t\u00e9cnica inspirada en los trabajos de investigaci\u00f3n, iniciados en 1930, que pretend\u00edan modelar computacionalmente el aprendizaje humano llevado a cabo a trav\u00e9s de las neuronas en el cerebro [RM86, CR95]. Posteriormente se comprob\u00f3 que tales modelos no eran del todo adecuados para describir el aprendizaje humano. Las redes de neuronas constituyen una nueva forma de analizar la informaci\u00f3n con una diferencia fundamental con respecto a las t\u00e9cnicas tradicionales: son capaces de detectar y aprender complejos patrones y caracter\u00edsticas dentro de los datos [SN88, FU94]. Se comportan de forma parecida a nuestro cerebro aprendiendo de la experiencia y del pasado, y aplicando tal conocimiento a la resoluci\u00f3n de problemas nuevos. Este aprendizaje se obtiene como resultado del adiestramiento (\" training \") y \u00e9ste permite la sencillez y la potencia de adaptaci\u00f3n y evoluci\u00f3n ante una realidad cambiante y muy din\u00e1mica. Una vez adiestradas las redes de neuronas pueden hacer previsiones, clasificaciones y segmentaci\u00f3n. Presentan adem\u00e1s, una eficienc ia y fiabilidad similar a los m\u00e9todos estad\u00edsticos y sistemas expertos, si no mejor, en la mayor\u00eda de los casos. En aquellos casos de muy alta complejidad las redes neuronales se muestran como especialmente \u00fatiles dada la dificultad de modelado que supone para otras t\u00e9cnicas. Sin embargo las redes de neuronas tienen el inconveniente de la dificultad de acceder y comprender los modelos que generan y presentan dificultades para extraer reglas de tales modelos. Otra caracter\u00edstica es que son capaces de trabajar con datos incompletos e, incluso, contradictorios lo que, dependiendo del problema, puede resultar una ventaja o un inconveniente. Las redes de neuronas poseen las dos formas de aprendizaje: supervisado y no supervisado; ya comentadas [WI98], derivadas del tipo de paradigma que usan: el no supervisado (usa paradigmas como los ART \" Adaptive Resonance Theory \"), y el supervisado que suele usar el paradigma del \" Backpropagation \" [RHW86]. Las redes de neuronas est\u00e1n siendo utilizadas en distintos y variados sectores como la industria, el gobierno, el ej\u00e9rcito, las comunicaciones, la investigaci\u00f3n aerospacial, la banca y las finanzas, los seguros, la medicina, la distribuci\u00f3n, la rob\u00f3tica, el marketing, etc. En la actualidad se est\u00e1 estudiando la posibilidad de utilizar t\u00e9cnicas avanzadas y novedosas como los Algoritmos Gen\u00e9ticos para crear nuevos paradigmas que mejoren el adiestramiento y la propia selecci\u00f3n y dise\u00f1o de la Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 154 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda arquitectura de la red (n\u00famero de capas y neuronas), dise\u00f1o que ahora debe realizarse en base a la experiencia del analista y para cada problema concreto. Estructura de las Redes de Neuronas Las redes neuronales se construyen estructurando en una serie de niveles o capas (al menos tres: entrada, procesamient o u oculta y salida) compuestas por nodos o \"neuronas\", que tienen la estructura que se muestra en la figura 2.35. Figura 3.33: Estructura de una neurona. Tanto el umbral como los pesos son constantes que se inicializar\u00e1n aleatoriamente y durante el proceso de aprendizaje ser\u00e1n modificados. La salida de la neurona se define tal y como se muestra en las ecuaciones 2.82 y 2.83. UwX NETN 1iii+ = = Ec. 2.82 f(NET)S= Ec. 2.83 Como funci\u00f3n f se suele emplear una funci\u00f3n sigmoidal, bien definida entre 0 y 1 (ecuaci\u00f3n 2.84) o entre -1 y 1 (ecuaci\u00f3n 2.85). x-e11f(x)+= Ec. 2.84 x xx x eeeef(x) += Ec. 2.85 Cada neurona est\u00e1 conectada a todas las neuronas de las capas anterior y posterior a trav\u00e9s de los pesos o \"dendritas\", tal y como se muestra en la figura 2.36. Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 155 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.34: Estructura de la red de neuronas. Cuando un nodo recibe las entradas o \"est\u00edmulos\" de otras los procesa para producir una salida que transmite a la siguiente capa de neuronas. La se\u00f1al de salida tendr\u00e1 una intensidad fruto de la combinaci\u00f3n de la intensidad de las se\u00f1ales de entrada y de los pesos que las transmiten. Los pesos o dendritas tienen un valor distinto para cada par de neuronas que conectan pudiendo as\u00ed fortalecer o debilitar la conexi\u00f3n o comunicaci\u00f3n entre neuronas parti culares. Los pesos son modificados durante el proceso de adiestramiento. El dise\u00f1o de la red de neuronas consistir\u00e1, entre otras cosas, en la definici\u00f3n del n\u00famero de neuronas de las tres capas de la red. Las neuronas de la capa de entrada y las de la capa de salida vienen dadas por el problema a resolver, dependiendo de la codificaci\u00f3n de la informaci\u00f3n. En cuanto al n\u00famero de neuronas ocultas (y/o n\u00famero de capas ocultas) se determinar\u00e1 por prueba y error. Por \u00faltimo, debe tenerse en cuenta que la estructura de las neuronas de la capa de entrada se simplifica, dado que su salida es igual a su entrada: no hay umbral ni funci\u00f3n de salida. Proceso de adiestramiento (retropropagaci\u00f3n) Existen distintos m\u00e9todos o paradigmas mediante los cuales estos pesos pueden ser variados durante el adiestramiento de los cuales el m\u00e1s utilizado es el de retropropagaci\u00f3n [Backpropagation] [RHW86]. Este paradigma var\u00eda los pesos de acuerdo a las diferencias encontradas entre la salida obtenida y la que deber\u00eda obtenerse. De esta forma, si las diferencias son grandes se modifica el modelo de forma importante y seg\u00fan van siendo menores , se va convergiendo a un modelo final estable. El error en una red de neuronas para un patr\u00f3n [x= (x 1, x2, ..., x n), t(x)] , siendo x el patr\u00f3n de entrada, t(x) la salida deseada e y(x) la proporcionada por la red, se define como se muestra en la ecuaci\u00f3n 2.86 para m neuronas de salida y como se muestra en la ecuaci\u00f3n 2.87 para 1 neurona de salida. = Ec. 2.87 Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 156 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda El m\u00e9todo de descenso de gradiente consiste en modificar los par\u00e1metros de la red siguiendo la direcci\u00f3n negativa del gradiente del error. Lo que se realizar\u00eda mediante 2.88. we wwe w wanterior anterior nuevo = + = Ec. 2.88 En la ecuaci\u00f3n 2.88, w es el peso a modificar en la red de neuronas (pasando de wanterior a wnuevo) y es la raz\u00f3n de aprendizaje, que se encarga de controlar cu\u00e1nto se desplazan los pesos en la direcci\u00f3n negativa del gradiente. Influye en la velocidad de convergencia del algoritmo, puesto que determina la magnitud del desplazamiento. El algoritmo de retropropagaci\u00f3n es el resultado de aplicar el m\u00e9todo de descenso del gradiente a las redes de neuronas. El algoritmo completo de retropropagaci\u00f3n se muestra en la figura 2.37. Paso 1: Inicializaci\u00f3n aleatoria de los pesos y umbrales. Paso 2: Dado un patr\u00f3n del conjunto de entrenamiento (x, t(x)) , se presenta el vector xa la red y se calcula la salida de la red para dicho patr\u00f3n, y(x). Paso 3: Se eval\u00faa el error e(x) cometido por la red. Paso 4: Se modifican todos los par\u00e1metros de la red utilizando la ec.2.88. Paso 5: Se repiten los pasos 2, 3 y 4 para todos los patrones de entrenamiento, completando as\u00ed un ciclo de aprendizaje. Paso 6: Se realizan n ciclos de aprendizaje (pasos 2, 3, 4 y 5) hasta que se verifique el criterio de parada establecido. Figura 3.35: Pseudoc\u00f3digo del algoritmo de retropropagaci\u00f3n. En cuanto al criterio de parada, se debe calcular la suma de los errores en los patrones de entrenamiento. Si el error es constante de un ciclo a otro, los par\u00e1metros dejan de sufrir modificaciones y se obtiene as\u00ed el error m\u00ednimo. Por otro lado, tambi\u00e9n se debe tener en cuenta el error en los patrones de validaci\u00f3n, que se presentar\u00e1n a la red tras n ciclos de aprendizaje. Si el error en los patrones de validaci\u00f3n evoluciona favorablemente se contin\u00faa con el proceso de aprendizaje. Si el error no desciende, se detiene el aprendizaje. 3.5.7. L\u00f3gica borrosa (\"Fuzzy logic\") La l\u00f3gica borrosa surge de la necesidad de modelar la realidad de una forma m\u00e1s exacta evitando precisamente el determini smo o la exactitud [ZAD65, CPS98]. En Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 157 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda palabras menos pretenciosas lo que la l\u00f3gica borrosa permite es el tratamiento proba- bil\u00edstico de la categorizaci\u00f3n de un colectivo [ZAD65]. As\u00ed, para establecer una serie de grupos, segmentos o clases en los cuales se puedan clasificar a las personas por la edad, lo inmediato ser\u00eda proponer unas edades l\u00edmite para establecer tal clasificaci\u00f3n de forma disjunta. As\u00ed los ni\u00f1os ser\u00edan aquellos cuya edad fuera menor a los 12 a\u00f1os, los adolescentes aquellos entre 12 y 17 a\u00f1os, los j\u00f3venes aquellos entre 18 y 35, las personas maduras entre 36 y 45 a\u00f1os y as\u00ed sucesivamente. Se habr\u00edan creado unos grupos disjuntos cuyo tratamiento, a efectos de clasificaci\u00f3n y procesamiento, es muy sencillo: basta comparar la edad de cada persona con los l\u00edmites establecidos. Sin embargo enseguida se observa que esto supone una simplificaci\u00f3n enorme dado que una persona de 16 a\u00f1os 11 meses y veinte d\u00edas pertenecer\u00eda al grupo de los adolescentes y, seguramente, es m\u00e1s pareci- do a una persona de 18 (miembro de otro grupo) que a uno de 12 (miembro de su grupo). L\u00f3gicamente no se puede establecer un grupo para cada a\u00f1o, dado que s\u00ed se reconocen grupos, y no muchos, con comportamientos y actitudes similares en funci\u00f3n de la edad. Lo que impl\u00edcitamente se esta des cubriendo es que las clases existen pero que la frontera entre ellas no es clara ni disjunta sino \"difusa\" y que una persona puede tener aspectos de su mentalidad asociados a un grupo y otros asociados a otro grupo, es decir que impl\u00edcitamente se est\u00e1 distribuyendo la pertenencia entre varios grupos. Cuando esto se lleva a una formalizaci\u00f3n matem\u00e1tica surge el concepto de distribuci\u00f3n de posibilidad, de forma que lo que entender\u00eda como funci\u00f3n de pertenencia a un grupo de edad ser\u00edan unas curvas de posibilidad. Por tanto, la l\u00f3gica borrosa es aquella t\u00e9cnica que permite y trata la existencia de barreras difusas o suaves entre los distintos grupos en los que se categoriza un colectivo o entre los distintos elementos, factores o proporciones que concurren en una situaci\u00f3n o soluci\u00f3n [BS97]. Para identificar las \u00e1reas de utilizaci\u00f3n de la l\u00f3gica difusa basta con determinar cuantos problemas hacen uso de la categoriz aci\u00f3n disjunta en el tratamiento de los datos para observar la cantidad de posibles aplicaciones que esta t\u00e9cnica puede tener [ZAD65].. Sin embargo, el tratamiento ortodoxo y purista no siempre est\u00e1 justificado dada la complejidad que induce en el procesamiento (pasamos de valores a funciones de posibilidad) y un modelado sencillo puede ser m\u00e1s que suficiente. A\u00fan as\u00ed, existen problem\u00e1ticas donde este modelado s\u00ed resulta justificado, como en el control de procesos y la rob\u00f3tica, entre otros. Tal es as\u00ed que un pa\u00eds como Jap\u00f3n, l\u00edder en la industria y la automatizaci\u00f3n, dispone del \"Laboratory for International Fuzzy Engineering Research\" (LIFE) y empresas como Yamaichi Securities y Canon hacen un extenso uso de esta t\u00e9cnica. 3.5.8. T\u00e9cnicas Gen\u00e9ticas: Algoritmos Gen\u00e9ticos (\"Genetic Algorithms\") Los Algoritmos Gen\u00e9ticos son otra t\u00e9cnica que tiene su inspiraci\u00f3n, en la Biolog\u00eda como las Redes de Neuronas [GOLD89, MIC92, MITC96]. Estos algoritmos representan el modelado matem\u00e1tico de como los cromosomas en un marco evolucionista alcanzan la estructura y composici\u00f3n m\u00e1s \u00f3ptima en aras de la supervivencia. Entendiendo la evoluci\u00f3n como un proceso de b\u00fasqueda y optimizaci\u00f3n de la adaptaci\u00f3n de las especies que se plasma en mutaciones y cambios de los Cap\u00edtulo 3 T\u00e9cnicas de Miner\u00eda de Datos basadas en Aprendizaje Autom\u00e1tico T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 158 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda genes o cromosomas, los Algoritmos Gen\u00e9ticos hacen uso de las t\u00e9cnicas biol\u00f3gicas de reproducci\u00f3n (mutaci\u00f3n y cruce) para se r utilizadas en todo tipo de problemas de b\u00fasqueda y optimizaci\u00f3n. Se da la mutaci\u00f3n cuando alguno o algunos de los genes cambian bien de forma aleatoria o de forma controlada v\u00eda funciones y se obtiene el cruce cuando se construye una nueva soluci\u00f3n a partir de dos contribuciones procedentes de otras soluciones \"padre\". En cualquier caso, tales transformaciones se realizan sobre aquellos especimenes o soluciones m\u00e1s aptas o mejor adaptadas. Dado que los mecanismos biol\u00f3gicos de ev oluci\u00f3n han dado lugar a soluciones, los seres vivos, realmente id\u00f3neas cabe esperar que la aplicaci\u00f3n de tales mecanismos a la b\u00fasqueda y optimizaci\u00f3n de otro tipo de problemas tenga el mismo resultado. De esta forma los Algoritmos Gen\u00e9ticos transforman los problemas de b\u00fasqueda y optimizaci\u00f3n de soluciones un proceso de evoluci\u00f3n de unas soluciones de partida. Las soluciones se convierten en cromosomas, transformaci\u00f3n que se realiza pasando los datos a formato binario, y a los mejores se les van aplicando las reglas de evoluci\u00f3n (funciones probabil\u00edsticas de transici\u00f3n) hasta encontrar la soluci\u00f3n \u00f3ptima. En muchos casos, estos mecanismos brindan posibilidades de convergencia m\u00e1s r\u00e1pidos que otras t\u00e9cnicas. El uso de estos algoritmos no est\u00e1 tan extendido como otras t\u00e9cnicas, pero van siendo cada vez m\u00e1s utilizados directamente en la soluci\u00f3n de problemas, as\u00ed como en la mejora de ciertos procesos presentes en otras herramientas. As\u00ed, por ejemplo, se usan para mejorar los procesos de adiestramiento y selecci\u00f3n de arquitectura de las redes de neuronas, para la generaci\u00f3n e inducci\u00f3n de \u00e1rboles de decisi\u00f3n y para la s\u00edntesis de programas a partir de ejemplos (\"Genetic Programming\"). Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 159 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Cap\u00edtulo 4. T\u00e9cnicas de An\u00e1lisis de Datos en Weka Introducci\u00f3n En este cap\u00edtulo se presenta de forma concisa y pr\u00e1ctica la herramienta de miner\u00eda de datos WEKA. WEKA, acr\u00f3nimo de Waikato Environment for Knowledge Analysis , es un entorno para experimentaci\u00f3n de an\u00e1lisis de datos que permite aplicar, analizar y evaluar las t\u00e9cnicas m\u00e1s relevantes de an\u00e1lisis de datos, principalmente las provenient es del aprendizaje autom\u00e1tico, sobre cualquier conjunto de datos del usuario. Para ello \u00fan icamente se requiere que los datos a analizar se almacenen con un cierto formato, conocido como ARFF (Attribute-Relation File Format ). WEKA se distribuye como software de lib re distribuci\u00f3n desarrollado en Java. Est\u00e1 constituido por una serie de paquet es de c\u00f3digo abierto con diferentes t\u00e9cnicas de preprocesado, clasificac i\u00f3n, agrupamiento, asociaci\u00f3n, y visualizaci\u00f3n, as\u00ed como facilidades para su aplicaci\u00f3n y an\u00e1lisis de prestaciones cuando son aplicadas a los datos de ent rada seleccionados. Estos paquetes pueden ser integrados en cualquier proyec to de an\u00e1lisis de datos, e incluso pueden extenderse con contribuciones de los usuarios que desarrollen nuevos algoritmos. Con objeto de facilitar su uso por un mayor n\u00famero de usuarios, WEKA adem\u00e1s incluye una interfaz gr \u00e1fica de usuario para acceder y configurar las diferentes herramientas integradas. Este cap\u00edtulo tiene un enfoque pr\u00e1ctico y f uncional, pretendiendo servir de gu\u00eda de utilizaci\u00f3n de esta herramienta desde su interfaz gr\u00e1fica, como material complementario a la escasa documentaci\u00f3n disponible. Para ello se obviar\u00e1n los detalles t\u00e9cnicos y espec\u00edficos de los diferentes algoritmos, que se presentan en un cap\u00edtulo aparte, y se centrar\u00e1 en su aplicaci\u00f3n, configuraci\u00f3n y an\u00e1lisis dentro de la herramient a. Por tanto, se remite al lector al cap\u00edtulo con los detalles de los algoritmos para conocer sus caracter\u00edsticas, par\u00e1metros de configuraci\u00f3n, etc. A qu\u00ed se han seleccionado algunas de las t\u00e9cnicas disponibles para aplicarlas a ejemplos c oncretos, siguiendo el acceso desde la herramienta al resto de t\u00e9cnicas implementadas, una mec\u00e1nica totalmente an\u00e1loga a la presentada a modo ilustrativo. Para reforzar el car\u00e1cter pr\u00e1ctico de este cap\u00edtulo, adem\u00e1s se adoptar\u00e1 un formato de tipo tutorial, con un conjunto de datos disponibles sobre el que se ir\u00e1n aplicando las diferentes facilidades de WEKA. Se sugiere que el lector Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 160 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda aplique los pasos indicados y realice lo s an\u00e1lisis sugeridos para cada t\u00e9cnica con objeto de familiarizarse y mejorar su comprensi\u00f3n. Los ejemplos seleccionados son contienen datos proven ientes del campo de la ense\u00f1anza, correspondientes a alumnos que realizar on las pruebas de se lectividad en los a\u00f1os 1993-2003 procedentes de diferentes centros de ense\u00f1anza secundaria de la comunidad de Madrid. Por tanto, es ta gu\u00eda ilustra la apl icaci\u00f3n y an\u00e1lisis de t\u00e9cnicas de extracci\u00f3n de conocimi ento sobre datos del campo de la ense\u00f1anza, aunque ser\u00eda directa su traslaci\u00f3n a cualquier otra disciplina. Preparaci\u00f3n de los datos Los datos de entrada a la herramienta, sobre los que operar\u00e1n las t\u00e9cnicas implementadas, deben estar codificados en un formato espec\u00edfico, denominado Attribute-Relation File Format (extensi\u00f3n \"arff\"). La he rramienta permite cargar los datos en tres soportes: fichero de texto, acceso a una base de datos y acceso a trav\u00e9s de internet sobre una direcci\u00f3n URL de un servidor web. En nuestro caso trabajaremos con ficheros de texto. Los datos deben estar dispuestos en el fichero de la forma si guiente: cada instancia en una fila, y con los atributos separados por comas. El formato de un fichero arff sigue la estructura siguiente: % comentarios atributos pueden ser principalmente de dos tipos: num\u00e9ricos de tipo real o entero (indicado con las palabra real o integer tras el nombre del atributo), y simb\u00f3licos, en cuyo caso se especifican los valores posibles que puede tomar entre llaves. Muestra de datos El fichero de datos objeto de an\u00e1lisis en esta gu\u00eda contiene muestras correspondientes a 18802 alum nos presentados a las prue bas de selectividad y los resultados obtenidos en las pruebas. Los datos que describen cada alumno contienen la siguiente informaci\u00f3n: a\u00f1o, convocatoria, localidad del centro, opci\u00f3n cursada (de 5 posibles), calificac iones parciales obtenidas en lengua, Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 161 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda historia, idioma y las tres asignatura s opcionales, as\u00ed como la designaci\u00f3n de las asignaturas de idioma y las 3 opc ionales cursadas, calificaci\u00f3n en el bachillerato, calificaci\u00f3n final y si el alumno se pres ent\u00f3 o no a la prueba. Por tanto, puede comprobarse que la c abecera del fichero de datos, \"selectividad.arff\", sigue el ... Objetivos del an\u00e1lisis Antes de comenzar con la aplicaci\u00f3n de las t\u00e9cnicas de WEKA a los datos de este dominio, es muy conveniente hacer una consideraci\u00f3n acerca de los objetivos perseguidos en el an\u00e1lisis. Co mo se mencion\u00f3 en la introducci\u00f3n, un paso previo a la b\u00fasqueda de relaciones y modelos subyacentes en los datos ha de ser la comprensi\u00f3n del dominio de aplicaci\u00f3n y establecer una idea clara acerca de los objetivos del usuario final. De esta manera, el proceso de an\u00e1lisis de datos (proceso KDD ), permitir\u00e1 dirigir la b\u00fas queda y hacer refinamientos, con una interpretaci\u00f3n adecuada de los resultados generados. Los objetivos, utilidad, aplicaciones, etc., del an\u00e1lisis efectuado no \"emergen\" de los datos, sino que deben ser considerados con de tenimiento como primer paso del estudio. En nuestro caso, uno de los objetivos perseguidos podr\u00eda ser el intentar relacionar los resultados obtenidos en las pruebas con caracter\u00edsticas o perfiles de los alumnos, si bien la descripci\u00f3n disponible no es muy rica y habr\u00e1 que atenerse a lo que est\u00e1 disponible . Algunas de las pregunt as que podemos plantearnos a responder como objetivos del an\u00e1lisis podr\u00edan ser las siguientes: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 162 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u00bfQu\u00e9 caracter\u00edsitcas comunes tienen lo s alumnos que superan la prueba? \u00bfy los alumnos mejor preparados que la superan sin perjudicar su expediente? \u00bfexisten grupos de alumnos, no conocidos de antemano, con caracter\u00edsticas similares? \u00bfhay diferencias significativas en los resultados obtenidos seg\u00fan las opciones, localidades, a\u00f1os, etc.?, \u00bfla opci\u00f3n seleccionada y el resu ltado est\u00e1 influida depende del entorno? \u00bfse puede predecir la calificaci\u00f3n del alumno con alguna variable conocida? \u00bfqu\u00e9 relaciones entre variables son las m\u00e1s significativas? Como veremos, muchas veces el re sultado alcanzado puede ser encontrar relaciones triviales o conocidas previa mente, o puede ocurrir que el hecho de no encontrar relaciones significativas , lo puede ser muy relevante. Por ejemplo, saber despu\u00e9s de un an\u00e1lisis ex haustivo que la opci\u00f3n o localidad no condiciona significativamente la calific aci\u00f3n, o que la prueba es homog\u00e9nea a lo largo de los a\u00f1os, puede ser una conclusi\u00f3n valiosa, y en este caso \"tranquilizadora\". Por otra parte, este an\u00e1lisis tiene un enfoque introductorio e ilustrativo para acercarse a las t\u00e9cnicas disponibles y su manipulaci\u00f3n desde la herramienta, dejando abierto para el investigador lle var el estudio de este dominio a resultados y conclusiones m\u00e1s elaboradas. Ejecuci\u00f3n de WEKA WEKA se distribuye como un fichero ej ecutable comprimido de java (fichero \"jar\"), que se invoca directamente s obre la m\u00e1quina virtual JVM. En las primeras versiones de WEKA se requer\u00eda la m\u00e1quina virtur al Java 1.2 para invocar a la interfaz gr\u00e1fica, desa rrollada con el paquete gr\u00e1fico de Java Swing . En el caso de la \u00faltimo versi\u00f3n, W EKA 3-4, que es la que se ha utilizado para confeccionar estas notas, se requiere Java 1.3 o superior. La herramienta se invoca desde el int\u00e9rprete de Java, en el caso de utilizar un entorno windows, bastar\u00eda una ventana de comandos para invocar al int\u00e9prete Java: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 163 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Una vez invocada, aparece la ventana de entrada a la interfaz gr\u00e1fica ( GUI- Chooser ), que nos ofrece cuatro opciones posibles de trabajo: Simple CLI : la interfaz \"Command-Line Interfaz\" es simplemente una ventana de comandos java para ejecutar las clases de WEKA. La primera di stribuci\u00f3n de WEKA no dispon\u00eda de interfaz gr\u00e1fica y las clases de sus paquetes se pod\u00edan ejecutar desde la l\u00ednea de comandos pasando los argumentos adecuados. Explorer : es la opci\u00f3n que permite llevar a cabo la ejecuci\u00f3n de los algoritmos de an\u00e1lisis implementados s obre los ficheros de entrada, una ejecuci\u00f3n independiente por cada prueba. Esta es la opci\u00f3n sobre la que se centra la totalidad de esta gu\u00eda. Experimenter : esta opci\u00f3n permite definir experimentos m\u00e1s complejos, con objeto de ejecutar uno o varios algoritmos sobre uno o varios conjuntos de datos de entrada, y comparar estad\u00edsticamente los resultados KnowledgeFlow : esta opci\u00f3n es una novedad de WEKA 3-4 que permite llevar a cabo las mismas acciones del \"Explorer\", con una configuraci\u00f3n totalm ente gr\u00e1fica, inspirada en herramientas de tipo \"data-flow\" para seleccionar componentes y conectarlos en un proyecto de miner\u00eda de datos, desde que se cargan los datos, se aplican algoritmos de tratmiento y an\u00e1lisis, hasta el tipo de evaluaci\u00f3n deseada. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 164 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda En esta gu\u00eda nos centraremos \u00fani camente en la segunda opci\u00f3n, Explorer . Una vez seleccionada, se crea una ventana con 6 pesta\u00f1as en la parte superior que se corresponden con diferentes tipos de operaciones, en etapas independientes, que se pueden realizar sobre los datos: Preprocess : seleccion de la fuente de dat os y preparaci\u00f3n (filtrado). Clasify : Facilidades para aplicar esquemas de clasificaci\u00f3n, entrenar modelos y evaluar su precisi\u00f3n Cluster : Algoritmos de agrupamiento Associate : Algoritmos de b\u00fasqueda de reglas de asociaci\u00f3n Select Attributes : B\u00fasqueda supervisada de s ubconjuntos de atributos representativos Visualize : Herramienta interactiva de presentaci\u00f3n gr\u00e1fica en 2D. Adem\u00e1s de estas pesta\u00f1as de selecci\u00f3n, en la parte inferi or de la ventana aparecen dos elementos comunes. Uno es el bot\u00f3n de \" Log\", que al activarlo presenta una ventana text ual donde se indica la secuencia de todas las operaciones que se han llevado a cabo den tro del \"Explorer\", sus tiempos de inicio y fin, as\u00ed como los mensajes de error m\u00e1s frecuentes. Junto al bot\u00f3n de log aparece un icono de actividad (el p\u00e1jaro WEKA, que se mueve cuando se est\u00e1 realizando alguna tarea) y un indica dor de status, que indica qu\u00e9 tarea se est\u00e1 realizando en este momento dentro del Explorer. Preprocesado de los datos Esta es la parte primera por la que se debe pasar antes de realizar ninguna otra operaci\u00f3n, ya que se precisan da tos para poder llevar a cabo cualquier an\u00e1lisis. La disposici\u00f3n de la parte de preprocesado del Explorer, Preprocess , es la que se indica en la figura siguiente. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 165 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Cargar datos, guar dar datos filtrados Selecci\u00f3n y aplicaci\u00f3n de filtros Atributos en la relaci\u00f3n actual Propiedades del atributo seleccionado Como se indic\u00f3 anteriormente, hay tres posibilidades para obtener los datos: un fichero de texto, una direcci\u00f3n URL o una base de datos, dadas por las opciones: Open file , Open URL y Open DB . En nuestro caso utilizaremos siempre los datos almacenados en un fic hero, que es lo m\u00e1s r\u00e1pido y c\u00f3modo de utilizar. La preparaci\u00f3n del fichero de datos en formato ARFF ya se describi\u00f3 en la secci\u00f3n 1.2. En el ejemplo que nos ocupa, abra el fich ero \"selectividad.arff\" con la opci\u00f3n Open File . Caracter\u00edsticas de los atributos Una vez cargados los datos, aparece un cuadro resumen, Current relation , con el nombre de la relaci\u00f3n que se indica en el fichero (en la l\u00ednea @relation del fichero arff), el n\u00famero de instancias y el n\u00famero de atributos. M\u00e1s abajo, aparecen listados todos los atributos disponibles, con los nombres especificados en el fichero, de modo q ue se pueden seleccionar para ver sus detalles y propiedades. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 166 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda En la parte derecha aparec en las propiedades del atribut o seleccionado. Si es un atributo simb\u00f3lico, se presenta la di stribuci\u00f3n de valore s de ese atributo (n\u00famero de instancias que tienen cada u no de los valores). Si es num\u00e9rico aparece los valores m\u00e1ximo, m\u00ednimo, valo r medio y desviaci\u00f3n est\u00e1ndar. Otras caracter\u00edsticas que se destacan del atributo seleccionado son el tipo ( Type ), n\u00famero de valores distintos ( Distinct ), n\u00famero y porcent aje de instancias con valor desconocido para el atributo ( Missing , codificado en el fichero arff con \"?\"), y valores de atributo que sola mente se dan en una instancia ( Unique ). Adem\u00e1s, en la parte inferior se pres enta gr\u00e1ficamente el histograma con los valores que toma el atributo. Si es si mb\u00f3lico, la distribuci\u00f3n de frecuencia de los valores, si es num\u00e9rico, un histogr ama con intervalos uniformes. En el histograma se puede presentar adem\u00e1s con colores distin tos la distribuci\u00f3n de un segundo atributo para cada valor del at ributo visualizado. Por \u00faltimo, hay un bot\u00f3n que permite visualizar los hi stogramas de todos los atributos simult\u00e1neamente. A modo de ejemplo, a cont inuaci\u00f3n mostramos el hi stograma por localidades, indicando con colores la distribuciones por opciones elegidas. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 167 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Se ha seleccionado la columna de la localidad de Legan\u00e9s, la que tiene m\u00e1s instancias, y donde puede verse que la pr oporci\u00f3n de las opciones cient\u00edficas (1 y 2) es superior a otras localidades , como Getafe, la segunda localidad en n\u00famero de alumnos presentados. Visualice a continuaci\u00f3n los histogramas de las calificaciones de bachillerato y calificaci\u00f3n final de la prueba, indicando como segundo atributo la convocatoria en la que se presentan los alumnos. Trabajo con Filtros. Preparaci\u00f3n de ficheros de muestra WEKA tiene integrados filtros que permit en realizar manipulaciones sobre los datos en dos niveles: atributos e instanc ias. Las operaciones de filtrado pueden aplicarse \"en cascada\", de manera que c ada filtro toma como entrada el conjunto de datos resultant e de haber aplicado un filt ro anterior. Una vez que se ha aplicado un filtro, la relaci\u00f3n ca mbia ya para el resto de operaciones llevadas a cabo en el Experimenter , existiendo siempre la opci\u00f3n de deshacer la \u00faltima operaci\u00f3n de f iltrado aplicada con el bot\u00f3n Undo . Adem\u00e1s, pueden guardarse los resultados de aplicar f iltros en nuevos ficheros, que tambi\u00e9n ser\u00e1n de tipo ARFF, para mani pulaciones posteriores. Para aplicar un filtro a los dato s, se selecciona con el bot\u00f3n Choose de Filter , despleg\u00e1ndose el \u00e1rbol con todos los que est\u00e1n integrados. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 168 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Puede verse que los filtros de esta opc i\u00f3n son de tipo no supervisado (unsupervised ): son operaciones independientes del algoritmo an\u00e1lisis posterior, a diferencia de los filtros s upervisados que se ver\u00e1n en la secci\u00f3n 1.9 de \"selecci\u00f3n de atributos\", que operan en conjunci\u00f3n con algoritmos de clasificaci\u00f3n para analizar su efecto . Est\u00e1n agrupados seg\u00fan modifiquen los atributos resultantes o seleccionen un s ubconjunto de instancias (los filtros de atributos pueden verse como filtros \"vertical es\" sobre la tabla de datos, y los filtros de instancias como filtros \"hor izontales\"). Como puede verse, hay m\u00e1s de 30 posibilidades, de las que destacare mos \u00fanicamente algunas de las m\u00e1s frecuentes. Filtros de atributos Vamos a indicar, de entre todas las po sibilidades implementadas , la utilizaci\u00f3n de filtros para eliminar at ributos, para discretizar at ributos num\u00e9ricos, y para a\u00f1adir nuevos atributos con expresiones, por la frecuencia con la que se realizan estas operaciones. Filtros de selecci\u00f3n Vamos a utilizar el filtro de atributos \" Remove \", que permite eliminar una serie de atributos del conjunt o de entrada. En primer lugar procedemos a seleccionarlo desde el \u00e1rbol desplegado con el bot\u00f3n Choose de los filtros. A continuaci\u00f3n lo configuraremos para det erminar qu\u00e9 atributos queremos filtrar. La configuraci\u00f3n de un filtro sigue el esquema general de configuraci\u00f3n de cualquier algoritmo integrado en WEKA . Una vez seleccionado el filtro espec\u00edfico con el bot\u00f3n Choose , aparece su nombre dentro del \u00e1rea de filtro (el lugar donde antes aparec\u00eda la palabra None ). Se puede configurar sus par\u00e1metros haciendo clic sobre esta \u00e1rea, momento en el que aparece la ventana de configuraci\u00f3n corre spondiente a ese filtro part icular. Si no se realiza esta operaci\u00f3n se utilizar\u00edan los valore s por defecto del filtro seleccionado. Como primer filtro de sele cci\u00f3n, vamos a eliminar de los atributos de entrada todas las calificaciones parci ales de la prueba y la calif icaci\u00f3n final, quedando como \u00fanicas calificaciones la nota de bach illerato y la calific aci\u00f3n de la prueba. Por tanto tenemos que seleccionar los \u00edndices 5,6,7,10,12,14 y 17, indic\u00e1ndolo en el cuadro de configuraci\u00f3n del filtro Remove : Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 169 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Como puede verse, en el conjunto de at ributos a eliminar se pueden poner series de valores contiguos delimitados por gui\u00f3n (5-7) o valores sueltos entre comas (10,12,14,17). Adem\u00e1s , puede usarse \"first\" y \"l ast\" para indicar el primer y \u00faltimo atributo, respectivamente. La opci\u00f3n invertSelection es \u00fatil cuando realmente queremos seleccionar un peque\u00f1o subconjunto de todos los atributos y eliminar el resto. Open y Save nos permiten guardar configuraciones de inter\u00e9s en archivos. El boton More , que aparece opcionalmente en algunos elementos de WEKA, muestra informaci\u00f3n de utilidad acerca de la configuraci\u00f3n de los mismos. Estas convenciones para designar y seleccionar atributos, ayuda, y para guardar y cargar configuraciones espec\u00edficas es co m\u00fan a otros elementos de WEKA. Una vez configurado, al accionar el bot\u00f3n Apply del \u00e1rea de filtros se modifica el conjunto de datos (se filtra) y se genera una relaci\u00f3n transformada. Esto se hace indicar en la descripci\u00f3n \"Current Relation\", que pasa a ser la resultante de aplicar la operaci\u00f3n co rrespondiente (esta informaci\u00f3n se puede ver con m\u00e1s nitidez en la ventana de log, que adem\u00e1s nos indicar\u00e1 la cascada de filtros aplicados a la relaci\u00f3n operativa). La rela ci\u00f3n transformada tras aplicar el filtro podr\u00eda almacenarse en un nuevo fichero ARFF con el bot\u00f3n Save , dentro de la ventana Preprocess. Filtros de discretizaci\u00f3n Estos filtros son muy \u00fatiles cuando se trabaja con atributos num\u00e9ricos, puesto que muchas herramientas de an\u00e1lisis requi eren datos simb\u00f3licos , y por tanto se necesita aplicar esta transformaci\u00f3n antes. Tambi\u00e9n son necesarios cuando queremos hacer una clasif icaci\u00f3n sobre un atributo num\u00e9rico, por ejemplo clasificar los alumnos apr obados y suspensos. Este filtrado transforma los atributos num\u00e9ricos seleccionados en at ributos simb\u00f3licos, con una serie de etiquetas resultantes de divi dir la amplitud total del atributo en intervalos, con diferentes opciones para seleccionar lo s l\u00edmites. Por defecto, se divide la amplitud del intervalo en tantas \"cajas\" como se indique en bins (por defecto 10), todas ellas de la misma amplitud. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 170 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Por ejemplo, para discretiz ar las calificaciones num\u00e9ricas en 4 categor\u00edas, todas de la misma amplitud, se configurar\u00eda as\u00ed: observe el resultado despu\u00e9s de aplicar el filtro y los l\u00edmites elegidos para cada atributo. En este caso se ha aplicado a todos los atributos num\u00e9ricos con la misma configuraci\u00f3n (los atributos selecc ionados son first-last, no considerando los atributos que antes del filtrado no eran num\u00e9ricos). Observe que la relaci\u00f3n de trabajo ahora (\"current relation\") ahora es el resultado de aplicar en secuencia el filtro anterior y el actual. A veces es m\u00e1s \u00fatil no fijar todas las cajas de la misma anchura sino forzar a una distribuci\u00f3n uniforme de instancia s por categor\u00eda, con la opci\u00f3n useEqualFrequency . La opci\u00f3n findNumBins permite opimizar el n\u00famero de cajas (de la misma amplitud), con un criter io de clasificaci\u00f3n de m\u00ednimo error en funci\u00f3n de las etiquetas. Haga una nueva discretizaci\u00f3n de la rela ci\u00f3n (eliminando el ef ecto del filtro anterior y dejando la relaci\u00f3n original con el bot\u00f3n Undo ) que divida las calificaciones en 4 intervalos de la mism a frecuencia, lo que permite determinar los cuatro cuartiles (intervalos al 25% ) de la calificaci\u00f3n en la prueba: los intervalos delimitados por lo s valores {4, 4.8, 5.76} Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 171 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda podemos ver que el 75% alcanza la nota de compensaci\u00f3n (4). El 50% est\u00e1 entre 4 y 5.755, y el 25% restante a partir de 5.755. Filtros de a\u00f1adir expresiones Muchas veces es interesante incluir n uevos atributos resultantes de aplicar expresiones a los existentes, lo que puede traer informaci\u00f3n de inter\u00e9s o formular cuestiones interesantes sobre lo s datos. Por ejemplo, vamos a a\u00f1adir como atributo de inter\u00e9s la \"mejora\" sobr e la nota de bachillerato, lo que puede servir para calificar el \"\u00e9xito\" en la pr ueba. Seleccionamos el filtro de atributos AddExpression , configurado para obtener la di ferencia entre los atributos calificaci\u00f3n en la prueba, y nota de bac hillerato, en las posiciones15 y 16: despu\u00e9s de aplicarlo aparece este atributo en la relaci \u00f3n, ser\u00eda el n\u00famero 19, con el histograma indicado en la figura: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 172 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Filtros de instancias De entre todas las posibi lidades implementadas para filtros de selecci\u00f3n de instancias (selecci\u00f3n de r angos, muestreos, etc.), nos centraremos en la utilizaci\u00f3n de filtros para seleccionar instancias cuyos atributos cumplen determinadas condiciones. Selecci\u00f3n de instancias con condiciones sobre atributos Vamos a utilizar el filtro RemoveWithValues , que elimina las instancias de acuerdo a condiciones definid as sobre uno de los atributos. Las opciones que aparecen en la ventana de configuraci\u00f3n son las indicadas a continuaci\u00f3n. el atributo utilizado para filtrar se indi ca en \"attributeIndex\". Si es un atributo nominal, se indican los valores a filtrar en el \u00faltimo par\u00e1metro, \"nominalIndices\". Si es num\u00e9rico, se filtran las instancias con un valor inferior al punto de corte, \"splitPoint\". Se puede invertir el cr iterio de filtrado mediante el campo \"invertSelection\". Este filtro permite verificar una cond ici\u00f3n simple sobre un atributo. Sin embargo, es posible hacer un filtrado m\u00e1s complejo sobre varias condiciones aplicadas a uno o varios atributos sin m\u00e1s que aplicar en cascada varios filtros Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 173 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda A modo de ejemplo, utilice tres filtros de este tipo para seleccionar los alumnos de Getafe y Legan\u00e9s con una calificaci\u00f3 n de la prueba entre 6.0 y 8.0. Compruebe el efecto de filtrado visua lizando los histogramas de los atributos correspondientes (localidad y calificaci\u00f3n en la prueba), tal y como se indica en la figura siguiente: Visualizaci\u00f3n Una de las primeras etapas del an\u00e1lisis de datos puede ser el mero an\u00e1lisis visual de \u00e9stos, en ocas iones de gran utilidad para desvelar relaciones de inter\u00e9s utilizando nuestra capacid ad para comprender im\u00e1genes. La herramienta de visualizaci\u00f3n de WEKA pe rmite presentar gr\u00e1ficas 2D que relacionen pares de atributos, con la opc i\u00f3n de utilizar adem\u00e1s los colores para a\u00f1adir informaci\u00f3n de un tercer atributo. Adem\u00e1s, tiene incorporada una facilidad interactiva para seleccionar instancias con el rat\u00f3n. Representaci\u00f3n 2D de los datos Las instancias se pueden vi sualizar en gr\u00e1ficas 2D que relacionen pares de atributos. Al sele ccionar la opci\u00f3n Visualize del Explorer aparecen todas los pares posibles de atributos en las coordenadas horizonta l y vertical. La idea es que se selecciona la gr\u00e1fica deseada par a verla en detalle en una ventana nueva. En nuestro caso, aparecer\u00e1n todas las combinaciones posibles de atributos. Como primer ejemplo vamos a visualizar el rango de calificaciones finales de los alumnos a lo largo de lo s a\u00f1os, poniendo la convocatoria (junio o septiembre) como color de la gr\u00e1fica. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 174 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Vamos a visualizar ahora dos variables cuya relaci\u00f3n es de gran inter\u00e9s, la calificaci\u00f3n de la prueba en funci\u00f3n de la nota de bachillerato, y tomando como color la convocatoria (junio o septiembre). en esta gr\u00e1fica podemos apreciar la re laci\u00f3n entre ambas magnitudes, que si bien no es directa al menos define una cierta tendenci a creciente, y como la convocatoria est\u00e1 bastante rela cionada con ambas calificaciones. Cuando lo que se relacionan son variab les simb\u00f3licas, se presentan sus posibles valores a lo largo del eje. Sin embargo, en esto s casos todas las instancias que comparten cada valor de un atributo simb\u00f3lico pueden ocultarse Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 175 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda (ser\u00edan un \u00fanico punto en el plano), raz\u00f3n por la que se utiliza la facilidad de Jitter . Esta opci\u00f3n permite introducir un de splazamiento aleatorio (ruido) en las instancias, con objeto de poder visualizar todas aque llas que comparten un par de valores de atributos simb\u00f3licos, de manera que puede visualizarse la proporci\u00f3n de instancias que aparece en c ada regi\u00f3n. A modo de ejemplo se muestra a continuaci\u00f3n la relaci\u00f3n entre las tres asignaturas optativas, y con la opci\u00f3n cursada como color Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 176 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda puede verse una marcada relaci\u00f3n entre las asignaturas opcionales, de manera que este gr\u00e1fico ilustra qu\u00e9 tipo de asignaturas engloba cada una de las cinco posibles opciones cursadas. Se sugiere preparar el sigu iente gr\u00e1fico, que relaciona la calificaci\u00f3n obtenida en la prueba con la localidad de origen y la nota de bachillerato, estando las calificaciones discretizadas en intervalos de amplitud 2 Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 177 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Aqu\u00ed el color trae m\u00e1s informaci\u00f3n, pues indica en cada intervalo de calificaciones de la prueba, la calificaci\u00f3n en bachillera to, lo que permite ilustrar la \"satisfacci\u00f3n\" con la calificaci\u00f3n en la prueba o resultados no esperados, adem\u00e1s distribuido por localidades. Filtrado \"gr\u00e1fico\" de los datos WEKA permite tambi\u00e9n realizar filtro s de selecci\u00f3n de instancias sobre los propios gr\u00e1ficos, con una interacci\u00f3n a trav \u00e9s del rat\u00f3n para aislar los grupos de instancias cuyos atributos cumplen determinadas condiciones. Esta facilidad permite realizar filtrados de instancia s de modo interactivo y m\u00e1s intuitivo que los filtros indicados en la secci\u00f3n 1.4. 2.2. Las opciones que existen son: Selecci\u00f3n de instancias con un valo r determinado (hacer clic sobre la posici\u00f3n en el gr\u00e1fico) Selecci\u00f3n con un rect\u00e1ngulo de un subconjunto de combinaciones (comenzando por el v\u00e9rtice superior izquierdo) ( Rectangle ) Selecci\u00f3n con un pol\u00edgono cerrado de un subconjunto ( Polygon ) Selecci\u00f3n con una l\u00edne a abierta ( Polyline ) Por ejemplo, a continuaci \u00f3n se indica la selecci\u00f3n de alumnos que obtuvieron una calificaci\u00f3n por debajo de sus expecta tivas (calificaci\u00f3n en la prueba inferior a su nota en el bachillerato), con la opci\u00f3n Polygon . Una vez realizada la selecci\u00f3n, la opci\u00f3n Submit permite eliminar el resto de instancias, y Save almacenarlas en un fichero. Reset devuelve la relaci\u00f3n a su estado original. Utilice estas facilidades gr\u00e1ficas para hac er subconjuntos de los datos con los alumnos aprobados de las opciones 1 y 2 frente a los de las opciones 3, 4 y 5. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 178 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Salve las relaciones filt radas para a continuaci\u00f3n cargarlas y mostrar los histogramas, que aparecer\u00e1n como se indica en la figura siguiente. Asociaci\u00f3n Los algoritmos de asociaci\u00f3n permiten la b\u00fasqueda autom\u00e1tica de reglas que relacionan conjuntos de at ributos entre s\u00ed. Son algoritmos no supervisados, en el sentido de que no existen relacione s conocidas a priori con las que contrastar la validez de los resultados, sino que se eval\u00faa si esas reglas son estad\u00edsticamente significativas. La ventana de Asociaci\u00f3n ( Associate en el Explorer), tiene los siguiente elementos: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 179 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Selecci\u00f3n y configuraci\u00f3n del algoritmo de asociaci\u00f3n Visualizaci\u00f3n de resultados y almacenamient oResultados (en texto) El principal algoritmo de asociaci\u00f3n implementado en WEKA es el algoritmo \"Apriori\". Este algoritmo \u00fanicament e puede buscar reglas entre atributos simb\u00f3licos, raz\u00f3n por la que se requier e haber discretizado todos los atributos num\u00e9ricos. Por simplicidad, vamos a aplicar un filtro de discretizaci\u00f3n de todos los atributos num\u00e9ricos en cuatro intervalos de la misma frecuencia para explorar las relaciones m\u00e1s significativas. El algor itmo lo ejecutamos con sus par\u00e1metros por defecto. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 180 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda las reglas que aparecen aportan poca info rmaci\u00f3n. Aparecen en primer lugar las relaciones triviales entre asignaturas y opciones, as\u00ed como las que relacionan suspensos en la prueba y en la calificaci\u00f3n final. En cuanto a las que relacionan alumnos pres entados con idioma seleccionado son debidas a la fuerte descompensaci\u00f3n en el idioma se leccionado. Lla abrumadora mayor\u00eda de los presentados a la prueba de idioma seleccionaron el ingl\u00e9s, como indica la figura siguiente: Con objeto de buscar relaciones no cono cidas, se filtrar\u00e1n ahora todos los atributos relacionados con descriptore s de asignaturas y calificaciones parciales, quedando \u00fanicamente los atributos: A\u00f1o_acad\u00e9mico convocatoria localidad opcion1\u00aa cal_prueba nota_bachi En este caso, T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 181 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda estas reglas aportan informaci\u00f3n no tan tr ivial: el 99% de alumnos con nota superior a 8 se presentan a la convocatoria de Junio, as\u00ed el 95% de los alumnos con calificaci\u00f3n en la prueba entre 5.772 y 7. es significativo ver que no aparece ninguna relaci\u00f3n importante entre las calificaciones, localidad y a\u00f1o de la conv ocatoria. Tambi\u00e9n es destacado ver la ausencia de efecto de la opci\u00f3n cursada. Si preparamos los datos para dejar s\u00f3lo ci nco atributos, A\u00f1o_acad\u00e9mico convocatoria localidad opcion1\u00aa cal_fina l, con el \u00faltimo discretizado en dos grupos iguales (hasta 5.85 y 5.85 hasta 10), tenemos que de nuevo las reglas m\u00e1s significativas relacionan convocatoria con calificaci\u00f3n, pero ahora entran en juego opciones y loca lidades, si bien bajando la precisi\u00f3n de T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 182 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Al filtrar la convocatoria, que nos origina relaciones bastante evidentes, tendremos las reglas m\u00e1s significativa s entre localidad, a\u00f1o, calificaci\u00f3n y opci\u00f3n. Como podemos ver, al lanzar el algoritmo con los par\u00e1metros por defecto no aparece ninguna regla. Esto es debido a que se forz\u00f3 como umbral m\u00ednimo aceptable para una regla el 90%. Vamos a bajar ahora este par\u00e1metro hasta el 50%: tanto, forzando los t\u00e9rminos, tenemos que los estudiantes de las 2 primeras opciones tienen mayor probabilidad de aprobar la prueba, as\u00ed como los estudiantes de la localidad de Legan\u00e9s. Los estudiantes de Getafe tienen una probabilidad superior de obten er una calificaci\u00f3n inferi or. Hay que destacar que estas reglas rozan el umbral del 50% , pero han sido seleccionadas como las m\u00e1s significativas de t odas las posibles. Tambi\u00e9n hay que considerar que si aparecen estas dos localidades en primer lugar es si mplemente por su mayor volumen de datos, lo que ot orga una significatividad superior en las relaciones encontradas. Si se consulta la bibliograf\u00eda, el primer criterio de selecci\u00f3n de reglas del algoritmo \"A priori\" es la precisi\u00f3n o confianza, dada por el porcentaje de veces que instancias que cumplen el antecedente cumplen el consecuente, pero el segundo es el sopor te, dado por el n\u00famero de instancias Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 183 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda sobre las que es aplicable la regla. En todo caso, son reglas de muy baja precisi\u00f3n y que habr\u00eda que considerar simplemente como ciertas tendencias. Agrupamiento La opci\u00f3n Cluster del Experimenter nos permite aplicar algoritmos de agrupamiento de instancias a nuestros dat os. Estos algoritmos buscan grupos de instancias con caracter\u00edsticas \"similare s\", seg\u00fan un criterio de comparaci\u00f3n entre valores de atributos de las inst ancias definidos en los algoritmos. El mecanismo de selecci\u00f3n, configurac i\u00f3n y ejecuci\u00f3n es similar a otros elementos: primero se sele cciona el algoritmo con Choose , se ajustan sus par\u00e1metros seleccionando sobre el \u00e1r ea donde aparece, y se despu\u00e9s se ejecuta. El \u00e1rea de agr upamiento del Explorer pr esenta los siguientes elementos de configuraci\u00f3n: Selecci\u00f3n y configuraci\u00f3n del algoritmo Evaluaci\u00f3n del resultado de cluster Visualizaci\u00f3n de resultados Clusters en texto Una vez que se ha realizado la selecci\u00f3n y configuraci\u00f3n del algoritmo, se puede pulsar el bot\u00f3n Start , que har\u00e1 que se aplique sobre la relaci\u00f3n de trabajo. Los resultados se presentar\u00e1n en la ventana de texto de la parte derecha. Adem\u00e1s, la vent ana izquierda permite listar todos los algoritmos y resultados que se hayan ej ecutado en la sesi\u00f3n act ual. Al seleccionarlos en esta lista de visualizaci\u00f3n se present an en la ventana de texto a la derecha, y adem\u00e1s se permite abrir ventanas gr \u00e1ficas de visualizaci\u00f3n con un men\u00fa contextual que aparece al pulsar el bot\u00f3n derecho sobre el resultado seleccionado. Por \u00faltimo, en esta opci\u00f3n de Agrupamiento aparecen las siguientes opciones adicio nales en la pantalla. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 184 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Ignorar atributos La opci\u00f3n Ignoring Attributes permite sacar fuera atri butos que no interesa considerar para el agrupamiento, de m anera que el an\u00e1lisis de parecido entre instancias no considera los atributos se leccionados. Al accionar esta opci\u00f3n aparecen todos los atributos disponibles. Se pueden seleccionar con el bot\u00f3n izquierdo sobre un atributo espec\u00edfico, o seleccionar grupos usando SHIFT para un grupo de atributos conti guos y CONTROL para grupos de atributos sueltos. Evaluaci\u00f3n La opci\u00f3n Cluster Mode permite elegir como evaluar los resultados del agrupamiento. Lo m\u00e1s simple es utilizar el propio conjunto de entrenamiento, Use tranining set , que indica que porcentaje de instancias se van a cada grupo. El resto de opciones realizan un entrenamiento con un conjunto, sobre el que construyen los clusters y a continuaci\u00f3n aplican estos clusters para clasificar un conjunto independiente que puede proporcionarse aparte (Supplied test ), o ser un porcentaje del conjunto de entrada ( Percentage split). Existe tambi\u00e9n la opci\u00f3n de com parar los clusters con un atributo de clasificaci\u00f3n ( Classes to clusters evaluation ) que no se considera en la construicci\u00f3n de los clusters. Nosotros nos centraremos \u00fanicamente en la primera opci\u00f3n, dejando el resto de opciones de evaluaci\u00f3n para m\u00e1s adelante, cuando lleguemos a los algoritmos de clasificaci\u00f3n. Finalmente, el cuadro opcional de almacenamiento de instancias, Store clusters for visualization , es muy \u00fatil para despu\u00e9s analizar los resultados gr\u00e1ficamente. Agrupamiento num\u00e9rico En primer lugar utilizar emos el algoritmo de agr upamiento K-medias, por ser uno de los m\u00e1s veloces y eficientes, si bien uno de los m\u00e1s limitados. Este algoritmo precisa \u00fanicam ente del n\u00famero de categor\u00edas similares en las que queremos dividir el conjunto de datos. Suel e ser de inter\u00e9s repetir la ejecuci\u00f3n del algoritmo K-medias con diferentes se millas de inicializa ci\u00f3n, dada la notable dependencia del arranque cuando no est\u00e1 cl ara la soluci\u00f3n que mejor divide el conjunto de instancias. En nuestro ejemplo, vamos a comprobar si el atributo \"opci\u00f3n\" divide naturalmente a los alumnos en grupos simi lares, para lo que seleccionamos el algoritmo SimpleKMeans con par\u00e1metro numClusters con valor 5. Los resultados aparecen en la ventana de texto derecha: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 185 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Nos aparecen los 5 grupos de ejemplos m\u00e1s similares, y sus centroides (promedios para atributos num\u00e9ricos, y valores m\u00e1s repetidos en cada grupo para atributos simb\u00f3licos). En este caso es de inter\u00e9s analizar gr\u00e1ficamente como se distribuyen diferentes valores de los atributos en los grupos generados. Para ello basta pulsar con bot\u00f3n derecho del rat\u00f3n sobre el cuadro de resultados, y seleccionar la opci\u00f3n visualizeClusterAssignments Si seleccionamos combinaciones del atributo opci\u00f3n con localidad, nota o convocatoria podemos ver la distribuci\u00f3n de grupos: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 186 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda A la vista de estos gr\u00e1ficos podemos c oncluir que el \"parecido\" entre casos viene dado fundamentalm ente por las opciones seleccionadas. Los clusters 0, 1 y 4 se corresponden con las opciones 3, 4 y 1, mientras que los clusters 2 y 3 representan la opci\u00f3n 3 en las conv ocatorias de junio y septiembre. Aprovechando esta posibilidad de bu scar grupos de semejanzas, podr\u00edamos hacer un an\u00e1lisis m\u00e1s particularizado a las dos localidades mayores, Legan\u00e9s y Getafe, buscando qu\u00e9 opciones y calificaciones aparecen con m\u00e1s frecuencia. Vamos a preparar los datos con filtros de modo que tengamos \u00fanicamente tres atributos: localidad, opci\u00f3n, y calificac i\u00f3n final. Adem\u00e1s, discretizamos las calificaciones en dos grupos de la mism a frecuencia (estudiantes con mayor y menor \u00e9xito), y \u00fanicamente nos quedam os con los alumnos de Legan\u00e9s y Getafe. Utilizaremos para ello los f iltros adecuados. A continuaci\u00f3n aplicamos el algoritmo K-medias con 4 grupos. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 187 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda vemos que los grupos nos muestran la presencia de buenos alumnos en Getafe en la opci\u00f3n 4, y buenos alumnos en Legan\u00e9s en la opci\u00f3n 1, siempre considerando estas conclusiones como tendencias en promed io. Gr\u00e1ficamente vemos la distribuci\u00f3n de clusters por combinaciones de atributos: Si consideramos que en Legan\u00e9s hay es cuelas de ingenier\u00eda, y en Getafe facultades de Humanidades, podr\u00edamos c oncluir que podr\u00eda ser achacable al impacto de la universidad en la zona. El algoritmo EM proviene de la estad\u00edstica y es bast ante m\u00e1s elaborado que el K-medias, con el coste de que requiere muchas m\u00e1s operaciones, y es apropiado cuando sabemos que los datos tienen una variabi lidad estad\u00edstica de modelo conocido. Dada esta mayor co mplejidad, y el notable volumen del Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 188 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda fichero de datos, primero aplicaremos un fi ltro de instancias al 3% para dejar un n\u00famero de 500 instancias aproximadament e. Para esto \u00faltimo iremos al preprocesado y aplicamos un filt ro de instancias, el filtro Resample , con factor de reducci\u00f3n al 3%: Una ventaja adicional del algoritmo de clustering EM es que permite adem\u00e1s buscar el n\u00famero de grupos m\u00e1s apropiado, para lo cual basta indicar a -1 el n\u00famero de clusters a formar, que es la opci\u00f3n que viene por defecto. Esto se interpreta como dejar el par\u00e1metro del n\u00famero de clusters como un valor a optimizar por el propio algoritmo. Tras su aplicaci\u00f3n, este algoritmo determina que hay cinco clusters significativos en la muestra de 500 al umnos, y a continuaci\u00f3n indica los centroides de cada grupo: Al igual que antes, es interesante analiz ar el resultado del agrupamiento sobre diferentes combinaciones de atributos, haciendo uso de la facilidad visualizeClusterAssignments Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 189 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Por tanto podr\u00eda concluirse que para este segundo algoritmo de agrupamiento por criterios estad\u00edsticos y no de distancias entre vectores de atributos, predomina el agrupamiento de los alumnos b\u00e1sicamente por tramos de calificaciones, independientemente de la opci\u00f3n, mientras que en el anterior pesaba m\u00e1s el perfil de asignaturas cursado que las calificaciones. Esta disparidad sirve para ilustrar la pr oblem\u00e1tica de la decis i\u00f3n del criterio de \"parecido\" entre instancias par a realizar el agrupamiento. Agrupamiento simb\u00f3lico Finalmente, como alternativa a los al goritmos de agrupamiento anteriores, el agrupamiento simb\u00f3lico tiene la ventaja de efectuar un an\u00e1lisis cualitativo que construye categor\u00edas jer\u00e1rquicas para or ganizar los datos. Estas categor\u00edas se forman con un criterio pr obabil\u00edstico de \"utilidad\", llegando a las que permiten homogeneidad de los valores de los atri butos dentro de cada una y al mismo tiempo una separaci\u00f3n entre categor\u00edas dadas por los atributos, propag\u00e1ndose estas caracter\u00edsticas en un \u00e1rbol de conceptos. Si aplicamos el algoritmo cobweb con los par\u00e1metros por defecto sobre la muestra reducida de instancias (dada la complejidad del algoritmo), el \u00e1rbol generado llega hasta 800 nodos. Vamos a modificar el par\u00e1metro cut-off , que permite poner condiciones m\u00e1s restrictiv as a la creaci\u00f3n de nuevas categor\u00edas en un nivel y subcategor\u00edas. Con los par \u00e1metros siguientes se llega a un \u00e1rbol muy manejable: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 190 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda la opci\u00f3n saveInstanceData es de gran utilidad para despu\u00e9s analizar la distribuci\u00f3n de valores de atributos entre las instancias de cada parte del \u00e1rbol de agrupamiento. Una vez ejecutado Cobw eb, genera un resultado como el siguiente: hay 3 grupos en un primer nivel, y el segundo se subdivi de en otros dos. De nuevo activando el bot\u00f3n derecho sobr e la ventana de resultados, ahora podemos visualizar el \u00e1rbol gr\u00e1ficamente: las opciones de visualizaci\u00f3n aparecen al pulsar el bot\u00f3n derecho en el fondo de la figura. Se pueden visualizar las instancias que van a cada nodo sin m\u00e1s que pulsar el bot\u00f3n derecho sobre \u00e9l. Si nos fijamos en como quedan distribuidas las instancias por clusters, con la opci\u00f3n visualizeClusterAssignments, llegamos a la figura: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 191 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda por tanto, vemos que de nuevo vuelve a pesar la opci\u00f3n como criterio de agrupamiento. Los nodos hoja 1, 3, 4 y 5 se corresponden con las opciones cursadas 2, 3, 1 y 4 respectivamente. En un primer nivel hay tres grupos, uno para la opci\u00f3n 2, otro para la opci\u00f3n 4 y otro que une las opciones 1 y 3. Este \u00faltimo se subdivide en dos grupos que se corresponden con ambas opciones. Clasificaci\u00f3n Finalmente, en esta secci\u00f3n abordamos el problema de la clasificaci\u00f3n, que es el m\u00e1s frecuente en la pr\u00e1ctica. En ocas iones, el problema de clasificaci\u00f3n se formula como un refinamiento en el an\u00e1lisis, una vez que se han aplicado algoritmos no supervisados de agrupami ento y asociaci\u00f3n para describir relaciones de inter\u00e9s en los datos. Se pretende construir un modelo que permi ta predecir la categor\u00eda de las instancias en funci\u00f3n de una serie de atri butos de entrada. En el caso de WEKA, la clase es simplemente uno de lo s atributos simb\u00f3licos disponibles, que se convierte en la va riable objetivo a predecir. Por defecto, es el \u00faltimo atributo (\u00faltima columna) a no ser que se indique otro expl\u00edcitamente. La configuraci\u00f3n de la clasificaci\u00f3n se efect\u00faa con la ventana siguiente: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 192 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Selecci\u00f3n y configuraci\u00f3n del algoritmo de clasificaci\u00f3n Modo de evaluaci\u00f3n del clasificador Visualizaci\u00f3n de resultados Modelo y evaluaci\u00f3n (en texto) Atributo seleccionado como clase la parte superior, como es habitual sirve para seleccionar el algoritmo de clasificaci\u00f3n y configurarlo. El resto de elementos a definir en esta ventana se describen a continuaci\u00f3n. Modos de evaluaci\u00f3n del clasificador El resultado de aplicar el algoritmo de clasificaci\u00f3n se efect\u00faa comparando la clase predicha con la clase real de las instancias. Esta evaluaci\u00f3n puede realizarse de diferentes modos, seg\u00fan la selecci\u00f3n en el cuadro Test options : Use training set: esta opci\u00f3n eval\u00faa el clasific ador sobre el mismo conjunto sobre el que se construye el modelo pr edictivo para determinar el error, que en este caso se denomina \"error de resu stituci\u00f3n\". Por tanto, esta opci\u00f3n puede proporcionar una estimaci\u00f3n demasiado optimista del comportamiento del clasificador, al eval uarlo sobre el mismo conjunto sobre el que se hizo el modelo. Supplied test set : evaluaci\u00f3n sobre conjunto independiente. Esta opci\u00f3n permite cargar un conjunto nuevo de da tos. Sobre cada dato se realizar\u00e1 una predicci\u00f3n de clase para contar los errores. Cross-validation : evaluaci\u00f3n con validaci\u00f3n cr uzada. Esta opci\u00f3n es la m\u00e1s elaborada y costosa. Se realizan tantas evaluaciones como se indica en el par\u00e1metro Folds . Se dividen las instancias en tantas carpetas como indica este par\u00e1metro y en cada evaluac i\u00f3n se toman las instancias de cada carpeta como datos de test, y el re sto como datos de entrenamiento para Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 193 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda construir el modelo. Los errores calc ulados son el promedio de todas las ejecuciones. Percentage split : esta opci\u00f3n divide los datos en dos grupos, de acuerdo con el porcentaje indicado ( %). El valor indicado es el porcentaje de instancias para construir el modelo, que a continuaci\u00f3n es evaluado sobre las que se han dejado aparte. Cuando el n\u00famero de instancias es suficientemente elevado, esta opci\u00f3n es suficiente para estimar con precisi\u00f3n las prestaciones del clasificador en el dominio. Adem\u00e1s de estas opciones para seleccio nar el modo de evaluaci\u00f3n, el bot\u00f3n More Options abre un cuadro con otras opciones adicionales: Output model : permite visualizar (en modo text o y, con algunos algoritmos, en modo gr\u00e1fico) el modelo construido por el clasificador (\u00e1rbol, reglas, etc.) Output per-class stats : obtiene estad\u00edsticas de los errores de clasificaci\u00f3n por cada uno de los valores que toma el atributo de clase Output entropy evaluation measures : generar\u00eda tambi\u00e9n medidas de evaluaci\u00f3n de entrop\u00eda Store predictions for visualization: permite analizar los errores de clasificaci\u00f3n en una v entana de visualizaci\u00f3n Cost-sensitive evaluation : con esta opci\u00f3n se puede especificar una funci\u00f3n con costes relativos de los diferentes errores, que se rellena con el bot\u00f3n Set en nuestro ejemplo utilizaremos los va lores por defecto de estas \u00faltimas opciones. Evaluaci\u00f3n del clasificador en ventana de texto Una vez se ejecuta el clasificador sele ccionado sobre los datos de la relaci\u00f3n, en la ventana de texto de la derecha a parece informaci\u00f3n de ejecuci\u00f3n, el modelo generado con todos los datos de ent renamiento y los resultados de la Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 194 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda evaluaci\u00f3n. Por ejemplo, al predecir el atributo \"presentado\" , con un \u00e1rbol de decisi\u00f3n de tipo J48, aparece el modelo textual of Leaves : 2 Size of the tree : 3 Se obtiene a partir de los dat os esta relaci\u00f3n trivial, salvo dos \u00fanicos casos de error: los presentados son los que tien en una calificaci\u00f3n superior a 0. Con referencia al informe de evaluaci\u00f3n del clasificador, podemos destacar tres elementos: Resumen (Summary ): es el porcentaje global de errores cometidos en la evaluaci\u00f3n Precisi\u00f3n detallada por clase : para cada uno de los valores que puede tomar el atributo de clase: el porc entaje de instancias con ese valor que son correctamente predichas (TP: true posit ives), y el porcentaje de instancias con otros valores que son incorrectam ente predichas a ese valor aunque ten\u00edan otro (FP: false positives). Las otras columnas, precision, recall, F- measure , se relacionan con estas dos anteriores. Matriz de confusi\u00f3n : aqu\u00ed aparece la informaci\u00f3n detallada de cuantas instancias de cada clase son predichas a cada uno de los valores posibles. Por tanto, es una matriz con N2 posiciones, con N el n\u00famero de valores que puede tomar la clase. En cada fila i, i=1...N, aparecen las instancias que realmente son de la clase i, mientras que las columnas j, j=1...N, son las que se han predicho al valor j de la clas e. En el ejemplo anterior, la matriz de confusi\u00f3n que aparece es la siguiente: === Confusion Matrix === a b <-- classified as 18647 0 | a = SI 2 153 | b = NO por tanto, los valores en la diagonal s on los aciertos, y el resto de valores son los errores. De los 18647 alumnos presentados, todos son correctamente clasificados, mientras que de los 155 no presentados, hay 153 correctamente clasificados y 2 con error. Lista de resultados Al igual que con otras opciones de an\u00e1lisis , la ventana izquier da de la lista de resultados contiene el resumen de toda s las aplicaciones de clasificadores sobre conjuntos de datos en la sesi\u00f3n del Explorer . Puede accederse a esta Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 195 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda lista para presentar los resultados, y al activar el bot\u00f3n derecho aparecen diferentes opciones de visualizaci\u00f3n, entre las que podemos destacar las siguientes: Salvar y cargar modelos: Load model, Save model . Estos modelos pueden recuperarse de fichero para posteriorm ente aplicarlos a nu evos conjuntos de datos Visualizar \u00e1rbol y errores de predicci\u00f3n: Visualize tree, Visualize classifier errors ,... el \u00e1rbol (permite almacenar Una vez se ejecuta el clasif icador seleccionado sobre los datos de la relaci\u00f3n, Selecci\u00f3n y configuraci\u00f3n de clasificadores Vamos a ilustrar la aplicaci\u00f3n de algor itmos de clasificaci\u00f3n a diferentes problemas de predicci\u00f3n de at ributos definidos sobre los datos de entrada en este ejemplo. El problema de clasificaci\u00f3n siempre se realiza sobre un atributo simb\u00f3lico, en el caso de utilizar un at ributo num\u00e9rico se precisa por tanto discretizarlo antes en intervalos q ue representar\u00e1n los valores de clase. En primer lugar efectuarem os an\u00e1lisis de predicci\u00f3n de la calificaci\u00f3n en la prueba de selectividad a partir de los sigu ientes atributos: a\u00f1o, convocatoria, localidad, opci\u00f3n, presentado y nota de bachill erato. Se van a realizar dos tipos de predicciones: aprobados, e intervalos de clasificaci\u00f3n. Por tanto tenemos que aplicar en primer lugar una combinaci\u00f3n de filtros que elimine los atributos no deseados relativos a calificaciones parci ales y asignaturas opcionales, y un filtro que discretice la calific aci\u00f3n en la prueba en dos partes: obs\u00e9rvese que se prefiere realizar las pr edicciones sobre la calificaci\u00f3n en la prueba, puesto que la calificaci\u00f3n fina l depende expl\u00edcitament e de la nota del bachillerato. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 196 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Clasificador \"OneR\" Este es uno de los clasificadores m\u00e1s sencillos y r\u00e1pidos, aunque en ocasiones sus resultados son sorprendentemente buenos en comparaci\u00f3n con algoritmos mucho m\u00e1s complejos. Simplemente sele cciona el atributo que mejor \"explica\" la clase de salida. Si hay atributos num\u00e9ricos, busca los umbrales para hacer reglas con mejor tasa de aciertos. Lo ap licaremos al problema de predicci\u00f3n de aprobados en la prueba a partir de los at ributos de entrada, para llegar al resultado siguiente: por tanto, el algoritmo ll ega a la conclusi\u00f3n que la mejor predicci\u00f3n posible con un solo atributo es la nota del bachillerato, fijando el umbral que determina el \u00e9xito en la prueba en 6.55. La tasa de acie rtos sobre el propio conjunto de entrenamiento es del 72.5% . Comp\u00e1rese este resu ltado con el obtenido mediante ejecuci\u00f3n sobre instancias independientes. Clasificador como \u00e1rbol de decisi\u00f3n: J48 El algoritmo J48 de WEKA es una implementaci\u00f3n del algoritmo C4.5, uno de los algoritmos de miner\u00eda de datos que m\u00e1s se ha utilizado en multitud de aplicaciones. No vamos a entrar en los detalles de todos lo s par\u00e1metros de configuraci\u00f3n, dej\u00e1ndolo par a el lector interesado en los detalles de este algoritmo, y \u00fanicamente resaltaremos uno de los m\u00e1s importantes, el factor de confianza para la poda, confidence level , puesto que influye notoriamente en el tama\u00f1o y capacidad de predicci\u00f3n del \u00e1rbol construido. Una explicaci\u00f3n simplificada de este par \u00e1metro de construcci \u00f3n del \u00e1rbol es la siguiente: para cada operaci \u00f3n de poda, define la probabilidad de error que se permite a la hip\u00f3tesis de que el empeor amiento debido a esta operaci\u00f3n es significativo. Cuanto m\u00e1s baja se haga esa probabilidad, se exigir\u00e1 que la diferencia en los errores de predicci\u00f3n antes y despu\u00e9s de podar sea m\u00e1s Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 197 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda significativa para no podar. El valor por def ecto de este factor es del 25%, y conforme va bajando se permiten m\u00e1s oper aciones de poda y por tanto llegar a \u00e1rboles cada vez m\u00e1s peque\u00f1os. Otra forma de variar el tama\u00f1o del \u00e1rbol es a trav\u00e9s de un par\u00e1metro que especifica el m\u00ednimo n\u00famero de instancias por nodo, si bien es menos elegante puesto que depende del n\u00famero absoluto de instancias en el conjunto de partida. Construiremos el \u00e1rbol de decisi\u00f3n con los par\u00e1metros por defecto del algoritmo J48: se llega a un clasificador con m\u00e1 s de 250 nodos, con una probabilidad de acierto ligeramente superior al del clasificador OneR . Modifique ahora la configuraci\u00f3n de l algoritmo para llegar a un \u00e1rbol m\u00e1s manejable, como el que se presenta a continuaci\u00f3n Obs\u00e9rvese que este modelo es un refi namiento del generado con OneR, que supone una mejorar moderada en las presta ciones. De nuevo los atributos m\u00e1s importantes son la calific aci\u00f3n de bachillerato, la c onvocatoria, y despu\u00e9s el a\u00f1o, antes que la localidad o las opciones. Analice las diferencias con evaluaci\u00f3n independiente y validaci\u00f3n cruz ada, y comp\u00e1relas con las del \u00e1rbol m\u00e1s complejo con menos poda. Podr\u00eda ser de inter\u00e9s analizar el efecto de las opciones y asignaturas seleccionadas sobre el \u00e9xito en la prueba, para lo cual quitaremos el atributo m\u00e1s importante, nota de bachi llerato. Llegamos a un \u00e1r bol como el siguiente, en el que lo m\u00e1s importante es la prim era asignatura optativa, en diferentes combinaciones con el a\u00f1o y segunda asignatura optativa: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 198 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Este resultado generado por el clasificador puede comp robarse si se analizan los histogramas de cada variable y visualizando el porcentaje de aprobados con el color, que esta variable es la que mejor separa las clases, no obstante, la precisi\u00f3n apenas supera el 55%. Otros problemas de clasificaci\u00f3n pueden fo rmularse sobre cualquier atributo de inter\u00e9s, a continuaci\u00f3n mostramos al gunos ejemplos a t\u00edtulo ilustrativo. Clasifiaci\u00f3n multinivel de las calificaciones el problema anterior puede inte ntar refinarse y dividir el atributo de inter\u00e9s, la calificaci\u00f3n final, en m\u00e1s niveles, en este caso 5. Los resultados se muestran a continuaci\u00f3n oneR J48 Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 199 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda La precisi\u00f3n alcanzada es tan s\u00f3lo del 60%, indicando que hay bastante incertidumbre una vez generada la predi cci\u00f3n con los modelos anteriores. Predicci\u00f3n de la opci\u00f3n Si dejamos todos los atributos en la m uestra y aplicamos el clasificador a la opci\u00f3n cursado, se desvela una relaci\u00f3n tr ivial entre opci\u00f3n y asignaturas en las opciones que predice con pr\u00e1ctica mente el 100% de los casos. A continuaci\u00f3n elimin amos estos designadores con un filtro de atributos. Si aplicamos el algoritmo J48 sobre los datos filtrados, llega mos a un \u00e1rbol de m\u00e1s de 400 nodos, y con much\u00edsimo sobre- ajuste (observe la diferencia de error de predicci\u00f3n sobre el conjunto de entrenamiento y s obre un conjunto independiente). Forzando la poda del \u00e1rbol, llegamos al modelo siguiente: los atributos m\u00e1s significativos para s eparar las opciones s on precisamente las calificaciones en las asignaturas optativ as, pero apenas predi ce correctamente un 40% de los casos. Por tanto, vemos que no hay una relaci\u00f3n directa entre opciones y calificaciones en la prueba, al menos relaciones que se puedan modelar con los algoritmos de clasific aci\u00f3n disponibles. Si nos fijamos en detalle en las calificaciones en func i\u00f3n de las opciones, podr\u00edamos determinar que apenas aparecen diferencia s aparecen en los \u00faltimos percentiles, a la vista de las gr\u00e1ficas siguientes: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 200 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda nota historia nota idioma nota lengua nota final nota asig 1 nota asig 2 Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 201 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda nota asig3 Vemos que las diferencias no son significa tivas, salvo quiz\u00e1 en los \u00faltimos percentiles. Predicci\u00f3n de localidad y opci\u00f3n La clasificaci\u00f3n se puede rea lizar sobre cualquier atributo disponible. Con el n\u00famero de atributos reducido a tres, loca lidad, opci\u00f3n y calif icaci\u00f3n (aprobados y suspensos), vamos a buscar modelos de clasificaci\u00f3n, para cada uno de los atributos: predicci\u00f3n de localidad predicci\u00f3n de opci\u00f3n Es decir, la opci\u00f3n 1 y 2 aparec en mayoritariamente en Legan\u00e9s, y las opciones 3 y 4 m\u00e1s en los alumnos que aprobaron la prueba en Legan\u00e9s. No obstante, obs\u00e9rvese que los errores son tan abrumadores (menos del 30% de aciertos) que cuestionan fuertement e la validez de estos modelos. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 202 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Mejora en la prueba Un problema de clasificaci\u00f3n intere sante puede ser determinar qu\u00e9 alumnos tienen m\u00e1s \"\u00e9xito\" en la prueba, en el se ntido de mejorar su calificaci\u00f3n de bachillerato con la calificaci\u00f3n en la pr ueba. Para ello util izaremos el atributo \"mejora\", introducido en la secci\u00f3n 1.4.2. 3, y lo discretizamos en dos valores de la misma frecuencia (obt enemos una mediana de -1.75, de manera que dividimos los alumnos en dos grupos: los que obtienen una diferencia menor a este valor y superior a este valor, para diferenciar los alumnos seg\u00fan el resultado se atenga m\u00e1s o menos a sus expectativas. Evidentemente, para evitar construir modelos triviales, tenemos que eliminar los atributos relacionados con las calificaciones en la prueba, para no llegar a la relaci\u00f3n que acabamos de construir entre la variable calculada y las originales. Vamos a preparar el problema de clasificac i\u00f3n con los siguientes atributros: Attributes: 7 A\u00f1o_acad\u00e9mico convocatoria localidad opcion1\u00aa nota_bachi Presentado mejora Llegamos al siguiente \u00e1r bol de clasificaci\u00f3n. Es decir, los atributos que m\u00e1s determinan el \"\u00e9xito\" en la prueba son: a\u00f1o acad\u00e9mico, opci\u00f3n y localidad. Para esto s resultados tenemos una precisi\u00f3n, con evaluaci\u00f3n sobre un conj unto independiente, en torno al 60%, por lo que s\u00ed podr\u00edamos tomarlo en consideraci\u00f3n. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 203 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Predicci\u00f3n num\u00e9rica La predicci\u00f3n num\u00e9rica se define en WE KA como un caso particular de clasificaci\u00f3n, en el que la clase es un valor num\u00e9rico. No obstante, los algoritmos integrados para cl asificar s\u00f3lo admiten cl ases simb\u00f3licas y los algoritmos de predicci\u00f3n num\u00e9ricas, que aparecen mayoritariamente en el apartado classifiers->functions , aunque tambi\u00e9n en classifiers->trees . Vamos a ilustrar algoritmos de predicci \u00f3n num\u00e9rica en WEKA con dos tipos de problemas. Por un lado, \"descubrir\" relaciones deterministas que aparecen entre variables conocidas, como calific aci\u00f3n en la prueba co n respecto a las parciales y la calificaci\u00f3n final con res pecto a la prueba y bachillerato, y buscar otros modelos de mayor posible inter\u00e9s. Relaci\u00f3n entre calificaci\u00f3n final y parciales Seleccionamos los atributos con las 6 ca lificaciones parciales y la calificaci\u00f3n en la prueba: Vamos a aplicar el modelo de predicci \u00f3n m\u00e1s popular: regresi\u00f3n simple, que construye un modelo lineal del atributo clase a partir de los atributos de entrada: functions->LinearRegresion Como resultado, aparece la relaci\u00f3n con los pesos relativos de las pruebas parciales sobre la calificaci\u00f3n de la prueba: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 204 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Hay que observar que en los problemas de predicci\u00f3n la evaluaci\u00f3n cambia, apareciendo ahora el coeficiente de corre laci\u00f3n y los errores medio y medio cuadr\u00e1tico, en t\u00e9rminos absolutos y relati vos. En este caso el coeficiente de correlaci\u00f3n es de 0.998, lo que indica que la rela ci\u00f3n es de una precisi\u00f3n muy notable. Si aplicamos ahora esta funci\u00f3n a la relaci\u00f3n entre calificaci\u00f3n final con calificaci\u00f3n en la prueba y nota de bachille rato (filtro que selecciona \u00fanicamente los atributos 15-17), podemos determinar la relaci\u00f3n entre es tas variables: qu\u00e9 peso se lleva la calificac i\u00f3n de bachillerato y de la prueba en la nota final. Vamos a hacerlo primero con lo s alumnos de una poblaci\u00f3n peque\u00f1a, de Guadarrama (posici\u00f3n 12 del atributo localidad). Aplicamos los filtros correspondientes para tener \u00fanicamente estos alumno s, y los atributos de calificaciones de la prueba, bachillerato y final: llegamos a 40 instancias: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 205 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda si aplic\u00e1ramos regresi\u00f3n lineal como en el ejemplo anterio r, obtenemos el siguiente resultado: el resultado deja bastante que desear por que la relaci\u00f3n no es lineal. Para solventarlo podemos aplicar el algoritmo M5P, sele ccionado en WEKA como trees->m5->M5P, que lleva a cabo una regresi\u00f3n por tramos, con cada tramo determinado a partir de un \u00e1rbol de regres i\u00f3n. Llegamos al siguiente resultado: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 206 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda que es pr\u00e1cticamente la relaci\u00f3n exacta utilizada en la actua lidad: 60% nota de bachillerato y 40% de la prueba, siempre que se supere en \u00e9sta un valor m\u00ednimo de 4 puntos. Si aplicamos este algoritmo a otro s centros no siempr e obtenemos este resultado, por una raz\u00f3n: hasta 1998 se ponderaba al 50%, y a partir de 1999 se comenz\u00f3 con la ponderaci\u00f3n anterio r. Verif\u00edquese aplicando este algoritmo sobre datos filtrados que contengan alumnos de an tes de 1998 y de 1999 en adelante. En este caso, el algoritmo M5 P no tiene capacidad para construir el modelo correcto, debido a la ligera difer encia en los resultados al cambiar la forma de ponderaci\u00f3n. Los \u00e1rboles obteni dos en ambos casos se incluyen a continuaci\u00f3n: hasta 1998 de 1999 en adelante Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 207 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Predicci\u00f3n de la calificaci\u00f3n Vamos a aplicar ahora este modelo para intentar construir un modelo aplicaci\u00f3n m\u00e1s interesante, o, al menos, analizar tendencias de inter\u00e9s. Se trata de intentar predecir la calif icaci\u00f3n final a partir de lo s atributos de entrada, los mismos que utilizamos para el problem a de clasificar los alumnos que aprueban la prueba. Si aplicamos el algo ritmo sobre el conjunto completo llegamos al siguiente modelo: obs\u00e9rvese c\u00f3mo trata el al goritmo los atributos nominal es para incluirlos en la regresi\u00f3n: ordena los valores seg\u00fan el va lor de la magnitud a predecir (en el caso de localidad, desde Collado hasta Los Pe\u00f1ascales y en el de opci\u00f3n, ordenadas como 4\u00ba, 5\u00ba, 3\u00ba, 2\u00ba, 1\u00ba), y va tomando variables binarias resultado de dividir en diferentes puntos, determinando su peso en la funci\u00f3n. En esta funci\u00f3n lo que m\u00e1s pesa es la convocator ia, despu\u00e9s la nota de bachillerato, y despu\u00e9s entran en juego la localidad, asignat uras optativas, y opci\u00f3n, con un modelo muy complejo. Si simplificamos el conjunto de at ributos de entrada, y nos quedamos \u00fanicamente con el a\u00f1o, opci\u00f3n, nota de bachillerato, y convocatoria, llegamos a: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 208 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda este modelo es mucho m\u00e1s manejable. Compare los errores de predicci\u00f3n con ambos casos: modelo extenso modelo simplificado Correlaci\u00f3n entre nota de bachillerato y calificaci\u00f3n en prueba Finalmente, es interesante a veces hacer un modelo \u00fanicamente entre dos variables para ver el grado de corre laci\u00f3n entre ambas. Continuando con nuestro inter\u00e9s por las relaciones entre calificaci\u00f3n en prueba y calificaci\u00f3n en bachillerato, vamos a ver las diferencias por opci\u00f3n. Para ello filtraremos por un lado los alumnos de opci\u00f3n 1 y los de opci\u00f3n 4. A continuaci\u00f3n dejamos Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 209 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u00fanicamente los atributos calificaci\u00f3n en prueba y nota de bachillerato, para analizar la correlaci\u00f3n de los modelos para cada caso. alumnos opci\u00f3n 1\u00ba alumnos opci\u00f3n 4\u00ba podemos concluir que para estas dos opc iones el grado de relaci\u00f3n entre las variables s\u00ed es significativamente difer ente, los alumnos que cursan la opci\u00f3n 1\u00ba tienen una relaci\u00f3n m\u00e1s \"lineal\" entre am bas calificaciones que los procedentes de la opci\u00f3n 4\u00ba Aprendizaje del modelo y aplicaci\u00f3n a nuevos datos. Para finalizar esta secci\u00f3n de clasificaci\u00f3 n, ilustramos aqu\u00ed las posibilidades de construir y evaluar un clasificador de forma cruzada con dos ficheros de datos. Seleccionaremos el conjunto atributos si guiente: la calificaci\u00f3n, \"cal_prueba\", lo discretizamos en dos intervalos. Vamos a generar, con el filtro de instancias dos conjuntos de datos correspondientes a los alumnos de Getafe y Torrelodones. Para ello primero seleccionamos las instancias con el atribut o localidad con valor 10, lo salvamos (\"datosGetafe\") y a continuaci\u00f3n las instan cias con dicho atri buto con valor 21 (\"datosTorrelodones\"). Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 210 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Ahora vamos a generar los modelos de cl asificaci\u00f3n de alumnos con buen y mal resultado en la prueba con el fichero de alumnos de la localidad de Torrelodones, para evaluarlo con los alumnos de Getafe. Para ello en primer lugar cargamos el fichero con los alumnos de Torrelodones que acabamos de generar, \"datosTorrel odones\", y lo evaluamos sobre el conjunto con alumnos de Getafe. Para ello, seleccionaremos la opci\u00f3n de evaluaci\u00f3n con un fichero de datos independiente, Supplied test set , y fijamos con el bot\u00f3n Set, que el fichero de test es \"dat osGetafe\". Obs\u00e9rvese el modelo generado y los resultados: Si ahora hacemos la operaci\u00f3n inversa, entrenar con los datos de Getafe y evaluar con los de Torrelodones, llegamos a: Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 211 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Hay ligeras diferencias en los model os generados para am bos conjuntos de datos (para los alumnos de Torrelodones , lo m\u00e1s importante es tener una calificaci\u00f3n de bachillerato superior a 6. 8, mientras que a los de Getafe les basta con un 6.5), y los resultados de evaluaci\u00f3n con los datos cruzados muestran una variaci\u00f3n mu y peque\u00f1a. El modelo cons truido a partir de los datos de Torrelodones predice ligeramente peor los resultados de Getafe que a la inversa. Selecci\u00f3n de atributos Esta \u00faltima secci\u00f3n permite autom atizar la b\u00fasqueda de subconjuntos de atributos m\u00e1s apropiados par a \"explicar\" un atributo objetivo, en un sentido de clasificaci\u00f3n supervisada: permite expl orar qu\u00e9 subconjuntos de atributos son los que mejor pueden clasificar la clas e de la instancia. Esta selecci\u00f3n \"supervisada\" aparece en contraposici\u00f3 n a los filtros de preprocesado comentados en la secci\u00f3n 1. 4.2, que se realizan de forma independiente al proceso posterior, raz\u00f3n por la que se etiquetaron como \"no supervisados\". La selecci\u00f3n supervisada de atributos tiene dos componentes: M\u00e9todo de Evaluaci\u00f3n ( Attribute Evaluator ): es la funci \u00f3n que determina la calidad del conjunto de atributos para discriminar la clase. M\u00e9todo de B\u00fasqueda ( Search Method ): es la forma de realizar la b\u00fasqueda de conjuntos. Como la evaluaci\u00f3n exhaus tiva de todos los subconjuntos es un problema combinatorio inabordable en cuanto crece el n\u00famero de atributos, aparecen estrategias que permi ten realizar la b\u00fasqueda de forma eficiente De los m\u00e9todos de evaluaci\u00f3n, podemos distinguir dos tipos: los m\u00e9todos que directamente utilizan un clasificador es pec\u00edfico para medir la calidad del subconjunto de atributos a trav\u00e9s de la ta sa de error del clasificador, y los que no. Los primeros, denominados m\u00e9todos \"wrapper\", porque \"envuelven\" al clasificador para explorar la mejor selecci\u00f3n de atributos que optimiza sus prestaciones, son muy costosos por que necesitan un proceso completo de entrenamiento y evaluaci\u00f3n en cada pa so de b\u00fasqueda. Entre los segundos podemos destacar el m\u00e9todo \"CfsSubsetEval\" , que calcula la correlaci\u00f3n de la Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 212 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda clase con cada atributo, y eliminan atributos que ti enen una correlaci\u00f3n muy alta como atributos redundantes. En cuanto el m\u00e9todo de b\u00fasqueda, vamo s a mencionar por su rapidez el \"ForwardSelection \", que es un m\u00e9todo de b\u00fasqueda sub\u00f3ptima en escalada, donde elije primero el mejor atributo, despu\u00e9s a\u00f1ade el siguiente atributo que m\u00e1s aporta y continua as\u00ed hasta llegar a la situaci\u00f3n en la que a\u00f1adir un nuevo atributo empeora la situaci\u00f3n. Otr o m\u00e9todo a destacar ser\u00eda el \" BestSearch\" , que permite buscar interacciones entre atri butos m\u00e1s complejas que el an\u00e1lisis incremental anterior. Este m\u00e9todo va analizando lo que mejora y empeora un grupo de atributos al a\u00f1adir elementos, c on la posibilidad de hacer retrocesos para explorar con m\u00e1s detalle. El m\u00e9todo \"ExhaustiveSearch\" simplemente enumera todas las posibilidades y la s eval\u00faa para seleccionar la mejor Por otro lado, en la configuraci\u00f3n del problema debemos seleccionar qu\u00e9 atributo objetivo se utiliza para la se lecci\u00f3n supervisada, en la ventana de selecci\u00f3n, y determinar si la evaluaci\u00f3n se realizar\u00e1 con todas las instancias disponibles, o mediante validaci\u00f3n cruzada. Los elementos por tanto a configurar en esta secci\u00f3n se resumen en la figura siguiente: Evaluaci\u00f3n de la selecci\u00f3n supervisada Visualizaci\u00f3n de resultados Resultados ( en texto) atributo de clase Algoritmo evaluador Algoritmo de b\u00fasqueda Siguiendo con nuestro ejemplo, vamo s a aplicar b\u00fasqueda de atributos para \"explicar\" algunos atributos objetivo. Para obtener resultados sin necesidad de mucho tiempo, vamos a seleccionar lo s algoritmos m\u00e1s eficientes de evaluaci\u00f3n y b\u00fasqueda, CsfSubsetEval y ForwardSelection Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 213 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Por ejemplo, para la calificaci\u00f3n final tenemos 8 atributos seleccionados: atributo: Selected attributes: 9 : 1 des_asig1 Por tanto, hemos llegado a los atribut os que mejor explican ambos (la calificaci\u00f3n en la prueba depende directam ente de las parciales, y la opci\u00f3n se explica con la 1\u00aa asignatura), si bien son relaciones bastante triviales. A continauci\u00f3n preparamos los datos para buscar relaciones no conocidas, quitando los atributos refe rentes a cada prueba parcial . Dejando como atributos de la relaci\u00f3n: Attributes: 7 A\u00f1o_acad\u00e9mico convocatoria localidad opcion1\u00aa cal_prueba nota_bachi Presentado la calificaci\u00f3n final llegamos a 2 atributos: Selected attributes: 6,7 : 2 nota_bachi Presentado y para 2: Selected attributes: 3,5,6 : 3 localidad cal_prueba nota_bachi No obstante, si observamos la figur a de m\u00e9rito con ambos problemas, que aparece en la ventana textual de resultados, vemos que este segundo es mucho menos fiable, como ya hemos comprobado en secciones anteriores. Cap\u00edtulo 4 T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 214 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 215 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Cap\u00edtulo 5. Implementaci\u00f3n de las t\u00e9cnicas de An\u00e1lisis de Datos en Weka 5.1. Utilizaci\u00f3n de las clases de WEKA en programas independientes 5.2. Tabla de Decisi\u00f3n en WEKA El algoritmo de tabla de decisi\u00f3n impl ementado en la herramienta WEKA se encuentra en la clase weka.classifiers.DecisionTable.java . Las opciones de configuraci\u00f3n de que disponen son las que vemos en la tabla 5.1. Tabla 5.1: Opciones de configuraci\u00f3n para el algo ritmo de tabla de decisi\u00f3n en WEKA. Opci\u00f3n Descripci\u00f3n useIBk (False) Utilizar NN (ver punt o 2.2.5.1) en lugar de la tabla de decisi\u00f3n si no la instancia a clasificar no se corresponde con ninguna regla de la tabla. displayRules (False) Por defecto no se muestran la s reglas del clasificador, concretamente la tabla de decisi\u00f3n construida. maxStale (5) Indica el n\u00famero m\u00e1 ximo de conjuntos que intenta mejorar el algoritmo para encontrar una tabla mejor sin haberla encontrado en los \u00faltimos n-1 subconjuntos. crossVal (1) Por defecto se eval\u00faa el sistema mediante el proceso leave-one-out . Si se aumenta el valor 1 se realiza validaci\u00f3n cruzada con n carpetas. En primer lugar, en cuanto a los atributos que permite el sistema, \u00e9stos pueden ser tanto num\u00e9ricos (que se discretizar \u00e1n) como simb\u00f3licos. La clase tambi\u00e9n puede ser num\u00e9rica o simb\u00f3lica. Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 216 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda El algoritmo consiste en ir sele ccionando uno a uno lo s subconjuntos, a\u00f1adiendo a cada uno de los ya probados c ada uno de los atributos que a\u00fan no pertenecen a \u00e9l. Se prueba la precis i\u00f3n del subconjunto, bien mediante validaci\u00f3n cruzada o leave-one-out y, si es mejor, se contin\u00faa con \u00e9l. Se contin\u00faa as\u00ed hasta que se alcanza maxStale . Para ello, una variable comienza siendo 0 y aumenta su valor en una unidad cuando a un subconjunto no se le puede a\u00f1adir ning\u00fan atributo para mejorarlo, volviendo a 0 si se a\u00f1ade un nuevo atributo a un subconjunto. En cuanto al proceso leave-one-out , es un m\u00e9todo de estimaci\u00f3n del error. Es una validaci\u00f3n cruzada en la que el n\u00famer o de conjuntos es igual al n\u00famero de ejemplos de entrenamiento. Cada vez se elim ina un ejemplo del conjunto de entrenamiento y se entrena con el resto. Se juzgar\u00e1 el acierto del sistema con el resto de instancias seg\u00fan se acierte o se falle en la predicci\u00f3n del ejemplo que se elimin\u00f3. El resultado de las n pruebas (siendo n el n\u00famero inicial de ejemplos de entrenamiento) se promedia y dicha media ser\u00e1 el error estimado. Por \u00faltimo, para clasificar un ejemplo pueden ocurrir dos cosas. En primer lugar, que el ejemplo corresponda exacta mente con una de las reglas de la tabla de decisi\u00f3n, en cuyo caso se devolve r\u00e1 la clase de dicha regla. Si no se corresponde con ninguna regla, se puede utilizar Ibk (si se seleccion\u00f3 dicha opci\u00f3n) para predecir la clase, o la medi a o moda de la clase seg\u00fan el tipo de clase del que se trate (num\u00e9rica o simb\u00f3lica). 5.3. ID3 en WEKA La clase en la que est\u00e1 codificado el algoritmo ID3 es weka.classifiers.ID3.java . En primer lugar, en cuanto a la im plementaci\u00f3n, no permite ning\u00fan tipo de configuraci\u00f3n. Esta implementaci\u00f3n se ajusta exactamente a lo descrito anteriormente. Lo \u00fanico rese\u00f1able es que para determinar si un nodo es hoja o no, se calcula la ganancia de informa ci\u00f3n y, si la m\u00e1xima ganancia es 0 se considera nodo hoja, indepe ndientemente de que haya ej emplos de distintas clases en dicho nodo. Los atributos introducidos al sistema deben ser simb\u00f3licos, al igual que la clase. 5.4. C4.5 en WEKA (J48) La clase en la que se implementa el algoritmo C4.5 en la herramienta WEKA es weka.classifers.j48.J48.java . Las opciones que permite este algoritmo son las que se muestran en la tabla 2.3. Tabla 5.2: Opciones de configuraci\u00f3n para el algoritmo C4.5 en WEKA. Opci\u00f3n Descripci\u00f3n minNumObj (2) N\u00famero m\u00edni mo de instancias por hoja. Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 217 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda saveInstanceData (False) Una vez finalizada la creaci\u00f3n del \u00e1rbol de decisi\u00f3n se eliminan todas las instancias que se clasifican en cada nodo, que hasta el momento se manten\u00edan almacenadas. binarySplits (False) Con los atributos nominales tambi\u00e9n no se divide (por defecto) cada nodo en dos ramas. unpruned (False) En caso de no activar la opci\u00f3n, se realiza la poda del \u00e1rbol. subtreeRaising (True) Se permite realizar el podado con el proceso subtree raising . confidenceFactor (0.25) Factor de confianza para el podado del \u00e1rbol. reducedErrorPruning (False) Si se activa esta opci\u00f3n, el proceso de podado no es el propio de C4.5, sino que el conjunto de ejemplos se divide en un subconjunto de entrenamiento y otro de test, de los cuales el \u00faltimo servir \u00e1 para estimar el error para la poda. numFolds (3) Define el n\u00famero de subconjuntos en que hay que dividir el conjunto de ejemplos para, el \u00faltimo de ellos, emplearlo como conjunto de test si se activa la opci\u00f3n reducedErrorPruning . useLaplace (False) Si se activa esta opci\u00f3n, cuando se intenta predecir la probabilidad de que una instancia pertenezca a una clase, se emplea el suavizado de Laplace . El algoritmo J48 se ajusta al al goritmo C4.5 al que se le ampl\u00edan funcionalidades tales como permitir la realizaci\u00f3n del proceso de podado mediante reducedErrorPruning o que las divisiones sean siempre binarias binarySplits . Algunas propiedades concretas de la implementaci\u00f3n son las siguientes: En primer lugar, en cuanto a los tipos de atributos admitidos, estos pueden ser simb\u00f3licos y num\u00e9ricos. Se permiten ejemplos con faltas en dichos atributos, tanto en el mom ento de entrenami ento como en la predicci\u00f3n de dicho ejemplo. En c uanto a la clase, \u00e9sta debe ser simb\u00f3lica. Se permiten ejemplos con peso. El algoritmo no posibilita la generaci \u00f3n de reglas de clasificaci\u00f3n a partir del \u00e1rbol de decisi\u00f3n. Para el tratamiento de los atribu tos num\u00e9ricos el algoritmo prueba los puntos secuencialmente, con lo que emplea tres de las cuatro opciones que se comentaron anteriormente (ver figura 2.3). La cuarta opci\u00f3n, que consist\u00eda en unir intervalos adyacent es con la misma clase mayoritaria no se realiza. Tambi\u00e9n respecto a los atributos num \u00e9ricos, cuando se intenta dividir el rango actual en dos subrangos se ejecuta la ecuaci\u00f3n 2.14. Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 218 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ncn0.1 nima Divisi\u00f3nM\u00edic\u00d7 = (2.14) En esta ecuaci\u00f3n nic es el n\u00famero de ejemplos de entrenamiento con el atributo i conocido, y nc el n\u00famero de clases. A dem\u00e1s, si el resultado de la ecuaci\u00f3n es menor que el n\u00famer o m\u00ednimo de ejemplos que debe clasificarse por cada nodo hijo, se i guala a \u00e9ste n\u00famero y si es mayor que 25, se iguala a dicho n\u00famero. Lo que indica este n\u00famero es el n\u00famero m\u00ednimo de ejemplos que debe haber por cada uno de los dos nodos hijos que resultar\u00edan de la divisi \u00f3n por el atributo num\u00e9rico, con lo que no se considerar\u00edan divisiones que no cumplieran este dato. El c\u00e1lculo de la entrop\u00eda y de la ganancia de informaci\u00f3n se realiza con las ecuaciones 2.15, 2.16 y 2 ic n logn n log nI (2.16) () = =) nv(A 1jij 1kijk 2 ijk ij n logn I (2.17) En estas ecuaciones, nic es el n\u00famero de ejemplos con el atributo i conocido, n el n\u00famero total de ejemplos, nc el n\u00famero de ejemplos conocidos (el atributo i) con clase c, nij el n\u00famero de ejemplos con valor j en el atributo i y nijk el n\u00famero de atributos con valor j en el atributo i y con clase k. Adem\u00e1s, la informaci\u00f3n de ruptura se expresa como se muestra en la ecuaci\u00f3n 2.18. () ic 2 ic) nv(A 1jij 2 ij + = = (2.18) En la ecuaci\u00f3n 2.18, nij es el n\u00famero de ejemplos con valor j en el atributo i, nic es el n\u00famero de ejemplos con valor conocido en el atributo i y n es el n\u00famero total de ejemplos. El suavizado de Laplace se emplea en el proces o de clasificaci\u00f3n de un ejemplar. Para calcular la pr obabilidad de que un ejem plo pertenezca a una clase determinada en un nodo hoja se emplea la ecuaci\u00f3n 2.19. ()Cn1nE|kPk ++= (2.19) Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 219 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda En la ecuaci\u00f3n 2.19, nk es el n\u00famero de ejemplos de la clase clasificados en el nodo hoja, n el n\u00famero total de ej emplos clasificados en el nodo y C el n\u00famero de clases para los que hay alg\u00fan ejemplo clasificado en el nodo. 5.5. \u00c1rbol de Decisi\u00f3n de un solo nivel en WEKA La clase en la que se implementa el algoritmo toc\u00f3n de decisi\u00f3n en la herramienta WEKA es weka.classifers.DecisionStump.java . As\u00ed, en WEKA se llama a este algoritmo toc\u00f3n de decisi\u00f3n [decisi\u00f3n stump]. No tiene opciones de configuraci\u00f3n, pero la im plementaci\u00f3n es muy completa, dado que admite tanto atributos num\u00e9ricos como simb\u00f3licos y cl ases de ambos tipos tambi\u00e9n. El \u00e1rbol de decisi\u00f3n tendr\u00e1 tres ramas: una de ellas ser\u00e1 para el caso de que el atributo sea desconocido, y las otras dos ser\u00e1n par a el caso de que el valor del atributo del ejemplo de test sea i gual a un valor concreto del atributo o distinto a dicho valor, en caso de los atributos simb\u00f3licos , o que el valor del ejemplo de test sea mayor o menor a un determinado valor en el caso de atributos num\u00e9ricos. En el caso de los atributos simb\u00f3licos se considera cada valor posible del mismo y se calcula la ganancia de informa ci\u00f3n con el atributo igual al valor, distinto al valor y valores perdidos de l atributo. En el caso de atributos simb\u00f3licos se busca el mejor punto de rupt ura, tal y como se vio en el sistema C4.5 (ver punto 2.2.2.2). Deben tenerse en cuenta cuatro posibles casos al calcular la ganancia de informaci\u00f3n: que sea un atributo simb\u00f3lico y la clase sea simb\u00f3lica o que la clase sea num\u00e9rica, o que s ea un atributo num\u00e9rico y la clase sea simb\u00f3lica o que la clase sea num\u00e9rica. A contin uaci\u00f3n se comenta cada caso por separado. Atributo Simb\u00f3lico y Clase Simb\u00f3lica Se toma cada vez un valor vx del atributo simb\u00f3lico Ai como base y se consideran \u00fanicamente tres posibles rama s en la construcci\u00f3n del \u00e1rbol: que el atributo Ai sea igual a vx, que el atributo Ai sea distinto a vx o que el valor del atributo Ai sea desconocido. Con ello, se calcula la entrop\u00eda del atributo tomando como base el valor escogido ta l y como se muestr a en la ij nlogn I (2.20) En la ecuaci\u00f3n 2.20 el valor de j en el sumatorio va desde 1 hasta 3 porque los valores del atributo se restringen a tres: igual a vx, distinto a vx o valor desconocido. En cuanto a los par\u00e1metros, nij es el n\u00famero de ejemplos con valor j en el atributo i, n el n\u00famero total de ejemplos y nijk el n\u00famero de ejemplos con valor j en el atributo i y que pertenece a la clase k. Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 220 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Atributo Num\u00e9rico y Clase Simb\u00f3lica Se ordenan los ejemplos seg\u00fan el atributo Ai y se considera cada zx, definido como el punto medio entre los valores vx y v x+1, del atributo como posible punto de corte. Se consideran entonces como po sibles valores del atributo el rango menor o igual a zx, mayor a zx y valor desconocido. Se calcula la entrop\u00eda (ecuaci\u00f3n 2.20) del rango tomando como base esos tres posibles valores restringidos del atributo. Atributo Simb\u00f3lico y Clase Num\u00e9rica Se vuelve a tomar como base cada vez un valor del atributo, tal y como se hac\u00eda en el caso Atributo Simb\u00f3lico y Clase Simb\u00f3lica , pero en este caso se calcula la varianza de la clase para los valores del atributo mediante la ecuaci\u00f3n 2.21. = =3 1j jj (2.21) En la ecuaci\u00f3n 2.21, Sj es la suma de los valores de la clase de los ejemplos con valor j en el atributo i, SSj es la suma de los valore s de la clase al cuadrado y Wj es la suma de los pesos de los ejem plos (n\u00famero de ejemplos si no se incluyen pesos) con valor j en el atributo. Atributo Num\u00e9rico y Clase Num\u00e9rica Se considera cada valor del atributo como punto de corte tal y como se hac\u00eda en el caso Atributo Num\u00e9rico y Clase Simb\u00f3lica . Posteriormente, se calcula la varianza tal y como se muestra en la ecuaci\u00f3n 2.21. En cualquiera de los cuatro casos que se han comentado, lo que se busca es el valor m\u00ednimo de la ecuaci\u00f3n calculada, ya sea la entrop\u00eda o la varianza. De esta forma se obtiene el atributo que ser\u00e1 ra\u00edz del \u00e1rbol de decisi\u00f3n y sus tres ramas. Lo \u00fanico que se har\u00e1 por \u00faltimo es construir dicho \u00e1rbol: cada rama finaliza en un nodo hoja con el valor de la clase, que ser\u00e1 la media o la moda de los ejemplos que se clasifican por ese camino, seg\u00fan se trate de una clase num\u00e9rica o simb\u00f3lica. 5.6. 1R en WEKA La clase weka.classifers.OneR.java implementa el algoritmo 1R. La \u00fanica opci\u00f3n configurable es la que se muestra en la tabla 2.4. Tabla 5.3: Opciones de configuraci\u00f3n para el algoritmo 1R en WEKA. Opci\u00f3n Descripci\u00f3n minBucketSize N\u00famero m\u00ednimo de ej emplos que deben pertenecer a un Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 221 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda (6) conjunto en caso de atributo num\u00e9rico. La implementaci\u00f3n que se lleva a cabo en WEKA de este algoritmo cumple exactamente con lo descrito anteriormente. Como vemos, 1R es un clasificador mu y sencillo, que \u00fanic amente utiliza un atributo para la clasificaci\u00f3n. Sin embargo, a\u00fan hay otro clasificador m\u00e1s sencillo, el 0R, implementado en weka.classifers.ZeroR.java , que simplemente calcula la media en el caso de tener una clase num\u00e9rica o la moda, en caso de una clase simb\u00f3lica. No tiene ning\u00fan tipo de opci\u00f3n de configuraci\u00f3n. 5.7. PRISM en WEKA La clase weka.classifers.Prism.java implementa el algoritmo PRISM. No tiene ning\u00fan tipo de configuraci\u00f3n posible. \u00dan icamente permite atributos nominales, la clase debe ser tambi\u00e9n nominal y no puede haber atributos con valores desconocidos. La implementaci\u00f3n de es ta clase sigue completamente el algoritmo expuesto en la figura 2.10. 5.8. PART en WEKA La clase weka.classifers.j48.PART.java implementa el algor itmo PART. En la tabla 2.5 se muestran las opciones de configuraci\u00f3n de dicho algoritmo. Tabla 5.4: Opciones de configuraci\u00f3n para el algoritmo PART en WEKA. Opci\u00f3n Descripci\u00f3n minNumObj (2) N\u00famero m\u00edni mo de instancias por hoja. binarySplits (False) Con los atributos nominales tambi\u00e9n no se divide (por defecto) cada nodo en dos ramas. confidenceFactor (0.25) Factor de confianza para el podado del \u00e1rbol. reducedErrorPruning (False) Si se activa esta opci\u00f3n, el proceso de podado no es el propio de C4.5, sino que el conjunto de ejemplos se divide en un subconjunto de entrenamiento y otro de test, de los cuales el \u00faltimo servir \u00e1 para estimar el error para la poda. numFolds (3) Define el n\u00famero de subconjuntos en que hay que dividir el conjunto de ejemplos para, el \u00faltimo de ellos, emplearlo como conjunto de test si se activa la opci\u00f3n reducedErrorPruning . Como se ve en la t abla 2.5, las opciones de l algoritmo PART son un subconjunto de las ofrecidas por J48, que implementa el si stema C4.5, y es Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 222 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda que PART emplea muchas de las clases que implementan C4.5, con lo que los c\u00e1lculos de la entrop\u00eda, del error esperado,... son los mismos. La implementaci\u00f3n que se realiza en W EKA del sistema PART se corresponde exactamente con lo comentado anteriorm ente, y m\u00e1s teniend o en cuenta que los implementadores de la versi\u00f3n son los propios creadores del algoritmo. Por \u00faltimo, en cuanto a los tipos de datos admitidos por el algoritmo, estos son num\u00e9ricos y simb\u00f3licos para los atri butos y simb\u00f3lico para la clase. 5.9. Naive Bayesiano en WEKA El algoritmo naive Bayesiano se encuentra implementado en la clase weka.classifiers.NaiveBayesSimple.java . No dispone de ninguna opci\u00f3n de configuraci\u00f3n. El algoritmo que im plementa esta clase se corresponde completamente con el expuesto anteriorm ente. En este caso no se usa el estimador de Laplace , sino que la aplicaci\u00f3n muestra un error si hay menos de dos ejemplos de entrenam iento para una terna atributo-valor-clase o si la desviaci\u00f3n t\u00edpica de un atributo num\u00e9rico es igual a 0. Una alternativa a esta clase que tambi\u00e9n implementa un clasificador naive Bayesiano es la clase weka.classifiers.NaiveBayes.java . Las opciones de configuraci\u00f3n de que disponen son la s mostradas en la tabla 2.6. Tabla 5.5: Opciones de configuraci\u00f3n para el algoritmo Bayes naive en WEKA. Opci\u00f3n Descripci\u00f3n useKernelEstimator (False) Emplear un estimador de densidad de n\u00facleo (ver punto 2.3.3) para modelar los atri butos num\u00e9ricos en lugar de una distribuci\u00f3n normal. En este caso, sin embargo, en lugar de emplear la frecuencia de aparici\u00f3n como base para obtener las probabilida des se emplean distribuciones de probabilidad. Para los atributos discretos o simb\u00f3licos se emplean estimadores discretos, mientras que para los atribut os num\u00e9ricos se emplean bien un estimador basado en la distribuci\u00f3n no rmal o bien un estimador de densidad de n\u00facleo. Se crear\u00e1 una distribuci\u00f3n para cada cl ase, y una distribuci\u00f3n para cada atributo-clase , que ser\u00e1 discreta en el caso de que el atributo sea discreto. El estimador se basar\u00e1 en una distribuci\u00f3n normal o kernel en el caso de los atributo-clase con atributo num\u00e9rico seg\u00fan se active o no la opci\u00f3n mostrada en la tabla 2.6. En el caso de los atributos num\u00e9ricos, en primer lugar se obtiene la precisi\u00f3n de los rangos, que por defecto en la implementaci\u00f3n ser\u00e1 de 0,01 pero que se Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 223 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda calcular\u00e1 siguiendo el algor itmo descrito, mediante ps eudoc\u00f3digo, en la figura 2.15. Precisi\u00f3n (ejemplos, atributo) { p = 0.01 // valor por defecto // se ordenan los ejemplos de acuerdo al atributo num\u00e9rico Ordenar_ejemplos (ejemplos, atributo) vUltimo = Valor(ejemplos(0), atributo) delta = 0; distintos = 0; Para cada ejemplo (ej) de ejemplos vActual = Valor (ej, atributo) Si vActual <> vUltimo Entonces delta = delta + (vActual - vUltimo) vActual = vUltimo distintos = distintos + 1 Si distintos > 0 Entonces p = delta / distintos Devolver p } Figura 5.1: Algoritmo empleado para definir la prec isi\u00f3n de los rangos para un atributo. Una vez obtenida la precisi\u00f3n de los ra ngos, se crea el estimador basado en la distribuci\u00f3n correspondiente y con la pr ecisi\u00f3n calculada. Se recorrer\u00e1n los ejemplos de entrenamiento y de esta forma se generar\u00e1 la distribuci\u00f3n de cada atributo-clase y de cada clase. Cuando se desee clasificar un ejemplo el proceso ser\u00e1 el mismo que se coment\u00f3 anteriormente, y que se basaba en la ecuaci\u00f3n 2.27, pero obteniendo las probabilidades a partir de estas distri buciones generadas. En el caso de los atributos num\u00e9ricos, se calcul ar\u00e1 la probabilidad del rango [x-precisi\u00f3n, x+precisi\u00f3n] , siendo x el valor del atributo. 5.10. VFI en WEKA El clasificador VFI se implementa en la clase weka.classifiers.VFI.java . Las opciones de configuraci\u00f3n de que disp one son las que se muestran en la tabla 2.7. Tabla 5.6: Opciones de configuraci\u00f3n para el algoritmo Bayes naive en WEKA. Opci\u00f3n Descripci\u00f3n weightByConfidence (True) Si se mantiene activa esta opc i\u00f3n cada atributo se pesar\u00e1 conforme a la ecuaci\u00f3n 2.29. bias (0.6) Par\u00e1metro de configuraci\u00f3n para el pesado por confianza. El algoritmo que se implem enta en la clase VFI es similar al mostrado en la figura 2.16. Sin embargo, sufre ca mbios sobretodo en el proceso de clasificaci\u00f3n de un nuevo ejemplar: Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 224 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda La normalizaci\u00f3n de los intervalos por clase se realiza durante la clasificaci\u00f3n y no durant e el entrenamiento. Si se activa la opci\u00f3n de pesado por confianza , cada voto de cada atributo a cada clase se pesa mediante la ecuaci\u00f3n 2.29 I(Ai) es la entrop\u00eda del atributo Ai, siendo n el n\u00famero total de ejemplares, nC el n\u00famero de clases y ni el n\u00famero de ejemplares de la clase i. El par\u00e1metro bias es el que se configur\u00f3 como entrada al sistema, tal y como se mostraba en la tabla 2.7. En cuanto a los atributos, pueden ser num\u00e9ricos y simb\u00f3licos, mientras que la clase debe ser simb\u00f3lica. Relacionado con este clasificador se enc uentra otro que se implementa en la herramienta WEKA. Se trata de la clase weka.classifiers.HyperPipes.java . Este clasificador no tiene ning\u00fan par\u00e1metro de configuraci\u00f3n y es una simplificaci\u00f3n del algoritmo VFI: En este caso se almacena para cada atributo num\u00e9rico el m\u00ednimo y el m\u00e1ximo valor que dicho atri buto obtiene para cada clase, mientras que en el caso de los atri butos simb\u00f3licos marca los valores que el atributo tiene para cada clase. A la hora de clas ificar un nuevo ejemplo, simplemente cuenta, para cada clase, el n\u00famero de atributos que se encuentran en el intervalo almacenado en el caso de at ributos num\u00e9ricos y el n\u00famero de atributos simb\u00f3licos con valor activado en dicha clase. La clase con mayor n\u00famero de coincidencias gana . 5.11. KNN en icador KNN el nombre IBk, concretamente en la clase weka.classifiers.IBk.java . Adem\u00e1s, en la clase weka.classifiers.IB1.java hay una versi\u00f3n simplificada del mismo, concretamente un clasificador NN [N earest Neighbor], sin ning\u00fan tipo de opci\u00f3n, en el que, como su propio nom bre indica, tiene en cuenta \u00fanicamente el voto del vecino m\u00e1s cercano. Por e llo, en la tabla 2.8 se muestran las opciones que se permiten con el clasificador IBk. Tabla 5.7: Opciones de configuraci\u00f3n para el algoritmo IBk en WEKA. Opci\u00f3n Descripci\u00f3n KNN (1) N\u00famero de vecinos m\u00e1s cercanos. distanceWeighting (No distance weighting) Los valores posibles 1-distance y Permite definir si se deben \"pesar\" los vecinos a la hora de votar bien seg\u00fan su semejanza o con la inversa de su distancia con respecto al ejemplo a clasificar. Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 225 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda crossValidate (False) Si se activa esta opci\u00f3n, cuando se vaya a clasificar una instancia se selecciona el n\u00famero de vecinos (hasta el n\u00famero especificado en la opci\u00f3n KNN) mediante el proceso hold-one-out . meanSquared (False) Minimiza el error cuadr\u00e1tico en lugar del error absoluto para el caso de clases num\u00e9ricas cuando se activa la opci\u00f3n crossValidate . windowSize (0) Si es 0 el n\u00famero de ejemplos de entrenamiento es ilimitado. Si es mayor que 0, \u00fanicamente se almacenan los n \u00faltimos ejemplos de entrenamiento, siendo n el n\u00famero que se ha especificado. debug (False) Muestra el proceso de construcci\u00f3n del clasificador. noNormalization (False) No normaliza los atributos. El algoritmo implementado en la herra mienta WEKA consiste en crear el clasificador a partir de los ejempl os de entrenamiento, simplemente almacenando todas las instanc ias disponibles (a menos que se restrinja con la opci\u00f3n windowSize ). Posteriormente, se clasif icar\u00e1n los ejemplos de test a partir del clasificador gener ado, bien con el n\u00famero de vecinos especificados o comprobando el mejor k si se activa la opci\u00f3n crossValidate . En cuanto a los tipos de datos permitidos y las propiedades de la implementaci\u00f3n, estos son: Admite atributos num\u00e9ricos y simb\u00f3licos. Admite clase num\u00e9rica y simb\u00f3lica. Si la clase es num\u00e9rica se calcular\u00e1 la media de los valores de la clase para los k vecinos m\u00e1s cercanos. Permite dar peso a cada ejemplo. El proceso de hold-one-out consiste en, para cada k entre 1 y el valor configurado en KNN (ver t abla 2.8), calcular el error en la clasificaci\u00f3n de los ejemplos de entrenamiento. Se escoge el k con un menor error obtenido. El error cometido para cada k se calcula como el error medio absoluto o el cuadr\u00e1tico (ver tabla 2. 8) si se trata de una clase num\u00e9rica. El c\u00e1lculo de estos dos errores se puede ver en las ecuaciones 2.33 y 2.34 respectivamente. Si la clase es simb\u00f3lica se tomar\u00e1 como error el n\u00famero de ejemplos fallados entre el n\u00famero total de ejemplos. myy i = = (2.34) En las ecuaciones 2.33 y 2.34 yi es el valor de la clase para el ejemplo i e iy es el valor predicho por el modelo para el ejemplo i. El n\u00famero m ser\u00e1 el n\u00famero de ejemplos. Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 226 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 5.12. K* en WEKA La clase en la que se implementa el algoritmo K* en la herramienta WEKA es weka.classifers.kstar.KStar.java . Las opciones que permite este algoritmo son las que se muestran en la tabla 2.9. Tabla 5.8: Opciones de configuraci\u00f3n para el algoritmo K* en WEKA. Opci\u00f3n Descripci\u00f3n entropicAutoBlend (False) Si se activa esta opci\u00f3n se calcula el valor de los par\u00e1metros x0 (o s) bas\u00e1ndose en la entrop\u00eda en lugar del par\u00e1metro de mezclado. globalBlend (20) Par\u00e1metro de mezclad o, expresado en tanto por ciento. missingMode (Average column entropy curves) Define c\u00f3mo se tratan los valores desconocidos en los ejemplos de entrenamiento: las opciones posibles son Ignore the Instance with missing value (no se tienen en cuenta los ejemplos con atributos desconocidos), Treat missing value as maximally different (diferencia igual al del vecino m\u00e1s lejano considerado), Normilize over the attributes (se ignora el atributo desconocido) y Average column entropy curves (ver ecuaci\u00f3n 2.41). Dado que los autores de la implementaci \u00f3n de este algoritmo en WEKA son los autores del propio algoritmo, dich a implementaci\u00f3n se corresponde perfectamente con lo visto anteriorment e. Simplemente son destacables los siguientes puntos: Admite atributos num\u00e9ricos y simb\u00f3licos, as\u00ed como pesos por cada instancia. Permite que la clase sea simb\u00f3lica o num\u00e9rica. En el caso de que se trate de una clase num\u00e9rica se empl ear\u00e1 la ecuaci\u00f3n 2.45 para predecir el valor de ejemplo de test. ()() () ===n 1in 1i a|b*Pv(b)*a|b*Pav (2.45) En la ecuaci\u00f3n 2.45 v(i) es el valor (num\u00e9rico) de la clase para el ejemplo i, n el n\u00famero de ejemplos de entrenamiento, y P*(i|j) la probabilidad de transformaci\u00f3n del ejemplo j en el ejemplo i. Proporciona cuatro modos de ac tuaci\u00f3n frente a p\u00e9rdidas en los atributos en ejemplos de entrenamiento. Para el c\u00e1lculo de los par\u00e1metros x0 y s permite basarse en el par\u00e1metro b o en el c\u00e1lculo de la entrop\u00eda. Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 227 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Las ecuaciones para el c\u00e1lculos de P* y de la esfera de influencia no son las comentadas en la explicaci\u00f3n del algoritmo, sino las empleadas en los ejemplos de las figuras 2.20 y 2.21. 5.13. Redes de Neuronas en WEKA La clase en la que se implementan las redes de neuronas en weka es weka.classifiers.neural.NeuralNetwork.java . Las opciones que permite configurar son las que se muestran en la tabla 2.10. Tabla 5.9: Opciones de configuraci\u00f3n para la s redes de neuronas en WEKA. Opci\u00f3n Descripci\u00f3n momentum (0.2) Factor que se utiliza en el proceso de actualizaci\u00f3n de los pesos. Se multiplica este par\u00e1metro por el peso en el momento actual (el que se va a actualizar) y se suma al peso actualizado. validationSetSize (0) Determina el porcentaje de patrones que se emplear\u00e1n como test del sist ema. De esta forma, tras cada entrenamiento se va lidar\u00e1 el sistema, y terminar\u00e1 el proceso de entrenamiento si la validaci\u00f3n da un valor menor o igual a 0, o si se super\u00f3 el n\u00famero de entrenamient os configurado. nominalToBinaryFilter (False) Transforma los atributos nominales en binarios. learningRate (0.3) Raz\u00f3n de aprendizaje. Tiene valores entre 0 y 1. hiddenLayers (a) Determina el n\u00fam ero de neuronas ocultas. Sus posibles valores proceso de validaci\u00f3n arroja unos resultados en cuanto al error que empeoran durante el n veces consecutivas (siendo nel valor de esta variable), se detiene el aprendizaje. reset (True) Permite al sistema modificar la raz\u00f3n de aprendizaje autom\u00e1ticamente (la divide entre 2) y comenzar de nuevo el proceso de aprendiz aje si el proceso de entrenamiento no converge. GUI (False) Visualizaci\u00f3n de la re d de neuronas. Si se activa esta opci\u00f3n se puede modificar la red de neuronas, parar el proceso de entrenamiento en cualquier momento, modificar par\u00e1metros como el de la raz\u00f3n de aprendizaje,... autoBuild (True) El sistema c onstruye autom\u00e1ticamente la red bas\u00e1ndose en las entradas, salidas y el par\u00e1metro hiddenLayers . Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 228 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda normalizeNumericClass (True) Normaliza los posibles valores de la clase si \u00e9sta es num\u00e9rica, de forma que est\u00e9n entre -1 y 1. decay (False) La raz\u00f3n de ganancia se modifica con el ciclo de aprendizaje: = /n, donde nes el n\u00famero de ciclo de aprendizaje actual. trainingTime (500) N\u00famero total de ciclos de aprendizaje. normalizeAttributes (True) Normaliza los atributos num\u00e9ricos para que est\u00e9n entre -1 y 1. randomSeed (0) Semilla para generar los n\u00fameros aleatorios que inicializar\u00e1n los par\u00e1metros de la red. La implementaci\u00f3n de redes de neuronas que se realiza en la herramienta se ci\u00f1e al algoritmo de retropropagaci\u00f3n. Algunas caracter\u00edsticas que se pueden destacar de esta implementaci\u00f3n son: Se admiten atributos num\u00e9ricos y simb\u00f3licos. Se admiten clases num\u00e9ricas (predi cci\u00f3n) y simb\u00f3licas (clasificaci\u00f3n). Permite la generaci\u00f3n manual de redes que no se ci\u00f1an a la arquitectura mostrada anteriormente, por ejem plo, eliminando conexiones de neuronas de una capa con la siguiente. Como funci\u00f3n sigmoidal se utiliza la restringida entre 0 y 1 (ver ecuaci\u00f3n 2.48). Los ejemplos admiten pesos: Cuando se aprende con dicho ejemplo se multiplica la raz\u00f3n de apr endizaje por el peso del ejemplo. Todo esto antes de dividir la raz\u00f3n de apr endizaje por el n\u00famero de ciclo de aprendizaje si se activa decay . 5.14. Regresi\u00f3n Lineal en WEKA Es en la clase weka.classifers.LinearRegression.java en la que se implementa la regresi\u00f3n lineal m\u00faltiple. Las opcio nes que permite este algoritmo son las que se muestran en la tabla 2.11. Tabla 5.10: Opciones de configuraci\u00f3n para el al goritmo de regresi\u00f3n lineal en WEKA. Opci\u00f3n Descripci\u00f3n AttributeSeleccionMethod (M5 method) M\u00e9todo de selecci\u00f3n del at ributo a elim inar de la regresi\u00f3n. Greedy y None . debug (False) Muestra el proceso de construcci\u00f3n del clasificador. Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 229 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda La regresi\u00f3n lineal se cons truye tal y como se com ent\u00f3 anteriormente. Algunas propiedades de la im plementaci\u00f3n son: Admite atributos num\u00e9ricos y nominales. Los nominales con k valores se convierten en k-1 atributos binarios. La clase debe ser num\u00e9rica. Se permite pesar cada ejemplo. En cuanto al proceso en s\u00ed, si bien se c onstruye la regresi\u00f3n como se coment\u00f3 anteriormente, se sigue un proceso m\u00e1s comp licado para eliminar los atributos. El algoritmo completo ser\u00eda el siguiente: 1. Construir regresi\u00f3n para los atribut os seleccionados (en principio todos). 2. Comprobar la ecuaci\u00f3n 2.64 sobre todos los atributos. cii iSSbc= (2.64) En la ecuaci\u00f3n 2.64, Sc es la desviaci\u00f3n t\u00edpica de la clase. Se elimina de la regresi\u00f3n el atributo con mayo r valor si cumple la condici\u00f3n ci>1.5. Si se elimin\u00f3 alguno, volver a 1. 3. Calcular el error cuadr\u00e1tico m edio (ecuaci\u00f3n 2.63) y el factor Akaike tal como se define en la ecuaci\u00f3n 2.65. ( )2ppm AIC + = (2.65) En 2.65 m es el n\u00famero de ejem plos de entrenamiento, p el n\u00famero de atributos que fo rman parte de la regresi\u00f3n al llegar a este punto. 4. Escoger un atributo: a. Si el m\u00e9todo es Greedy , se generan regresiones lineales en las que se elimina un atributo distinto en cada una de ellas, y se escoge la regresi\u00f3n con menor error medio absoluto. b. Si el m\u00e9todo es M5, se calcula el valor de ci (ecuaci\u00f3n 2.64) para todos los atributos y se escoge el menor. Se genera la regresi\u00f3n sin el atributo i y se calcula la regresi\u00f3 n lineal sin dicho atributo. Se calcula el error medio absolut o de la nueva regresi\u00f3n lineal. c. Si el m\u00e9todo es None , se finaliza el proceso. 5. Mejorar regresi\u00f3n. Se calcula el nuevo factor Akaike con la nueva regresi\u00f3n como es mues tra en la ecuaci\u00f3n 2.66. Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 230 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda () 2p pmMSEMSEAICcc+ = (2.66) En la ecuaci\u00f3n 2.66 MSE c es el error cuadr\u00e1tico medio absoluto de la nueva regresi\u00f3n lineal y pc el n\u00famero de atributos de la misma. Mientras, MSE es el valor obtenido en el punto 3 y p el n\u00famero de par\u00e1metros al llegar al mismo. Si el valor nuevo de AIC es menor que el anterior, se actualiza \u00e9ste como nuevo y se m antiene la nueva regresi\u00f3n lineal, volviendo a intentar mejorarla (volver a 4). Si no es as\u00ed, se finaliza el proceso. 5.15. Regresi\u00f3n Lineal Ponderada Localmente en WEKA El algoritmo se implementa en la clase weka.classifers.LWR.java . Las opciones que permite configurar son las que se muestran en la tabla 2.12. Tabla 5.11: Opciones de configuraci\u00f3n para el algoritmo LWR en WEKA. Opci\u00f3n Descripci\u00f3n weightingKernel (0) Indica cu\u00e1l va a ser el m\u00e9todo para ponderar a los ejemplos de entrenamiento: 0, lineal; 1, inverso; 2, gaussiano. debug (False) Muestra el proceso de construcci\u00f3n del clasificador y validaci\u00f3n de los ejemplos de test. KNN (5) N\u00famero de vecinos que se tendr\u00e1n en cuenta para ser ponderados y calcular la regresi\u00f3n lineal. Si bien el valor por defecto es 5, si no se modifica o confirma se utilizan todos los vecinos. En primer lugar, las ecuac iones que se emplean en los m\u00e9todos para ponderar a los ejemplos de entrenamiento son: para el m\u00e9todo inverso, la ecuaci\u00f3n 2.67; para el m\u00e9todo lineal, la ecuaci\u00f3n 2. 68; y para el m\u00e9t odo gaussiano, la ie= (2.69) El proceso que sigue el algoritmo es el que se coment\u00f3 anter iormente. Algunas propiedades que hay que mencionar s obre la implementaci\u00f3n son: Se admiten atributos simb\u00f3licos y num\u00e9ricos. Se admiten ejemplos ya pesados, en cuyo caso, el peso obtenido del proceso explicado anterio rmente se multiplica por el peso del ejemplo. Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 231 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Se toma como par\u00e1metro de suavizad o la siguiente distancia mayor al del k-\u00e9simo ejemplo m\u00e1s pr\u00f3ximo. Para la generaci\u00f3n de la regresi\u00f3n lineal se emplea la clase explicada en el punto anterior (ver punto 2.3.1. 1), con los par\u00e1metros por defecto y con los ejemplos pesados. 5.16. M5 en WEKA La clase en la que se implementa el al goritmo M5 en la herramienta WEKA es weka.classifers.m5.M5Prime.java . Las opciones que permite este algoritmo son las que se muestran en la tabla 2.13. Tabla 5.12: Opciones de configuraci\u00f3n para el algoritmo M5 en WEKA. Opci\u00f3n Descripci\u00f3n ModelType (ModelTree) Permite seleccionar como model o a construir entre un \u00e1rbol de modelos, un \u00e1rbol de regres i\u00f3n o una regres i\u00f3n lineal. useUnsmoothed (False) Indica si se realizar\u00e1 el proceso de suavizado ( False ) o si no se realizar\u00e1 ( True). pruningFactor (2.0) Permite definir el factor de poda. verbosity (0) Sus posibles valores son 0, 1 y 2, y permite definir las estad\u00edsticas que se most rar\u00e1n con el modelo. En cuanto a la implementaci\u00f3n conc reta que se lleva a cabo en esta herramienta del algoritmo M5, cabe destacar lo siguiente: Admite atributos simb\u00f3licos y num\u00e9ricos; la clase debe ser, por supuesto, num\u00e9rica. Para la generaci\u00f3n de las regresi ones lineales se emplea la clase que implementa la regresi\u00f3n lineal m\u00fa ltiple en WEKA (punto 2.3.1.1). El n\u00famero m\u00ednimo de ejemplos que deben clasificarse a trav\u00e9s de un nodo para seguir dividiendo dicho n odo, definido en la constante SPLIT_NUM es 3.5, mientras la otra condici\u00f3n de parada, que es la desviaci\u00f3n t\u00edpica de las clases en el nodo respecto a la desviaci\u00f3n t\u00edpica de todas las clases del conjunto de entrenamiento, est\u00e1 fijada en 0.05. En realidad no se intenta minimizar el SDR tal y como se defini\u00f3 en la ecuaci\u00f3n 2.71, sino que se intenta minimizar la ecuaci\u00f3n 2.75, que se muestra a continuaci\u00f3n. Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 232 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 5 2 DD 5 2 II 5 2SnnSnnS SDR = (2.75) En la ecuaci\u00f3n 2.75 n es el n\u00famero total de ejemplos, nI y nD el n\u00famero de ejemplos del grupo izquierdo y derecho respectivamente; y S, S2 I y S2 D la varianza del conjunto completo , del grupo izquierdo y del grupo derecho respectivamente, defini\u00e9ndose la varianza como se muestra (2.76) En la ecuaci\u00f3n 2.76 n es el n\u00famero de ejemplos y xi el valor de la clase para el ejemplo i. El c\u00e1lculo del error de estimaci \u00f3n para un nodo dete rminado, mostrado en la ecuaci\u00f3n 2.73, se modifica ligeramente hasta llegar al que se muestra en la ecuaci\u00f3n 2.77. () la ecuaci\u00f3n 2.77 p es el factor de podado que es configurable y, como se ve\u00eda en la tabla 2.13, por defecto es 2. Por \u00faltimo, la constante k empleada en el modelo de suavizado (ecuaci\u00f3n 2.70) se co nfigura con el valor 15. Por lo dem\u00e1s la implement aci\u00f3n que se lleva a cabo respeta en todo momento el algoritmo mostrado en la figura 2.26. 5.17. Kernel Density en WEKA Es en la clase weka.classifiers.KernelDensity en la que se implementa eel algoritmo de densidad de n\u00facleo. No se puede configurar di cho algoritmo con ninguna propiedad. Adem\u00e1s, s\u00f3lo se admit en clases simb\u00f3licas, a pesar de que los algoritmos de densidad de n\u00facleo, como se com ent\u00f3 anteriormente nacen como un m\u00e9todo de estimaci\u00f3n no para m\u00e9trica (clases num\u00e9ricas). A continuaci\u00f3n se muestran las principales propiedades de la implementaci\u00f3n as\u00ed como los atributos y clases permitidas: Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 233 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda En cuanto a los atributos, pueden ser num\u00e9ricos y simb\u00f3licos. La clase debe ser simb\u00f3lica. Como funci\u00f3n n\u00facleo [kernel] se emplea la distribuci\u00f3n normal o gaussiana (ecuaci\u00f3n 2.83) normalizada, esto es, con media 0 y desviaci\u00f3n t\u00edpica 1. Como tama\u00f1o de ventana se emplea n1h= , siendo n el n\u00famero de ejemplos de entrenamiento. Para clasificar el ejemplo Ai, para cada ejemplo de entrenamiento Aj se calcula la ecuaci\u00f3n 2.92. la ecuaci\u00f3n 2.92, dist es la distancia entre el ejemplo de test y uno de los ejemplos de entrenam iento, definida tal y co mo se describe en la figura 2.19. El resultado de esta ecuaci\u00f3n para el par Ai-Aj se sumar\u00e1 al resto de resultados obtenidos para la clase a la que pertenezca el ejemplo Aj. El pseudoc\u00f3digo del algorit mo implementado por WEKA es el que se muestra en la figura 2.30. Kernel Density (ejemplo) { Para cada ejemplo de entrenamiento (E) Hacer prob = 1 c = clase de E Para cada atributo de E (A) Hacer temp = V(ejemplo, A) Si temp < LB Entonces prob = prob * LB Si no prob 5.2: Pseudoc\u00f3digo del algoritmo Kernel Density. La clase que obtenga una mayor probabilid ad ser\u00e1 la que resulte ganadora, y la que se asignar\u00e1 al ejemplo de test. En cuanto a la constante LB, se define en la ecuaci\u00f3n 2.93. 1-t1min LB= (2.93) En el n\u00famero m\u00edni mo almacenable por un double en Java y t el n\u00famero de atributos de los ej emplos (incluida la clase). Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 234 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 5.18. k-means en WEKA El algoritmo de k-medias se encuentra implementado en la clase weka.clusterers.SimpleKMeans.java . Las opciones de configuraci\u00f3n de que disponen son las que vemos en la tabla 2.14. Tabla 5.13: Opciones de configuraci\u00f3n para el algoritmo k-medias en WEKA. Opci\u00f3n Descripci\u00f3n numClusters (2) N\u00famero de clusters. seed (10) Semilla a partir de la cu\u00e1l se genera el n\u00famero aleatorio para inicializar los centros de los clusters. El algoritmo es exactamente el mi smo que el descrito anteriormente. A continuaci\u00f3n se enumeran los tipos de datos que admite y las propiedades de la implementaci\u00f3n: Admite atributos simb\u00f3licos y num\u00e9ricos. Para obtener los centroides inicia les se emplea un n\u00famero aleatorio obtenido a partir de la semilla empleada. Los k ejemplos correspondientes a los k n\u00fameros enteros siguient es al n\u00famero aleatorio obtenido ser\u00e1n los que conformen dichos centroides. En cuanto a la medida de similaridad, se emplea el mismo algoritmo que el que ve\u00edamos en el algoritmo KNN (figura 2.19). No se estandarizan lo s argumentos, sino que se normalizan (ecuaci\u00f3n 2.96). l ll il min Maxminx (2.96) En la ecuaci\u00f3n 2.96, xif ser\u00e1 el valor i del atributo f, siendo min f el m\u00ednimo valor del atributo f y Max f el m\u00e1ximo. 5.19. COBWEB en WEKA El algoritmo de COBWEB se enc uentra implementado en la clase weka.clusterers.Cobweb.java . Las opciones de configur aci\u00f3n de que disponen son las que vemos en la tabla 2.15. Tabla 5.14: Opciones de configuraci\u00f3n para el algoritmo COBWEB en WEKA. Opci\u00f3n Descripci\u00f3n Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 235 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda acuity (100) Indica la m\u00ednima varianza permitida en un cluster cutoff (0) Factor de poda. Indica la mejora en utilidad m\u00ednima por una subdivisi\u00f3n para que se permita llevar a cabo. La implementaci\u00f3n de COBWEB en WEKA es similar al algoritmo explicado anteriormente. Algunas caracter\u00edsti cas de esta implementaci\u00f3n son: Se permiten atributos num\u00e9ricos y simb\u00f3licos. La semilla para obtener n\u00fameros aleatorios es fija e igual a 42. Permite pesos asociados a cada ejemplo. Realmente el valor de cutoff es ()21 0.01 \u00d7. En el caso de que el ejemplo que se desea clas ificar genere, en un nodo determinado, un CU menor al cutoff , se eliminan los hijos del nodo (poda). 5.20. EM en WEKA El algoritmo EM se encuentra implementado en la clase weka.clusterers.EM.java . Las opciones de configur aci\u00f3n de que disponen son las que vemos en la tabla 2.16. Tabla 5.15: Opciones de configuraci\u00f3n para el algoritmo EM en WEKA. Opci\u00f3n Descripci\u00f3n numClusters (-1) N\u00famero de clusters. Si es n\u00famero es -1 el algoritmo determinar\u00e1 autom\u00e1ticamente el n\u00famero de clusters. maxIteration (100) N\u00famero m\u00e1ximo de it eraciones del algoritmo si esto no convergi\u00f3 antes. debug (False) Muestra informaci\u00f3n sobre el proceso de clustering. seed (100) Semilla a partir de la c u\u00e1l se generan los n\u00famero aleatorios del algoritmo. minStdDev (1e-6) Desviaci\u00f3n t\u00edpica m\u00ednima admis ible en las distribuciones de densidad. En primer lugar, si no se especifica el n\u00famero de clusters, el algoritmo realiza un primer proceso consist ente en obtener el n\u00famero \u00f3ptimo de clusters. Se realiza mediante validaci\u00f3n cruzada con 10 conjuntos [folders]. Se va aumentando el n\u00famero de clusters hasta que se aumenta y empeora el resultado. Se ejecuta el algoritmo en diez ocasiones, c ada una de ellas con nueve conjuntos de entrenam iento, sobre los que se ejecuta EM con los par\u00e1metros escogidos y posteriormente se valida el sistema sobre el conjunto de test, obteniendo como medida la verosimilitud sobre dicho conjunto. Se calcula la media de las diez medidas obtenidas y se toma como base para determinar si se contin\u00faa o no aumentando el n\u00famero de clusters. Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 236 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Una vez seleccionado el n\u00famer o \u00f3ptimo de clusters, se procede a ejecutar el algoritmo EM sobre el co njunto total de entrenamient o hasta un m\u00e1ximo de iteraciones que se configur\u00f3 previamente (ver tabla 2.16) si es que el algoritmo no converge previamente. En cuanto a los tipos de atributos con admite el algoritmo y algunas propiedades interesantes, \u00e9stas son: En cuanto a los atributos, \u00e9stos pueden ser num\u00e9ricos o simb\u00f3licos. Se entiende que se converge si en la siguiente iteraci\u00f3n la verosimilitud aumenta en menos de 1e-6. No tiene en cuenta posibles co rrelaciones entre atributos. 5.21. Asociaci\u00f3n A Priori en WEKA La clase en la que se impl ementa el algoritmo de as ociaci\u00f3n A Priori es weka.associations.Apriori.java . Las opciones que permite configurar son las que se muestran en la tabla 2.17. Tabla 5.16: Opciones de configuraci\u00f3n para el algori tmo de asociaci\u00f3n A Priori en WEKA. Opci\u00f3n Descripci\u00f3n numRules (10) N\u00famero de reglas requerido. metricType (Confidence) Tipo de m\u00e9trica por la que ordenar las reglas. Las opciones de la m\u00e9trica empleada. Su valor por defecto depende del tipo de m\u00e9trica empleada: 0.9 para Confidence, 1.1 para Lift y Conviction y 0.1 para Leverage. delta (0.05) Constante por la que va decreciendo el soporte en cada iteraci\u00f3n del algoritmo. upperBoundMinSupport (1.0) M\u00e1ximo valor del soporte de los item-sets . Si los item- sets tienen un soporte mayor, no se les toma en consideraci\u00f3n. lowerBoundMinSupport (0.1) M\u00ednimo valor del soporte de los item-sets . significanceLevel (-1.0) Si se emplea, las reglas se validan para comprobar si su correlaci\u00f3n es estad\u00edstic amente significativa (del nivel requerido) mediante el test 2. En este caso, la m\u00e9trica a utilizar es Confidence. removeAllMissingsCols (False) Si se activa, no se toman en consideraci\u00f3n los atributos con todos los valores perdidos. -I (s\u00f3lo modo texto) Si se activa, se muestran los itemsets encontrados. Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 237 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda En primer lugar, el algoritmo que implementa la herramienta WEKA es ligeramente distinto al explicado anteriorm ente. En la figura 2.36 se muestra el algoritmo concreto. Apriori (ejemplos, MS, mS) { /* MS: M\u00e1x. soporte; mS: M\u00edn. soporte */ S = MS-delta Mientras No Fin Generar ItemSets en rango (MS, S) GenerarReglas (ItemSets) MS = MS-delta S = S-delta Si suficientes reglas O S menor que mS Entonces Fin } GenerarReglas (ItemSets) { Para cada ItemSet Generar posibles reglas del ItemSet Eliminar reglas seg\u00fan la m\u00e9trica } Figura 5.3: Algoritmo A Priori en WEKA. As\u00ed, el algoritmo no obtiene de una vez todos los item-sets entre los valores m\u00e1ximo y m\u00ednimo permitido, sino que se va iterando y cada ve z se obtienen los de un rango determinado, que ser\u00e1 de tama\u00f1o delta (ver tabla 2.17). Adem\u00e1s, el algoritmo permite seleccionar las reglas atendien do a diferentes m\u00e9tricas. Adem\u00e1s de la confianza (e cuaci\u00f3n 2.106), se puede optar por una de las siguientes tres m\u00e9tricas. Lift: Indica cu\u00e1ndo una regla es me jor prediciendo el resultado que asumiendo el resultado de forma aleat oria. Si el resultado es mayor que uno, la regla es buena, pero si es menor que uno, es peor que elegir un resultado aleatorio. Se mues tra una regla de as ociaci\u00f3n indica la proporci\u00f3n de ejemplos adicionales cubiertos por dicha regla (tanto por la parte izquierda como por la derecha) sobre los cubiertos por cada parte si fueran independientes. Se mues tra en medida de implicaci\u00f3n. Es direccional y obtiene su m\u00e1ximo valor (infinito) si la implicac i\u00f3n es perfecta, esto es, si siempre que A ocurre sucede tambi\u00e9n B. Se muestra en la ecuaci\u00f3n 2.109. ()()() () B!APB!P*APB A convicci\u00f3n= (2.109) Cap\u00edtulo 5 Implementaci\u00f3n de las T\u00e9cnicas de An\u00e1lisis de Datos en Weka T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 238 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Por \u00faltimo, cabe destacar que esta implementaci\u00f3n permite \u00fanicamente atributos simb\u00f3licos. Adem\u00e1s, para mejora r la eficiencia del algoritmo en la b\u00fasqueda de item-sets , elimina todos los atributos que tengan sus valores desconocidos en todos los ejemplos. Cap\u00edtulo 6 Ejemplos sobre casos de estudio T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 239 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Cap\u00edtulo 6. Ejemplos sobre casos de estudio Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 240 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Bibliograf\u00eda [ACO02] Acosta Franco, Javier. \" Aplicaci\u00f3n de los Sistemas Clasificadores tradicionales al an\u00e1lisis de dat os. Adquisici\u00f3n autom\u00e1tica de reglas \". Proyecto Fin de Carrera, Universidad Carlos III de Madrid, 2002. [AGR96] A. Agresti. An Introduction to Categorical Data Analysis. New York: John Wiley & Sons, 1996. [AIS93b] R. Agrawal, T. Imielinski, and A. Swami. Mining association rules between sets of item s in large . In Proc. pages 207-216, Washington, DC, May 1993. [AKA73] Akaike, A. (1973). \" Information theory and an extension of the maximum likelihood principle \" In Petrov, B. N., and Saki, F. L.(eds.), Second International Sy mposium of Information Theory . Budapest. [AS94a] R. Agrawal R. Srikant. \" Fast algorithms for mining association rules in large databases \". In Research Report RJ 9839, IBM Almaden Research Center, San Jose, CA, June 1994. [AS94b] R. Agrawal and R. Srikant. \" Fast Mining Marketing, Sales, and Customer Support \" John Wiley, NY, 1997. [BMS97] S. Brin, R. Motwani, and C. Silverstein. \" Beyond market basket: Generalizing association rules to correlations In of Data (SIGMOD'97), pages 265-276, Tucson, AZ, May 1997. Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 241 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [BRIS96] G. Briscoe, and T. Caelli, \" A Compendiu m Machine Learning . Vol. 1: Symbolic Machine Learning.\" Ablex Publishing Corporation, New 1996. J. Cendrowska (1987). \" PRISM: An algorithm for concept to implementation . Prentice Hall 1997. [CHY96] M.S. Chen, J. Han, and P. S. Yu. \" Data mining: An overview from a database perspective \". IEEE Trans. and Data Engineering, 8:866-883, 1996. [CLTR95] John, G. Cleary and \" the 12th International C onference on Machine learning , pp. 108- 114. [CN89] P. Clark and T. Niblett. The CN2 induction algorithm. Machine Learning\". 3:261-283, 1989. [CODD70] E. F. Codd, \" A Relational Model of Data for Large Shared Data Banks ,\" Communications of the ACM, Vol. 13, 1970. [CR95] Y.Chauvin and D. Rumelhart. \" Backpropagation: Hillsdale, NJ: Lawrence Erlbaurn Workshop on Knowledge Discovery in Databases , Defense Advanced Research Projects Agency, Pittsburgh, PA, June 1998. [DEA97] Deaton, A. (1997): \" The Analysis of Household Surveys. A Microeconometric Approach to De velopment Policy. The World Bank \". The Johns Hopkins University Press. [DECI] Decision Support Journal , Elsevier/North Holland Publications. [DEGR86] T. DeGroot, \" Probability and Statistics ,\" Addison Wesley, MA, 1986. [DEV95] J. L. Devore. \" Probability and Statistics for Engineering and the Sciences \". 4th ed. New York: Duxbury Press, 1995. [DFL96] DiNardo, J., Fortin, N. and Lemieux, T. (1996): \" Labor Market Institutions and 64, No.5. September. Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 242 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [DH73] R. Duda and P. Hart. \" Pattern Classification and Scene Analysis \". New York: John Wiley & Sons, 1973. [DOB90] A. J. Dobson. \" An Introduction to Generalized Linear Models \". New York: Chapman and Hall, 1990. [FAYY96] U. Fayyad, et al. \" Advanced in Knowledge Discovery and Data Mining ,\" MIT Press, MA, 1996. [FIS87] D. Fisher, \" Improving inference through conceptual clustering \". Proc. 461-465, Seattle, WA, July 1987. [FRWI98] Eibe and Ian H. Witten (1998). \" Generating Ach\u00farate Rule Sets Without Global Optimization. \" In Shavlik, J., ed., Machine Learning: Proceedings of the Fifteenth Inter national Conference , Morgan Kaufmann Publishers, San Francisco, CA. [FU94] L. Fu, \" Neural Networks in Computer Intelligence \", New York, McGraw Hill, 1994 [FUR87] Furnas, G. W. et al. \" The vocabulary proble m in human system communication \". Communications of the ACM, 30, n\u00ba 11, Nov. 1987. [HALI94] H\u00e4rdle, W. and \" Science. [HH96] Hearst, M.; Hirsh, H. (eds.) Papers from the AAAI Spring Symposium on Machine Learning in information Access, Stanford, March 25-27, 1996. http://www.parc.xerox.com/istl/projects/mlia [HMM86] J. Hong, I. Mozetic, and R. S. Michalski. \" AQ15: Incremental learning of attribute-based de scriptions from examples, the In Report ISG 85-5, UIUCDCS-F-86-949, Departm ent of Computer Science, University of Illinois at Ur bana-Champaign, 1986. [HOL93] R.C. Holte (1993). \" on most commonly pp. 63-91. \"Parallel Architectures for Databases ,\" IEEE Tutorial, 1989 Hurson et al.). [JAM85] M. James. \" Classification Algorithms \". New York: John Wiley & Sons, 1985. [JOH97] G. H. John. \" Enhancements to the Data Mining Process \". Ph.D. Thesis, Computer Science D ept., Stanford University, 1997. Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 243 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [KB00] Kosala, R.; Blockeel, H. \" ACM SIGKDD Explorations, Newsletter of the ACM Special Interest Group on Knowledge Discovery and Data Mining, June 2000, Vol. 2, n\u00ba 1, pp. 1-15 [KODR90] Kodratoff, Y. and Michalski, R.S., \" Machine Learning and Artificial Inteligence Approach, Vol. 3 \", San Mateo, CA: Morgan Kaufmann, 1990 [LAN96] P. Langley. \" Elements of Machine Learning \". San Francisco: Morgan Kaufmann, 1996. [LOP01] A. L\u00f3pez Cilleros, \" Modelizaci\u00f3n del Comportamiento Humano para un Agente de la Robocup m ediante Aprendizaje Autom\u00e1tico \". Proyecto Fin de Carrera, Universidad Carlos III de Madrid, 2001. [MAC67] MacQueen. \" Some methods for classification and M. Kubat. Machine Learning and Data Mining. Methods and Applications. New York: John Wiley & Sons, 1998. [MIT97] T. Mitchell, \" Machine Learning ,\" McGraw Hill, NY, 1997. [MM95] J. Major and J. Mangano. \" Selecting among rules induced from a Systems\" , 4:39-52, 1995. [MORE98a] D. Morey, \" Knowledge \" Handbook of Data Management, J.G. Carbonell, and T.M, editors, Machine Lear ning: An Artificial Intelligence Approach, Vol 1. San Mateo, CA: Morgan Kaufmann, 1983. [PSF91] Piatesky-Shapiro Databases . Cambridge, MA:AAA/MIT Press, 1991 [PTVF96] W. H. Press, S. A. Teukolos ky, W. T. Vetterling, and B. P Flannery. \" Numerical Recipes in C: The Art of Scientific Computing \". Cambridge, UK: Cambridge University Press, 1996. [QUIN79] J.R.Quinlan, \" Discovering Rules from Large Collections of In Expert Systems in the Microelectronic Age, Michie, D. (Ed.), Edimburgo Univer sity Press, Edimburgo. 1979 Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 244 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [QUIN86] J.R. Quinlan, \" Induction of Decision Trees (ID3 algorithm )\", Machine Learning J., vol. 1, n\u00fam. 1, pp. 81-106. 1986 [QUIN87] J.R. Quinlan, \"Simplifying decision [QUIN93] J.R. C4.5: Programs for Machine ,\" Morgan Kaufmann, CA, 1993. [RHW86] D. E. Rumelhart, G. E. Hin ton, and R. J. Williams. \" Learning internal representations by error propagation \". In D. E. Rumelhart J. L. McClelland, editors, Parallel Distributed Processing. Cambridge, MA: MIT Press, 1986. R\u00edos Monta\u00f1o, Pablo Miguel. \" Sistemas Clasificadores Extendidos (XCS). Desarrollo de una librer\u00eda y comparaci\u00f3n CS vs. XCS \". Proyecto Fin de Carrera, Universidad Carlos III de Madrid, 2002. [RM86] D. E. Rumelhart and J. L. McClelland. \" Parallel Distributed Processing \". Cambridge, MA: MIT Press, 1986. [RMS98] S. Ramaswamy, S. Mahajan, and A. Silberschatz. \" On the discovery of interesting patterns in association rules \". In Proc. 1998 Int. Conf Very Large Data Ba ses (VLDB'98), pages 368-379, New York, Aug. 1998. [SIL86] Silverman, B. W. (1986): \" Density Estimation for Statistics and Data Analysis \", London and New York, Chapman and Hall. [SLK96] E. Simoudis, B. Livezey and 373. Cambridge, MA:AAAI/MIT Press, 1996 [SN88] K. and R. Nakano. \" Medical diagnostic expert system based on PDP model \". In Proc. IEEE International Conf on Neural Networks, volume 1, pages 225262. San Mateo, 1988. [THUR99] B. Thuraisingham, \" Data Mining: Technologies, Techniques, Tools and Trends .\" CRC Press, 1999 [UTG88] P.E. Utgoff, \" An Incremental ID3 .\" In Proc. Fifth Int. Conf. Machine Learning, pages 107-120, San Mateo, CA, 1988 [VIS95] Proceedings of the 1995 Workshop on Visualization and GA, October 1997 (Ed: [VIS97] the , Phoenix, AZ, October 19 97 (Ed: G. Grinstein). Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 245 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [WF00] H. Witten, and E Frank (2000). Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations . San Francisco, CA: Morgan S.M \" Mining \". San Francisco: Morgan Kaufmann 1998 [WK91] S.M. Weiss and C. A. Kulikowski. \" Computer Systems That Learn: Classification and Prediction Met hodsfrom Statistics, Neural Nets, Machine Learning, and Expert Systems \". San Mateo, CA: Morgan Kaufmann, 199 1. [ACM90] Special Issue Computing Surveys, 1990. on Next Generation Da Systems, Com- munications of the ACM, October 1991. [ACM95] Special Issue on Digital Librar ies, Communications of the ACM, May 1995. [ACM96a] Special Issue on Data Mini ng, Communications of the ACM, November 1996. [ACM96b] Special Issue on Electronics Commerce, Communications of the ACM, June 1996. [ADRI96] Adriaans, P., and Zantinge, D., \"Data Mining,\" Addison Wesley, [AFCE97] Proceedings the Federal Data Mining Symposium, Washington D.C., December 1997. [AFSB83] Air Force Summer Study Board Report on Multilevel Secure Database Systems, Department of Defense Document, 1983. Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 246 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [AGRA93] Agrawal, A. et al.., \"Databas e Mining a Performance Perspective,\" IEEE Transactions on Knowledge ani Da ta Engineering, Vol. 5, December 1993. [BANE87] Banerjee, J. et al., \"A Data Model for Object-Oriented Applications,\" ACM Transactions on Office Infonnat ion Systems, Vol. 5, 1987. [BELL92] Bell D. an d Grimson, J., \"Distributed Database Systems,\" al., \"Evolv able Systems Initiative for Realtime Command and Control Systems,\" Proceedings of the Ist IEEE Complex Systems Conference, Orl ando, FL, November 1995. [BERN87] Bernstein, P. et al., \"Concurrency Contro l and Recovery in Database Systems,\" Addison Wesley, MA, 1987. [BERR97] Berry, M. and Mining Co mpendium of Machine Vol. 1: Symbolic Machine Learning .\" Ablex Publishing Corporat ion, New Jersey, 1996. [BROD84] Brodie, M. et al., \"On C onceptual Modeling: Artificial NY, 1984. [BROD86] Brodie, M. and Mylopoulos, J., \"On Knowledge Base Management Systems,\" Springer Verlag, NY, 1986. [BROD88] Brodie, M. et al., \"Readings in Artificial Intelligence and Databases,\" Morgan Kaufmann, CA, 1988. Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 247 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [BROD95] Brodie M. and Stonebraker, M., \"Migrating Legacy Databases,\" Morgan CA, Mi Handbook McGraw Hill, NY, 1984. [CHAN73] Chang C., and Lee R., \"Symbolic Logic and Mechanical Theorem Proving,\" Academic Press, NY, 1973. Chen, hip Model - Unified View of Data,\" ACM Transactions on Data Mining,\" Proceedings of the ACM S IGMOD Conference Workshop on Data Mining, Montreal, Canada, June 1996. Clifton, C., \"Image Mining,\" Private Communication, Bedford, MA, July 1998. Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 248 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [CLIF98b] Clifton C., \"Privacy Issues for Data Mining,\" Private [CODD70] Codd, E. F., \"A Relational M odel of Data for Large Shared Data Banks,\" Communications of the ACM, Vol. 13, 1970. [COOL98] Cooley, R., \"Tax onomy for Web Mining,\" Private Communication, Bedford, MA, August 1998. [DARPA98] Workshop on Knowledge Di scovery in Databases, Defense Advanced Research Projects Agency, Pittsburgh, PA, June 1998. [DAS92] Das, S., \"Deductive Databases and Logic Programming,\" Addison Wesley, MA, 1992. [DATE90] J., \"An Introducti on to Database Addison Wesley, in by Addison Wesley). [DCI96] of t he DCI Conference on Databases and Client Server Computing, Boston, MA, March 1996. [DE98] Proceedings of the 1998 Data Engineering Conferenc e, February 1998. and Statistics,\" B. and Murphy, P.T., \"An Architecture for a Bussiness and Information System\". IBM Sys, J 27 , No 1, 1988 Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 249 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [DIGI95] Proceedings of the Advances in Digital Libraries Conference, McLean, VA, May 1995, (Ed: N. Adam et al.). [DIST98] Workshop on Distributed and Parallel Data Mining, Melbourne, Australia, April 1998. [DMH94] Data Management Handbook, Auer bach Publications, NY, 1994 (Ed: B. von Halle and D. Kull). [DMH95] Data Management Handbook S upplement, Auerbach Publications, NY, 1995 (Ed: B. von Halle and D. Kull). [DMH96] Data Management Handbook S upplement, Auerbach Proceedings of the 1994 DoD Database Colloquium, San Diego, CA, August 1994. [DOD95] Proceedings of the 1994 DoD Database Colloquium, San Diego, CA, August 1995. [DSV98] 1998. [FAYY96] Fayyad, U. et al., \"Advanc ed in Knowledge Discovery and Data Mining,\" MIT Press, MA, 1996. [FELD95] Feldman, R. and Dagan, I., \"Knowledge Discovery in Textual Databases (KDT),\" Proceedings of the 1995 Knowledge Discovery in Databases Conference, Mont real, Canada, August 1995. Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 250 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [FOWL97] Fowler, M. et al., \"UML Dis tilled: Applying the Object Modeling Language,\" Addiso n Wesley, MA, 1997. [FROS86] Frost, R., \"On Knowledge Ba se Management Systems,\" Collins NY, Goldberg, D., \"Genetic Algorit hms in Search, 1989 [GRIN95] Grinstein, and Th ning and Visualization: A Position Paper,\" Proceedings of the Workshop on Databas es in Visualization, Atlanta GA, October Mining,\" ote Address, Second Pacific Asia Conference on Data Mining, Mel bourne, Australia, April 1998. [HAN98] Han, J. M., in Database Management Systems,\" Proceedings of the 1988 Conference on Security and Privacy, Oakland, CA, April 1988. [ICTA97] Panel on Web Mining, International Conference on Tools for Artificial Intelligence, Newport Beach, CA, November 1997. Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 251 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [EEE89] \"Parallel Architectures for Da tabases,\" IEEE Tutorial, 1989 Hurson et al.). [IEEE91] Special Issue in Multidatabas e Systems, IEEE Computer, December 1991. [IEEE98] IEEE Data Engineer ing Bulletin, June 1998. Proceedings of the IDFIP Holland. [IFIP97] Mining,\" Proceedings of the 1997 IFIP Conference in Database Security, Lake Tahoe, CA, August 1997.. [ELP97] Summer School on Inductive Logic Programming, Prague, Czech Republic, September 1998. [INMO88] : The Information 1988. lding Warehouse,\" NY, 1993. [JUNG98] Junglee Corporation, \"Virtual Database Technology, XML, and the Evolution of the Web,\" IEEE Data E ngineering Bulletin, June 1998 (authors: Prasad and Rajaraman). [KDD95] Proceedings of the First Knowledge Discovery in Databases Conference, Montreal , Canada, August 1995. [KDD96] Proceedings of the Second Knowledge Discovery in Databases Conference, Portland, OR, August 1996. Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 252 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [KDD97] Proceedings of the Third Knowledge Di scovery in Databases Conference, Newport Beach, CA, August 1997. [KDD98] Proceedings of the Fourth Knowledge Di scovery in Databases Conference, New York, NY, August 1998. [KDP98] Panel on Privacy Issues for Data Mining, Knowledge Discovery in Databases Conference, New York, NY, August 1998. [KDT98] Tutorial on Commercial Data Mi ning Tools, Knowledge Discovery in Databases Conference, August 1998 (Pre senters: J. Elder and D. Abbott) [KIM85] Kim, W. et al., \"Query Processing in Database Systems,\" \"Machine and CA: Morgan Kaufmann, H. and NY, 1986. [KOWA74] R. A., \"Predica te Logic as a Programming Language,\" Information Processing 74, Stockhol m, North Holland Publications, 1974. [LIN97] Lin, T.Y., (Editor) \"Rough Sets and Data es,\" Addison Wesley, MA, 1995. Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 253 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [MAIE83] Maier, D., \"Theor y of Relational Databases,\" Computer Science 1983. [MATTO98] Mattox, al., \"So ftware Agents for Data Handbook of NY, 1998 (Ed: B. Thuraisingham). [MDDS94] Proceedings of the Massive Digital Data Systems Workshop, published by the Community Management Staff, Washington D.C., 1994. [MERL97] Merlino, A. \"Br oadcast News Navigation Proceedings erence, Seattle, WA, November 1998. [META96] Proceedings of the Ist IEEE Metadata Conference, Silver Spring, MD, April 1996 (Originally pub lished on the web, Editor : R. Musick, Livermore National Laboratory). [MICH92] Morgan Technical Reports on Data Quality, Sloan School, Massachusetts Institute of Technology, Cambridge, MA. [MIT96] Mitchell, M., \"An Introduction to Genetic T., \"Machine Learning,\" McGraw Hill, NY, 1997. Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 254 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [MORE98a] Morey, D., \"Knowledge Mana gement Architecture,\" Proc eedings of the 1987 ACM SIGMOD Conference, San Francisco, CA, June 1987. [NG97] Ng, R., \"Image Mining,\" Privat e Communication, Vancouver, British Columbia, December 1997. [NISS96] Panel on Data Warehousing, Da ta Mining, and Security, Proceedings of the 1996 National Information Systems Security Conference, Baltimore, MD, October 1996. [NISS97] Papers on Internet Securi ty, Proceedings of the 1997 National Information Systems Conference, Baltimore, MD, October 1997. [NSF90] Proceedings of t he Database Systems Worksh op, Report published by the National Science Foundation, 1990 (also in ACM SIGMOD Record, December 1990). [NSF95] Proceedings of t he Database Systems Workshop, Report published by the National Science Foundation, 1995 (a lso in ACM SIGMOD Record, March 1996). [NWOS96] Nwosu, K. Morgan Kaufmann, CA, 1993. Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 255 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [OMG95] \"Common Object Request Broker Architecture and Specification,\" OMG Publications, Databases Conference, Singapore, February 1997. [PAKDD98] Proceedings of the Second Knowledge Di in Databases Conference, Melbourne, Australia, Collections of Systems in the Microelectronic Age , Michie, D. (Ed.), Edimburgo University Press, Edimburgo. [QUIN86] Quinlan, J.R.: \"Induction of Decision Trees (ID3 algorithm)\", Machine Learning J ., vol. 1, 1986 [QUIN87] n Machine Intelligence , pp. 164-168, Ithaca, NY, June. 1989 Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 256 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [QUIN90] Quinlan, J.R., l 725-730, OR, Ramakrishnan, R., (Editor) Ap of the ACM SIGMOD Workshop on Data Mining, Montreal, Canada, May 1996. [SIGM98] Proceedings of t he 1998 ACM SIGMOD Conf erence, Seattle, WA, June 1998. [SIMO95] al., \"Recon Data Mining S ystem,\" Technical Report, Lockheed Martin Corporation, 1995. [SQL3] \"SQL3,\" American Na tional Standards Institute, Draft, 1992 (a version also presented by J. Melton at t he Department of Navy's DISWG NGCR meeting, Salt Lake City, UT, November 1994). Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 257 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [STAN98] Stanford Databas e Group Workshop, Jungalee Virtual Relational Database, 1998 (a lso by an Inference Engine ,\" Computers 1987. System MITRE Report, June 1990 (also published in the Proceedings of the 1992 Computer Security Foundations Workshop, Franconia, NH, June 1991). [THUR90b] Thuraisingham, B., \"Recursion T heoretic Properties of the Inference Problem,\" MITRE Report, June 1990 (als o presented at the 1990 Computer Security Foundations Works hop, Franconia, NH, June 1990). [THUR90c] Thuraisingham, B., \"Novel Approaches to Handle the Inference Problem,\" of the 1990 RADC Workshop in Castile, NY, June 1990. [THUR91] Thuraisingham, B., \"On the Use of Conceptual Structures to the Inference Problem,\" Proceedings Holland, Vol. 8, December 1993. [THUR95] Thuraisingham, B. and Ford, W., \"Security Constraint in Multilevel Secure Distributed Da tabase Management System,\" IEEE Transactions on Knowledge and Da ta Engineering, Vol. 7, 1995. [THUR96a] Thuraisingham, B., \"Data War ehousing, the 10th IFIP Database Security Conference, Como, Italy, 1996. Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 258 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [THUR96b] Thuraisingham, B., \"Inter net Thuraisingham, ve Data Paris, France, December 1996. [THUR97] Thuraisingham, B., \" Data Management System s Evolution and Interoperation,\" CRC Press, FL, May 1997. [THUR98] Thuraisingham, B., \"Data housing, Data Mining, and Security (Version 2),\" Keynote Address at Sec ond Pacific Asia Conference on Data Mining, Melbourne, Australia, April 1998. [THUR99] CRC Press, 1999 [TKDE93] Special Issue on Data Mini ng, IEEE Transactions on Knowledge and Data Engineering, December 1993. [TKDE96] Special Issue on Data Mini ng, IEEE Transactions on Knowledge and Data Engineering, December 1996. [TRUE89] Trueblood, R. and Potter, W., \"H yper-Semantic Data Modeling,\" Data and Knowledge Engineering Journal, Vol. 4, North Holland, 1989. [TSUR98] Tsur, D. et al., \"Query Flocks: A Generalization of Rule Mining,\" Proceedings of the 1998 ACM S IGMOD Conference, Seattle, WA, June 1998. [TSIC82] Tsichritzis, D. and Lochovsky, F., \"Data Models,\" Prentice Hall, NJ, 1982. Bibliograf\u00eda T\u00e9cnicas de An\u00e1lisis de Datos P\u00e1gina 259 de 266 \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [ULLM88] Ullman, J. D., \"Principle s of Database and Knowledge Base Management Systems,\" Volumes I and 11, Computer Science Press, MD 1988. [UTG88] Utgoff, P.E., \"An Incremental ID 3.\" In Proc. Fifth Int. Conf. Machine Learning, pages 107-120, San Mateo, CA, 1988 [VIS95] Proceedings of t he 1995 Workshop on Visualiz ation and Databases, Atlanta, GA, October 1 997 (Ed: [VIS97] Proceedings of the Very Large Database Conf erence, New York City, NY, August 1998. [WIED92] Wiederhold, G., \"Mediators in t he Architecture of Future Information Systems,\" IEEE Comp uter, March Databases,\" Proceedings of the ACM SIGMOD Conference, Washington DC, June 1986. [XML98] Extended Markup Language, Docu ment published by the World Wide Web Consortium, Cambridge, MA, February 1998. "}