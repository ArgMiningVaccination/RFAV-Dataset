{"title": "PDF", "author": "PDF", "url": "https://rua.ua.es/dspace/bitstream/10045/117498/1/PLN_67.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "................................ 27 Constructing Corpus and Word Embedding for Data Kyungjin Hwang ................................ ................................ ................................ ................................ .......... 37 Procesamiento de Punctuation and Capitalisation System for Spanish and Basque Ander Gonz\u00e1lez Aitor Montes -y-G\u00f3mez, Berta Chulvi ................................ ...... 83 El sentimiento de las letras de las canciones y su relaci\u00f3n con las caracter\u00eds ticas musicales Marco Palomeque, Juan de Lucio ................................ ................................ ................................ ............... 95 Reconocimiento y clasificaci\u00f3n de entidades nombradas en textos legales en espa\u00f1ol Doaa Samy ................................ ................................ ................................ ................................ ................. 103 Un enfoque sem\u00e1ntico en la selecci\u00f3n de caracter\u00edsticas basadas en l\u00e9xico para la detecci\u00f3n de emociones Haro ld Gonz\u00e1lez -Guerra, Alfredo Sim\u00f3n -Cuevas, Jos\u00e9 M. Perea -Ortega, Jos\u00e9 A. Olivas ...................... 115 Inducci\u00f3n autom\u00e1tica de una taxonom \u00eda multiling\u00fce de marcadores discursivos: primeros resultados en castellano, ................................ ................................ ..... tareas de evaluaci\u00f3n Overview of the EmoEvalEs task on emotion detection for Spanish at IberLEF 2021 Flor Miriam Plaza -del-Arco, Salud Mar\u00eda Jim\u00e9nez -Zafra, Arturo Montejo -R\u00e1ez, M. Dolores Molina - Gonz\u00e1lez, L. Alfonso Ure\u00f1a -L\u00f3pez, M. Teresa Mart\u00edn -Valdivia System for Text Mexican Tourism Miguel \u00c1. \u00c1lvarez -Carmona, Ram\u00f3n Aranda, Samuel Arce -Cardenas, Daniel Fajardo -Delgado, Rafael Guerrero -Rodr\u00edguez, A. Pastor L\u00f3pez -Monroy, Juan Mart\u00ednez Overview Beyond Text in Cross -Lingual Stance Detection Rodrigo Agerri, Roberto Centeno, Mar\u00eda Espinosa, Joseba Fernandez de Landa, \u00c1lvaro Rodrigo ........ 173 Overview of MeOffendEs at IberLEF 2021: -del-Arco , Marco Casavantes, Hugo Jair Escalante, M. Teresa Mart\u00edn Arturo Montejo -S\u00e1nchez, Jorge Carrilo -de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, Miriam Comet, Trinidad Donoso ................................ ................................ Mariona Ariza, Montserrat Nofre, Enrique Amig\u00f3, Paolo Rosso ................................ . 209 Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021 \u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalISSN: 1135 -5948 Comit\u00e9 Editorial Consejo de redacci\u00f3n L. Alfonso Ure\u00f1a L\u00f3pez Universidad de Ja\u00e9n laurena@ujaen.es (Director) Patricio Mart\u00ednez Barco Universidad de Alicante patricio@dlsi.ua.es (Secretario) Manuel Palomar Sanz Universidad de Alicante mpalomar@dlsi.ua.es Felisa Verdejo Ma\u00edllo UNED felisa@lsi.uned.es ISSN : 1135 electr\u00f3nico 1989 -7553 Dep\u00f3sito Legal : B:3941 -91 Editado en: Universidad de Ja\u00e9n A\u00f1o de edici\u00f3n: 2021 Editores: Eugenio Mart\u00ednez C\u00e1mara Universidad de Granada emcamara@decsai.ugr.es \u00c1lvaro Rodrigo Yuste UNED alvarory@lsi.uned.es Paloma Mart\u00ednez Fern\u00e1ndez Universidad Carlos III pmf@inf.uc3m.es Publicado por: Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural Departamento de Inform\u00e1tica. Universidad de Ja\u00e9n Campus Las Lagunillas, EdificioA3. Despacho 127. 23071 Ja\u00e9n secretaria.sepln@ujaen.es Consejo asesor Buenaga Universidad de Alcal\u00e1 (Espa\u00f1a) Sylviane Cardey -Greenfield Centre en lingu et traitement automatique des langues (Francia) Jos\u00e9 Camacho Collados Cardiff University (Reino Unido) Irene Castell\u00f3n Masalles Universidad de Barcelona (Espa\u00f1a) Arantza D\u00edaz de Ilarraza Universidad del Pa\u00eds Vasco (Espa\u00f1a) Antonio Ferr\u00e1ndez Universidad de Alicante (Espa\u00f1a) Koldo Gojenola Universidad del Pa\u00eds Vasco (Espa\u00f1a) Xavier G\u00f3mez Guinovart Universidad de Vigo (Espa\u00f1a) Jos\u00e9 Miguel Go\u00f1i Universidad Polit\u00e9cnica de Madrid (Espa\u00f1a) Ram\u00f3n L\u00f3pez -C\u00f3zar Delgado Universidad de Granada ( Espa\u00f1a) Mariana Lara Neves German Federal Institute for Risk Assessment (Alemania) Elena Lloret Universidad de Alicante (Espa\u00f1a) Bernardo Magnini Fondazione Bruno Kessler (Italia) Nuno J. Mamede Instituto de Engenharia de Sistemas e Computadores (Portugal) M. Teresa Mart\u00edn Valdivia Universidad de Ja\u00e9n (Espa\u00f1a) Patricio Mart\u00ednez -Barco Universidad de Alicante (Espa\u00f1a) Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021 \u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalEugenio Mart\u00ednez C\u00e1mara Universidad de Granada (Espa\u00f1a) Paloma Mart\u00ednez Fern\u00e1ndez Universidad Carlos III (Espa\u00f1a) Raquel Mart\u00ednez Unanue Universidad Nacional de Educaci\u00f3n a Distancia (Espa\u00f1a) Leonel Ruiz Miyares Centro de Ling\u00fc\u00edstica Aplicada de Santiago de Cuba (Cuba) Ruslan Mitkov University of Wolverhampton (Reino Unido) Manuel Montes y G\u00f3mez Instituto Nacional de Astrof \u00edsica, \u00d3ptica y Electr\u00f3nica (M\u00e9xico) Llu\u00eds Padr\u00f3 Universidad Polit\u00e9cnica de Catalu\u00f1a (Espa\u00f1a) Manuel Palomar Universidad de Alicante (Espa\u00f1a) Ferr\u00e1n Pla Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a) German Rigau Universidad del Pa\u00eds Vasco (Espa\u00f1a) Horacio S aggion Universidad Pompeu Fabra (Espa\u00f1a) Paolo Rosso Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a) Emilio Sanch\u00eds Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a) Kepa Sarasola Universidad del Pa\u00eds Vasco (Espa\u00f1a) Encarna Segarra Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a) Thamar Solorio University of Houston (Estados Unidos de Am\u00e9rica) Maite Taboada Simon Fraser University (Canad\u00e1) Mariona Taul\u00e9 Universidad de Barcelona Juan-Manuel Torres -Moreno Laboratoire Informatique d'Avignon / Universit \u00e9 d'Avignon (Francia) Jos\u00e9 Antonio Troyano Jim\u00e9nez Universidad de Sevilla (Espa\u00f1a) L. Alfonso Ure\u00f1a L\u00f3pez Universidad de Ja\u00e9n (Espa\u00f1a) Rafael Valencia Garc\u00eda Universidad de Murcia (Espa\u00f1a) Ren\u00e9 Venegas Vel\u00e1sques Pontificia Universidad Cat\u00f3lica de Valpara\u00eds o (Chile) Felisa Verdejo Ma\u00edllo Universidad Nacional de Educaci\u00f3n a Distancia (Espa\u00f1a) Manuel Vilares Universidad de la Coru\u00f1a (Espa\u00f1a) Luis Villase\u00f1or -Pineda Instituto Nacional de Astrof\u00edsica, \u00d3ptica y Electr\u00f3nica (M\u00e9xico) Revisores adicionales Miguel \u00c1ngel \u00c1lvarez Carmona Consejo Nacional de Ciencia y Tecnolog\u00eda (M\u00e9xico) Mario Ezra Arag\u00f3n Instituto Nacional de Astrof\u00edsica, \u00d3ptica y Electr\u00f3nica (M\u00e9xico) V\u00edctor Manuel Darriba Bilbao Universidad de Vigo (Espa\u00f1a) Agust\u00edn Daniel Delgado Mu\u00f1oz Universidad Nacional de Educaci\u00f3n a Distancia (Espa\u00f1a) Ana Garc\u00eda Serrano Universidad Nacional de Educaci\u00f3n a Distancia (Espa\u00f1a) Salud Mar\u00eda Jim\u00e9nez Zafra Universidad de Ja\u00e9n (Espa\u00f1a) Fernando Mart\u00ednez Santiago Universidad de Ja\u00e9n (Espa\u00f1a) Mar\u00eda Dolores Molina Gonz\u00e1lez Universidad de Ja\u00e9n (Espa\u00f1a) Flor Miriam Plaza del Arco Universidad de Ja\u00e9n (Espa\u00f1a) Anselmo Pe\u00f1as Universidad Nacional de Educaci\u00f3n a Distancia (Espa\u00f1a) Francisco Manuel Rangel Pardo Symanto (Espa\u00f1a) Francisco J. Ribadas -Pena Universidad de Vigo (Espa\u00f1a) Mar\u00eda Auxiliadora Rodr\u00edguez Barrios Universidad Complutense de Madrid (Espa\u00f1a) \u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalISSN: 1135 -5948 Pre\u00e1mbulo La revista Procesamiento del Lenguaje Natural pretende ser un foro de publicaci\u00f3n de art\u00edculos cient\u00edfico -t\u00e9cnicos in\u00e9ditos de calidad relevante en el \u00e1mbito del Procesamiento de Lenguaje Natural (PLN) tanto para la comunidad cient\u00edfica nacional e internacional, como para las empresas del sector. Ade m\u00e1s, se quiere potenciar el desarrollo de las diferentes \u00e1reas relacionadas con el PLN, mejorar la divulgaci\u00f3n de las investigaciones que se llevan a cabo, identificar las futuras directrices de la investigaci\u00f3n b\u00e1sica y mostrar las posibilidades reales de aplicaci\u00f3n en este campo. Anualmente la SEPLN (Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural) publica dos n\u00fameros de la revista, que incluyen art\u00edculos originales, presentaciones de proyectos en marcha, rese\u00f1as bibliogr\u00e1ficas y res\u00famenes de tesis doctorales. Esta revista se distribuye gratuitamente a todos los socios, y con el fin de conseguir una mayor expansi\u00f3n y facilitar el acceso a la publicaci\u00f3n, su contenido es libremente accesible por Internet. Las \u00e1reas tem\u00e1ticas tratadas son las s iguientes: Modelos ling\u00fc\u00edsticos, matem\u00e1ticos y psicoling\u00fc\u00edsticos del lenguaje Ling\u00fc\u00edstica de corpus Desarrollo de recursos y herramientas ling\u00fc\u00edsticas Gram\u00e1ticas y formalismos para el an\u00e1lisis morfol\u00f3gico y sint\u00e1ctico Sem\u00e1ntica, pragm\u00e1tica y discurso Lexicograf\u00eda y terminolog\u00eda computacional Resoluci\u00f3n de la ambig\u00fcedad l\u00e9xica Aprendizaje autom\u00e1tico en PLN Generaci\u00f3n textual monoling\u00fce y multiling\u00fce Traducci\u00f3n autom\u00e1tica Reconocimiento y s\u00edntesis del habla Extracci\u00f3n y recuperaci\u00f3n de informaci\u00f3n monoli ng\u00fce, multiling\u00fce y multimodal Sistemas de b\u00fasqueda de respuestas An\u00e1lisis autom\u00e1tico del contenido textual Resumen autom\u00e1tico PLN para la generaci\u00f3n de recursos educativos PLN para lenguas con recursos limitados Aplicaciones industriales del PLN Sistemas de di\u00e1logo An\u00e1lisis de sentimientos y opiniones Miner\u00eda de texto Evaluaci\u00f3n de sistemas de PLN Implicaci\u00f3n textual y par\u00e1frasis El ejemplar n\u00famero 67 de la revista Procesamiento del Lenguaje Natural contiene trabajos correspondientes a dos apartados diferentes: comunicaciones cient\u00edficas y res\u00famenes de las tareas de evaluaci\u00f3n competitiva de la edici\u00f3n del a\u00f1o 2021 del foro de evaluaci\u00f3n Iberian Language Evaluation Forum (IberLEF). Todos ellos han si do aceptados mediante el proceso de revisi\u00f3n Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021 \u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturaltradicional en la revista. Queremos agradecer a los miembros del Comit\u00e9 Asesor y a los revisores adicionales la labor que han realizado. Se recibieron 37 trabajos para este n\u00famero, de los cuales 25 eran art\u00edcul os cient\u00edficos y 12 res\u00fames de las tareas de evaluaci\u00f3n competitiva del foro de evaluaci\u00f3n IberLEF 2021. De entre los 25 art\u00edculos recibidos, 12 han sido finalmente seleccionados para su publicaci\u00f3n, lo cual fija una tasa de aceptaci\u00f3n del 48%. El Comit\u00e9 Asesor de la revista se ha hecho cargo de la revisi\u00f3n de los trabajos. Este proceso de revisi\u00f3n es de doble anonimato: se mantiene oculta la identidad de los autores que son evaluados y de los revisores que realizan las evaluaciones. En un primer paso, cad a art\u00edculo ha sido examinado de manera ciega o an\u00f3nima por tres revisores. En un segundo paso, para aquellos art\u00edculos que ten\u00edan una divergencia m\u00ednima de tres puntos (sobre siete) en sus puntuaciones, sus tres revisores han reconsiderado su evaluaci\u00f3n en conjunto. Finalmente, la evaluaci\u00f3n de aquellos art\u00edculos que estaban en posici\u00f3n muy cercana a la frontera de aceptaci\u00f3n ha sido supervisada por m\u00e1s miembros del comit\u00e9 editorial. El criterio de corte adoptado ha sido la media de las tres calificaciones, siempre y cuando hayan sido iguales o superiores a 5 sobre 7. Septiembre de 2021 Los editores. \u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalISSN: 1135 -5948 Preamble The Natural Language Processing journal aims to be a forum for the publication of high -quality unpublished scientific and technical papers on Natural Language Processing (NLP) for both the national and international scientific community and companies. Furthermore, we want to strengthen the development of different areas related to NLP, widening the dissemination of research carried out, identifying the future directions of basic research and demonstrating the possibilities of its application in this field. Every year, the Spanish Society for Natural Language Processing (SEPLN) publishes two issues of the journal that include original ongoing projects, book reviews and published all freel The addressed are the following: Linguistic, Mathematical and Psychological to and Synthesis Dialogue Systems Machine Translation Machine Learning in NLP Monolingual and multilingual Information Extraction Question Answering Automatic Text Analysis Automatic Summarization NLP Resources for Learning NLP for languages with limited resources Business Applications of Analysis Mining Paraphrases The 65 th issue of the Procesamiento del Lenguaje Natural journal contains scientific papers and summarie of the shared -tasks of the edition of 2021 the evaluation forum Iberian Languages Evaluation Forum (IberLEF) . All of were accepted by a peer review process. We would like to thank the Advisory Committee members and additional reviewers for their work. Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021 \u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalThirty -seven papers were submitted for this issue, from which tw enty-five were scientific papers and twelve were summaries of evaluation tasks of the evaluation forum . From twenty -five selected twelve (48%) for publication. The Advisory Committee of the journal has reviewed the papers in a double -blind process. Under double -blind review the identity of the reviewers and the authors are hidden from each other. In the first step, each paper was reviewed blindly by three reviewers. In the second step, the three reviewers have given a seco nd overall evaluation of those papers with a difference of three or more points out of seven in their individual reviewer scores. Finally, the evaluation of those papers that were in a position very close to the acceptance limit were supervised by the edit orial board. The cut -off criterion was the mean of the three scores given , as long as it is equal or greater than 5 out of 7 . September 2021 Editorial board. \u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural ISSN: 1135 -5948 Art\u00edculos Sarcasm Detection with ................................ 27 Constructing Corpus and Word Embedding for Data Kyungjin Hwang ................................ ................................ ................................ ................................ .......... 37 Procesamiento de Capitalisation System for Spanish and Basque Ander Gonz\u00e1lez Aitor Montes -y-G\u00f3mez, Berta Chulvi ................................ ...... 83 El sentimiento de las letras de las canciones y su relaci\u00f3n con las caracter\u00edsticas musicales Marco Palomeque, Juan de Lucio ................................ ................................ ................................ ............... 95 Reconocimiento y clasificaci\u00f3n de entidades nombradas en textos legales en espa\u00f1ol Doaa Samy ................................ ................................ ................................ ................................ ................. 103 Un enfoque sem\u00e1ntico en la selecci\u00f3n de caracter\u00edsticas basadas en l\u00e9xico para la detecci\u00f3n de emociones Harold Gonz\u00e1lez -Guerra, Alfredo Sim\u00f3n -Cuevas, Jos\u00e9 M. Perea -Ortega, Jos\u00e9 A. Olivas ...................... 115 Inducci\u00f3n autom\u00e1tica de una taxonom \u00eda multiling\u00fce de marcadores discursivos: primeros resultados en castellano, ................................ ................................ ..... tareas de evaluaci\u00f3n Overview of the EmoEvalEs task on emotion detection for Spanish at IberLEF 2021 Flor Miriam Plaza -del-Arco, Salud Mar\u00eda Jim\u00e9nez -Zafra, Arturo Montejo -R\u00e1ez, M. Dolores Molina - Gonz\u00e1lez, L. Alfonso Ure\u00f1a -L\u00f3pez, M. Teresa Mart\u00edn -Valdivia for Text Mexican Tourism Miguel \u00c1. \u00c1lvarez -Carmona , Ram\u00f3n Aranda, Samuel Arce -Cardenas, Daniel Fajardo -Delgado, Rafael Guerrero -Rodr\u00edguez, A. Pastor L\u00f3pez -Monroy, Juan Mart\u00ednez Overview of Going Beyond T ext in Cross -Lingual Stance Detection Rodrigo Agerri, Roberto Centeno, Mar\u00eda Espinosa, Joseba Fernandez de Landa, \u00c1lvaro Rodrigo ........ 173 Overview of MeOffendEs at IberLEF 2021: Miriam Plaza -del-Arco, Marco Casavantes, Hugo Jair Escalante, M. Teresa Mart\u00edn -Valdivia, Arturo Montejo -S\u00e1nchez, Jorge Carrilo -de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, Miriam Comet, Trinidad Donoso ................................ ................................ Enr ique Amig\u00f3, Paolo Rosso ................................ . 209 Procesamiento Natural, Revista n\u00ba 67, septiembre de 2021 \u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del of FakeDeS at IberLEF 2021: Fake News Detection in Spanish Shared Task Helena G\u00f3mez -Adorno, Juan Pablo Posadas -Dur\u00e1n, Gemma Bel Enguix, Claudia Porto Capetillo ...... 223 Overview of the Analyzing Santiago Castro, Santiago G\u00f3ngora, Aiala Ros\u00e1, J. Freit as ................................ ........ 2021: Automatic Detection of Unassimilated Borrowings in the Spanish Press Elena \u00c1lvarez Mellado, Luis Espinosa Anke, Julio Gonzalo Arroyo, Constantine Lignos, Jordi Porta Zamorano ................................ ................................ ................................ ................................ ................... \u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural Art\u00edculos Sarcasm Detection with BERT Detecci\u0013 on de Sarcasmo con BERT Elsa Scola, Isabel Segura-Bedmar Universidad Carlos III de Madrid, Legan\u0013 es, what we really mean. Thus, automatic sarcasm detection in textual data is one of the most challenging tasks in Natural Language Processing (NLP). It has also become a relevant research area due to its importance in the improvement of sentiment analysis. In this work, we explore several deep learning models such and Bidirectional Encoder (BERT) to address the task of sarcasm detection. While most research has been conducted using social media data, we eva- luate our models using a news headlines dataset. To the best of our knowledge, this is the rst study that applies BERT to detect sarcasm in texts that do not come from social media. Experiment results show that the BERT-based approach overco- mes Sarcasm Detection, Deep Learning, BiLSTM, BERT. Resumen: El sarcasmo se usa con frecuencia para realizar cr\u0013 \u0010tica o burla indirec- ta, a veces hiriendo los sentimientos de alguien. Algunas veces, las personas tienen di cultades para reconocer los comentarios sarc\u0013 asticos, ya que decimos lo contra- rio de lo que realmente queremos decir. Por lo tanto, la detecci\u0013 on autom\u0013 atica de sarcasmo en textos es una de las tareas m\u0013 as complicadas en el Procesamiento del Lenguaje Natural (PLN). Adem\u0013 as, se ha convertido en un \u0013 area de investigaci\u0013 on relevante debido a su importancia para mejorar el an\u0013 alisis de sentimientos. En es- te trabajo, exploramos varios modelos de aprendizaje profundo, como Bidirectional Long Short-Term Memory (BiLSTM) y Bidirectional Encoder Representations from Transformers (BERT) para abordar la tarea de detecci\u0013 on de sarcasmo. Si bien la mayor\u0013 \u0010a de los trabajos anteriores se han centrado en datasets construidos con textos de redes sociales, en este art\u0013 \u0010culo, evaluamos nuestros modelos utilizando un dataset formado por titulares de noticias. Por tanto, este es el primer estudio que aplica BERT para detectar el sarcasmo en textos que no provienen de las redes sociales. Los resultados de los experimentos muestran que el enfoque basado en BERT supera el estado del arte en este tipo de conjunto de datos. Palabras clave: Sarcasm Detection, Deep Learning, BiLSTM, BERT. 1 Introduction The Cambridge Dictionary de nes sarcasm as\\the use of remarks that mean the opposi- te of what they say, made to hurt someone's feelings or to criticize something in a humo- rous way\" . However, understanding is a task hard for humans, as it is highly dependent on the context and sense of humor of each person(Capelli, Na- kagawa, and Madden, 1990). The and Americans perceive sarcasm in di erent ways (Joshi et al., 2016). In the following senten- ce taken from the study presented by Joshi et al. (2016): \\Love going to work and being sent home after two hours\" , Indian annotators do not with Americans. Indian annotators labeled the instance as non-sarcastic as they Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 13-25 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturaldid not have any context about long commu- ting to work and that `being sent home' could accompanied suggests detecting sarcasm is a very challenging task for humans, and is even harder for algo- rithms. Automatic sarcasm detection is one of the most challenging tasks in Natural Language Processing (NLP) (Eke et al., 2020) and can be used in a variety of applications, ranging from knowing the customers opinions about products or services o ered by a company or even identifying inappropriate or harming comments in social media at sarcasm their models (Pt\u0013 a\u0014 cek, Haber- nal, and Hong, 2014). However, these data- sets are noisy and add di\u000eculty to task do not oc- cur in standard texts. Moreover, the lack of context could also be to However, signi cantly less e ort has been put into ex- ploiting other types of texts to train and eva- luate models for sarcasm detection. Long Short Term from formers (BERT) (Devlin ad- dress the task of sarcasm detection from texts. BERT is a model Zheng and Yang, 2019; Hakala and Pyysalo, 2019). To the best of our knowled-ge, this is the rst study that applies BERT for sarcasm detection in texts that are not extracted from social media. This paper is organised as follows: 3 describes the dataset and methods used in this work. In Section 4, we show the evaluation of the Sarcasm is a form of expression in which peo- ple convey the opposite of what say to or harder for ma- chines to detect. Therefore, this is one of the most challenging tasks in NLP nowadays. The task of automatic sarcasm detection has been most commonly de ned, in past work, as a classi cation task. That is, given a pie- ce of text, the goal is to predict whether it is sarcastic or not. In this section, we review the main datasets as well as the most recent approaches to address this task. Datasets it considers tweets without pre- de ned tags as non-sarcastic, however, some of them could express sarcasm. One of earliest Twitter dataset for sar- casm et (2013). This dataset contains a total of 3,000 tweets, of which 2,307 are non-sarcastic and 693 sarcastic. Due to Twitter's data policy, only the tweet ids are share, so their tweets can be directly down- loaded from Twitter by using those ids. Ho- wever, many of the original tweets have been removed since 2013, and therefore, the data- set is a bit outdated. Another Twitter Elsa was part of the SemEval-2018 competition (Apidianaki et al., 2018). Although it is orien- ted to the task of irony detection, it can in order to An innovative contribution was made by Oprea and Magdy (2020). This proposal shows an original way of collecting sarcas- tic tweets. They have designed an online sur- vey where they ask Twitter users to provide links to one sarcastic and three and 3,707 as non- sarcastic. datasets from Reddit, a dis- cussion the collected by Khodak, Saunshi, and Vodrahalli (2018), It was generated by scraping comments that contained the s tag. This tag is often used by Redditors to indicate that should not be taken seriously. forums. each post, 3,260 posts per class (sarcastic and is, of 6,520 posts. hyperbole 582 posts questions 851 posts per class. Several options can be the tilingual panorama for sarcasm detection. for sarcasm detection on Twit- ter in English and Czech. While dataset comments) in Spanish. The corpus 9,000 short about di e- rent topics written in Spanish {3,000 from Cuba, 3,000 from Mexico and 3,000 from Spain- and annotated with irony. Approxi- mately, 80 % of corpus corresponds to the training dataset, whereas the remaining 20 % corresponds to the test set. As can be seen from the above, most data- sets for sarcams detection are collected from social media. An alternative dataset was pre- sented by Misra and Arora (2019). They pro- posed a dataset based on new headlines to overcome the limitations of Twitter and other social media datasets. Sarcastic headli- nes were collected from TheOnion,1which is a news website whose sole purpose is to pro- duce sarcastic content. Non-sarcastic headli- nes were extracted from the Hu Post,2which is a real news website. This dataset is descri- bed in detail in Subsection 3.1. 2.2 Approaches for sarcasm detection We now review the main approaches that addressed this machine learning algorithms. Lie- brecht, Kunneman, and van den Bosch (2013) proposed a model for text classi cation, which is based on Balanced Winnow highest-ranking features for one class. To train the classi er, the authors used a co- llection of from by hashtag (the Dutch word for sarcasm) and a random sample of tweets without it. They explored the e ect of balanced (50-50) and imbalanced (25 % sarcastic and 75 % non- sarcastic) data. The classi er provided 75 % TPR (True Positive Rate or recall) and 16 % the an imbalanced dataset had a positive e ect on FPR (5 %), TPR dropped markedly to 56 %. Error analysis showed that sarcasm is often indicated by the usage of intensi ers and ex- clamations. When these are not present in the tweet, there often lent of nonverbal expressions that people use in real life to express sarcasm. One of the li- mitations of this study is that the tweets were automatically annotated without further ma- nual review. The same year, Rilo et al. (2013) pro- posed an alternative approach to de ne and identify sarcasm in text. They state that sar- casm is often de ned in terms of contrast or \\saying the opposite of what you mean\". As they stated it in their study, \\[It] is common on Twitter: expression (e.g., in ce to a negative activity or state (e.g., \\ta- king an exam\" or identify ty- pe contrast in the text by recognizing po- sitive sentiments with negative situations in sentences. for phrasesin their tweets. This system achieved an F1 of 22 %. Additionally, they tested a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) classi er providing an F1 of 48 %. Fi- nally, they combined both approaches in an attempt results. The hybrid approach obtained with a slight drop in precision, which resulted in an F1 of 51 %. In 2014, Pt\u0013 a\u0014 cek, Habernal, and (Nigam, mo- to a multilingual study by using an English da- taset, as well as a Czech These clas- er achieved an F1 of 94.7 % % 91.4 % on the balanced data and 88.6 % on the imbalanced data. Experiments showed lo- wer results for the Czech dataset. This may be due to the Czech dataset being much sma- ller than the English dataset, as well as to inner grammatical complexity of the Czech language. MaxEnt obtained an F1 of 57 %, while SVM gave the best F1 (58.2 %) on the Czech dataset. Bamman and Smith (2015) approached the problem from an original perspective by attempting to introduce one the formation context of of the tweets). As a clas- Scola, inclusion of author features. 2.2.2 Deep Learning Approaches We now present some of the latest work on sarcasm detection research, in which mostly deep learning techniques et al. relations between users and their content. One of the main advantages of this work is that it avoids ture of networks were investigated, sho- F1 of 70.56 % for the Discussion Fo- rum data and an F1 of 73.45 % on the Twitter dataset. The most recent studies transformers to provi- de for This model is its outstanding performance for multiple NLP tasks. Xu and Xu (2019) presented an investi-gation of erent models to BERT were The implemented LSTM models were all of them right) and used pre-trained Glo- Ve(Pennington, Socher, and Manning, 2014) as a word embedding model. Two datasets are used for this study, Discussion Forum da- ta described above. The re- sults of this study show that BERT achieved better results than all the LSTM models for both datasets. In the case of the LSTM mo- del, the performance varies depending on the dataset. For example, it obtains 73.23 % the Discussion Forum dataset, but 67.32 % for Reddit data. This is probably due to the lack of \\quality.of the Reddit data- set as it relies perform Red- is full of typos and slang terms, it hard for P ers in combination as they meaning the vector representa- best both embeddings, % when BERT embed- dings are used, and an F1 of 69 % Glove embeddings. Misra and Arora exclusive usa- ge Twitter datasets the casm detection. This system pre- Sarcasm Detection with BERT 17professionals in a formal there no spelling mistakes slang terms. (there are not reply posts), it is easier to spot the sarcastic elements of the sentence. This hybrid architecture obtained an accuracy of 89.7 %. 3 Approaches This section describes the dataset used in this work and the two deep learning approaches proposed to deal with the task of sarcasm detection. 3.1 Sarcasm detection studies often make #sar- turn out to be noisy due to the informal use of lan- guage in social media. Social media texts as emoticons, hyperlinks and information. In order to avoid these drawbacks of tweets, in this study, the Da- taset For Sarcasm Detection Post. produ- are written models (Mikolov et al., 2013; Pennington, and 2014; part of threads, which would translate in a lack of context. Since the sole purpose of The Onion is to publish sarcastic news, it could pro- vide a higher guarantee of the correct- ness of the data in comparison to some Twitter datasets based in keyword hash- tags. In studies like (Rilo et al., 2013), it is assumed that human labeling from Twitter users an- notation of a sample % of those tweets were indeed sarcas- tic. Therefore, pers Onion provided in JSON format. Each headline is represented by its text, the link to the original news article, and an value of 0 (if it is a non-sacarstic headline) or 1 (if it is a sarcastic are sarcastic. Finally, the dataset was split into 70 % for training (with 9,498 sarcastic head- lines and 10,454 non-sarcastic ones), (with 1,342 sarcastic headlines and 1,508 non-sarcastic ones) and (with 2,712 sarcastic headlines and 2,989 non-sarcastic ones). As can be seen, the three datasets are balanced, that is, same. These datasets were used to train the models and evaluate their performance. 3.2 Methods 3.2.1 Long Short-Term Memory successfully used for text classi - cation (Zhou et al., 2016; Wang et al., 2018). LSTM is a unidirectional model that proces- ses the inputs from left to right, but not from right to left. Hence, during the training, it can only preserve relevant information from the Elsa Scola, Isabel Segura-Bedmar 18left part of the input, but it does not know about what is the information on the right part. However, sometimes, to correctly un- derstand a text, we need to take into account not just the previous words, but also the co- ming words. For example, \\Sometimes I need what only you can provide: your absence.\", and \\Sometimes I need what only you can provide: your love.\" , are sentences that is model connects two hidden layers of opposite directions to the same output. In this way, the output layer can get infor- mation from the past (forward) and future (backward) states simultaneously. Therefore, BiLSTM can capture past (left) and In our experi- mentation, we apply a BiLSTM layer with 128 units each direction. number of units was The network is initialized with word em- beddings. To do this, the headlines are toke- nized and each token is represented and Manning, 2014), developed by Google . In particular, we use glove.6B.200d, which was trained with the Wikipedia 2014 + Giga- word 5 corpora and contains 6B tokens. The dimension of word vectors is 200. In deep learning models, it is important not to take the last result of each cell, but rather the best result of it. For this reason, after the BiLSTM layer, a global maxpooling layer downsamples the entire feature map to a single value. This is done each This us to identify the strongest trait of a headline and highlight the tokens with the most relevant information. For example, it could identify a word that is particularly funny in the headline, which would be helpful for sarcasm detection. After the global maxpooling layer, we add two fully connected layers, the rst one with 40 units and a dropout probability of 0.5, and the second one with 20 units and a dro- pout probability of 0.5. The addition of fully connected layers in deep learning models hasshown to improve the performance of the text classi it is sparsely activated (it all negative inputs, and reby, units often do not activate are actually sing meaningful aspects of the problem. For the output layer, one single unit with a sigmoid function has been used that allows us to probability of sarcasm. whereas if p <0;5 then it would the negative class (non-sarcastic). 3.2.2 Bidirectional Encoder Representations the-art results in a wide range al., 2020; Zheng and Yang, 2019; Hakala and Pyysalo, 2019), has hardly for sarcasm detection (Khatri and P, 2020). Thus, one of the main contributions of our study is the use of BERT (Devlin et al., 2019) to address the task of sarcasm de- tection from news learn the contextual relationships between the words in a text. The purpose of BERT is to generate a language representation model. Therefore, an encoder is needed in which an For the implementation, the o\u000ecial script by BERT was used, is progressively being upda- ted with the latest improvements. After the encoding process, the tokens, masks are ob- tained. Each of these will correspond to an input layer of the network. There are di e- rent versions of the BERT model (Devlin et al., 2019): BERT-Base and BERT-Large. The last one is an improved and computationally more intensive version of the rst model. This model has the following parameteres: L=24, H=1024, A=16, where L is stacked encoders, H is the hidden size and A is the number of heads in the MultiHead At- tention layers. Therefore, we use the BERT- Large model (bert enuncased L24 H1024 A- 16), which was pre-trained for English on Wi- kipedia and Books Corpus. Inputs are \\un- Sarcasm Detection with BERT 19cased\", which means process, random input king is applied independently to word pieces, as described in (Devlin et al., 2019). The to- as well as the encoding process, correspond to the inputs of the BERT layer. The output of the BERT layer is then pro- cessed by the tfoplayer strided slice layer, which performs the extraction of a straded slice of a tensor. Then, a Sigmoid layer with one single unit receives the output of this la- yer and obtains a probability of an instance being sarcasm. It often occurs in the eld lear- ning, that an algorithm performs incredibly well on the training dataset, but poorly on the test set. This common phenomenon is ca- lled over tting. That is, the model has a high variance, which makes it di\u000ecult to genera- lize (Srivastava 2014) is the standard A value between 0 and 1 is speci ed, which is the fraction of the input units to drop. It has been shown, that a dropout rate of 0.5 is e ective in most scenarios (Kim, 2014). Therefore, there is a probability of 50 % that a node will be remo- ved from the network. This, ultimately, re- sults in a much simpler network that helps to prevent over tting. Early stopping was al- so used to prevent the model from over tting. Early stopping is a method in which an ar- bitrarily large number of training epochs are speci ed, and the training process is stopped once the performance stops tience of 3 was used, which means that the network is allowed to continue training for up to an additional 3 epochs, after the point that validation loss stopped improving. This allows us to or nd some additional improvement during the training process. Then, the last best model is the one that is stored for posterior predictions. We used the well-known API written in Python, Keras (2.3.1), for building and trai-ning deep learning models. Keras runs on top of the machine learning platform TensorFlow. We also use a TensorFlow 2 (TF2) which is the recommended way share pre-trained models and model pieces on Ten- sorFlow Hub. These models can be integrated with Keras by making use of TensorFlow's high-level API. The chosen optimizer is Adam, which is an adaptive learning rate optimizer introdu- ced by Kingma erent pa- rameters from estimates of rst and second moments of the gradients\". For training the Bi-LSTM model, we used the default para- meters in Keras for Adam.However, for our BERT model, we use the default parameters, except for the learning rate, whose value was modi ed to 2e-6. for both models is binary cross standard number of epochs is 25 (early stopping at the 5th) and the batch size is 100. For the BERT model, The number of epochs is 10 (early stopping at the 5th) and the batch size is 20. As the environment to train and test the models, Google Colab was used with GPU activated. Google Colab is a Google Research product that enables running Python code on the browser for free with computational resources, such as GPU. The dataset as well as the code to replicate the experiments can be found in the GitHub repository.3 4 Results and Discussion Table 1 shows the displaying di of an SVM classi er and a CNN model(Kim, 2014). They have been considered as the baseline systems, as they have 2014; Amir et al., 2016). As in the BiLSTM model, the CNN mo- del was also initialized with word Manning, 2014) and used the adam opti- mizer for the training. The convolutional la- yer has 128 lters of size 5. After this layer, a maxpooling layer is added to select the most important features. All the deep learning si- milar results, being BiLSTM slightly better. It can be seen that BERT performed in gene- ral better than the BiLSTM ticular, provides over More speci - cally, BERT has atten- tion mechanism of BERT surpasses it in this task. To the best of our knowledge, this is the rst study that applies BERT to detect sarcasm in texts that are not media compare results and 2019) sin- ce both studies use the same dataset. Our BERT model shows an improvement of \u0018 1;7 % in the results, compared to the hybrid network architecture proposed by (Misra and Arora, 2019). It also described in Section 2, although our results are not comparable to what have been repor- ted in systems which focused on social me- dia texts. BERT was also used in (Xu and Xu, 2019; Khatri and P, 2020), but their re- sults were much worse than those obtained by our BERT model. Like BERT, our CNN and BiLSTM models also have signi cant better performance deep trained and and P, 2020). This agrees with the fact of social media texts are cha- racterized by a lack of context, which leads to ambiguity and makes the task de- tecting sarcasm even more di\u000ecult. We have sample of false positives and negatives produced by the mo- dels to identify their major weak points in which these models fail. Table 2 shows so- me headlines that were wrongly classi ed as sarcastic by both models. Having a look in-to these false positives, it can be hypothesi- zed that models seem could the absurdity of the sentence, which might result in the models conside- ring it as a joke. The same situation happens for sentences like \\Farting teen sparks ght.\", which can be interpreted as jokes. This is due to the lack of knowledge of the model on the context. Paying attention to another senten- ce, which \\Passport robot tells man of Asian descent his eyes are too closed.\", it can be seen that the models might learn to see the humor in si- tuations necessarily collectives, as community in this case. This is an issue that goes beyond the scope of this study, ho- wever, it is interesting to observe and analyze this kind of phenomenon. The data that is gi- ven to a model to learn humor can eventually lead the model to reproduce the same racist stereotypes that we see in society. Thus, spe- cial care should be put in curating the trai- ning data and reach a consensus of what type of humor is funny and which is harmful or of- fensive. In the current study, to various neighbour moving a sarcastic one, BERT was able to recog- nise that there was humor in the sentence but was not meant to be sarcastic. That dif- ferentiation is key in order to obtain more accurate results in the task. On the other hand, the headline \\Jailed for being too poor\" was correctly non-sarcastic by BiLSTM, but wrongly as sarcastic by BERT. We now review some of the False Nega- tives. For example, for the headline \\Angeli- na Jolie coming for your baby\" , both models agree that this is not sarcastic, even if it is indeed sarcastic. This is probably the lack of context handling in the two models on who Angelina Jolie is and what is known for. The same situation happens with the sarcastic headline , which was wrongly classi Sarcasm Detection brought deadly terrorist attacks upon itself. man of false positives for BiLSTM and BERT models. models. It is needed to know that Tim Co- ok is the CEO of Apple to understand the sentence. It could be helpful in sentences and getting some context from them, in order to help the models make more accurate predictions. While BiLSTM ed it as non-sarcastic. Again, this may be due to the lack of context in the model. There are some headlines particularly hard to identify as they could be real facts, independently of the context the model is gi- ven. example, the headline \\Visit to Goo- gle Earth reveals house is on re\" , even if the headline was given to a human that is aware of what Google Earth is and had the context to understand the headline, the person could think is a real sentence as it is a possibility, and therefore, the context by itself, would not play a big role for this type of sentences. If the models were able to know the \\ab- solute truth\" around the as providing context to the model is a challenging task for which some approaches were proposed in so- cial network data (see Section 2). Neverthe- less, this is a challenging task, which we plan to address in future work. 5 Conclusions To the best of our knowledge, this is the rst study that applies BERT to detect sarcasm in texts that are not social media texts. Our experiments show that BERT using the news headlines da- taset. As future work, we plan to extend eva- with other sarcasm datasets to our di erent ty- pes of texts. We also plan to study how to encode knowledge of the world in our deep learning models, which will help us to obtain a correct interpretation of any text. evolving eld of virtual assistants. Intonation in the speech of the user could be indicative of sarcasm. This type of research is still in the early stages, however, a few datasets have been presented in this direction (Castro et al., 2019). Acknowledgments This work has been supported by the Madrid Government (Comunidad de Madrid) un- der the Multiannual Agreement with UC3M in the line of \\Fostering Young Doctors Research\"(NLP4RARE-CM-UC3M), as well as in the line of \\Excellence of University Professors\"(EPUC3M17), and in the context of the V PRICIT of Research and Technological Innovation). Bibliograf\u0013 \u0010a Amir, S., B. C. Wallace, H. Lyu, P. Carvalho, and M. J. Silva. 2016. Modelling con- Elsa Scola, Isabel Segura-Bedmar 22text with user sarcasm de- tection in social media. In Proceedings of The 20th SIGNLL Conference on Compu- tational Natural Language Learning, pages 167{177, Berlin, Germany. Apidianaki, M., S. M. Mohammad, J. May, E. Shutova, S. Bethard, and M. Carpuat, editors. 2018. 12th Workshop on Semantic Eva- luation, New Orleans, Louisiana. Bamman, D. and N. Smith. 2015. Con- textualized sarcasm detection on twitter. InProceedins of the 9TH International AAAI Conference On Web And Social Media , Oxford, UK. Cai, Y., H. Cai, and X. Wan. 2019. Multi- modal sarcasm detection in twitter with hierarchical fusion model. In Proceedings of the 57th A., N. Nakagawa, and C. M. Mad- den. 1990. How children understand sar- casm: The role of context and Castro, S., D. Hazarika, V. P\u0013 erez-Rosas, R. Zimmermann, R. Mihalcea, and ria. Towards multimodal sarcasm detection Obviously perfect paper). InProceedings of Association for Computational Lin- guistics, pages 4619{4629, Florence, Italy. Cortes, C. networks. Machine learning , 20(3):273{297. tanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computatio- nal Linguistics: Human Language Tech- nologies, NAACL-HLT 2019, pages 4171{ 4186, Minneapolis, USA. Eke, C. I., A. A. Norman, L. Shuib, and H. F. Nweke. 2020. Sarcasm identi cation , at tweet level Evaluation Forum co-located with 35th Conference of the Spanish Society for Natural Language Processing, IberLEF- SEPLN 2019 , volume 2421, pages 191{ 196, Bilbao, Garain, A. and S. Mahata. 2019. Sen- timent SEPLN (TASS)-2019: the Iberian Languages Evaluation co- located with 35th Conference of Spa- nish Society for Natural Language Pro- cessing, IberLEF@SEPLN 2019, volume 2421, pages 611{617, Bilbao, on Computatio- nal Approaches to Subjectivity, Sentiment and Social Media Analysis , pages 161{169, San Diego, California. Ghosh, D., A. Richard Fabbri, and S. Mure- san. 2017. The role of with multi- In Workshop on BioNLP Open Shared Tasks , pages 56{61, Hong Kong, China. Hernandez Farias, D., V. Patti, and P. Rosso. 2016. Irony detection in twitter: The role of on Internet Technology , 16:1{24. Hochreiter, 1997. Long short-term Neural compu- tation, 9:1735{80, 12. Joshi, M. Carman, J. Saraswati, and R. Shukla. 2016. How cultural di erences impact the quality of sarcasm annotation?: A case study of Indian annotators and Ameri- can text. In Proceedings of the 10th SIGHUM Workshop on Language Techno- logy for Cultural Heritage, Social Sciences, and Humanities, pages 95{99, Berlin, Ger- many. Sarcasm Detection with International Conference on Learning Representations (ICLR). Khatri, A. and P. P. 2020. tweets with bert and glove embeddings. InProceedings of the Second Workshop on Figurative Language Processing, pages 56{ 60. Khodak, M., N. Saunshi, and K. Vodrahalli. 2018. A large self-annotated corpus for sarcasm. In Proceedings of the 11th Inter- national Conference and Evaluation sentence classi the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746{1751, Doha, Qatar. Association for Computatio- nal Linguistics. Kingma, D. P. and J. Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, Conference Track Proceedings, San Diego, CA, USA. Lee, J., W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang. 2020. Biobert: a pre-trained biomedical Liebrecht, C., F. Kunneman, and A. van den Bosch. 2013. The perfect solution for de- tecting sarcasm in tweets on Computatio- nal Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 29{37, Atlanta, Georgia. Littlestone, N. 1988. Learning quickly when 2(4):285{318. Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. Distribu- ted representations of words and phrases and their compositionality. Advances in neural information processing systems , 2019. cation. In Proceedings of IJCAI-99 Workshop on Machine Learning for Information Filtering, pages 61{67. Oprea, S. and W. Magdy. sarcasm. In Pro- ceedings of Association for Computational Lin- guistics, pages 1279{1289, Online. Oraby, S., V. Harrison, L. Reed, E. Hernan- dez, E. Rilo , and M. Walker. In of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialo- gue, pages 31{41, Los Angeles, USA. Oraby, S., V. Harrison, L. Reed, E. Hernan- dez, E. Rilo , and M. A. Walker. the SIGDIAL 2016 Conference, The 17th Annual Meeting of the Special Interest Group on Discourse and Dialo- gue, 13-15 September 2016, Los Angeles, CA, USA, pages 31{41. The Association for Computer Linguistics. Ortega-Bueno, R., F. Rangel, Hern\u0013 andez Far\u0010as, M. Montes-y G\u0013 omez, and J. E. Me- dina Pagola. 2019. Overview of the task on irony detection in R. Socher, and C. D. Man- ning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1532{1543, Doha, Qatar. Pt\u0013 a\u0014 cek, T., I. Habernal, and J. Hong. 2014. Sarcasm detection on Czech and English Twitter. In Proceedings of COLING 2014, the 25th International Conference on E., A. Qadir, P. Surve, L. De Silva, N. Gilbert, and R. Huang. 2013. Sar- casm as contrast between a positive sen- Elsa Segura-Bedmar 24timent and negative situation. In Pro- ceedings of the 2013 Conference on Em- pirical Methods in Natural Language Pro- cessing (EMNLP) , pages 704{714, Seattle, Washington. Rockwell, P. and E. M. Theriot. 2001. Cultu- re, gender, and gender Sutskever, and R. Salakhutdinov. 2014. Dropout: A simple way to prevent neu- ral networks from over tting. Journal of Machine Learning Research, 15:1929{ 1958, 06. Wang, J.-H., T.-W. Liu, X. Luo, and L. Wang. 2018. An lstm approach to short text sentiment classi cation with (ROCLING 2018) , pages 214{223, Hsinchu, Taiwan. Xu, M. Yang. 2019. A new method of improving bert for text classi cation. InProceedings of International Conferen- ce on Intelligent Science and Big Data En- gineering , pages 442{452, Nanjing, China. Zhou, P., Z. Qi, S. Zheng, J. Xu, H. Bao, and B. Xu. 2016. Text classi cation based on Probabilistic Topics In uencia de la Longitud del Texto en Tareas de Recuperaci\u0013 on de Informaci\u0013 on models to describe texts. In large document collections, these models need to reduce the dimensions of the vectors to make the operations manageable each other from their topic distributions. As in many other AI techniques, the texts used to train the models have an impact on their performance. Particularly, we are inter- ested on the impact that length texts may have to create PTM. We have how it in uences to relate multilingual documents and to derived results recuperaci\u0013 on de informaci\u0013 on ha utilizado tradicionalmente modelos vectoriales para describir los textos. A gran escala, estos modelos necesitan reducir las dimensiones de los vectores para que las operaciones sean manejables sin com- prometer su rendimiento. Los modelos probabil\u0013 \u0010sticos de t\u0013 opicos (MPT) proponen espacios vectoriales m\u0013 as peque~ nos. Las palabras se organizan en t\u0013 opicos y los doc- umentos se relacionan entre s\u0013 \u0010 a partir de sus distribuciones de t\u0013 opicos. Como en muchas otras t\u0013 ecnicas de IA, los textos utilizados para entrenar los modelos in uyen en su rendimiento. En particular, nos interesa el impacto de la longitud de los textos al crear MPT. Hemos estudiado c\u0013 omo in uye al relacionar sem\u0013 anticamente docu- mentos multiling\u007f ues y al capturar el conocimiento derivado de sus relaciones. Los resultados sugieren que los textos m\u0013 as adecuados deben ser de igual o mayor longi- tud que los utilizados para hacer inferencias posteriormente y las relaciones deben basarse en m\u0013 etricas de similitud jer\u0013 (PTM) (Hof- mann, 2001) (Blei, 2003) are statistical methods based on bag-of-words that analyze the words of the original texts to discover the themes that run through them, how those themes are connected to each other, or how they change over time. PTM do not require any prior annotations or labeling of the documents. The topicsemerge, as hidden structures, from the anal- over terms that is biased around those words associated to a single theme. Figure 1 shows some topics that have emerged when creating a topic model with the collection of Wikipedia articles to better understand what Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 27-36 1: Topics discovered topic six most representative words, i.e., those words by linear sub- space that capture most of the information in a collection. LSI has shown to yield high correlation with human improved LSI by introducing the con- cept of topic as a multinomial distribution over the vocabulary of a collection. In pLSI each document is described with a vector of topic proportions, capturing the idea that there is a xed number of common themes ex- hibited in a di erent proportion by the docu- ments in a collection. But it was not a training collection. Latent Dirichlet Al- location (LDA) (Blei, Ng, and Jordan, 2003) solved the inferring problem of pLSI by plac- ing a Dirichlet distribution over the topic pro- portions for the documents and allowing for the discovery of the themes running through the documents. It is considered the simplestgenerative Probabilistic Topic Model. LDA is one when processing texts using NLP techniques domains (Jelodar et 2017). with large cor- pora of texts, which are generally from the same domain for which we want to make in- ferences. Documents can be related mation domains such as health (Nzali et al., 2017), legal (O'Neill et al., 2017), news (He, Li, and Wu, 2017), and hy- brid proposals combining topic 2020). However, the ability of topics to ex- press the inherent knowledge on which the re- lationships between documents are built has not been yet analyzed from the texts used to train the models. As far as we know, there are no studies that evaluate how the text length in uences on created by functions. In this work we have studied the impact that the text length has, since it determines the space where words can for the and how the evaluation was presented discussed in section 4, along with the nal remarks and future work in sec- tion 5. 2 Text Similarity based on Probabilistic Topics works have evaluated documents. In (Syed the topics was measured based on the abstract or the full text of scienti c articles. It concluded that full-text ics in small ability to similar papers, (Badenes-Olmedo, and of scien- ti c papers to describe them (i.e. abstract, method, background, etc). It concluded that the background section allows relating them cosine-similarity, but consider the simplex space created by the Dirichlet distribution to support the compar- probabilistic models, and considerations, and sum-equal-one (Mao et to that the document representation is those metrics do not scale well as size of Simplex metrics do er a semantic explanation for the the bigger components of the topic proportion vector) can still have high similarity due to the sum of distances between the less topics (i.e. extended to support semantic restrictions to enrich queries in the corpus. To issues, a a sets Jaccard index, a metric that compares how similar two sets are by how many objects they share. In our experiments, a linear distribution of weights (i.ewi= 3\u0000i) has been how text length in uences the probabilistic topics that are created from a document corpus, and how this in uences the calculations and both multilingual ually tagged with categories, we train topic (a) (b) After text processing. Figure 2: bag-of-words to documents. We corpus several by grouping doc- uments to make inferences across all datasets. In this way we evaluate the performance of topic models to relate simi- lar documents when the length the multilingual corpora was created English and the Spanish editions of the JRC- Acquis (Steinberger et al., 2006) and DGT- Acquis al., 2014) of the Eu- (EU) the 1950s to 2011. The length of the texts or could have been used, but we want to avoid the noise they might introduce in their inferences. The me- dian length of the texts, since Acquis is a parallel corpus, is 152 terms for English texts and 150 terms for Spanish texts with a high variance from less than 7 terms the short- 1.300 terms in texts 1). The of according of to- kens is Standards tary information of the EU institutions ( ISO thesaurus as politics, inter- national relations, law, economics, etc. In our study we used the 452 root concepts iden- independence between probabilistic topics when creating the mod- is a restric- tion imposed by topic models as they are by density functions. 3.2 Text Pre-Processing and Texts were pre-processed to topic Rare Words were lemmatized and transformed to A and an upper limit on the number of words were de- ned the bounds are based interquartile priors = 0:1 and = 0:01 were set follow- ing the conclusions from (Hu et al., 2014). Models with 50, 100, 300 and 500 topics were considered to analyze their ability to cap- knowledge to accurately re- late similar documents. 3.3 Experimental task in inherent knowledge of corpus and allow documents to be related to each other from representations is evaluated by comparing the relationships obtained by this obtained from the manual labels they share. Each document in original corpus is manually annotated with EuroVoc cate- gories. The reduced 452 independently identi ed ar- that the are considered a creating topic models for of the same texts of simi- lar length. We have considered 3, 6 and 9 divisions of the original corpus in order to have enough detail when analyzing the re- sults. The higher the number of divisions, the greater the detail but the lower the num- ber of documents in each subset and this may a ect the quality of the trained topic model. With these three scenarios we have an ade- quate balance between detail them. inter-quartile index (\u00b11.5) was taken account to discard too short or long texts ( see Table 2). Data was divided into a sample subset (5%) for testing and the rest (95%) was used to train a topic model. The test set was de- scribed to compare and obtain the most similar ones. The top10 most simi- lar documents are evaluated in av- erage results of a query are by taking the mean of all average precisions for the rst 10 results when comparing a list tables A.1 to A.6). had the worst per- formance in the unsupervised split, but, due to groups not having the same number of documents, the biggest document group been tomatically documents from their texts. As shown in Table 3, the use of probabilistic top- ics to automatically relate documents o ers a performance with an accuracy above 0.8. This performance is slightly higher than for Spanish texts. We pect this is due to the di erence in qual- ity of the text processing tools for each lan- guage (i.e. lemmatized, is high. This could be due to the fact that topics have are de- all have the weight when measuring distance. The sum them (MAP@10) Training Set 1 2 3 es into from topic models o er a to that o ered by cate- gories manually assigned from the the Acquis legal corpus to relate texts. In the case of large and heterogeneous collections, i.e. with a high number of topics allows auto- matically what is in collection of documents, and the knowl- edge ered by allows understanding why documents are re- lated in a similar way as it would be done with manually assigned labels. 4.2 Text Length length of the texts used for training a ects we evaluated three dif- scenarios where the corpus is divided into subsets with similar text sizes. In the rst scenario we have created three equal and shows the mean average precision when using a training set (columns) and a test set (rows) from among the 3 subsets into which the initial corpus was divided. The same experiment has been re- peated in an analogous way for the scenarios with 6 (Table 5) and 9 (Table 6) subsets. Our aim is to analyze if there is any behavior that is common in all of them. Models created from texts, i.e. train- ing set, with greater or equal length to the texts used in the inferences, i.e. set, Carlos Badenes-Olmedo, Borja Lozano-\u00c1lvarez, Oscar Corcho 32Acquis-6 (MAP@10) Training Set 1 2 3 4 5 6 es en es en es en es en es en corpus divided into six subsets. o ered better performance in document re- trieval tasks regardless of the language used. This behavior appears in the tables 4, 5 and 6 in the cells whose column is greater than or equal to its row. This is evidenced by the fact that those models performed better for almost all sets. Although for some evalua- large they were not signi cantly di erent from the models. For small documents both met- rics performed similarly. On cantly outperformed JSD for longer remarkable case is the table 6. The re- sults for the evaluation of the 9thset (group with biggest document) with the 9thmodel (trained with the biggest document set) were 13% better in the English case and 21% better, suggesting that, with enough text data, PTM models produce small variations in topic proportion vectors from WJL to the point of archiving an document representation, which is way more time consuming than the HE metric square- roots. For the same reason, with a small Topics 33Acquis-9 (MAP@10) Training Set 1 2 3 4 5 6 7 8 9 es en es en es en es en es en es en es en es en corpus divided into nine subsets. 5 Conclusions In this paper we have studied the impact that the length of texts has, since they de- termine the space where words can co-occur, to semantically relate documents ics measures. Multi- ple document retrieval were performed on a collection of legal documents, compar- ing the EuroVoc cat- egories. bilistic topic models to facilitate the collections of documents. The their training. Their ability to general- ize such knowledge only seems to in one direction: with texts isequal This allows us to conclude for example, the knowledge extracted from the topics inferred from a collection of tweets (texts of no more than 260 characters), can- to (more we assume that complexity of a text increases as its increases, used to more than proposed during training. If we consider that the complexity of text is directly proportional to its length, proba- bilistic models generalize they acquire during their train- ing to process more complex texts. other words, the knowledge during training. In addition, the larger the corpus and the more topics it contains (i.e. the more diverse the content of its documents), the more ap- based on hierarchical representations of the topics (see Figures 5 and 6). The noise introduced by the less present topics in a text of labels. Under these conditions, PTM can guide the corpus exploration providing There are still challenges and questions that will have to be solved in future work, namely nding the in uence account rhetoric its discourse scienti (.e.g. only create the topic distributions); And even observing their behaviour in di er- ent languages. Acknowledgments This work is C., O. Corcho. Iberlegal co-located with 32nd International Conference on Legal Knowledge and Information Systems orga- nized by the Foundation for Legal Knowl- edge Based Systems (JURIX). Badenes-Olmedo, C., J. parts. In Proceedings of the First Workshop on Enabling Open Seman- tic Science (SemSci), pages 15{22. Blei, D., A. Ng, and M. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research , 3(4-5):993{1022. Deerwester, S., S. T. Dumais, G. W. Fur- nas, T. K. Landauer, and R. Harshman. 1990. Indexing by latent semantic analy- sis. Journal of the American Society for Information Science, 41(6):391{407. Dieng, A. B., F. Ruiz, and D. Blei. 2020. Topic modeling in putational Linguistics, 8:439{453. He, J., L. Li, self-adaptive sliding window based topic model for non-uniform texts. In - Conference Mining, ICDM , volume 2017- Novem, In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 50{57. Hu, Y., K. Zhai, Boyd- In Proceedings of the 52nd Annual for 1: Long Pa- pers), pages 1166{1176. Jelodar, H., Y. Wang, C. Yuan, X. Feng, X. Jiang, Y. Li, and L. Zhao. 2017. Latent dirichlet allocation (lda) and topic model- ing: applications, a survey. Jung, K. H., E. Ruthru , and T. Goldsmith. 2017. T. Opitz. 2017. What Patients Can Cancer. JMIR medical informatics , 5(3):e23. O'Neill, J., C. Robin, L. O'Brien, and P. Buitelaar. 2017. An analysis 2013. Similarity measures based on la- tent dirichlet allocation. In International Conference on Intelligent Text Process- ing and Computational Linguistics, pages 459{470. Springer. 15th Conference the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers , pages 432{436, Valencia, Spain. Association for Computational Lin- guistics. uter, M. Pouliquen, C. Ignat, T. Erjavec, D. Tu \u0018 s, 20+ languages. ence on Language Resources and Evalua- tion - Proceedings p. 2142-2147, May. Science and Advanced Analytics (DSAA), pages 165{174. Carlos Badenes-Olmedo, Borja Lozano-\u00c1lvarez, Oscar Corcho 36Constructing Corpus and Word Embedding for Spanish Covid- 19 Data Construcci \u00f3n de corpus y word e mbedding para datos de Covid -19 en espa\u00f1ol Kyungjin Hwang Korea University, Seoul, Republic of Korea kjhwang0624@korea.ac.kr Abstract as coronavirus, escalated into a global pandemic with severe transmission and mortality rates in 2019. Despite the virus' worldwide impact in 2020, numerous studies on Natural Language Processing in Spanish have neglected corpus construction or word embedding, embedding conducted in the medical field do not display efficacy in production pertaining to coronavirus or infectious diseases. To supplement this potentially detrimental insufficien cy, collects Spanish Language data to build a relevant coronavirus corpus through appropriate preprocessing and then obtains a word embedding. Performance of the corpus and word embedding are then tested through word similarity evaluations, a co similarity evaluation, and a visualization evaluation with the existing Spanish corpus. After comparison, corpus and word embedding suitable for coronavirus will be suggested. Keywords: corpus, word embedding, coronavirus . Resumen La Enfermedad Inf ecciosa por Coronavirus -19 (en adelante Covid -19), que comenz\u00f3 a extenderse globalmente en diciembre de 2019, mostr\u00f3 una alta tasa de infecci\u00f3n y mortalidad, y tuvo un gran impacto en el mundo en 2020. Sin embargo, los estudios existentes de procesamiento del lenguaje natural en espa\u00f1ol no han utilizado la construcci\u00f3n de corpus o la incrustaci\u00f3n de palabras para enfermedad es infecciosas, incluido el coronavirus. La construcci\u00f3n de corpus y la incrustaci\u00f3n de palabras en el campo biom\u00e9dico no han mostrado un rendimiento eficaz en la ayuda para luchar contra las enfermedades infecciosas, por lo tanto, este estudio recopila da tos en espa\u00f1ol relacionados con el coronavirus para proceder despu\u00e9s a construir un corpus de coronavirus en espa\u00f1ol e incrustar palabra s a trav\u00e9s de un preprocesamiento adecuado. Posteriormente, nos gustar\u00eda presentar un corpus e incrustaci\u00f3n de palabras adecuadas para coronavirus mediante la comparaci\u00f3n de la similitud del coseno y la evaluaci\u00f3n de visualizaci\u00f3n con el corpus espa\u00f1ol existente. Palabras C lave: corpus, word embedding, coronavirus . 1 Introduction Natural Language Processing is th e field by which computers understand and utilize human language. Part -of-speech name recognition for semantic analysis, document classifi cation, and machine translation are only some of the applications and components necessary for Natural Language Processing . Successful Natural Language Processing requires that the human language be converted in such a way so that it can be understood by a computer. Word embedding, which is the most representative method to do so, is defined as transforming a word into a vector that can be calculated. Various field s in Natural Language Processing are often established by natural Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 37-44 Sociedad Espa\u00f1ola para Procesamiento del Lenguaje Naturaldivisions of human life, developing specialty fields of study according to need. A representative ex ample of this is Natural Language Processing research in the field of biomedical and medical science. Recently, research in biomedical data, such as biomedical papers, patent or electronic medical records are also increasing. However, according to Cohen et al. (2014), in the study of Natural Language Processing in the biomedical field, there are many terms that are not part of the common lexic on, such as certain more obscure with meanings that differ from those used in or as abbreviations. to and c most Spanish biomedical embeddings do not accurately reflect the worldwide coronavirus pandemic of 2020. The coronavirus, begun in diseases such was previously prevalent. Considering the fact tha t coronavirus is affecting not only biomedical fields but also various social fields around the world, the necessity of constructing a corpus that reflects coronavirus, first attem pts to compile a coronavirus -related Spanish corpus. This Spanish language coronavirus corpus aims to include data from biomedical and various other related fields. Second, word embedding is implemented based on this corpus, and its performance verified through comparison and visualization evaluation existing structure of this paper is as follows: Section 2 describe s previous stu dies relevant to this study, Section 3 details the data and methodology use d in this study, Section 4 describes the experiment and its results, and in Section 5 this author presents the conclusions of this study. 2 Previous Studies 2.1 Word Embedding Word embedding is the process by which text is translated into a representative vector that can be calculated and understood by a computer. This process is widely used in Natural Language Processing in combination with deep learning models by considering n ot only the meaning of a singular word but also the information that can be understood through a phrase. frequency (IDF) parameter words and high weight to more meaningful terms. other hand, the most representative technique of words This is once again divided into FastText extension (Bojanowski et al., 2017) , which differs from word2vec in that it assumes that there ar e various sub -words within a single word. FastText has the advantage of being able t o calculate the similarity of new words by its classification of -words then recognizing sub -words elsewhere. Consequently, with low frequency can as previously classified or recognized su b-words. Kyungjin Hwang Spanish Corpus and Biomedical Data Two corpora used in Spanish Natural Language Processing are CORPES XXI1 and SBWCE2 (Spanish Billion Word Corp us). Both include both Peninsula /European Spanish and the varieties of Spanish spoken in Latin America. Additionally, there are v arious Spanish data sets available in the biomedical field. IBECS3 (The Spanish Bibliographical Index in Health is a dataset built by examining various journals in the health and medical fields. SciELO (Scientifi c Electronic Library Online) 4 is a dataset of various scientific journals published in Latin America, South Africa, and Spain. 2.3 Spanish Biomedical Word Embedding Spanish language word embedding is applied in various fields, and research corpus, to simplify drug description data , but there was a limitation in that they did not build a special separate cor pus for their result ing data set. Villegas et al. (2018) also collected biomedical data through text mining, but Santiso records. Althou gh this word embedding in biomedical fields was attempted using Electronic Health Records (EHRs) of Spanish hospitals, there is a disadvantage in that no intrinsic evaluation was performed. 1 CORPES is a corpus created by Real Academia Espa\u00f1ola, Spain, and contains more than texts and oral manuscripts. It is a corpus that in cludes literary works such as novels, films, scripts , and as well as words from non-literary books and periodicals, blogs, and Internet resources. Peninsular Spanish data accounts for about 30% of the resources and Latin American Spanish data for abo ut 70% (Corpes XXI\". Real Academia Espa\u00f1ola. https://www.rae.es/banco -de-datos/corpes -xxi, December 21, 2020) 2 SBWCE is a corpus created by the University of Chile which contains approximately 1 billion words. It is a Spanish langua ge corpus made from var ious existing Spanish corpora is 4 Sores et in the biomedical terms a re often used in a manner inconsistent with the typical dictionary definition, or with a different connotation, such as in the case of the word 'positive'. Or , for example, when two words are combined to represent a single meaning as th open compound words, for 'brain dead'. Therefore, in for producing an of the words by breaking them down and analyzing them in N-grams a s opposed to as a wh ole. This s tudy not only accomplished the feat of obtaining a Spanish word embedding in the biomedical field, but also overcame the prior limitations of word embedding in the Spanish biomedical field by performing both the intrinsic eva luation and extrinsic evalu ation through the recognition of the entity name, which had not been done before. Following this, Rivera & Martineza (2020) implemented deep learning neural network model in Spani sh medical records. Data was collected from the existing Spanish biomedical corpora and the biomedical Bidirectional Encoder mentioned models word2vec and FastText, in less precise word embedding s. Ever since this advance in technology it has been the objective of this study to overcome the limitations of the existing (NER) using available with the context-based Spanish biomedical word embedding generation process in the future. In the case of Guti\u00e9rrez -Fandi\u00f1o et.al . (2021), the Spanish biomedical corpus and the medical record corpus were embedded using the FastText using Byte Pair Encoding. Corpus data was Constructing Corpus and Word Embedding for Spanish Covid-19 Data 39collected from crawls, books, SciELO, Pubmed, and patent documents. This study i s meaningful in that it has created and embedded a corpus large enough to be used in the future in Natural Language Processing in the biomedical field. However, compared to the existing corpus, it is limited in that it has undergone no intrinsic or extrinsic evaluation. In short, research on word embedding in the field of Spanish biomedical science is indeed progressing, despite the disadvantages of that have shadowed the advances. However, research in the field of epidemiology, including the all too t opical coronavi rus, has not been conducted well, and there is a constraint in that, in many cases , the existing s, an been conducted. 3 Data and Method The objective of this author's study is to construct a corpus and word embedding that effectively reveal s information related to infectious dis eases, especially in regard to the coronavirus. Therefore, data from various fields related to coronavirus and infectious diseases was gathered, pre -processed, and then a corpus was built, and word embedding was performed through FastText. This study chose the FastText model as embedding whose biomedical definitions lexicon . In addition, FastText can implement a s mall number of words as i ts data set . The performance of this word embedding was then tested through a cosine similarity existing and Additionally, the corpus and embeddings will be of related word construction through visualization evaluation. Constructing Corpus and of this study related data was divided into two categories: bio-medical data and social data. The reason this study included both bio -medical data and social data in the corpus was to create a corpus that reflects various issues which included, not only the coronavirus -related biomedical domain, but also various social, economic, and cultural domains as well. Data in the field of biomedical science was extracted from medical journal articles containing keywords addition to this collected data, hea lth science data from the IBEC and SciELO data sets was used as well. Social sector data w as obtained from onlin e major daily newspapers from Spain, Mexico, Chile, Peru, Colombia, and Argentina. These articles contained the keywords \"Coronavirus\", \"Covid-1 9\", was used to collect data fro m websites such as W ikipedia. Following data collection, the text was standardized for processing. Capital letters were changed using the Spanish Word Corpus. In addition, a pre -processin g procedure was performed to remove all English words using the NLTK English corpus so that English was not included in the final results . Thus, the Spanish data related to coronavirus was as follows. Number of Token Academ ia Wikipedia 19,374,235 Table Coronavirus Results and Training FastText is an opensource embedding method. It was develo ped by Facebook as an alternative way to turn words into vectors. Developed after word2vec, it exhibits similar functions to the skip- gram model of word2vec and the mechanism of CBOW. However, w ord2Vec word as that there are words. In FastText, can be represented by a set of N- grams made of letters, and after the learning process o f the artificial neural network is complete , each N-gram of all the words in the dataset al so embedded (Bojanowski et al., 2017). For example, in the \"virus\", an N-gram score function FastText is as follows. (1) In the above equation, z gT is the vector of each word in N-gram and v c is the word vector included in the context. It is by virtue of this functio n that it is possible to calcu late a word's degree of similarity with other words , even in the case of unlearned words. The FastText Model of word embedding is therefore superior to Word2Vec , at least in regard to the latter's inability with unknow n words or to accurately embed words with low frequency within a word set. 3.3 Evaluation The resulting corpus and word embedding of this study were evaluated through w ord similarity and visual evaluations. 3.3.1 Word Test method to evaluate the quality of word embeddings. First, a series of word p airs is c reated . Then an evaluation of the similarity of the word pairs is conducted by human evaluators. After that the correlation between the scores obtained through evaluations of word embedding evaluation select ed. WordSim data was human evaluated by dividing the degree of similarity between words by 0 -10 points, with a total of 322 evaluation sets. In this study, WordSim data written in English was first translate d using Google Translator, and if the meaning of the machine translated data was inaccurate when compared to the original WordSim data, the researcher the word embedding from the biomedical field. ayoSRS in another biomedical (Pakh omov 2010). This data selected consisted of 566-word pairs from a data set in which the similarity of UMLS (Unified Medical Language System) concepts datasets a nd, like WordSim, after initial translation through Google Translator, if the resul ting word was different compared to the original data, it was directly translated and evaluated using embedding. visualization evaluation is another method of evaluating word embeddings, and it is a techniqu e in which words with similar meanings appear in close proximity to pictures so that humans easily understand them. Through this method for quality . Since word embedding is t- SNE evaluated the visualization. 5 t-SNE expresses high dimensional data in a two-dimensional plane by use of an algorithm that preserves the structure of neighbor ing data and distance as much as possibl e. In this way researchers may visually analyze their word embeddings. Constructing Corpus and Word Embedding for Spanish Covid-19 Data 414 Result 4.1 Cosine Similarity est with SBWCE, of this evaluation were conducted mainly using the data Word -sim data and which are shown in <Table 2>. SCC (Spanish Co rona Corpus), the created this study, showed higher cosine similarity than the ng SBWCE case of Word -Sim, which is composed of pairs of common words, the cosine similarity of was 0.4796 . This m higher than that of SBWCE w hich is 0.2674. In the case o f the MayoSRS evaluation, which was composed of medical terms, the cosine similarity o f SCC was 0.2025. Compare this again the cosine similarity of of SCC was 0.4873, this similarity evaluation test sets, SCC, this study, showed better results than the evaluate semantic similarity, and in the case of Spanish, it consists of about 1900 pairs of words. This show that the coronavirus d ata-based corpus conducted in this study demonstra te better performance than the existing Spanish language strong evidence that the corpus and word embedding in this study are greatly efficien t. 4.2 Visualization Test As a result of the word emb edding visualization of the Spanish C orona Corpus in this study (Figure 1 & Figure 2) , many epidemiological terms that do not exist are related to the economi c problem s of the also . Terms related easily displayed . It seems to have \" online\", and \"c olegio\" (elementary school), In shor t, is possible to understand the impact of coronavi rus on people in disparate area s of society, fr om economic sectors to education. From the results of the visualization, i t can easily be seen that the positions bet ween common similar words are visually near each other . Conclusion The coronavirus infection (Covid -19) has greatly impacted the world, and the need to process coronavirus -related data has grown ever more ur gent. Due to the absence of a dedicated Spanish language coronavirus corpus with an implemented word embedding, this study is essential to the future to the field of Natural Language Processing, specificall y in the field o f epidemiology and bio medicine, in that it can be used for future related studies by presenting this study contains various epidemiological terms that are not included in the existing biomedical corpus and word embedding. This dedicated coronavirus corpus has a high cosine similarity between similar words compared to the existing Spanish language corpus making Figure 2: Zoom in overlapping part of Figure 1. Constructing Corpus and Word Embedding for Spanish Covid-19 Data 43effective word embedding possible. From the evidence shown successfully implemented. words on topics rela ted to the fields economics, society, education, and miscellany are clustered, providing researchers with coronavirus specifically suggests in each field. Most importantly, the word embedding created in this study has shown a consistently superior alongside the SCC despite its smaller size and serve s as proof of its efficacy. Howeve r, the data in this study limited to biomedical academic papers and major daily newspapers sourced from Spain, Mexico, Chile, Peru, and Argentina. It is necessary to secure additional data such as various online journals, blogs, SNS, and books in the futur e. Furthermore, there is a need for supplementary experiments for more effective word embedding implementation. First, by running several word model consecutively Computational Cohen, K. B., and D. Demner -Fushman. 2014. Biomedical Natural Language Processing , 11. Publishing Company . Guti\u00e9rrez-Fandi\u00f1o, A., J. Armengol- Estap P. Carrino, De Gibert, A. Agirre, and arXiv preprint arXiv:2102.12843. Mikolov, T., Chen, G. Corrado, and J. Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. Pakhomov, S., B. McInnes, T. Adam, Y. Pedersen, and G. B. . 2010. Semantic similarity and S. G. B. Melton, A. Ruggieri, and C. G. Chute. 2011. Towards a framework for Workshop Proceedings. S ., A. Casillas, A. P\u00e9rez, and M. Oronoz. 2019. Word embeddi ngs for negation detection in health written in Soft Computing , 23:10969- 10975. leaflets -Agirre, M. Krallinger, and J. Armengol- Estap\u00e9, 2019. Medical word embeddings for Spanish: Development and evaluation. Proceedings of the 2nd Clinical Natural Language Processing Workshop , pages 123- 133. Villegas, M., A. Intxaur rondo, A. Gonzalez -Agirre, M. Marimon, and M. Krallinger. 2018. The MeSpEN resource for English- Spanish medical machine Hwang 44Procesamiento de Expresiones Multipalabra en g allego mediante Aprendizaje Profundo Multipalabra es todav\u00b4 a una tarea pen- diente en el Procesamiento del Lenguaje Natural. En este trabajo pretendemos de- terminar experimentalmente la utilidad de los modelos de Aprendizaje Autom\u00b4 atico para el procesamiento de Expresiones Multipalabra en gallego. Para ello usamos CORGA, un corpus con 40 millones de palabras, con el cual entrenamos modelos transformer de Aprendizaje Profundo, y comparamos su rendimiento con el de mo- delos m\u00b4 as tradicionales de campo aleatorio condicional. Palabras clave: Expresiones multipalabra, aprendizaje autom\u00b4 atico, transformers, gallego. Abstract: Treatment of Multiword Expressions is still a pending task in Natural Language Processing. In this work, we want to experimentally determine the useful- ness of Machine Learning models for Multiword Expression processing in Galician. With that aim, we use CORGA, a 40 million word corpus, with which we train Deep Learning-based transformers , comparing their performances \u00b4 unicadenici\u00b4 onuniver- salmente aceptada, se suelen considerar como Expresiones Multipalabra (EMs) las \"combi- naciones de palabras habituales y recurrentes del lenguaje com\u00b4 un\" (Firth, 1957). Hay mu- chos tipos (\"cantar las P\u00b4 etc. LasEMssoncomunesentodoslosidiomas y dominios (Jackendo, 1997). Por ejemplo, Ramisch (2015) informa que el 51,4% de los nombres y el 25,5% de los verbos de la ver- si\u00b4 on inglesa de WordNet son multipalabra. Desde el punto de vista l\u00b4 exico, muchas EMs tienden a comportarse como palabras indivi- duales, y su sem\u00b4 antica no tiene por qu\u00b4 e resul- tar de la simple composici\u00b4 on del signicado de sus palabras constituyentes. Por lo tanto, es aconsejable incorporar el tratamiento de EMs en tareas basadas en Procesamiento del Lenguaje Natural (PLN) (Ramisch, 2015). En la literatura se distinguen dos tareas en el procesamiento de EMs (Constant et al., 2017): detecci\u00b4 on e identicaci\u00b4 on. La detec-ci\u00b4 on se centra en encontrar EMs no vistas anteriormente en corpora textuales, con el n de almacenarlas en alg\u00b4 un tipo de reposito- rio (como un lexic\u00b4 on) para su uso futuro: por ejemplo, detectar como EM el nombre propio \"Oseja de Sajambre\" cuando aparezca en un corpus. Por su parte, la identicaci\u00b4 on consis- te en anotar autom\u00b4 aticamente las EMs pre- sentes en un texto. Ambas tareas est\u00b4 an re- lacionadas. Una lista de EMs obtenidas me- diante detecci\u00b4 on pueden ser utilizadas como un recurso externo por herramientas de iden- ticaci\u00b4 on. Por otra parte, una herramienta de identicaci\u00b4 on con capacidad de generali- zaci\u00b4 on a partir de ejemplos conocidos puede usarse para detectar nuevas EMs. Este trabajo se centra en el uso de herra- mientasdeAprendizajeAutom\u00b4 atico(AA)su- pervisadoparaelprocesamientodeEMsenel idioma gallego. Para ello, tratamos la detec- ci\u00b4 on e identicaci\u00b4 on de EMs como tareas de etiquetaci\u00b4 on de secuencias. Cada palabra in- dividual en el corpus de entrenamiento recibe una etiqueta que indica si forma parte de una EM o no, de modo que el modelo entrenado Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 45-57 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalcon dicho corpus aplica el mismo esquema de anot aci\u00b4 on a cualquier nuevo texto que reciba como entrada, intentando asignar la secuen- cia de etiquetas m\u00b4 as id\u00b4 onea a cada frase de dicho texto. En este sentido, nuestro prop\u00b4 osito es tes- tear la viabilidad de dos modelos de Aprendizaje Profundo (AP): BERT mul- tilingue (mBERT) (Devlin et al., 2019) y XLM-RoBERTa (XLM-R) (Conneau et al., 2020). Se trata de dos arquitecturas trans- former, basadas en la generaci\u00b4 on de modelos de lenguaje para m\u00b4 ultiples idiomas, que des- pu\u00b4 es pueden ser reentrenados para una tarea concreta. Para estimar la posible mejora in- troducida por estos modelos respecto he- rramienta de que permite entrenar modelos de campo aleatorio condicional ( Conditional Random Fields o CRF) (Laerty, McCallum, y Pereira, 2001) para la anotaci\u00b4 on autom\u00b4 ati- ca de EMs en textos. Para entrenar y probar dichos modelos usamos el Corpus de Referencia do Galego Actual(CORGA)(CentroRam\u00b4 onPi neiropa- ra a Investigaci\u00b4 on en Humanidades, 2019a), que cuenta con una versi\u00b4 on con anotaci\u00b4 actica y lematizaci\u00b4 on realizadas au- tom\u00b4 aticamente, en la que se analizan como una sola unidad, en forma de \"palabras con espacios\", dos tipos de EMs: Entidades Nom- bradas (Named Entities o NEs) y locuciones. La estructura del resto de este art\u00b4 culo es la siguiente. La Secci\u00b4 on 2 presenta trabajos anteriores sobre procesamiento de EMs usan- doAA,yenlaSecci\u00b4 on3sedescribenlameto- dolog\u00b4 a, modelos y datos empleados en nues- tros experimentos. En la Secci\u00b4 on 4 se pre- sentan los resultados de los mismos, y en la Secci\u00b4 on 5 se detallan nuestras conclusiones. 2 Trabajos relacionados Centraremos nuestro an\u00b4 alisis principalmente en los trabajos basados en t\u00b4 ecnicas de AA supervisado, m\u00b4 as populares en la identica- ci\u00b4 on de EMs, aunque tambi\u00b4 en veremos como se han usado en la tarea de detecci\u00b4 on. 2.1 Identicaci\u00b4 on En esta tarea es habitual utilizar alg\u00b4 un ti- po de modelo de etiquetaci\u00b4 on de secuencias. Un ejemplo es Blunsom y Baldwin (2006), que asignan tipos l\u00b4 exicos a las palabras de la secuencia de entrada usando un modeloCRF, mientras que Vincze, Nagy T., y Be- rend (2011) entrenan un CRF con un corpus previamente anotado con EMs, junto con le- xicones externos, y Diab y Bhutada (2009) entrenan M\u00b4 aquinas de Vectores de Soporte (SVMs) sobre un corpus anotado con cons- trucciones nombre-verbo, con el objetivo de distinguir aquellas que son idiom\u00b4 aticas. Es frecuente integrar informaci\u00b4 on de dic- cionarios externos de EMs para calcular ca- racter\u00b4 sticas de las entradas usadas en el pro- ceso de entrenamiento. As\u00b4 , Constant y Si- gogne (2011) entrenan un modelo CRF pa- ra la realizaci\u00b4 on conjunta del exi- co y etiquetaci\u00b4 on morfosint\u00b4 actica partiendo de un banco de \u00b4 arboles. Por su parte, Cons- tant, Sigogne, y Watrin (2012) comparan el rendimiento de un etiquetador de secuencias (un CRF) frente a un analizador sint\u00b4 actico con reordenaci\u00b4 on de \u00b4 arboles. Las caracter\u00b4 sti- cas obtenidas a partir de lexicones usadas en este trabajo son renadas por Schneider et al. (2014) para el entrenamiento de un per- ceptr\u00b4 onestructurado, usandoun juegode eti- quetas que permite anotar no contiguas y EMs anidadas. Finalmente, Riedl y Bie- mann (2016) intentan determinar el impacto en el rendimiento de caracter\u00b4 sticas obteni- das a partir de anotaci\u00b4 on manual y mediante anotaci\u00b4 on autom\u00b4 atica, partiendo de las usa- das en los trabajos anteriores. Otra aproximaci\u00b4 on es usar informa- ci\u00b4 on sint\u00b4 actica en proceso (Constant, Sigogne, y Watrin, 2012). Di- cha informaci\u00b4 on puede integrarse como ca- racter\u00b4 sticas de un modelo de etiquetaci\u00b4 on de secuencias (Maldonado et al., 2017), pe- ro es m\u00b4 as habitual identicar an\u00b4 alisis sint\u00b4 actico, entrenando un mode- lo a partir de un banco de \u00b4 arboles anotado con EMs. Se pueden usar diferentes formalis- mos gramaticales: Green et al. (2011) traba- jan con gram\u00b4 aticas de sustituci\u00b4 on de \u00b4 arbo- les, anotando cada EM como un sub\u00b4 arbol de estructura plana, aproximaci\u00b4 on tambi\u00b4 en usada por Green, de Marnee, y Manning (2013) junto con gram\u00b4 aticas independientes del contexto probabil\u00b4 sticas. Otra dependencias, em- pleando arcos espec\u00b4 para denotar los componentes de una EM (Vincze, Zsibrita, y Nagy T., 2013; Simk\u00b4 o, Kov\u00b4 acs, y Vincze, 2017). Tambi\u00b4 en se pueden escoger diferentes representaciones seg\u00b4 un el tipo de EM: Can- dito y Constant Yerai Doval, Elmurod Kuriyozov 46(2016) dependencia pa- ra las expresiones sint\u00b4 acticamente regulares y una estructura plana para las irregulares. La popularidad creciente de las Redes de Neuronas ha llevado a su empleo en la identi- caci\u00b4 on de EMs (Legrand y Collobert, 2016), haciendo uso de representaciones vectoriales continuas ( word embeddings ). M\u00b4 ultiples ar- quitecturas de esta tarea: Klyueva, Doucet, y Straka mientras Tas- limipoor des Convolucionales Short-Term Me- mory(LSTMs). Recientemente, se ha arquitecturas trans- former: Taslimipoor, Bahaadini, y Kochmar (2020) usan BERT como modelo de lengua- je para el aprendizaje conjunto de EMs y an\u00b4 alisis sint\u00b4 actico de dependencias, mientras que Kurfal (2020) emplea BERT y mBERT para la identicaci\u00b4 on de EMs verbales. 2.2 Detecci\u00b4 on Lo m\u00b4 as habitual en esta tarea es utilizar m\u00b4 etodos no supervisados, distinguiendo EMs en base a sus puntuaciones en m\u00b4 etricas de tambi\u00b4 en existen trabajos basados en t\u00b4 ecni- cas supervisadas de AA, usando a menudo las m\u00b4 etricas mencionadas como caracter\u00b4 sti- cas en el aprendizaje con informaci\u00b4 on ling\u00a8 u\u00b4 stica. Por ejemplo, Pecina ca, An\u00b4 alisis Discriminante Lineal y para el descubrimiento de colocaciones. Los SVMs son tambi\u00b4 en usados por Farahmand y Martins (2014) en conjunci\u00b4 con medi- das estad\u00b4 sticas y caracter\u00b4 les de EMs, prejos y sujos. Por su parte, Lapata y Lascarides (2003) comparan el rendimiento de \u00b4 arboles de decisi\u00b4 on y cla- sicadores bayesianos para extraer nombres compuestos,mientrasqueDubremetzyNivre (2014) prueban varios algoritmos de aprendi- zaje para extraer grupos nombre-nombre y nombre-adjetivo, obteniendo los mejores re- sultados con redes bayesianas. Los modelos as\u00b4 entrenados pueden utilizarse para deter- minar cuales de las caracter\u00b4 sticas usadas en el entrenamiento son m\u00b4 as \u00b4 utiles (Ramisch et al., 2008), o para intentar reducir su depen- dencia sobre recursos de entrenamiento, co- mo Rondon, Caseli, y Ramisch (2015), queimplementan un sistema iterativo basado en SVMs que, partiendo de un lexic\u00b4 on y mode- lo iniciales, mina texto de la web, anota sus EMs y usa el nuevo texto anotado para en- trenar una nueva versi\u00b4 on del modelo. 2.3 Nuestra aportaci\u00b4 on Este trabajo usa un enfoque similar al de Kurfal (2020), estudiando la viabilidad de las arquitecturas transformer para el trata- miento de EMs, si bien hay grandes diferen- cias entre ambas aproximaciones. En primer lugar, nos centraremos en un \u00b4 unico idioma, el gallego, minoritario y pobre en recursos para PLN, y tratamos de determinar la me- jor conguraci\u00b4 on de entrenamiento en base a la informaci\u00b4 on l\u00b4 exica contenida en CORGA. arquitecturas mBERT y y usamos un modelo CRFparadeterminarsiofrecenunmejorren- dimiento que aproximaciones m\u00b4 as tradiciona- les. Finalmente, en lugar de centrarnos sola- mente en la tarea de identicaci\u00b4 on, tambi\u00b4 en tratamos de medir el rendimiento de estas ar- quitecturas en el reconocimiento de nuevas EMs para la tarea de detecci\u00b4 on. 3 Enfoque En esta secci\u00b4 on vamos a examinar los recur- sos usados en nuestros experimentos, as\u00b4 co- mo el dise no de los mismos. 3.1 CORGA ElCorpus de Referencia do Galego Ac- tual(Centro Ram\u00b4 on neiro para a Investi- gaci\u00b4 on en Humanidades, 2019a) es una co- lecci\u00b4 on documental, abierta y representativa que recoge textos y transcripciones de habla en gallego, desde 1975 hasta la actualidad, con el objetivo de proporcionar datos para el estudio ling\u00a8 u\u00b4 stico de dicho idioma. Contiene 40.178.271 millones de palabras y 48.184.012 millones de elementos gramaticales (palabras oentidadesl\u00b4 exicasamalgamadasenpalabras, signos de puntuaci\u00b4 on, etc) procedentes de diversas fuentes: peri\u00b4 odicos, revistas, libros, guiones del el do Galego Actual(XIADA) (Centro Ram\u00b4 on Pi neiro pa- Procesamiento de Expresiones Multipalabra en gallego mediante Aprendizaje Profundo 47ra a Investigaci\u00b4 on en Humanidades, 2019b), una herramienta basada en Modelos de Mar- koventrenadasobreunsubconjuntodeCOR- GA con 744.530 elementos gramaticales, con anotaci\u00b4 onautom\u00b4 aticacorregidaporexpertos. Para recoger la rica morfolog\u00b4 a de la lengua gallega, se usa un juego de 453 etiquetas.1 Aunque no se ha medido la exactitud de la versi\u00b4 on actual del de etiquetaci\u00b4 on/lematizaci\u00b4 on de XIADA incluye la identicaci\u00b4 on de dos tipos de EMs: a) NEs, como \"Universida- de ceso de identicaci\u00b4 on de estas \u00b4 ultimas, XIA- DA hace uso de un lexic\u00b4 on extra\u00b4 do de varias fuentes, con un total de 628 locuciones. Dicha tarea de identicaci\u00b4 on es una carac- ter\u00b4 stica opcional del etiquetador, por lo que hemos dispuesto de dos versiones de CORGA con etiquetaci\u00b4 on y lematizaci\u00b4 on autom\u00b4 aticas: una con cada EM etiquetada y lematizada como una unidad, y otra en donde cada en- tidad constituyente de una EM es etiquetada y lematizada por separado. Ello permite re- cuperar la informaci\u00b4 on morfosint\u00b4 actica de los constituyentes de las EMs, combinando la in- formaci\u00b4 on de ambas versiones del corpus. Otra caracter\u00b4 stica importante es la se- paraci\u00b4 on de amalgamas. El idioma gallego presenta un gran n\u00b4 umero de l\u00b4 exicas, verbos con cl\u00b4 esos constituyentes l\u00b4 exicos, lo que hace posible el tratamiento de EMs di- rectamente sobre las formas amalgamadas o sobre las entidades l\u00b4 exicas que las integran. 3.2 Modelos de aprendizaje En nuestros experimentos empleamos mode- los de AP bien conocidos en el estado del 1Verhttp://corpus.cirp.gal/xiada/etiquetario/taboa/ para una descripci\u00b4 on de las mismas.arte en PLN y basados en la arquitectura transformer : mBERT (Devlin et al., 2019) y XLM-R(Conneauetal.,2020).Elcomponen- te principal en estos modelos es el mecanis- mo de atenci\u00b4 on, mediante el cual se pueden realizar c\u00b4 alculos para un elemento de la se- cuencia de entrada ponderando las contribu- ciones del resto de dicha secuencia. Ello viene a sustituir al mecanismo de recurrencia de las redes basadas en celdas LSTMs (Hochreiter y Schmidhuber, 1997) o GRUs (Cho et al., 2014) que s\u00b4 olo consideraban para sus c\u00b4 alculos el estado anterior al procesar en secuencia el elemento actual. Sin embargo, esta indudable ventaja se obtiene a costa de un aumento sig- nicativo de la complejidad computacional. Otro elemento caracter\u00b4 stico de estos mo- delos es su entrenamiento en dos etapas: una primera de preentrenamiento en una tarea no supervisada, normalmente una variaci\u00b4 on del modelado del lenguaje, y una segunda de en- trenamiento (semi-)supervisado en la tarea espec\u00b4 ca que se desea resolver, como el pro- cesamiento de EMs en nuestro caso. Tanto mBERT como XLM-R est\u00b4 an preen- trenados sobre grandes cantidades de texto en varios idiomas: volcados de la Wikipedia en cada idioma en el primer caso y una ver- si\u00b4 on ltrada de Common Crawl particionada por idioma en el segundo. El gallego es uno de los idiomas incluidos, si bien la propor- ci\u00b4 on del mismo en los datos de entrenamien- to de estas arquitecturas es muy peque na en comparaci\u00b4 on con idiomas m\u00b4 as mayoritarios como el ingl\u00b4 es.2Sin embargo, dado el alto coste de reentrenar y optimizar estos mode- los desde el principio, consideramos a priori estos recursos multiling\u00a8 ues como adecuados para nuestros experimentos. La principal di- ferencia entre ambas arquitecturas radica en la escala del corpus usado durante sus respec- tivos preentrenamientos, mucho mayor en el caso de XLM-R, as posee un mayor dos variantes: una la totalidad del texto pasado a min\u00b4 usculas, y otra con el texto original. A nivel de implementaci\u00b4 on, usamos la li- brer\u00b4 a Python Transformers de HuggingFa- ce (Wolf et al., 2020) junto con los modelos preentrenados y distribuidos desde sus cana- les. Dichos modelos poseen un tama no con- siderable: 179 millones de par\u00b4 ametros distri- 2Tal y como se puede ver en la Figura 1 de Con- neau et al. (2020). V\u00edctor Darriba, en de par\u00b4 ametros preentrena- daentextoenmin\u00b4 usculas)yalrededorde270 millones de par\u00b4 ametros distribuidos de forma similar en el caso de XLM-R. Para el entre- namiento, mantenemos los valores por defec- to en la mayor parte de los hiperpar\u00b4 ametros y especicamos un tama no de batchy una longitud m\u00b4 axima de secuencia de 64 elemen- tos, sin realizar ning\u00b4 un proceso de optimiza- ci\u00b4 on de estos valores. El proceso se desarrolla durante una sola iteraci\u00b4 on sobre el conjunto completo de datos de entrenamiento. Para comparar los resultados de las t\u00b4 ecni- cas de AP con aproximaciones m\u00b4 as tradicio- nales, usamos mwetoolkit (Ramisch, 2015). entrenar un mo- delo CRF (Laerty, McCallum, y Pereira, 2001), implementado con CRFSuite (Okaza- ki, 2007). El usuario puede determinar las caracter\u00b4 sticas extra\u00b4 das de las entradas del modelo, que en nuestro caso son las sugeri- das por los autores de mwetoolkit (unigra- mas, bigramas y trigramas de etiquetas mor- fosint\u00b4 acticas y lemas, alrededor de la palabra actual), y que est\u00b4 an inspiradas por los traba- jos de Constant y Sigogne (2011), Schneider et al. (2014) y Riedl y Biemann (2016). Con respecto a los par\u00b4 ametros de la arquitectura, el \u00b4 unico congurable desde mwetoolkit es el coeciente de regularizaci\u00b4 on L2, teniendo el resto los valores por defecto jados por CRF- Suite. Tras algunas pruebas preliminares, he- mos elegido =0,1 para todos los tests. Finalmente, tambi\u00b4 en hemos implementa- do un modelo simple de referencia ( Baseli- ne), consistente en extraer del corpus de en- trenamiento las secuencias de etiquetas mor- fosint\u00b4 acticas correspondientes a EMs, elimi- nar aquellas con un n\u00b4 umero de ocurrencias inferior a 500 y anotar como EMs en el con- junto de prueba las entidades l\u00b4 exicas con esas secuencias de etiquetas. En caso de que ha- ya dos o m\u00b4 as posibles EMs que empiezan o terminan en la misma entidad l\u00b4 exica, selec- cionamos la m\u00b4 as larga; y si hay dos posibles EMs que se solapan, nos quedamos con la co- rrespondiente a la secuencia de etiquetas m\u00b4 as frecuente en el conjunto de entrenamiento. 3.3 Dise no experimental Para poder entrenar los modelos de apren- dizaje es necesario disponer de un esque- ma de etiquetaci\u00b4 on para EMs. Hemos elegidoIOB2 (Ramshaw y Marcus, 1995),3que cons- ta de tres etiquetas: B para marcar el elemen- to gramatical en donde comienza una EM, I para el resto de constituyentes de una expre- si\u00b4 on,yOparaloselementosfueradeunaEM. En el caso particular de mwetoolkit se usa el formato DiMSUM (Schneider et al., 2016), quepermiteincluirinformaci\u00b4 onadicional,co- mo el lema y etiqueta morfosint\u00b4 actica. En el caso de usar palabras de las etique- tas de sus entidades l\u00b4 exicas constituyentes Para encontrar el formato de CORGA que ofrezca el mejor rendimiento para el trata- miento de EMs, consideramos 8 conguracio- nes de prueba para cada arquitectura, combi- nando las siguientes opciones: a) usando amalgamadas, fra- sesdelcorpusenelaprendizaje,os\u00b4 ololasque contienen EMs; y c) usando las palabras o le- mas tal y como aparecen en el corpus, o con- virti\u00b4 endolas a min\u00b4 usculas. Inicialmente, esta \u00b4 ultima modalidad se incluy\u00b4 o para probar la versi\u00b4 on de mBERT preentrenada con texto en min\u00b4 usculas, pero la hemos extendido a las otras arquitecturas. Alprobarelrendimientoendiferentescon- guraciones de prueba, usamos la misma par- tici\u00b4 on del conjunto de datos inicial en tres subconjuntos de entrenamiento, validaci\u00b4 on y prueba, con una proporci\u00b4 on aproximada del 80%/10%/10% de las frases. El conjunto de entrenamiento se utiliza en el aprendizaje del modelo. Una vez entrenado, su rendimiento en la predicci\u00b4 on de EMs se mide sobre el con- junto de prueba con las m\u00b4 etricas Accuracy , Precision, Recall4y valor F1. Los conjuntos de validaci\u00b4 on se han empleado para seleccio- nar el coeciente en mwetoolkit y el um- bral de ocurrencias de Baseline, adoptando los valores asociados a los mejores resultados de F1 sobre dichos conjuntos. Tambi\u00b4 en que- remos emplearlos en el futuro para optimizar los hiperpar\u00b4 ametros de mBERT y XLM-R. Identicaci\u00b4 on. Primero hemos ltrado de CORGA fragmentos en idiomas extranjeros y frases cuya diferente segmentaci\u00b4 on en las versiones con o sin EMs hace imposible incor- porar los lemas y etiquetas a los constituyen- 4En espa nol, Exactitud, Precisi\u00b4 on dad, respectivamente, pero usaremos los nombres en ingl\u00b4 es por su mayor difusi\u00b4 on. Procesamiento de Expresiones Multipalabra en gallego mediante Aprendizaje Profundo 49Entrenamiento Validaci\u00b4 on Prueba Todas las N\u00b4 umero de elementos gramaticales en los conjuntos de entrenamiento usados para las pruebas de detecci\u00b4 on de EMs. Indicamos entre par\u00b4 entesis el tama no de los conjuntos de entrenamiento incluyendo s\u00b4 olo las frases sin EMs. tesdelasEMsautom\u00b4 aticamente.Acontinua- ci\u00b4 on, particionamos las secciones de CORGA (peri\u00b4 odicos, revistas, libros, guiones televisi- vos, blogs y transcripciones de programas ra- diof\u00b4 onicos) de modo que el primer 80% de las frases de cada secci\u00b4 on se asigna al conjun- to de entrenamiento, el siguiente 10% al de validaci\u00b4 on, y el 10% nal al de prueba. De este modo, el n\u00b4 umero de elementos gramati- cales es de conjunto de en- trenamiento (14.173.449 si s\u00b4 olo incluimos las frases con EMs), 4.955.264 para el de valida- ci\u00b4 on y 4.573.942 para el de prueba, incluyen- do 738.118 EMs.5. Con este particionamien- to, nos aseguramos de que s\u00b4 olo el 75,22% de las EMs presentes en los conjuntos de prueba sean conocidas durante el entrenamiento. Detecci\u00b4 on. El particionamiento anterior tiene el problema de que el reducido n\u00b4 umero de locuciones distintas identicadas en COR- GA hace que la pr\u00b4 actica totalidad de las mis- mas sean conocidas durante el aprendizaje, lo que no permite estimar la capacidad de generalizaci\u00b4 on de las arquitecturas considera- das para identicar nuevas locuciones. Dicha capacidad de generalizaci\u00b4 on puede ser \u00b4 util, adem\u00b4 as, en la tarea de detecci\u00b4 on, aplicando modelos ya entrenados a nuevo texto y extra- yendo las EMs desconocidas encontradas. Por lo tanto, pretendemos evaluar el ren- dimientodelasarquitecturasconsideradasen la detecci\u00b4 on de EMs, endonos en el caso m\u00b4 as desfavorable: una partici\u00b4 on del corpus en la que no haya solapamiento entre las EMs presentes en los conjuntos de entrenamiento, validaci\u00b4 on y test. Para ello, seleccionamos un subconjunto de las EMs presentes en COR- GA,loordenamosaleatoriamenteya nadimos las frases conteniendo el primer 80% de las expresiones en el conjunto de entrenamiento, las correspondientes a otro 10% en el conjun- to de validaci\u00b4 on y las del 10% nal en el con- 5Eltotal de elementos gramaticales supera a la cifra dada Dom\u00b4 nguez Noya, Barcala Rodr\u00b4 guez, y Molinero \u00b4Alvarez (2009) porque nosotros contamos separadamente los constituyentes de las EMs.junto de prueba. Completamos los conjuntos a nadiendo frases sin EMs con la misma pro- porci\u00b4 onde80%/10%/10%,aunquesinllegar a usar CORGA en su totalidad. Adem\u00b4 as, realizamos el mismo proceso de evaluaci\u00b4 on de arquitecturas sobre las NEs y locuciones, respectivamente. En el primer ca- so, generamos los conjuntos de entrenamien- to, validaci\u00b4 on y prueba como se detalla en el p\u00b4 arrafo anterior, pero usando s\u00b4 olo las fra- ses con NEs, y cambiando a O las etiquetas IOB2 de las locuciones que aparezcan en di- chasfrases.Tambi\u00b4 ena nadimosfrasessinEMs para las conguraciones de prueba que las re- quieran. El mismo proceso se realiza para los tests sobre locuciones, pero prescindiendo en ese caso de las frases con NEs. La Tabla 1 incluye los tama nos de los con- juntos de entrenamiento, validaci\u00b4 on y prueba usados para la detecci\u00b4 on de EMs, NEs multi- palabra y locuciones, y el n\u00b4 umero de estos. Los n\u00b4 umeros entre par\u00b4 entesis corresponden al tama no de los conjuntos de entrenamien- to cuando incluimos s\u00b4 olo las frases sin EMs. El n\u00b4 umero total de EMs en estos tests es de 488.035,siendoel70,53%delasmismasNEs, y el 29.47% locuciones. 4 Resultados En esta secci\u00b4 on vamos a presentar los resul- tados de nuestros experimentos, para todas las arquitecturas de aprendizaje y congura- ciones de prueba, tanto en el caso de la iden- ticaci\u00b4 on como de la detecci\u00b4 on de EMs. 4.1 Identicaci\u00b4 on Los resultados de los experimentos sobre identicaci\u00b4 on de EMs se presentan en la Ta- bla2.Comoseespecic\u00b4 oenlaSubsecci\u00b4 on3.3, tenemos 8 conguraciones de prueba, con re- sultados para las cuatro arquitecturas de en- trenamiento, para un total de 32 tests. En cada uno de ellos, medimos Accuracy (Acc), Precision (P),Recall(R) y valor F1. A la izquierda de la tabla se muestran los resultados para las conguraciones en la que amalgamas 2: Resultados de los tests de identicaci\u00b4 on de EMs. se ha usado texto con palabras amalgama- das, mientras que a la derecha aparecen los resultados para el entrenamiento con entida- des l\u00b4 exicas sin amalgamas. \"Todo\" identica a las conguraciones en las que se ha usado todo el corpus en el proceso de aprendizaje y prueba, mientras que \"Solo EMs\" correspon- de a los casos en los que s\u00b4 olo hemos usado las frases que incluyen EMs en el conjunto de en- trenamiento. Finalmente, \"Mins\" correspon- de a los tests en los que hemos usado texto en min\u00b4 usculas y \"Mays\" a aquellos en los que hemos mantenido las may\u00b4 usculas y min\u00b4 uscu- las tal y como aparecen en CORGA. Respecto a los valores de las m\u00b4 etricas, re- presentanporcentajesy,pormotivosdeespa- cio, se representan con dos cifras decimales. Se destaca en negrita el mejor resultado de cada m\u00b4 etrica para todos los tests, y se mues- tran subrayados los mejores resultados en ca- da conguraci\u00b4 on de prueba. Para determinar los mejores valores se han usado m\u00b4 as de dos cifras decimales. Tambi\u00b4 en hemos usado la prueba de los rangos con signo de Wilcoxon para compro- bar la signicancia estad\u00b4 stica de las conclu- siones obtenidas analizando la tabla. Para ahorrar espacio, s\u00b4 olo citaremos el valor de p en la minor\u00b4 a de casos en los que p0,05. Lo primero que salta a la vista en la tabla 2 es el liderazgo en rendimiento de mwetoolkit. Obtiene los mejores valores pa- raAccuracy (99,85%), Precision (97,49%), y F1 (96,94%), mientras capaz de aventajarlo mwetoolkit obtiene los mejores va- etricasentodaslascon-guraciones de prueba, excepto Recall, para la que mBERT es el mejor en las cuatro con- guraciones que p= 0,055). Con respecto a las conguraciones de prueba, los mejores valores de las m\u00b4 etricas se agrupan en la conguraci\u00b4 on \"Sin amal- gamas\"+\"Todo\"+\"Mays\", excepto en el ca- cuyo caso es \"Sin amal- gamas\"+\"Solo EMs\"+\"Mays\". Comparando cada opci\u00b4 on de conguraci\u00b4 sin amalgamas mejora los valores de Accuracy , Precision y F1, respectivamente, en un (p = 0,353) y 62,5% (p= 0,372) de los casos, aunque entrenar con amalgamas consigue un mejor Recallen el 75% de los tests. Para comparar los resultados de \"Todo\" vs \"Solo EMs\" y \"Mays\" vs \"Min\", hemos exclu\u00b4 do Baseline, dado que obtiene el mis- mo rendimiento en cada par de opciones.6 El entrenamiento con \"Todo\" obtiene mejo- res o iguales valores de Accuracy ,Precision y F1 en todos los tests, mientras que \"Solo EMs\" siempre genera un mejor Recall. Por may\u00b4 usculas y min\u00b4 usculas tam- bi\u00b4 en mejora de Accuracy (100% de los tests), Precision (91,67% de los tests), Recall(83,33%) y F1 (91,67%), lo que no resulta sorprendente, dado que buena parte de las EMs que estamos intentando identi- car son nombres propios. Hasta la versi\u00b4 on de mBERT preentrenada con texto en min\u00b4 uscu- tener peores resultados que la versi\u00b4 on preen- 6Haremos lo mismo en los tests de detecci\u00b4 on. Procesamiento de Expresiones Multipalabra en gallego mediante Aprendizaje Profundo 51Con amalgamas Sin amalgamas Acc P R 3: enada respectivamente), lo que se repetir\u00b4 a en los tests de detecci\u00b4 on de EMs. Enresumen,losvaloresdetodaslasm\u00b4 etri- cas son bastante altos, lo que prueba la viabi- lidad de las arquitecturas empleadas, si bien no son tan signicativos estad\u00b4 sticamente en aquelloscasosenlosquenohay unaarquitec- tura/conguraci\u00b4 on netamente superior. Esto es esperable y se repetir\u00b4 a en el resto de tests. 4.2 Detecci\u00b4 on En esta tarea, para la que intentamos prede- cirEMsdesconocidasenelentrenamiento,se- guimos las mismas conguraciones de prueba y tests de signicancia que en la subsecci\u00b4 on anterior, pero realizamos tres juegos de tests distintos, seg\u00b4 un las EMs que se procesan: to- das, s\u00b4 olo NEs y s\u00b4 olo locuciones. Todas las EMs. En la Tabla 3 presenta- mos los resultados para el primer caso, en el que mBERT obtiene los mejores valores de Accuracy (99,79%), Precision (93,68%)y Re- call(94,94%),XLM-RlideraenF1(88,44%), y mwetoolkit sigue obteniendo los mejores re- sultados en la mayor\u00b4 a de las conguracio- nes de prueba: mejor Accuracy en 4 de las 8, mejorPrecision en 7 y mejor F1 en 6. Por su parte, mBERT obtiene los mejores valo- res deRecallen 4 de dichas conguraciones (p= 0,334). Respecto a las opciones de conguraci\u00b4 on, \"Sin amalgamas\"+\"Todo\"+\"Mays\" obtiene la mejor Precision y F1, mientras que , y Recallcorresponde a con amalgamas mejora en el 75% ( p= 0,281), 68,75% y 75% de los casos, respec- tivamente. Sin embargo, empeora los valores deAccuracy en un 78,75% de los tests. Por su parte, \"Todo\" da mejores valores de Accu- racy,Precision y F1 en todos los tests, aun- que \"Solo EMs\" siempre da lugar a mejores valores de Recall, situaci\u00b4 on como aparecen en en los resultados de Accuracy ,Precision, Recall yF1enel58,33%(p = 0,190),100%,83,33% y 100% de los tests. En general, observamos un empeoramien- to de los resultados con respecto a la tarea de identicaci\u00b4 on, lo que es razonable dado que estamos asegurando que todas las EMs presentes en el conjunto de prueba sean des- conocidas durante el entrenamiento. S\u00b4 olo NEs multipalabra. Presentamos los tests correspondientes a este caso en la Ta- bla 4, con mBERT liderando en Accuracy (99,98%), Precision (98,14%) y valor F1 (98,16%), mientras que XLM-R obtiene los mejores resultados en Recall(98,47%). Sin embargo, mwetoolkit obtiene mejores valores que las arquitecturas transformer enAccu- racy,Precision y F1 para 6 de las 8 congu- raciones posibles, y mejor Recallen 4. A mo- do de comparaci\u00b4 on, mBERT obtiene el mejor Recallen 3 conguraciones, tests de detecci\u00b4 on de NEs multipalabr a. to de m\u00b4 etricas en s\u00b4 olo 2 casos. De manera similar a lo que ocurr\u00b4 a en los tests de identicaci\u00b4 on, los mejores valores de lasm\u00b4 etricasseencuentranenlaconguraci\u00b4 on \"Sin amalgamas\"+\"Todo\"+\"Mays\", excepto en el caso es \"Sin amalgamas\"+\"Solo EMs\"+\"Mays\". Compa- opci\u00b4 on de sin amalgamas mejora los valores de Accu- racy,Recall,Precision y F1, respectivamen- te, en un 100%, 87,5%, 66,25% y 72,5% de los tests, mientras que entrenar con \"Todo\" permite obtener mejores valores de Accuracy , Precision y F1 en todos los tests, y entrenar con \"Solo EMs\" siempre genera mejores valo- res deRecall. Entrenar con \"Mays\" da lugar a mejores valores para todas las m\u00b4 etricas res- pecto a usar s\u00b4 olo general, los valores de las m\u00b4 etricas tienden a ser mejores que en los tests previos. S\u00b4 olo locuciones. Este caso es, sin duda, el que arroja peores resultados, como se puede observar en la Tabla 5. La mejor Accuracy co- rresponde a mBERT (99,92%), la mejor Pre- cision(82,5%) a mwetoolkit, y los mejores valores de Recall(73,88%) y F1 (55,41%) a XML-R. Todos los valores anteriores, excep- toAccuracy , son sustancialmente m\u00b4 as bajos que sus hom\u00b4 ologos en los juegos de tests an- teriores, y el mejor resultado de Precision se obtiene a costa de un Recallmuy bajo (y viceversa). Adem\u00b4 as, no hay una arquitectu- ra m\u00b4 as consistente que las dem\u00b4 as: mwetool- kit obtiene 0,390) y Precision (p= 0,232) de mejor p= 0,232) obtienen los mejores valores de F1 3 veces cada uno. Con respecto a las conguraciones de prueba, entrenar con amalgamas mejora Ac- curacy,Recall,Precision 0,798), 93,75%, 93,75% de los tests. Esto es comple- tamente opuesto a lo visto las pruebas ante- riores, en las que usar entidades l\u00b4 exicas no amalgamadas mejoraba sustancialmente los resultados. Con respecto a las frases usadas en el entrenamiento, usar todas s\u00b4 olo ofre- ce mejoras en Accuracy (en todos los tests), mientras que usar s\u00b4 olo las frases con EMs mejoraPrecision, Recally F1 en un 66,33% (p= 0,545), 100% y 87,33% de los casos, res- pectivamente. De nuevo, ello contrasta fuer- temente con lo que hab\u00b4 amos visto en tests anteriores.Finalmente,\"Mays\"mejora Preci- sionyRecall(en ambos casos, en un 58,33% de los con p= 0,238 y p= 0,193, respectivamente), pero \"Mins\" ofrece mejo- resAccuracy y F1 (ambas en un 58,33% de los tests, con p= 0,361 para la segunda). Los resultados con locucionesson pobres y diferentes al resto de los tests, con menos sig- nicancia estadistica. Ello puede deberse a la mayor heterogeneidad de estas construccio- nes en relaci\u00b4 on con las NEs, al menor n\u00b4 umero de las mismas, y a que la lista de locuciones en gallego anotadas en CORGA no es nece- sariamentecompleta.Tampocopodemosdes- cartarquelosposibleserroresdeetiquetaci\u00b4 on morfosint\u00b4 actica tambi\u00b4 en tengan un efecto en el rendimiento. Procesamiento de Expresiones Multipalabra en gallego mediante Aprendizaje Profundo 53Con amalgamas Sin amalgamas Acc P R de los tests de detecci\u00b4 on de locuciones. 5 C onclusiones y trabajo futuro Podemos extraer algunas conclusiones preli- minares de este trabajo. La primera de ellas es que las arquitecturas de AA consideradas parecen funcionar bastante bien en la tarea de identicaci\u00b4 on, si bien los resultados son m\u00b4 as relevantes para NEs multipalabra que para locuciones, dado que estas \u00b4 ultimas son conocidas durante el entrenamiento casi en su totalidad. En la tarea de detecci\u00b4 on, los re- sultados son prometedores para NEs, y algo decepcionantes para las locuciones. Enloreferentealasarquitecturasconside- radas, mwetoolkit ha funcionado mejor de lo esperado en relaci\u00b4 on a las modelos transfor- mer, si bien se benecia de una informaci\u00b4 on (etiquetas morfosint\u00b4 acticas y lemas) no siem- pre disponible. Adem\u00b4 as, el gallego s\u00b4 olo repre- senta una peque na parte de los recursos usa- dos para entrenar dichos modelos transfor- mer. Por otra parte, en la tarea de detecci\u00b4 on, mBERT tiende a obtener los valores m\u00b4 as al- tos en las m\u00b4 etricas consideradas m\u00b4 as veces que XLM-R, a pesar de usar un modelo m\u00b4 as peque no y menos complejo. Con respecto a las posibles conguracio- nes de CORGA para el entrenamiento, ex- cluyendo la detecci\u00b4 on de locuciones, los re- sultados de los tests indican que: a) se tiende a obtener los mejores resultados entrenando con entidades l\u00b4 exicas sin amalgamar, aunque en la mayor parte de las conguraciones usar amalgamas parece benecioso; b) incluir tex- to de entrenamiento sin EMs parece mejorar los resultados en Precision y F1, pero em- peora los de Recall; c) por lo tanto, si nosinteresa recuperar el mayor n\u00b4 umero posible de EM a expensas de la Precision, entrenar s\u00b4 olo con frases que contengan EMs puede ser buena idea; y d) pasar a min\u00b4 usculas el texto no parece mejorar los resultados, incluso en una arquitectura (mBERT) espec\u00b4 camente preentrenada con texto en min\u00b4 usculas. a la identicaci\u00b4 on de locu- ciones, nuestra intenci\u00b4 on es estudiar por qu\u00b4 e en queremos comprobar has- taqu\u00b4 epuntolaoptimizaci\u00b4 ondehiperpar\u00b4 ame- tros puede mejorar el rendimiento en los mo- delostransformer . Para ello, nos proponemos hacer dicha optimizaci\u00b4 on en los tests en los que mBERT y XLM-R hayan sacado peores resultados. Finalmente, hasta ahora s\u00b4 olo hemos tra- bajado con la versi\u00b4 on de CORGA etiquetada autom\u00b4 aticamente. Nos proponemos hacer ex- perimentos con la porci\u00b4 on del mismo que ha sido corregida de manera manual y utiliza- da para entrenar XIADA, para comprobar si usar un corpus corregido manualmente tiene alg\u00b4 un impacto en los resultados. Agradecimientos Este trabajo ha sido parcialmente nanciado por la Xunta de Galicia, a trav\u00b4 es del Conve- nio de colaboraci\u00b4 on plurianual entre el Cen- tro Ram\u00b4 on Pi neiro para la Investigaci\u00b4 on en Humanidades y la Universidad de Vigo, y la Ayuda para la Consolidaci\u00b4 on y Estructura- ci\u00b4 on de Unidades de Investigaci\u00b4 on Competi- tivas ED431C 2018/50, y por el Ministerio de Econom\u00b4 a, Industria y Competitividad a trav\u00b4 es deep lexical acquisition for HPSGs via supertagging. En Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, p\u00b4 Asso- ciation Multiword Expres- sion EnACL'14 - Annual Meeting of the Association for Computational Lin- guistics, Baltimore, United States. ACL. Centro Ram\u00b4 on Pi neiro para a Investigaci\u00b4 on en Humanidades. 2019a. Corpus de rencia do Actual (CORGA) Ga- Bahdanau, F. Bougares, Y. Bengio. 2014. Learning phrase re- presentationsusingRNNencoder-decoder for statistical machine translation. En Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, p\u00b4 aginas 1724-1734, tar, Octubre. Association A., Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzm\u00b4 an, E. Grave, M. Ott, Zettlemoyer, y V. 2020. Unsupervised y Nivre. transition- based system for joint lexical and syn- tactic analysis. En Proceedings of the Meeting part-of-speech tagging with a CRF model and lexical resources. En Pro- ceedings of the Workshop on Multiword Expressions: from Parsing and Genera- tion to the the Association Papers) , p\u00b4 Lin- guistics. Devlin, J., guage understanding. En Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, Volume , Bhutada. 2009. Verb noun MWE token classication. y M. \u00b4A. S. y F. M. Barca- la Rodr\u00b4 guez. 2019. O Corpus de Referencia do Galego actual (CORGA): composici\u00b4 on, codicaci\u00b4 on, etiquetaxe e explotaci\u00b4 Galego de Filolox\u00b4 a , Anexo 74:179-219. Procesamiento de Expresiones Multipalabra en gallego mediante Aprendizaje Profundo 55Dubremetz, M. y J. Nivre. 2014. Ex- tr action of nominal multiword expres- sions in French. Abril. Association for Computational Linguistics. Farahmand, M. y R. Martins. 2014. of aginas10-16,Gothenburg,Swe- den, Abril. Association for Computational Linguistics. Firth, J. R. 1957. Papers in Linguistics, 1934-1951. Oxford University Press, Lon- don. Green, S., M.-C. de Marnee, J. Bauer, y C. D. Manning. 2011. Multiword ex- pression identication with tree substitu- tion grammars: A parsing tour de force with French. En Proceedings of the 2011 Conference on Empirical Methods in Na- tural Language Processing, p\u00b4 aginas Scotland, Computational Linguistics. Green, S., M.-C. de Marnee, y C. D. 2013. 9(8):1735-1780, 11. Kurfal, M. 2020. TRAVIS at PARSEME shared task 2020: How good is (m)BERT at seeing the unseen? En Proceedings of the Joint and C. Pe- reira. 2001. Conditional Random Fields:Probabilistic Models for Segmenting and Sequence Data. En Proceedings of the Eighteenth Conferen- on Machine Learning, ICML '01, p\u00b4 nas San Francisco, CA, Inc. Lapata, M. y A. role 10th Conference the Association for Agosto. Association for Computational Linguistics. Maldonado, A., L. Han, E. Moreau, A. Al- sulaimani, K. D. Chowdhury, C. Vogel, y Q. Liu. 2017. of N. 2007. CRFsuite: Frame- work, volumen XIV de Theory and Ap- plications of Natural Language Processing. Springer. Ramisch, C., A. Villavicencio, L. Moura, y M. Idiart. 2008. Picking them up and guring them out: Verb-particle construc- tions, noise and idiomaticity. En Clark y K. Toutanova, editores, Proceedings of the Twelfth ter, UK. Marcus. 1995. Text chunking using transformation-based lear- ning. En ora, VLC@ACL 1995, Cambridge, Mas- sachusetts, USA, June 30, 1995. Riedl, M. y C. Biemann. 2016. Impact of N., E. Danchik, C. Dyer, y N. A. Smith. 2014. Discriminative with Schneider, N., D. USzeged: par- sing techniques. Proceedings of the Joint Workshop Multiword Ex- pressions and O. Rohanian. SHO- MA expressions and named entitiesin the wiki50 Proceedings of the International Conference Recent Ad- vances in Natural Language Processing 2011, p\u00b4 aginas Hissar, Bulgaria, Septiembre. Association for Computatio- nal Linguistics. Vincze, V., J. Zsibrita, y I. Nagy T. 2013. Dependency parsing for identifying Hun- garian light verb constructions. En Pro- ceedings of the Sixth International Joint Conference on Natural Language Proces- sing,p\u00b4 aginas207-215,Nagoya,Japan,Oc- tubre. Asian Federation of Natural Lan- T., L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. R. Louf, M. Funtowicz, J. Davison, S. Sh- leifer, P. von Platen, C. Ma, Y. Jerni- te, J. Plu, C. Xu, T. L. Scao, S. Gug- ger, M. Drame, Q. Lhoest, y A. M. Rush. 2020. Transformers: State-of-the-art na- tural language processing. En Procee- dings of the 2020 Conference on Empi- rical Methods Language Pro- cessing: System Mexico, USA, Agosto. Association for Computational Linguistics. Procesamiento de Expresiones Multipalabra en gallego mediante Aprendizaje Profundo 57A utoPunct: A BER T-based Automatic Punctuation andCapitalisation System for Spanish and Basque AutoPunct: Sistema de Puntuaci\u00f3n y Mayusculizaci\u00f3n Autom\u00e1tico basado en BER T para Castel lano y Ander Alliance T A), Mikeletegi 57, 20009 Donostia-San Sebasti\u00e1n (Spain) 2University usually con- sists a stream of without any casing of this output, and capitalisation have to be included. In this context, we present AutoPunct, a T ransformers-based automatic punctuation information (the words themselves). W e Bidirectional Recurrent Neural Networks (BRNN) on both individually and simultaneously . The result is a system that high accuracy for punctuation and capitalisation in both languages at the same time, with a throughput of several thousand words per capitalisation, low-resource languages. Resumen: La salida en bruto de un sistema de Reconocimiento Autom\u00e1tico del Habla generalmente consiste en una secuencia de palabras sin may\u00fasculas ni si- gnos de puntuaci\u00f3n. Para mejorar la legibilidad y posibilitar posteriores usos de esta salida es necesario incluir la puntuaci\u00f3n y las may\u00fasculas. En este contexto, presentamos AutoPunct, un modelo para puntuaci\u00f3n y mayusculizaci\u00f3n basado en arquitecturas de T ransformers que combina tanto informaci\u00f3n ac\u00fastica (silencios) como l\u00e9xica (palabras). Hemos comparado su desempe\u00f1o con un sistema basado en redes neuronales recursivas bidireccionales (BRNN) en euskera (un idioma de po- cos recursos) y castellano, as\u00ed como combinando ambos idiomas. El resultado es un sistema que obtiene buenos resultados aplicando mayusculizaci\u00f3n y puntuaci\u00f3n de manera simult\u00e1nea en dos idiomas diferentes, con una velocidad de proceso que alcanza varios miles de palabras por segundo en una GPU est\u00e1ndar. Palabras clave: puntuaci\u00f3n, mayusculizaci\u00f3n, lenguas con pocos recursos. 1 Introduction Automatic Speech Recognition (ASR) sys- tems are increasingly more integrated in our daily lives and workflows through differ- ent solutions such as interfaces, speech-to-text applications or biometrics, among others. The growth of this technology has been mainly due to the evolution of Deep Learning techniques and their integration to develop neural models for speech (Nassif et al., 2019), in addition to the continuous release of more and more training data and the availability of powerful hardware devices for high perform-ance computing. The aim of an ASR system is to trans- form an audio input into text that may be exploited for further Natural Language Pro- cessing applications. However, the output string is usually composed by a raw sequence of words which does not casing nor punctuation marks, which noticeably reduces al., and its pos- sibility of being employed as input a correctly text (Peitz Therefore, is usually concat- enated to other technological modules which Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 59-68 del Lenguaje in charge of enriching the initial raw and adding punctuation and capitalisation. In this work, we compared the perform- ance of two neural of recovering the punctuation Spanish and Basque languages, individually and simultaneously . The first system was considered as a reference baseline. It is inspired on the architecture proposed by Alum\u00e4e includes a neural net- work (BRNN) model of Gated Recur- for punctuation decisions. As an alternative to this initial system, we present AutoPunct , which is composed by architecture based Encoder Representations from T ransformers (BER T) 2019), that com- bines the text representation obtained by the related to of the e tested this second system using several architectural variations in order to evaluate different the also evaluated with the integration of an ex- tra head to predict whether a word should be capitalised or not. The rest of the paper is structured as it fol- lows. Section 2 introduces related work in the field. Section 3 presents AutoPunct along with its main architecture. Section initial in Finally , Section 7 concludes the pa- per and presents future work. 2 Related work The challenge of automatically recovering capitalisation the literature. tems can be divided into three main These prosodic feature-based archi- tectures show that this type of information is indeed useful for the task, although they tend 2001; Kim W last years, the problem of recov- ering punctuation marks have been faced Deep Learning algorithms, such as Convolutional Networks (Che attention Alum\u00e4e, 2016), the word- and speech- embeddings (Yi and T ao, 2019), ransformers based on BER T-like architectures 2019), which have been shown obtain values as high as 83.9% (Courtland, on F1score in the well-known and reference IWSL T 2012 data set (F ederico et al., 2012). Different architectures show the use of a pre-trained BER T model such as RoBER T a (Liu et al., 2019) in order to ob- tain the word-embeddings, which are fed to various networks based on BiLSTM (Alam, Khan, and Alam, 2020) or focal loss (Yi et al., 2020b), or aggregated and McElvain, 2020). Another performs both punctuation and capital- isation simultaneously can be found in (Sunk- ara et al., 2020), where the word- or subword- embeddings obtained from a pre-trained or a custom BER T are fed to two Softmax layers for punctuation and specific domain of medical Nevertheless, variety of icated to been mostly evaluated over a single language: English. The system presented in this paper addresses 13 different punctuation marks, which are described in the section 5. Moreover, architecture of the proposed model features a multi-label output, which means predict more than a single punctuation mark per word. 3 Description of the systems As it was initially introduced, this work presents a comparison for capitalisation and punctu- both individually and simultaneously . These systems are described in more detail in the following subsections. 3.1 BRNN-based system The architecture proposed for the first system is based on Punctuator use of both GRU or LSTM layers recurrent layers, whilst the attention by a late output of the at- tention model to directly interact with the state of the recurrent layer while not interfer- ing with its memory . A simplified diagram of this architecture is shown in Figure 2. F or this work, we trained the model in two different stages. In the first stage, only annotated text is used to train an so learns punctuation second stage, a new model is estimated adding acoustic information as in- put, therefore learning by end time-codes given by the speech recognition system at word level. 3.2 BER T-based system Our BER T-based system, called system. a pre-trained BER T model with the acoustic information coming from silence duration between words. These two sources of information are further combined using an additional T ransformer model (a custom BER T trained from scratch). This system is trained to predict both punctuation and cap- italisation at the same time. A high-level dia- gram of the architecture of AutoPunct is shown in Figure 3. This particular architecture was care- fully considering should priorit- ised so the system can Capitalisation to exploit si- lence duration between words as an ad- ditional source of prosodic information. 4. The architecture should be language- independent. It should serve as a basis to train models for different languages by changing the training data and the pre-trained T ransformer model. In the following subsections, the main components of A BERT-based Automatic tem; this is, a paired with their silence duration value. A pre-trained BER T (Devlin et al., 2019) model is used to encode words as they are out- put into words, according to the language model represented by BER T. More informa- tion about the pre-trained models used in the experiments can be found in Section 6. This lexical and semantic information ob- tained by the pre-trained BER T is the silence duration for each in- dividual word. Given that the silence ues between word are scalars, they cannot be combined with the word-embeddings in this state. In order to solve this problem, we evaluated two different approaches of in- information to the sys- involves repeating the silence value as many times as the size of each word embedding (768 for the BER T- of parti- range 10 leading to 1000 dis-crete silence values. These discrete values are used as indexes a layer that provides, for each silence value, a trainable the embeddings. Nevertheless, using discrete silence val- with such a fine granularity has a major drawback: most of the discrete values will probably never appear in the training data; therefore their embeddings will not be used and they not capture any valuable in- formation. T o prevent this, all the embed- dings from a window of 1 second centred on the original value are averaged into a single embedding. W e evaluated the system with two types of averaging: uniform averaging, which each embedding embedding compute a weighted average of the embeddings. This final silence embed- ding is added to the word-embedding com- puted by the pre-trained BER T in order to obtain a combined representation. 3.2.2 Custom BER T As an additional step, we evaluated the sys- tem adding an extra T ransformer module layer after the word and silence combin- ation. This custom T ransformer, (hence- forth Custom BER T) is much smaller than a base BER T (4 hidden layers with 4 atten- tion heads each), and it is initialised from scratch. The rationale for adding this inter- mediate T ransformer is to endow the model with the ability to attend to the whole se- quence, instead of focusing on isolated com- binations of word and silence pairs, the among inputs to the T rans- former. The impact of this layer is also , the representation of each word is fed into two classification heads. These heads are composed of a dense layer, followed by a non-linearity , a dropout layer and a final linear layer that maps the input into the cor- responding label space, namely , the the capitalisation the capital- isation Since of the punctuation head is fed into the capitalisation head along with the word embedding generated by the pre-trained BER T at the beginning of the network. The punctuation head is treated as a multi-label classification head to cope with the fact that some words may bear attached, e.g. closing quotation mark and a resources, while the final models were constructed over the same main dataset employed in this work. With regard to the BRNN-based system, we followed the training stages described in subsection 3.1. At a first stage, an initial was using text features These were a text cor- pus of generic domain consisting of web news crawled from digital newspapers in Basque and Spanish. This generic text corpus is described in more detail in subsection 5.1. Then, the initial model was fine-tuned in a second stage of acoustic in- exploiting as mintzai- ST (Etchegoyhen et al., 2021). This corpus is also described in more detail in subsection 5.2. The models for all the languages were es- timated using the same training the validation perplexity was not improved at the first time, with a maximum epochs of 50 and a minibatch size of 128. The hidden layers consist of 256 units and we em- ployed a learning rate of 0.02. Regarding the second stage where the acoustic information was integrated, the training process was set to be finished when the validation perplexity was not improved in the last 3epochs with a maximum epochs of 50 and a minibatch size of 128. The hidden layers consist of 256 units and the learning rate was fixed to 0.02. The input vocabularies had a maximum size of 100,000 most fre- quent words that occur at least two times in the training corpus. The output vocabulary was composed by a sym- bol defined as O. The trainings were perfomedon a 12 GB Nvidia Titan X GPU card. Regarding the BER T-based system AutoPunct, we employed a specific pre- trained BER T model for each language: BETO for Spanish (Ca\u00f1ete et al., 2020), BER T eus for Basque (Agerri et al., 2020), and IXAmBER T for Spanish+Basque (Otegi et al., 2020). As in the previous architecture, the final models of the BER T-based systems were estimated on the mintzai-ST dataset and using the same hyper-parameters to make them comparable. The learning-rate was set to 2\u00b7105with a warm-up period of 5 epochs and using Adam W (Loshchilov and Hutter, 2019) as the optimizer. The training mini-batches were of size 8. The training of each model was performed using an Nvidia GeF orce R TX 2080ti GPU with 11GB of memory for a maximum of 50 epochs with an early-stopping patience of 20 epochs, monitoring the F1metric on corresponding development set. 5 Main datasets In this section, we first describe the text cor- pus used to estimate the initial models of the BRNN-based models, and then we present the main dataset employed to generate the final 5.1 Generic text corpus This corpus is composed by news of generic domain crawled from digital newspapers from 2012 to 2019. The number of words for each partition is shown the generic text corpus per language. The ES+EU corpus is a concatenation of the data from EU and ES. 5.2 Mintzai-ST corpus As it was previously mentioned, the final models of both BRNN-based and BER T- based systems were trained and evaluated with the which incorpor- ates both textual and acoustic features. This corpus is composed by a collection of manual transcriptions of proceedings of the Basque AutoPunct: A BERT-based Automatic Punctuation and Capitalisation System for Spanish and Basque 63Parliamen t from 2011 to 2018, which com- prises content in Basque and Spanish. The original train, development and test parti- tions of the corpus were maintained in both languages as they are described in its related paper. T able 2 presents the amount of the mintzai-ST per language. The ES+EU corpus is a concatenation of the data from EU and ES. The original mintzai-ST corpus was pro- cessed in order to represent the information needed to train models. This information is related to each word and consists of the silence duration, and the capitalisation label. The word and the silence value act as inputs, while the other two are the outcomes the sys- tem should the cor- pus mintzai-ST can be found in Figure 4. primer O 0.00 FIRST_CAP punto O 0.00 O del O 0.00 O orden O 0.00 O del O 0.00 O d\u00eda COLON 0.00 O pregunta OPEN_QUOTE 0.00 FIRST_CAP formulada O 0.00 O por O 0.00 don O 0.00 O O is Olabel to indicate words that bear no punctuation. A single word can have more than one punctuation label attached to it. The punctuation labels were derived from the different Unicode code-points present in the original transcriptions of the dataset, e.g. the code-points U+00BB (\u00bb) (\") are both labelled as QUOTE. T able 3 shows the distribution in percent- ages of the punctuation labels for Basque (EU), Spanish (ES) and Spanish+Basque (ES+EU). The distributions shows a label unbalance. This is be expected since some punctuation marks, such as periods or com- mas, are much than the others. label EU ES ES+EU COMMA 54.62% .41% QUESTION 1.87% 1 COLON 1.88% 1 .49% 1 .61% DASH 1.21% 1 .54% 1 .44% OPEN_QUES 0.02% 1 .68% 1 .20% SEMICOLON 1.08% 1 .20% .16% OPEN_QUOTE 1.15% 1 .09% 1 .11% QUOTE 1.02% 1 FIRST_CAP if the first letter of the word is a capital letter, and ALL_CAPS if the whole word is written in capital letters. Similarly to the punctuation, the label Oin- dicates that the word is not capitalised. F or words that do not fall into any of these cat- egories (e.g. EiTB), these same criteria are similarly the first letter is cased it would carry the label FIRST_CAP, and O otherwise. 6 Evaluation In tained by each neural architecture proposed in this work on the test partition of the mintzai-ST corpus. In the case of our BER T- based system, it is composed of our experiments, we constructed several models with different combinations in order to with discrete lences). or skipping the additional BER T). Punctuation shows the micro-averaged F1score experiment with AutoPunct, and the comparison with Punctuator. S D f B B: Adding custom BER T. achieved by the BRNN-based system are also displayed. As be Auto- Punct obtains scores show a moderate improvement towards the lower part of the table, where discrete silences are used in combination with the intermedi- ate custom BER T. In T able 5, the evaluation results for each individual punctuation label PERIOD for Basque and Spanish+Basque. Nevertheless, the rest of punctuation marks the BER for is 0.00%, such as AutoPunct reaching F1scores higher than 50%. This can be due to the fact that in the first corpus used to train the BRNN- based system (Section 5.1) does not contain these well as to their low appear- ance rate in the corpus of mintzai-ST. As it can be appreciated, for the Basque language, the absence of predicted marks are not used this language. F or Spanish, in contrast, OPEN_QUES reaches a 64.8%. In the case of the labels with lower F1scores, it seems that the number of occurrences in this dataset are not enough for a proper training and evaluation. Finally , in the case of the Spanish+Basque model there is a small performance loss, but it can be considered a reasonable module. F urthermore, it is Basque to Spanish or sen- tences in casual model that deals with both lan- guages at the same time can be advantageous on scenarios. 6.2 Capitalisation results The presented AutoPunct, since the BRNN-based does not perform this T able 6, follow- ing the same experiment breakdown. As it can be seen in T able 6, the obtained micro-averaged F1values are very high in all the cases and for every language scen- ario. The variations in the architecture do not show a high impact in the final scores. 6.3 Inference speed In addition to evaluating the quality of the systems in the automatic punctuation and capitalisation parameters like assess second at inference time. measure has been carried on during the evaluation, using the set eval- AutoPunct: A BERT-based Automatic Punctuation and Capitalisation System for Spanish and Basque 65Basque (EU) S Df B COM PER QUES 61.0 51.2 0.0 0.0 0.0 0.0 0.0 \u00d7 - 75.5 85.0 63.5 47.1 3.6 0.0 5.4 0.0 6.0 86.1 62.0 52.2 23.4 0.0 0.0 12.4 0.0 0.0 0.0 0.0 0.0 Spanish (ES) S D f B COM PER QUES COL DASH O_QS Spanish+Basque (ES+EU) S COM 16.1 2.0 able 5: Class-wise F1scores each language. Labels with a F1score of 0.0 in the three language were contiguous buckets. B: Adding a custom BER T. S D f B EU ES ES+EU \u00d7 - ets. B: Adding a custom BER T. uations have been run using a Nvidia orce R 2080ti GPU1. BRNN-based system. These results are shown in T able 7. F rom the results of the table 7, it can be observed that the computation speed is fast enough to enable a real-time processing. The trend in the numbers show that the use of discrete silences requires more time, but this is not surprising due to the extra amount of computation to select and average the silence embeddings. The same reasoning applies to the intermediate Custom tural be since different hardware workloads different Haritz B: Adding a custom BER T. AutoPunct is also adding capitalisation to the output in the same process. 7 Conclusions In this work we present AutoPunct , an automatic punctuation and capitalisation system based on BER T that also makes use of the silence duration between words. The system was trained for 13 different punc- tuation labels and two types of capitalisa- tion. It several punctuation marks is lan- guage agnostic and only depends on training data, it can be even trained on several lan- guages at the same time. Due to its infer- ence speed, ranging from 3000 to 7000 words per second depending on the chosen The system was tested in to also the system Punctuator (Tilk times. As fu- ture test additional ar- chitectural variations and hyper-parameters, and also train and more styles and application domains. Ackowledgments This work was supported by the Department of Economic Development and Competitive- ness of the Basque Government under pro-jects GAMES (ZL-2020/00074) and Deep- T ext (KK-2020-00088). References Agerri, R., I. S. Vicente, J. A. Campos, A. Barrena, X. Saralegi, A. Soroa, and E. Agirre. 2020. Give your text repres- entation models some love: the case for basque. arXiv preprint arXiv:2004.00033 . Alam, T., A. Khan, and G. Chaperon, R. F uentes, J.-H. Ho, H. Kang, and J. P\u00e9rez. 2020. Span- ish pre-trained bert model and evaluation data. In PML4DC at ICLR 2020. Che, X., C. W ang, H. Y ang, and C. Meinel. 2016. Punctuation prediction for M.-W. Chang, K. Lee, and K. T outanova. 2019. BER T: Pre-training of deep June. Etchegoyhen, H. Arzelus, Gete Ugarte, A. and Cettolo, Bentivogli, P . Michael, and S. Sebastian. 2012. Over- view the 2012 T ranslation, pages 12-33. Jones, D. A., F. W olf, E. Gibson, E. Willi- ams, E. F edorenko, D. A. Reynolds, and M. Zissman. 2003. Measuring the ability of BERT-based Capitalisation Spanish and Basque 67Kim, J.-H. and P . C. W oodland. 2003. A generation M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy , M. Lewis, L. Zettlemoyer, and V. eight Decay Regularization. In Proceedings of the Seventh International Conference on Learning Representations (ICLR 2019), pages 1-18. Nassif, A. B., I. Shahin, M. , 7:19143-19165. Otegi, A., A. Agirre, J. A. Campos, and E. resource dataset case study for basque. In Proceedings of The 12th Language Re- sources and Evaluation Conference, pages 436-442. Peitz, S., M. F reitag, A. Mauser, and H. Ney . 2011. Modeling punctuation prediction as machine translation. In orkshop on Spoken Language T ransla- tion (IWSL T) 2011. 1997. Bidirectional recurrent M., S. Ronanki, K. Dixit, S. Bodapati, K. Kirchhoff. 2020. Robust prediction of punctuation and truecasing for medical asr. pages 7270-7274. Yi, J., J. T ao, Y. Bai, Z. Tian, and C. F an. 2020a. Adversarial transfer learning forpunctuation restoration. arXiv preprint arXiv:2004.00248. Yi, J., J. T ao, Z. Tian, Y. Bai, and C. F an. ocal caracter\u00edsticas en el dataset multimodal CMU-MOSEI: Caracter\u00edsticas no correlacionadas y convolucionadas Daniel Mora Melanchthon Grupo T ecling.com Pontificia Universidad Cat\u00f3lica de V alpara\u00edso, Chile the use of Convolutional Neural Network (CNN) models to act as emotion feature extractors. Experiments are performed with Random F orest (RF). show, that the feature not change model's performance, time, and Este estudio investiga dos caminos con el fin de mejorar las caracter\u00edsti- cas unimodales que son utilizadas para el reconocimiento de emociones en el dataset multimodal CMU-MOSEI. El primer camino es la selecci\u00f3n de caracter\u00edsticas basado en la correlaci\u00f3n de Spearman al interior de cada modalidad (textual, ac\u00fastica, vi- sual). El segundo camino es utilizando una Red Neuronal Convolucional (CNN) para extraer caracter\u00edsticas unimodales que sean relevantes para el reconocimiento de emociones. Los experimentos comparan los distintos sets de caracter\u00edsticas uti- lizando un Bosque Aleatorio (Random F orest). Los resultados muestran, primero, que el uso de caracter\u00edsticas unimodales no correlacionadas no modifican el resul- tado del modelo, lo que permite reducir la cantidad de par\u00e1metros, tiempo de en- trenamiento y almacenamiento computacional. Segundo, el uso de caracter\u00edsticas generadas por el modelo de Redes Neuronales Convolucionadas utilizadas en un Bosque Aleatorio s\u00ed genera mejoras para la modalidad ac\u00fastica, lo que sugiere que futuras mejoras puedan desarrollarse en esta l\u00ednea. Palabras clave: CMU-MOSEI, reconocimiento de emociones, correlaci\u00f3n de Spear- man, extracci\u00f3n de caracter\u00edsticas. 1 Introduction Emotion Recognition is one of the goals of to man emotions (Picard, 1995). How Machine Learning models perform on these re- lies mainly on available datasets and features extracted from them. There has been a ma- jor improvement in accomplishing these multimodal information, when and how to fuse it (Atrey et al., 2010), and even how to build consistent and appropriate datasets, are still active research Emotion Recogni- tion on can be attributed to model's improve- ments, but there have not been much consid- Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 69-81 Sociedad para el Procesamiento del Lenguaje Naturalerations regarding what features have been used or if they could be improved. This re- search shows how unimodal Random F orest model was used to performance of un- correlated feature sets and CNN-embeddings els used. Section shows results of the experiments, and finally section 7 discusses the main findings and proposes future areas of investigation. 2 Background Emotion Recognition is the task of recogniz- ing specific affective states feelings that are classified certain typology . In classification changes observed that there is a pitch in- crease when feeling fear, but a decrease when happy , and much slower when feeling sad- ness. Regarding visual cues, emotions are expressed by the movement of face muscles: when compared to a neutral face, a happy state moves the cheek muscles upwards ized by rising eyebrows, and opening of the eyes mouth.This line of highly dis- criminative of affective states. This is the case The Extended Geneva Minimalistic Acoustic Parameters Set (eGeMAPs) (Eyben et al., 2016) which derived from decades of research on how acoustic has more than 6,000 features and demands high computer resources for capabilities to learn represen- tations from the sary pre-processing steps (Guyon and Elisse- eff, 2003). In this sense, it is common prac- tice to select a subset of features following a feature-knowledge or data criteria (Liu and Y u, 2005). In this study , Spearman correla- tion as a knowing Y is a much each word is in- dicative of a given emotion. Jin et al. (2015) proposed a Support V ector Machine that achieves derstanding ally not a one-to-one straightforward map- ping between features and outputs. This is made even more evident when affective states are spontaneous and not elicited. used studies on scripted As spontaneous to achieve higher performance, which suggests that the manner CMU-MOSI (Zadeh et al., 2016), CMU-MOSEI (Bagher Zadeh et al., 2018). One of the reasons for this advancement is not better extraction of features or a deeper understanding of how they task. However, this improvement comes with the drawback of a lack understanding in- resentations. T tional erm Memory (LSTM) same dataset, T zirakis, Zhang, and (2018) used a CNN model to learn feature raw waveforms, and added a Bidirectional LSTM (BiLSTM) on top to model contextual information for emotion recognition task. Their experiments confirm that an end-to-end model text, visual, and acoustic modalities an fusion architecture. The three modalities are concatenated and serve as input for CNN, LSTM, or BiLSTM models. The BiLSTM model yielded the best performance winning the Grand Challenge and W orkshop on Hu- man several Neural Network models have focus on model's architecture -mainly LSTM et al., 2018), and even on unaligned modalities (T sai et al., 2019). The outer-product between bimodal representa- tions was proposed as an alternative (Sun et and a gating mechanism to learn bimodal interactions (Kumar and V epa, 2020). The use of a wider context has been ex- plored to account for temporal variation of emotion in conversational setups. Shenoy and Sardana (2020) made use of the model's outputs for previous sentences as modali- model is more robust if any modality is missing (e.g. there are no vi- sual cues), and T sai et al. (2020) sought different modalities' contribu- the best results on et al. (2019) Analysis F ear text, visual, and acoustic modalities. Each modality provides information from a different channel text features are the words pro- nounced, to signal of CNN et al., 2014), and lately BER T embeddings (Devlin et al., toolkits , (Ji et al., 2013), F acet (iMotions), or openF ace (Baltrusaitis et al., these at expense the model's interpretability (Buhrmester, M\u00fcnch, and Arens, 2019), especially when to understanding the performance at a fea- ture level. Knowing which features better model sentiment and emotion allows us to reduce the amount of trainable parameters, the model's training time, and storage 2015). It also helps us in in order to reduce unimodal feature sets, and if a CNN model is able to better generate feature representations for features used are BER T embeddings as text modality due to their high performance across different tasks; openSMILE to toolkit CMU- MOSEI dataset and at the time largest and each segment lasts 7.6 seconds. According to Zadeh et al. (2018), videos were automatically extracted from the Y ouT ube platform and in which the camera stood still in front of the speaker; presence of text, acoustic, and visual modalities; (57% all segments, 28% have emo- tions, and 72% present 1 or 2: 'Happiness' is the most common occurring approximately on 50% of all video segments, and 'F ear' is the least common with an appearance of just 0.08% on the set, with 1 summarizes the relation between presence and absence per emotion on the dataset. With respect to arousal values, emotions are either not (arousal = present (arousal = 0.33). The emotion 'Hap- number 1https://gith ub.com/A2Zadeh/CMU- MultimodalSDK/ Daniel Mora Melanchthon 72Modalit y Description T ext sp hello everyone sp and welcome to a sp customer Acoustic pause ). 'Acoustic' is about recognizing a face). of annotations than the rest of the emo- tions, and shows a decreasing curve = 3.0), whereas is the least common F eatures F or and Minimalistic Parameter Set (eGeMAPs). F or visual features, the open- F ace toolkit (Baltrusaitis et al., 2018) was used. multidisciplinary on affecting pro- as the (GeMAPs) that has 62 features as well as its extended version eGeMAPs with 88 features. The selection of acoustic refer section man, 1980), in addition with other cues, such as how the head moves and what direction the eyes are looking to- wards. The general working process considers detecting a face, and computing ex- tract 3D facial landmark location, and based on this it is able to extract head pose, eye gaze, face alignment, and F AU. It outputs in total 709 features every 0.333 seconds. T o ensure reliability and real-time starts initialization However, if a confidence thresh- old is not achieved, then it outputs 0 values for all features. 4.3 T ext F eatures W e used pre-trained-uncased BER T base- model (W u et al., 2016)- and us- ing information regarding the position of the word in the sentence as contextualized infor- mation. The result is a dynamic representa- tion of each word. It outputs 12 layers of 768 dimensional vectors per word. This contrasts (Pennington, Socher, ning, 2014), not take into account the position of words, thus word embeddings regardless entire following Zipf's Law, a majority of words are only once in a corpus. Sentences were using the Accord- ing to Devlin et al. (2019), the last 4 layers without fine-tuning show competitive results, thus these are used 5 T aking the initial feature sets as a baseline, two feature sets were derived: the first one with uncorrelated features and the second one with convolved features. Experiments were tried out using Random F orest model. 5.1 Random F orest Random F orest (Breiman, 2001) is an ensem- ble method that combines multiple Decision T rees (Quinlan, 1986) by taking the average of all T rees' output. The main motivation of doing this was the ease of interpretation of the model: a tree-based criterion is Gini index. 5.2 CNN as F eature Extractor Besides their high performance across com- mon benchmarks on 2D and 1D signal (Khan et al., 2020), Convolutional Neural Network (LeCun et al., 1989) was cho- sen due to its ability to work as a feature ex- tractor and achieve competitive performance, even when Convolutional Neural Networks, but instead of a matrix multiplication, they a linear operation called convolution (Good-fellow, Bengio, and Courville, 2016). In its general form, the convolution layer has a ker- nel that traverses the input data applying the convolution a the input data outputting features maps. Because kernel has Kavukvuoglu, F arabet, 2010). It is common practice apply a pooling layer to extract highly-discriminative dimen- sions only and reduce parameters along the net (Khan et al., 2020), and regularize Figure the CNN's architecture: it is fed with features that act and six applied before fully-connected that to ranking the samples from each variable (Zwillinger and Kokoska, 1999). Its value ranges from -1 (as one variable in- creases, the other decreases), to +1 (as one variable the other increases). 0 value value are assigned the mean rank of their position. T o obtain correlation, following formula is Melanchthon 74Figure Architecture of CNN model used to act as feature extractor. The star symbol rep- resents the convolution operation. 'F final connected is as input for Random F orest model. The output are six values for emotion recognition value per , and d2 iis the squared difference of the linkage on correla- tion matrix, and its result is procedure into a binary tree (Bar- Joseph, Gifford, and Jaakkola, 2001). It starts from each sample being a cluster on its own, and iteratively Euclidean distance between all elements from a cluster and its mean. Then, the variance of each cluster is summed up and obtained the variance value of the merge. The merge that increase retrieved. T o flatten the dendrogram, a threshold tindicates what the maximum distance will be considered to cluster. If tis low, then only nearby clusters will be merged. If tis high, then clusters and openF ace feature sets. As evidenced by the openF gaze' and 'landmarks' groups (see fig- ure 4) that have similar measures but with different metrics (e.g. 2D and 3D), and it is considerably grouping ters slight correlation degree of correlation is ex- given that made related LLDs (e.g. loud- ness and energy). 6 Experiments Random F orest was used to compare three feature the convolved feature set. The train, valid, and test set are the ones predefined by the and commonly tested. The one is uncorrelated feature sets will leave model's performance unchanged, given that the same information is conveyed by multi- ple features. The second one is that convolved-features are expected to better model emotion com- pared to initial feature sets. This difference should be notable for eGeMAPs and openF ace feature sets compared that are Figures on the left are the initial feature set, and on the right the reduced set with a cophenetic distance of 1. Y ellow means high correlation blue means sets tion. consequence of be- ing the only emotion that is present in almost 50% of the dataset. Results per emotion ness' is better predicted with acoustic modal- ity -in line with T sai et al. (2020) inter- pretation of acoustic fail, except for BER and eGeMAPs that show an imperceptible improvement. The improvement per emotion regarding each feature set interpreted as how each modality , t = 1. perceived these emotions are for label anno- tators through these modalities. A closer look openF to predict the emotion 'Happiness' whether it is present absent. However, as there are fewer samples of each emotion, the model tends to predict absence of the emotion (i.e. negative predic- tion) most of the time. As can be seen, for 'Anger' the model is able to correctly classify almost 10% when the emotion is present, and for 'Surprise' it never predicts the presence of that emotion, which yields an F1-score of 0.0. Regarding how Random F orest performed with the convolved features sets, two conclu- sions are derived. The first one is that for text modality it was not useful. The intuition that fine-tuning BER T features according im- provement is rejected. This is also the case for visual modality that did not see any im- provement but decreased spe- cially for 0.0 0.0 0.0 0.0 BER T 0.17 0.70 0.05 0.02 0.0 0.23 0.005 BER T-CNN 0.02 feature set. Bold numbers reflect the acoustic modality . is huge in- 'Anger', 14 points, respectively . Along all feature sets, eGeMAPs-CNN is the best recogniz- ing emotions with a F1-macro score of 0.21 which is even better than BER T. This difference in how the CNN model be- haves as of features set is rele- vant. instance, eGeMAPs has 88 fea- tures against openF ace with 709 and BER T with 768. Even though eGeMAPs-CNN de- creased 3 points such use of features that depict the same information, as the case of openF ace feature set, yields highly correlated features that are not crucial for the Emotion Recognition task. F or example, the initial set could be reduced from 709 129 dropping 'eye 'landmarks' training time, using less In general, all three modalities perform similar on this task. T wo reasons might ex- plain this behaviour. One reason is that the dataset is unbalanced towards one particu- could not be consistent throughout the dataset. F or ex- ample, not present and convey misleading (background music, no recognition of faces, im- ages, etc.). One way to address this issue could be assigning more weight to if features are reliable. When considering the capability of a deep CNN model as feature extractor, results show that text acoustic modality rep- resentations the could be improved by Neural Network models. these experiments show that fea- ture engineering is useful, but how to worth investigating kind of information the modalities are in fact conveying and to what extent this has an impact on the model's re- sults. Acknowledgment This work was supported by the of Chile through \"Proyecto F ondecyt Regu- lar 1191481: Inducci\u00f3n autom\u00e1tica de tax- onom\u00edas de a in- vestigator Language - Multi- Comp. Atrey . K., A. El Saddik, and M. S. Kankanhalli. 2010. Multi- modal fusion for multimedia analysis: a survey . Multimedia Systems, 16(6):345- 379, November. Bagher Zadeh, A., P ceedings of 56th Annual the Association for Linguistics (V olume 1: Long Papers) , pages 2236- 2246, Melbourne, Australia. Association for Computational Linguistics.Baltrusaitis, T., A. Zadeh, Y. C. Lim, and L.-P . 2018. Behavior Analysis T oolkit. In 2018 13th IEEE International Conference on Automatic F ace Gesture Recognition (FG 2018), pages 59-66, May . Bar-Joseph, Z., D. K. Gifford, and T. S. Jaakkola. 2001. F ast optimal October. Buhrmester, and M. Arens. Analysis of Explainers of Black Box Deep Neural Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan. 2008. IEMOCAP: interactive soulis, G. V otsis, S. Kollias, W. F ellenz, J. T aylor. 2001. Emotion recognition in human-computer IEEE Signal Processing Magazine, 18(1):32-80, January . Conference Name: IEEE Signal Processing Magazine. Degottex, G., J. Kane, T. Drugman, T. Raitio, and S. Scherer. 2014. CO- voice analysis K. Lee, and K. T outanova. 2019. BER T: Pre- training of Deep Bidirectional Ancoli. 1980. F acial signs of emotional ence. Journal of Personality and Social Psychology Daniel Mora Melanchthon 78Eyben, F. 2016. Real-time and Music Classification by Large Audio F ea- Space Extraction. Springer Theses. Springer International Publishing, Cham. F., K. R. Scherer, B. W. Schuller, J. Sundberg, E. Andre, C. Busso, L. Y. Devillers, J. Epps, P . Laukka, S. S. Narayanan, and 2016. The Geneva Minimalistic Acoustic for oice F., B. Schuller. 2010. the munich versatile and fast open-source audio feature extrac- tor. In Proceedings of the international conference on Multimedia - MM '10, page 1459, Firenze, Italy . ACM Press. Ghosal, D., M. S. Akhtar, of the 2018 Conference on Empirical Methods in Natural Language Processing Y. and A. Courville. 2016. Deep Learning. MIT Press. Guyon, I. and A. Elisseeff. 2003. An Intro- duction to V ariable and F eature Selection. page 26. Hinton, G., L. Deng, D. Y u, G. E. Dahl, A.-r. Mohamed, N. Jaitly , A. Senior, V. V an- houcke, P . Nguyen, T. N. Sainath, and Speech Recognition: The Shared Views of F our Research Groups. IEEE Signal Processing Magazine, 29(6):82-97, November. Con- ference Ji, S., W. Xu, M. Y ang, and K. Y u. 2013. 3D Convolutional Neural Networks for Hu-man Action Recognition. 35(1):221-231, January . Jin, Q., C. Li, S. Chen, and H. W u. 2015. Speech emotion recognition with acoustic and lexical features. In 2015 IEEE Inter- national Conference R. Video Classification with Convolutional Neural Networks. IEEE on Computer Vi- and Pattern Recognition, pages 1725- 1732, June. ISSN: 1063-6919. Khan, A., A. Sohail, U. Zahoora, and A. S. Qureshi. 2020. A survey of the recent architectures of neural networks. Artificial Intel , April. Kim, J., E. Andre, M. Rehm, T. V ogt, and J. W agner. 2005. Integrating Hen- derson, R. E. Howard, Hubbard, D. and in on Circuits and Systems (IS- CAS'10). IEEE. Liu, H. and L. Y u. 2005. T oward integrat- ing feature selection Engineering. Louppe, G. 2015. Understanding Ran- dom and C. Manning. 2014. Glove: Global V ectors for W ord Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Morency Poria, S., E. Annual Meeting Association for Lin- Canada. Association for Computational Linguistics. Quinlan, J. R. 1986. Induction of deci- sion trees. Machine Learning, 1(1):81- 106, March. Ringeval, F., A. Sonderegger, J. Sauer, and D. Lalanne. 2013. Introducing the RECOLA multimodal corpus of remote affective interactions. 10th IEEE International Confer- ence and W orkshops on Automatic Recognition (FG) , M. Pantic. teraction - ICMI '12, 449, Santa Monica, California, USA. ACM Press. Shenoy , A. and A. Sardana. 2020. Multilogue-Net: A Context A ware RNN for Multi-modal Emotion Sentiment W. Sethares, and Y. Liang. 2019. Learning Relationships between T ext, Audio, and Video via Deep Canonical Correlation for Multimodal Language Analysis. arXiv:1911.05544 [cs, stat], November. arXiv: emotions Agents - 2017, pages 45-46, Glasgow, UK. ACM Press. T rigeorgis, G., F. Ringeval, R. Brueckner, E. Marchi, M. Con- and Signal March. ISSN: 2379-190X. T sai, Y.-H. H., S. Bai, Liang, 1906.00295. T sai, Y.-H. H., M. Q. Ma, M. Y ang, R. L.-P . Recog- nition Using International Signal Processing (ICASSP), pages modal Language , pages 11-19, Melbourne, Australia. for Computational Linguistics. Daniel Mora Melanchthon 80W u , Y., M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey , M. Krikun, Y. Cao, Q. Gao, K. Macherey , J. Klingner, A. Shah, M. Johnson, X. Liu, . Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. W ang, C. Y oung, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean. 2016. Google's Neural Machine T ranslation System: Po- ria, E. and L.-P . Zwillinger, D. and Kokoska. 1999. CRC Standard Probability and Statistics T ables Stereotype Identi cation Modelos Basados en Enmascaramiento y en BERT para la Identi caci\u0013 on de Estereotipos social bias increasingly present in the human interaction in social networks and political speeches. This challenging task being studied computational of rise of hate o ensive language, and discrimination that many people receive. In this work, we propose using based on Transformers; and a text masking technique that has been recognized by its capabilities to deliver good and human-understandable results. Finally, we show the suitability of the two models for the task and o er some examples of their advantages in Resumen: Los estereotipos sobre inmigrantes son un tipo de sesgo social cada vez m\u0013 as presente en la interacci\u0013 on humana en redes sociales y en los discursos pol\u0013 \u0010ticos. Esta desa ante tarea est\u0013 a siendo estudiada por la ling\u007f u\u0013 \u0010stica computacional debido al aumento de los mensajes de odio, el lenguaje ofensivo, y la discriminaci\u0013 on que reciben muchas personas. En este trabajo, nos proponemos identi car estereotipos sobre inmigrantes utilizando dos enfoques diametralmente opuestos prestando atenci\u0013 on a la explicabilidad de los mismos: un modelo de aprendizaje profundo basado en Transformers; y una t\u0013 ecnica de enmascaramiento de texto que ha sido reconocida por su capacidad para ofrecer buenos resultados a la vez que comprensibles para los humanos. Finalmente, mostramos la idoneidad de los dos modelos para la tarea, y ofrecemos algunos ejemplos de sus ventajas en t\u0013 erminos de explicabilidad. Palabras clave: sesgo social, estereotipos BETO, t\u0013 ecnica de 1 Introduction Nowadays, among im- pact on people perceive reality. Very of- ten, the information consumers are not aware of how biased is what they are exposed to. To mitigate this situation, many computa- tional linguistics et al.,2018; Liang et al., 2020; Dev et al., 2020). The immigrant stereotype is another type of social bias that is present when a message about immigrants disregards the great diver- sity of this group of people and highlights a small set of their characteristics. This pro- cess of homogenization of a whole group of people is at the very heart of the stereotype concept (Tajfel, Sheikh, and Gardner, 1964). As (Lipmann, 1922) said in his seminal work Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 83-94 do not rst see and then de ne, we de ne rst and then see\". In short, we can say that a stereotype is being used in language when a whole group of people, itself very diverse, is represented by appealing to a against Asian taken recently (Tessler, and 2020). the success anti-immigration par- ties phenomenon (Dennison and Geddes, consequences that prejudices and attitudes, in many cases negative, may have. There have been some works related to the problem of immigrant stereotypes iden- ti cation (Sanguinetti et al., 2018), but they are mainly focused on the expressions of hate speech; or social it is nec- essary to have a whole view of immigrant stereotypes, taking into account both which stereotypes are re ected in texts. This more re ned analysis of stereo- types would make it possible to detect them not in clearly dogmatic or violent mes- sages, but also in other parliamentary debates. Similar to applications of healthcare, se- curity, and social analysis, in this task is not enough to achieve high results, but it is also mandatory that results could be un- derstood or interpreted by human experts on the domain of study (e.g., social psycholo- gists). Taking into account these two as- pects, performance and explainability, the objective of this work compare two ap- proaches opposite to each other in the text classi cation state of the art. On the one hand, a transformer-based model, which has shown outstanding performance, but high complexity and poor explainabil- ity; and, on the other hand, a masking-based model, which requires fewer computational-resources and showed a good performance in pro ling. We aim to BETO nal prediction. With the masking technique (Stamatatos, 2017; S\u0013 anchez-Junquera et al., 2020), it possible to know what are the most important words that the model pre- ferred from Spanish Congress of Deputies. The research questions aim to answer in this work are: RQ1: Is the transformer tions the predictions of the models, to allow human interpretability about the immigrant stereotypes? The rest of the paper is organized as fol- low: Section that we propose. Section 3 de- scribes the two models, and Section 4 the dataset used in the experiments. Sections 5 and 6 contain the experimental settings and the discussion about the results. Finally, we conclude work men- tion future directions. 2 Work 2.1 Detection There have been attempts to study stereo- types from a computational point of view, such as gender, racial, religion, 2018; Boluk- basi al., 2016; Liang et al., 2020). Those works prede ne two opposite categories (e.g., men vs. women) and use word embeddings to detect the words that tend to be more associated with one of the categories than with the other. In (Nadeem, tests a sentence de- the target group and a set of three attributes that correspond to a stereotype, an anti-stereotype, and a neutral option. Sec- ond, the inter-sentence test, anti- stereotypical attribute; and lastly, a neutral sentence. These tests are similar to the idea of (Dev et al., 2020) that consist in using natural religion domains. On the other hand, stereotypes are not al- ways the (explicit) association of words (seen as attributes or characteristics) from two op- posite social groups, like women vs. men in the context of gender bias. Such is the case of immigrant stereotypes, in which sentences like>Por qu\u0013 e ha muerto una persona joven? (Why did a young person die?) do not con- tain an attribute of the immigrant group al- though from its context1it is possible to ering. Also, it is not clear the representative word of the social group, since persona joven (young person) is neutral to immigrants and non-immigrants. Other works have built annotated data to foster the development of supervised ap- proaches. In (Sanguinetti et al., 2018), was presented an Italian corpus focused on hate speech against immigrants, which includes annotations about whether a tweet is a stereotype or not. This corpus was used in the HaSpeeDe shared task at EVALITA 2020 (Sanguinetti et al., 2020). Most partic- ipant teams only adapted their of hate conclusions was that immigra- 2020), it 1Fragment of a political speech from a Popular Parliamentary Group politician in 2006. The speaker is mentioning some of the conditions of immigrants in Spain in that period.was proposed a dataset that includes the do- reli- gion, and profession). Although not the study of stereotypes about immigrants, its authors reported the word \\immigrate\" as one of the most rele- vant keywords that characterized the racism domain. 2.2 On the explainability of AI models Since eXplainable Arti cial Intelligence (XAI) systems have become an integral part of many real-world applications, there is an increasing number of XAI approaches (Islam et al., 2021) including white and black boxes. The rst group, which includes decision trees, hidden Markov models, logistic regressions, and other machine learning algorithms, are inherently explainable; whereas, the second group, which includes deep learning models, are less explainable (Danilevsky et al., 2020). XAI to di erent aspects, for example, (i) by the the level of the explainability, for each single prediction (local explanation ) prediction process as a whole in accor- dance the source of the explanations, for example: (i) surrogate models, in which the model predictions are explained by learning a second model as a proxy, such is the case of LIME (Ribeiro, Singh, and Guestrin, 2016); (ii)example-driven, in which of (labeled) similar Rossini, and 2019); (iii) attention layers, which appeal to human intuition and help to indicate where the neural network model is \\focusing\"; and importance al., 2020). Taking into account this characterization, frame our approach in the self-explaining scope, and consider two di to obtain local explanations of the predicted texts. In this sense, use the attention lay- erswhich have been commonly applied models (Mullenbach et al., 2018; Bodria et al., 2020). Masking and BERT-based Models for Stereotype was based, high- lighting the tokens that the models found the most relevant. Similarly, in (Clark et al., 2019) the authors tasks like dependency parsing, of BERT, and found relevant lin- guistic knowledge in the hidden states and at- tention maps, such as direct objects of whilst are not, their interpretation depends on their context. The other self-explaining model that we use to obtain the local explanations, is a masking technique which can be described as a white box. In this case, the explain- able strategy is based et set of irrelevant words. 3 Models In this section, we brie y describe the two models that we use in our experiments. BETO: it is based on BERT, but it was pre-trained exclusively on a big Spanish dataset (Ca~ nete et al., 2020). The framework of BETO consists of two steps: pre-training and ne-tuning, similar to BERT (Devlin et al., 2018). For the pre-training, the collected data included Wikipedia and other Spanish sources such as United Nations and Govern- ment journals, TED Talks, Subtitles, News Stories among others. has 12 self- attention layers with 16 attention-heads each, and uses 1024 as hidden size, 110M parameters. The vocabulary contains tokens. For rst initial- ized with the pre-trained parameters, and all of the are ne-tuned using la- beled data from the downstream task, which in our case is a stereotype-annotated dataset (see Section 4). The rst token of every se-quence is always a special classi cation token ([CLS]), which representation for classi cation to the representa- tion two dense words are neutral symbol. The irrelevant terms are task-dependent and have to be de- ned advance, following tof text by a sequence of \\*\". The length of the sequence is determined by the number of characters that tcontains. One example of this is 1 the Spanish stopwords. In 5.1, explain which are the relevant words that we considered better to mask. After all texts are distorted by the mask- ing technique, we use a traditional classi er to be compared we use (LR) clas- si er which has been used before to be com- pared with BERT (Alaparthi and texts on immi- grant stereotypes ParlSpeech dataset and Schwalbach, as Non-stereotypes). These are from the speeches of the Spanish Congress of Deputies (Congreso de los Diputados ), and are written in Spanish. In the construction of StereoImmigrants (S\u0013 anchez-Junquera et al., 2021), we proposed new to the study of immigrant stereotyping elaborating a taxonomy to an- notate the corpus that covers the whole spec- trum of beliefs that make up the immigrant stereotype. The novelty of this taxonomy and this annotation process is that the work has not focused on the characteristics attributed to the group but on the narrative contexts Javier S\u00e1nchez-Junquera, Chulvi 86Original text la inmigraci\u0013 on sigue siendo hoy - lo con rman los \u0013 ultimos sondeos del CIS - el principal problema que preocupa a los ciudadanos del estado (Immigration is still today - con rmed by the latest CIS polls - the main problem that worries of the state) Masking stop keeping which the immigrant group is repetitively sit- uated in the public discourses of politicians. To this, the authors applied the frame the- ory {a social psychology theory{ to the study of stereotypes. The frame theory allows us to show that politicians in their speeches cre- ate and recreate di erent frames (Scheufele, 2006), i.e. di erent scenarios, where they place the group. The result of this rhetori- cal activity of framing ends with the creation of a stereotype: a diverse group is seen only with the characteristics of the main actor in a particular scenario. In (S\u0013 anchez-Junquera et al., 2021), to speak about immigrants that could be classi ed in one of the following categories: (i) present the im- migrants as equals to the majority but the target of xenophobia (i.e., they must have the same rights and same duties but are discrim- inated), (ii) as victims (e.g., they are people su ering from poverty or labor exploitation), (iii) as an economic resource (i.e., they are workers that contribute to economic devel- opment), (iv) as a threat for the group (i.e., they are the cause of disorder because they are illegal, too many, and introduce unbal- ances in societies), or (v) as a threat for the individual (i.e., they are competitors for lim- ited resources or a danger to personal wel- fare and safety). In the the StereoImmigrants an expert explic- itly or implicitly, to the people that integrates \\immigrants\". After this expert annotation, ve non-experts sen- tence and decided if they agreed with it of per label and the average length (with standard deviation) their instances. onomy sentences Victims orThreat, (i) and (ii) belong to the Victims supra-category, and (iv) and (v) belong to theThreat supra-category. Table 1 shows the distribution per label of the dataset. Table 2 shows orThreat. From these ex- amples, it is possible to see that the dataset contains stereotypes that are not merely the association of attributes or characteristics to the group, but texts which re ect biased representations of the group (i.e., how the immigrants are indirectly perceived or associated with speci c situations Experimental Nonos vale que se contabilice todo lo que se dedica a inmigraci\u0013 on porque no estamos hablando de lo mismo. (We are not worth accounting for everything that is dedicated to immigration because we are not talking about the same thing.) ElGobierno est\u0013 a desbordado por la inmigraci\u0013 on, por su pol\u0013 \u0010tica improvisada, irresponsable, descoordinada y unilateral. (The e persona joven? (Why did a young person die?) Hay una situaci\u0013 on de desamparo en muchas personas a la que necesitamos dar una soluci\u0013 on. (There is a helplessness situation in many people to which we need to provide a solution.) Stereot yp e: Threat Espa ~ na hoy est\u0013 a desbordada con la inmigraci\u0013 on ilegal. (Spain today is overwhelmed with illegal immigration.) Esta alarmante situaci\u0013 on, agravada por la incapacidad del Gobierno socialista, ha producido el colapso, el desbordamiento de los servicios humanitarios, judiciales y policiales que han generado una gran alarma social. (This alarming situation, aggravated by the incapacity of the collapse, the over ow of humanitarian, judicial and police services that have generated great social alarm.) Table 2: Examples value 0.3 to the last dense layer. We have selected a value of 180 for the max length hyperparameter according to the maximum length of all the texts in the dataset. The model was ne- tuned for 10 epochs on the training data for each task. For the approach, we used the taken by except for the optimization method: we model used detection have found list of words that tend to be associ- ated with one of two opposite social groups (e.g., female vs. male, Asian vs Hispanic people) (Bolukbasi et al., 2016; Garg et al., 2018). In the immigrant stereotypes case, it is particularly di\u000ecult to de ne groups and consequently to nd such biased words. paper, the dataset de-scribed in Section 4 to nd which could be the most relevant terms to be used in the we found masking words high- were masked). In Each Discussion We report the results of the models in Ta- ble 3. It is possible to see high results of LR with the original texts. However, we observe that masking the terms out of the listRelFreq is slightly better than using the original text. suggest ilarly stereotypes: (countries), examples of words included in RelFreq that are indeed re some bias accord- ing to the example, it is not surprising to nd derechos (rights), humanos breza muerto (dead), and ham- associated to immigrants seen as victims; and irregular (ir- regular), ilegal (illegal), regularizaci\u0013 (reg- masiva in speeches where im- semantic and syntactic information, and richer in of the words are taken into account. However, tween europ ea personas canarias derec hos uni\u0013on humanos gobierno problema materia derec ho irregular pa\u0013 \u0010s grupo mujeres ilegales irregular pol \u0010ticas on coop eraci\u0013 on integraci\u0013 on ilegal ilegales gobierno mundo espa~ na humanos moci\u0013 on vida problema ciudadanos europ eo solidaridad proceso irregulares ley asilo masiv a efecto parlamen tario condiciones llegado origen comisi\u0013 on millones aeropuertos centros c\u0013amara muerto ministro ilegal desarrollo refugiados control drama consenso social efecto acogida subcomisi\u0013 on miseria pateras llamada socialista internacional llamada menores com\u0013 un ciudadanos medidas vida temas xenofobia llegada ma as tema hambre inmigran tes llegada asuntos viven marruecos masiv a grupos emigran tes cayucos extranjeros emigran tes muerte presi\u0013on Table 4: of texts they could be focusing on. For this purpose, we looked at the last layer of BETO and computed the average of the attention heads. Therefore, for each text, we had an atten- tion matrix from which we could compute the attention that the transformer gave to each that texts. Figure 2 shows examples of texts where the two models agreed on the right label. From the gure, it is possible to see what words were relevant for both approaches. Al- though some of the relevant words are func- tion words (e.g., para, muy) and are not too informative at rst glance for human inter- pretation, we can observe that some content- related words can be helpful for expert's analysis. Victims desamparo (abandonment), personas (people), necesita- mos dar una soluci\u0013 on (we need to give a so- lution), re ecting how immigrants were seen as people more than their illegal status (e.g., see Tables 4), and the target of problems that need solutions. Moreover, in the example of Threat, some of the words and phrases re- ceiving more importance (such as, problema muy serio, problema muy importante ) re ect how immigrants were seen as a problem These examples were correctly classi ed by both models. The more intense the color, the greater is the weight of attention given by the model. the continent and the country, but not the country where immigrants come from. Table 5 presents some of the words with the highest attention scores in only these Table 6 the percentage of RelFreq words (which were not masked) that were present in the top of the ranking as more discriminative from BETO. In the top 30 of the ranking, we found the vast majority of the not masked words. This suggests that the two approaches have seen similar cues. For now, we have seen that BETO and themasking technique achieved similar in discrimina- tive words they focused on in the texts (which in fact answer RQ2), despite one of them is a resource-hungry model and the other re- quires less computational resources. We do not think that not be used be- cause of complexity: one of the di erences we should highlight is that for the masking technique the list of words should be pre- de ned with some limitations that binary score like masking technique or un- masked). Therefore, we apply from the transformer of the importance of the di erent words (like it was visualized in Figure 2), which legada ilegales efecto irregulares llamada costas expulsiones trabajadores pateras xenofobia ma a condiciones legalidad dinero avalancha vienen peninsula muertes miles humanitaria coladero preocupaconsejo asuntos temas c omparecen producen recibir acuerdos esfuerzo di\u0013 alogo pacto congreso necesidad cumbre proyecto zapatero enmiendas miembros colabora conferencia gobiernos exterior importantes acci\u0013 ondere cho esclavitud mujeres asilo refugiados pobreza xenofobia muerto sistema devoluciones miseria desgracia dif\u0013 \u0010cil grupos racismo hambre refugio persona situaciones explotaci\u0013 on denuncia muerte democraciamasiva pater as llamada avalancha aeropuertos trasladar llegada zapatero caldera alarma ayudas ilegalmente afrontar judiciales capacidad archipi\u0013 elago delincuencia grave the top of words with the highest attention. from the context of the words in the texts. In the next sections, we con rm the advantages of both models by analyzing the results of an ideal ensemble and other utilities of the at- tention mechanisms. 6.2 An Ideal Ensemble We have seen the results achieved by the proposed models and the intersection set of words they focus on in the texts. Therefore, one could think that these models are correctly the same texts. In this sec- tion, that the models are misclas- sifying di erent instances in general. Table 7 classi cation task. The have good performances, so it is licit to think in an ideal ensemble that could wisely combine their predictions. The resultant ensemble will miss only the texts where both models are wrong: 272 texts ed y LR 624 263 misclassi ed b y BETO 518 259 misclassi ed b y both 272 115 predicted ideal will be correctly clas- si ed. A similar analysis can be done in the Victims vs. Threat classi cation task, which will result in 1477 texts that will be the in our dataset is inmigraci\u0013 on (immigration); since we found its scores high in the two op- posite classes, we did not count it as discrim- inative by BETO. However, we think that as the heads have the attention that each word gives to the others in the texts, we can ob- serve how the \\noisy\" words are in the classes, by looking at the relations with their context. We hypothesize that the immigration-related words are used er- ent contexts in the opposite classes. 8 the with inmigraci\u0013 on are the most scored in each class. We omitted the ones in the Non-stereotype class due to they are not informative. Interestingly, that words like criminal, and enfer- medades (diseases) are now in top of the Threat category attention mechanism should be exploited in in this sense. Probably the atten- tion scores could be a source only in terms of biased words from list or the ones e Threat muertes saturado miseria p obres policiales internamiento dramaticos descontrol empresarios humanitario costas avalancha garant\u0013 \u0010as delincuencia tr\u0013 a co devueltos ilegales pateras expulsi\u0013 ondiscriminaci\u0013 on colectiv os mujeres consenso dentro refugiados familias educativo planteamos miseria refugiados pobreza reto mujeres voto voluntad especi cas pobreza iniciativa enmienda pod\u0013 \u0010an sabenllega nuev o delincuencia aeropuertos procedente zapatero saturado policiales retenci\u0013 on madrid enfermedades congreso evoluci\u0013 on entran francia aeropuertos tropicales criminal aeropuerto intentos llegaron coladeroTable 8: Words with are contextualized. 7 Conclusion and Future Work im- of by a traditional classi er. We demonstrate that relevant the masking technique uses. These two di focused portions of the BETO gave the highest attention. Fur-thermore, to highlight some stereotype cues that could be studies about of the reported results, we conclude that both models are e ective at identifying the immigrant stereotypes, and could be combined to build an ideal ensem- ble that overcomes the results of each one. We also point out that BETO can help in- vestigate we cannot rule out the use of either model. To our knowledge, this is the rst work on immigrant stereotypes identi cation that compares deep learning approaches paying special at- tention to the explicability of the models in this task. However, more work is necessary to explore more deeply the advantages of the attention mechanisms in this sense. In future work, we plan to combine the two approaches to increase the performance; and to so- cial media and political speeches. Agradecimientos The of ecnica of Val\u0012 encia Spanish Ministry of Science and Inno- vation under the research project speech (PGC2018-096212- B-C31). Experiments were carried out on the GPU cluster at PRHLT thanks to the PROMETEO/2019/121 (DeepPattern) Mishra. odyssey. Journal of pages 1{9. Bodria, F., A. Panisson, A. Perotti, Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word J., G. Chaperon, R. Fuentes, J.-H. Ho, Kang, and J. P\u0013 erez. 2020. Span- ish pre-trained bert model and evaluation data. In PML4DC at ICLR 2020 . Clark, K., U. Khandelwal, O. Levy, and C. D. Manning. 2019. What does BERT look at? an analysis of BERT's attention. In Proceedings Association for Computational Linguistics. Croce, D., D. Rossini, and R. Basili. 2019. Auditing deep learning processes through kernel-based explanatory models. In Pro- ceedings of the 2019 Conference on Em- pirical Methods in Natural Language Pro- cessing and the 9th International Joint Conference on Natural Language Process- ing (EMNLP-IJCNLP), pages 4037{4046, Aharonov, Y. Katsis, B. Kawas, and P. Sen. 2020. A survey of the state of explainable ai for natural language processing. arXiv preprint arXiv:2010.00711. Dennison, J. and A. the salience the rise of anti-immigration europe. The political quar- terly, 90(1):107{116. Dev, S., T. Li, J. M. Phillips, and V. Sriku- mar. 2020. On measuring and mitigating Chang, K. and K. Toutanova. and W. Van Atteveldt. 2018. Studying muslim stereotyping through mi- croportrait extraction. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018) .Garg, N., L. Schiebinger, 2018. of gender and ethnic of the National Academy of Sciences, 115(16):E3635{ E3644. Granados, A., M. Cebri\u0013 an, D. Camacho, and F. De Borja Rodr\u0013 \u0010guez. 2011. Reduc- ing the loss nealing text distortion. IEEE on Knowledge and Data Engineer- ing, 23(7):1090{1102. cited By 19. Islam, S. R., W. Eberle, S. K. Ghafoor, and M. Ahmed. K. M. Figueroa Mora, J. Anzurez Mar\u0013 \u0010n, J. Cerda, J. A. Carrasco-Ochoa, International Publishing. Liang, P. P., I. M. Li, E. Zheng, Y. C. Lim, R. Salakhutdinov, and Wiegre e, J. Duke, J. Sun, and J. Eisenstein. 2018. Explainable pre- diction of medical codes from clinical text. InProceedings of the 2018 Conference of the North American Chapter of the As- sociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 1101{1111, New Orleans, Louisiana, June. Association for Computational Linguistics. Nadeem, set: Full-text 6.3 million chambers of nine represen- tative democracies. Harvard Dataverse. Ribeiro, M. T., S. Singh, C. Guestrin. 2016. \"why should i trust you?\": Explain- ing the of classi er. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, page 1135{1144, New York, NY, USA. As- sociation for Computing Machinery. Sanguinetti, M., G. Comandini, E. Di Nuovo, S. Frenda, M. A. Stranisci, C. Bosco, C. Tommaso, V. Patti, R. Irene, et al. 2020. Haspeede 2@ evalita2020: Overview of the evalita EVALITA 2020 Seventh tion Campaign Language Italian , pages 1{9. CEUR. Sanguinetti, M., F. Poletto, C. Bosco, V. Patti, and M. Stranisci. 2018. An italian twitter corpus of hate speech against immigrants. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018) . Scheufele, D. A. 2006. Framing as a Theory of Media E ects. Journal S\u0013 anchez-Junquera, J., B. Chulvi, P. Rosso, and S. P. Ponzetto. 2021. How do 135:122{130.Tajfel, and R. C. Gardner. 1964. Content of stereotypes and the in- ference groups. 22(3):191{201. Tessler, H., M. Choi, and G. Kao. 2020. The anxiety of being asian american: Hate crimes and negative biases during the covid-19 pandemic. American Journal of Criminal Justice, S\u00e1nchez-Junquera, Paolo Rosso, Manuel Montes-y-G\u00f3mez, Berta Chulvi 94El sentimient o de las letras de las canciones y su relaci\u00f3n con las caracter\u00edsticas musicales The sentiment of the lyrics of the songs and their relationship w ith the musical characteristics Marco Palomeque , Juan de Lucio marco.palomeque@uah.es , juan.de lucio@uah.es Resumen: El trabajo analiza el sentimiento de las letras de las canciones m\u00e1s exitosas semanalmente durante el periodo 1958 -2020 con el prop\u00f3sito de captar el sentir social a lo largo del tiempo . Observamos que el conjunto de informaci\u00f3n recogido por las letras difiere de aqu\u00e9l que se refiere a la m\u00fasica, lo que nos indica que las letras aportan informaci\u00f3n complementaria a la extra\u00edda a partir de las caracter\u00edsticas musicales. Palabras cla ve: Letra, canci \u00f3n, m\u00fasica , sentimiento. Abstract: The work the sentiment of the lyrics of the most successful songs on a weekly basis during the period 1958 -2020 to capture the social sentiment over time. We observe that the set of information co llected by the lyrics differs from that which refers to music, which indicates that the lyrics provide information that is complementary to that extracted musical characteristics. Keywords: Lyric, song, music , sentiment. 1 Introducci\u00f3n Las letras de las canciones reflejan el sentir de sus oyentes , sus preocupaciones y sus intereses. Por lo tanto, a nivel agregado, las canciones m\u00e1s consumidas reflejan el sentir mayoritario del conjunto de la sociedad. El Billboard Hot 100 es el ranking semanal de las 100 canciones m\u00e1s consumidas en Estados Unidos tanto en ventas de discos , como escuchas en radio y, m\u00e1s recientemente, en fuentes digitales como Youtube o Spotify. El Billboard Hot 100 comenz\u00f3 a elaborarse en 1958. Es el registro m\u00e1s antiguo y constituye una referencia para otras listas de \u00e9xitos. Billboard Hot 100 configura as\u00ed un registro del sentir social a lo largo de l tiempo. Las personas usan la m\u00fasica para aliviar el estr\u00e9s y la ansiedad y para mejorar su bienestar emocional y mental. Park et al. ( 2019 ) y Heggli et al. ( 2021 ) muestran que se producen fluctuaciones en las preferencias musicales a lo largo del d\u00eda y seg\u00fan el d\u00eda de la semana. La m\u00fasica es particularmente efectiva para apoyar el bienestar emocional y regular las emociones durante las fluctuaciones relacionadas con eventos con consecuencias emocionales durante ciertos per\u00edodos de la vida (Saarikallio , 2011), suponiendo un elemento de autorregulaci\u00f3n . Hanser et al. ( 2016 ) muestra que la m\u00fasica es la fuente de consuelo m\u00e1s importante en comparaci\u00f3n con otros comportamientos relajantes ofrec iendo alivio en situaciones de p\u00e9rdida y tristeza. La m\u00fasica , en s\u00ed misma , y la letra, en particular, son los aspectos m\u00e1s importantes de una canci\u00f3n. Las letras de las canciones pueden generar datos que completen y valid en resultados a partir de m\u00e9todos puramente ac\u00fasticos (Mahedero et al., 2005). Mihalcea y Strapparava (2012) proponen una clasificaci\u00f3n de las emociones de 100 canciones anotadas, utilizando la m\u00fasica y las letras de las canciones. Pyrovolakis et al. (2020) muestran que, para l a detecci\u00f3n del estado de \u00e1nimo en el marco de la m\u00fasica, tanto la letra como el audio, contienen informaci\u00f3n \u00fatil. Este trabajo pretende medir el sentimiento en las letras y compararlo con indicadores de las caracter\u00edsticas musicales. El objetivo principal Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 95-102 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalde este art\u00edculo consiste en determinar si el sentir de la canci\u00f3n reflejado en las letras contiene informaci\u00f3n propia o se acomoda perfectamente a las caracter\u00edsticas de la m\u00fasica que le acompa\u00f1a. El resto del trabajo se organiza de la siguiente manera. La pr\u00f3xima secci\u00f3n introduce las t\u00e9cnicas utilizadas . La secci\u00f3n 3 presenta la base de datos construida, mientras que en la secci\u00f3n 4 se ofrece evidencia de que el contenido informativo sobre el sentimiento de las canciones de la le tra y la m\u00fasica no es exactamente el mismo, aunque est\u00e1n relacionados . La secci\u00f3n 5 concluye con algunas reflexiones adicionales. 2 T\u00e9cnicas utilizadas Para realizar el an\u00e1lisis ser\u00e1n necesarias varias t\u00e9cnicas. La primera, el llamado \"web scrapping\", con e l cual extraeremos las letras de las canciones del Billboard Hot 100. La segunda tarea ser\u00e1 unificar el idioma de las canciones por medio de la librer\u00eda para Python de Google Translate. La tercera ser\u00e1 el an\u00e1lisis de sentimiento de cada texto, una t\u00e9cnica proveniente del campo del Procesamiento del Lenguaje Natural (PLN). Este an\u00e1lisis se har\u00e1 a trav\u00e9s de la herramienta conocida como VADER, la cual analiza cada texto en t\u00e9rminos de positividad, negatividad o neutralidad. Tambi\u00e9n se ha descargado un indicado r de \"valencia\", a trav\u00e9s de la API de Spotify. La valencia mide c\u00f3mo de feliz o triste es una canci\u00f3n de acuerdo con sus caracter\u00edsticas musicales (recabadas tambi\u00e9n por Spotify), sin tener en cuenta la letra de la canci\u00f3n . Adem\u00e1s, se han utilizado otras herramientas que permiten analizar la positividad en textos para contrastar que la clasificaci\u00f3n de VADER es correcta. 2.1 Web scrapping Para realizar el an\u00e1lisis de sentimiento de las letras de las canciones m\u00e1s consumidas de cada sema na lo primero que necesitaremos ser\u00e1 construir nuestra base de datos. Como ya hemos explicado en la introducci\u00f3n, el Billboard Hot 100 es un ranking semanal que incluye las 100 canciones m\u00e1s consumidas de cada semana, desde agosto de 1958 hasta hoy en d\u00eda, en Estados Unidos (existen rankings de otros pa\u00edses, pero su periodo temporal es mucho m\u00e1s corto). La medici\u00f3n del consumo de una canci\u00f3n ha variado a lo largo del tiempo, ya que si bien cuando comenz\u00f3 el registro la venta de discos y la emisi\u00f3n en la rad io eran las dos formas m\u00e1s habituales de consumir canciones, actualmente esto ha cambiado, siendo Spotify y YouTube, entre otras plataformas, dos de los medios m\u00e1s comunes para escuchar una canci\u00f3n. De esta forma, para adaptarse a los cambios en el consumo , Billboard ha ido cambiando su forma de medir cada unidad de consumo, dando una ponderaci\u00f3n distinta a cada forma de consumirla, dado que los beneficios econ\u00f3micos que genera son diferentes. As\u00ed mismo, cabe destacar que una canci\u00f3n puede aparecer en difer entes semanas en el ranking, siempre y cuando siga siendo una de las 100 canciones m\u00e1s consumidas. No obstante, existen ciertas reglas para que una canci\u00f3n no se repita un n\u00famero excesivo de veces, endureciendo sus condiciones para llegar al top (normalmen te a partir de la semana 20). A partir de este ranking, obtenemos el nombre de la canci\u00f3n y el de su int\u00e9rprete para todas las canciones que han aparecido en el mismo. Para buscar la letra de las canciones necesitaremos de ambos, dado que existen muchas ca nciones con el mismo nombre, aunque sean distintas. Con esta informaci\u00f3n, dise\u00f1amos nuestro c\u00f3digo de web scrapping, con el cual buscamos la letra en distintas p\u00e1ginas web (Google, AZ Lyrics y Songs Lyrics). Cada web requiere un c\u00f3digo distinto que se adap te a la forma de escribir el enlace web en el que se almacena la letra de cada canci\u00f3n, as\u00ed como para identificar dentro del c\u00f3digo html la parte de texto que corresponde a la letra para poder extraerla , si bien es cierto que en el caso de Google el enlace admite peque\u00f1as variaciones y sigue extrayendo el resultado adecuado . De esta forma obtenemos la letra de la gran mayor\u00eda de las canciones que han llegado hasta el Billboard Hot 100, como veremos en la secci\u00f3n 3. 2.2 Traducci\u00f3n de letras Aunque, trat\u00e1ndose de las canciones m\u00e1s consumidas en Estados Unidos, la gran mayor\u00eda de canciones est\u00e1n en ingl\u00e9s, lo cierto es que no todas lo est\u00e1n. Para realizar el an\u00e1lisis de sentimiento de las letras armonizamos todas ellas al ingl\u00e9s. Para esto, uti lizamos la librer\u00eda de Google Translate para Python. Esta librer\u00eda nos permite : primero, identificar el idioma de cada texto, con lo que encontramos que 143 canciones, un 0. 5% del total, est\u00e1n en idiomas diferentes al ingl\u00e9s. En concreto, contamos con Marco Palomeque, Juan de Lucio 96 119 canciones en castellano, 20 en coreano y 1 en portugu\u00e9s, italiano, franc\u00e9s y baeggu. Para la traducci\u00f3n de las canciones al ingl\u00e9s , utilizamos tambi\u00e9n Google Translate. Aunque las caracter\u00edsticas y connotaciones de los idiomas no son iguales y la traducci\u00f3n autom\u00e1tica no es perfecta , dado que el volumen de letras en otros idiomas es menor y un porcentaje elevado de oyentes estadounidense pueden entender el espa\u00f1ol , entendemos que esta falta de precisi\u00f3n no es preocupa nte en exceso . 2.3 2.3 An\u00e1lisis de sentimiento V ADER El an\u00e1lisis de sentimiento lo llevamos a cabo con VADER (Hutto y Gilbert , 2015 ). VADER utiliza una combinaci\u00f3n de m\u00e9todos cualitativos y cuantitativos para construir una lista de caracter\u00edsticas l\u00e9xicas junto con sus medidas de intensida d de sentimiento asociadas. Luego, combina estas caracter\u00edsticas con algunas reglas que incorporan convenciones gramaticales y sint\u00e1cticas para expresar la intensidad del sentimiento. Esta herramienta se ha utilizado para diferentes prop\u00f3sitos, como analiz ar los sentimientos expresados en Twitter (Elbagir y Yang , 2020 ), o qu\u00e9 tan positivas o negativas son las evaluaciones de los estudiantes sobre la docencia (Newman , 2018 ). En este trabajo estudiaremos qu\u00e9 tan positivas o negativas son las letras de las canciones que han llegado al Billboard Hot 100. VADER proporciona tres coeficientes , con valores comprendidos entre 0 y 1 : uno de positividad, otro de negatividad y otro de neutralidad; sumando 1 entre los tres . Los valores cercanos a 1 indican que el texto est\u00e1 principalmente influenciado por el sentimiento que mide el coeficiente, y los valores cercanos a 0 indican lo contrario. Ejecutamos el an\u00e1lisis de sentimiento para todas las canciones de nuestro conjunto de datos . Los resultados se pueden observar en la secci\u00f3n 3. 2.4 2.4. Caracter\u00edsticas musicales Para analizar si la emotividad que transmiten las canciones a nivel musical coincide con el sentimiento que transmite su letra necesitamos descargar esta informaci\u00f3n de Spotify . Para esto, utilizaremos la valencia de cada canci\u00f3n, se trata de un coeficiente entre 0 y 1 que mide c\u00f3mo de alegre es una canci\u00f3n, teniendo en cuenta caracter\u00edsticas musicales como pueden ser el modo, el ritmo o el tempo. De esta forma, si una canci\u00f3n tiene una valencia de 0.5 se considerar\u00e1 una canci\u00f3n neutra, mientras que si tiene un valor mayor ser\u00e1 considerada alegre y si es menor triste. Tambi\u00e9n consideramos las siguientes caracter\u00edsticas de las comp osiciones musicales: clave, duraci\u00f3n, energ\u00eda y tempo de las canciones. 2.5 2.5. Robustez Para contrastar los resultados obtenidos por VADER, hemos utilizado tres m\u00e9todos alternativos para medir la positividad en textos. En primer lugar , se utiliza el an\u00e1li sis de sentimiento de Textblob (Loria et al , 2014), que funciona de un modo similar a VADER. Textblob entrega un \u00fanico coeficiente, entre -1 y 1, siendo un texto negativo cuanto m\u00e1s cercano a -1 y positivo cuanto m\u00e1s cercano a 1. El segundo m\u00e9todo ha consi stido en calcular el porcentaje de palabras positivas contenidas en cada letra, para lo cual se ha utilizado una lista de 265 palabras positivas en ingl\u00e9s. El tercer m\u00e9todo ha sido la herramienta de an\u00e1lisis de sentimiento de Pytorch, la cual utiliza trans formadores pre -entrenados (Cheng , 2020). Este m\u00e9todo no admite textos con m\u00e1s de 512 tokens , por lo que hemos excluido de este m\u00e9todo las canciones con m\u00e1s tokens . Las correlaciones entre los indicadores extra\u00eddos de las distintas t\u00e9cnicas tienen los signos esperados y validan el uso de VADER, ver figura 1. Figura 1: Coeficientes de correlaci\u00f3n de los indicadores de sentimiento extra\u00eddos por distintas t\u00e9cnicas. 3 Base de datos Una canci\u00f3n estar\u00e1 en la base de datos si se encuentra entre las 100 canciones m\u00e1s consumidas durante una determinada semana de El sentimiento de las letras de las canciones y su relaci\u00f3n con las caracter\u00edsticas musicales 97referencia. Disponemos de informaci\u00f3n desde la primera semana publicada, el 4 de agosto de 1958, hasta la \u00faltima semana de 20 20. Durante las 32 58 semanas que transcurrieron en dicho periodo, 29663 canciones diferentes alcanzaron un puesto en la lista de canciones m\u00e1s demandadas. Una misma canci\u00f3n puede aparecer en el top varias semanas distintas , si bien Billboard endurece las condiciones para permanecer en el ranking cuando se alcanza la semana 20 semana de presencia en el ranking (estas condiciones no son siempre las mismas, actualmente solo se mantienen si est\u00e1n por encima del puesto 50) . Por esta raz\u00f3n, observa mos un escal\u00f3n en la semana 20 en el histograma de la Figura 2 (n\u00f3tese que el gr\u00e1fico muestra el n\u00famero m\u00e1ximo de semanas que estuvo cada canci\u00f3n, por lo que muchas canciones que podr\u00edan haber estado 21 o m\u00e1s semanas se acumulan en la semana 20 ). En promed io, una canci\u00f3n permanece en la lista durante 9 semanas. Figura 2: N\u00famero de canciones en funci\u00f3n de las semanas que se mantiene n en el ranking. A trav\u00e9s del web scrapping, hemos podido recopilar la letra de 2 6250 canciones , un 88.5% del total de canc iones que han aparecido en la lista. El porcentaje medio de canciones que se dispone en cada semana es del 94%. Esto supone que las canciones con mayor permanencia tienen una probabilidad mayor de estar recogidas en la base de datos de letras. El porcentaj e de canciones para las cuales hemos obtenido letra en relaci\u00f3n con el total de letras posibles aumenta cada a\u00f1o, tal y como se puede observar en la Figura 3 en la que se presenta el porcentaje de canciones que contiene de la base de datos tanto para el top 50 como para el top 100 durante el periodo de publicaci\u00f3n de la clasificaci\u00f3n de canciones . Esto se debe a que algunas canciones antiguas son m\u00e1s dif\u00edciles de encontrarse actualmente y no se encuentran en ninguna de las p\u00e1ginas web que hemos utilizado. Otra raz\u00f3n que explica la no identificaci\u00f3n de la letra se deriva de la formulaci\u00f3n espec\u00edfica del t\u00edtulo de la canci\u00f3n o de los artistas responsables de la misma . En el periodo m\u00e1s reciente se observa una mayor frecuencia de canciones con colaboraciones de diversos artistas, lo cual dificulta la obtenci\u00f3n de la letra por medios autom\u00e1ticos . El \u00e9xito de una canci\u00f3n tambi\u00e9n determina la facilidad para encontrar su letra en la web. Si nos ce\u00f1imos al top 50 de cada a\u00f1o, nos encontramos con que el porcentaje obtenido anual nunca baja del 87.5%, mientras que en el caso del top 100 completo hay alg\u00fan a\u00f1o en el que solo obtenemos un 80% del total , ver Figura 3. Figura 3: Porcentaje de canciones obtenidas por a\u00f1o. Para observar de forma m\u00e1s clara el efecto de la posici\u00f3n del ranking en la trascendencia a largo plazo de la canci\u00f3n (considerando que una canci\u00f3n ha sido m\u00e1s transcendente si es f\u00e1cil encontrar su letra en la web), en la Figura 4 vemos c\u00f3mo el porcentaje de canciones para las cuales hemos encontrado letra va disminuyendo de forma clara conforme baja la posici\u00f3n de la canci\u00f3n en el ranking, desde un m\u00e1ximo cercano al 99% para aquellas canciones que han es tado en los primeros puestos . Figura 4: Porcentaje de canciones con letra obtenida por posici\u00f3n. En cuanto a la medici\u00f3n de consumo de letras positivas y negativas, utilizaremos dos m\u00e9todos. Marco Palomeque, Juan de Lucio 98 El primero se trata simplemente del promedio semanal de los c oeficientes positivo y negativo obtenidos mediante VADER para las canciones que se encuentran en el Billboard Hot 100 . Podemos observar la distribuci\u00f3n de estos promedios en la Figura 5, en la que se representa tanto el valor medio por semana y una media m\u00f3vil de anual (52 semanas) . Nos encontramos con que la mayor parte de las letras de las canciones tienen un mensaje principalmente neutral, dado que las suma de los coeficientes positivo y negativo suele estar por debajo de los 0.3 juntos, por lo que el coeficiente neutro de las letras gira entorno al 0.7. Figura 5: Evoluci\u00f3n del sentimiento positivo y negativo por semana y media m\u00f3vil de 52 semanas . En la Figura 5 se observa que el consumo de letras positivas est\u00e1 disminuyendo y el consumo de letras negativas est\u00e1 creciendo , aunque no es una tendencia exenta de fluctuaciones. Otro descriptiv o relevante es el an\u00e1lisis de la positividad / negatividad de las letras en funci\u00f3n de su posici\u00f3n. En la figura 6 se observa que las canciones positivas alcanzan posiciones m\u00e1s elevadas en los rankings . Figura 6: Evoluci\u00f3n del sentimiento positivo y posici\u00f3n en el ranking. Proponemos un segundo m\u00e9todo para medir el consumo de letras positivas y negativas . Consideraremos que una canci\u00f3n es positiva si el coeficiente positivo es superior al doble del negativo, mientras que clasificaremos negativas a aquellas en las que ocurra lo contrario. De esta forma, el valor seman al ahora ser\u00e1 el porcentaje de canciones clasificadas como positivas o negativas respecto al total. En la Figura 7 podemos observar los resultados de esta medida . Vemos c\u00f3mo , de nuevo , las canciones positivas est\u00e1n disminuyendo y las negativas aumentando, aunque ahora se observa de manera m\u00e1s clara la diferencia entre el consumo de canciones positiv as y negati vas, ya que hay a\u00f1os en los que casi el 80% de las canciones tienen un mensaje el doble de positivo que de negativo mientras que las que tienen un men saje el doble de negativo que de positivo apenas superan el 10% en su pico. Figura 7: Evoluci\u00f3n de l porcentaje de canciones positivas y negativas , indicador alternativo . La Figura 8 presenta la positividad de las canciones en relaci\u00f3n con la posici\u00f3n ordinal en la clasificaci\u00f3n. Se observa que las canciones con un coeficiente de positividad m\u00e1s alto llegan a puestos m\u00e1s altos. Figura 8: Coeficiente medio de positividad por posici\u00f3n. A continuaci\u00f3n , en la base de datos introducimos el indicador de va lencia facilitado El sentimiento de las letras de las canciones y su relaci\u00f3n con las caracter\u00edsticas musicales 99por Spotify . Como se dijo anteriormente , la valencia toma valores entre 0 y 1, a mayor valor mayor alegr\u00eda de la canci\u00f3n seg\u00fan sus caracter\u00edsticas musicales. En la Figura 9 observ amos que el valor de valencia est\u00e1 disminuyendo, al igual q ue en el caso de las letras positivas, por lo que la clasificaci\u00f3n musical del estado de \u00e1nimo de Spotify parece coincid ir parcialmente con nuestro an\u00e1lisis de texto ( hay que tener en cuenta que Spotify no usa letras para estimar la valencia). Figura 9: Evoluci\u00f3n de la valencia por semana . De la misma forma, observamos en la Figura 10 c\u00f3mo la valencia es m\u00e1s alta en los puestos m\u00e1s altos del ranking. Figura 10: Coeficiente medio de valencia por posici\u00f3n. Tambi\u00e9n hemos obtenido otras variables musicales, procedentes de Spotify, que ser\u00e1n objeto de l an\u00e1lisis de correlaci\u00f3n con nuest ros coeficientes positivo y negativo. Estas variables: la \"duraci\u00f3n\" (cu\u00e1nto tiempo dura la canci\u00f3n, en milisegundos), el \"tempo\" ( la velocidad a la que est\u00e1 tocada la canci\u00f3n, medido en beats -per- minute , BPM), la \"clave\" (indica la tonalidad en la que est \u00e1 compuesta la canci\u00f3n, tomando el valor \"1\" si est\u00e1 en modo mayor y \"0\" si est\u00e1 en modo menor) y la \"energ\u00eda\" (un coeficiente entre 0 y 1 que mide la intensidad de la canci\u00f3n, tomando valores altos las canciones r\u00e1pidas y ruidosas). La evoluci\u00f3n temporal de estos indicadores se presenta en la Figura 11. Figura 11: Otras caracter\u00edsticas ac\u00fasticas de las canciones . 4 Relaciones entre indicadores En la Figura 12 se pueden observar las correlaciones entre las variables musicales procedentes de Spotify y los coeficientes positivo y negativo calculados por VADER para cada canci\u00f3n . La primera observaci\u00f3n es que la correlaci\u00f3n entre el coeficiente positivo y la valenc ia es positiva, mientras que entre el negativo y la valencia es negativo, siendo ambas significativas al 1%. Esto es una demonstraci\u00f3n de que, de acuerdo con lo que Spotify considera una canci\u00f3n positiva seg\u00fan sus caracter\u00edsticas musicales y lo que VADER c onsidera como un texto positivo, los compositores tienden a componer m\u00fasica positiva para acompa\u00f1ar a letras positivas. Esta afirmaci\u00f3n, aunque pueda parecer obvia, hasta donde conocemos, es la primera vez que se analiza cuantitativamente. Es un primer pas o para realizar an\u00e1lisis basados en el comportamiento del consumidor de m\u00fasica y de qu\u00e9 contextos provocan que prefieran consumir canciones que expresan un sentimiento u otro, pudiendo analizarse de manera independiente tanto la letra como la m\u00fasica. Marco Palomeque, Juan de Lucio 100 Figura 12: Coeficientes de correlaci\u00f3n por canci\u00f3n entre las distintas variables . Teniendo en cuenta el resto de las variables , podemos destacar las siguientes correlaciones entre las letras positivas y negativas y las caracter\u00edsticas musicales: Las canciones positivas son significativamente m\u00e1s cortas (correlaci\u00f3n del -4.2%) . Podr\u00eda estar reflejando que la expres i\u00f3n musical de problemas requiere de m\u00e1s tiempo que transmitir ideas alegres . Por su parte la valencia tambi\u00e9n tiene una correlaci\u00f3n negativ a con la duraci\u00f3n (correlaci\u00f3n del -14%). La energ\u00eda se relaciona de manera clara con la valencia (correlaci\u00f3n del 3 5%). Sin embargo, l as canciones negativas son algo m\u00e1s en\u00e9rgicas (correlaci\u00f3n del 2 .6%) que las positivas (correlaci\u00f3n de l -12%) que parece n ser algo m\u00e1s relajadas . Las canciones en\u00e9rgicas son canciones m\u00e1s ruidosas, por lo que pueden ser canciones m\u00e1s enfocadas a la protesta o a g\u00e9neros que tratan tem\u00e1ticas m\u00e1s pesimistas u oscuras como el heavy metal. Que una canci\u00f3n est\u00e9 compuesta en una tonalidad mayor o menor no indica que su mensaje sea positivo o negativo (correlaci\u00f3n nula) . La relaci\u00f3n tampoco es significativa con la valencia, por lo que es algo que coincide tanto a nivel de letra como musical. El tempo es algo m\u00e1s lento en cancione s positivas, pero sobre todo el tempo se incrementa con la energ\u00eda de la canci\u00f3n . Esto se puede deber a que un mensaje positivo casa con m\u00fasica relajada, mientras que uno negativo puede requerir de m\u00e1s potencia. 5 Conclusiones El an\u00e1lisis de las letras de l as canciones contiene informaci\u00f3n sobre las preferencias de sus oyentes. A escala agregada refleja las preferencias musicales de la sociedad y de su estado de \u00e1nimo. Los rankings musicales, como el Billboard Hot 100, acumulan el sentir social reflejado en la m\u00fasica , que se recoge tanto en las caracter\u00edsticas musicales como en las letras que incorporan. Las t\u00e9cnicas de an\u00e1lisis de texto nos han permitido construir un indicador del sentimiento positivo / negativo de las canciones. Con \u00e9l, hemos podido comprobar c\u00f3mo el consumo de mensajes positivos en las canciones est\u00e1 decayendo con el paso de los a\u00f1os, lo cual es la primera conclusi\u00f3n del estudio. Este indicador de sentimiento se correlaciona con indicadores construidos a parti r de las caracter\u00edsticas musicales de las canciones como es el indicador de valencia de Spotify. Esta correlaci\u00f3n nos indica una concordancia limitada entre la m\u00fasica y la letra . Aunque se observa una relaci\u00f3n positiva con las letras positivas y negativa c on las negativas , los coeficientes de correlaci\u00f3n son reducidos . Tambi\u00e9n hemos mostrado la existencia de una relaci\u00f3n entre el sentimiento expresado en las letras con otras caracter\u00edsticas musicales. Los resultados muestran que el an\u00e1lisis de los textos de las canciones proporciona indicadores adicionales a los que ya se elaboran enfoc\u00e1ndose en las caracter\u00edsticas de la m\u00fasica . Este tipo de an\u00e1lisis puede ayudar, por ejemplo, a los sistemas de recomendaci\u00f3n . Por \u00faltimo, el objetivo de este estudio es servir de primer paso para estudios futuros. Ahora sabemos que el consumo de canciones con mensajes positivos est\u00e1 disminuyendo, pero nos falta un por qu\u00e9. Es aqu\u00ed donde entran estudios centrados en el bienestar del individuo basado en la situaci\u00f3n socioecon\u00f3 mica general, ya que este cambio en el consumo musical es agregado. Esperamos pues que la implementaci\u00f3n de herramientas de NLP en aspectos como la industria musical pueda no solo El sentimiento de las letras de las canciones y su relaci\u00f3n con las caracter\u00edsticas musicales 101ayudar a la propia industria musical o a la investigaci\u00f3n centrada en el len guaje, sino tambi\u00e9n a otros campos como pueden ser la psicolog\u00eda o la econom\u00eda. Agradecimientos Los autores agradece n la financiaci\u00f3n recibida por la Comunidad de Madrid y la UAH (ref: EPU -INV/2020/006) . Bibliograf\u00eda Cheng, Pytorch Towards J. . 2020 . Sentiment Analysis on Twitter with Python's Natural Language Toolkit VADER Sentiment Analyzer. En Iaeng Transactions on Engineering Sciences: Special Issue For The International Association Of Engineers Conferences 2019 (p. 63). World Scientific. Hanser, W. E., T. F. ter Bogt, A. J. Van den Tol, R. E. Mark y A. J. Vingerhoets . 2016. in Vader: A parsimonious rule -based model for sentiment analysis of media text. Conference: Proceedings of the Eighth International AAAI Conference on Weblogs and Social Media . Loria, S., P. Keen, M. Honnibal, R. Yankovsky, D. Karesh, y E. Dempsey. 2014. Secondary TextBlob: simplified J. P., Mart \u00ednez, P. Cano, M. Koppenberger, y F. Gouyon. 2005. Natural language processing of lyrics. En Proceedings of the 13th annual ACM international conference on Multimedia , pp. 475-478. Mihalcea, R., y C. Strapparava. 2012. Lyrics, music, and emotions. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , pp. 590 -599. Newman, H. y D. Joyner. 2018. Sentiment analysis of student evaluations of teaching. En Lecture Notes in Computer Science, pages 246-250. Springer International Publishing. Park, M., J. Thom, S. Mennicken, H. Cramer, y M. Macy. 2019. Global music streaming data reveal diurnal and seasonal 307 -327. Marco Palomeque, Juan de Lucio 102 Reconocimiento y clasificaci\u00f3n de entidades n ombradas en textos legales en espa\u00f1ol Named Entities Recognition and in Spanish Legal Texts Doaa Samy Cairo University , Giza, Egypt Instituto de Ingenier\u00eda del Conocimie nto (IIC), Madrid, Spain doaasamy@cu.edu.eg Resumen: El reconocimiento y la clasificaci\u00f3n de las entidades nombradas (NER/NERC) es una tarea principal en las \u00e1reas del Procesamiento del Lenguaje Natural (PLN) y la Extracci\u00f3n de la Informaci\u00f3n. El papel de NERC en el dominio legal es imprescindible en el desarrollo de sistemas legales inteligentes. El presente trabajo pretende dar un primer paso hacia establecer un \"baseline\" para la tarea NERC en el espa\u00f1ol jur\u00eddico. El objetivo principal consiste en proporcionar un recurso ling\u00fc\u00eds tico anotando cinco tipos b\u00e1sicos de entidades nombradas en los textos legislativos en espa\u00f1ol peninsular. Los cinco tipos de entidades nombradas son: Personas, Organizaciones, Lugares, Fechas absolutas y Referencias a leyes, decretos, \u00f3rdenes, normativas y art\u00edculos. Se adopta una metodolog\u00eda h\u00edbrida que re\u00fane tres t\u00e9cnicas principales: Patrones de expresiones regulares, listas de fuentes externas y el entrenamiento de tres modelos NERC utili zando la librer\u00eda abierta spaCy v3. De los tres modelos entrenado s, el mejor ha obtenido un f -score de 0.93 alcanzando en algunos tipos como las menciones a leyes o fechas valores de 0.9 8 y 0.97 respectivamente . El peor de los modelos ha alcanzado una media de f-score de 0.85 que sigue siendo un resultado satisfactorio comparado con el estado de la cuesti\u00f3n . Palabras clave: Entidades Nombradas , Procesamiento de t extos legales, Procesamiento del espa\u00f1ol jur\u00eddico , Extracci\u00f3n de la informaci\u00f3n en textos legales. Abstract: Named Entity Recognition and Classification (NER/NER C) is a major task in Natural Language Processing (NLP) and Information Extraction (IE). In the legal domain, NERC is indispensable in developing legal intelligent systems . This study pretends to take a first step towards a baseline for Spanish NERC in the legal domain. The main objective is to provide a linguistic resource by regulations, etc. To achieve this goal, by and training of three NERC model s using the architecture of spaCy . The best model achieved a general f -score of 0.93 with some types of entities such as Legal entities and Dates reaching and 0.97 respectively. worst model achieved a general f -score of 0.85, which is still satisfactory given the state of the art. Keywords: Named Entities Legal Text Processing , Information Extraction in Legal Texts, Spanish Legal Text Processing. 1 Introducci\u00f3 n El reconocimiento y la clasificaci\u00f3n de las entidades nombradas (en adelante NER/NERC por las siglas en ingl\u00e9s: Named Entity Recoginition and Classification ) es una tarea principal en las \u00e1reas del Procesamiento del Lenguaje Natural (PLN) y la Extracci\u00f3n de la Informaci\u00f3n. El t\u00e9rmino de entidades nombradas fue acu\u00f1ado por primera vez en la serie de los congresos MUC ( Message Understanding Conference ) en el a\u00f1o 1995 para referirse al proceso de extraer unidades relevantes de informaci\u00f3n a partir de textos no - Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 103-114 \u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural estructurados (Sekine, 2004) (Nadeau y Sekine, 2007) . Estas unidades incluyen nombres propios de personas, organiz aciones, lugares o expresiones num\u00e9ricas como fechas o cantidades , etc. Dada su relevancia en el an\u00e1lisis sem\u00e1ntico, la tarea NERC se ha convertido en una piedra angular para aplicaciones inteligentes como los sistemas de Pregunta -Respuesta (QA), la gener aci\u00f3n de res\u00famenes autom\u00e1ticos, la mejora de los sistemas de recuperaci\u00f3n de la informaci\u00f3n, la traducci\u00f3n autom\u00e1tica, la anonimizaci\u00f3n de textos, la generaci\u00f3n de grafos de conocimientos, etc. En cuanto a las metodolog\u00edas y t\u00e9cnicas, los m\u00e9todos empleados para abordar la tarea de NERC han ido desarrollando desde modelos basados en reglas con patrones de expresiones regulares, listas o gazetteers hacia modelos de aprendizaje autom\u00e1tico supervisado y semi - supervisado como Hidden Markov Models (HMM), Support Vector Machine (SVM) y Conditional Random Field (CRF) siendo este \u00faltimo de los m\u00e1s eficientes en NERC (Roy, 2021). En los \u00faltimos a\u00f1os, el uso de las redes neuronales con el aprendizaje profundo y la integraci\u00f3n de modelos del lenguaje con los WordEmbeddi ngs ha supuesto un cambio en el paradigma del PLN en general y en las tareas espec\u00edficas como NERC (Roy, 2021). El papel de NERC es imprescindible en el desarrollo d e sistemas legales inteligentes . Dado el gran volumen de textos que se suele manejar en est e dominio , ha surgido un inter\u00e9s , cada vez mayor , por el procesamiento de textos legales , en general y por la tarea NERC, en particular. Este inter\u00e9s se fundamenta en el gran potencial de las t\u00e9cnicas de PLN y su capacidad de ofrecer soluciones inteligent es que benefici en a usuarios claves del sector como los abogados, los jueces, los juristas, los documentalistas jur\u00eddicos, adem\u00e1s del sector de la adminis traci\u00f3n p\u00fablica que, aunque no trate textos estrictamente jur\u00eddicos, s\u00ed maneja textos admin istrativos con un alto contenido legal como es el caso de la contrataci\u00f3n p\u00fablica o los convenios. Por tanto, los avances en el procesamiento de textos legales constituyen un gran potencial para agilizar procesos internos de la administraci\u00f3n p\u00fablica, simplificar lo s procedimientos y mejorar el acceso de la ciudadan\u00eda a la informaci\u00f3n legal y administrativa. Para impulsar la apertura de datos p\u00fablicos, la transformaci\u00f3n digital inteligente y la agilizaci\u00f3n d e procesos administrativos, legales y judiciales, existen iniciativas y programas a nivel europeo como el portal de e-Justice . Adem\u00e1s, el programa de Europa Digital \" Digital Europe Programme \" pone \u00e9nfasis en el papel de la inteligencia artificial en la administraci\u00f3n p\u00fablica para mejor ar la interacci\u00f3n digital entre ciudadanos y administraci\u00f3n p\u00fablica. Son numerosas las soluciones inteligentes que puede ofrecer el PLN , en general, y la tarea NERC , en particular, al \u00e1mbito legal y administrativo. Identificar sentencias parecidas para fundamentar un caso , enlazar documentos a trav\u00e9s de las entidades, construir grafos de documentos, anonimizar datos personales o genera r una l\u00ednea temporal de las leyes y los hechos en un documento legal son solo algunos ejemplos de c\u00f3mo el PLN y NERC pueden asistir tanto a abogados, juristas como a jueces en realizar sus tareas . Pese a las oportunidades que supone el dominio legal, son pocos los estudio s, recursos y herramientas de PLN en este dominio, sobre todo en espa\u00f1ol . Sin embargo, en los \u00faltimos a\u00f1os, han aparecido algunas iniciativas . En diciembre de 2019 y c on el fin de impulsar el desarrollo de recursos y herramientas de PLN en el dominio legal en espa\u00f1ol, catal\u00e1n, vasco y gallego, se organiz\u00f3 la jornada \"IberLegal\" dentro del marco de las a ctividades del Plan espa\u00f1ol de Tecnolog\u00edas del L enguaje. Las actas de la jornada ofrecen un abanico de temas de inter\u00e9s como la extracci\u00f3n de terminolog\u00eda legal, b\u00fasquedas inteligen tes en documentos y recuperaci\u00f3n de informaci\u00f3n legal, herramientas para asistir a la ciudadan\u00eda en la redacci\u00f3n de textos para la administraci\u00f3n p\u00fablica y, por \u00faltimo, expresione s temporales en textos legales (PlanTL, 2019). Otra iniciativa es el corpus Legal -ES (Samy et al. 2020) , considerado como un meta corpus que re\u00fane varias fuentes del dominio en lengua espa\u00f1ola con m\u00e1s de dos mil millones de palabras recopiladas a partir de fuentes de datos abiertos espa\u00f1ol as, europeas, hispanoamericanas e internacionales. Estas fuentes representan una variedad de textos jur\u00eddicos que incluyen textos legislativos, jurisprudenciales ( sentencias ) y textos administrativos. Adem\u00e1s, el estudio presenta resultados preliminares so bre c\u00e1lculos de Embeddings del espa\u00f1ol jur\u00eddico y un modelo Doaa Samy 104 de t\u00f3pico entrenado sobre el conjunto de legislaci\u00f3n. No obstante, los trabajos en esta \u00e1rea se enfrentan con retos como: 1) El n\u00famero limitado de recursos y herramientas de PLN adaptados al domin io en general; 2) La predominancia del ingl\u00e9s, ya que la mayor\u00eda de los recursos y las herramientas disponibles se desarrollan para el tratamiento de textos en ingl\u00e9s; 3) Una adopci\u00f3n ralentizada de las tecnolog\u00edas inteligentes en el sector legal y adminis trativo en comparaci\u00f3n con otros sectores como el sector biom\u00e9dico o financiero. Estos retos han influido en que la consolidaci\u00f3n de la tarea NERC en el dominio legal ha tardado unos a\u00f1os en comparaci\u00f3n con otros dominios. De ah\u00ed, el presente estudio prete nde afrontar la tarea en los textos legales espa\u00f1oles teniendo como objetivo principal el reconocimiento y la clasificaci\u00f3n de cinco tipos b\u00e1sicos de entidades nombradas en textos legislativos espa\u00f1oles. El trabaj o se estructura en ocho secciones adem\u00e1s d e la introducci\u00f3n y las conclusiones. Las primeras secciones presentan un enfoque te\u00f3rico con an\u00e1lisis del estado de la cuesti\u00f3n de la tarea de NERC legal , en general y la NERC legal en espa\u00f1ol. El resto de las secciones se centra n en aspectos pr\u00e1cticos do nde se describe el trabajo y los experimentos realizados detallando el alcance, la metodolog\u00eda, los datos utilizado s y las fases del estudio desde la extracci\u00f3n de los datos , pasando por la pre - anotaci\u00f3n, la validaci\u00f3n, el entrenamiento hasta la evaluaci\u00f3n y la visualizaci\u00f3n final de los resultados obtenidos por los modelos entrenados . 2 Estado de l arte : NERC en el dominio legal Para ofrecer una visi\u00f3n panor\u00e1mica acerca del desarrollo de los estudios de NERC en el dominio legal, subrayamos algunas inic iativas y estudios en esta \u00e1rea teniendo en cuenta tres criterios: a) La lengua objeto de an\u00e1lisis, b) el tipo de texto legal (textos legislativos, textos jurisprudenciales (sentencias), resoluciones, contratos, convenios, registros legales, etc .) y c) la evoluci \u00f3n de las t\u00e9cnicas . En primer lugar, destacamos los estudios que han tratado el tema en otras lenguas y , en segundo lugar, nos centramos en los estudios que han abordado la tarea en espa\u00f1ol. 2.1 NERC de textos legal es en otras lenguas El a\u00f1o 2006 marca la cel ebraci\u00f3n de la primera tarea de evaluaci\u00f3n dedicada a la recuperaci\u00f3n de textos en el dominio legal \" TREC Legal\", organizada por el Instituto Estadounidense de Est\u00e1ndares y Tecnolog\u00eda (NIST ) (Cormack et al., 2010 ). Desde esa fecha, e l inter\u00e9s por las entidades nombradas en los textos legales ha seguido cobrando mayor importancia por su relevancia en la extracci\u00f3n y la recuperaci\u00f3n de la informaci\u00f3n . En 2010 se public\u00f3 el volumen titulado \"Semantic Processing of Legal Texts\" (Francesconi et al., 2010) incluyendo uno de los estudios pioneros sobre la identificaci\u00f3n y resoluci\u00f3n de las entidades nombradas en textos legales en ingl\u00e9s (Dozier et al., 2010). En cuanto a t\u00e9cnicas, en esos a\u00f1os dominaban los modelos de aprendizaje autom\u00e1tico cl\u00e1sico y se combinab an con reglas. Dozier et al. (2010) empleaban reglas, listas y modelos estad\u00edsticos (SVM) para reconocer jueces, abogados, empresas, tribunales y \u00e1reas de jurisdicci\u00f3n alcanzando valores de f-score por encima del 0 ,90. El estudio se centra en textos de jur isprudencia basada en casos ( case law ), deposiciones/declaraciones, juicios y alegatos. Siguiendo la misma aproximaci\u00f3n , Quaresma y Gon\u00e7alves (2010) propon en utilizar rasgos sint\u00e1cticos combinando el uso de un analizador sint\u00e1ctico (parser) con un modelo S VM para reconocer nombres, lugares, fechas y referencias a documentos en textos de convenios internacionales del corpus europeo Eur-lex en cuatro lenguas : ingl\u00e9s, alem\u00e1n, portugu\u00e9s e italiano . Por otro lado, en esos a\u00f1os, se observa el auge de las ontolog\u00ed as y los datos enlazadas. Landthaleret al. , (2016) generan redes de grafos de textos legislativos del c\u00f3digo civil alem\u00e1n bas\u00e1ndose en entidades nombradas . Cordellini et al. (2017) extraen las entidades nombradas a partir de un conjunto de textos formados por juicios de la Corte Europea de Derechos Humanos y la Wikipedia en ingl\u00e9s con el fin de enriquecer una ontolog\u00eda. Los modelos de aprendizaje autom\u00e1tico cl\u00e1sico y las aproximaciones h\u00edbridas siguen dominando el panorama en el tratamiento de textos legal es, Chalkidis et al., 2017 aplican clasificadores lineales ( Logistic Regression y Reconocimiento y clasificaci\u00f3n de entidades nombradas en textos legalesen espa\u00f1ol 105 SVM) junto a reglas para el reconocimiento de entidades en contratos en ingl\u00e9s . Asimismo, Glaser et al. (2018) integra n diferentes aproximaciones para el reconocimiento y la desambiguaci\u00f3n de entidades nombradas en contratos en alem\u00e1n . Por \u00faltimo, Andrew y Tannier (2018) combinan un modelo estad\u00edstico (CRF) y reglas ling\u00fc\u00edsticas para identificar entidades nombradas y enlazarlas en textos de registros legal es en franc\u00e9s . El paso de los modelos estad\u00edsticos y el aprendizaje autom\u00e1tico cl\u00e1sico al aprendizaje profundo fue inminente en el panorama del PLN . Por eso, siguiendo e sta l\u00ednea, Chalkidis and Androutsopoulos (2017) aplican modelos de aprendizaje profundo utilizando un model o de BiLSTM ( Bidirectional Short Term Memory ) para identificar y clasificar 11 tipos de elementos en contratos en ingl\u00e9s obteniendo resultados que superan los estudios anteriores que aplicaban SVM. Los modelos de aprendizaje profundo ya se consolidan y superan los resultados de los modelos cl\u00e1sicos. Leitner et al. (2019) proponen una metodolog\u00eda para el reconocimiento de un total de diecinueve tipos de entidades nombradas agrupadas en siete clases generales. La tarea NERC fue aplicada a un conjunto de sente ncias alemanas con Conditional Random Fields (CRFs) y BiLSTMS . Los modelos de BiLSTM han alcanzado mejores resultados con un f-score de 0,9546 para el conjunto de 19 tipos y 0.9595 para el conju nto de las 7 clases generales. 2.2 NERC de textos legal es en esp a\u00f1ol Son pocos los estudios que han abordado la tarea NERC en el dominio legal en espa\u00f1ol. Sin embrago, u no de los trabajo s pioneros es el estudio de Mart\u00ednez -Gonz\u00e1lez et al. (2005) que tiene com o objetivo automatizar la extracci\u00f3n de referencias en textos legales, su resoluci\u00f3n y su indexaci\u00f3n mediante reglas y gram\u00e1ticas para mejorar la recuperaci\u00f3n de la informaci\u00f3n en este dominio. El estudio se ha realiz ado sobre colecciones de documentos de una editorial de textos legales, pero no queda claro qu\u00e9 tipo de documentos son si son legislativos o de otra categor\u00eda. Pasan muchos a\u00f1os hasta que empiece n a aparecer otros estudios como Badji (2018) , en el que se presenta una aproximaci\u00f3n basada en reglas y patrones para reconocer y enlazar entidades legales (re ferencias a leyes, decretos, sentencias, cortes o fases judiciales) en fuentes legislativas espa\u00f1olas y fuentes no oficiales como las noticias y redes sociales donde aparecen con otras denominaciones populares (Ej. El caso de la \"Ley Cela\u00e1\") . En la misma l\u00ednea, Rodr\u00edguez -Doncel et al. (2018) han transformado un conjunto de la legislaci\u00f3n espa\u00f1ola en \"Linked Data\" en formato RDF bas\u00e1ndose en identificar entidades ( leyes, organismos/empresas y lugares ) en los textos del Bolet\u00edn Oficial del Estado (BOE) y enl azarlas con fuentes externas como la Wikipedia, Wikidata, el Regist ro Europeo de Autoridades, el Directorio Com\u00fan de Unidades Org\u00e1nicas y Oficinas espa\u00f1olas o la base terminol\u00f3gica europea Eurovoc. Por otro lado , Haag (2019) adopta una metodolog\u00eda h\u00edbrida combinando modelos estad\u00edsticos y reglas para el reconocimiento de las entidades legales en las fuentes legislativas argentinas utilizando el corpus d e legislaci\u00f3n argentina InfoLEG, en el que se recogen los textos del Bolet\u00edn Oficial de la Rep\u00fablica Argen tina. Por \u00faltimo, Navas -Loro ( 2020) y (Navas - Loro y Rodr\u00edguez -Doncel, 2019) , bas\u00e1ndose en reglas y gram\u00e1ticas, han abordado las expresiones temporales en textos legales desarrollando \"TimeLex\" para la detecci\u00f3n , la normalizaci\u00f3n en TimeML y la resoluci\u00f3n d e las expresiones temporales en textos legales . Tras este an\u00e1lisis del estado del arte, se puede observar la relevancia de la tarea NERC en el dominio legal y la evoluci\u00f3n de las t\u00e9cnicas empleadas a lo largo de los a\u00f1os . Sin embargo, los esfuerzos y los r ecursos desarrollados en espa\u00f1ol se centran en tipos concretos de entidades nombradas como las referencias a leyes o las expresiones temporales desde una perspectiva vertical enfocada en unas entidades espec\u00edficas . La \u00fanica excepci\u00f3n, a este respecto, es el estudio sobre la conversi\u00f3n de la legislaci\u00f3n espa\u00f1ola a \"Linked Data\" (Rodr\u00edguez -Doncel et al., 2018) donde se ha tratado varios tipos de entidades en los textos legislativos. No obstante, las entidades nombradas no constituyen el objetivo principal, da do que el enfoque se centra en enlazar los datos. Partiendo de estas observaciones , se echa en falta una aproximaci\u00f3n transversal para las entidades nombradas en el dominio legal espa\u00f1ol que aborda la tarea a gran escala. Este planteamiento integral es im prescindible como Doaa Samy 106 punto de partida para establecer un baseline de la tarea NERC en el espa\u00f1ol jur\u00eddico. Adem\u00e1s, ayudar\u00e1 a analizar aspectos principales como: los tipos de entidades m\u00e1s frecuentes, su distribuci\u00f3n a trav\u00e9s de los distintos tipos de texto le gal, por ejemplo , entre textos legislativos y textos jurisprudenciales, etc. De ah\u00ed, el presente trabajo pretende dar este primer paso hacia la tarea NERC en el espa\u00f1ol jur\u00eddico . No obstante, para conseguir un baseline objetivo es necesario unir esfuerzos y contrastar aproximaciones mediante tareas de evaluaci\u00f3n (nuestro pr\u00f3ximo objetivo) o iniciativas y proyectos interdiscipl inarios e inter-institucionales como el proyecto europeo LYNX (Rehm et al., 2019) o los recursos desarrollados por Stanford Codex \"Stanford Center for Legal Informatics \" (Waltl y Vogl, 2018) (Rios, 2019) . 3 Alcance y metodolog\u00eda El objetivo de este estudio es ano tar cinco tipos b\u00e1sicos de entidades n ombradas en los textos legislativos en espa\u00f1ol peninsular. Los cinco tipos de entidades nombradas son: Personas Organizaciones Lugares Referencias a leyes, decretos, \u00f3rdenes, normativas y art\u00edculos . Fechas absolutas entendidas como expresiones temporale s referentes a fechas y d\u00edas concretos como por ejemplo \"el 3 de mayo de 2021\" . Como primer acercamiento a la anotaci\u00f3n de NE en el \u00e1mbito legal, se ha decantado por las categor\u00edas b\u00e1sicas de NE (Organizaciones , Personas y Lugares) junto a dos categor\u00edas relevantes del dominio legal ; las fechas y las referencias a leyes. La selecci\u00f3n de las dos \u00faltimas categor\u00edas ha teniendo en cuenta tres criterios: 1) La utilidad de cara a futuras aplicaciones (Ej. Enlazar las leyes, las l\u00edneas temporales, detecci\u00f3n de pe riodos de vigencia, etc.); 2) La alta frecuencia de estos tipos de entidades en el dominio legal como se demostrar\u00e1 en secci\u00f3n 5 y, por \u00faltimo, 3) La naturaleza de los textos legales donde las referencias a leyes, art\u00edculos, etc. es el rasgo distintivo, po r excelencia del dominio objeto de estudio . No se han ampliado las categor\u00edas a cantidades u otras categor\u00edas porque este estudio se considera como un primer paso y para futuros trabajos, s\u00ed se valora incluir nuevos tipos de entidades. El previo an\u00e1lisis d el estado del arte demuestra que los mejores resultados en cuanto a acierto y cobertura , se han logrado combinando diferentes estrategias para afrontar la tarea de anotaci\u00f3n, especialmente si se trata de distintos tipos de entidades. Por este motivo, hemos optado por una metodolog\u00eda h\u00edbrida que re\u00fane tres t\u00e9cnicas principales y que emplea las \u00faltimas t\u00e9cnicas en PLN y en el tratamiento de textos legales : Patrones de expresiones regulares Listas de fuentes externas Entrenamiento de tres modelo s NERC utilizan do la librer\u00eda abierta spaCy, v3 El trabajo se estructura en seis fases principales: Extracci\u00f3n de datos, p re- anotaci\u00f3n , validaci\u00f3n parcial , entrenamiento , evaluaci\u00f3n y visualizaci\u00f3n Figura 1:Estructura del trabajo . 4 Datos Para el presente estudio , se ha utilizado el conjunto de la legislaci\u00f3n del BOE incluido junto a otros conjuntos en el meta -corpus Legal -ES (Samy et al., 2020) . Cabe se\u00f1alar que el portal del BOE incluye m\u00e1s conjuntos como anuncios o c\u00f3digo electr\u00f3nicos, etc. Sin embargo, aqu\u00ed nos limitamos al conjunto de Legislaci\u00f3n. 4.1 Estructura del conjunto El conjunto consiste en un total de 21587 0 ficheros legislativos desde el a\u00f1o 1 661 hasta septiembre de 2019. Los ficheros est\u00e1n en formato XML seg\u00fan el est\u00e1ndar de ELI (European Lesgislation Identifier ). Al analizar la estructura y la nomenclatura del conjunto, se han identificado las siguientes categor\u00edas: Reconocimiento y clasificaci\u00f3n de entidades nombradas en textos legalesen espa\u00f1ol 107 Categor\u00eda N\u00ba fichero s E xplicaci\u00f3n Leyes -Decretos BOE-T 1135 Sentencias constitucionales DOUE 69448 Diario Oficial de la Uni\u00f3n Europea - Legislaci\u00f3n , Decisiones, Recomendaciones y otros Algunos boletines de Comunidades Aut\u00f3nomas 688 Algunos conjuntos de diarios oficiales de Comunidades Aut\u00f3nomas Tabla 1. Categor\u00edas de documentos en el BOE legislativo . Para esta fase , se ha decidido limitarse a los subconjunto s BOE -A y DOUE porque son los mayor es conjunto s y, por tanto, son m\u00e1s representativo s. Se han descartado los subconjuntos de boletines oficiales de comunidades aut\u00f3nomas porque pueden contener lengua s oficiales que no est\u00e1n en el alcance del presente estudio. Los conjuntos del DOUE emplean diferentes normas a la hora de referenciar leyes o directivas , lo cual implica incluir distintos patrones para el reconocimiento de estas entidades . A continuaci\u00f3n, se muestran algunos ejemplos de referencias a leyes en el BOE y en el DOUE teniendo en cuenta las variaciones en el BOE a lo largo de los a\u00f1os Ejemplo s de referencia a ley es en el BOE - Ley 13/2016, de 28 de julio - Ley EDU/1234/2020 En leyes recient es se a\u00f1aden a veces tres letras para indicar el campo tem\u00e1tic o/ministerios en cuesti\u00f3n como Educaci\u00f3n, Fomento. -EL DECRETO MIL QUINIENTOS SESENTA/MIL NOVECIENTOS SETENTA Y CUATRO, DE TREINTA Y UNO DE MAYO, En textos de los 60 y los 70, se mencionaban las fechas de forma alfab\u00e9tica. Ejemplos de referencia a una Directiva Europea - Directiva 2006/123/CE, de 12 de diciembre, del Parlamento Europeo y del Consejo - Directiva 2006/112/CE del Consejo, de 28 de noviembre de 2006 Directiva 2006/112/CE 4.2 Extrac ci\u00f3n de los datos Para cada fichero XML del conjunto, se ha procedido a la extracci\u00f3n autom\u00e1tica del t\u00edtulo a partir de los metadatos adem\u00e1s del contenido y se ha transformado el contenido extra\u00eddo a ficheros en formato txt para facilitar su posterior trat amiento. Gracias a la nomenclatura , se ha podido organizar el proceso de extracci\u00f3n y transformaci\u00f3n en grupos divididos por ventanas temporales correspondientes a las diferentes d\u00e9cadas. Por ejemplo, se han agrupado los ficheros de BOE -A de la d\u00e9cada de los 70, los 80, los 90, etc. Esta agrupaci\u00f3n tambi\u00e9n nos permite analizar la evoluci\u00f3n de las formas de referenciar las leyes y las fechas . Por ejemplo, en los textos legislativos de los a\u00f1os 70, los n\u00fameros en leyes y fechas se oscilaba entre las referenci as num\u00e9ricas y las referencias alfab\u00e9ticas . Al finalizar el proceso, el recuento final del texto extra\u00eddo de BOE -A se asciende a : 370860 624 tokens y DOUE 201840806 tokens . Aunque se ha pre -anotado todo el conjunto, los experimentos de este trabajo se centr an en el BOE -A, puesto que manejar este volumen de datos es imposible por l a inviabilidad de validar esta cantidad y las limitaciones de infraestructura y capacidad de c\u00f3mputo. Por estos motivos y de cara al entrenamiento del modelo , se han creado de forma aleatoria tres conjuntos de datos del BOE -A: Un conjunto de entrenamiento (training ) 1272254 tokens (21116 oraciones) . Un conjunto de desarrollo (develop/validation ) 151600 tokens . Un conjunto de evaluaci\u00f3n (test) 200438 tokens . Al crear estos conju ntos, se ha tenido en cuenta que sean textos relativamente modernos para garantizar la utilidad de l modelo en el contexto actual, ya que un enfoque diacr\u00f3nico queda fuera del alcance del presente estudio . Siguiendo este criterio, los tres conjuntos se han creado a partir de los textos del BOE de las tres \u00faltimas d\u00e9cadas: los a\u00f1os 90, la d\u00e9cada de 2000-2010 y de 2010 -2019. 5 Pre-anotaci\u00f3n Para realizar la pre -anotaci\u00f3n, es imprescindible decidir: 1) \u00bfQu\u00e9 se va a anotar? 2) \u00bfC\u00f3mo se va a realizar esta pre -anotaci\u00f3n? Las respuestas a estas preguntas suelen recogerse en las gu\u00edas de anotaci\u00f3n. Establecer Doaa Samy 108 unos criterios claros y un\u00edvocos garantiza la consistencia de la anotaci\u00f3n y, por consiguiente, la calidad del proceso. Para el presente trabajo, se ha n desarr ollado unas gu\u00edas de anotaci\u00f3n internas b\u00e1sicas como documentos preparativos para una tarea de evaluaci\u00f3n (IberLegal2020@Iberlef) que al final, no se ha celebrado (PlanTL, 2020) . Para estas gu\u00edas, s e ha partido de las gu\u00edas de referencia empleadas en tareas de evaluaci\u00f3n de entidades nombradas en Iberlef (Porta - Zamorano y Espinosa -Anke, 2020) . Es importante se\u00f1alar que la decisi\u00f3n acerca de la t ipolog\u00eda de entidades surge de las caracter\u00edsticas propias del corpus y el dominio. Los componentes NERC gen\u00e9ricos suelen incluir tipos b\u00e1sicos como Personas, Organizaciones, Lugares y Miscel\u00e1nea u Otros. Se han realizado dos pruebas iniciales con los comp onentes NER de las librer\u00edas de spaCy y Stanza, pero los resultados no fueron satisfactorios, ya que no se adapten al dominio en cuesti\u00f3n. Por eso , hemos estimado necesario crear un modelo nuevo con una tipolog\u00eda que refleje la naturaleza de l dominio y que sea de utilidad. Por otro lado, en los textos legales , son comunes las entidades anidadas, es decir, compuestas. Por ejemplo, son comunes las menciones a leyes que incluyen una fecha como \"La ley 1234/2010 , de 12 de mayo de 2010\". Ante estos casos, se ha optado por una aproximaci\u00f3n simplificada considerando cada tipo de entidad de forma independiente. Eso resulta en anotar la menci\u00f3n \"Ley 1234/2010\" como una entidad legal y el segmento \"12 de mayo de 2010\" como fecha. En cuanto a la metodolog\u00eda de anotar, se ha rec urrido a las tres estrategias mencionadas en la se cci\u00f3n 3 y se han integrado diferentes recursos dependiendo de cada tipo de entidad. Leyes, r eferencias a leyes, decretos, normativas, \u00f3rdenes, art\u00edculos . Se ha desarrollado una serie de pa trones de expresiones regulares para identificar referencias como \"Ley 27/2014\" . Adem\u00e1s, se ha recopilado una lista con los nombres oficiales completos de todas las leyes aprobadas desde el a\u00f1o 1977 disponibles en la p\u00e1gina del Senado . (Ej. Ley 27/2014, de 27 de noviembre, del Impuesto sobre Sociedades). Fechas absolutas. Para este tipo de entidades se han desarrollado pat rones de expresiones regulares que cubre n menciones alfab\u00e9ticas y num\u00e9ricas. Organismos. Se han obtenido diferentes listas del Directorio Com\u00fan de Unidades Org\u00e1nicas y Oficinas con un to tal de 16 mil entradas. Sin embargo, fue imprescindible un proceso de depuraci\u00f3n para evitar duplicados, norma lizar formatos y corregir faltas de ortograf\u00eda , etc. Se ha a\u00f1adido una lista adicional que incluye todos los nombres de ministerios en todas las legislaturas. Lugares. Las listas del Directorio Com\u00fan incluyen pa\u00edses, comunidades aut\u00f3nomas, provincias y loc alidades, tipos de v\u00eda, etc. Para utilizar estas listas, fue necesario un proceso de depuraci\u00f3n porque presentaban los mismos problemas se\u00f1alados anteriormente . Adem\u00e1s, se ha optado por excluir algunos nombres de localidades por su ambig\u00fcedad y por el posible ruido que puede causar en forma de falsos positivos . Por ejemplo, se han eliminado de la lista localidades como \"Mar\u00eda\", \"Javier\" o \"Caso \". Personas. Para esta categor\u00eda, hemos optado por utilizar el componente NER de spaC y, dado que los resultados son aceptables, aunque hay un margen de mejora. Asimismo, se ha enriquecido la pre-anotaci\u00f3n de este tipo con una lista de cargos y puestos. La salida de la pre-anotaci\u00f3n es un texto enriquecido con las entidades marcadas en formato de \"offsets\", es decir, incluyendo las posiciones de inicio y fin de cada entidad junto a su tipo. Se ha optado por este formato de salida porque es uno de los que admite spaCy para el entrenamiento . En el siguiente ejemplo, se anotan dos entidades: a) Referencia a la Orden INT/985 /2005 empezando en posici\u00f3n 77 y termina en 95. b) Fecha \"7 de abril \" que empieza en el car\u00e1cter 100 y termina en el car\u00e1cter 110. (\"Uno. Se introducen las siguientes modificaciones en el apartado Cuarto de la Orden INT/985/2005 , de 7 de abril:\", {\"entitie s\": (77, 95, 'LEGAL')]}) Se ha pre -anotado el total del conjunto , pero para gestionar esta cantidad de texto, se ha realizado la pre-anotaci\u00f3n en agrupaciones dividid as por las ventanas temp orales indicadas en la secci\u00f3n 4.1. El resultado es un total de 10424 216 entidades nombradas pre- Reconocimiento y clasificaci\u00f3n de entidades nombradas en textos legalesen espa\u00f1ol 109 anotadas en el BOE1. La cifra indicada refleja la alta frecuencia de entidades en los textos legislativos, lo cual confirma la relevancia d e la tarea NERC en este dominio . Conviene se\u00f1ala que esta cifr a es antes de l proceso de validaci\u00f3n y puede contener falsos positivos, as\u00ed como entidades anotadas dos veces por dos categor\u00edas. No obstante , estas cifras nos puede n ofrecer algunos indicadores generales acerca de la distribuci \u00f3n de los distintos tipos de entidades seg\u00fan la tabla siguiente: Tipo Porcentaj e2 % Leyes -nombres completos 11-15% Organizaciones 35-40% Personas 6-7% Lugares 10-11% Tabla 2. Rangos de d istribuci\u00f3n de los tipos de entidades nombrad as. 6 Validaci\u00f3n Una vez terminada la fase de pre -anotaci\u00f3n, se procede a una validaci\u00f3n parcial de los conjuntos creados para el entrenamiento . Esta validaci\u00f3n pretende revisar de forma manual las anotaciones ambiguas para asegurar un conjunto de entrenami ento de calidad . Se han observado dos casos comunes de ambig\u00fcedad: Persona vs. Lugar. Algunos apellidos coinciden con nombres de lugares. Por ejemplo, \"Segovia \" que aparece como apellido y como ciudad . Organizaci\u00f3n vs. Lugar. Las entidades como \"Comunidad de Madrid\" puede referirse a un lugar o a una instituci\u00f3n. Esa distinci\u00f3n depende del contexto y requiere de un proceso de desambiguaci\u00f3n que queda fuera del alcance de este estudio. Por otro lado, la anotaci\u00f3n de las menciones a leyes en su forma completa constituye un reto por la longitud y la complejidad sint\u00e1ctica de la entidad. Asimismo, a veces aparece completa y a veces aparece de forma parcial. De est e modo , a la misma ley se puede referir 1 Ejemplo del conjunto disponible en: https://github.com/dosamy/NERC -Legal-ES- Example - 2 Se incluyen rangos porque la distribuci\u00f3n var\u00eda entre los subconjuntos de las distintas ventanas temporales. de tres maneras: el t\u00edtulo completo, el t\u00edtulo parcial o la referencia con el n\u00famero, el a\u00f1o y la fecha. Respecto a la categor\u00eda de Persona , esta categor\u00eda incluye tanto las menciones a nombres propios como a puestos o cargos3. La preanotaci\u00f3n de esta categor\u00eda depende, en parte, de la anotaci\u00f3n autom\u00e1tica de spaC y y, en otra parte de listas de cargos. Por eso , requiere de una revisi\u00f3n manual para evitar introducir al modelo ejemplos err\u00f3neos, sobre todo, en lo que se refiere a los nombres propios anotados de forma autom\u00e1tica. 7 Entrenamiento del modelo Se ha elegid o la arqui tectura de spaCy por su flexibilidad y su eficiencia . Adem\u00e1s, ofrece una forma relativamente sencilla de mane jar el entrenamiento de modelos permitiendo aplicar nuevas t\u00e9cnicas de aprendizaje profundo a la tarea de NERC de una forma flexible y sencilla. El entrenamiento de modelos en spaCy se basa en la arquitectura de aprendizaje autom\u00e1tico \" thinc\" que implementa redes neuronales convolucionales profundas (Deep CNN ) integrando los Bloom embedding s (Honnibal y Mo ntani, 2017). Cabe se\u00f1alar que, aunque spaCy 3.0 ha incluido modelo s de lenguaje de Transformers para el espa\u00f1ol, pero hasta la fecha, el componente NER no est\u00e1 disponible para el modelo de Transformers espa\u00f1ol. As\u00ed que , se han utilizado los vectores del modelo grande de spaCy para e l espa\u00f1ol (es_core_news_lg). Hemos entrenado tres modelos de NERC para comparar los resultados y valorar las diferentes opciones de entrenamiento que ofrece la arquitectura de spaCy 3. Se han utilizado los mismos conjuntos de da tos de entrenamiento, desarrollo y evaluaci\u00f3n en el entrenamiento de los tres modelos. En cuanto a la arquitectura del modelo, se ha utilizado los par\u00e1metros de entrenamiento de spaCy por defecto, ya que el objetivo del estudio se centra en el recurso y no en la arquitectura en s\u00ed. NERC -Legal -1. S e trata de una actualizaci\u00f3n sobre el modelo original de spaCy (model_update ). NERC -Legal -2. Se desde cero ( blank -model ) bas\u00e1ndose en la arquitectura del modelo NER de 3 El presente estudio no aplica sub -clases. Doaa Samy 110 spaCy, pero desde cer o sin tener en cuenta el modelo de NER ofrecido por defecto . NERC -Legal -3. Es b\u00e1sicamente el mismo que el modelo anterior, pero utilizando la arquitectura optimizada que ofrece la \u00faltima versi\u00f3n de spaCy. Es un modelo entrenado desde cero. El proceso se h a realizado mediante el fichero de configuraci\u00f3n de spaCy . Model NERC - Legal -1 NERC - Legal -2 NERC - Legal -3 Iteraciones 20 20 10 Drop -out 0.1 0.1 0.1 Batch_size 256 256 1000 Tabla 3. Par\u00e1metros de entrenamiento. 8 Evaluaci\u00f3n Para la evaluaci\u00f3n de los tres modelos se ha utilizado el mismo conjunto de evaluaci\u00f3n. Se ha realizado la evaluaci\u00f3n mediante el Scorer de spaCy que aplica una evaluaci\u00f3n estricta. Los resultados obtenidos demuestran que entrenar un modelo de NERC en el dominio legal alcanza resultado s comparables con el estado de la cuesti\u00f3n. Otro aspecto a resaltar, es que los altos valores de precisi\u00f3n y cobertura se deben a la alta frecuencia de entidades y al uso formal y normalizado del lenguaje en los textos jur\u00eddico, lo cual ayuda al modelo a aprender estas estructuras y generalizarlas. Modelo Evaluaci\u00f3n de los tres modelos . A continuaci\u00f3n, se presentan los resultados obtenidos por cada tipo de entidad nombrada. Las fechas y las entidades legales han obtenido los mejores resultados, mientras que los tipos de Persona y Organizaciones han tenido valores inferiores al resto de las categor\u00edas . Entidades legale Tabla 5. Resultados de los tres modelos por cada tipo de enti dad. En cuanto a la categor\u00eda Persona , la raz\u00f3n detr\u00e1s de los bajos valores en precisi\u00f3n y cobertura es la escasez de ejemplos en el conjunto de datos de entrenamiento, dada su poca frecuencia en comparaci\u00f3n con otros tipos en los textos legislativos. Adem \u00e1s, la pre - anotaci\u00f3n de este tipo de entidades se llev\u00f3 a cabo de forma autom\u00e1tica salvo la anotaci\u00f3n de puestos y cargos que se han anotado a partir de una lista. Todo esto influye en la calidad de los ejemplos y por tanto afecta negativamente al aprendiz aje del modelo. Por otro lado, en este tipo de texto, las entidades de tipo Organizaci\u00f3n son frecuentes , pero las listas empleadas en la pre -anotaci\u00f3n solo incluyen entidades p\u00fablicas espa\u00f1olas. No incluye entidades internacionales ni privadas. Adem\u00e1s, la lista del Directorio Com\u00fan es poco consistente, lo cual influye el proceso de la pre-anotaci\u00f3n y por tanto los resultados del entrenamiento. Por \u00faltimo, se ha llevado a cabo una evaluaci\u00f3n de la anotaci\u00f3n autom\u00e1tica por reglas y listas compar\u00e1ndola con los resultados del modelo NERC -Legal -1. La Tabla 6 presenta la comparativa entre la anotaci\u00f3n por reglas y la anotaci\u00f3n por el modelo NERC -1. Precisi\u00f3n Recall f-score Reglas 0.9230 0.9316 Tabla 6. Anotaci\u00f3n -reglas vs. Modelo. El uso de modelo supone una peque\u00f1a mejora (teniendo en cuenta que hay poco margen de mejora dados los altos valores de la Reconocimiento y clasificaci\u00f3n de entidades nombradas en textos legalesen espa\u00f1ol 111 pre-anotaci\u00f3n) . De ah\u00ed, surge una cuesti\u00f3n : \u00bfEs viable optar por modelos cuyo coste de entrenamiento es alto, cuando se puede obtener resultados parecidos con t\u00e9cnicas menos costos as como las reglas y las listas? Sin embargo, la respuesta es s\u00ed, es m\u00e1s viable a medio y largo plazo porque una vez entrenado , el modelo puede generalizar y, por tanto, permite mayor cobertur a y flexibilidad en comparaci\u00f3n con un anotador basado en reglas y listas que requieren un alto coste de mantenimiento pese a su precisi\u00f3n . La ventaja m\u00e1s destacada es que, el modelo ofrece mayor eficiencia en cuanto a tiempos de anotaci\u00f3n, lo cual permit e mejor integraci\u00f3n en soluciones que requieren tiempos de ejecuci\u00f3n reducidos y , por consiguiente , permite una mejor escalabilidad. A dem\u00e1s , al ser entrenado con spaCy, permite beneficiarse d el abanico de posibilidades que ofrece esta librer\u00eda como integra r el componente en pipelines adaptados al dominio legal espa\u00f1ol como este ejemplo de pipeline del ingl\u00e9s jur\u00eddico4. Por otro lado, este modelo puede adaptarse para subdominios como la jurisprudencia o los textos administrativos. 9 Visualizaci\u00f3n Por \u00faltimo, p ara la visualizaci\u00f3n de los resultados del modelo, se ha utilizado \"displacy\" que ofrece spaCy. En Figura 2 , presentamos el resultado del texto de evaluaci\u00f3n anotado con el Modelo -NERC - Legal -1. 10 Conclusiones y trabajo futuro El presente trabajo ha abordado la importancia de la tarea NERC en el dominio legal destacando los retos en cuanto a los recursos y herramientas de PLN para el espa\u00f1ol legal. En la parte pr\u00e1ctica, se ha presentado una metodolog\u00eda para la anotaci\u00f3n de 5 tipos b\u00e1sicos de entidades mediant e diferentes t\u00e9cnicas. La anotaci\u00f3n no pretende resolver todos los retos de los diferentes tipos de entidades, sino que se trata de una aproximaci\u00f3n transversal b\u00e1sica y como un primer paso en un camino que requiere m\u00e1s esfuerzo y trabajo. Se han entrenado tres modelos y se han presentado l os resultados de los modelos entrenados con spaCy. Los resultados son muy satisfactorios, ya que son 4 https://spacy.io/univ erse/project/blackstone comparables con los resultados de modelos de l NERC espa\u00f1ol en dominio general (Agerri y Rigau, 2020) . La alta frecuencia y el uso normalizado de las menciones a leyes y fechas, etc. son factores que ayudan a obtener altos valores de precisi\u00f3n y cobertura. Partiendo de estos resultados esperanzadores , se abre camino para un abanico de posibilidades para l\u00edneas futuras como las entidades anidadas o tipolog\u00eda s jer\u00e1rquica s donde se contemple n subtipos de entidades como art\u00edculos dentro de una ley, etc. Asimismo, se plantea abordar los textos legales hispanoamericanos, otros sub -dominios como las sentencias, los contratos. Por \u00fal timo, otra l\u00ednea es el tratamiento m\u00e1s completo de las expresiones temporales para incluir expresiones de\u00edcticas o abordar los acr\u00f3nimos y abreviaturas . Por otro lado, se plantea organizar una tarea de evaluaci\u00f3n con la finalidad de contrastar aproximacion es y asentar criterios en el tratamiento de los textos legales en espa\u00f1ol. En cuanto a las conclusiones generales, reiteramos que el dominio legal es un \u00e1mbito que ofrece numerosas oportunidades para la Inteligencia Artificial, en general, y el PLN, en particular. Por \u00faltimo, destacar la administraci\u00f3n p\u00fablica como otro sector relacionado con el dominio legal donde el PLN puede desempe\u00f1ar un papel relevante en el tratamiento de documentos y en el desarrollo de aplicaciones que asistan en optimizar procesos internos y agilizar los servicios p\u00fablicos de cara a la ciudadan\u00eda. Figura 2: Visua lizaci\u00f3n de la anotaci\u00f3n Modelo -NERC . Agradecimientos Este estudio se ha hecho realidad gracias al apoyo continuo del Coordinador del PlanTL , David P\u00e9rez -Fern\u00e1ndez. Asim ismo, agradezco a Prof. Amal Shawer y a \u00d3scar Redondo - Carrasco por su apoyo en todo momento. Doaa Samy 112 Bibliograf\u00eda Agerri, R. y Workshop . Co-located with 36 th Conference of the Spanish Society for Natural Language Processing (SEPLN 2020). M\u00e1laga, Spain, September 2020. Disponible en: http://ceur -ws.org/Vol - 2664/capitel_paper2.pdf Andrew, J. y X. Tannie r. 2018. Automatic Extraction of Entities and Relation from Legal Documents. En Proceedings Named Entities Workshop , Association for Computational Linguistics. pages 1 -8. Melbourne, Australia, July 20, 2018. Badji, I. 2018 . Legal enti ty extraction with NER Systems . Tesis (Master), E.T.S. de Ingenieros Inform\u00e1ticos (UPM) . Cardellino, C., M. Teruel , L. Alemany, y S. Proceedings of the 16th edition of the International Conference on Artificial Intelligence and Law . Chalkidis, I. Conf. on Artificial Intelligence and Law , pages 19 - 28, London, UK, 2017. Chalkidis I . e I. Androutsopoulos contract element extraction. En Proceedings of the 30th International Conference on Legal Knowledge and Infor mation Systems , Luxembourg, pp 155 -164. Cormack, G., R. Grossma n, B. Hedin ., y D. Oard. 2010. Overview of the Legal Track. TREC. Dozier, C., Kondadadi , Light, Montemagni, S., Peters, W., Tiscornia, D. (eds.) Semantic Processing of Legal Text s. LNCS (LNAI), vol. 6036, pp. 27 -43. Springer, Heidelberg (2010). https://doi.org/10.1007/978 -3-642-12837 -0 2. Francesconi, E., S. Montemagni, W. Peters, y D. Tiscornia. 2010. Semantic Processing of Legal Texts: where the language of law meets the law of language (Lecture notes in computer science: notes in Leitner, J. Moreno Named Entity Recognition in Legal Documents . En Maribel Acosta, et al., (eds.), Semantic Systems. The Power of AI and Knowledge Graphs. Proceedings Lecture Notes Karlsruhe, Germany, 9. Springer. 10/11 September 2019. Mart\u00ednez -Gonz\u00e1lez, M., P. de la Fuente D.J. Vicente 2005. Reference extraction a nd texts. E n International Conference on Pattern Recognition and Machine Intelligence , pages 218 -221. Springer. Nadeau, D., y S. Sek ine. 2007. A survey of named entity recognition with Temporal Expressions in the Legal Domain. Proceedings of the Doctoral Consortium, Cha llenge, Industry Track, Tutorials and Posters . Navas -Loro, M. y V. Rodr\u00edguez -Doncel. 2020. Annotador: a Temporal Tagger for Spanish, Journal of Intelligent and Fuzzy Systems, Vol. 39 (2020) PlanTL -IberLegal. 2019. Recursos y aplicaciones de tecnolog\u00edas del lenguaj e Reconocimiento y clasificaci\u00f3n de entidades nombradas en textos legalesen espa\u00f1ol 113 para el dominio legal en lenguas de la Pen\u00ednsula Ib\u00e9rica. Disponible en: https://plantl.mineco.gob.es/tecnologias - lenguaje /comunicacion - formacion/eventos/Paginas/iberlegal - 2019.aspx PlanTL -IberLegal. 2020. Tarea de evaluaci\u00f3n de Entidades Nombradas en textos legales (Cancelada). Disponible en: https://temu.bsc.es/iberlegal/ Porta-Zamorano, J . y L. Espinosa Qi, P., Y. Zhang, Y. Zhang, J. Bolton , y C. D. Manning . 2020. Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. En Association for Computational Linguistics ( , 2019 . Developing and Orchestrating a Portfolio of Natural Legal Language Processing a nd Document Curation Services. E n: Aletr as, N., et al. (eds.) Proceedings of Workshop on Natural Legal Language Processing (NLLP 2019), co-located with NAACL 2019, Minneapolis, USA, 7 June 2019, pp. 55 -66. Rios, S. 2015. Lead Generation for BigLaw? The Business and Ethics of Providing Free Legal Tools and Information Online, 2015. Working paper. Disponible en: E. Montiel -Ponsoda , y P. Casanovas . 2018. Data. TERECOM@JURIX P\u00e9rez - Fern\u00e1ndez . 2020 . Legal -ES: A Set of Large Scale Resources for Spanish Legal Text Processing. En Samy, D. et al. (eds.) Proceedings of Workshop on Language Technologies in Government and Public Administration (LT4Gov 2020), co -located with Marseille, France . Sekine, S. 2004. and Future. nyu. edu/sekine/papers/ Waltl, B. y R. Vogl. 2018. Explainable Artificial Intelligence - the New Frontier in Legal Informatics . En Jusletter basadas en l\u0013 exico para la detecci\u0013 emociones A semantic approach in the lexicon-based e Antonio \u0010a, La Habana, Cuba 2Universidad de Extremadura, Badajoz, Espa~ na 3Universidad de Castilla-La asimon@ceis.cujae.edu.cu, jmperea@unex.es, joseangel.olivas@uclm.es Resumen: La emociones es una tarea del an\u0013 alisis de sentimientos que trata la extracci\u0013 on y el an\u0013 alisis de las emociones en textos. Reconocer emociones impl\u0013 \u0010citas es uno de los principales desaf\u0013 \u0010os en enfoques basados en palabras claves o lexicones. Este trabajo presenta un enfoque h\u0013 \u0010brido de detecci\u0013 on de emociones, que combina la selecci\u0013 on de caracter\u0013 \u0010sticas relevantes de emoci\u0013 on basada en un lexic\u0013 on, con un enfoque cl\u0013 asico de aprendizaje para determinar la emoci\u0013 on. El proceso de selecci\u0013 on de caracter\u0013 \u0010sticas propuesto se centra en capturar el signi cado emocio- nal del texto mediante el c\u0013 alculo de la relaci\u0013 on sem\u0013 antica entre su contenido y el vocabulario del lexic\u0013 on, con el objetivo de incrementar el reconocimiento de emocio- nes impl\u0013 \u0010citas. La soluci\u0013 on propuesta fue evaluada en la clasi caci\u0013 on de emociones en tweets en espa~ nol incluidos en el corpus AIT, con diferentes alternativas para computar la relaci\u0013 on sem\u0013 antica y varios algoritmos de clasi obteni\u0013 endose resultados muy prometedores. Palabras anticas. Abstract: is of sentiment analysis that deals with the extraction and analysis of emotions in texts. Recognizing implicit emotions one of the main challenges in keyword or lexicon-based approaches. This paper presents emotion detection approach, which combines lexicon-based emotion-relevant feature selection with emotion. The focuses on capturing the text by computing the semantic relationship between its content and the lexicon vocabulary, goal of increasing implicit emotion solution was classi cation of emotions in Spanish tweets included in the AIT corpus, with di erent alternatives on Las emociones son rasgos b\u0013 asicos que nos ca- racterizan como humanos y que in uyen en las acciones, los pensamientos y, por supues- to, en nuestra forma de comunicarnos. A pe- sar de no considerarse entidades propiamen-te ling\u007f u\u0013 \u0010sticas, las emociones se expresan a trav\u0013 es del lenguaje por lo que, desde hace va- rios a~ nos, han sido estudiadas por investiga- dores de diferentes disciplinas como la psi- colog\u0013 atica (Ekman, 1992). Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 115-126 del Lenguaje los \u0013 ultimos a~ nos, la comunidad cient\u0013 \u0010 - ca relacionada con el PLN ha mostrado espe- cial inter\u0013 es en la detecci\u0013 on de emociones en conversaciones textuales, ya que su investiga- ci\u0013 on puede encontrar varias aplicaciones en el mundo digital actual (Strapparava, 2016; Mohammad et al., 2018; Chatterjee et al., 2019). Por ejemplo, en el \u0013 ambito de la aten- ci\u0013 on al cliente, redes sociales como Twitter est\u0013 an ganando protagonismo y los clientes es- peran respuestas r\u0013 apidas. En caso de un gran ujo de tweets el tiempo de respuesta aumen- ta, por lo que si los tweets se pudieran priori- zar seg\u0013 un su contenido emocional, la satisfac- ci\u0013 on del cliente seguramente aumentar\u0013 \u0010a. Por otro lado, en esta era de la mensajer\u0013 \u0010a ins- tant\u0013 anea, dado mensajes, podr\u0013 enviaran mensajes de enfado inapro- piados a otros usuarios. En estos casos, si se utilizara una aplicaci\u0013 on de detecci\u0013 on de emo- ciones, se podr\u0013 \u0010an tomar medidas como, por ejemplo, mostrar una advertencia al usuario antes de enviar el mensaje. La tarea de la detecci\u0013 on de emociones en textos presenta un importante desaf\u0013 \u0010o al ca- recer de la ayuda que, en cualquier comu- nicaci\u0013 on visual, proporcionan las expresiones faciales y las modulaciones de voz. Adem\u0013 as, el reto de detectar las emociones en un tex- to se ve agravado por la di cultad de com- prender algunos aspectos relacionados con la comunicaci\u0013 on, como pueden ser el contexto, el sarcasmo, la ambig\u007f uedad del propio len- guaje natural, o la creciente jerga que est\u0013 a provocando el uso masivo de aplicaciones de mensajer\u0013 \u0010a instant\u0013 anea (Shivhare y Khetha- wat, 2012; Khan et al., 2016). En la literatura existen varios enfoques para abordar esta ta- rea. Uno de los m\u0013 as utilizados es el basado en reglas (Strapparava y Mihalcea, 2008; Syko- ra et al., 2013), que trata de explotar el uso de palabras clave y su coocurrencia con otras palabras que tienen asociado un determinado valor emocional o afectivo. Ese valor asocia- do a determinadas palabras del lenguaje sue- le establecerse a partir de diferentes recursos l\u0013 exicos existentes, algunos muy conocidos co- mo WordNet-A ect o SentiWordNet. Por esa raz\u0013 on, a los m\u0013 etodos que siguen este enfoque tambi\u0013 en se les conoce como m\u0013 etodos basados en palabras clave o en lexic\u0013 on. M\u0013 as recientemente, se reportan acerca- mientos donde se combinan el uso de lexic\u0013 on con modelos de aprendizaje autom\u0013 atico, loscuales son reconocidos dentro de los enfoques h\u0013 (Alswaidan y Menai, 2020). Dispo- ner de un lexic\u0013 on o palabras clave con de- terminado valor emocional o afectivo per- mite determinar con alta e cacia el esta- do emocional de los textos, sin embargo, se reconocen ciertas limitaciones en los enfo- ques basados en lexic\u0013 on que est\u0013 an relacio- nadas con la cobertura del contenido procesamiento un lexic\u0013 on desde una perspecti- va sem\u0013 antica permite car caracter\u0013 \u0010sticas emocio- nales de un texto, sin necesidad de aumen- tar el tama~ no de dicho vocabulario. Sin em- bargo, esta orientaci\u0013 on sem\u0013 antica del uso de los lexicones, por ejemplo, mediante c\u0013 ompu- to de relaciones sem\u0013 anticas subyacente entre las palabras de emoci\u0013 on y el contenido tex- tual a procesar, ha sido muy poco explotado. Precisamente, estudios realizados sobre solu- ciones reportadas en la literatura se~ nalan co- mo limitaciones el no uso de caracter\u0013 \u0010sticas sem\u0013 anticas en la detecci\u0013 on de emociones en textos (Alswaidan y Menai, 2020). En este trabajo se propone un m\u0013 etodo pa- ra la detecci\u0013 on de emociones en textos cor- tos escritos en espa~ nol (tweets), que se ba- sa en la sem\u0013 antica de las palabras clave del texto. Nuestra hip\u0013 otesis se centra en que si logramos determinar un buen grado de a - nidad sem\u0013 antica del texto del tweet con ca- on a detectar (relaci\u0013 on sem\u0013 antica una selecci\u0013 caracter\u0013 \u0010sticas m\u0013 as enfocada a las emocio- nes, lo que supondr\u0013 \u0010a un mejor proceso de aprendizaje en el algoritmo encargado de la clasi caci\u0013 on. Por tanto, la principal novedad que aporta este trabajo est\u0013 a relacionada con el enfoque propuesto para capturar el gra- do de a nidad sem\u0013 antica entre el contenido del tweet y cada uno de los vocabularios que caracterizan a cada emoci\u0013 on, lo que permite una selecci\u0013 on de caracter\u0013 \u0010sticas de la opini\u0013 on donde relevancia est\u0013 e m\u0013 as orien- tado al dominio del problema (emociones), en lugar de a aspectos estad\u0013 \u0010sticos dentro del contenido, como com\u0013 unmente se suele adop- tar. Como principal contribuci\u0013 on destaca el enfoque sem\u0013 antico propuesto con el que se trata el vocabulario del lexic\u0013 on y su relaci\u0013 on con el contenido del texto. En general, las so- luciones basadas en conocimiento (lexicones) Harold Gonz\u00e1lez-Guerra, Alfredo Sim\u00f3n-Cuevas, Jos\u00e9 M. Perea-Ortega, Jos\u00e9 A. Olivas 116hacen uso de este tipo de recursos como si fueran una lista de palabras sin signi cado. Nuestra contribuci\u0013 on aporta una soluci\u0013 on al desaf\u0013 \u0010o que representa extender la cobertura de ese conocimiento en el procesamiento del contenido textual a clasi car y lograr identi- car caracter\u0013 \u0010sticas del texto m\u0013 as relevantes para predecir las clases de emociones. El resto del art\u0013 \u0010culo se estructura de la siguiente manera: los principales trabajos re- lacionados se exponen en la Secci\u0013 on 2; la es- tructura general y los principios del m\u0013 etodo propuesto se describen en la Secci\u0013 on 3; los da- tos experimentales, la evaluaci\u0013 on, y el an\u0013 alisis de los resultados obtenidos se muestran en la Secci\u0013 on 4; en la Secci\u0013 on 5 se resume el trabajo y se propone una direcci\u0013 on futura. 2 Trabajos relacionados En la literatura se pueden encontrar diferen- tes enfoques para abordar la tarea de la de- tecci\u0013 on de emociones, siendo incluso clasi ca- dos categor\u0013 \u0010as gen\u0013 erica es la propuesta por Cambria (2016), que establece dos categor\u0013 \u0010as: los basados en reglas y los basados en aprendizaje autom\u0013 ati- co. Al primero pertenecen aquellos m\u0013 etodos que hacen uso de recursos l\u0013 exicos tales como lexicones, bolsas de palabras e incluso onto- log\u0013 \u0010as. El segundo engloba aquellos m\u0013 etodos que aprendizaje basa- dos en caracter\u0013 \u0010sticas ling\u007f u\u0013 \u0010sticas. basados en reglas al- gunos m\u0013 etodos explotan el uso de palabras clave en los textos y su coocurrencia con otras palabras clave con valor emocional/afectivo expl\u0013 \u0010cito (Strapparava y Mihalcea, 2008). Pa- ra ello, se utilizan varios recursos l\u0013 exicos co- mo WordNet-A ect (Strapparava y Valitut- ti, 2004) y SentiWordNet (Esuli y Sebastia- ni, 2006) para el idioma ingl\u0013 es. En cuanto a la disponibilidad de este tipo de recursos en otros idiomas, el n\u0013 umero es muy limitado. Para el espa~ nol, destacan el Spanish Emo- tion Lexicon (SEL) (Sidorov (Molina-Gonz\u0013 alez et al., m\u0013 eto- dos basados en este enfoque tambi\u0013 en tratan de explotar la sintaxis de las palabras cla- ve mediante el uso de etiquetadores POS y, aunque suelen obtener buena precisi\u0013 on, su- fren de una baja cobertura (recall ) porquemuchos textos no contienen palabras afecti- vas a pesar de transmitir emociones (Gupta et al., 2017). La mayor\u0013 \u0010a de los trabajos presentados en el conocido foro evaluaci\u0013 on SemEval du- rante Chatterjee et al., 2019), utilizan l\u0013 exi- cos de afecto y concluyen que son una fuente de informaci\u0013 on muy valiosa porque propor- cionan informaci\u0013 on previa sobre el tipo de emoci\u0013 on asociada a cada palabra del texto. Adem\u0013 as, en WASSA, otro foro de evaluaci\u0013 on relacionado con estas tareas, tambi\u0013 en se de- mostr\u0013 o que el uso de caracter\u0013 \u0010sticas de l\u0013 exicos util para tareas de miner\u0013 \u0010a de emociones (Mohammad y Bravo- Marquez, 2017). En este sentido, Bandhakavi et al. (2017) estudian el problema de la selec- ci\u0013 on de prop\u0013 osito general. A\u0013 un as\u0013 \u0010, existen varias revisiones recientes que po- nen de mani esto las limitaciones asociadas al uso de la sem\u0013 antica que se encuentran en estos procesos para detectar o clasi car emo- ciones en 2020; basados zaje autom\u0013 atico (Machine Learning, ML) la mayor\u0013 \u0010a los m\u0013 etodos se basan en la ex- tracci\u0013 on de caracter\u0013 \u0010sticas, tales como la pre- sencia de n-gramas frecuentes, la negaci\u0013 on, la puntuaci\u0013 on, los emoticonos, los hashtags, etc., para as\u0013 \u0010 formar una representaci\u0013 on de caracter\u0013 \u0010sticas del texto que luego se utili- za como entrada por los clasi cadores para predecir la salida (Canales y Liew y Turtle, 2016). Estos m\u0013 etodos un arduo proceso de selecci\u0013 on de caracter\u0013 \u0010sticas y no logran una alta cober- tura (recall ) debido a las diversas formas de representar las emociones. Dentro de los enfoques ML se han reporta- do m\u0013 etodos basados en redes neuronales pro- fundas, que han tenido un \u0013 exito considera- ble en diversas tareas aplicadas a texto, ha- bla e imagen. Variaciones de las redes neu- ronales recurrentes (RNN), como la LSTM y la BiLSTM, han sido e caces para mo- delar informaci\u0013 on secuencial. Por su parte, la introducci\u0013 on de las redes neuronales con- volucionales (CNN) en el dominio del texto ha demostrado su capacidad para clasi car caracter\u0013 \u0010sticas de las emociones (Mundra et Un enfoque sem\u00e1ntico en la seleccion de caracter\u00edsticas basadas en l\u00e9xico para la detecci\u00f3n de emociones 117Figura 1: Flujo de trabajo de la soluci\u0013 on propuesta. al., 2017). A pesar de los buenos resultados, los enfoques basados en aprendizaje profun- do presentan algunas desventajas respecto a modelos tradicionales de aprendizaje, lo que en algunos contextos pueden llevar a descar- tar su elecci\u0013 on. Algunas de ellas son: deman- da de mucha mayor cantidad de datos bien etiquetados para entrenamiento, mayores exi- gencias en la capacidad de c\u0013 omputo, y por \u0013 ultimo, su naturaleza de funcionamiento ti- po \\caja negra\", que no permite una adecua- da comprensi\u0013 on del proceso de aprendizaje (Kowsari et al., 2019). El gran \u0013 exito de los m\u0013 etodos tradiciona- les basados en aprendizaje autom\u0013 atico, jun- to con el hecho de que recursos l\u0013 exicos como SEL aportan informaci\u0013 on muy valiosa sobre el tipo de emoci\u0013 on asociada a cada palabra, nos ha motivado a probar una combinaci\u0013 on de ambos enfoques para detectar emociones. Partiendo del uso de SEL, proponemos calcu- lar la relaci\u0013 on sem\u0013 antica texto-emoci\u0013 de caracter\u0013 \u0010sticas m\u0013 as enfocada a las emociones, lo que se supone redundar\u0013 a en un mejor aprendizaje del algo- ritmo de clasi caci\u0013 on. 3 Soluci\u0013 on propuesta Esta secci\u0013 on describe la soluci\u0013 on propuesta en este trabajo. Las principales etapas del pro- ceso de desarrollo se describen en la Figura 1. En primer lugar, los textos de opini\u0013 on son preprocesados y normalizados. A continua- ci\u0013 on, se lleva a cabo el proceso de determinar la orientaci\u0013 on emocional de los textos, que consiste en establecer un grado de a nidad (relaci\u0013 on sem\u0013 antica) de cada texto con cada vocabulario asociado a cada emoci\u0013 on del le- xic\u0013 on SEL. Finalmente, se realiza el proceso de selecci\u0013 on de caracter\u0013 \u0010sticas, tomando co- mo base el grado de a nidad obtenido en la fase anterior, y que permite generar los vec- tores caracter\u0013 \u0010sticos que ser\u0013 an utilizados por el algoritmo de aprendizaje supervisado.3.1 Preprocesamiento La naturaleza no estructurada de los textos, m\u0013 as a\u0013 un en el caso de los tweets y textos de opini\u0013 on, requiri\u0013 o la ejecuci\u0013 on de tareas de preprocesamiento o normalizaci\u0013 on. Estas ta- reas consistieron en tokenizar los tweets usan- do NLTK TweetTokenizer1, convertir todo a uscula y eliminar stop words, sig- nos de puntuaci\u0013 on y caracteres raros. No fue necesario ning\u0013 un proceso de traducci\u0013 on pre- via porque dicha herramienta soporta traba- jar con textos en castellano. 3.2 Extracci\u0013 on de caracter\u0013 \u0010sticas orientadas a la emoci\u0013 on 3.2.1 Determinar la orientaci\u0013 on emocional En esta tarea se determina la a nidad del contenido de los tweets a cada una de las emociones incluidas en el lexic\u0013 on de emocio- nes, desde el punto de vista sem\u0013 antico. Es- pec\u0013 \u0010 camente, el lexic\u0013 on SEL (caso de estu- dio) contiene un total de 2.036 palabras en espa~ nol que se organizan en 6 emociones di- ferentes: enojo ( anger ). Por tanto, cada emoci\u0013 on est\u0013 a representada trav\u0013 es de t\u0013 erminos, donde Factor of A ective ) (Sidorov et al., 2012). Ejemplos de palabras que se incluyen en cada categor\u0013 \u0010a son: amistad, bienestar, carcajada, celebrar... (alegr\u0013 \u0010a), enfadado, enfurecer, en- rabiar, ira... (enojo), espeluznante, fobia, te- mor, terror... (miedo), asqueroso, detestable, inmundo, repugnante... (repulsi\u0013 on), asombro- so, incre\u0013 \u0010ble, maravilloso, perplejo... (sorpre- sa), infeliz, luto, pena, p\u0013 erdida... (tristeza). La Figura 2 muestra el n\u0013 umero de palabras por emoci\u0013 on que contiene SEL. Al determinar 1http://www.nltk.org/api/nltk.tokenize. html Harold Gonz\u00e1lez-Guerra, Alfredo Sim\u00f3n-Cuevas, Jos\u00e9 M. Olivas 118Figura Estad\u0013 del lexic\u0013 on SEL por emoci\u0013 on). esta a nidad (orientaci\u0013 on) es posible realizar un proceso de selecci\u0013 on de caracter\u0013 \u0010sticas de los tweets m\u0013 as enfocado en las emociones, lo cual no es posible en los modelos de relevan- cia basados en frecuencia, tales como TF o TF-IDF. La determinaci\u0013 on de la a nidad emocional se basa en el an\u0013 alisis de la relaci\u0013 on sem\u0013 antica existente entre el contenido de los textos de opini\u0013 on y cada emoci\u0013 on del lexic\u0013 on, tomando como referencia su vocabulario. En este tra- bajo se proponen evaluar dos variantes para el c\u0013 omputo de esa relaci\u0013 on sem\u0013 antica opini\u0013 on- emoci\u0013 on: (1) basada en el corpus de opinio- nes, y (2) basada en un recurso sem\u0013 antico externo. Entre las medidas basadas en cor- pus que han sido aplicadas en el \u0013 ambito del an\u0013 alisis de sentimientos se encuentran Point- wise Mutual Information (PMI) y chi-square, entre otras, siendo la primera la adoptada en esta propuesta. En el caso de la varian- te que se apoya en recursos sem\u0013 anticos, entre los tweets y las emociones del lexic\u0013 on se representa mediante una matriz de a nidad, la cual es construi- da para cada tweet u opini\u0013 on que se vaya a procesar. En esta matriz, las las identi- can cada una de las emociones del lexic\u0013 on SEL, y las columnas las palabras de la opi- ni\u0013 on que constituyen caracter\u0013 \u0010sticas candida- tas. La orientaci\u0013 on emocional O(wi;Ej) entre cada palabra wide la opini\u0013 on y la cantidad de t\u0013 erminos del vocabulario de la emoci\u0013 on SEL presentes en la opini\u0013 on Ej, es calcula- da seg\u0013 un la Ecuaci\u0013 on 1, y se almacena en la intersecci\u0013 on t\u0013 ermino-emoci\u0013 on de la matriz dea nidad. Esta matriz constituye la base para la siguiente fase (selecci\u0013 on de caracter\u0013 \u0010sticas basada en la emoci\u0013 on) donde son selecciona- das las caracter\u0013 \u0010sticas m\u0013 as relevantes (las que m\u0013 as con las emociones). O(wi;Ej) =P vi;j2Ejrelsem(wi;vi;j) jEjj(1) Seg\u0013 un se muestra en la Figura 2, en SEL existe un desbalance signi cativo en el ta- ma~ no de los vocabularios de cada emoci\u0013 on, lo que se traduce tambi\u0013 en en una mayor dis- persi\u0013 on de valores PFA en las palabras que los componen, interpret\u0013 andose este tambi\u0013 en como un valor de relevancia para la emoci\u0013 on. Todo ello indica que en un acercamiento ba- sado en la relaci\u0013 on sem\u0013 antica entre opini\u0013 on- emoci\u0013 on, como el que se propone, se podr\u0013 \u0010a estar determinando esa relaci\u0013 on sem\u0013 antica a partir de rangos de valores muy diferentes de PFA y, derivado de ello, originarse afectacio- nes en el c\u0013 alculo de O(wi;Ej). Las emocio- nes que m\u0013 as palabras tengan en su vocabula- rio tendr\u0013 an m\u0013 as in uencia en el c\u0013 alculo de la orientaci\u0013 on de la opini\u0013 on, dado que hay m\u0013 as probabilidad de que los valores resultantes de este c\u0013 alculo sean superiores. Una manera de reducir el efecto de este problema es mediante un mecanismo de poda o ltro del vocabula- rio de las emociones a partir de un umbral de relevancia m\u0013 \u0010nima PFAmin. 3.2.2 Pointwise Mutual Information (PMI) La medida se deriva de la teor\u0013 \u0010a de la informaci\u0013 on y proporciona una v\u0013 \u0010a formal de modelar la informaci\u0013 on mutua entre las ca- racter\u0013 \u0010sticas (ej. palabras de la opini\u0013 on) y las clases (ej. emociones a clasi car) (Aggarwal y Zhai, 2013). En el \u0013 ambito de la clasi ca- ci\u0013 on de textos, este tipo de medidas cons- tituye una alternativa para evaluar la rele- vancia de caracter\u0013 \u0010sticas potenciales del tex- to a clases espec\u0013 \u0010 cas. Esta in- formaci\u0013 on mutua puntual entre la palabra wiy la clase de emoci\u0013 on Ejse de ne so- bre la base de la coocurrencia entre las pa- labrasvi;jdel vocabulario de la emoci\u0013 on Ej y la palabra wide la opini\u0013 on, y se calcu- lar\u0013 \u0010a seg\u0013 un la on caso relsem(wi;vi;j) =PMI (wi;vi;j). Este plan- teamiento se basa en la suposici\u0013 on de que las palabras afectivas (incluidas en el lexic\u0013 on) que coocurren con frecuencia con las palabras de la opini\u0013 on tienden a estar sem\u0013 anticamente Un enfoque sem\u00e1ntico en la seleccion de caracter\u00edsticas basadas en l\u00e9xico para la detecci\u00f3n de emociones 119relacionadas (Agrawal de relaci\u0013 on sem\u0013 antica basadas en WordNet constituyen otra de las alterna- tivas viables para demostrar la hip\u0013 otesis que ha motivado este trabajo. A diferencia del PMI, donde el c\u0013 omputo se sustenta en el pro- cesamiento solo del contenido de los textos, este tipo de medidas explotan la estructura topol\u0013 ogica que forman los synsets y sus rela- on sem\u0013 antica es un concep- to que abarca la relaci\u0013 on entre dos palabras, tanto por similitud entre signi cados, como por v\u0013 \u0010nculos contextuales (ej. (Budanitsky y Hirst, 2006). Diversas m\u0013 etricas han sido de nidas para medir ambos tipos de relaciones, cuyas implementaciones se ofrecen en el paquete de software libre WordNet::Similarity2(Peder- sen, Patwardhan, y Michelizzi, 2004). La dis- ponibilidad de estas m\u0013 etricas constituye una ventaja, dado que se podr\u0013 \u0010an evaluar diferen- tes alternativas individuales y tambi\u0013 en com- binarlas. No obstante, se reconoce que el uso de estas m\u0013 etricas tiene como limitante que las palabras que se comparan est\u0013 en incluidas en alg\u0013 un synset en WordNet. En esta propuesta se ha adoptado para su evaluaci\u0013 on la m\u0013 etri- ca sultados reportados muestran que, en un an\u0013 alisis experi- mental comparativo, la medida JCN es una de las que mejor se comporta. 3.2.4 Poda del vocabulario de las emociones El valor PFA que posee cada palabra en el le- xic\u0013 on SEL sugiere cierta imprecisi\u0013 on y vague- dad subyacente en la relaci\u0013 on palabra-clase de emoci\u0013 on, cuyo grado o pertenencia a la cla- se se expresa a trav\u0013 es de dicho valor de PFA. Esta imprecisi\u0013 on se puede propagar hacia el resultado nal afectando su calidad. En este sentido, se propone un mecanismo de poda o ltrado del vocabulario del lexic\u0013 on que per- mite descartar el subconjunto de vocabulario 2http://wn-similarity.sourceforge.netque mayor imprecisi\u0013 on posee, aspecto no con- siderado en otras soluciones reportadas. El mecanismo propuesto de poda o ltra- do del vocabulario de las emociones a partir delPFAminpermite reducir la dispersi\u0013 on en los valores de PFA de las palabras a conside- rar en el c\u0013 omputo de la a nidad emocional de las opiniones, y llevar a cabo una selec- ci\u0013 on de caracter\u0013 \u0010sticas guiada por un vocabu- lario de mayor relevancia para las emociones. De esta forma, tambi\u0013 en se reduce la carga computacional de este proceso, dado que se disminuir\u0013 \u0010a la cantidad de interacciones en el c\u0013 omputo de las relaciones sem\u0013 anticas, aspec- to muy relevante cuando se utilizan medidas basadas en WordNet. En este sentido, se de - nieron y evaluaron tres criterios para obtener el umbral para la poda del vocabulario de las emociones en el lexic\u0013 on: 1. Tomar como PFAminel valor m\u0013 \u0010nimo de la media de PFA calculada para cada una de las emociones del SEL (enfoque optimista). 2. Tomar como PFAminel valor m\u0013 aximo de la media de PFA calculada para ca- da una de las emociones del SEL (m\u0013 as restrictivo que el anterior, enfoque pe- simista). 3. Tomar como PFAminel valor resultan- te de aplicar un operador de agregaci\u0013 on compensatorio (enfoque fuzzy de la se- lecci\u0013 on del umbral de poda), que permi- ta obtener un \u0013 unico valor representati- vo de los valores medios de PFA obteni- dos de los vocabularios de cada emoci\u0013 on (ei), como es el caso del operador pro- puesto por Zimmermann y Zysno (1980) (Ecuaci\u0013 on 3), donde es el grado de compensaci\u0013 on proporcionado y se podr\u0013 \u0010a calcular seg\u0013 un Ecuaci\u0013 on 4 (Yager y balov, 1998), en la que T(e1;e2;:::;en) es funci\u0013 on t-norma y se podr\u0013 \u0010a calcular seg\u0013 A. Olivas 120T(e1;e2;:::;en) =nY i=1ei (5) Luego de obtenido el umbral de poda (PFAmin), se ltra el vocabulario de cada emoci\u0013 on seleccionando solo aquellas palabras que tengan un PFAi\u0015PFAmin, y se lleva a cabo el an\u0013 alisis de la relaci\u0013 on sem\u0013 antica entre cada una de las opiniones y las emociones del lexic\u0013 on (podadas). 3.3 Selecci\u0013 on de caracter\u0013 \u0010sticas En esta fase se seleccionan las caracter\u0013 \u0010sticas de las opiniones, tomando como base la ma- triz de a nidad construida en la fase anterior y con el objetivo de generar los vectores ca- racter\u0013 \u0010sticos que ser\u0013 an utilizados por los algo- ritmos de clasi caci\u0013 on supervisados. Inicial- mente, a partir la matriz de a nidad de cada opini\u0013 on ( o), se obtiene un grado de a nidad sem\u0013 antica (SAD, Semantic A\u000enity Degree ) con respecto a cada una de las emociones (Ej), seg\u0013 un la Ecuaci\u0013 on 6. Luego, se seleccio- na la clase de emoci\u0013 on con la cual la opini\u0013 on tiene mayor a nidad, siendo esta la que arro- je un valor m\u0013 as alto de SAD(o,E j). La emo- ci\u0013 on sobre la que se exprese mayor a nidad ser\u0013 a la que determine qu\u0013 e palabras afectivas (de las candidatas representadas) caracteri- zan opini\u0013 on (tweet). on Ejcon mayor SAD(o,E j), se ltra la matriz de a ni- dadde la opini\u0013 on que se est\u0013 a procesando, eli- minando las las correspondientes a las emo- ciones restantes. El vector caracter\u0013 \u0010stico de la opini\u0013 on se construir\u0013 a con las palabras de la opini\u0013 on que posean O(wi;Ej)>0. En es- te enfoque se logra un proceso de selecci\u0013 on de caracter\u0013 \u0010sticas donde la evaluaci\u0013 on de su relevancia tiene m\u0013 as en cuenta la sem\u0013 antica alrededor de las emociones, a diferencia de otras propuestas donde el peso fundamental de la relevancia est\u0013 a en enfoques basados en la frecuencia (Plaza-del Arco et al., 2020). Esta propuesta de selecci\u0013 on propicia la re- ducci\u0013 on de caracter\u0013 \u0010sticas redundantes en la construcci\u0013 on de los vectores de las opiniones, dado que las caracter\u0013 \u0010sticas se determinan por una emoci\u0013 on en particular. en per- mite reducir las caracter\u0013 \u0010sticas no informati-vas (o poco informativas) y que no tengan un alto poder discriminatorio, debido a la irrele- vancia o redundancia con respecto a la clase (una misma caracter\u0013 \u0010stica es relevante en di- ferentes grados para varias clases). Todo ello propiciar\u0013 \u0010a la mejora de los resultados del re- conocimiento de emociones basado en un le- xic\u0013 on. Luego de identi cadas las caracter\u0013 \u0010sticas de cada opini\u0013 on, se procede a la \u0013 ultima ta- rea para la construcci\u0013 on del vector de las opi- niones referente al pesado de las caracter\u0013 \u0010sti- cas. El valor de peso de cada una de las ca- racter\u0013 \u0010sticas debe expresar un grado de re- levancia de la misma, y como parte de este trabajo se estudiaron algunas alternativas de peso, tales como O(wi;Ej), como relevancia directa de la caracter\u0013 \u0010stica wien funci\u0013 on de la emoci\u0013 on que determin\u0013 o su selecci\u0013 on; y la frecuencia de ocurrencia de esa palabra ca- racter\u0013 \u0010sticas dentro del corpus de opiniones, dado que es la alternativa m\u0013 as com\u0013 un en solu- ciones de clasi caci\u0013 on supervisada de textos. Sin embargo, resultados parciales experimen- tales arrojaron que un modelo binario de re- presentaci\u0013 on del vector obtuvo mejores resul- tados que esas dos alternativas, con rm\u0013 ando- se lo reportado en Agarwal y Mittal (2016), donde se plantea que, en el \u0013 ambito del an\u0013 alisis de sentimientos, este tipo de modelos ofrece mejores resultados que el basado en la fre- cuencia. En este sentido, se adopt\u0013 o el mode- lo de representaci\u0013 on binario para construir el vector caracter\u0013 \u0010stico de las opiniones, tenien- do en cuenta que el peso de una caracter\u0013 \u0010stica witiene valor 1 si el O(wi;Ej)6= 0, y valor 0 en caso contrario. 4 Resultados experimentales La soluci\u0013 on propuesta fue evaluada en la clasi caci\u0013 on de cuatro emociones (anger, fear,joy, sadness ) en tweets escritos en es- pa~ nol, utilizando el corpus AIT empleado en SemEval-2018 Task 1: A ect in Tweets (sub- tarea EI-oc ). A partir de ese corpus, se toma- ron aleatoriamente 800 tweets por cada una de las emociones (3.200 tweets en total) pa- ra conformar el corpus de prueba. La Tabla 1 muestra la estad\u0013 \u0010stica del corpus de tweets utilizado durante la experimentaci\u0013 on. Los experimentos fueron realizados utili- zando los algoritmos de clasi librer\u0013 \u0010a Un enfoque sem\u00e1ntico en la seleccion de caracter\u00edsticas basadas en l\u00e9xico para la detecci\u00f3n de emociones 121Emoci\u0013 on#tw eets #palabrasmedia pal. lizado durante la experimentaci\u0013 on. scikit-learn3de Python. El 80 % (2.560) de los tweets incluidos en el corpus de prueba se utilizaron para el entrenamiento de los algo- ritmos de clasi caci\u0013 on y el 20 % (640) para test. Los experimentos realizados pretendie- ron, por un lado, evaluar el comportamiento de las medidas de relaci\u0013 on sem\u0013 antica seleccio- nadas y, por otro, evaluar el comportamiento de la poda del lexic\u0013 on por relevancia del vo- cabulario (en sus varias alternativas). En la evaluaci\u0013 on de los resultados se uti- lizaron las m\u0013 etricas de Precision (P), Recall (F1), se de nieron los resul- tados de la selecci\u0013 on de basada en la frecuencia de t\u0013 erminos (TF) como ca- so base. En las Tablas 2-5, se muestran los resultados obtenidos con cada uno de los cla- si cadores, SVM, LR, MLP, y muestran que el clasi cador que mejores resultados obtiene es SVM, al igual que los experimentos reporta- dos en Plaza-del Arco et al. (2020). La Figu- ra 3 muestra una comparativa de los mejores resultados F1 obtenidos por cada clasi ca- dor, independientemente de la medida utili- zada y el criterio empleado para la obtenci\u0013 on delPFAmin. En todas las pruebas realizadas, respecto a la alternativa m\u0013 as com\u0013 unmente usada, se obtienen mejores resultados cuan- do el proceso de selecci\u0013 on de caracter\u0013 \u0010sticas est\u0013 a guiado solo por el an\u0013 alisis de relevancia a partir de la a nidad sem\u0013 antica de la opi- ni\u0013 on respecto a cada una de las emociones, utilizando como referencia el vocabulario que las representa en el lexic\u0013 on SEL, lo que po- ne en evidencia la contribuci\u0013 on del enfoque propuesto. Entre las m\u0013 etricas empleadas para el c\u0013 omputo de la relaci\u0013 a basada en WordNet, independientemente del clasi - cador y del tratamiento del lexic\u0013 on (Figura 4). Una de las posibles causas del compor- 3http://scikit-learn.org Figura 3: Comparativa de los mejores resul- tados F1 entre los clasi cadores utilizados. Figura 4: Comparativa de los mejores resul- tados F1 entre las medidas PMI y WN. tamiento de la m\u0013 etrica basada en WordNet es la ausencia de palabras del lexic\u0013 on o los tweets en ese recurso externo, lo cual es m\u0013 as probable en su versi\u0013 on en espa~ nol. No obs- tante, considerando que los resultados en ge- neral son bastante cercanos, podr\u0013 \u0010a resultar una alternativa prometedora combinar estas y otras m\u0013 etricas en la evaluaci\u0013 on de la a ni- dad emocional de las opiniones. Cabe destacar tambi\u0013 en en estos resultados la contribuci\u0013 on que representa trabajar con un vocabulario de emociones que tenga un mayor equilibrio en cuanto a grados de rele- vancia de los t\u0013 erminos que integre, as\u0013 cantidad de t\u0013 erminos. La exactitud (me- dida Acc) alcanzada por todos los algoritmos de clasi caci\u0013 on fue superior cuando los voca- bularios del lexic\u0013 on fueron podados a partir del umbral de relevancia m\u0013 \u0010nimo ( PFAmin), independientemente de los criterios de selec- ci\u0013 on de este umbral y de la medida utilizada (Figura 5). El aporte al mejoramiento de cada Harold Gonz\u00e1lez-Guerra, Alfredo Sim\u00f3n-Cuevas, Jos\u00e9 M. Perea-Ortega, Jos\u00e9 Medidasenojo miedo alegr \u0013 \u0010a tristezaAccP R F1 P R F1 P R F1 P R F1Po dadoOptimistaPMI .62 .76 .68 .74 .70 .72 .80 .69 .74 .65 .62 .63 .69 WN .36 .66 .42 .60 . 40 .48 .63 .51 .56 .51 .38 .43 .52 PesimistaPMI .60 .73 .66 .67 . 61 .64 .72 .70 .70 .65 .60 .62 .66 WN .40 .68 .50 .61 . 50 .60 .64 .47 .54 .50 .39 .44 .52 FuzzyPMI .62 .77 .69 .74 .58 .65 .65 .66 .66 .69 .66 .68 .67 WN .38 .65 .48 .52 . 37 .43 .65 .46 .53 .52 .45 .48 .51 Sinp odarPMI .51 .62 .55 .53 . 50 .51 .57 .51 .53 .48 .49 .48 .52 WN .45 .38 .41 .48 . 42 .44 .55 .44 .49 .36 .61 .45 .46 TF(caso base) .33 .35 .33 .35 . 30 .32 .35 .36 .35 .34 .33 .33 .35 Tabla 2: Resultados obtenidos con el R F1 P R F1 P R F1 P R F1Po dadoOptimistaPMI .53 .73 .61 .64 .65 .64 .83 .66 .73 .63 .51 .57 .64 WN .38 .69 .49 .57 . 37 .45 .54 .53 .54 .45 .26 .33 .46 PesimistaPMI .50 .71 .59 .65 .60 .62 .71 .68 .69 .71 .52 .60 .63 WN .37 .65 .47 .60 . 45 .51 .51 .45 .48 .44 .28 .34 .48 FuzzyPMI .53 .75 .62 .61 . 57 .59 .69 .61 .64 .69 .54 .61 .62 WN .38 .73 .50 .54 . 33 .40 .47 .43 .45 .48 .28 .35 .44 Sinp odarPMI .49 .65 .55 .53 . 52 .52 .54 .57 .55 .68 .43 .53 .55 WN .34 .61 .44 .51 . 30 .37 .51 .46 .48 .45 .33 .38 .44 TF(caso base) .33 .32 .32 .32 . 31 .31 .37 .34 .37 .34 .30 .32 .34 Tabla 3: Resultados obtenidos con el R F1 P R F1 P R F1 P R F1Po dadoOptimistaPMI .55 .51 .53 .56.60 .58 .62 .53 .57 .45 .52 .48 .54 WN .38 .62 .47 .53 .39 .45 .55 . 48 .51 .44 .36 .39 .46 PesimistaPMI .55 .59 .57 .53 .54 .53 .59 .63 .61 .58 .49 .53 .56 WN .38 .64 .48 .55 .43 .48 .57 . 44 .48 .41 .34 .37 .47 FuzzyPMI .56 .56 .56 .50 .42 .46 .57 .63 .60 .47 .50 .49 .53 WN .38 .62 .47 .57 .40 .47 .50 . 47 .48 .51 .40 .44 .49 Sinp odarPMI .49 .48 .48 .47 .34 .40 .51 . 45 .47 .41 .42 .41 .47 WN .37 .59 .46 .45 .37 .40 .57 . 41 .47 .47 .38 .42 .45 TF(caso base) .26 .25 .28 .28 .29 .28 .35 . 31 .33 .33 .32 .32 .31 Tabla 4: Resultados obtenidos con el R F1 P R F1 P R F1 P R F1Po dadoOptimistaPMI .43 .63 .52 .51 . 29 .37 .48 .34 .40 .37 .47 .41 .45 WN .34 .81 .48 .40 . 24 .32 .45 .23 .34 .34 .30 .32 .38 PesimistaPMI .42 .67 .51 .41 . 25 .31 .40 .26 .31 .38 .43 .41 .40 WN .35 .81 .49 .30 . 31 .30 .55 .20 . 26 .36 .25 .29 .39 FuzzyPMI .42 .65 .51 .42 . 23 .30 .46 .41 .43 .41 .41 .41 .43 WN .32 .80 .46 .60 .32 .41 .41 .12 .18 .43 .31 .36 .38 Sinp odarPMI .32 .44 .32 .31 . 21 .21 .23 .21 .21 .32 .34 .32 .31 WN .32 .75 .44 .49 . 29 .36 .37 .30 .33 .33 .25 .28 .37 TF(caso base) .29 .28 .28 .28 . 29 .28 .34 .30 .32 .32 .28 .29 .31 Tabla 5: Resultados obtenidos con el clasi cador NB. una de las m\u0013 etricas de evaluaci\u0013 on de los crite- rios para la obtenci\u0013 on del PFAminevaluados muestra cierta dispersi\u0013 on, con in del etrica de relaci\u0013 on sem\u0013 que se emplee. En general, los mejores resul- tados se concentran en los criterios optimistayfuzzy, siendo el primero con el que se obtie- nen los mejores resultados combinando SVM y PMI. Por \u0013 ultimo, cabe rese~ nar que, aunque los experimentos no se hayan realizado con el 100 % del corpus (63,4 % del total de tweets Un enfoque sem\u00e1ntico en la seleccion de caracter\u00edsticas basadas en l\u00e9xico para la detecci\u00f3n de emociones 123Figura 5: Comparativa de los mejores resul- tados F1 entre aplicar poda y sin podar. de entrenamiento), los resultados obtenidos se consideran muy prometedores dado que, si establecemos una comparaci\u0013 on con el me- jor resultado de exactitud (Acc) obtenido en Plaza-del Arco et al. (2020) con el lexic\u0013 on SEL y el clasi cador SVM, que fue de 0,76, nuestra soluci\u0013 on alcanza 0,69 con ese mismo clasi cador pero con un 46 % menos de tweets de entrenamiento. 5 Conclusiones y trabajo futuro En este trabajo se aborda la tarea de la de- tecci\u0013 on de emociones en textos mediante un enfoque h\u0013 \u0010brido, que combina un proceso de selecci\u0013 on de caracter\u0013 \u0010sticas basado en lexic\u0013 on enfoque cl\u0013 asico de aprendizaje au- tom\u0013 atico. La novedad de la propuesta radica en el enfoque sem\u0013 antico propuesto para cap- turar el grado de a nidad entre el contenido de opini\u0013 on y el vocabulario que caracteriza cada emoci\u0013 on, de manera que se consigue una selecci\u0013 on de caracter\u0013 \u0010sticas en las opiniones que es m\u0013 as adecuada para la tarea de clasi - caci\u0013 on objetivo. Para evaluar la soluci\u0013 on propuesta se lle- varon a cabo diversos experimentos utilizan- do el corpus AIT de emociones en tweets en espa~ nol. En los experimentos se evaluaron di- ferentes alternativas para calcular el grado de a nidad sem\u0013 antica texto-emoci\u0013 on y se testea- ron varios algoritmos de clasi caci\u0013 on. Como principal conclusi\u0013 on, se considera que el en- foque propuesto es bastante prometedor a la hora de realizar una selecci\u0013 on de caracter\u0013 \u0010sti- cas m\u0013 as adecuada, es decir, m\u0013 as enfocada en las emociones. Los buenos resultados obteni- dos as\u0013 \u0010 lo avalan, habida cuenta que en la experimentaci\u0013 on se utilizaron un 37 % menosde tweets del total disponible en el corpus. Como parte del trabajo futuro, adem\u0013 as de aplicar el m\u0013 etodo propuesto a diferentes cor- pus de tweets, evaluar otras medidas de rela- ci\u0013 on sem\u0013 antica y otros operadores de agrega- ci\u0013 on, pretendemos ampliar este enfoque para entrenar modelos de clasi caci\u0013 on que tengan en cuentan otra informaci\u0013 on ling\u007f u\u0013 \u0010stica del contexto de la opini\u0013 on como, por ejemplo, la negaci\u0013 on. Agradecimientos Este trabajo ha sido parcialmente nancia- do por el Fondo Europeo de Desarrollo Re- gional (FEDER), la Junta de Extremadura (GR18135), y el Ministerio de Ciencia, Inno- vaci\u0013 on y Universidades de Espa~ na, proyecto C. X. Zhai. 2013. Mining text data, volumen 9781461432234. Sprin- ger. Agrawal, A. y A. An. 2012. Unsupervi- sed emotion detection from text using se- mantic relations. En Pro- ceedings - 2012 IEEE/WIC/ACM Inter- national on Web Intelligence, WI 2012, p\u0013 aginas 346{353. Alswaidan, N. y M. E. B. Menai. 2020. A survey of state-of-the-art approaches for emotion recognition in text. Knowled- ge and Information Systems, 62(8):2937{ 2987. Bandhakavi, A., Jos\u00e9 A. Olivas 124Cambria, E. 2016. A ective Computing and Sentiment Analysis. IEEE Intelligent Sys- tems, detection from text: A survey. En Proceedings of the Workshop on Natu- ral Language Processing in the 5th Infor- mation Systems Research Working K. Srini- vasan, V. Sharma, C.-Y. Chang, y D. G. Reina. 2019. Emotion ai-driven sentiment analysis: A survey, future Applied Sciences, 9(24). Chatterjee, A., N. Narahari, SemEval-2019 Task Contextual Emotion De- tection in Text. on Semantic Emotion , Hemmatian, F. y Intelligence Review, 52(3):1495{ 1545. Jiang, J. J. y D. W. Conrath. 1997. Seman- tic similarity based on corpus statistics and Linguistics and Chinese Language Proces- sing (ACLCLP), Agosto. Khan, M. T., M. Durrani, A. Ali, I. Inayat, S. Khalid, y K. H. Khan. 2016. Sentimentanalysis and the complex natural langua- darysafa, Barnes, y D. Brown. 2019. Text classi cation algorithms: A survey. Information, 10(4). Liew, J. S. y H. R. Turtle. 2016. ne-grained emotion detection in tweets. Associa- Bravo-Marquez. 2017. Wassa-2017 shared task on emotion intensity. Mohammad, S. M., F. Bravo-Marquez, M. Salameh, y S. Kiritchenko. 2018. SemEval-2018 Task 1: A ect Tweets. EnProceedings of International Workshop on Semantic Evaluation (SemEval-2018), New USA. Molina-Gonz\u0013 alez, Mart\u0013 \u0010nez-C\u0013 ama- ra, M. \u0010n-Valdivia, 2013. classi Mundra, M. Sinha, y in contact cen- ter Lecture Notes Computer Science subseries Lecture the relatedness of concepts. L. A. Ure~ na-L\u0013 opez, y R. Mit- kov. 2020. Improved emotion in Spanish social media through incorpo- ration lexical knowledge. Future Gene- K., Dhaliwal, J. Rokne, y R. Alhajj. 2018. Emotion detection from text and speech: a survey. Social Network Analysis and Mining, 8(1). Un enfoque sem\u00e1ntico en la seleccion de caracter\u00edsticas basadas en l\u00e9xico para la detecci\u00f3n de emociones 125Shivhare, S. N. y S. Khethawat. 2012. Emo- tion A. y of identify emotions in text. of the 2008 1556{ 1560. Sykora, M. D., T. y S. Elayan. 2013. Emotive ontology: IADIS International Conference In- telligent Systems and Agents 2013, ISA 2013, Proceedings of the IADIS European Conference on Data Mining 2013, ECDM 2013. Yager, R. y in Cybernetics, Part B: Cyberne- tics, 28(6):757{769. Zimmermann, H. J. y P. Zysno. 1980. La- tent connectives in human decision ma- king. Fuzzy Sets and Systems, 4(1):37{51. Harold Gonz\u00e1lez-Guerra, Alfredo Sim\u00f3n-Cuevas, Jos\u00e9 M. Perea-Ortega, Jos\u00e9 autom\u0013 de una taxonom\u0013 \u0010a multiling\u007f ue marcadores primeros resultados en castellano, ingl\u0013 es, franc\u0013 es, alem\u0013 an y catal\u0013 an Automatic induction of a multilingual taxonomy of discourse markers: rst results in Spanish, English, French, German and Catalan Rogelio Nazar Instituto de Literatura y Ciencias del Lenguaje Ponti cia Universidad Cat\u0013 atica de una taxonom\u0013 \u0010a multiling\u007f ue de marcadores discursivos, que en el caso del castellano corresponden a unidades tales como sin embargo, por lo tanto, por un lado, etc. Se propone primeramente un m\u0013 etodo para separar estas unidades del resto del vocabulario por medio del c\u0013 alculo de su cantidad de informaci\u0013 on, seguido de su agrupaci\u0013 on en categor\u0013 \u0010as funcionales mediante un corpus paralelo. Finalmente, esta categorizaci\u0013 on se utiliza como base para la obtenci\u0013 on y clasi caci\u0013 on de nuevas unidades. Adem\u0013 as del m\u0013 etodo, se describen los primeros resultados, consistentes en una base de datos que actualmente supera ya los 2.600 marcadores. Palabras clave: inducci\u0013 on de taxonom\u0013 \u0010as, lexicograf\u0013 \u0010a computacional. Abstract: This paper presents a methodological proposal por the automatic induc- tion of a multilingual taxonomy of discourse markers which, in the case of English, correspond to units such as however, therefore, by the way a method is proposed to separate such units from the rest of the vocabulary using a measure of information, followed by a method group them using a parallel corpus. Finally, this categorization is cation new 1 Introducci\u0013 on es un tema nuevo en ling\u007f u\u0013 \u0010stica, los marcadores del discurso (MD) han esta- do en el foco de inter\u0013 es de la \u0010a otros). Los MD son part\u0013 \u0010culas discursivas que cumplen una amplia variedad de funciones, pero que no forman parte del contenido proposicional de los segmentos a los que afectan. Los ejem- plos de estas part\u0013 \u0010culas pueden ser muy di- versos, como se explicar\u0013 a m\u0013 as adelante, pero entre los m\u0013 as frecuentes encontramos los co- nectores aditivos (adem\u0013 as, tambi\u0013 en, etc.), los contraargumentativos ( sin embargo, no obs-tante, etc.), los causales (por este motivo, por lo tanto , etc.) los reformulativos (es decir, en otras palabras, etc.), entre un variado n\u0013 umero de otras categor\u0013 \u0010as. La gran mayor\u0013 \u0010a de las investigaciones que se han realizado sobre este tema han sido en el \u0013 ambito de la ling\u007f u\u0013 \u0010stica Secci\u0013 on 2). Los m\u0013 etodos dominantes hasta ahora han sido la intros- pecci\u0013 on y, en menor medida, el trabajo con corpus. Sin embargo, en este \u0013 ultimo caso, el corpus es utilizado como herramienta explo- ratoria, mediante examen visual de l\u0013 \u0010neas de concordancia de uno o algunos MD. Comparativamente, son pocos los inten- tos de afrontar este tema con las herramien- tas del procesamiento del lenguaje natural Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 127-138 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural(PLN), tanto en castellano como en otras len- guas. La ventaja m\u0013 as evidente del PLN sobre los m\u0013 etodos investigaci\u0013 on \u0010stica en este caso particu- lar es la posibilidad de obtener un inventa- rio masivo de marcadores. Esto es porque, a pesar de corresponder a la categor\u0013 \u0010a de uni- dades funcionales dentro del vocabulario, no corresponden a una lista cerrada, como la de las preposiciones, y no existe hasta la fecha por tanto un cat\u0013 alogo completo de los MD. Tampoco se ha producido hasta ahora total acuerdo entre los especialistas acerca de c\u0013 omo se pueden clasi car, ya que los enfoques y teor\u0013 \u0010as son muy diversos y a menudo incom- patibles. El presente art\u0013 \u0010culo pretende hacer un aporte precisamente en la l\u0013 \u0010nea del inventa- riado y la taxonomizaci\u0013 on de los MD existen- tes en distintas lenguas. Ofrece una descrip- ci\u0013 on de los resultados preliminares de un pro- yecto de investigaci\u0013 on en curso en el campo de los MD mediante herramientas de PLN. Se trata de una propuesta metodol\u0013 ogica pa- ra la inducci\u0013 on autom\u0013 atica de una taxonom\u0013 \u0010a multiling\u007f ue de MD a partir de corpus parale- los, utilizando algoritmos exclusivamente es- tad\u0013 \u0010sticos. En su estado actual, los resultados del proyecto consisten en una base de datos de 2.636 MD clasi cados en franc\u0013 es, y catal\u0013 an. Estos datos se encuentran disponi- bles para descarga desde la web del proyecto1, y van aumentando en cantidad en la medida en que se contin\u0013 ua con el desarrollo. La metodolog\u0013 \u0010a del proyecto incluye una cadena de procesamiento en la que en ning\u0013 un momento existe intervenci\u0013 on humana. Los re- sultados que se ofrecen, sin embargo, han si- do ya revisados por un grupo de ling\u007f uistas, hablantes nativos en cada caso, para corregir posibles errores. La tasa de error en los re- sultados de las diferentes lenguas no super\u0013 o el 5 % con excepci\u0013 on del alem\u0013 an, en donde la tasa de error lleg\u0013 o al 16 %. El m\u0013 etodo propuesto tampoco utiliza re- cursos ling\u007f u\u0013 material con el que tra- baja es un corpus paralelo de gran tama~ no, lo que facilita en gran medida la reproducci\u0013 on de los experimentos en otras lenguas. Como recurso propiamente ling\u007f u\u0013 \u0010stico, se Zorraquino y Portol\u0013 es (1999) para los nombres de las categor\u0013 \u0010as de los MD en castellano, pero esta funciona a modo de metadato externo al propio m\u0013 etodo y es igual de v\u0013 alida para las distintas lenguas. Las unidades que el algoritmo inicialmen- te elige y segmenta como candidatos a MD consisten en palabras o secuencias de pala- bras consideradas con bajo nivel de infor- un c\u0013 sem\u0013 antica tiene una palabra, como es el caso de aquellas palabras con una denominaci\u0013 on precisa (aerosol, marxismo, trastorno obse- sivo compulsivo, etc.), mayor es su cantidad de informaci\u0013 on. Las palabras funcionales o gramemas, tales como las preposiciones, pero tambi\u0013 en los MD, obtienen seg\u0013 un este coe - ciente una cantidad de informaci\u0013 on m\u0013 as baja. Una vez obtenidos los listados de candida- tos a MD, estos son organizados en categor\u0013 \u0010as funcionales a partir del corpus paralelo, ex- plotando su similitud en cuanto a equivalen- tes en la otra lengua. Esta organizaci\u0013 on en grupos funcionales se convierte en una cla- si caci\u0013 on que se realiza, en un primer nivel, con la ayuda de la taxonom\u0013 \u0010a ofrecida por Mart\u0013 \u0010n Zorraquino y Portol\u0013 es (1999), de la que se obtienen los nombres para etiquetar los grupos gracias a los ejemplos que se in- cluyen. Esta taxonom\u0013 \u0010a, sin embargo, es a su vez subdividida y enriquecida con subcate- gor\u0013 \u0010as que resultan emergentes del corpus, y que no pueden ser etiquetadas porque exce- den el nivel de granularidad de dicho recurso. Adem\u0013 as del inter\u0013 es que puede tener la pro- puesta en tanto metodolog\u0013 \u0010a, existe tambi\u0013 en el que ofrece el resultado mismo. Esto es por- que en la bibliograf\u0013 \u0010a sobre el tema es fre- cuente encontrar diferentes taxonom\u0013 \u0010as y lis- tados de ejemplos, pero en la mayor parte de los casos estos alcanzan unos pocos centena- res, cuando las unidades utilizadas realmente como MD en la lengua se cuentan por mi- les. La base de datos que resulta puede te- ner diversas aplicaciones. Por un lado, puede informar los m\u0013 etodos y las conclusiones de estudios en ling\u007f u\u0013 \u0010stica te\u0013 orica sobre el tema. Por otro lado, puede ser utilizado tambi\u0013 en como herramienta en el PLN para el parsing discursivo en tareas de extracci\u0013 on de informa- ci\u0013 on. Por \u0013 ultimo, en su estado actual puede ser tambi\u0013 en de inter\u0013 es para usuarios nales, ya sea traductores o quienes necesiten redac- Rogelio Nazar 128tar en su propia lengua o en una L2, y bus- quen equivalentes o deseen cuidar la riqueza de vocabulario de sus textos. 2 Trabajo relacionado 2.1 Antecedentes te\u0013 oricos Entre los pioneros del estudio de los MD se encuentran en particular muchos gram\u0013 aticos de la lengua castellana, tales como Antonio de Nebrija, Gregorio Garc\u0013 es, Andr\u0013 es Bello y m\u0013 as recientemente Gili Gaya (1943), pero la verdadera profusi\u0013 on de investigaciones en el tema es posterior. Comenz\u0013 o con el trabajo de van Dijk (1973), quien describi\u0013 o las relaciones l\u0013 ogicas que se producen entre proposiciones a trav\u0013 es del uso de distintos conectores, tales como los de disyunci\u0013 on, conjunci\u0013 on, causali- dad, condici\u0013 on, contraste, etc. Algunos a~ nos m\u0013 as tarde, esta l\u0013 \u0010nea de investigaci\u0013 on se vio extendida por el trabajo de Halliday y Hasan (1976), que presentaron ya una taxonom\u0013 \u0010a m\u0013 as completa para el caso del ingl\u0013 es, inclu- yendo otras categor\u0013 \u0010as adem\u0013 as de las mencio- nadas por van Dijk. En paralelo, en el \u0013 area de los estudios de la argumentaci\u0013 on en franc\u0013 es, Anscombre y Ducrot (1976) profundizaron en las funciones de part\u0013 \u0010culas y conectores que hoy englobar\u0013 \u0010amos en la categor\u0013 \u0010a de MD. Tal como se~ nala Stubbs (1983), el an\u0013 alisis de este tipo de unidades evidenci\u0013 o las limita- ciones de lo que hasta los nos sido una gram\u0013 atica oracional y justi c\u0013 en buena medida el lanzamiento de una gram\u0013 ati- ca del texto, precedente de lo que luego ser\u0013 \u0010a el an\u0013 alisis del discurso. A partir de los a~ nos ochenta se multiplicar\u0013 \u0010a la cantidad de inves- tigaciones en esta subdisciplina y, particular- mente, en el campo de los MD. Los sucesivos trabajos de investigaci\u0013 on intentaron delinear las propiedades de nitorias de estas unida- des, es decir, aquellas que los de nen como subconjunto del vocabulario, y tambi\u0013 en aque- llas propiedades que permiten organizarlas en categor\u0013 \u0010as. Parece existir consenso en que los MD re- presentan un fen\u0013 omeno com\u0013 un a todas las lenguas, pero no son f\u0013 acilmente de nibles co- mo conjunto de unidades. A menudo son de- nidos como part\u0013 \u0010culas discursivas que sir- ven para facilitar las relaciones de coherencia en los textos (Fraser, 1999; Pons Border\u0013 \u0010a, 2001), en el sentido de que ofrecen instruccio- nes para la interpretaci\u0013 on y van organizando la argumentaci\u0013 on. Su aparici\u0013 on, sin embargo, no es estrictamente necesaria ya que igual-mente en su ausencia es posible inferir rela- ciones l\u0013 ogicas entre proposiciones como, por ejemplo, la causalidad. A pesar de que a veces no hacen falta, son sin embargo un elemento clave para facilitar el trabajo interpretativo del lector y reducen el riesgo de error o de ambig\u007f uedad. El rol de los MD tambi\u0013 en consiste en re- gular la interacci\u0013 on entre participantes. Esto sucede con mayor frecuencia en la comunica- ci\u0013 on oral, aunque no exclusivamente. En este sentido, se puede decir que tienen tambi\u0013 en una funci\u0013 on interpersonal adem\u0013 as de la lugar de Mosegaard Hansen (1998), por ejemplo, men- ciona los indicadores de cambio de tema o de cambio de turno de los participantes en la in- teracci\u0013 on. Esto hace que consideremos en la categor\u0013 \u0010a de MD a todas aquellas part\u0013 \u0010culas pragm\u0013 aticas que tienen una funci\u0013 como part\u0013 \u0010culas modales e inter- jecciones, lo que di culta el establecimiento de un l\u0013 \u0010mite preciso. Desde un punto de vista morfol\u0013 ogico, los MD pueden tener diversas categor\u0013 \u0010as grama- ticales: conjunciones, adverbios o preposicio- nes, casi siempre como expresiones pluriver- bales. Es posible decir que se caracterizan por ser (relativamente) invariables, ya que no pre- sentan la exi\u0013 on t\u0013 \u0010pica de Mart\u0013 \u0010n Zorra- Portol\u0013 es (1999), los MD no presentan exi\u0013 on de g\u0013 enero ( *por cierta ) ni n\u0013 casi nunca modi - cadores (*muy sin embargo, pero s\u0013 \u0010 muy por el contrario ); no pueden ser negados ( *no a saber ) ni coordinados (*a saber y sin embar- go). Desde un punto de vista sint\u0013 actico, Schif- frin (2001) ha se~ nalado que una posici\u0013 on inicial en la oraci\u0013 on, pe- ro tambi\u0013 en pueden ocupar otras posiciones. Suelen ser tambi\u0013 en parent\u0013 eticos, es decir que suelen aparecer entre pausas o, en el caso de la lengua escrita, signos de puntuaci\u0013 on, co- mo comas o puntos. Esto parece indicar que no forman parte de la estructura sint\u0013 actica de la oraci\u0013 on. Sin embargo, nuevamente esta tampoco parece una regla rme, ya que tam- bi\u0013 en es posible encontrarlos en una posici\u0013 on no parent\u0013 etica. En cualquier caso, no est\u0013 an con nados a la oraci\u0013 on, y tienen la capacidad de afectar alternativamente a distintos nive- les, ya sea al intraoracional o bien al extra- oracional o discursivo (Pons Border\u0013 \u0010a, 2001; Inducci\u00f3n autom\u00e1tica de una taxonom\u00eda multiling\u00fce de marcadores discursivos: primeros resultados en castellano, ingl\u00e9s, franc\u00e9s, alem\u00e1n y catal\u00e1n 129Brinton, 2010). Posiblemente sea el punto de vista sem\u0013 antico el \u0013 unico que permita una distinci\u0013 on m\u0013 as clara del conjunto, ya que se caracterizan por una falta de contenido referencial o pro- posicional. Aqu\u0013 \u0010 tambi\u0013 en es preciso hacer la salvedad, sin embargo, ya que es posible que algunos conserven parte del signi cado l\u0013 exi- co que alguna vez tuvieron y que perdieron durante la evoluci\u0013 on hist\u0013 orica de la lengua a trav\u0013 es de un proceso de gramaticalizaci\u0013 on (Traugott y Dasher, 2002; Wichmann y Cha- net, 2009). Adem\u0013 as de los intentos por de nir a los MD como conjunto, otro aspecto que ha preo- cupado a los te\u0013 oricos es el de diferenciar las distintas clases que existen. En este aspec- to, sin duda el trabajo de Halliday y Hasan (1976) es pionero en el esfuerzo de establecer categor\u0013 \u0010as. Sin embargo, nuevamente destaca la tradici\u0013 on espa~ nola como la que m\u0013 as se ha centrado en la categorizaci\u0013 on, como se puede apreciar en los trabajos de Casado Velarde (1993), Montol\u0013 \u0010o (2001), Calsamiglia y Tus\u0013 on (1999) y, en Mart\u0013 \u0010n Zorraquino y Portol\u0013 es (1999). El \u0013 ultimo trabajo es el que ha ofrecido la taxonom\u0013 \u0010a m\u0013 as exitosa y que ha in uido in- cluso en la clasi caci\u0013 on de MD en otras len- guas, como el alem\u0013 an Loureda, clasi caci\u0013 on en dos niveles: primero ofrece una serie de categor\u0013 \u0010as m\u0013 as generales que luego se subdividen en as es- as generales con las que ya han sido se~ naladas por otros autores, tales como los estructuradores de la informaci\u0013 on, los conectores, reformula- dores, operadores argumentativos y marca- dores conversacionales. Pero luego cada una de estas grandes categor\u0013 \u0010as se subdivide y as\u0013 \u0010 tenemos entonces, por ejemplo en el caso de los conectores, los aditivos ( adem\u0013 as, en- cima, aparte, etc.); consecutivos ( por tanto, por consiguiente, por ende, etc.) y contraar- gumentativos ( sin embargo, no obstante, en cambio, etc.). 2.2 Antecedentes de an\u0013 alisis de MD con herramientas de PLN El tema de los MD ha recibido m\u0013 as atenci\u0013 on por parte de la que de o del PLN. En par- ticular, es llamativamente poco tratado en la bibliograf\u0013 \u0010a alisis m\u0013 natu- ral encontrarlo. En comparaci\u0013 on el enor- me volumen de t\u0013 \u0010tulos del \u0013 area, son pocos los trabajos que tratan expl\u0013 \u0010citamente sobre MD, como Stubbs (1996) o Moore y Wiemer- Hastings (2003). Adem\u0013 as, la gran mayor\u0013 \u0010a de de- MD consiste en el an\u0013 ali- sis cualitativo de uno o unos pocos casos de MD, como por ejemplo el caso de Urgelles- Coll (2010) en ingl\u0013 es o Cardona (2014) en castellano, entre muchos otros. Comparati- vamente, son pocos los intentos por ofrecer cat\u0013 alogos exhaustivos de los MD que existen en distintas lenguas, que es justamente el \u0013 area en la que las herramientas de PLN podr\u0013 \u0010an prestar un mejor servicio. S\u0013 \u0010 existen algunos esfuerzos por recopilar inventarios amplios de MD, como pueden ser el trabajo de Knott (1996) en el caso del ingl\u0013 es, el de Stede (2002) para el caso del alem\u0013 an, el de Roze, Danlos, y Muller (2012) para el caso del franc\u0013 es, o los de Santos R\u0013 \u0010o (2003) y Briz, Pons, y Portol\u0013 es (2008) para el caso del castellano, entre otros. Sin embargo, el esfuerzo humano que exige la compilaci\u0013 on manual de estos listados implica una gran di cultad para la obtenci\u0013 on de lis- tados verdaderamente exhaustivos. Tal como se~ nalan Lopes et al. (2015), las herramientas de PLN son ideales para esta tarea, y esto puede explicar la aparici\u0013 on de una nueva ten- dencia en ling\u007f u\u0013 \u0010stica computacional que des- cubre renovado inter\u0013 es por la extracci\u0013 on y catalogaci\u0013 on de MD. Y un rasgo com\u0013 un que presentan estos estudios m\u0013 as recientes parece ser el an\u0013 alisis de pares de lenguas, frecuente- mente mediante corpus paralelos. En el caso del citado trabajo de Lopes et al. (2015), el par de lenguas viene dado por la aplicaci\u0013 on de un sistema de traducci\u0013 on autom\u0013 atica. Parten de un listado de MD en ingl\u0013 es generado de manera manual y se limi- tan a realizar la traducci\u0013 on de este listado a diferentes lenguas. En un trabajo anterior (Robledo y Nazar, 2018) se propuso un enfoque basado en clus- tering a partir en corpus paralelo aplicado al caso de los MD en castellano. Aquel m\u0013 eto- do consisti\u0013 o en obtener grupos de MD con equivalencia funcional, la cual viene dada por compartir equivalentes en otra lengua. La li- mitaci\u0013 on de dicho m\u0013 etodo es que implica la utilizaci\u0013 on computacionalmente costosos debido a su complejidad cuadr\u0013 atica. Otros autores han optado por el uso de algoritmos de aprendizaje autom\u0013 atico, como Sileo et al. (2019), en el que utilizan como material de entrenamiento un grupo de MD en ingl\u0013 es generado de manera manual. Se con- centran en la extracci\u0013 on de MD parent\u0013 eticos de alta frecuencia y en posici\u0013 on inicial de ora- ci\u0013 on, y el insumo que utilizan son las pistas contextuales, entendidas como enegramas de palabras. Tambi\u0013 en en este caso se trata de una metodolog\u0013 \u0010a de alta complejidad, tanto conceptual como computacional, que necesi- ta de variados recursos externos que di cul- tan la reproducci\u0013 on de experimentos en otras lenguas. En relaci\u0013 on con estos esfuerzos recientes para el procesamiento de MD dentro de la ling\u007f u\u0013 \u0010stica computacional, el presente art\u0013 \u0010cu- lo representa contribuci\u0013 on m\u0013 as en la mis- ma direcci\u0013 on, ya que se propone conseguir un listado amplio de MD. De los trabajos men- cionados, el que m\u0013 as se le parece es el de Ro- bledo y Nazar (2018), en tanto explota el uso de corpus paralelos para encontrar la equi- valencia entre MD de una misma lengua. En contraste con todos los mencionados traba- jos, sin embargo, la virtud principal del que se presenta ahora es que se trata de un m\u0013 etodo mucho m\u0013 as simple, ya que no requiere pr\u0013 acti- camente de ning\u0013 un recurso externo. Esto re- presenta una gran ventaja en dos sentidos: en primer lugar, disminuye el coste compu- tacional, lo cual facilita el procesamiento de grandes vol\u0013 umenes de datos, y en segundo lugar, posibilita la reproducci\u0013 on de los expe- rimentos en diferentes lenguas. Finalmente, en contraste con los estudios cualitativos, la ventaja de un enfoque como el que se pre- senta en este art\u0013 \u0010culo es la gran cantidad de datos que genera, ya que se obtienen listados de miles de MD, en contraste con los pocos centenares a los que llegan la mayor\u0013 \u0010a de los enfoques cualitativos e incluso varios de los que proponen m\u0013 etodos automatizados. 3 Metodolog\u0013 \u0010a Como ya se en la introducci\u0013 on, con esta metodolog\u0013 \u0010a nos proponemos en pri- mer lugar identi car los MD del corpus se- par\u0013 andolos del resto de las unidades del voca- bulario (Secci\u0013 on 3.1), para luego clasi carlos de manera inductiva en categor\u0013 \u0010as funciona-les (Secci\u0013 on 3.2), que son luego etiquetadas de modo tambi\u0013 en autom\u0013 atico (Secci\u0013 on 3.3). Una vez que existe una taxonom\u0013 \u0010a nuclear o b\u0013 asica, comienza el proceso de poblamiento extensivo de esta estructura (Secci\u0013 on 3.4). 3.1 Vaciado de MD a partir del corpus La primera fase de la metodolog\u0013 \u0010a consiste en responder a la pregunta de c\u0013 omo separar las unidades consideradas MD del resto de las palabras del corpus. Para ello, la decisi\u0013 on fue apostar por una caracter\u0013 \u0010stica propia, aun- que no exclusiva, de los MD, que es su bajo contenido informativo. Naturalmente, no se puede decir que los MD no tengan informaci\u0013 on en el sentido de que no sean portadores de ning\u0013 un tipo de sig- ni cado. Como se mencion\u0013 o en la Secci\u0013 on 2, los MD poseen un signi cado funcional, ya que son el veh\u0013 \u0010culo de distintas relaciones de sentido. Pero este es un tipo de signi cado distinto al valor designador o referencial que tienen t\u0013 \u0010picamente las unidades l\u0013 exicas. En el extremo de las palabras funcionales encontra- mos las preposiciones, clase cerrada y perfec- tamente catalogada en las lenguas conocidas, y en el extremo opuesto los t\u0013 erminos especia- lizados. Pero entre un extremo y otro de este continuum encontramos una gran diversidad de unidades que no poseen el signi cado l\u0013 exi- co espec\u0013 \u0010 co de los nombres o, si lo tuvieron alguna vez, lo perdieron en un proceso de gra- maticalizaci\u0013 on en la historia de la lengua (cf. Secci\u0013 on 2.1). En este caso, de nimos cantidad de infor- maci\u0013 on en un sentido formal como un valor que indica cu\u0013 anto ayuda a predecir una varia- ble aleatoria el resultado de otras variables. Claramente, la distribuci\u0013 on de palabras en el corpus no es aleatoria ya que, si lo fuera, la aparici\u0013 on de una palabra no podr\u0013 \u0010a informar- nos acerca de la aparici\u0013 on de otras. Por ejem- plo, si en un texto aparece la palabra caballo , existe una probabilidad de que tambi\u0013 en apa- rezcan otras palabras de su campo sem\u0013 antico, y esta probabilidad se incrementa cuanto m\u0013 as especializada sea esta unidad. De esta forma, si encontramos una unidad como trastorno obsesivo compulsivo , existe una alta probabi- lidad de encontrar otras que tienen relaci\u0013 on con este trastorno, t\u0013 erminos de la psiquiatr\u0013 \u0010a tales como los s\u0013 \u0010ntomas asociados o los f\u0013 arma- cos que se utilizan para tratarlo. No todas las unidades del vocabulario po- Inducci\u00f3n autom\u00e1tica de una taxonom\u00eda multiling\u00fce de marcadores discursivos: primeros resultados en castellano, ingl\u00e9s, franc\u00e9s, alem\u00e1n y catal\u00e1n 131seen esta propiedad, es decir, esta misma can- tidad de informaci\u0013 on, ya que encontramos tambi\u0013 en palabras en este sentido mucho me- nos informativas: su aparici\u0013 on en el texto no ayuda a predecir la aparici\u0013 on de otras. Este es el caso de los MD, palabras funcionales cuya aparici\u0013 on no tiene relaci\u0013 on con el contenido de los textos en los que aparecen. Es posible apreciar esta diferencia de ma- nera gr\u0013 a ca. En el primer caso, la Figura 1 muestra la distribuci\u0013 on de frecuencias de las palabras que aparecen en los contextos de aparici\u0013 on de democracia , en un corpus en cas- tellano, excluyendo gramemas (preposiciones y art\u0013 \u0010culos). Como puede apreciarse, el con- junto de las oraciones que contienen esta pa- labra contienen tambi\u0013 en un grupo relativa- mente amplio de otras unidades que apare- cen con alta frecuencia, tales como humanos, respeto, libertad, etc. Es en este sentido que decimos que la aparici\u0013 on de la palabra demo- cracia nos permite predecir la aparici\u0013 on de otras palabras. Figura 1: Distribuci\u0013 on de frecuencias de las palabras que coocurren con la expresi\u0013 on de- mocracia . Se trata, sin duda, de una propiedad uni- versal del lenguaje, en el sentido de que todas las lenguas ofrecer\u0013 an un comportamiento si- milar. No es, sin embargo, el caso de todas las unidades del vocabulario, ya que no ser\u0013 a posible predecir qu\u0013 e palabras van a coocurrir con aquellas que tienen un signi cado funcio- nal en lugar de l\u0013 exico. En este sentido es que se puede decir que estas palabras tendr\u0013 an un comportamiento parecido al de una variable aleatoria y, por tanto, su cantidad de infor-maci\u0013 on ser\u0013 a mucho m\u0013 as baja. Ser\u0013 \u0010a el caso de una expresi\u0013 on como de todas maneras en el mismo corpus (Figura 2). Figura 2: Distribuci\u0013 on de frecuencias en el caso de de todas maneras . Comparativamente, las unidades de voca- bulario que aparece en las oraciones de unida- des funcionales presentan muy baja frecuen- cia de coocurrencia y son, adem\u0013 as, ellas mis- formas poco informativas ( ser\u0013 \u0010a, siendo, etc.). No siempre funcionar\u0013 a esta dis- tinci\u0013 on, ya que hay MD como por un lado o por una parte que s\u0013 \u0010 permiten la predicci\u0013 on de otras unidades. Pero al menos es posible una primera divisi\u0013 on del vocabulario en dos clases (palabras informativas vs. palabras no informativas), y los MD genuinos que queden excluidos aqu\u0013 \u0010 se podr\u0013 an recuperar m\u0013 as tar- de (apartado 3.4). La divisi\u0013 on se lleva a cabo utilizando el coe ciente (1), que pone en con- traste la suma de las frecuencia de los coocu- rrentes y la frecuencia de la unidad elegida como diana. I(x) =log2Pn i=1Rx;i log2jm(x )j(1) Conm(x) nos referimos a los contextos de una unidad xyRx;ies la frecuencia de la unidad ien el ranking de los nvocablos m\u0013 as frecuentes en esos contextos (en nuestros ex- perimentos, n= 20). Este coe ciente asigna a cada unidad un valor num\u0013 erico y, por lo tanto, continuo, en lugar de una separaci\u0013 on discreta entre dos clases. Ello obliga a elegir un valor de corte arbitrario kpara poder es- tablecer la clasi caci\u0013 entre la \u0010a 132C(x) =\u001a L I (x)> k Fotherwise(2) Para llevar a cabo esta tarea de clasi ca- ci\u0013 on, todas las unidades l\u0013 exicas del corpus de- ben ser analizadas. Esto requiere la de nici\u0013 on de un vocabulario V, en el que 8x2V,xde- be ser una palabra o una secuencia de hasta cuatro palabras. En cuanto al material desde el cual obtener esta informaci\u0013 on, bastar\u0013 \u0010a con la utilizaci\u0013 on de un corpus monoling\u007f ue lo su- cientemente grande como para disponer de unos 5.000 contextos de cada unidad analiza- da. Sin embargo, como posteriormente vamos a necesitar un corpus paralelo de todos mo- dos, utilizamos para todas las operaciones el mismo corpus, el Opus Corpus ofrecido por Tiedemann (2012). 3.2 Organizaci\u0013 on en grupos de los MD extra\u0013 \u0010dos El paso anterior permite obtener, por cada lengua l(en, fr, es, de, ca), un conjunto MD l de candidatos. El paso siguiente consiste en- tonces en la agrupaci\u0013 on de estas unidades en conjuntos funcionales, para lo cual utilizamos el ya mencionado corpus paralelo. Es preciso observar aqu\u0013 \u0010 algunas de las particularidades del Opus Corpus. Se trata de un conjunto de archivos TMX que se ofrece en pares de lenguas, t\u0013 \u0010picamente en 30 archi- vos por par, en el que cada uno representa un corpus. Cada corpus re\u0013 une material de una determinada \u0013 area tem\u0013 atica o de especialidad, aunque tambi\u0013 en se encuentra material que corresponde al vocabulario general. Los ar- chivos se encuentran alineados generalmente a nivel de oraci\u0013 on. La cantidad total de mate- rial disponible var\u0013 \u0010a, por supuesto, seg\u0013 un las lenguas elegidas, pero en el caso de las len- guas europeas, cada par est\u0013 a en torno a los 3.500 millones de palabras. En primer lugar, para poder agrupar los ejemplares de MD obtenidos en el la Secci\u0013 on 3.1, es necesario encontrar los equivalentes de cada uno en otra lengua. Esto es lo que lleva a trabajar por pares de lenguas y, por ende, a la utilizaci\u0013 on de corpus paralelos. Por una cues- ti\u0013 on pr\u0013 actica (la mayor disponibilidad de ma- terial) estos pares suelen involucrar al ingl\u0013 es como una de las lenguas, con excepci\u0013 on del catal\u0013 an, donde tiene m\u0013 as sentido utilizar el par castellano - catal\u0013 an, que es mayor que el par ingl\u0013 es - catal\u0013 an. As\u0013 \u0010, para el caso de un par cualquiera, como por ejemplo castellano- catal\u0013 an, para la alineaci\u0013 on de los conjuntos MD esyMD caen un listado de equivalentes, utilizamos un coe ciente de asociaci\u0013 on basa- do en un criterio de coocurrencia (3) para en- contrar la asociaci\u0013 on entre un candidato ien castellano (como, por ejemplo, en todo caso ) y uno jen catal\u0013 an (tal como en tot otros como el de la simi- litud ortogr\u0013 a ca para el caso de los cognados que son frecuentes en lenguas emparentadas, por ejemplo, nuevamente, el caso del par cas- tellano - catal\u0013 an. Pero se ha preferido dejar ese recurso de lado para simpli car al m\u0013 axi- mo el m\u0013 etodo. El prop\u0013 osito de alinear los MD en pares de lenguas es \u0013 unicamente poder agrupar despu\u0013 es los MD de una misma lengua en funci\u0013 on de los equivalentes que comparten en la otra. De esta manera, se descubrir\u0013 a la similitud entre dos MD en castellano tales como en todo ca- soyen cualquier caso por su mutua relaci\u0013 on de equivalencia con un MD en catal\u0013 an como en tot cas . Un aspecto clave de este proceso es que un mismo MD puede ser alineado con distintos equivalentes en otra lengua. Esto su- cede con mayor frecuencia en el caso de los MD que en el resto de las unidades l\u0013 exicas. Para el descubrimiento de estas relaciones de similitud es preferible evitar el uso de al- goritmos de clustering aglomerativo. En lugar de esto, se opt\u0013 o por un m\u0013 etodo alternativo de mayor simplicidad. nuevo m\u0013 est\u0013 a ins- pirado en las din\u0013 amicas sociales que pueden observarse, por ejemplo, en la forma en que se aglutina la gente en las pausas de caf\u0013 e de los congresos. Imaginamos un espacio en el que entran personas de a pares, ya que es la situaci\u0013 on que tenemos con nuestros MD alineados. El primero puede ser un par cual- quiera, como por ejemplo en todo caso yen tot cas . Si un segundo par que entra no tiene relaci\u0013 on con el anterior, entonces permane- cen como dos grupos independientes. Ser\u0013 \u0010a el caso, por ejemplo, de un par como en otras palabras yen altres paraules. Ahora bien, si se presenta un tercer par constituido por en cualquier caso yen tot cas, en ese caso este nuevo par es asimilado el grupo 1, como si en cualquier caso fuese presentado a en todo ca- Inducci\u00f3n autom\u00e1tica de una taxonom\u00eda multiling\u00fce de marcadores discursivos: primeros resultados en castellano, ingl\u00e9s, franc\u00e9s, alem\u00e1n y catal\u00e1n 133soporen tot cas. Esta din\u0013 amica continuar\u0013 \u0010a de la misma forma, creando distintos grupos, hasta agotar la cantidad de pares alineados. El proceso resulta econ\u0013 omico porque no hay una tabla de distancia en la que se comparen todos los MD entre s\u0013 \u0010. En cambio, cada par se va comparando con cada uno de los grupos creados hasta el momento. El orden en el que son examinados los pares es aleatorio. 3.3 Etiquetado de los grupos con categor\u0013 \u0010as funcionales El paso anterior resulta en un n\u0013 umero inde- terminado de clusters de MD en cada lengua y que se presentan a su vez alineados entre s\u0013 \u0010. Por ejemplo, el cluster que re\u0013 une los conec- tores contraargumentativos en ingl\u0013 es aparece alineado con el cluster correspondiente en el resto de las lenguas. Estos grupos, sin em- bargo, no poseen un nombre, tal como suele suceder con el resultado de cualquier proce- so de clustering. Esto es, el algoritmo re\u0013 une estos conectores por su similitud, pero no los etiqueta con la categor\u0013 \u0010a correspondiente. Ante este resultado, interesa proporcio- nar una etiqueta a cada cluster por un cri- terio l\u0013 ogico de ordenamiento pero tambi\u0013 en para facilitar el descubrimiento de las rela- ciones que es posible percibir a simple vista entre algunos clusters. Con este n, tal co- mo adelantamos ya en la introducci\u0013 on, uti- lizamos Mart\u0013 Zorraquino y procedimiento es tambi\u0013 en aqu\u0013 \u0010 bastante sim- ple. Gracias a que estos autores proporcionan varios ejemplos por cada una de estas cate- gor\u0013 \u0010as, es posible encontrar coincidencias (4) entre los miembros de cada una de las cate- gor\u0013 \u0010as miem- bros cada cluster se se- leccionar\u0013 a la categor\u0013 \u0010a que ofrezca la coinci- dencia m\u0013 as Mart\u0013 \u0010n Zorraquino y el c\u0013 alculo de la in- tersecci\u0013 on solamente puede hacerse con los clusters que est\u0013 an en castellano. Pero esto, por supuesto, no representa un problema de- bido a que los clusters est\u0013 an alineados inter- ling\u007f u\u0013 \u0010sticamente. De este modo se consiguetambi\u0013 en el efecto deseado de agrupar clus- ters que pueden corresponderse a una misma categor\u0013 \u0010a funcional. 3.4 Poblamiento de la taxonom\u0013 \u0010a con nuevos ejemplares El resultado del paso anterior es una taxo- nom\u0013 \u0010a A partir de este punto, dicha taxonom\u0013 \u0010a puede ser enriquecida mediante la adici\u0013 on de nuevos MD extra\u0013 \u0010dos del corpus. Para cualquier nuevo candidato a MD ( c), la existencia de la TMD posibilita decidir si ces efectivamente un MD y, si efectivamen- te lo es, asignarle una categor\u0013 \u0010a. Para ambas tareas recurrimos nuevamente al corpus pa- ralelo inicial. Si un candidato ces un MD genuino, en- tonces su condici\u0013 on ser\u0013 a delatada por la pre- sencia de otros MD de la otra lengua en los pares alineados, que ahora es posible descu- brir sin di cultad gracias a paralelo franc\u0013 es-ingl\u0013 es, caparece alinea- do con elementos tales como in the same way, likewise, similarly , etc., \u0010a a c, operamos de manera si- milar a 3.3, eligiendo la categor\u0013 \u0010a que ofrece la coincidencia m\u0013 as alta. En el caso del ejem- plo, esta corresponder\u0013 \u0010a a la de los conectores aditivos. 4 Resultados En el momento actual, los resultados del pro- yecto implican la creaci\u0013 on de una TMD mul- tiling\u007f ue de divididos funcionales. Todav\u0013 \u0010a no ha comen- zado el proceso de poblamiento masivo de es- ta taxonom\u0013 \u0010a, pero s\u0013 \u0010 ha sido posible comple- tar una primera fase de evaluaci\u0013 on de la me- todolog\u0013 \u0010a empleada en el proceso. Esta eva- luaci\u0013 on consiste en medir la capacidad del al- goritmo para distinguir entre un MD genuino y una unidad l\u0013 exica de otra categor\u0013 \u0010a. La tabla 1 muestra un ejemplo de cluster que corresponde a la categor\u0013 \u0010a de los conecto- es (1999). Un grupo de ling\u007f uistas hablantes nativos de cada una de las lenguas analizadas llev\u0013 o a cabo una revisi\u0013 on manual de los resultados para evaluar si la selecci\u0013 on de marcadores era correcta. Es importante aclarar que lo que se Rogelio Nazar 134Ingl\u0013 es all cambio; ahora al contrario; aparte de eso; a pesar de ello; a pesar de eso; a pesar de esto; a pesar de todo; cualquier de cualquier modo; de todas formas; de todas maneras; de todos modos; dicho esto; en cambio; en lugar de eso; en vez de incluso aunque; no obstante; pero; aun as\u0013 a ello; pese a todo; por el contrario; si bien ; sin embargo; todo lo contrario; y i aix\u0013 \u0010; tot i aix\u0012 o Tabla 1: Ejemplo de uno de los clusters que corresponde a la categor\u0013 \u0010a de conectores contraar- gumentativos. revis\u0013 o fueron listados de MD fuera de con- texto. Esto se debe a que analizar instancias de estas unidades en textos particulares equi- valdr\u0013 \u0010a a una tarea diferente, ya que una mis- ma unidad puede funcionar como MD en un contexto y en otro no. La revisi\u0013 on revel\u0013 o que los datos son de buena calidad, con una pureza en torno el 95 % de media en las distintas lenguas con excepci\u0013 on del alem\u0013 an, donde la precisi\u0013 on al- canz\u0013 o el 84 %. Las razones del desempe~ no in- ferior en alem\u0013 an no est\u0013 an del todo claras, pe- ro probablemente puedan estar relacionadas con las caracter\u0013 \u0010sticas morfol\u0013 ogicas de esta lengua. Esto debe continuar estudi\u0013 andose en trabajo futuro. Otra caracter\u0013 \u0010stica llamativa de los resultados es que en general parece ha- ber una tendencia a tener una cantidad de MD en castellano ligeramente mayor que en las otras, como si esta lengua permitiese ma- yor diversidad en el uso de estas part\u0013 \u0010culas. Nuevamente, esto debe profundizarse en un estudio contrastivo entre las diferentes len- guas. La presente investigaci\u0013 on no ha preten- dido, en todo caso, dar respuesta a estos in- terrogantes sino ofrecer una propuesta meto- dol\u0013 ogica para la obtenci\u0013 on de los datos. En relaci\u0013 on con el desempe~ no general del algoritmo en comparaci\u0013 on con otros traba- jos mencionados en la Secci\u0013 on 2, es posible a rmar que los resultados obtenidos con el presente m\u0013 etodo son m\u0013 as numerosos y pre- sentan menor tasa de error. Particularmente en el caso de Robledo y Nazar (2018), que es el m\u0013 as comparable en t\u0013 erminos de meto-dolog\u0013 \u0010a aunque trabajen m\u0013 etodo presentado aqu\u0013 \u0010 es m\u0013 as sensible a los elementos de mediana y baja frecuencia, y la tasa de error a la hora de extraer MD es inferior. Hay que se~ nalar, de cualquier mane- ra, que los objetivos de ambos estudios son distintos. En el caso del estudio anterior se trataba de encontrar categor\u0013 \u0010as de MD. En el presente estudio, en cambio, el foco est\u0013 a puesto en reunir un listado exhaustivo de MD particulares. Para complementar la evaluaci\u0013 on manual general y poner en perspectiva los resultados, invitamos a un grupo de estudiantes avanza- dos en licenciatura en ling\u007f u\u0013 \u0010stica a participar de un experimento de evaluaci\u0013 on. En total participaron 6 j\u0013 ovenes, que fueron elegidos entre los que mejores cali caciones obtuvie- ron en la asignatura de Gram\u0013 atica del Tex- to, de la Ponti cia Universidad Cat\u0013 olica de Valpara\u0013 \u0010so, que trata de manera extensiva el tema de los MD. Cada estudiante recibi\u0013 o una planilla con 720 unidades en castellano en los cuales se mezclaron MD aut\u0013 enticos con palabras o se- cuencias de palabras correspondientes a otras diversas categor\u0013 \u0010as. La proporci\u0013 on fue de dos tercios de MD. La instrucci\u0013 on era marcar con un 1 cada unidad que consideraran co- mo MD. No se les permiti\u0013 o consultar diccio- narios ni ning\u0013 un otro recurso y la tarea era individual, sin posibilidad de dialogar con los compa~ neros. Tambi\u0013 en se les pidi\u0013 o que con a- ran en su primera intuici\u0013 on como hablantes, sin dedicar mucho tiempo a cada decisi\u0013 on. La Inducci\u00f3n autom\u00e1tica de una taxonom\u00eda multiling\u00fce de marcadores discursivos: primeros resultados en castellano, ingl\u00e9s, franc\u00e9s, alem\u00e1n y catal\u00e1n 135misma tarea fue realizada por el algoritmo, es decir la de aceptar o rechazar los candidatos del mismo listado. En la Tabla 2 se muestran los resultados de cada uno. Anotador Pre Rec F1 Algoritmo 97 94 95 Estudiante 1 96 50 65 Estudiante 2 95 60 73 Estudiante 3 95 41 57 Estudiante 4 95 59 72 Estudiante 5 94 65 76 Estudiante 6 92 75 82 Tabla 2: Comparaci\u0013 on del desempe~ no entre algoritmo y humanos en la tarea de separar MD de unidades l\u0013 exicas (precisi\u0013 on, cobertura y F1). En general, todos los estudiantes tuvieron un buen desempe~ no en t\u0013 erminos de precisi\u0013 on, en el sentido de que, si seleccionaban una uni- dad como MD, casi siempre la decisi\u0013 on era correcta. El problema en general es que tu- vieron tendencia a ser poco exhaustivos. En comparaci\u0013 on con los estudiantes, el algoritmo present\u0013 o m\u0013 as o menos la misma tasa de pre- cisi\u0013 on, pero la tasa de cobertura fue mayor. En una serie de entrevistas realizadas con posterioridad a la entrega del ejercicio, casi todos los estudiantes coincidieron en explicar que adoptaron una actitud conservadora, de modo que ante la duda pre rieron no elegir unidades que, aunque puedan cumplir la fun- ci\u0013 on de un MD, no presentan todav\u0013 \u0010a las mar- cas de los MD protot\u0013 \u0010picos o que todav\u0013 \u0010a no han nalizado su proceso de gramaticaliza- ci\u0013 on. Unidades como en estas circunstancias oen t\u0013 erminos m\u0013 as generales, por ejemplo, fueron rechazadas en la mayor\u0013 \u0010a de los casos a pesar de que en el listado original guraban como MD aut\u0013 enticos. En otros casos, los es- tudiantes consultados hicieron referencia a la alta polifuncionalidad (Pons Border\u0013 \u0010a y Fis- cher, 2021) de los candidatos inspeccionados, es decir, algunas unidades podr\u0013 \u0010an funcionar como MD solo en algunos casos muy espec\u0013 \u0010 - cos, mientras que en general no tendr\u0013 \u0010an esa funci\u0013 on. Este ejercicio puso de mani esto el proble- ma de la falta de acuerdo entre los hablantes acerca de lo que es un MD y tambi\u0013 en la di- cultad de tratar con MD fuera de contexto. M\u0013 as bien, lo propio ser\u0013 \u0010a decir que una de- terminada unidad funciona como MD en uncontexto determinado. Esto representa una interesante v\u0013 \u0010a de trabajo futuro pero, nue- vamente, trasciende el objetivo de la presente investigaci\u0013 on. 5 Conclusiones Este art\u0013 \u0010culo ha presentado una nueva pro- puesta metodol\u0013 ogica para la extracci\u0013 on au- tom\u0013 atica de una base de datos multiling\u007f ue de MD, incluyendo una evaluaci\u0013 on de sus pri- meros resultados. Dicha propuesta es original y, en comparaci\u0013 on con trabajos aparecidos recientemente sobre el mismo tema, resulta m\u0013 as simple en t\u0013 erminos conceptuales, de de- pendencia de recursos y en materia de coste computacional. Esto resulta de gran impor- tancia para la reproducci\u0013 on de los experimen- tos en distintas lenguas. La base de datos de MD desarrollada has- ta el momento se encuentra disponible para su descarga desde la p\u0013 agina web del proyecto (cf. nota 1) y, aun trat\u0013 andose de un traba- jo en curso, puede ya servir para m\u0013 ultiples prop\u0013 ositos. Posibles usuarios nales pueden ser traductores o redactores, y posiblemen- te tambi\u0013 en docentes de L1 o L2. Los datos pueden ser \u0013 utiles tambi\u0013 en para la comunidad del PLN, ya que pueden emplearse para di- versidad de tareas vinculadas con el an\u0013 alisis discursivo y la extracci\u0013 on de informaci\u0013 on. Muchas tareas han quedado pendientes, como continuar explorando distintas varia- ciones en la metodolog\u0013 \u0010a. Esto puede incluir probar con categor\u0013 \u0010as distintas para la cla- si caci\u0013 on, probar distintos tama~ nos para la ventana de contexto y hacer un estudio m\u0013 as riguroso del desacuerdo entre anotadores en las distintas lenguas. Otras posibilidades de trabajo futuro ser\u0013 \u0010an reproducir experimen- tos en otras lenguas y, nalmente, una v\u0013 \u0010a que parece atractiva es la de utilizar la taxo- nom\u0013 \u0010a creada hasta el momento para el des- cubrimiento de MD polifuncionales. Agradecimientos Esta investigaci\u0013 on ha sido nanciada por el Gobierno de Chile a trav\u0013 es del Proyecto Fon- decyt ati- ca de \u0010as de marcadores discursivos a partir de corpus multiling\u007f ues (2019-2021). Agradezco a los revisores por sus comentarios y a Irene Renau, por ayudarme a mejorar el art\u0013 de part\u0013 nol. Calsamiglia, y A. Tus\u0013 on. 1999. Las cosas del decir: manual de an\u0013 alisis del discurso . Ariel, Madrid. Cardona, A. L. 2014. Aproximaci\u0013 on funcio- nal a los marcadores discursivos. An\u0013 ali- sis y aplicaci\u0013 on lexicogr\u0013 a ca . Peter Lang, Frankfurt am Main. Casado Velarde, M. 1993. Introducci\u0013 on a la gram\u0013 atica del texto del espa~ nol. Arco libros, Madrid. Fraser, B. 1999. What are discourse mar- kers? Journal of Pragmatics, (31):931{ 952. Gili Gaya, S. 1943. Curso superior de sinta- xis espa~ nola. Minerva, Mexico. Halliday, M. y R. Hasan. 1976. Cohesion in English. Longman, London. Knott, A. 1996. A coherence relations tesis, University of Edinburgh, UK. British Library, EThOS. Lopes, A., D. M. de Matos, V. Cabarr~ ao, R. Ribeiro, H. Moniz, I. Trancoso, y A. I. Mata. 2015. Towards using machine translation techniques to markers. Mart\u0013 \u0010n Zorraquino, M. A. y J. Portol\u0013 es. 1999. Los marcadores del discurso. En Gram\u0013 atica Descriptiva de la Lengua p\u0013 aginas 4051{ 4214.Montol\u0013 Conectores de la lengua escrita. Contraargumentativos, consecuti- vos, aditivos y organizadores de la infor- maci\u0013 on. Ariel, Barcelona. Moore, J. D. y P. Wiemer-Hastings. 2003. Discourse in computational linguistics and arti cial intelligence. En A. C. Graesser A. Gernsbacher, y S. R. Fun- A study with discourse segmentation to account for the polyfunctionality of discourse mar- kers: The case of well. Journal of Pragma- tics, 173:101{118. Robledo, H. y R. Nazar. 2018. Clasi ca- ci\u0013 on automatizada de Lenguaje Natu- ral, (61):109{116. Roze, C., L. Danlos, y P. Muller. a french lexicon discourse connec- tives. de linguistique, psycholinguistique et \u0010o, L. 2003. Diccionario de part\u0013 \u0010cu- las. Luso-espa~ nola de ediciones, Salaman- 2001. H. Hamilton, Analysis. Blackwell, Oxford, p\u0013 Sileo, D., T. Van De Cruys, C. Pradel, y P. Muller. 2019. Mining discourse for unsupervised sentence representa- tion learning. En Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technolo- gies, Volume 1 (Long and Short Papers) , p\u0013 aginas 3477{3486, Minneapolis, Minne- sota, Junio. Association nal Linguistics. Inducci\u00f3n autom\u00e1tica de una taxonom\u00eda multiling\u00fce de marcadores discursivos: primeros resultados en castellano, ingl\u00e9s, franc\u00e9s, alem\u00e1n y catal\u00e1n 137Stede, M. 2002. DiMLex: A lexical ap- proach to discourse En y V. D. Tomaso, editores, and Computation. Edi- dell'Orso, Alessandria. Stubbs, M. 1983. Discourse Analysis. The Sociolinguistic Analysis of Natural Language. University of Chicago Press, Chicago. Stubbs, M. 1996. Text and Corpus Analysis . Blackwell, Oxford. Tiedemann, J. 2012. Parallel data, tools and interfaces in OPUS. En Procee- dings of the Eighth International E. y R. Dasher. 2002. Regularity in semantic change. Cambridge University Press, New York. Urgelles-Coll, M. 2010. The Syntax and Semantics of Discourse Markers. Conti- nuum, London. van Dijk, T. 1973. Text Grammar and Text Logic. En Studies in Text Grammar . Reidel, Dordrecht, Specialized Corpus Extracci\u00f3n de T\u00e9rminos Relacionados Sem\u00e1nticamente con Colp\u00f3nimos: Evaluaci\u00f3n en un Corpus Especializado de Peque\u00f1o T ama\u00f1o Juan Rojas -Garcia Universi ty of ts the geographic were extract terms re lated to colponym mentioned in their semantic relation s. Since the evaluation of DSMs in small, specialized corpora has received little attentio n, this study identif ied both parameter combinations in DS Ms and five similarity/distance measures suitable for the extr action of terms which relate d colponyms through semantic rel ations takes_place_in , ones ; the similarity/distance measures performed quite similar except for the Euclide an distance ; and the detection of a specific relati on base de conocimiento termin ol\u00f3gica sobre el medioam biente, c uyo dise\u00f1o permite la contextualizaci\u00f3n geogr \u00e1fica de colp\u00f3nimos, esto es, bah\u00edas con nombre propio (BNP) (v.gr., Bah\u00eda de Pensacola ). Se aplicaron model os sem\u00e1n ticos distribucionales ( MSD ), basados en recuento s y predictivo s, a un corpus espec ializado de peque\u00f1o tama\u00f1o en ing l\u00e9s para extraer t\u00e9rminos relacionados con l as BNP y sus relaciones sem\u00e1nticas. Puesto que la evaluaci\u00f3n de MSD en corpus especi alizado s de peque\u00f1o tama\u00f1o ha sido menos explo rada, en este art\u00edculo se identifica n tanto la combin aci\u00f3n de par\u00e1met ros como la s cinco medida s de similitud adecuada s para extraer t\u00e9rminos q ue man tengan con las BNP las relaciones tiene_lugar_ en, localizado_en y atribu conju ntos de datos anotados m anualmen te. Los resultados indican que : los modelos basados en recuento s superan a los modelos predictivos ; las medidas d e similitud brindan resultados s emejantes , excepto l a distancia eucl\u00eddea ; y la detecci\u00f3n de una relaci\u00f3n espec\u00edfica dep ende del tama\u00f1o de la vent ana contextual. Palabras cla ve: Colp\u00f3nimo, Terminolog\u00eda, R d in s pecialized texts the environment, their representa tion and inclusion in terminological knowledge bases ( TKBs) have received little attention, as evidenced by the lack of named landforms in t erminological resources for the as DiCoEnviro1, https://cutt.ly/cbATjnQ 2 Term contra River Nile ), and colponyms (e.g., San Francisco Bay), is barely ta ckled in terminological resource s for two reasons, in our op inion: They are n\u00ba septiembre de 2021, pp. 139-151 \u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural mere ins tances (i.e., examples) of concepts such as BEACH , RIVER , or BAY, and their relatio nal behavi or with other concepts in a speci alized knowled domain is described; (2) their are landf orm, and how these terms are related to ea ch other . This is evidently a time -consuming task taki ng into accoun t that te rminologists do not often reso rt to natural language processing (NLP) systems beyond corpus query tools such as Sketch Engine (Kilgarriff A. et al., 2004) . As a result , knowledge resources h ave limited themselves to representing concepts such as BAY, RIVER or BEACH, on the questionable assumption that the co ncepts linked to e ach of them are also related to all named bays, rivers and beaches to th is assumption , Rojas Faber P. (2019a and multilingual TKB on env ironmental science tha t is the practical application of Frame-based Terminology ( Faber P., 2012). The permits the representation and contextual ization of data so that they a re more rele vant to sp ecific s ubdomains, communicativ e situations, and geographic areas. With the ultimate of repre senting in EcoLexicon a small -sized, English specialized corpus on Costal Engineerin g (7 million tokens), the terms related t o each named landf orm tic relations have be manually extracted from the corpus by quer ying it in Sketch Engine . In this work, we focus ed on colponyms (Room A., 199 6: 23), namely by named bays in the corpu s. Since this is a time-consuming task, th e overall aim of thi s study was to provide terminologist s with three lists of term candidates in DSMs and s imilarity/distance measures suitabl e for the extraction of those terms from the small specialized corpus mentio ned above . 6 http://ecolexicon.ugr.es Hence, t were using evaluation same One of the terms was always a colponym , and the other one was either a proce ss (e.g., storm surge ), an entity (e.g., benthic geo logic habitat), or y (e.g., water quality ). The semantic relatio ns that link ed were: (a) takes_place_in (e.g., STORM SURGE takes_place_in be seen terms that hold these relation s name bays and also conducted for named riv ers by Rojas -Garcia J. Faber P. name d rivers, the w indow size ha d to be 3 words to extra ct terms linked to rivers with the relatio n, whereas in the case of named bays, the same window size of 3 it is not possible to generali ze the results from named rivers ual as previou stated, this considered \"case as a \", but rather as a research objective itself. Besides the analysis of different D SMs and similarity measures for a small -sized, specialized corpus , an important contribution of this work is the creation of both the corpus on named landforms in the Coastal Engineering domain, and the three gold standard dat asets for information retrieval system evaluation . The rest of th is paper is or ganize d as f ollows. Section 2 provides background on DSM s, as wel l as a literature review on their a pplication and evaluation . Section s 3 and methods, and DSMs this stu dy, and the constr uction of the g old standard d ataset s. Section 5 shows the results obtained. Finally, Sec the res ults, and presents the conclusions derived from this work along with plans for future research. Juan Rojas-Garcia 140 2 Background and Literature Revi ew models ing of a a vector, based on its statist ical co-occurrenc e with other the the distributional hypothesis, semantic ally similar terms have di stributions and Charles W.G. Huang A. (2008) for a detailed des cription of the se five measures ). Depend ing on the la nguage model (B aroni M. et al., 2014), DSMs are count r gi ven number of terms on eithe r side of the target te rm). Corre lated Occurren ce Analogue to Lexical Semantic (COALS) (Rohde D. et al., 2006) is a n examp le of th is type of mod el. Predictio n-based models exploit probabil istic language models, erms by predi the next (Bojanowski P. et al., 2017 ), and state-of-the-art transfer learning models such as BERT ( Devlin J. et al., 2019). Instead, GloVe model (Pennington J. et al., 2014 ) Clark S., 2014; Lapesa G. et al., 2014; Sahlgre n M. and Lenci A., 2016). Research show s that parameters, such as context w indow size, relations examined in much research are either phrasal associates (e. g., help - wanted) (Lapesa G. et al., 2014) or syntagmat ic predicate preferenc es (Erk K. et al., 2010) in general language. The present study focused on the specific syntagmatic relations takes_place_in , located_ relations activated in the Engineering in our c orpus. Count -based mode have also been ently compared . Baroni M. al. (2014) contrast ed them on seve ral datasets and found that the prediction -based models provided better resu lts. In contrast, Ferr et O. (2015) found that count -based models performed better. In another study that compare d the abil ity of both DSMs to semantic rela tions detected by the DSMs depend ed on the window si ze, but al so that the values of this parameter mostly coincided in both O. (2015) yielded valuable insights, show ing the following: (1) When the paramete rs of the model s were correctly tuned, count and prediction -based mod els racy; and (2) the best m odel depend ed on the task to be carried out . Nevertheless, A sr F. et al. (2016) , Sahlgren M. and Lenci A. (2016) , and Nematzade h A. et al. (2017) reporte d that count -based models outperform ed predic tion-based ones on small -sized corpora of 10 illion tokens. Work in lexical sema ntics and DSMs rel (Bertels verbs into semantic groups (Gries S. and Stefanowitsch A., 2010), and the use of word vec tors as features for automatic reco gnition of text corpora (El Bazi I. and Laachfoubi N., 2016). 3 Materials 3.1 Corpus Data The colponyms and related terms were extracted from exts on Coastal Engi . This subcorpus, compris ing is an integral part of the EcoLexicon E nglish Corpus (23.1 million tokens) (Le\u00f3n-Ara\u00faz P. et al., 2018 ). It is wo rth clarifying that we were interested in the semantic beh avior colponyms in . Since this behavior of colponyms , like that of all specialized terms, is different in the specialized language than it is in the general language (Pearson J., 1998 ; Sager J.C. et al., 1 980), from a n epistemological and methodological point of view, it makes no sense to expand our corpus neither with a genera l language corpus such as Wikiped ia, nor with other specialized with topics other than Coastal Engineering. Furtherm ore, the domain of the training has an impact on the sema ntic a doma in-specific word Evaluationin a Small Specialized Corpus 141 (Chen Z. et al., 2018). Consequently, it also makes no sense to create s automatic o f the colponyms corpus was performed with a GeoNames database dump. GeoNames7 has over 10 ent designations, designation s, latitude, lo ngitude, an d location name i s stored. 3.3 Gold Stan dard Datasets The DSMs , built on our domain -specific corpus, were evaluated on gold standard data. We were unab le to find gold standard res ources tal Engineering. , the gold standard data were manually extracted from the same corpus and assesse d by Termino logy expert s on Coastal Engineering a common both in (Manning doing so, the research community could also employ our corpus and the gold standard dat a as test collection for the evaluation of systems dealing with semant ic relation extraction from specialized corpora. 8 of designa tions and eaning of these relations are those used in EcoLexicon (Faber P. et al., 2009 ). The three semantic relations always lin ked the norma lized designation of a colponym (e.g., Josiah's Bay and Josiah Bay were normalized to Josias Bay ) to either a process, an entity, or (e.g., flooding ) or More specifically, the takes_place_in relation holds between a process (e.g., storm surge) and the bay where th e process occurs (see Table of an entity (e.g., inundation a bay (see Table 2). _of is used for terms that designate propertie s (e.g., wind speed) of a bay (see Table 3). 7 http://www .geonam es.org 8 The dat asets and the corpus will be av ailable on th e website of the LexiCon researc h group of the University o f Spain ) (http://lexicon.ugr.es/ ). from corpus: (1) Within the Pensacola Bay and Escambia Bay, the sh allow Extract fro m the geolog ic hab itat located_at Greenwich Bay Example from the c orpus : (1) The Port Geelong located on Port Phillip Bay has a coastal governance arrangements . the located_at mark attribute _of Pensacola Bay Example from the the simulated and observed high wate r marks at six stations Bay agree ... Table Extract from for the attribute_of relation the three datasets for relation , which w ere all used for the eva luatio n, therefore, the three dat asets added up to 300 tri plets; (2) the 50 most frequentl y mentioned bays in the corpus , the same 5 0 bays in the three dat asets, since 50 information needs have usually been found to be a sufficient minimum for information retrieval system evaluation (Manning C.D. et al., 2009: 152); and (3) the two most frequen t terms related to the same bay, which amoun ted to 100 triplets , therefore, the same bay w as related to a total of six terms pair terms ext racted from the corpus was c arried ou t by three te rminologis ts from the LexiCon research group of the University of Granada (Granada, Spain ), with wide experience in environmental knowledge representation of inter -annotation agreement, and the scores all the annotator pairs 90% for ed, lowercased with the Stanford CoreNL P package (Manning C.D. et al., 2014) for R programming language . The m ulti-word terms stored in Eco Lexicon were then automatic ally m atched in the lemmatized c orpus an d joined with underscores. In the DSM s, only terms larger than two character s were conjunction s, relative the minimal occurrence frequency was s et to 5 so that the co -occurrences were statistic ally reliable (Evert S., 2008). 4.2 Named Recognition Both norma lized a nd alterna te names o f the bays in GeoNames were searched in the lemmatiz ed Most bays of the corpus were in GeoNames (90%), while others were ide ntified by manual ins pection (10%). Anaphoric e lements referring a bay were by correspo nding colponym in the l the anaphora resol ution fu nction from CoreNLP package was u sed, and other cases were manua lly replaced. The 294 bays mention ed in the corpus are shown on the map in Figure 1. Figure 1: Heatmap with the location 294 named ba 4.3 experimen t involved a comparative evaluation of three types of DSM for a small -sized, specialized corpus, na mely, count -based , vector entation of a term b ased on the contexts in which it ap peared in our corpus. For this study , the contexts of a target t erm (i.e., a colponym ) were the terms that co -occurred with it a s liding context windo w, whic h spanned a terms on either side of th target prediction -based DSMs have various parameters that must b e set to build the model s. The par ameters impinge on both the term representations and of the similarity term vectors when the models are compared M. et al. , 2014). There fore, to assess the influence of the para meters of both DSMs on their abil ity to capture the targeted g the Count -based Models The first model type evaluated w as a count-based model, also called bag -of-words (BOW) model. The BOW mode l was built with the R pa ckage quanteda (Benoit K. et al., 2018 ) for text mining. To bu ild a BOW model, a term-term matrix of co-occurren ce frequenci es was first computed specific si ze for the sliding context window. Then, the was subjected to a specific weighting scheme , namely, an association t he importance of the context terms t hat are more i ndicative of the m eaning of the target term . The 1,000 most frequent terms wer e used, include d all the three evaluation ranging from 1 to 10 words on either side of the target term, and the context window was all owed to span senten ce boundaries. The context window shape was always rectangular (i.e., the increment added t o the co-occurrence frequency of a pair of terms was always 1, regardle ss of the distance betwe en the t The frequen cies observed on the left and right of a target te rm w ere added . With respect to the weighting scheme s, thre e assoc iation mea sures, colloca tion, were and relati ons (Lapesa G. et al., 2014), and perfor m better for me dium- to low-frequenc y data than other association measures (Alra bia M. al., Extraction of Terms Semantically Related to Colponyms: Evaluationin a Small Specialized Corpus 143 2014). PPMI and t-score, on the other hand, have been found to work adequately for dif ferent applications in previous rese arch when comp ared to other association mea sures (Baroni M. et al. 2014 ; Kiela D. and Clark S., 2014). Finally, f ollowi ng Lapesa G. et al. (2014), the associa tion scores were transformed to reduce skewness in this way: l og-likelihood and PPM I scores were b oth transforme d by adding 1 and calculating then the natural logarithm (ln), whereas t-scores Prediction prediction -based els were evaluate d, name ly, the word2vec (Mikolov T. et al., 2013), the fastText P. et al., 2017 ), and the GloVe et the neural n etwork on a corpus accor ding to architectures. The continuous bag-of-words (CBOW) architectur e predict context architect ure predicts terms of a target t erm. The W2V model wa s built with t he origina l word2vec package . 9 For W2V , five hyperparameters w ere examined the same s those tested by Bernier -Colborne G. and Drouin P. (2016) for paradigmatic rel ations and syntactic derivati ves. The first one was the architecture used to learn the term vector s. The second one was the training algorithm, either us ing a hierar by sampling negative examples in which case t he number of negative sample s must be selected . The , are rand omly Finally, the nality of the term vector s, and the size of the context window were the other hyperparameters. The settings tested for each o f the rs Architecture: CBOW or ski p-gram. 2. Negative sampl es: 5, 10 or none (in this c ase, al softmax 0. 5. Size o f context wi ndow: 1 -10 words (FTX), is extens the model , all substrings contain ed in a word between a minimum and a maxim um size. Hence , the vector for a wo rd is made of the sum of th ese subword vector s. The FTX model was bu ilt with the original fastTe xt package .10 For FTX, the same five hyperparameters as thos e for W2V were probed. All the subword s between 3 and 6 characters were taken (default values for the model ). ratios of co-occurrence probabilities as the basis for learning. The model was built w ith the original GloVe packa ge,11 term embeddings: 100 3 00. 2. Size o f context win dow: 1 -10 FTX, the number of epochs was fixed to 1 0, and the learning rate to 0.05. 4.3.3 Pre-trained Models Lenci A., 2010). For that reason, we also assess ed the pre -trained word2vec and fastText models (Mikolov T. et al., 2018 ),12 and the pre-trained GloVe mode l,13 all of t hem trained on the Common Cr awl cor pus (600-840 billio n tokens ) with 300-dimension vectors . considered . The parameter values of the pre -trained were already set in the pre -training phase . and fastText models was fixed to 15 words , and that of the pre-trained GloVe mode l was fixed to 10 words. Consequently , the window size of these three pre -trained models could not be modified for our evaluatio n. This was deemed to be a drawback with resp ect to the overall goal of this st udy, since it aimed to provide termi nologists per relation . Instead, a pre-trained model could only extract single list of term candidates for of the tra ining corpus and vocabulary in the three pre-trained model s, they had less termino logy coverage t han the doma in-specific model s evalu ated in this work . This pitfall has been already reported by Nooralahzadeh F. et al. (2018), and it is hardly surprising given that pre vious studies have terms account for mo re than 90% o f the te rms of a spe cialized knowledge domai n (Krieger MG. and Sager J.C. et al., 1980). As a consequence , since the pre -trained models did not contai n most of the multi-word terms used in our special ized corpus and evaluat ion data (96% of the terms in the gold standard data are multi -word units we called Basic Additive Model (BAM ) (Mitchell J. an d Lapat a M., 2008). BAM com putes the vector of a mu lti-word term by adding its component model ( et al., 2019) was also a single, embedding for each word in a corpus . Instead , BERT is a contextual deep learning model which generates as many representations f or a target word as the number of tim es it appears in a c orpus, since each representati on is based on the other that the target word in each sentence . We employed the uncased version of the BERT -Base model in Python ,14 with 768-dimension vectors. This mo del has 12 encoder layers, 768 hidden units in t he feed-forward networks , and 12 self-attention heads . The terms of our corpus were added to the vocabulary fil e of the model . Each of the contextualized embeddings for a term was obtained by adding up the vectors from the last four encoder layers , a proce dure already applied by Devlin J. et al. (2019). Nevertheless, f or the model evaluation, we used averaged embedding for which all th e different context ualized embeddings fo r the same As of GloVe , W2V , and FTX, the number of epochs was fixed to 1 0, and the learning rate to 0. 05. In addition , the parameter for the maximum sentence length was set to 64 because: ( 1) It is one of the values recomme nded by Devlin J. et al. (2019); and ( 2) the maximum sentence length of our was 57 words. Although there exists the pre-trained SciBE RT model (Beltag y I. et al., 2019 ), based on BERT but trained on a large corpus of scientific text s, SciBERT 14 https://github.com/google -research/bert was not used because the training corpus consisted of paper s from the computer science and biomedic ine domains , which are far from bein g related to the Coastal Engine ering domain of our corp us. In summary, we applied evaluated eight : BOW , W2V, FTX, G loVe, pre-trained BERT, -trained models . 4.4 Evalua tion of the DSMs First, for each bay includ ed in the gold standard datase ts, a sorted list of neighbours was obtained by computi ng a similarity/distance measure between the bay's vector and vectors of othe Subsequently, ts of neighbo urs were evaluated on the whole gold standard dat aset const ructed each to the models computed using unordered sets of items , MAP is more the evaluat levels , and average area under the precision -recall curve for a set of queries . Additionally, MAP has been shown to and , l evel of the sorted of neighbours obtained for all bay queries on the rank of the related terms according to the gold standard. The n earer the related te rms are to the top of the list for each bay, the high er the MAP . The evaluatio n proce ss delineated above the five similarity/distance measures computed between a bay's vector and the vectors of a ll othe r context te rms. The five measures evaluated in space constraints, we refer reader s to Huang A. (2008) for a det ailed description of the properties and formulas of these measure s. 5 Resu lts The eight mode ls were compared by obser ving the MAP of each mo del on the three datasets. Regarding the similarity/distance measures , it was found that, except for the Euclidean distance , which performed the wor st, the other four measures ha d comparab le effectiveness for all the DSMs and semantic relation s, according to the resu lts Terms Semantically Related to Colponyms: Evaluationin a Small Specialized Corpus 145 determine the significance o f the performance -wise differences am ongst t he similarity/distance measures used as the basis of comparison. This behavio r is in line with previous research on similarity measure comparison by Huang A. ( 2008), and Strehl A. et al. ( 2000). For th at reason, Table 4 shows the m aximum M AP ac hieved by each mode l when applied cosi ne similarity, cosine similarity are shown in b rackets, LL stands for the log -likelihood we ighting scheme . The results indica ted that the BO W model obtained the best performance in term s of MAP on the th ree semant ic ately capt ured all thi s relation . The greater accuracy of takes_place_in may be due to the large number of instances in specialized texts in Coastal Engineering pr bays. As instances of both seman tic relati ons in the whole corpus is not large enou gh for the DSMs to represent takes_pl ace_in instan ces. Table 4 also shows that the maximum MAP of the BOW model was achieved when: 1. The statistic al association measure for the semantic relations was log-likeliho od, transforme d by addin g 1 and calculating the natural log arithm. 2. The window size for the takes_place_in relation was 4 words . 3. window size for the attribute_of relation was 3 words. 4. The size for the located_at relation was 2 words. Strikingly, the BERT and the compo sition al . factors are to associated with behavio r. First ly, in NLP systems for is considerably smaller are inefficiently represented in pre-trained since statistical the underlyin g general-domain corpora for M.T. N., 2016) each data set, the max imum MAP of the BOW, GloVe, FTX, and W2V models was reached when the window size was the same . For that reason , to assess the impact of the win dow si ze on the accuracy of the DSMs, the average M AP for each setting of this parameter (i.e., for each window size betwe en 1 and 10 word s) is illustr ated in Figure 2. The average MAP was used, instead of the maximum, because it allowed us to s ettings u sed for the oth er paramete rs. Juan Rojas-Garcia 146 Figure 2: Average MAP upper right ), FTX (middle and (bottom right) w .r.t. window si ze. In Figure 2 we can observe that, in the four DSM s (BOW, GloVe, FTX, and W2 V), the opti mal window the 3 pre owing t o their extrem ely suboptimal performance and the ir fixed window sizes count-based model BOW models the sake of simplicity, the setting influence of the other four FTX and W2V are succinctly reported becau se they did not le accurac y improvements on either summarized worked, CBOW a samp ling o f 10 samples reached a larger MAP tha n the hierarchi cal softmax ; (3) the subsampling threshold was not conducive to significa nt gains; and (4) the optimal the thus in line relations . This behavio r may be li nked to the fact that, as FTX exploit s character -level similarities between ra (Bojanowski P. et al., 2017 : 140-141). Regard ing GloV e, with 300 -dimension vectors, it the ve model whose perform ance reached va lues similar to those of BOW . There is some evidenc e that the generalization ability of neu ral network-based models, such as FT X, W2V, and BERT , decreases w hen they learn on a limited amount of data (Collobert R. et al., 2011). Accord model did not se em undul y affected by the reduced corpus s ize. In order to verify our observation s on the be havio r of the BOW, GloVe, FTX, and W2V models , statistical tests were run to determine the signif icance of the performance -wise differences am ongst t models . used as the basis of comparison . As they did not deviate from to Shapiro post-hoc multiple pairwise comparisons . For the multiple testing correction , we employ -Hochberg (Benjamini Y. and Ho chberg Y., 1995). The conclusions dr awn from the statistical test results can be outlined as follows , and apply to the three sem antic relations : (1) The performance of BOW was not significantly better than that of GloVe (2) there was no significant difference between t he performance of the FTX and W2V models (p-value>0.05 ), despite those for The MAP could initial ly be re garded at the tuned to work in the sp ecified scen ario with three semant appreciate of the task involved , we compare our results to those of two other studie s that addressed similar scenarios, and compar ed the abil ity of both types of DSM to capture relations from the web-crawled PANACEA Environment Englis h monolingual corpus (Prokopid is P. et al., 2012),15 with a size of over 50 million token s. The authors reported maximum MAP figures ranging from 0.199 to 0.54 These values are surpris ingly simila r to th ose found in our study for the BOW, Glove, FTX, and W2V models (from Table 4), althou gh the size of our corpus is much small er (7 mill ion tokens ). 15 a Small Specialized Corpus 147 On the othe r hand, Nguyen N .T.H. et al. (2017), among other objectives, aimed to extract , with both of DS M, scientific and ver nacular names synonymous to plant sp ecies from the English sub set of the Biodiversity He ritage Librar y (BHL) (Gwinn N. and Rinaldo C., 2009 ),16 an open -access repository containing millio ns of digiti zed pa le gacy literature on biodiversity . The enormo us corpus size of the English subset o f BHL amounts to around 49 gigabytes of data. None In co ntrast, 4 shows that the maximum MAP valu es obtained by the BOW model varied from 0.552. These are extremely promising measures, consider ing the tiny size of our corpus compared to that of BHL corpus. Overall, the MAP values of our BOW model are strikin g because they are quite high despite the small size of the corpus . Finally, t he err or analysis revealed that the terms in the gold standard datasets with the lower number of mentions in the corpus systematically occupied EcoLexicon of the conceptu al structures (Faber P., 20 12) that underlie the usa ge of colponyms in a small -sized, English Coastal Engin eering corpus requires terminologists to manu ally extract the corpus the terms which relate to each colponym through the semantic relations takes_place_ in, relation s held by named bays in the corpu s. Since this is a time-consuming task, th e overall aim of this study was to provide terminologist s with three lists of term applied to the corpus . Since t he constructi on of DSMs is high ly parame Ms and similarity measures suitable for the extr action of terms which relate d to colponyms through the above mentione d semantic using three gold standard datasets. the log -likelihood association measure, showed the best performanc e for 16 the three semantic r elations. The se results reinforce the fi ndings of previou s research that states, on the one hand , that count -based DSM s surpass prediction -based ones on small -sized corpora of under 10 million tokens (A sr F. et al., 2016; Sahlgre n M. and Lenci A., 2016; Nematzadeh A. et al., 201 7), and on the o ther h and, that log-likelihoo d achieve s greater accuracy for med ium- to low-frequency data than other association measures (Alrabia M. et al., 2014). In this respect , research on the applicati on of DS Ms in plethora of in large corpora that further insights can be gained into the efficient representa tion of small specialized corpora in DS relation that wa s to be captu red, and t he specific values coincided in bo th types of DSM, namely, of for located_ at. The dependence of the window size on the speci fic semantic rel ation is in line with the findings by Bernier -Colbo rne G. and Drouin P. was also fou nd that the by located _at and f. This was possibly due to the insufficient number of i nstances of both semantic relations in the corpus for the DSMs to re present them as accura tely . In addition, they only provided a single list of term candidate s for a colponym , which became l ess meaningful because it clear the terms to the colponym . Regarding the similarity measures, it was found that, except for the Euclidean distance , which performed the w orst, the other four measures had comparab le effectiveness for all the DSMs and semantic relation s. This behavio r is in agreement with previous research on similar ity measu re comparison by Huang A. ( 2008), and Strehl A. et al. (2000). Finally, a n extens ion of this work will incl ude testing the same DSMs and similarity/distance measures on sta ndard datasets f named beaches . Acknowledgements This project PID2020 -118369 GB-I00, Transversal Integration of Base on Environment (TRANSCULTURE ), funded by the Spanish Ministry of Science and Innovation . Juan Rojas-Garcia 148 Reference s A. Al-Salman, and E. Atwell. 2014. An empirical study o n the Holy Quran base d on a l arge classical Arabic corp Willits, . 2016. Comparing predictive (eds.), Proceedings the 3 8th Annua of the hia Cohan . 2019. SciBERT: A pretrained language model for scientific text. In Proce edings of the 20 19 Conference o n Empirical Method s in Natural L anguage Proces sing, Hong Kong, 3615-3620. Benjamini, Y., and Y. Ho chberg. 1995. Contro lling the false discovery rate: a practical and powe rful approach to multiple testing. Journal of the Royal Statistical Soc iety, 57(1): 289-300. Benoit K ., K. Watanabe, H. Wang, P. Nulty, A. Obeng, S. M\u00fcller , and A. Matsuo . quanted a: An R pa ckage for the quantitative analysis of textual data. Journal of Open Source Software , 3(30): 774. Bernier-Colborne, G. , oach. In Proceedi ngs of the Enriching of the Conference on Artificial Intelligence , Palo Alto, 2146-2152. Chen, Z., Z. He , X. Liu, and J. Bian. 2018. Evaluating in neural word nd DecisionMaking G. Crichton , A. Korhonen, an d S. Pyysalo. 2016. How t o train good word embeddings for biomedica l NLP . In Proce edings of the 15th o n Bio medical NLP, Berlin, 166 -174. Collober t, R., J. langu age processin g (a lmost) fr om scratch. Journal of Machine Learning Research , 12(A ug): 2493-2537. Devlin, J., . 2019. BERT: Pre -training of deep understandi ng. In arXiv preprint arXiv:1810.04805v Science and Informa tion Security , 14(8): 956-965. Erk, K., S. Pad\u00f3, and U. Pad\u00f3. 2010. A flexible, corpus -driven model of regular an for collocation identification. I n Proceedings of the eLex 20 17 Conferen ce, Leiden , 531-549. Faber, P. (ed.). 2012. A Cogn itive Linguis tics View o f Term Language . Berlin/Boston: D Mouton. Faber, P., J.A. Semantic re lations, dynamicity, and and the i dentification of colle xeme classes. In S. Ric e, and J. Newman (eds.) , Empirical and E Cognitive /Functional Research . Stanford (California) : CSLI, 73 -90. Gwinn N. , and C. Rinaldo. 2009. The Biodiversity Heritage Librar y: Sharing biodiv ersity with the world. IFLA Journal , 35(1):25 -34. Huang, A. 2008. Similarity measures for text document clustering. In Proceedings of the New Zealand Co mputer Science Research Student Conference 2008, Christchurch , 49-56. Ide, N. , Linguistic Annotation Dordrecht : Kiela, D., and S. Clark . 2014. A systematic st udy of semantic vector space model param eters. In Proce edings 2 nd Worksho on C ontinuous Vecto r D. Tugwell. 2004. The Sketch Engine. In Procee G., S. Evert, Schulte im Wa lde. 2014. Contrasting l semantic models. In 3rd Conference on and Computa tional Seman tics, Dubli n, Mart\u00edn, and A. Reimerink . 2018. The EcoLexicon English corpus as an open corpus in Sketch Engine. In Proce . 2014. The St anford CoreN LP Na tural Language Processing Toolkit. In Proceedi ngs of the 52nd Annual Meeting of the Association for Computa tional L inguistics : System Demonstration s, , of Resources and Evaluation , Miyazaki, 52-55. Mikol ov, T., K. Chen , Corrado, and J. Dean . 2013. Efficien t estimati on of word representations in vector space . In Workshop Proce edings P . effectiveness of counting words near other words. In Proceedings the 39th nnual resources. In Proceedings of the 11th International Con ference on Language Resources and Evaluation , zaki, 1438-1445. Pearson, J. the 2014 C Empirical Methods for Natural Language P rocessing (EMN Doha , 1532 -1543. Prokopidis, P., V. Papavassiliou, A . .P. . Frontini , F. Rubino, and G . Thurmai r. 2012. Final report on th e corpus & tic In Proceedings of the 15th Workshop o n Biomedical Natural Language Processing, Berlin, 12-16. Rohde, D., L. Gonnerman , and D. Plaut. 2006. An impro ved mod el of semantic lexica Rojas -Garcia Faber. 2019a. Extr action of construction -Garcia J. Faber. 2019 b. P. Faber. 2019 c. Evaluation of distributional semantic models for the extraction of semantic relations for named rivers from a small specialized corpus . Procesamiento del Lenguaje Natural , 63: 51-58. Room, A. 1996. An Alphabetical Guide to the Language of Na me Studies . Lanha m/London: The Scarecrow Press. Lenci . 2016. The range on distributional semantic models. In Proceedings of the 201 6 Conference on Empi rical Methods in Na tural Language Process ing, Aus tin (Texas) , 975-980. Sager, J .C., D . Dungworth, and P .F. McDonald. 1980. English Special Languages. Principles and Practice in Science and Technol ogy. Wiesbaden: Verlag. Strehl, A., web -page clus tering. In AAAI -2000: Workshop on Artificial Extraction of Terms Semantically Corpus 151 IberLEF 2021: Res\u00famenes de las tareas Overview of the EmoEvalEs task on emotiondetection for Spanish at IberLEF 2021 Resumen de la tarea de detecci\u0013 on de emociones en espa~ nol EmoEvalEs na-L\u0013 M. Teresa Computer Science Department, Universidad de Ja\u0013 shared task, organized at IberLEF 2021, as part of the 37th International Conference of the Spanish Society for Nat- ural Language Processing (SEPLN 2021). The aim of this task is to promote the Emotion detection and Evaluation for Spanish . of emotion classi cation of tweets from the EmoEvalEs corpus one of these seven classes: anger, disgust ,fear,joy,sadness, surprise, or others. 15 submitted results Most with neural being the most widely used model. It should be noted that few considered the of o en- siveness and event the texts. presenta la tarea EmoEvalEs, organizada en IberLEF 2021, en el marco del de la 37 edici\u0013 on de la Conferencia Internacional de la Sociedad Espa~ nola para el Procesamiento del Lenguaje Natural. El objetivo de esta tarea es promover la Detecci\u0013 on y Evaluaci\u0013 on de Emociones en Espa~ nol. Consiste en la clasi- caci\u0013 on de grano no de los tweets del corpus EmoEvent en una de las siguientes siete clases: ira,asco, miedo ,alegr\u0013 \u0010a ,tristeza, sorpresa uotros. En esta edici\u0013 on, se registraron 70 equipos, 15 enviaron resultados y 11 presentaron art\u0013 \u0010culos describi- endo sus sistemas. La mayor\u0013 \u0010a de los equipos experimentaron con redes neuronales, siendo Transformers el modelo m\u0013 as utilizado. Cabe destacar que pocos equipos con- sideraron tambi\u0013 en las caracter\u0013 \u0010sticas de ofensividad y evento que se proporcionaron en el corpus aparte de los textos de los tweets. Palabras clave: EmoEvalEs, detecci\u0013 on de emociones, procesamiento del lenguaje natural. 1 Introduction Emotion detection from text is a research task in Natural Language Processing (NLP) aimed at classifying emotion categories. This task can be considered an extension of the polarity classi cation task due to the presence of ne-grained categories based on fundamental emotion theories, with the Ek- man (Ekman, 1992) and Plutchik (Plutchik, 2001) models the most commonly cited ones. In the last years, great e orts have been made to address one of the most Sentiment However, emotion classi cation is still considered a challenge for the NLP systems and its sig- ni cance has increased in recent years. With the aim of promoting research in the emotion analysis area in Spanish, the \\Emo- tion Detection\" task was introduced for the rst time in the TASS workshop (Vega et al., 2020) at IberLEF 2020. This year we contin- ued with the only two teams participated in TASS Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 155-161 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural2020, this edition has attracted 70 team reg- istrations and received 11 system description papers, which demonstrates the interest of the research community in this topic. With the for Task\" classifying the emotion in tweets related to ,surprise and oth- ers. Unlike previous edition, this year, two new features in dataset have been pro- vided to participants: the event correspond- ing to the domain associated with the tweet and whether the tweet expresses o ensive- ing https://competitions.codalab. Section 2 describes the EmoE- valEs shared Section 3 presents the that the participants used in the com- petition. and 2 Task description Understanding the emotions expressed by users on social media is a hard task due to the absence of voice modulations and facial expressions. ish\", has been to encourage research in this area. The task consists in classify- ing the emotion expressed in a tweet as one among the following surprise includes distraction others : the emotion expressed in a tweet is neutral or there is no emotionThe challenges faced in this task are: 1. Lack of context: tweets are short (up to 240 characters) 2. Informal emojis of tweets per category does not follow the same distribution For developing their approaches, partici- pants received the development and training partitions of the dataset and, at a later an- notations to test their methods and deter- mine the winner of the challenge. The metrics used to evaluated the task selected ranking the sys- tems. 3 Dataset In this section, we describe of the EmoEvalEs shared task. EmoEvent (Plaza- del-Arco et multilingual corpus of tweets based on events that took place in April 2019. They are re- instance in the dataset is labeled with the main emotion expressed in the tweet by annotators categories: anger, disgust ,fear,joy, sadness, surprise, \\ neutral or no emotion \". The nal emotion label of the tweet is the majority emotion labeled by the annotators, but in case the labeled the tweet with di erent label is \\neutral or no emo- tion\". In particular, for this task we used the Spanish version of EmoEvent. This year, compared to the rst edi- tion in TASS 2020 (Vega et al., 2020), two new features from the EmoEvent dataset have been released ensiveness) event to Flor Miriam Plaza-del-Arco, Salud Mar\u00eda Jim\u00e9nez-Zafra, Arturo Montejo-R\u00e1ez, M. Dolores Molina-Gonz\u00e1lez, L. Alfonso Ure\u00f1a-L\u00f3pez, M. Teresa Mart\u00edn-Valdivia 156Emotion Training Dev Test Joy 1,227 181 354 Sadness 693 104 199 Anger 589 85 168 Surprise 238 35 67 Disgust 111 16 33 Fear 65 9 21 Others 2,800 414 814 Total 5,723 844 1,656 Table 1: Dev, the query hash- tags by the keyword HASHTAG in order to prevent the automatic classi er relying on hashtags the During the pre- evaluation phase, training and development (Dev) were provided to the participants; for the evaluation phase, the test set was re- leased. Table 1 shows the number and emotion highly imbalanced in the dataset. 4 Participant approaches In this edition, 70 teams registered on the task, 15 submitted results and 11 presented working notes describing their systems. The following is a brief summary of the nal pro- posals GSI-UPM team - 1st (Vera, Araque, and Iglesias, group has XLM-RoBERTa model. the best submission in the competition was their ne-tuned version of the multilingual RoBERTa model (XLM-RoBERTa), the reported scores on development set not much than with Logistic along with sentiment analysis scores. BERT4EVER team - (Fu et al., 2021). The authors adopt the monolin- gual Spanish BERT to tackle the task (BETO). In addition, they leveraged two augmented strategies to dataset, namely continual was obtained with the pre-training of BETO on the training set provided by the ignoring the lower proportion: disgust ,fear and surprise. aug- lem of data scarcity and data imbalance. Chinese and English were used as inter- mediate languages for back translation. He the and disgust categories. The best result by enter- ing the o ensive labels plus tweets text detection problem proposed by this team was a ne-tuned version of BETO. They tried both, cased and uncased models, and a third tun- ing reducing, by a 30%, the number of samples within the others category. The best result was obtained with a voting system over the three trained models. haha team - 5th (Li, 2021) . the event and o ensive as new Then, words masked language model technique for data augmentation in order to increase the training set and avoid over- tting. Experiments with per- obtained with the XLM- RoBERTa model. The that technique used for data aug- mentation the generalization of model. Overview of the EmoEvalEs task on emotion detection for On the other hand, they used using an en- semble model based on the mode. In their analysis, they show the potential of the linguistic features to provide 2021). They incorporate a diverse set classical machine learning algorithms and traditional neural features from word2vec and BERT embeddings along with a the k word feature selection by a variance (ANOVA F-value) categories to classify by the model were disgust and WSSC these three sets of features are the input of one or more other participants. (de Arriba, Oriol, and Franch, 2021). The authors pro- pose an lemmatized the text. They concluded that the submit- ted system is less accurate for detecting the emotion categories with a small num- ber of samples in the dataset: fear,dis- gust and surprise. Dong team - 14th (Qu, Yang, and Que, 2021). It presents a TextCNN linear one). In data to XML-RoBERTa obtain word vectors with global features of sentences. Then they input the Transformer Encoder for secondary fea- ture extraction, TextCNN network. Finally, they passed the model to a fully con- nected layer for classi cation. Qu team - 15th (Qu, Jia, and Zhang, ) . The authors use the XLM- RoBERTa model to extract the features from training samples and then input the acquired word features into the Bi- GRU Shared Task. We participants. 11 teams provided their working notes ex- plaining the systems that were developed for the competition. From all submissions, the best scoring system was that by GSI- UPM team, followed by BERT4EVER and Yeti. Between the rst two participants, it can be observed that the di erence in terms of macro-F1 and a ne-tuned XLM- RoBERTa model. The team ranked in second Flor Miriam Plaza-del-Arco, Salud Mar\u00eda Jim\u00e9nez-Zafra, Arturo Montejo-R\u00e1ez, M. Dolores shown in parenthesis). position was BERT4EVER F1 model, namely solve problem of data scarcity imbalance, and tried to input the o ensive labels and the text of the tweet into the BETO-cased model. Most of the teams used neural network solutions to address the task. In particu- lar, Transformers are the most widely used model by the participants in to obtain text, and (2) ne- tuning the pre-trained model on the task of emotion classi cation. As tweets from the dataset were in Spanish, most of the teams adopted two available pre-trained language models on multilingual XLM-RoBERTa and the monolingual three teams considered o ensive and information in their approaches (GSI- UPM, haha and WSSC). In most cases, this led to a slight improvement of the system, so both semantic information related general, the enrichment of the learning process with additional data (by means of data augmentation techniques) models are the classifying those emotions whose presence in the dataset is lower. particu- these emotions are by tion applied of class Also, some teams emotions, for example, the pairs disgust and anger, fear and sadness were often confused, a fact that is re ected by their close locations in the two- dimensional models of emotions. 6 Conclusions Emotion classi cation is still considered a in systems and its signi cance has increased in recent With this ish IberLEF 2021, we want to promote research in the area of emotion analysis in Spanish, using the Spanish version of EmoEvent dataset. As organizers, we are very satis ed with participation volume, as it was high. In this edition of EmoEvalEs, 70 participants reg- istered, contributed with a description Overview of the EmoEvalEs task on emotion detection for Spanish at IberLEF 2021 159of their systems. As expected, deep learn- ing approaches constitute the trend in this text classi cation task. In addition, the com- bination of linguistic information con rms the bene ts of opting for hybrid solutions. Certainly, some of the most interesting chal- lenges that participants faced were class im- balance and how to combine fea- tures neuronal networks encond- from those of past editions have been provided this year, there is a clear improvement in performance. Best result reported in macro-F1 in was of 0.447. Compared to the best macro-F1 score tasks. the participation has raised from 2 to 15 teams. We believe that moving the competition to CodaLab had the additional e ect of more visibility, as can be noticed by the fact that ve teams are lo- cated in China (four of them from Yunnan University), which represents one third of to- tal participants. As future work, we plan to include the En- glish version of the EmoEvent dataset in the competition in order to promote multilingual emotion analysis research how emotions are for each event based on cultural di erences between English and Spanish speakers. Acknowledgements This work has been partially supported by a grant from Fondo Social Europeo, Administration of the Junta de An- daluc\u0013 \u0010a (DOC 01073 and P20 00956-PAIDI Desarrollo Re- gional (FEDER), LIVING-LANG project (RTI2018-094653-B-C21) and the Ministry of [FPI-PRE2019-089310]) from the M. Franch. 2021.Applying Sentiment Analysis on M. \u0013A. G. Cumbreras, E. Mart\u0013 \u0010nez-C\u0013 amara, D. Moctezuma, A. M. R\u0013 aez, M. A. S. Cabezudo, E. S. Tellez, M. Gra , and S. Miranda-Jim\u0013 enez. 2019. Overview of tass 2019: One more further the global spanish sentiment analysis corpus. In IberLEF@SEPLN. Ekman, P. 1992. An basic emo- tions. Cognition & emotion , 6(3-4):169{ 200. Fu, Y., Z. Yang, N. Lin, L. Wang, and F. Chen. 2021. Sentiment Analy- sis for case study regarding infectious diseases in america. Future Generation Computer Systems, 112:614{657. Garc\u0013 \u0010a-D\u0013 \u0010az, Arturo Montejo-R\u00e1ez, M. Dolores Molina-Gonz\u00e1lez, L. Ure\u00f1a-L\u00f3pez, M. Teresa Mart\u00edn-Valdivia 160Luo, H. 2021. Emotion Detection for with Data A. M. R\u0013 aez, A. Montoyo, R. Mu~ A. Piad-Mor\u000es, Opinions, health and emotions. In TASS@SEPLN. Montes, M., P. Rosso, J. Gonzalo, E. Arag\u0013 on, R. Agerri, M. \u0013Alvarez Carmona, E. \u0013Alvarez Guti\u0013 Language Re- sources and Conference , pages 1492{1498, Marseille, France, May. Euro- pean Language Resources Association. Plutchik, R. 2001. The nature of emotions: Human emotions have deep evolutionary roots, a fact that American scientist , 89(4):344{350. Q. Que. 2021. Emo- tion Classi cation for Spanish Qu, Y., S. Jia, and Y. Zhang. Sentiment Analysis in Spanish Tweets: The Model based on XLM-RoBERTa and Bi-GRU. Rosenthal, Nakov. Semeval-2017 task 4: Sentiment anchez, J. A. S. M. Herranz, and R. M. Unanue. 2021. URJC-Team at EmoE-valEs 2021: BERT \u0013A. G. Cumbreras, F. M. Jim\u0013 enez-Zafra, E. Mart\u0013 \u0010nez- C\u0013 amara, C. Aguilar, M. A. S. Cabezudo, L. Chiruzzo, and D. Moctezuma. 2020. Overview of tass 2020: Introducing tion detection. In Igle- 2021. Fine-tuning the XLM- RoBERTa IberLEF 2021 161Overview of Rest-Mex at IberLEF 2021:Recommendation System for Tourism Resumen de la tarea Rest-Mex en IberLEF 2021: Sistemas de recomendaci\u0013 on de Educaci\u0013 on Superior de Ensenada 2Consejo Nacional de Ciencia y Tecnolog\u0013 \u0010a 3Tecnol\u0013 results from the Rest-Mex track at IberLEF 2021. This System task consists in predicting the degree of satisfaction that a tourist may have when recommending a destination of Nayarit, Mexico, based on places visited by the tourists and their opinions. On the other hand, the Sentiment Analysis task predicts the polarity of an opinion issued traveled to the most repre- sentative places in Guanajuato, Mexico. For both tasks, we built new corpora considering Spanish opinions from TripAdvisor website. paper compares and discusses Este art\u0013 \u0010culo presenta los resultados de la tarea del Rest-Mex en Iber- LEF 2021. Este evento consider\u0013 o dos sub tareas, Sistema de Recomendaci\u0013 on y An\u0013 alisis de Sentimientos, ambas utilizando textos tur\u0013 \u0010sticos con inter\u0013 es tur\u0013 \u0010stico en M\u0013 exico. La tarea del Sistema de Recomendaci\u0013 on consiste en predecir el grado de satisfacci\u0013 on que tendr\u0013 a un turista al recomendar un destino de Nayarit, M\u0013 exico, a partir del historial de los lugares visitados por el turista y las opiniones que se le dan a cada uno de ellos. Por otro lado, la tarea de An\u0013 alisis de Sentimiento consiste en predecir la polaridad de una opini\u0013 on emitida por un turista que viaj\u0013 o a los lugares m\u0013 as representativos de Guanajuato, M\u0013 exico. Para ambas tareas, se han construido dos nuevas colecciones utilizando las opiniones en espa~ nol del sitio web TripAdvisor. Este art\u0013 \u0010culo compara y analiza los resultados de los participantes para ambas tareas. Palabras clave: Rest-Mex 2021, Sistemas de Tur\u0013 \u0010sticos Mexicanos. 1 Tourism is social, phenomenon related to people's movement to places outside personal or business/professionalreasons (Di-Bella, including Mexico1, 1Mexico is in the world top ten and the second Iberoamerican country related to the arrival of inter- natinal tourists. Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 163-172 tourism represents 8.7% of the national GDP, generating around 4.5 million direct jobs (Elorza, 2020). With the pandemic generated by the SARS-COV-2 virus, which spread out in Mexico in mid-March 2020, tourism was one of the most a ected it- in the quality and safety of touristic products and services (Elorza, 2020). Natural Language Processing (NLP) is an arti cial intelligence area that can help store tourism generating mechanisms for also be developed considering the user and desti- nation information to recommend the places where the user may have better tourist expe- riences. In this way, the tourism sector and the tourists themselves could be supported by the NLP (Anis, Saad, and Aref, 2020). Few recommendation systems for tourist sites are based on the a user's le compared to each place's description. The data collections to train these types of systems are mainly utmost importance to generate resources that allow the generation of systems that help develop intelligent systems in Spanish-speaking coun- tries as well. in tourist has Stantic, as attention of scienti c communication e orts have focused on the English language. Although some studies have focused on Span- ish, only a few of them address Spanish out- side from the country of Spain. These ap- proaches typically not this edition, we proposed two Recommendation System and Sentiment Analysis on Mexican tourist texts. For this purpose, two data sets Nayarit, Mexico, for the recommendation system task. As for the sentiment analy- sis task, 7,413 opinions were collected from tourists who visited Guanajuato, Mexico. To the best of our knowledge, this is the rst time that an evaluation forum is dedi- cated to solving Tourism issues is organized as follows: Section 2 describes the collection building process for this forum and the met- rics for the evaluation. Section 3 summarizes by the both tasks. Section 4 shows the results obtained by the participants' systems and the analysis. Finally, 5 presents the con- clusions obtained 2 Evaluation the measures used for both tasks. 2.1 Recommendation System corpus The rst subtask consists in a classi cation task where the participating system can pre- dict the degree of satisfaction that a tourist may have when recommending a destination. Mexico. This col- lection was obtained from the tourists who shared their satisfaction on TripAdvisor be- tween 2010 and 2020. Each an integer between [1, 5], where: 1. Very bad 2. Bad 3. Neutral 4. Good 5. Very good Each instance consists of Miguel \u00c1. \u00c1lvarez-Carmona, Ram\u00f3n Aranda, Samuel Daniel Fajardo-Delgado, Guerrero-Rodr\u00edguez, 164Class Train instances Test instances 1 45 20 2 53 24 3 167 72 4 457 196 5 860 369 \u0006 1582 681 Table 1: Instances distribution for the rec- ommendation system task. Location: The place of origin of the tourist (the central, northeast, northwest, west, and southeast re- gions refer to the regions of Mexico). Date: Date when the recommenda- tion was issued. Type: Type of trip that the tourist would do. The type would be in [Family, Friends, Alone, Couple, Business] History: The text de- and a series of rep- resentative characteristics of the place as a type of tourism that can be done there (adventure, beach, relaxation, among others.), If it is a family atmosphere, pri- vate or public, it is free or paid, among others. We use a 70/30 partition to divide into train and test. This means that we used 1,582 labeled instances unlabeled instances for the test partition. Table 2.1 shows the distribution of the in- stances for the recommendation system task for the train and test partitions. The class imbalance is clear since class 5 represents around 50 % of the total instances, which makes this a task a very di\u000ecult one. Formally the problem of this task is a TripAdvisor tourist and a tourist place, the goal is to automatically obtain the degree of satisfaction (between 1 and 5) that the tourist may have when visit- ing that place.\" 2.2 Sentiment Analysis corpus The second subtask is a classi cation task where the participating system can pre-dict the of an opinion issued traveled to the places of Mexico. This col- lection was from the tourists who shared their opinion on TripAdvisor between 2002 and 2020. Each opinion's class is an integer between [1, 5], where: 1. Very negative 2. Negative 3. Neutral 4. Positive 5. Very positive Each tourist example: \"Un callej\u0013 on donde tienes que besar a tu amante por a~ nos de felicidad, en el amor es parte de un mito en esta ciudad espe- cial. El callej\u0013 on estrecho con escalones no es muy especial en s\u0013 \u0010 mismo. Lo que lo hace especial es toda la historia a su alrededor\" {Polarity: 5 (Very positive) {Nationality: Mexico {Gender: Male \"Este museo de tres pisos se vende como sede de muchas obras de Diego Rivera, sin embargo, despu\u0013 es de recorrer todo el museo, y ante la frustraci\u0013 on de no encon- trar m\u0013 as que dibujos y bocetos, decid\u0013 \u0010 preguntarle a uno de los guardas, aqu\u0013 \u0010 me aclar\u0013 o que las obras de dos pisos com- pletos se encuentran en restauraci\u0013 on, y en otra exhibici\u0013 on en Jap\u0013 on. No dejando as\u0013 \u0010 al p\u0013 ublico ni una sola hora de pintura para apreciar.\" {Polarity: 1 (Very negative) {Nationality: Nicaragua {Gender; of 7,413 opinions shared by Like the recommendation task, we use a 70/30 partition to divide into train and test. This means that we used 5,197 labeled instances for the test Table 2.2 the distribution of the in- stances for the sentiment analysis task for the train test partitions. Overview of Rest-Mex at IberLEF Tourism 165Class Train instances Test instances 1 80 35 2 145 63 3 686 295 4 1596 685 5 2690 1138 \u0006 As with the other subtask, the class imbal- ance is clear since, again, class 5 represents around 50 % of the total instances, which makes this a task with a signi cant degree of di\u000eculty too. Formally the problem task is de- ned as: \\Given an opinion about a Mexican tourist place, the goal is to determine the po- larity, between 1 and 5, of the text.\" 2.3 Performance measures Systems are will rank the submissions for both subtasks. MAE are de ned as equation 1. MAE Sx=1 nnX i=1jT(i)\u0000Sx(i)j (1) Where Sxis result of the instance iaccording to the Ground Truth, and is of the participant instance i. Finally, nis the number of instances in the collection. 3 Overview of the Submitted Approaches This section presents the results by the for the of system and sentiment analysis. 3.1 Recommendation system overview For this study, two teams have submitted their solutions for the recommendation sys- tem task. From what they explained in their notebook papers, this section of synonymy- antonymy. to use the ComplEx model for the recom- mendation task. The model was modi ed to predict the target label, considering it as a relationship be- tween a User and a Place. Recommenda- tion System for Mexican Tourism. the REST-MEX Shared Task at et al., Summary: The team rst one is based on Doc2vec. The Doc2Vec model was applied to the user and in- of the to the design matrix. Finally, for the other user variables, a hot en- coding was applied to be incorpo- rated in the design matrix to be modeled through a Neural Network with one hidden layer and ordinal encoding to deal with the unbal- anced problem of the They tributed representations BERT approach. 3.2 Sentiment analysis overview For this study, seven teams have submitted their solutions and descriptions for the sen- timent analysis task. From what they ex- plained in their notebook papers, this section summarizes their pre- features, and cation algorithms. Miguel Guerrero-Rodr\u00edguez, tuning The Team: Summary: The their The sec- ondary method has better result sists of ed ish BERT-base architecture model. The BERT-Base architecture was modi ed by removing the last layer of the network. Then, the last two layers of the modi ed BERT ar- chitecture were concatenated to be used as the input to a dense layer with a swish activation function. As a nal layer, a dense layer was used with ve outputs (one for each class) using softmax as activation function. For their rst run, the model was trained with 70 percent of the training data (with data aug- mentation for classes 1 and 2) and used the remaining 30 percent as a validation set. On the other hand, the second model was trained usingthe whole training data (again in- cluding the additional data) and the InHouseTest as the validation set to prevent nally pro- pose a weighting deter- mine the best classi cation Representations of method for keyword extraction in order to prototypical words with the labels of the texts they An SVM does clas- si resentations system, the model development and experi- ments were carried 2021) { Team: The last { Summary: The proposal of this team consists of calculating the Jac- card distance between each instance in the test participation with the average of each of the 5 classes in the train. Jaccard's distance weighted by used to determine the class of each instance in the test. 4 Experimental evaluation and analysis of results This section in detail the performance of their submitted solutions. For the nal phase of the chal- lenge, participants sent their predictions for the test partition, the performance on this data was used to rank them. The MAE was used as the primary evaluation measure. In the following, we report the results obtained by participants. Due to the nature of the data being unbalanced, a system that always results in the majority class would have an acceptable result; however, it would not be helpful. For this reason, as a baseline, is pro- posed the system that always results in class 5 for both tasks. 4.1 Recommendation system results Table 4.1 ob- tained by each team for the recommendation system task. The MAE was used to rank participants. approach of all metrics. It is remarkable to observe how this system improved the base- line with 0.42 in MAE. It can also be seen that it surpassed the baseline at 23.37 baseline would not have good evident since experi- ments per class. Table 4.1 shows the best F-measure results by class in the recommendation task. In this task, for all classes, the best result was ob- tained by the same team that obtained the best MAE result, that is, the Alumni-MCE 2GEN team. Unlike which can be intu- ited, worst performance class was Class 2 with 0.24, followed by Class 3 with 0.32, when the minority class is the Class 1, which obtained a performance of 0.32. For Class 4, a result of 0.67 was obtained, and nally, for Class 5, which is the majority class, a re- sult of 0.96 was obtained. It should be noted that the baseline of the majority class gets zero from F-measure for all classes, except for Class 5, where it gets 0.69. 4.1.1 Perfect assemble for the recommendation system task To analyze the complementarity of the pre- dictions by the participants' systems, we built a theoretically perfect ensemble from their runs, as calculated in (Arag\u0013 on et al., 2019). That is, we considered that a test in- stance was correctly classi at proach; greatest among the runs. Finally, it 108 instances were not classi ed correctly by any system. Within these instances, none belong to class 5. On the other hand, 88 instances were correctly classi ed by all systems. All these instances belong to class 5. Table 4.1.1 shows the re- sult the Miguel \u00c1. \u00c1lvarez-Carmona, Ram\u00f3n Aranda, Samuel Arce-Cardenas, Daniel Guerrero-Rodr\u00edguez, Performance for the Sentiment Analysis these results, it is possible to ob- serve that the Nevertheless, the result 4.2 Sentiment analysis results 4.2 shows a summary of each team sentiment analy- sis task2. In total, eight teams with 14 di er- ent systems participated. For this task, the Miner\u0013 \u0010a UNAM Run1 team obtained the best best accuracy; however, the UCT-UA Run2 team obtained the best this task, eight sys- tems improved the baseline with the MAE measure, 7 improved it in accuracy, and in the same way, as in the recommendation task, all the systems improved the majority class in F-measure. Table 4.2 shows the best F-measure re- sults by class in the sentiment analysis task. Unlike the recommendation task, di erent 2For systems with *, the authors did not send description.F-measure Performance MAE F-measure Sentiment Analysis task. teams obtained the best result for some of the classes. For minority classes like 1 and 2, the best result was obtained by the UCT-UA Run2 team 0.39, respectively. The best results for classes 3 4 were obtained by the Miner\u0013 \u0010a UNAM Run1 team with 0.47 and 0.44, respectively. Finally, the best result for class 5, which is the majority class, was obtained by the Miner\u0013 \u0010a UNAM Run2 team. 4.2.1 Perfect assemble for the analysis task As in the section 4.1.1, the complementarity of the systems was analyzed for the sentiment analysis task. We calculated the perfect as- semble and the vote approaches. Since there are more participating systems in this task, it is also possible to experiment with vote approaches but with fewer systems. The simple vote approach considers all sys- there are below the baseline, which could be putting more noise in the vote. For this reason, it is proposed to select the approaches to vote concerning the ranking of the table 4.2. In this way, it is proposed to use only the sys- tems above the baseline, that is, the 8 best results. It is also proposed to use the top 5 of systems and nally the top 3. Table 4.2.1 shows the the vote approaches results. As in the recommendation task, it is possible to observe that the perfect ensem- ble performance is better than Overview of are com- plementary to each other again, with an er- ror result very close to zero. Nevertheless, the vote approach indicates that the the fewer teams are taken into account for the vote, the better the com- bination result. This may be because the best systems are taken, and the lower the num- ber of systems, the noise decreases. How- ever, the trend of results indicates that the vote will obtain the same result as the best of the systems in the best cases, making a vote meaningless. Although the accuracy and F-measure statistics were improved in the 3 best results, the MAE measure could not be improved. 4.2.2 1. Opinions that were classi ed correctly by all systems. 2. Opinions that were not classi ed cor- rectly by any system. For the rst type, there were 17 opinions in which all systems correctly predicted their class. The 17 opinions belong to class 5. This means that they are very positive, and the text of the opinion clearly shows it. Examples of these opinions are: \\Su arquitectura, sus columnas, todo su interior es hermoso su iluminaci\u0013 on adem\u0013 as de la gente de Guanajuato que lo hacen un lugar mas para visitar \". \\Esta bas\u0013 \u0010lica es una maravilla tanto en su exterior como interior. Vale la pena conocerla y admirar todos los detalles que tiene. \" \\Llegar de noche a este majestuoso lu- gar, brinda la oportunidad de contemplar una parte bella de la ciudad .\" For those of the second type, 70 opinions were found that were not correctly classi ed by any system. It is important to note that none of these opinions are from Class 5. Ex- amples of these opinions are:\\En tu visita pasa por ah\u0013 \u0010 es muy espe- cial que lo visites y te enteres de lo que pasa con los cuerpos en ese lugar, es im- presionante.\" {Class: 1 {Average of the systems monumento est\u0013 a pre- cioso pero de d\u0013 \u0010a hay que visitarle, de noche abstenerse ya que no hay seguri- dad p\u0013 ublica en el lugar y te pueden {Class: 1 {Average of the systems de sus tur\u0013 importantes, es una l\u0013 astima la condici\u0013 on en que se encuentra el museo, sucio, sin gu\u0013 \u0010as, poca informaci\u0013 on, encerrado, un decorado sin sentido. \" {Class: 1 {Average of the systems output: 3.57 In the rst example, it is clear that the opinion is positive. However, the class awarded by the same tourist is 1 (the low- est). It is possible that the tourist confused the order of the scale, which makes it very di\u000ecult to classify this type of opinion cor- rectly. the tourist positive opinion but ends with a negative connotation talking about safety issues. Al- though the word assault (asaltar ) gives a neg- ative connotation, the other part have in the third example, a negative opin- ion can be observed, but the systems gave a higher rating, possibly due to the bias class towards the more posi- tive classes. For more details the re- sults of both tasks, it is possible to go to the following web https://sites.google.com/cicese.edu.mx/rest- Conclusions stands mendation system in Spanish tourists text for Mexican places . Two Miguel \u00c1. \u00c1lvarez-Carmona, Ram\u00f3n Aranda, Samuel Arce-Cardenas, Daniel Fajardo-Delgado, Rafael Guerrero-Rodr\u00edguez, sentiment analysis. Mainly, given a set of opinions in Spanish, the par- ticipants had to determine the degree of sat- isfaction that a tourist may have when vis- iting a Mexican place as well as the polar- ity of a tourist opinion. For these tasks, we built the two data sets derived from TripAd- visor. The shared task lasted more attracted 31 teams from coun- tries such as Mexico, Spain, Cuba, Brazil, Chile, Colombia, and the USA. Out of these teams, 9 sent the results of their systems, and 8 sent their report and description of their systems. The best MAE result for the recommen- dation task was obtained by (Arreola et al., 2021), while the best result in the senti- results were obtained through representations based on BERT, which again gives evidence that the future of textual classi cation is directed to the use and application of this type of archi- tecture. Finally, it is shown the systems of it does not seem easy to be able to take advantage of the information that each one of them correctly classi es to unite it and improve individual results. This could be an interesting research direction in the future of these tasks. Acknowledgements Our special thanks go to all the organizers, and their insti- tutions. 2019. Sentiment analysis in tourism: capitaliz- ing on big data. Journal of Travel Re- search, 58(2):175{191. Anis, S., S. Saad, and M. Aref. 2020. A survey on sentiment analysis in tourism.International Journal of Intelligent Com- puting and Information Sciences , pages 1{ 20. Arag\u0013 M. E., M. A. \u0013Alvarez-Carmona, M. Montes-y G\u0013 omez, H. J. Escalante, L. V. Pineda, and 2019. Overview of mex-a3t at iberlef 2019: Au- spanish tweets. In IberLEF@ SE- PLN, pages 478{494. Arreola, J., L. Garcia, J. and A. task at iberlef 2021. ings Introducci\u0013 on al tur- ismo. Elorza, S. R. 2020. Turismo y sars-cov-2 1 en m\u0013 exico. perspectivas hacia la nueva econom\u0013 ciedad, relational arcamo, and D. Nava Vel\u0013 azquez. 2020. Perspectivas del turismo en el marco de la pandemia covid-19. Roldan Reyes, E. 2021. Techkatl: A senti- ment of mexican's tourism opinions. In analysis of tourism re- views. of sentiment analysis in span- ish. In Proceedings of Guerrero-Rodr\u00edguez, VaxxStance@IberLEF 2021: Descripci\u0013 on de la tarea de detecci\u0013 on de actitudes basada en el uso de informaci\u0013 on m\u0013 as all\u0013 a del texto Rodrigo Joseba Fernandez de Landa1,\u0013Alvaro Rodrigo2 1HiTZ - Ixa, University of the Basque Country UPV/EHU 2NLP&IR group at Universidad Nacional de Educaci\u0013 rodrigo.agerri@ehu.eus, topic in the current is proposed in a multilingual set- ting, providing data for Spanish is to explore crosslingual textual information with from the social network. The results \u0010culo se describe tarea VaxxStance celebrada en el marco de IberLEF 2021. La tarea propone detectar la actitud de un conjunto de tweets rel- ativos a las vacunas, a un tema muy actual y pol\u0013 emico en estos tiempos de pandemia. La tarea se ha propuesto en un marco multiling\u007f ue, euskera y espa~ nol. Adem\u0013 as del texto de cada tweet, se ha proporcionado adem\u0013 as informaci\u0013 on relacionada con la red social de los usuarios autores de los tweets. Los resultados de los participantes han corroborado que el uso de informaci\u0013 on de la red social permite mejorar el rendimiento en esta tarea, particularmente en un entorno crossling\u007f ue. Palabras clave: Detecci\u0013 on de Actitudes, Introduction Stance detection is one of the tasks within the universe of Fake News detection and as et 2019), al., 2019), Fact-checking and Claim Veri cation (Thorne et al., 2018), among The most popular formula- tions are perhaps those in Task 6: Detecting Stance in Tweets (Mohammad et al., 2016) and by the Fake News Challenge (Stage 1)1. In the rst, stance ned as establishing whether a given tweet expresses a FAVOR, 1http://www.fakenewschallenge.org/to a provided the Fake News Challenge, stance has to be inferred between a claim and a text commenting on the claim. In this case the stance category can be to a pre-de on the al., 2017; Gorrell et al., 2019). Furthermore, as it is usually the case in the Natural Language Processing (NLP) eld, most works have experimented on En- Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 173-181 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalglish only, with some exceptions. An Arabic corpus integrated the tasks of fact-checking and detection (Baly et al., dataset from comments of news was devel- oped for Czech language (Hercig et al., 2017), and there also works for French (Evrard et al., 2020) and Russian (Vychegzhanin, 2019). Finally, an interesting new dataset for Italian was released in 2020 as part of the SardiS- tance@Evalita 2020 shared task (Cignarella et al., 2020), which included not only the texts of the tweets labeled with stance, but also social network information relative to the authors of the tweets. This social net- work information includes retweets, user ac- counts pro le, friends and followers, among works have tried to ad- dress stance detection from a multilingual point view. The IberEval 2017 and 2018 shared tasks (Taul\u0013 e et al., 2018) provided a dataset in Catalan and Spanish to clas- sify stance with respect to the Independence of Catalonia, while Lai et al. (2020) pro- vided datasets in French and Italian. How- are hindered extremely skewed class distribution in the Catalan IberEval data, or by the fact that the data for each language was not collected on the same timeframe and addressed di er- ent topics. This features in their dataset. In this context, propose the VaxxS- tance shared task at IberLEF 2021 (Montes et al., 2021), with the aim of detecting stance in social media on vaccines in general. The task provides data in two languages, Basque and Spanish, and its objective is to promote crosslingual research on stance detection us- ing both the text and the information pro- vided by the Twitter social network. Thus, and unlike we of gold-standard in a corpus which al- lows to experiment using both social by the shared task (Cignarella et al., 2020), the dataset includes two di er- ent types of data: Textual and Contextual (retweets, friends and user data), for two lan- guage, Basque and Spanish. The dataset is publicly available in the task website2. 2.1 Collection and Annotation In a rst attempt we tried to do the data collection and annotation for both languages in the same manner. However, as it will be explained below, due to the idiosyncrasies of Basque it was necessary to devise an alterna- tive, more viable, method for that language, especially to obtain the required textual data. In any case, we did specify a number of criteria that both languages ply First, the datasets a required to have a balanced distribution in the ratio users/tweets to avoid that a large number of tweets belonged to a very few users. Second, the tweets in the training set had to be writ- ten by di erent users from those contained in the test set. This is to avoid obtaining arti cially high results due to the existence of user-based information in both the train- ing and test sets. As such, the general idea is that both the textual and user-based (or contextual) knowledge would help each other in order better classify stance. Finally, we use the annotation guidelines from the Se- mEval 2016 task (Mohammad et al., 2016). 2.1.1 Basque Basque is spoken by roughly the 30% of the population in the Basque Country, and un- derstood by around 50%. Due to the fact that Basque is a co-o cial language, it does have presence in the regional public admin- istration, as well as in the education sys- tem and some news media, including a public television broadcaster. Still, the presence of Basque in mass media is extremely low, es- pecially when compared to Spanish, the 4th most spoken language in the world. In this context, the increasing popular- ity of Twitter among Basque speakers is of 2https://vaxxstance.github.io/ Rodrigo Agerri, Roberto Centeno, Mar\u00eda Espinosa, Joseba Fernandez de Landa, \u00c1lvaro Rodrigo 174particular importance for a low resource written in Basque network. This provides a valu- only for popu- lar languages, but also for low resourced ones. Still, the collection process of enough tweets relevant to the VaxxStance task was nd Basque tweets those topics were relatively low. We therefore decided to try an alternative, more brute-force, method. First, we collected all the available timelines of users that are identi ed to write mostly in Basque (around 10k users). content same previous attempt. Third, a rst a blindly annotated by a second annotator. The nal composition of the textual part of the dataset can be seen in Table 1. Train Test Tweets 1,072 312 Favor 327 85 Neutral 524 135 Against 219 92 Users 149 61 Table 1: Textual data in the Basque dataset. We would like to note that the most di\u000ecult part in the process was nding enough users that expressed written lected and as to avoid were collected from the Twitter until current time. They were also restricted to the peninsular vari- ant of the Spanish language in order to avoid problems derived from the use of di erent terms in other variants such as Colombian, Peruvian, etc. To guide this process we used the Google tool \\Google Trends\"3which al- lowed to had occurred, iden- tifying the type of event and the date on which it happened. Some examples are the peaks in tra\u000ec for and against the vacci- nation against measles, which was a conse- quence measles COVID-19 vaccination process. Train Test Tweets 2,003 694 Favor 937 359 Neutral 591 195 Against 475 140 Users 1,261 414 Table 2: Textual data in the Spanish dataset. In addition to the tweets collected through the events identi ed in Google Trends, for the rest of the tweets collected we followed the following process. First, movements, e.g., \\chickenpox\", \\autism\", \\MMR\", etc. After a rst manual analysis, we observed that the vast majority of the tweets collected did not express a stance. In order to solve this prob- lem, we then extracted the hashtags most commonly used in these tweets and manu- ally analysed those that were used to ex- press a position in favour and/or against vaccines. Some increase the number of tweets to manual Stance Detection 175a third annotator the web platform created by Cignarella et al. (2020), to whom we would like to thank for their help using it. Once the manual annotation was com- pleted, the set of AGAINST tweets was much smaller than those expressing a FAVOR or issue, identi step was performed taking care in complying with the general cri- teria of not including more than 10 tweets per user in the nal corpus, as well as not overlapping users between the training and evaluation set. In this nal process we man- aged to increase by about 200-250 tweets the AGAINST class. 2.2 Social Media Information The main objective of this task is studying the usefulness of the context provided by so- cial media information to classify stance in a crosslingual setting. this objective in mind, we collected contextual information relative to the friends of the authors of the tweets as well as their retweets . The con- text be leveraged to generate relation graphs that in may be used to improve the classi ers. Table 3 shows the social media data gath- ered with respect to the tweets in the train and test partitions for each of the languages. In addition to of the tweets cided to collect the the namely, by extracting the the users' This strategy was applied order to the small number of retweets obtained from the tweets in the train and test partitions. Train Test Basque Friends 119,977 53,029 Retweets to user. 2.3 Final Dataset Table composition the VaxxS- tance both textual and contextual information. Regarding the tex- tual information, it can bee seen that the Spanish set is roughly double in size with re- spect to the Basque one, although the distri- bution of classes across the train and test set, as shown by Tables 1 and 2, is quite similar. Train Test Basque Tweets 1,072 312 Users 149 the contextual informa- tion we can see that for Basque there are very few users, around 10% of the number of users for Spanish. This is a re ection of the much smaller community of Twitter users that write in Basque. In this sense, thefriends graph also re ects the same ratio, as the number of friends relations is around 10% of those obtained for Spanish. If we look at the retweets , however, we can see that for Basque we only managed to obtain very few of them. That is why we decided to also pro- vide the retweets for each user in the train and test sets (retweets TL). In summary, the VaxxStance dataset pro- textual and contextual fea- tures. the Basque set is slightly smaller some previous approaches (Taul\u0013 still the data provided for any of the topics in the SemEval 2016 dataset, which is perhaps the most popular benchmark for stance detection (Mohammad et al., 2016). Rodrigo Agerri, Roberto Centeno, Mar\u00eda Espinosa, Joseba Fernandez de Landa, \u00c1lvaro Rodrigo 1763 Task De nition In this task we aimed to promote research on multilingual and crosslingual approaches to stance detection Twitter. Ideally, this type of research requires annotated datasets on a common topic for more than one language and obtained on the dates (coetaneous However, erent c evalua- Only for is allowed. two evaluation settings: {Textual: Only the provided tweets in the target language can be used for development. No data augmen- tation is allowed. {Contextual: Text plus Section 2.2. Open Track: Participants can use any kind of data, including additional tweets obtained by the for train- ing. The main objective is to explore how to develop systems that do not have access to text in the target language, es- pecially using Twitter-related informa- tion. Participants could submit their systems to any of the tracks, but it was compulsory to participate in both languages by mEval 2016 task on Stance Detection (Mo- hammad et al., 2016) which reports F1 macro-average score of two classes, FAVOR and AGAINST, although the script in task web- site4. baselines, one tex- tual information and second one using just social or contextual features: Textual: The textual classi er with RBF is vec- torized using a TF-IDF vectorizer and er. Both Cand Gamma hyperparameters are tuned by means of grid search and 5 fold CV on the training data. The best con gura- tion is used to evaluate on of features (friends count, sta- tus count, emojis in bio, etc.) which are then used to train a XGBoost classi er. Before feeding the classi er, each class weighted in order to create a bal- anced sample. by the baselines show that both tracks are harder for Basque. With respect to the Textual track, stance in Span- ish seems to be expressed more explicitly. social baseline, the low of 4https://vaxxstance.github.io/ VaxxStance@IberLEF 2021: Overview of the Cross-Lingual Stance Detection 177Basque Participants and Results Twenty for the task and However, only three groups nally submitted runs. Table 6 shows the information of the participant groups and the reference to their reports. Team the participants submitted 28 14 per language. While all the three teams participated in both Textual and Con- textual settings of the Close Track, only one team, WordUp, participated in the Zero-shot and Open Tracks. Thus, any comparisons between the participant systems will be per- formed on the Close Track. 4.1 Close Track Table 7 shows the results for the Close Track, which received 20 submission runs. We re- port results for each language and evaluation setting (Textual and Contextual). For all the four rankings, the best results are always ob- tained by the WordUp team. As it was the case with our baseline re- sults, the participant systems score system- atically higher for Spanish. The best results for Spanish are over an 80 F1 score. These results seem to con rm that the Spanish set is easier than the Basque one. For each language, the results improve by using contextual information, except for the MultiAztertest Basque submissions. Still, re- sults con rm the e ectiveness of employing both textual and social information. Regarding results are improved by all the teams in Basque (except one run from WordUp which obtains a very low score) and at least one run per group, except SQYQP, in Spanish. These results suggest that, de- spite their and use ers, approaches followed by both base- lines represent an adequate starting participants, tested two dif- system which used pre-trained transformers- based language models (run 1) and another one based on training a classi er with a set of and stylistic features such as word The rst approach performed much better than the second in the textual track. For the contextual track, they employ only the infor- mation relative to user. More speci cally, for user they select idea worked well in Spanish, it was detri- mental in (run 1) in Basque. The SQYQP team addressed the textual setting by training the and Vollgraf, 2018). For the contextual setting, they among users following the approach done by Espinosa et al. (2020). While their results for Basque are below the textual base- line, they improve results by adding tual information. WordUp! team employed a large number languages. The tweets were then used re- ferring to stance in English, and translated it to Spanish and Basque. In the contex- tual setting they tried several network-based measures features to the logis- tic regression classi er. They report a large number of experiments which resulted in the best performing team across all evaluation tracks. In general, the results obtained by the par- ticipants show that, text only. Rodrigo Agerri, Roberto Centeno, Mar\u00eda Espinosa, Joseba Fernandez de Landa, \u00c1lvaro Rodrigo 178Against Favor F1 Macro Basque Textual 4.2 Open Track The only participant in this track was the WordUp! team, which aug- of tweets speci cally obtained for particular task and also augmented the contextual information by ex- tracting the social network of each user. As it can be seen in the results reported in Ta- ble 8, their results are quite similar to those obtained in the Close Track - Contextual set- ting. This might be due to ad-hoc generated FastText em- beddings also in the Close Track. 02 shows the results obtained by the only participant in this track, in which theparticipants could not use the text (tweets) of the target language for training. The most surprising aspect of the results is perhaps the Basque, the themselves in order to perform of the VaxxStance@IberLEF 2021 shared eval- vaccines across nov- elty for stance detection in these and contextual infor- VaxxStance@IberLEF 2021: Overview of the Text in Cross-Lingual Stance Detection 179mation to train small community of users such as Basque. In this sense, textual results are in general improved by adding social network features. The datasets for both tematically ob- for Basque. Finally, given that just one team participated in the Open and Zero- shot Tracks, one of the main objectives of the task, to promote research on crosslin- gual approaches to stance detection, completely been achieved. Therefore further work is required on this particular line of re- search. Acknowledgments This work has been partially supported by the European Social Fund through the Youth Employment Initiative (YEI 2019) and the Spanish Ministry of Science, Innovation and Universities (DeepReading the RYC-2017-23647 Mirko Lai and Alessandra Cignarella for sharing with us their experience organizing the SardiStance 2020 shared task. References Akbik, A., D. Blythe, and R. Vollgraf. 2018. Contextual string embeddings for sequence. In ternational Conference on Computational Linguistics, pages 1638{1649, Santa Fe, New Mexico, USA. Baly, R., M. Mohtarami, J. Glass, L. M\u0012 Fact Checking in a Uni ed Corpus. In Proceedings of the 2018 Conference of the North American Chapter of the As- sociation for Computational Linguistics: Human Language Technologies, Volume2 (Short Papers) , pages 21{27, New Orleans, Louisiana, June. Association for Computational Linguistics. Basile, V., C. Bosco, E. Fersini, D. Nozza, V. Patti, F. M. Rangel Pardo, P. Rosso, and M. Sanguinetti. 2019. SemEval- 2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter. In Proceedings of the 13th In- ternational Com- putational Linguistics. Bojanowski, E. Grave, A. word vectors with subword Proceedings. Cignarella, A. T., M. Lai, C. Bosco, V. Patti, and P. Rosso. 2020. SardiS- tance@EVALITA2020: Overview of the Task on Stance Detection in Italian Tweets. In V. Basile, D. Croce, M. Di Maro, and L. C. Passaro, editors, Proceedings of the 7th Evaluation Cam- paign 2020) Bontcheva, M. International Workshop on Espinosa, M. S., R. Agerri, and R. Centeno. 2020. Deepreading Combining textual, so- cial and emotional features. In V. Basile, D. Croce, M. D. Maro, and L. C. Passaro, editors, Proceedings of the Seventh Evalu- ation Campaign of Pro- cessing and Speech Fi- nal December 17th, 2020, volume 2765 Rodrigo Agerri, Roberto Centeno, Mar\u00eda Espinosa, Joseba Fernandez de Landa, \u00c1lvaro Rodrigo 180ofCEUR Workshop Proceedings . CEUR- WS.org. Evrard, M., R. Uro, Herv\u0013 e, and B. Ma- zoyer. 2020. French Tweet Corpus Au- tomatic Stance uation Conference, pages 6317{6322, Mar- seille, France, May. European Bengoetxea. A. of the 13th International Work- shop on Semantic Evaluation, pages 845{ 854, Minneapolis, Minnesota, USA, June. Association for Computational Linguis- tics. Hercig, T., P. Krejzl, B. berger, and L. Detect- Pro- ceedings, pages 176{180, Bratislava, Slo- vakia. E. Vin- cent, P. Adineh, D. Corney, B. Stein, M. Potthast. 2019. SemEval-2019 task 4: Hyperpartisan news detection. In Pro- of 13th International Work- shop on Semantic Evaluation, pages 829{ 839, Minneapolis, Minnesota, USA, June. Association for Computational Linguis- tics. Lai, M., A. Cignarella, D. Hernandez Farias, C. Bosco, V. Patti, and P. Rosso. 2020. Multilingual Stance Detection in So- cial Media Political Debates. Computer Speech & Language, 02.Lai, M., A. T. Cignarella, L. S. Kiritchenko, P. Sobhani, X. Zhu, and C. Cherry. 2016. SemEval- task 6: Detecting stance in tweets. InSemEval-2016), pages 31{41. Montes, M., P. Rosso, J. Gonzalo, E. Arag\u0013 on, R. Agerri, M. \u0013Angel \u0013Alvarez Carmona, E.\u0013Alvarez Mellado, J. C. de Albornoz, L. Chiruzzo, L. Freitas, H. G. Adorno, Y. Guti\u0013 errez, S. M. J. Zafra, S. Lima, F. M. P. de Arco, and M. T. (eds.). 2021. Proceedings F. M. R. Pardo, M. A. Mart\u0013 \u0010, and P. Rosso. 2018. Overview of the Multimodal Stance J., Vlachos, C. 2018. FEVER: a dataset for fact extraction and VERi cation. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technolo- gies, Volume 1 (Long Papers), pages 809{ 819, New Orleans, Louisiana, June. Asso- ciation for Computational on Ensembles 170:114547. VaxxStance@IberLEF 2021: Overview of the on Going Stance Detection in Spanish tarea MeO endEs en IberLEF 2021: Detecci\u0013 on de lenguaje ofensivo en las variantes del espa~ nol Flor Miriam Plaza-del-Arco1, Marco Casavantes2, Tecnolog\u0013 \u0010as del (INAOE), Mexico 3Centre de Recherche GRAMMATICA at IberLEF 2021 and co-located with the 37th International Conference of the Spanish Society for Natural Language Processing (SEPLN 2021). The main purpose of MeO endEs is to promote research on the detection of o ensive language in Spanish variants. The shared task involve four subtasks, the rst two correspond to the identi of o ensive language categories generic from di erent social media platforms, while subtasks 3 and 4 are related to the identi cation of o ensive langua- ge targeting the Mexican variant of Spanish. Two annotated datasets on o ensive language have been released to the Natural Language Processing community. MeOf- fendes attracted a large number of participants: a total of 69 signed up to participate in the task, 12 submitted o\u000ecial runs on the test data, and 10 submitted system description papers. Corpora and results are available at the shared task website at https://competitions.codalab.org/competitions/28679. Keywords: MeO endEs, detecci\u0013 on de lenguaje ofensivo, procesamiento del lenguaje natural, clasi caci\u0013 on de textos. Resumen: Este art\u0013 \u0010culo presenta tarea MeO endES 2021, organizada en iber- LEF 2021 junto a la 37\u00aa Conferencia Internacional de la Sociedad Espa~ nola para el Procesamiento del Lenguaje Natural (SEPLN 2021). El objetivo principal de MeOf- fendEs es promover la detecci\u0013 on del lenguaje ofensivo en las variantes del espa~ nol. La tarea compartida implica cuatro subtareas, las dos primeras corresponden a la iden- ti caci\u0013 on de categor\u0013 \u0010as de lenguaje textos gen\u0013 ericos en espa~ nol extra\u0013 \u0010dos de diferentes redes sociales, mientras que las subtareas 3 y 4 est\u0013 an relacionadas con la identi caci\u0013 on de lenguaje ofensivo dirigido a la variante mexicana del espa~ nol. Para la competencia se han puesto a disposici\u0013 on de la comunidad del Procesamiento del Lenguaje Natural dos conjuntos de datos anotados con lenguaje ofensivo. MeOf- fendes ha atrav\u0013 \u0010do a un gran n\u0013 umero de participantes: un total de 69 se inscribieron para participar en la tarea, 12 presentaron resultados o ciales sobre los datos de evaluaci\u0013 on y 10 presentaron art\u0013 \u0010culos describiendo su sistema. Los conjuntos de da- tos y los resultados o ciales est\u0013 an disponibles en el sitio web de la tarea compartida: https://competitions.codalab.org/competitions/28679. Palabras clave: MeO endEs, o cation. 1 Introduction O ensive language detection broader of classi analysis (Medhat, Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 183-194 para del Lenguaje NaturalHassan, and Korashy, 2014), emotion detec- tion 2014) hate speech detection (MacAvaney et al., 2019). O ensive language is individual- oriented, as o enses expressions. The participa- tion of people in social media has raised the problem of a frequent use of these an uncontrollable means to publish against others. It is not di\u000ecult to realize the large number of works regarding this and on languages interest, which extends so far, has motivated several evaluation campaigns, being the more recent the IberLEF 2020 (Arag\u0013 on et al., 2020b), O enseval at SemEval 2020 (Zampieri et al., 2020), or OSACT4 shared task at AOSCT 2020 (Mubarak et al., 2020). Regarding the approaches applied to tac- kle the challenge of detecting o ensive texts, they share many common methods to current lear- ning based ones (Plaza-del Arco et al., 2021). With the aim of promoting research in the detection of o ensive language for Spanish and its Mexican variant, we introduced the MeO endEs task at IberLEF 2021 (Montes et al., 2021) with four subtasks. The rst two subtasks involve a novel dataset for o ensive language research in general Spanish (O en- dEs). The contains ter, Instagram). the including the in uencer ID and the social me- dia source of the post. Systems have to face with di erent challenges in these subtasks: (i) di erent language for task 2. Finally, we have received four des-cription papers from the participants in the- se o detection in identi di o ensi- language detection targeting the Mexican variant of Spanish (subtasks 3 and 4). These are a continuation of previous e orts in trying to in the IberLEF (Arag\u0013 on et al., 2020a; Arag\u0013 et al., 2019) and Ibe- rEval forums ( \u0013Alvarez-Carmona et al., 2018) for the same variant of Spanish. As in pre- vious editions, participants had to develop solutions to accompanying tweets, with the ho- pe that such information could be bene cial for improving the Also, build those used in editions. The main being tee of annotators (see Section 3.2). Our goal was to provide a curated dataset that could result in more previous editions, the aim of sub- tasks 3 and 4 were to motivate research on the analysis of o ensive language in Mexi- can Spanish. A language whose characteris- tics and variations make it unique in its kind and di erent to other languages, making it necessary to have tailored techniques for this language. Likewise, cultural aspects like the use of language with sexual connotation for non o ensive communications make it in terms of in other evaluation fo- and 4 attracted a most of language modeling tools available. In gene- ral terms, performance of solutions was lo- wer than that achieved in previous years (see, Flor Miriam Plaza-del-Arco, Marco Casavantes, Hugo Jair Escalante, M. Teresa Villase\u00f1or-Pineda 184e.g., (Arag\u0013 on et al., 2020a)), this could be due to the more careful annotation process. For subtask 3, which we were expecting, subtask 3 received more submissions even when additional data was provided for subtask 4. Overall, results are encouraging and motivate further research. As with the other subtasks, we will keep the leaderboard of the competition open, so that users can keep making submissions despite the competition is over. remainder paper is organized as follows. Section 2 introduces the four sub- tasks are MeO endEs. Then, Section describes the corpora conclusions section describes in detail the four sub- tasks that the MeO endEs com- petition at IberLEF 2021. The whole the CodaLab platform2and every phase in which participants had access to labeled trial and training data, and whe- re they could make submissions to test the platform; and (2) a nal phase in which unla- beled test data was released and participants uploaded the predictions of their systems to evaluation and raking for the o\u000ecial results performance in di erent subtasks on di erent corpora di erent categories associated with the O endEs corpus (see subsection 3.1). No information about the comment (source or in uencer ID) 1Please note that a probability for each class, so they all sum 1.0) for the four considered ca- tegories, in order to evaluate the agree- ment of predictions with con dence considered the micro-averaged pre- where participants submit con dence values 0 meta- data (information about targeted users and the related social media) is provided to participants. Participants had access to information associated with posts: so- cial media source, in uencer genre and in uencer name. The same evaluation measures as subtask 1 or non-o ensive the is, text Contextual binary clas- as subtask 3, but metada- ta about participants. subtask, partici- had access to information ted and their authors li- ke: date, retweet count, etc. The aim of including this informa- tion was to determine to what extent information and users is detec- tion The same evaluation measures as subtask 3 were used for this one. 3 Datasets and baselines 3.1 O endES For subtasks 1 and 2 we have released a no- vel dataset for o ensive language of endEs). Focusing on young in uencers from the well-known social platforms of Twitter, Instagram, and YouTu- be, we corpus of the corpus is labeled with three annotators while another subset is labeled with con- label computed as the ratio of annotators that agreed on the majority label over the total of the competition, we have selected 30,416 posts from the total. The are labeled with the following categories: - O person (OFP). O ensive text targeting a speci c individual. - O ensive, target is a group of peo- ple or collective (OFG). O ensive text targeting a group of people belonging to the same ethnic gender or sexual political or common characteristic. O ensive, target is di erent from a person or a group (OFO). O ensive text where the target does not belong to any of the previous categories, e.g., an organization, an event, a place, - Non-o ensive, but ru- de words, blasphemes or swearwords but without the aim of o ending, and usually with a positive connotation. - Non-o ensive lan- features were also provided as \\contex- tual\" the social platform where that comment was posted to, and the gender addressee of the comment, i.e. the targeted user. Finally, di erent sets have been relea- sed for the competition. During the pre- evaluation phase training and development (Dev) sets were provided to the participants and for the evaluation phase the test set wasrelease, per- to each set the categories. Label Training Development Test 13,212 64 1,235 22 2340 OFP 2,051 4 211 Total 16,710 100 13,606 Tabla 1: Distribution of the O multiouput regression task we a multiouput regressor along with the Epsilon-Support Vector Regression. No pre- processing has been applied to the text, nor has a hyperparameter search been performed. We refer to these baselines as baseline-svm. 3.2 O endMEX For subtasks 3 and 4 we have released a novel dataset in Mexican Spanish collected from Twitter and manually labeled not o ensive). rent problem is a multi label cation one (e.g., a tweet can be o ensive but not vulgar), we are the all of the labels for the training data as additional information that participants can exploit when develo- ping their solutions. Such information was not provided in the test set partition. Additionally, for subtask 4, we distributed metadata information associated with tweets in the corpus, these date, retweet count, status count, Flor Miriam Plaza-del-Arco, Marco Casavantes, Hugo Jair Escalante, M. Teresa account was created, among a few others associated to the user pro le and ima- ge. Detailed information on the considered metadata can be found in the corresponding API documentation (Twitter, 2021). Partition Tweets O . No . Table summarizes the O endMex data set used for subtasks 3 and 4. The Trial par- tition was rather small, as the idea was to use such partition for testing the submission sys- tem. Training and test partitions are larger and present an approximate class implemented two popular approaches that have shown to be hard to beat in both subtasks: i) a Bidirectional Ga- ted stopwords (with pronouns); in order to enrich the hashtags were seg- mented by words (e.g. #EsDeLesbianas - es de lesbianas), and all emojis were conver- ted into words embeddings were used, and a fully-connected softmax layer handle for it of emojis. This ensemble involves two stages. In the rst stage the messages BETO predictions were at the end by a XG- Boost classi er (Chen and Guestrin, 2016). We refer to these two baselines as baseline-dl below. In addition to the where a similar preprocessing as above was applied. The goal of this baseli- ne was to evaluate the added value metadata when using a linear model, and to assess the margin of bene ts of approaches over a di- rect baseline method. We will refer to these baseline-bow. 4 Participant approaches and results 4.1 Subtask a multi-output brief of participants' systems and Sentiment Analysis data. They show that Sentiment Analysis and the social domain adaption is bene cial for the problem of of- fensive language detection. The system ran- of features com- bine BERT). Word embeddings were evaluated isolated from the showing the behavior. these featu- res were using mutual information. Also, several approaches varied num- ber of experiments in order to identify the best con guration for system hyperparame- ters. From o\u000ecial results the solution. When linguistic features were removed, the system obtained the second position in the competition. The GDUFS DM team applied se- quence classi ne-tuned a pre-trained BERT model and composing the nal encodings for the text from a max poo- ling of the sentence encondings from all layers and token encondings from last layer. Two additional techniques were integrated in the nal system: pseudo-labeling and focal loss. The former technique training, were labels predicted and re-entered into the learning process to produ- ce a larger training set. Focal loss was used as a way to correct class imbalance. The system ranked in the third position in the competi- tion. Marta Navarr\u0013 on and Bidirectio- Transfor- mers (BERT). The best results was archived by the BERT model. The system ranked in the fourth position in the competition. 4.2 Subtask 2 UMUTeam was the only team in submit- ting results to this subtask. They applied the same system to add to the set of features applied one-hot two features, as done with negation ones. Compared to what was obtained in subtask 1, the integration of con- textual information contributed to a small, but consistent improvement in nal scores. 4.3 Analysis of subtasks 1 and 2 Table 3 and Table 4 provides a summary of the o\u000ecial results for 1 notice that all the teams outperformed our baseline-svm which shows the success of ployed regres- sor baseline, which shows the success of the classical learning algorithm in this setting. For the non-contextual multiclass classi ca- tion, it can be seen that the scores of the rst three teams are very close. This close- ness in performance could be due to the fact that most of these top ranked participants relied on similar pretrained models in their solutions (Spanish BERT model, except for NLP-CIC, who ne-tuned a multilingual Ro- BERTa model). But greater di erences can be observed when looking at the MSE error. The lower MSE value is, the closer is the sys- tem to the behaviour of human annotators. In that case, XML-RoBERTa almost reduces to a half the error of the second system in the ranking. Finally, both F1scores and MSE errors are consistent in terms of ranking or- der. For the second subtask, only one team eva- luate their system. We can observe that the contextual information did not improve per- formance, in terms of F1score, to that system subtask1. But regar- ding MSE, including those additional media platform and gender of the targeted user) do led to a system closer to human annotator behaviour. Subtask 1: language in results obtained by parti- cipants of subtasks 3 and 4. the former, a total of 10 teams submitted runs for the phase did not qualify for o\u000ecial ranking. For subtask 4 we received the submissions from three di erent teams. This was somewhat disappointing as we were ex- the the former,i.e., non-contextual binary classi cation, it can be seen that there were only 3 teams that did not beat the baselines associated with the task. The best performance was obtained by the CIMAT-MTY-GTO team with a relative improvement over the bow and dlbaselines of 21 % and 34 %, respectively. Followed clo- sely by the next 4 teams in the ranking. This closeness in performance could be due to the fact that most of these top ranked partici- pants relied on similar pretrained Table 6. Interestingly, baseline-bow outperformed the one based on the Bi-GRU. This could be due to the fact that the latter model was trai- ned only on the available data, which may be of limited size given the complexity of GRU models (see Table 6). Regarding task 4, from Table 5 it can be seen that no team outperformed baseline-dl. This is due in part to the competitiveness of the baseline model, but also, to the fact that participants did not do any special processing for the provided metadata (see below). Still, two out of the three teams were close to the baseline. On the other hand, only one team did not outperformed the baseline-bow. The improvement of baseline-dl over baseline-bow could be due the fact that a baseline-bow in both tasks, it is observed that the added value of metadata in subtask 4 only yield a negligible improvement. This con rms that the sole inclusion the features is not enough to improve performance. teams that participated in subtasks 3 and 4, it shows the adopted models and highlights any novel aspect of the di erent approaches. In the following, we outline the proach were common. Most teams relied on pretrai- ned transformers for Spanish in the mo- deling process, we assume this was with the purpose of alleviating the small sam- ple problem. This is in line trends of NLP, and in general it was very helpful: of top ranked participants used transformers. we mecha- nisms could help to boost performance when using transformers. Advanced linguistic features were not considered in most of teams CIC-IPN teams) a genetic programming (DCCD-INFOTEC). It is interesting that their performance was competitive, even when no transformer was used. It may be that for top is a promising way for further improving the performance of transformers. No special treatment for processing metadata for subtask 4. It was so- mewhat disappointing that participants of subtask 4, did of were conca- input spaces and feed to classi cation models. We are still con dent that recognition performance can be improved when these features are used e ectively. analyze performed an plimentariness of the predic- tions from erent teams. For this analy- sis we used the last run from every team in the di erent subtasks. We measured the ganthan, Yao, ex- presses with a number in [0;1]the extend to the made less correlated the errors are). On the other hand, to measure the complimentariness of predictions, we calculated the Maximum Pos- sible Accuracy (MPA), this is the accuracy that can be obtained if we consider a tweetcorrectly classi ed when at least one of the considered systems classi ed it correctly. Table 7 shows the CFD and values obtained for the considered runs from sub- tasks 3 4, for completion we also report the best accuracy (BA) obtained by any of the considered systems. From these results, it is possible to observe that the MPA in both subtasks is considerably greater than the BA in both subtasks, suggesting that the partici- systems and complementary to each other: performance could improve by 16 % and 11 % if the pre- dictions from the available systems were com- bined 3 respec- tively. The values of the CFD measure show that there is a high diversity among errors in the predictions of both for subtask are the in subtasks. As a result applying the MPA metric, it possible those common errors across all systems3. In fact, there are only 34 tweets that no system could classify correctly. Nine of them are o ensive tweets that were classi ed as non-o ensive. Below we present some of these tweets (a rough translation to English is provided), where we can identify o enses with no vulgar or profane words (e.g. \\gata\"), the use of out of the voca- bulary (e.g. than whore and fat #sorrynotsorry ) Yo no te quise decir gata, pero bueno. Eres una gata (ENG: I did not mean to say gata. But well. You are a gata. ) Soy yo o @USUARIO est\u0013 a bien pen.... hace dra- mas se pone loca y pierde jajaja @ExatlonMx. (ENG: It is me or @USER is crazy and loses hahah @ExatlonMx. ) B\u0013 asicamente, el feminismo se trata de feas pe- leando por los derechos de las guapas. (ENG: Basically, feminism is about the ugly girls gh- ting for the rights of the pretty ones. ) couple of mistakenly are the following: @USUARIO Vas en micro, camina, se suben unos HDP y les quitan todo a todos En un taxi, te pueden secuestrar... En el metro hay n car- teristas. (ENG: @USER you are going in bus, it moves, some HPD get in, they steal everyo- ne. In a can you can be kipnaped.... in the subay there are n pinpockets.) 3NOTE: In this section we include examples of language that may be o ensive to some readers, these do not represent the perspectives of the authors. Flor Miriam Plaza-del-Arco, Marco Casavantes, Hugo Jair Escalante, M. was used to augment of and detection, in addition to models trained on the provided obtaining the transformer was feed to a CNN based model.XLM and CNN (Qu, Que, and Shuang- jun, 2021) YNU qyc The output of the feed an LSTM model, a K-folding ensemble scheme was adopted.XLM-RoBERTa and LSTM(Qu, and 6: Summary system descriptions that participated in 3 and 4.Indicates this team participated in both tasks. We separate systems that quali ed for the o\u000ecial results and additional systems. Subtask BA MPA CFD 0.8277 0.9844 0.8185 0.9271 metric; NoT stands for Number of Teams. Sus pinches relaciones empiezan con un Invita a tus amigas las m\u0013 as putas y piden delidad, malditos ilusos. (ENG: Your damn relationships start with an invite to your friends, the most promiscuous and you ask delity, fkng dreamer.)This analysis the variation of language and detection of o ensiveness in text. Moti- vating this subject. important aspect mention is that the corpus used this year took a sub- set of last year's data and was relabeled with the guide proposed by (D\u0013 \u0010az-Torres et al., 2020). For this task, a group of labelers of di erent ages was selected (3 adults, 6 youth) and balancing the number of males and fema- les (4 females, 5 males). This new relabeling was the main decrease the change as well as gender) increased the variations present in both the training set and the test set. The creation of robust systems for this task must consider these scenarios both during the training pha- se and to provide a con dence rating of the prediction made by the automatic method. 5 Conclusion The MeO endEs shared task at IberLEF at- tempts to continue to the research in o ensive language detection in Spanish. A new data- set on generic Spanish has been prepared for this edition, as a companion collection to the existing one on Mexican Spanish, enabling in- tensive experimentation over a large e ectiveness of the approaches and contributing to the advance of o ensive language detection systems. A total of 69 participants registered to the MeO endEs shared task. However, only 12 teams participated in the phase of challenge. Interesting con- clusions have drawn Given we llenge website open so that anyone interested in trying their own methods can do it at any time. Acknowledgements We would like to thank CONACyT for par- tially under grants and the Thematic Networks program (Language Technologies Network). Hugo Jair Escalante is suppor- ted by CONACyT under project CB-S-26314. DER), LIVING-LANG In P. Rosso, J. Gonzalo, R. Mart\u0013 \u0010nez, S. Montalvo, and J. C. de Al- bornoz, editors, Proceedings of the Third Workshop on Con- ference of the Spanish Society for Natural Language Processing (SEPLN 2018), Se- villa, Spain, September 18th, 2018 , on, H. J. Escalante, L. Villase~ at iber- lef 2019: Authorship and in mexican spanish tweets. In M.\u0013A. G. Cumbreras, J. Gonzalo, E. M. C\u0013 amara, R. \u0010nez-Unanue, Ros- R. Ortega-Bueno, 35th Conference of Spa- nish Society for Natural Language Pro- cessing, IberLEF-SEPLN 2019, Spain, September Proceedings, lef 2020: Fake and aggressiveness analysis in mexican spanish. In M. \u0013A. G. Cumbreras, J. Gonzalo, E. M. C\u0013 amara, R. Mart\u0013 \u0010nez-Unanue, P. Rosso, S. M. J. Zafra, J. A. O. Zambrano, A. Miranda, J. P. Zamorano, Y. Guti\u0013 errez, A. Ros\u0013 a, M. Montes-y-G\u0013 omez, located with 36th Conference of the Spa- nish Society for Natural Language Pro- Flor Miriam Plaza-del-Arco, Marco Casavantes, Hugo Jair Escalante, M. Teresa 2020: and aggressiveness analysis in mexican spanish. In Notebook Papers of 2nd SEPLN Workshop on Iberian Languages Evaluation Forum (IberLEF), 2021. Dccd-infotec 2014. Emotion detection from text: A survey. InProceedings of the workshop on natu- ral language processing in the 5th infor- mation systems research working days (JI- SIC), pages 37{43. Ca~ nete, J., G. Chaperon, R. Fuentes, J.-H. Ho, H. Kang, and J. P\u0013 erez. 2020. Spanish pre-trained bert model and evaluation da- ta. In PML4DC at ICLR 2020 . Chen, T. and C. Guestrin. 2016. XGBoost: A scalable tree boosting system. In Pro- ceedings of the 22nd ACM SIGKDD page \u0010n. Automatic detection of o ensive language social media: De - ning linguistic criteria to build a Mexican Spanish dataset. In Proceedings theSecond Workshop on Trolling, Aggression and Cyberbullying, pages 132{136, Mar- Resources a-D\u0013 o ensiveness in o ensiveness Gupta, A. Jou- lin, and T. Mikolov. 2018. Learning word vectors for 157 languages. In Proceedings of the International Conference on Lan- guage Resources and Evaluation (LREC 2018), page Using detecting of- and lutions. and H. Korashy. 2014. Sentiment algorithms and engi- neering journal , 5(4):1093{1113. Montes, M., P. Rosso, J. Gonzalo, E. Arag\u0013 on, R. Agerri, M. \u0013Alvarez Carmona, E. \u0013Alvarez o ensive language detection shared task. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Sha- red Task on O ensive Language Detection, pages 48{52. Plaza-del Arco, F. speech detection. Expert Systems with Applications, Arti Heidelberg. Springer Berlin Heidelberg. anchez-Vega, F. and Transformer based o ensi- ve language K., P. N. Suganthan, and X. Yao. 2006. An analysis of diversity measures. Mach. Learn., 65(1):247{271. 2021-06-30. Wiebe, J., T. Wilson, R. Bruce, M. Bell, and M. Martin. 2004. Learning subjec- tive language. Mubarak, Flor Miriam Plaza-del-Arco, anchez1, Jorge Carrillo-de-Albornoz1, Trinidad 1UNED NLP & IR Group, Universidad Nacional de Educaci\u0013 on a Distancia 2PRHLT Research Center, Universitat sEXism Identi in Social neTworks (EXIST) challenge, a shared the tweets gabs, both in Spanish and English. We have received a total of 70 runs for the sexism identi cation task and 61 for sexism categorization submitted by 31 di erent teams We present the dataset, the evaluation methodology, an overview of the texts from two social networks (Twitter and Gab) and its development has been supervised and monitored by experts in gender issues. Keywords: presente art\u0013 \u0010culo describe la organizaci\u0013 on, objetivos y resultados de competici\u0013 on sEXism Identi cation in Social neTworks (EXIST), vez en EXIST 2021 propone dos tareas: la identi y la categorizaci\u0013 on de sexismo en ingl\u0013 es y espa~ nol. Se han recibido un total de 70 runs para la tarea de identi caci\u0013 on de sexismo y 61 para la categorizaci\u0013 on de sexismo, enviadas por 31 equipos de 11 pa\u0013 \u0010ses. En este trabajo, se presentan el dataset, la metodolog\u0013 \u0010a de evaluaci\u0013 on, un an\u0013 alisis de los sistemas propuestos por los participantes y los resultados obtenidos. El dataset nal est\u0013 a compuesto por m\u0013 as de 11,000 textos anotados procedentes de dos redes sociales (Twitter y Gab) y su elaboraci\u0013 on ha sido supervisada por expertas en temas de g\u0013 enero. Palabras clave: Detecci\u0013 on de Sexismo, Twitter, Gab, Espa~ nol, Ingl\u0013 es. 1 Introduction The phenomenal tech- nologies has interaction people erent backgrounds. With more than 4 billion people around the world now using social media each month1, social networks are undoubtedly one of the most ways of communicating. Al- though the advantages and positive are the report greater hostile sexism (Fox, Cruz, and Lee, 2015) and emboldens them to engage in Furthermore, rapid spread of on- line information in social networks has made these behaviours extremely dangerous. In this context, inequality and discrimination remain of sexist content is still a di\u000ecult task for social media platforms. Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 195-207 Lenguaje NaturalFor instance, Amnesty International pub- lished a report2where they ter as a \\toxic place\" for women. Accord- ing to this report, Twitter is promoting vio- lence and hate against people based on their The report also suggests that is failing to protect women against ha- rassment and it could harm their freedom of speech. Recently, members of the U.S. Congress asked Facebook to do more to pro- tect women in their platform3. According to some semination extremely dangerous so that solutions are required a faster and even better user generated-content mod- eration or to serve as a tool that helps hu- man moderators to reduce the volume of sex- ist content still present in online platforms. The Oxford English Dictionary de nes sexism as \\prejudice, stereotyping or discrim- ination, typically against women, As stated in (Rodr\u0013 \u0010guez-S\u0013 anchez, Carrillo-de Albornoz, and 2020), sex- ism is frequently found in many forms in social networks, includes a range may sound \\friendly\": the statement \\Women must be loved and respected, than men. Sexism may sound \\funny\", as it is the case of sexist jokes or humour (\\You have to love women... just that... You will never understand them.\"). Sexism may sound and degrade your- self as the fucking bitch you are if you want a real man to give you attention\". However, subtle forms of sexism can af- 2https://bit.ly/2TMPAJD facets of their Ac- cording to (Swim et al., 2001), non-hateful sexism can women's psychological well- being by decreasing their comfort, increasing their anger and depression, and decreasing their stated self-esteem. Similarly, (Berg, 2006) found a non-violent sexism and post- the rst Span- ish corpus of Twitter, the MeTwo and with to misogyny de- tection systems. In line with previous hate speech research, the AMI shared task focused on the automatic identi cation of misogyny (hate or prejudice against women) in Twitter (Fersini, Rosso, Anzovino, 2018). Teams were proposed to identify tweets the sEXism Identi at IberLEF 2021 (Montes et al., 2021). The EXIST challenge is the rst shared task on sexism detection in social networks whose aim is to identify classify sexism in broad sense, expressions that of sexism and built and the un- censored social network Gab.com (Gab) in English and Spanish. To collect these posts, we de ned as seed terms a set of a number of popular terms, both in English and Span- ish, commonly used to underestimate the role of women in our society. All these terms, as well as the sexism categorization proposed in this work, have been supervised perts in gender Francisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrilo-de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, Miriam Comet, Trinidad Donoso 196is a report or a description of a sexist be- haviour. To the best of our knowledge, the EXIST dataset is the rst multilingual cor- pus designed to sexism in hostile to subtle and benevolent sexism. 2 Tasks 2.1 Task Description The EXIST 2021 shared task is de ned as a multilingual classi cation task. In particu- lar, the EXIST challenge accord- ing to two main subtasks: (i) sexism iden- ti cation (task 1), which aims identify if a message or post contains sexist content; and (ii) sexism categorization (task the type of sexism contained in a given sexist message or post. Partici- pants were welcome to present systems that attempt both subtasks or one of them. Task 1 is de ned as a binary classi cation problem, where every system should deter- mine whether a text or message is sexist or not. It includes any type of sexist expression or related phenomena, like descriptive or re- ported assertions where the sexist message is a report or a description of a sexist event. In particular, we consider the tweet or not express any sexist behaviour or dis- course. Once a message has been classi ed as sex- ist, task 2 aims to categorize the message according to the type of sexism it encloses. The categorization has been revised by two experts in gender issues, Trinidad Donoso and Miriam Comet from the University of Barcelona, and takes into account the di er- ent aspects of women that are undermined. This task is de ned as a multi-class classi - cation problem where each sexist tweet or one of the 5 following classes: rejects inequality between men and as victims of gender-based oppression. Stereotyping and dominance: The text ideas about womenthat suggest they are more suitable to ful ll certain (mother, wife, family caregiver, etc), or men are somehow superior to women. Objecti cation: The text presents women as objects apart from their dig- nity and personal aspects, or assumes or describes certain physical qualities that women must have in order to ful ll tra- ditional gender roles bodies Sexual suggestions, requests for sexual favors or harassment of a sexual nature (rape or sexual as- sault) are made. Misogyny and non-sexual in Table 1. 2.2 Evaluation Measures and Baselines In order to evaluate the performance of the di erent approaches proposed by the partici- pants, we will use the Evaluation Framework EvALL4(Amig\u0013 o 2017; Amig\u0013 o, Spina, and Carrillo-de Albornoz, 2018; Amig\u0013 o et al., 2020). Within this framework, we will evalu- ate the system outputs as and F1. All metrics will be also computed by language. In particular, 197Text Task 1 Task 2 Where are all the white w omen at? non-sexist non-sexist Feminism is a war on men, but it's also a war on women. It's a war on female nature, a war on femininity. sexist ideological-inequality Woman careful! sexist stereotyping-dominance ense but I've nev er seen an attractive african american hooker. Not a single one sexist objecti cation I wanna touch y our tits..you can't imagine what I can do on your body. sexist sexual-violence I hate misogyny Sexism Categorization, we will use macro-averaged F1-score to rank the sys- tem outputs. Similarly, we will compute other measures such features each record based on the majority class (Ma- 3 Dataset The EXIST 2021 shared task uses data from Twitter and Gab in English and Spanish. Twitter data was used for both training and testing while Gab was only included in the EXIST test set so that it can be analysed the di erences between social networks with and without \\content control\". In order to provide training and testing data for both tasks, we have collected a number of popular expressions and terms, both in English and Spanish, commonly used to underestimate the role of women in our society. The terms have been extracted from di erent sources: (i) other com- mon hashtags that are not so frequently in sexist contexts in order between and non-sexist expressions. These terms were analysed and ltered by Trinidad Donoso and Miriam Comet, which examined examples of tweets extracted using these terms as seeds. The nal set contains 116 seed terms for Spanish and 109 for English. We used the Twitter API to search for tweets written in English or Spanish contain- ing some of the selected keywords selected keywords. The 662,895 for English. To ensure an appropriate balance between seeds, we have removed those with less than 60 tweets. The nal set of seeds used contains 91 seeds for Spanish and 93 seeds for English. To ex- tract posts from Gab (gabs), we downloaded the most recent Gab dump from 2020) containing the selected keywords. We gathered 1853 gabs for Spanish between the 12th September 2016 and the 12th August 2019, and 1,356,266 between 12th August 2016 and 12th August 2019 for English. In this case, we did not remove any information since we did not have many gabs for Span- ish. We only could nd 38 seeds for Spanish and 81 for English, introducing a consider- able seed bias for this subset of the dataset. The sampling process was di erent for each data source. Regarding Twitter, ap- proximately 50 tweets (50 tweets for Spanish and 48 for English) were randomly selected for each seed term within the period from 1st to 31st of December 2020 for the training set, and 22 tweets per seed within the period from 1st to 28th February of 2021 for the 5https://everydaysexism.com/ 6https:// les.pushshift.io/gab/ Francisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrilo-de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, Miriam Comet, Trinidad Donoso 198each language to build the nal sampled set composed of 4500 tweets per language for the training set and 2000 tweets per language for the test set. The Gab sampling process was more complex since we did not have an uni- form distribution of gabs by seed. available seeds and removed gabs containing numer- ous. Previously, we removed gabs from users with more information to mitigate user bias. The nal sampled set was composed of 500 gabs for each language. The whole sampling process was de ned taking into account di erent sources of bias. In particular, we considered three main sources of bias: seed, temporal and user bias. We tried to mitigate seed bias by includ- ing a wide range of which used in both sexist and non-sexist context (116 terms for Spanish and 109 for English). To con- trol temporal bias, we set a temporal gap of one month (January) between the train- ing and test data and checked the temporal gap between tweets for each seed (around 0.5 days for training and 1 day for testing) to ensure that data is spread over all appropriate balance. In particular, around 1 message was generated per user except for gabs in Spanish where each user posted 2 gabs. We also took into account this principle to split the dataset into training and test sets and removed from the test set users who were also present in the training set to avoid user bias. The sampled data sets were experts in gender issues an annotation guide in English and Spanish in which we provided a clear explanation of each label along with a number of examples. In order to evaluate the quality of the annotation guide, three ex- perts a 0.58 kappa for task 1 and 2. These results indicated a moderate aligns with the the sexism detection task from a broad perspective is not simple. Sexism is even more subjective than experiment were used to modify the annota- tion guide. Then, we did an annotation experiment using MTurk. To this aim, a gold standard was created and labeled by two experts (one man and one woman with 2 years of expe- rience in sexism classi cation), whose third ex- perienced contributor. It was composed of 100 Spanish tweets and 100 English tweets. Each tweet from the in USA or UK for English, location in Spain, USA or Chile for Spanish, work ap- proval rate bigger than 98% and inter- the ma- jority vote from annotators to the label selected by the experts. Table 2 shows results for this experiment. As we to the majority vote between 5 annotators all cases to select annotators). Texts in one class for task 1 (binary problem) and with disagreement for task 2 (2 categories with 2 votes) were manually reviewed by two experts (one man and one woman) with more than two years of expe- rience analysing sexist content in net- works. Around 10% of all posts were changed by the experts for English and 14% for Span- ish. We implemented (deviation from label distribution by annotator, time to complete the task, etc.). The nal EX- IST dataset consists of 6977 tweets for train- ing and for testing, where both sets are randomly selected from the 9000 and 4000 sampled sets, training, and test respec- tively, to ensure class balancing according to obtaining 492 gabs in English and 490 in Spanish from the 500 la- beled sets. We discarded posts in both data sources due to a since Mturk does not sup- port The training data was takes values or \\es\". text: contains the text. nes whether text is sexist takes values \\sexist\" and \\non- not express any sexist behaviours or discourses. Concerning the test data, we removed and \\task2\" labels from the le that was provided to the participants. Once the evaluation phase was over, we shared the labels for the test set in case participants description of the dataset, as well as the number of texts per class for both training and test sets, and the distribution by language. 4 Overview of the Submitted Approaches 76 groups from 11 countries (Spain, China, Germany, India, Italy, Mexico, Austria, Switzerland, England, Greece, and Pakistan) of them sub- mitted runs for task 1, and 27 for task 2. In this challenge, each team had the chance to submit a maximum of 6 runs, 3 runs for each task. We received a total of 70 runs for task 1 and 61 runs for task 2. Regarding the classi cation approaches, the majority of participants exploited both tasks. 23 teams used some sort of trans- former architecture, of which 14 teams used BERT (Devlin et al., 2019) (or multilingual BERT - mBERT), 10 used a Spanish version of BERT called BETO (Canete et al., 2020), 6 used RoBERTa (Liu et al., 2019) and 5 used a multilingual version of RoBERTa called XLM-R (Conneau et al., 2019). Traditional machine learning methods like Support Vector Machines (SVM), Random Forest (RF), or Logistic Regression a few experimented methods (i.e. Long short-term memory networks - LSTM) and with the fastText library (Joulin et al., 2017). Following, we list the participants and brie y describe the approaches used by each group. AI-UPV participated in both tasks and one run for task. They used an ensemble of di erent transformer models with BERT for English, BETO for Spanish and mBERT for multilingual mod- els. They also implemented individual mod- els with translation for both English and Spanish texts. AIT FHSTP participated in both tasks and submitted 3 runs for each task. Their best approach to the task is based on a ne-tuned XLM-R dataset (Basile et al., 2019). Alclatos submitted 3 runs for each task. Francisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrilo-de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, Miriam Comet, Trinidad Donoso 200Training Test Spanish English Spanish English Twitter Twitter Twitter Gab Twitter Gab Total Sexist 1741 1636 858 265 858 300 5658 Non-sexist 1800 1800 812 225 858 192 5687 Ideological-inequalit y 480 386 215 73 233 100 1487 Misogyn y-non-sexual-violence 401 284 199 58 152 63 1157 Objecti cation 244 256 124 50 121 29 824 Sexual-violence 173 344 131 71 150 48 917 Stereot yping-dominance 443 366 189 13 202 60 1273 Table 3: Dataset distribution. Their best system was based on transformers, where BETO was used for Spanish messages and RoBERTa for English. Almuoes3 submitted one run task 1, used an ensemble of the 3 models whereas LR was used for task 2. Andrea Lisa submitted one run for each task. They proposed a They submitted run for each task. Their system was an ensemble of 3 models for Spanish (BETO) and English (BERT) with di erent hyperparameter con gurations for task 1. For task 2, they ne-tuned one model for each language using only the sexist texts. Free submitted one run for each task. They trained one model for each language, RoBERTa for English and mBERT for Span- ish. GuillemGSubies submitted 3 runs Their best system used back transla- tion from English to Spanish and vice versa. They ne-tuned BERT for English texts and BETO for Spanish. IREL group submitted 3 a RoBERTa model using unlabeled data predictions. LaSTUS submitted one run for each a multilingual transformer erent transformer model for each language: DEBERTA (He et al., 2020) for English and XLM-R for Spanish. They applied an LSTM network for each Their was based on a Graph Convolutional Network (GCN) task link between words. MessGroupELL in task three di runs. Their best ap- proach consisted in each language: XGBoost, SVM, RoBERTa task sub- mitted one run. a voting label prediction as input the output of three di erent models. The rst two models used BERT for English texts and BETO for Spanish, and the last model used mBERT. Multiaztertest only participated in task 1 with three di erent runs. Their best run used a di erent model for each lan- guage whereas LR was their solution 2. nlpuned team submitted 3 runs for each task. They multi-task learning ap- proaches. Their best for 1 a results for task 2. ORDS CLAN submitted 3 runs for task. icon. They used XLM-R as base model and the outputs from the last 4 hidden layers were fed into a BiLSTM layer. Recognai submitted 2 runs for each task. Their di erent transformer model for each language: RoB-Tw (Barbieri et al., for English texts BETO for Spanish. SINAI-TL submitted 3 auxiliary for Spanish and BERT for English were used as base models. Their best run used polarity classi cation as the auxiliary task by shared model with the InterTASS dataset (Mart\u0013 \u0010nez-C\u0013 amara et al., 2017). runs for task 1 and 2 runs for task 2. They employed two machine learning techniques (RF, SVM) and one deep ployed some words used in the English tweets or hashtags count. For task 1, RF was their best result and SVM for task 2. Sexist submitted 3 runs for each task. They experimented and transformer-based model achieved the best results. Ujaonly participated in task 1 with one run. They used a di erent transformer model for each language: for Spanish. di transformer for for detection and studied the applicability the detection of They di ne-tuned di erent models (mBERT, RoBERTa XLM-R) and conducted soft voting the predicted results of the three models. ZZW submitted one task. 2 were evaluated independently. In the following subsections, we will show re- sults for each task and language. Teams were ranked by accuracy for task 1 and macro- averaged F1-score (F1) for 2. evaluation metrics such Recall. 5.1 Task 1 teams participated in task 1 for both, En- glish and Spanish, presenting 70 runs in to- tal. In Table 4, the best run for each team is shown, as well as the two baselines: Base- linesvm t df and Majority Class. All runs ranking is available at the task website9. Regarding the best run ranking, 26 teams achieved an Accuracy above the Base- are below 9http://nlp.uned.es/exist2021/ Francisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrilo-de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, Miriam Comet, Trinidad Donoso 202Ranking Team run Accuracy base- line, The best performing is overall of 0.7804. InAI-UPV the participants exploited an ensemble of transformers di er- ent con gurations: multilingual, language- speci c, and language-speci (via translation). The baseline based on majority vote was one of the worst- performing solutions (29 of 33). Although the languages, we also presented rank- by language (English and Spanish) for each task. Table 5 shows the top-10 runs for English and Table 6 for Spanish. Regarding the English results, SINAI-TL achieved the best results with a multi-task approach withtwo each language. The win- ning team AI-UPV ranked third with around 1% di erence in accuracy. Regard- techniques, all based on these Traditional two may suggest that transformer-based models bene t from train- ing with more data from related tasks, even if the EXIST dataset is one of the largest cor- pus in this area. It is 2%) between Span- ish and English tasks. We expected that transformers models would perform better in English since they have been trained on corpus mainly composed of English texts. However, datasets, multilingual transformers per- form very well for this language. 5.2 Task 2 27 teams participated in task 2 for both, En- glish and Spanish, presenting 61 runs in to- tal. In Table 7, the best run for each team is shown, as well as the two baselines. all achieved an F1 the Baseline svm df, while only 3 teams are below the benchmark model. For the Major- baseline, 27 teams achieved a higher F1, whereas only 1 team is below the base- It is interesting to highlight the F1 ranging from 0.5787 to 0.1069. The best performing team for task 2 is again AI-UPV. The worst have been solve the task. 8 and 9 show results for the top-10 teams in English and Spanish respec- tively. Again, the task winner AI-UPV per- formed better in Spanish than in an enhanced ver- sion of BERT and RoBERTa models. In this task, the di erence in performance between English and Spanish increases. How- ever, it is important relatively low results, showing the di\u000eculty of this task. 6 Conclusions In this paper, we have presented the re- sults of the rst shared task on sexism de- tection in a broad sense, tion systems in multilingual scenarios (En- along with di cial networks (Twitter and Gab). The runs that of categorization still remains a challenging problem. We found out that modern transformer-based models overcome Francisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrilo-de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, Miriam Comet, Trinidad Donoso 204Ranking Team run Accuracy sexism detection in social networks is chal- lenging, with a room for high number of participating teams at EXIST 2021 community around sexism detection in social networks. We think that provided dataset will foster research on this topic. Acknowledgments This work was supported by the Spanish Ministry of Science and Innovation under the MISMIS research project on E., J. Albornoz, M. J. F. Verdejo. 2017. Evall: Open access evaluation for information access systems. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 1301{1304. Amig\u0013 o, E., J. Gonzalo, S. Mizzaro, and J. Carrillo-de Albornoz. 2020. An e ec- tiveness metric for 2018. An In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval , pages 625{634. Barbieri, F., L. Camacho- Collados. 2021. Xlm-t: arXiv:2104.12250. Barbieri, F., J. Camacho-Collados, L. Es- pinosa and L. Neves. Basile, V., C. Bosco, E. Fersini, N. Deb- ora, V. Patti, F. M. R. Pardo, P. Rosso,M. Sanguinetti, et al. 2019. Semeval- 2019 task 5: Multilingual detection of hate speech against immigrants and women in twitter. In 13th International Workshop on Semantic Evaluation , pages 54{63. As- sociation for Computational Basile, V. Patti. 2018. Hurtlex: A multilingual lexicon of words to J., S. Zannettou, B. J. Blackburn. 2020. The pushshift reddit dataset. In Proceedings of the international AAAI conference on web and social media , volume 14, pages 830{839. Berg, S. H. 2006. J., G. Chaperon, R. Fuentes, and J. P\u0013 erez. 2020. Spanish pre-trained bert model and evaluation data. at ICLR, 2020. Chiril, P., V. Moriceau, Benamara, A. Mari, G. Origgi, and M. Coulomb- Gully. 2020. He said \\who's gonna take care of your children when you are at ACL?\": Reported sexist acts are not sex- ist. In Proceedings Annual Meeting of the Association for Computa- tional V. Chaudhary, G. Wenzek, F. Guzm\u0013 an, E. Grave, M. Ott, Zettlemoyer, and V. Pre-training deep bidirectional transformers for lan- guage understanding. In Proceedings of the 2019 Conference of the North Amer- ican Chapter of the Association for Com- putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171{4186, Minneapolis, Minnesota, June. Association for Compu- tational Linguistics. Francisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrilo-de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, Miriam Comet, Trinidad Donoso 206Donoso-V\u0013 azquez, T. and Rebollo-Catal\u0013 an. 2018. Violencias de g\u0013 enero en entornos virtuales. Ediciones Octaedro. Fast, E., B. Chen, and M. S. Bernstein. 2016. Empath: Understanding topic signals in large-scale text. In Proceedings of the 2016 CHI conference on human factors in computing systems, pages 4647{4657. Fersini, E., P. Rosso, and M. Anzovino. 2018. Overview of the task on at ibereval 2018. IberEval@ SEPLN, 2150:214{228. Fox, C. Cruz, and 2015. Perpet- uating online sexism Rosso. 2019. Online speech women: Automatic identi ca- tion of misogyny and sexism on twitter. Journal of Intelligent & Fuzzy Systems , 36(5):4743{4752. He, P., X. Liu, J. Gao, and W. Chen. 2020. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint Bag 15th Chapter of the Association for Computa- tional Linguistics: Volume for Com- putational Linguistics, April. Liu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettle- moyer, and V. University Press. \u0010nez-C\u0013 sexism . Cam- bridge University Press. Montes, M., P. Rosso, J. Gonzalo, E. Arag\u0013 on, R. Agerri, M. \u0013Angel \u0013Alvarez Carmona,E.\u0013Alvarez Mellado, J. C. de Albornoz, Chiruzzo, L. Freitas, H. G. Adorno, Y. Guti\u0013 errez, S. Lima, A. Montejo-R\u0013 aez, F. M. P. de Arco, and M. Taul\u0013 e. 2021. 2020. Automatic classi cation of sexism in social networks: An empirical study on twitter data. IEEE Access, 8:219563{219576. Swim, J., L. Hyers, L. Cohen, and M. Fer- guson. 2001. Everyday sexism: Evidence for its incidence, nature, and psychologi- cal impact from three daily diary studies. Journal of Social Issues , 57:31 { 53. Waseem, Z. 2016. Are you racist or am I seeing things? annotator in uence on hate speech detection on Twitter. In Pro- ceedings of the First Workshop on NLP and Computational Social Science , of the NAACL Student Re- search Workshop, pages 88{93, San Diego, California, June. Association de DETOXIS en IberLEF 2021: DEtecci\u0013 on de TOXicidad en comentarios Espa~ Mariona Taul\u0013 e1, Alejandro Ariza1, Montserrat Nofre1, Enrique o2, Paolo 1CLiC, Universitat de Barcelona, Spain 2Research Group in NLP and IR, Universidad Nacional de Educaci\u0013 on a Distancia, Spain 3PRHLT Research Center, DETOXIS task, DEtection of TOxicity in comments In Spanish, which took place as part of the IberLEF 2021 Workshop on Iberian Languages Evaluation Forum Conference. We describe the NewsCom-TOX dataset used for training and testing the systems, the metrics applied for their evaluation and the results obtained by the submitted approaches. We also provide an error analysis of the la tarea DETOXIS, DEtecci\u0013 on de TOxici- dad en comentarios en espa~ nol, que tuvo lugar en el Iberian Languages Evaluation Forum workshop (IberLEF 2021) en el congreso de la SEPLN 2021. Se describe el corpus NewsCom-TOX utilizado para entrenar y evaluar los sistemas, las m\u0013 etricas para evaluarlos y los resultados obtenidos por las distintas aproximaciones utilizadas. Se proporciona tambi\u0013 en un an\u0013 alisis de los resultados obtenidos por estos sistemas. Palabras clave: Detecci\u0013 on is the detec- tion of toxicity in in related task, which are described in Section 2. The presence of toxic messages on social media and the need to identify and mitigate them leads to the development of systems for their automatic detection. The automatic detection of toxic language, espe- cially in tweets and comments, is a task that has attracted growing interest from the Nat- ural Language Processing (NLP) community in recent years. This interest is re ected in the diversity of the shared tasks that have been organized recently, among which we highlight those held over the two years: HateEval-20191(Basile et al., 2019) on 1https://competitions.codalab.org/competitions/19935hate speech against immigrants and women in English and Spanish tweets; TRAC-2 task on Aggression Identi cation2(Kumar et al., 2020) English, Bengali and Hindi in comments extracted from YouTube; o ensive et O for German on Twitter (Stru\u0019 et al., 2019); Multilingual Toxic Comment Classi ca- tion Challenge5, in which the task is focused on building multilingual models (English, French, German, Italian, Portuguese, Rus- sian with English-only training data from Wikipedia comments. DETOXIS is septiembre de 2021, pp. 209-221 para del Lenguaje Naturalon the detection of di erent levels of in articles written in Spanish. The main nov- elty of the present task is, on the one hand, the methodology applied to the annotation of the dataset that will be used for training and testing the participant models and, on the other hand, the evaluation metrics that will be applied to evaluating the participant models in terms of their system use pro le applying four di erent metrics (F-measure, The methodology proposed toxicity by taking into account the con- textual the erent levels of toxicity. The rest of the overview is structured as follows. In Section 2 we present the two sub- tasks of DETOXIS. In Section 3 the corpus NewsCom-TOX used as dataset is described together with the way it was gathered and annotated. In Section 4 the di erent metrics used for the evaluation of the systems and the results they obtained are presented, as well as a description of the techniques and models used by systems. Finally, in Section 5 the conclusions are drawn. 2 Task Description The aim of the DETOXIS task is the detec- tion of toxicity in in related (toxic=no). is a more ne-grained classi cation task in which the aim is to identify the level of toxicity also al- lowed to just were also encouraged to submit mul- tiple used online selected The comments were selected in the same order in which they appear in the time thread in the web. The author (anonymized), the date and the time when the comment posted were also retrieved. The number of comments ranges from 65 to 359 comments per article. On average, 31.16% of the com- ments are toxic. 3.1 Annotation Scheme We considered that a comment is toxic when attacks, threatens, o ends, deni- grates or disquali es a group of people on the basis of characteristics such as race, ethnicity, religion, gender and sexual orientation, among others. This attack can be expressed in di erent ways {explicitly (through and most aggressive, the latter being those comments that incite hate or even physical violence). We use tox- icity as an umbrella de (Waseem and 2016), (Schmidt (Davidson et abusive (Nobata et 2016), aggres- toxic (Kolhatkar et Mariona Taul\u00e9, Alejandro Ariza, Montserrat Nofre, amig\u00f3, Paolo Rosso 210al., 2020) or o ensive ?) di di erent aspects comment in those that were annotated rst as toxic8. (1a) Contra la desinformaci\u0013 on datos. En el INE ten\u0013 eis los datos de los inmigrantes. Delitos, cotizaciones, prestaciones, etc. <toxicity=not> <toxicity level=0> (1b) Against misleading data. In the INE immigrants. contributions, subsidies, etc. rabble news programs <toxicity=yes > <toxicity level=1> (3a) Lo que importa en realidad es sentirse mejor con uno mismo. Sumar karma. A los putos negros les pueden joder bien. <toxicity=yes > <toxicity level=2> (3b) What really matters is feeling better about yourself. Accumulating karma. Fucking blacks can get fucked up pretty good. <toxicity=yes > <toxicity level=2> (4a) A estos putos animales sarnosos que los encierren y tiren la llave. <toxicity=yes > <toxicity level=3> (4b) With these mangy fucking animals, lock them up and throw away the key. <toxicity=yes > <toxicity level=3> In addition to annotating whether or not a comment was toxic and its level of toxic- the value `1' is assigned when the feature is present). We rst annotated these fea- tures and we then used them to establish the toxicity of comments and to determine their level of toxicity. It is worth noting that the level of toxicity is especially determined by the type of features combined. All this in- formation was included only in the training dataset that was used for the task. In or- der to assign each of these features and be able to interpret the global meaning of com- ments, it was crucial to take the context into account, that is, the conversational thread. the comment is direct comment to an article and `2' for indicating that the comment is a comment. `topic' also comment an inter-annotator agreement test was carried out once all the comments on each article had been anno- tated. Then, disagreements agreement was reached. The team of two trained anno- tators, who were linguistics. Ta- the results obtained We provided participants with 80% of the NewsCom-TOX corpus for training their models (3,463 comments), and the remain- ing 20% of the corpus (896 comments) was used for testing their models.9Both train- ing and test les were provided in csvfor- mat. The training dataset contains all the features included in the annotation of the NewsCom-TOX corpus (see Subsection 3.1), level. The dataset con- 996 mildly toxic (21.90%), 310 toxic (7.36%) and 79 very toxic (1.81% ). 4 Systems and Results This section contains of the evaluation and their implica- tion when models some interesting systems insights and a brief error analysis of the submitted approaches are presented. 4.1 Baselines A benchmark was set with the introduction of three di erent RandomClassi of toxicity values f0, 1, 2, 3g to each comment in the test set without any kind of weighting strategy. Second, the BOW- Classi er consists of a Support Vector Classi er (SVC) that receives the features ex- tracted by a TF-IDF (an advanced version of the classical Bag-Of-Words tech- nique). In particular, the SVC model uses 9In order to avoid any con ict with the sources of comments regarding their Intellectual Property par- ticipant that was interested in the task. The corpus will be only available for research purposes.a linear kernel with a detection), with the same con gura- tion as mentioned before. It is worth their baseline nature, no hyperparameter optimization was performed on any of the models. In addition, all base- lines were implemented used. Table par- ticipating teams according to F-measure. In general, the SINAI system outperforms the rest of systems. SINAI is outperformed by some of the other approaches in terms of recall, although at the cost of mid-ranking po- sition in the ranking, with low precision but medium high recall. In a similar position is ChainBOW, with low recall and medium high precision. In general, is signi cant room for improvement between participants runs and the and recall scores achieved by the systems. The precision of all systems is between 0.25 and 0.75. Mariona Taul\u00e9, Alejandro Ariza, Montserrat Nofre, Enrique amig\u00f3, Paolo Rosso 212Feature Comments Percentage mildly toxic (level 1) 996 21.90% toxic (level 2) 310 7.36% very toxic (level 3) 79 1.81% Total for the best in Subtask 1. 4.3 Subtask 2: Toxicity 2, the tion task is a more ne grained classi cation task in which the aim is to identify the level of toxicity dicted and real categories. based absolute First, it assumes no equidistance between categories. That is, an error between category 0 and 2 is not necessarily twice as serious as an error between categories 0 and 1. In addition, it assigns more CEM is sets, and when the relative distance between categories beyond the way Rank Biased Precision (RBP): RBP is a metric (Mo at and Zobel,2008). We rank the output of the sys- tem and the Gold Standard on the basis of toxicity levels (from highest Basi- cally, RBP is computed as the sum of the actual values (Gold Standard) along the ranking generated by the system with a weight function that decreases as we de- cay in the ranking of the system output. dcorresponds to the categorized items in the system output s, withpos(d) being the ranking position of din the system outputsandg(d) being the real cate- gory value in the pis at Origi- applies to rankings with one item in each ranking position. However, in our scenario, the items are ordered in four levels in the ranking generated by the system (level 4= not toxic; level 3= mildly toxic; level the the ties: f(d) =1 ns(d)MaxPos(d)X i=MinPos(d)pi\u00001 wherens(d)represents the out- put.MinPos(d) and This metric is when items according to their toxicity. For instance, the scenario in which IberLEF In Spanish 213 most toxic comments ts in this metric. Accuracy: is the classi cation does not consider the order cate- gories. That is, an error is penalised regardless of the distance to the actual category. Furthermore, it does not com- pensate for the e ect of imbalance in the data set. This metric is not appropriate for most possible scenarios, although it has the advantage of being very easy to interpret in terms of the percentage of hits. Mean av- equidistance between categories and does not for the e ect of imbalance between cate- gories in the data set. MAE is priate, for example, when predicting the average between categories. The di erence with respect to MAE is that it does not require the system to predict the category of the actual scale. In addition, it compensates for the e ect of imbalance between categories by giving more weight to those categories that are more infrequent. Obtaining a high Pearson value is interesting, for example, when predicting the evolution of toxicity over time in a comment stream or compare the average toxicity of two streams. Table 6 shows the results obtained by the best run of each team in terms of CEM. The baselines are in the middle positions of the systems ranking. In this case, the BOW- Classi er approach performs better the systems SINAI and Team Sabari outper- form most of the other approaches for all metrics. There is only one exception. Thesystem output DCG outperforms SINAI and Sabari terms of ranking (RBP metric). This means that, to Figure 2, we nd a between CEM consider intervals. the proximity of the esti- mated categories to the values (numeric cat- egory labels) in the system output and the gold standard. This means that the e ect of 'scale shifts' and the e ect of assuming numeric the CEM metric MAE metrics. The ference is that compen- sates for the imbalance between categories in the data set, giving more weight to errors or hits in infrequent categories. As Figure 2 shows, there is a set of runs that obtain simi- lar Accuracy we interested in predicting or ap- proximating the actual category in as many cases as possible. For example, this would be the case if we wanted to calculate the average toxicity of a set of comments. However, if we are interested in detecting particular cases of toxicity (low, medium or high) then we can assert that some of these runs will be more e ective than others. 4.4 Systems Insights A total of 31 teams (Subtask 1) and 24 teams (Subtask 2) sent a of ve submissions to be ated. Not surprisingly, and based on recent success of transfer learning with pre- trained Transformer-like language models, the top ve teams achieved their best scores using BETO10(the ish version of the BERT model). The dif- ference in performance between their re- spective submissions Mariona Correspondence preprocessing steps. Although the of classical machine learn- as TF-IDF with Random Forests/Support Vector Machines/Logistic Regression Classi ers were not as high as those achieved by BETO, they were also used as in- credibly even the language of the dataset (comments in Spanish) di ers from the usual English language present in most of the gen- eral benchmark NLP datasets. di erent directionality of the text, as well as for the selection of other pre-processing steps. For instance, the GTH-UPM team performed an analysis multiple pre-processing steps prior to a pre-trained Transformer- Entities such respective shared token) improves BETO's performance as opposed to analysis that is worth paying attention to regarding the language of the texts is a performance comparison among pretrained els with a Spanish-to-English transla- tor and Spanish models. Alejandro Mosquera built a stacked model composed of at least one model for each variant in conjunction with additional features and extracted the feature importance of the individual models via cross-validation on the training dataset. The results show that pretrained multilin- gual models such as XLM do not provide as much predictive power in this toxicity context as a neural network a Span- pronounced when we compare it with the translator followed by the model trained on English embeddings. A similar compari- son was performed by the AI-UPV team task in which classical machine learning models (both generative and discriminative), multi- conclude that transformer-like architectures outperform classical statistical models in this complex task, they also Overview of DETOXIS at than multilingual for all of their top ve best con gurations and in both subtasks. Furthermore, Alejandro Mosquera's was the only team to introduce side informa- tion to the topic to and the belong. As a preliminary analysis using the use of present in the thread/conversation seems to enrich overall model performance. Further research on how to better exploit thread and topic tion may be not subtasks (SINAI team proved the importance of enriching the model by ne- tuning it on similar tasks related to senti- ment and analysis, thus training and making more thanks to this Multi-Task Learning strategy. In fact, SINAI was the only team in the competition that took the provided extra features (see the other par- study of the in extra feature on the model's predic- tive power is yet to be performed. 4.5 Error Analysis The tasks of toxicity and toxicity level de- tection are particularly di\u000ecult for machine learning models. In fact, even the most re- cent transformer models struggle the includes 13 additional features that classify the text according to other helpful semantic dimensions from the raw can most chal- lenging comments the di\u000eculty of their detection. Ta- ble 3 contains the average performance of the top ve best submissions per task (based on the each feature is present and a single (best) run per team. Regarding the rst task (toxicity detec- tion), the o\u000ecial metric focuses on the the models to detect all toxic com- ments and pays zero attention non- toxic comments) that are marked as toxic are usually the most di\u000ecult ones to detect (see examples 5, 8, 10, 13 and 14 in Table 4). A similar situation appears in comments with a Other dimensions that the di\u000eculty of this task are sarcasm andstereo- make an informed decision on whether the comment is toxic or not. On the other hand, the di\u000eculty of lin- of level toxic be really the receiver. Consequently, an e ort to systems is utmost instance, example 12 is usually the top performing submitted systems although the actual toxicity is at its maximum level due to the implicit aggressiveness within the mes- sage. This di CEM Although all of the top- performing systems made use of the Spanish version of the BERT model (a Transformer encoder), a variety of insights vious challenges on hate speech, we have Mariona Taul\u00e9, Alejandro Ariza, Montserrat Nofre, Enrique amig\u00f3, Paolo Rosso 216Feature Size (Toxic) argumentation constructiveness 0.8465 0.6764 5. target person 108 (90) 0.7243 0.5672 6. target group 67 (62) 0.5444 7. (38) in each for the subset of comments the corresponding is positive the of each and the number of toxic instances in them. provided 13 additional features and a ne- grained toxicity degree target with four pos- sible Furthermore, the thread to which they belong and the topic of the article they are posted in, thus, allowing multilingual models are outperformed by the Spanish counterparts in this challenge most probably due to the speci c embedding space fully optimized in the given language and the use of a more language-oriented token vocabulary. Tech- niques such addition of Multi-Task Learning with datasets belong- ing to similar tasks in order to increase the available data used at the netuning step has turned out to be bene cial in a scenario in which the number of comments small were unbalanced. more elab- orate ways to extract information from the conversation and the topic are needed, the only com- mon strategies. Finally, it is important to mention that, depending on the nal application and its speci c requirements, the selection of the model may require a di erent evaluation met- ric. Thanks to our selection of metrics, we were able to provide a simple visualization of di erent trade-o s when opting for a certainsystem and how models perfectly suitable for a particular goal may not perform at the same level when those requisites change. A clear example we have presented is parison between SINAI and DCG sys- tems, in in scenarios in which prioritizing the most toxic comments is the priority (according to the RBP ranking metric). Regarding future work, systems are as yet far from the Gold Standard and this is mainly due to the di\u000eculty these models each individual feature makes this an even more challenging task. Additional research on the in uence of these implicit features on the nal toxicity of the comment and and ideas are spread faster than ever. Some of the paths that were not explored by participants and could be worth looking into are: larger language mod- els or ments to the semantic contextualization of language models by the use of techniques that reduce their anisotropicity problem Ratio Toxicity Comment 1 8 1.0000 1 Como se echa de menos la opini\u0013 on de los guardianes de la moral en la noticia de la ejecuci\u0013 on en Portland. Bueno, en muchos casos est\u0013 an sus negativos pero no sus siempre dignos comentarios. 2 10, 11 1.0000 1 Directamente :peineta: 3 4 1.0000 1 Los andaluces, no los espa~ noles 4 - 0.9677 1 Y 600000 votos m\u0013 as, para el pandemias. 5 1, 4, 12 0.9677 1 No intentaba poner ni a favor ni en contra. Ni intentaba realizar una cr\u0013 \u0010tica en el origen de los EEUU. Mi comentario solo trataba de dar perspectiva del problema en USA. En mi opini\u0013 on personal todos los usanos con armas podr\u0013 \u0010an salir a la calle hasta matarse los unos a los otros que el mundo en un plazo m\u0013 as corto que largo ir\u0013 \u0010a a mejor. Pero es mi opini\u0013 on. 6 3, 4, 5, 10 0.9677 1 Que f\u0013 acil es hablar bocachancla 7 5 0.9677 1 Expulsion de echenique de espa~ na ya1 8 1, 4, 6, 9, 10 0.9355 1 En Madrid no hay campo, no hablo de la p\u0013 urria de ciudad. Hablo de la gente que todo los a~ nos entra en Espa~ na para hacer estos trabajos estacionales + los que est\u0013 an siempre aqu\u0013 \u0010 y que se han ido 9 4, 11 0.9355 1 Como conseguir 600.000 votos. Vomitivo 10 1, 5 0.9355 1 El segundo es el que le tira al suelo, pero mira el primero tambi\u0013 en, gura. youtu.be/neUnhYO2Ehc 11 6, 7 0.9355 1 Pues m\u0013 as raz\u0013 on para hacer como ellos contra ellos. La diferencia es que nosotros sabemos que esta mal, ellos creen que es lo correcto. Nosotros podemos parar cuando lo estimemos, ellos no. 12 12 0.9355 3 Tengo esperanzas en que legalicen la noche de 'La Purga'. La peli cuenta como las clases acomodadas impulsan esta celebraci\u0013 on para que el lumpen se autorregule. Es sencilla, pero a mi me entretuvo mucho : ) 13 1, 4, 5, 9 0.9355 1 Echenique miente mas que corre....aun regularizando estos irreg- ulares, no serian de hecho \"ciudadanos espa~ noles\" y por lo tanto SIN derecho a voto. Pura propaganda bolchevique. 14 1, 4, 5 0.9355 1 ss. 29.304 simplemente exige que tengas licencia de caza (que no se sabe publicamente si el implicado la tiene o no) [...] Ergo, podemos deducir que el acusado no estaba violando la 948.60(2)(a) al usar un arma larga con 17 a~ nos. Hay que hacer los deberes antes de soltar a rmaciones tan categ\u0013 oricas. O sea que necesita una licencia de caza y como no sabemos si la tiene o no vamos a suponer que SI la tiene pero el que tiene que hacer los deberas antes de hacer a rmaciones categ\u0013 oricas soy yo. Pos fueno, pos fale, pos malegro, campe\u0013 on. al parecer al nal no va a haber acusaci\u0013 on por la tenencia de armas, >Fuente? 15 5, 8 0.9355 2 Si le hacen tantas pregunta a \"NUMERO 3\", es posible que se cortocircuite? 16 4, 13 0.8710 1 Deportaciones masivas ya! Table 4: List of the top 16 comments in the test set that are misclassi ed as non-toxic by the majority of the systems. The Features column shows the feature identi ers from Table as positive for the given comment, FN Rate is the ratio of systems that marked the comment as non-toxic and Toxicity c comment. Acknowledgments The work has been carried out of the following projects: (PGC2018-096212-B), funded by Ministerio CLiC SGR Fondazione Compangia Paolo.References Amig\u0013 o, E., J. Gonzalo, S. Mizzaro, and J. Carrillo-de Albornoz. 2020. E ec- tiveness metric ordinal Formal sults. In Proceedings of the 58th Annual Meeting of tional Linguistics. Association for Compu- tational Linguistics. Basile, V., C. Bosco, E. Fersini, N. Deb- ora, V. Patti, F. M. R. Pardo, P. Rosso, Mariona Taul\u00e9, Alejandro Ariza, Montserrat Nofre, Enrique amig\u00f3, Paolo Rosso 218M. Sanguinetti, et al. 2019. Semeval- 2019 task 5: Multilingual detection of hate speech against immigrants and women in twitter. In 13th International Workshop on Semantic Evaluation , pages 54{63. As- sociation for Computational Linguistics. D. Warmsley, Macy, and I. Weber. 2017. Automated hate guage. In Proceedings of the International AAAI Conference on Web and Social Me- dia, volume 11. Kolhatkar, V., H. Wu, L. Cavasso, E. Fran- cis, K. Shukla, and M. Taboada. 2020. The sfu opinion and comments corpus: A corpus for the news com- ments. Corpus Pragmatics , 4(2):155{190. cation in social media. on Trolling, Aggression and Cyberbullying (TRAC-2018), pages 1{11. Kumar, R., in social media. In Proceed- ings of the Second Workshop on Trolling, Aggression and Cyberbullying, pages 1{5. Mo at, A. and J. Mehdad, and Y. Chang. 2016. Abu- sive language detection in of the 25th interna- tional conference on world wide web, pages 145{153. Nockleby, J. T. 2000. Hate speech. En- cyclopedia of the American constitution , 3(2):1277{1279. Poletto, F., V. Basile, M. Sanguinetti, C. Bosco, and V. Patti. 2020. Resources and benchmark corpora for hate speech detection: a systematic review. Language Resources and Evaluation , 1{47. Schmidt, A. and M. vey on hate speech detection using nat- ural language processing. In Proceedings of the fth international workshop on nat- ural language processing for social media , pages 1{10.Stru\u0019, J. M., M. Siegel, J. Ruppenhofer, M. Wiegand, M. Klenner, et al. 2019. Overview of germeval task 2, 2019 shared task on the cation of o ensive lan- guage. In Preliminary 15th Conference on Natural Language Processing (KONVENS 2019). Waseem, Z. the NAACL student re- search workshop, pages 88{93. Zampieri, M., P. Nakov, S. Rosenthal, P. Atanasova, G. Mubarak, in media (O ensEval 2020). In Proceedings of the Fourteenth both subtasks. Table 5 ranks run participat- ing according to F-measure. Table 6 shows the results obtained by the best run of each team in terms of CEM (column 3). The rest of columns show the results in terms of MAE, RBP, Pearson and Accuracy metrics. The baselines are in the middle positions of systems ranking. Overview of DETOXIS at Enrique amig\u00f3, Paolo Rosso 220Ranking Team CEM MAE RBP Pearson Accuracy Gold Standard 1 1 IberLEF Fake Task Resumen de FakeDeS en IberLEF 2021: Tarea compartida para la detecci\u0013 on de noticias falsas en espa~ nol Helena G\u0013 omez-Adorno1, Juan Pablo Posadas-Dur\u0013 an2, Gemma Capetillo1 1Instituto de Investigaciones en Matem\u0013 aticas Aplicadas y en Universidad Nacional Ciencia e \u0010a Computaci\u0013 overview of FakeDeS 2021, the second edition of this lab under the IberLEF conference. The FakeDeS shared task aims related news This year edition brings two main challenges: thematic and language variation. For this purpose, we introduce a new testing corpus containing news related to COVID- 19 and general de la tarea compartida Fake- DeS 2021, cuya segunda edici\u0013 on ha tenido lugar en 2021 bajo el congreso IberLEF, aunque se trata de la primera vez con esta denominaci\u0013 on. La tarea FakeDeS tiene por objetivo explorar diferentes m\u0013 etodos y estategias relacinados con la detecci\u0013 on de noticias falsas en espa~ nol, principalmente en su variante de M\u0013 exico. La edici\u0013 on de esta \u0010stica. Para ello, se introduce un nuevo corpus de prueba que contiene noticias relacionadas con COVID 19 y noticias de otros pa\u0013 \u0010ses and social networks have increased the fast spreading of news and the power of individuals to create and share their own content, that is usually partial and un-veri ed. Traditional media have been for- ced to adapt to this new online scenario that favours \\spectacle over restraint and veriica- tion\" (Chen, Conroy, and Rubin, 2015). This situation has an impact in the arising of Fake News, which either have the objec- tive manipulate people or have not been conveniently fact-checked. Misinformation is present in every area: politics, science, in sports. It spreads in seconds sands of people. Therefore, systems in order to iden- tify the false information in the web and so- cial media. As every task in NLP, Fake News detec- has and given the number of speakers of Spanish around the world, methods that of misleading news is the previously reviewed news, i.e., annotated corpora. The FakeDeS task is the second edition of the fake new detection task organized in Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 223-231 para el Procesamiento del Lenguaje Naturalthe IberLeF conference. The rst edition was integrated in the MEX-A3T (Arag\u0013 on et al., 2020), a forum for the analysis of social media content in Mexican Spanish, at IberLeF. The 2021 FakeDeS task at Iberlef 2021 (Montes taken into account the global pandemic situation. A new corpus containing news associated with COVID-19 will be used as a testing corpus, while the corpus used in the 2020 edition has been pro- vided as the training set. Our aim is to ex- plore the robustness of methods when trained on generic news and then evaluated in news associated with a very speci c theme. The rest of this paper is organized Section conclusions from this shared task. 2 FakeDeS 2021 Corpus and evaluation framework For the development of the solution propo- sals for the FakeDeS task, the complete cor- pus, i.e. both training and testing partitions of the corpus, used in the previous edition of the task (Fake News Detection track at MEX- A3T) was made available to the participants. The corpus used for the training is a com- pilation of news between January and web: of news- papers and media companies, special websi- tes dedicated to the validation of news following the procedure described in (Posadas-Dur\u0013 an et al., 2019). The training corpus includes true-fake news pairs of di- verse events to have a corpus as balanced as possible. The data contains 971 news divided into 491 real news and 480 fake news. The news compiled is related to and So- ciety) established by the source of the news. Table 2 shows the topic distribution in the train corpus. Participants can evaluate their approach to achieve satisfying performance split used in the previous editionsTopic True Fake Science 46 43 Sport 66 58 Economy 24 19 Education 10 12 Entertainment 70 78 Politics 175 148 Health 23 23 Security 17 25 Society 60 74 \u0006 491 480 Tabla 1: Topic distribution in the train cor- pus. of the task (Arag\u0013 on et al., 2020). In order to evaluate of the submitted ap- proaches, a new set of news were collected to conform the evaluation corpus. The test corpus contains of fake and true publi- cations about di erent events, that were co- llected from November 2020 to March 2021. Di erent sources from the web were used to gather the information, but mainly of two types: 1) newspapers and media fact-cheking in fact-checking. Only two of the the badge issued by IFCN, Helena Bel Enguix, Claudia Porto Capetillo 224on their website they describe the methodo- logy they follow for the fact-checking. For the evaluation, assembled test cor- pus has 572 instances. The were la- beled using the same two classes considered in train set, i.e., true or fake. The test corpus is balanced with respect to these two clas- ses. To compile the true-fake news pair of the test corpus, the following guidelines were fo- llowed: - A fake news is added to the corpus if any of the selected fact-checking sites deter- mines it. its true news coun- terpart is added if there is evidence that it has been published in a reliable site (established newspaper site or media si- te). The topics covered in the test corpus that match the training corpus are Science, Sport, Politics, and Society. Three new to- pics, COVID-19, Environment, and Interna- tional, were added to the test corpus. Table 2 shows the topic distribution in the test cor- pus. The test corpus includes mostly news articles, however, on this occasion social me- in of news. 90 posts were inclu- ded as fake news (15.73 % of the total). This posts were mainly from Facebook and WhatsApp. Topic True Fake Posts Environment 2 2 1 Science 6 7 3 COVID-19 118 119 59 Sport 1 1 0 International 7 7 0 Politics 53 54 17 Society 99 96 10 \u0006 286 286 90 Tabla 2: Topic distribution in the test corpus. The use of the various fact-checking si- tes involved consulting pages from di erent countries that o er content in Spanish in ad- dition to Mexico, so di erent variants of Spa- nish are included in the test corpus. These sites included countries like Argentina, Boli- via, Chile, Colombia, Costa Rica, Ecuador, Spain, United States, France, Peru, Uruguay, England and Venezuela.3 Overview of the Submitted Approaches At this edition, twenty one teams submitted one or more solutions to the task through the codalab platform2. CodaLab Competitions is a powerful that involve or code returns a performance evaluation based on the metrics de ned for each task. This section presents a summary of preprocessing steps, features, and classi cation algorithms. indicate the general approach used for each team. It can be appreciated BERT and Sample (Huang, Xiong, and Jiang, 2021) { Team Spanish attention mechanism. The method consists of taking the rst and last segments of the texts and feeding them into a BERT obtaining of the head and tail beddings; this matrix is used in an attention with the rest of the input (the ned head and nal embedding is formed Neural Networks X BoW, n-grams, Stylometrics Tabla approach on weak classi ers. Their model not only analyzes the content of the news, but also authors analyze the e ectiveness of di erent classi ers, support vector machines, random forest, gradient boosting and Wi- SARD; achieving that adds a dense layer on top of the last layer of the pre-trained BERT mo- del and then trains the whole mo- del on the presence of the training process and the maxi- mum length of the news is set to 512. ForceNLP at FakeDeS 2021: Analysis of Text in ferent news. objective is to analyze the impact of text features in the task of fake news detection. Helena G\u00f3mez-Adorno, Juan Pablo Posadas-Dur\u00e1n, Gemma XLM-RoBERTa-Large. rst ne-tuned the pre- multi-language model XLM- BETO, the { Summary: The authors mo- di ed ALBERT to directly receive the news and concatenate the hid- den state of the rst token sequence of the last three hidden layers into the classi er. The results of the mo- di cation on 5-fold cross-validation show an improvement over the AL- BERT base model. Bribones tras la esmeralda perdi- da@FakeDeS corpora (Lomas-Barrie et al., 2021) { Team name: Bribones tras la esmeralda perdida { Summary: The authors used a simple BOW-based an hypothesis: true fake vocabulary they use.This hypothesis is rejected in the conclusions of the paper. 4 Experimental evaluation and analysis of results This section summarizes the Iber- LEF 2021: Fake News Detection Task. We compare and analyze in de- tail the performance of the submitted solu- For nal phase of the challenge, participants sent their predictions for the test partition, the performance on this data was used to rank them. We used the F1score over the fake class as the main evaluation measu- re. For computing the evaluation scores we re- lied on the Codalab Competitions platform. In this section, we report by participants as evaluated by Codalab baseline implemented th- a cation mo- trained on the of repre- sentation, classi cation classi- cation model based on transformers using BERT. For the baseline, the corpus was pre- processed using techniques described words, punctuation tokens are lo- wercasing, and lemmatization. is a mo- del trained on the BoW representation, classi kernel. second baseline is classi cation trained on the n-grams representation. The preprocessing techniques are the same as in the rst baseline. The text is represen- ted by character 3-grams and all vocabulary The last baseline follows a deep learning approach, it is based on the Bidirectional Encoder Representation from Transformers (BERT) architecture(Devlin et al., 2019). We use the pre-trained model BETO: Overview of FakeDeS at trained the classi er using an Adam optimizer with a learning rate of 2 e\u00005. The epoch and maximum sentence length is 10 and 512, respectively. For the process of - ne tuning and sequence classi cation, we use Bert For Sequence Classi cation as imple- mented in HuggingFace's Transformers (Wolf et al., 2020) which has a linear layer added at the top for classi cation to be used as a sentence classi shows a summary of obtained by each team in the FakeDeS sha- red task. We report the F1score in both fa- ke and true classes, the macro and the accuracy. We used the F1over the fake class to rank participants. In this edition of the FakeDeS shared task, the approach sub- mitted by the GDUFS DM team outperfor- med all the other approaches and the baseli- nes. GDUFS DM used an with an ensemble classi er. These results show that for this task, classic approa- ches are still competitive with respect to deep learning. it is important to mention that the third and fourth best approaches did not send their system description papers. All participated teams that send system of the news. For the analysis of the complementariness and the diversity of the predictions of the dif- ferent approaches we use the Maximum and Coincident Failure Diversity (CFD) metrics(Tang, Suganthan, and Yao, 2006). The MPA is de ned as the ratio between the correctly classi ed instan- ces over the total number instances. An instance is considered as correctly of the participating teams classi ed it correctly. On the other hand, the CFD metric give us the error diversity among or incorrectly me- to the neural networks ticipant team used this approach. The MPA for the row of all teams has the highest va- lue, which means that the team's approaches other. In rent approaches, the Transformes n- grams, Stylometrics approach. However, the MPA of all approaches with paper submis- sions shows a 3 % of increase over these individual approaches, suggesting that the transformers approaches and traditional ap- proaches are complementary to each other. the CFD score are compara- ble among all approaches, which means that their predictions are complementary to me extend, this lead us to conclude that there is still di erent information learned by traditio- nal and transformer based approaches. The Table 6 shows the results of the F1 score for the fake class in the di erent topics of the test corpus. It can be appreciated that the Sports category achieved the overall lo- wer results for all the evaluated approaches, even though there were six systems that co- rrectly classi ed all the instances in this to- pic. This is related to the fact that in this category there are only two news items, one fake and one true. The performance of the systems does not seem related to the availa- bility of the topics in the training set. While there were no news related to Covid-19 in the training set, the results achieved in this topic are comparable to those on topics available in the training set such as Politics and Society. The news belonging to the International topic were the easiest to classify in average, it can be observed that systems achieved F1scores above 15 % and 20 % in this topic. We identi ed the common prediction errors across all the systems and nd that there were only 2 news, both in the true class that none of the approaches classify correctly. All fake news were identi ed by at least one system. Table 7 shows the classi ed instan- Helena G\u00f3mez-Adorno, Juan Posadas-Dur\u00e1n, task on the test set. Approach Best Accuracy MPA CFD Number of of MPA and CFD results between the di erent general approaches. ces, it can be observed that one of the news belong to the Politics category and the other one to the Society category. It is interesting that all news (fake and true) in the Covid-19 topic were classi ed correctly by at least one team. 5 Conclusions This paper described the design and results stands for Fake news Detection in Spanish . This has been the se- cond edition of the task, the rst with this name, since the last year was presented as the fake news detection track in the wider shared task MEX-A3T (Arag\u0013 on et al., 2020). Although the best results in this shared task were reached by a team that proposed toapproach the problem with method based results. This seems to indicate the complexity of the task, that needs to be tackled by systems that con- sider The results that fake news de- tection is a hard problem in the area of Na- tural Language Processing. The development of techniques especially designed to generate disinformation is a challenge for the area that has as a goal to identify such disinformation. Summing up, the FakeDeS evaluation task promotes the work in Spanish in this crucial area of NLP, encourages the scienti c exchan- ge between researchers, and corpus Spanish openly lable the scienti at Detection in Spanish Shared 229Team Politics Society International Sports 0.62 0.50 1.00 by topics. Topic Source Title Politics El Mundo No, no ha habido un misterioso apag\u0013 on en el Vaticano... Society El Comercio La falsa noticia de la empleada que defeca en la mesa de su jefe ... Tabla 7: True Instances Missclassi ed as Fake by Systems. and the cutting-edge methods implemented by the participants, help to place Spanish among the languages with an increasing num- ber of resources and experiments oriented to fake news detection. The use of transformers again achieved the best results this year showing the the INAOE Supercomputing Labo- ratory's Deep Learning Langua- Helena G\u00f3mez-Adorno, Juan Pablo Posadas-Dur\u00e1n, Gemma Bel Enguix, Claudia Arag\u0013 y G\u0013 omez, H. L. V. Pineda, H. G\u0013 omez-Adorno, J. Posadas-Dur\u0013 an, 2020) co-located with 36th Conference of the Spanish Society for Natural Language Pro- cessing (SEPLN 2020). Chen, Y., N. Conroy, and V. Rubin. 2015. News in an online world: The need for an \\automatic crap detector\". Proceedings of the Association for Information Science and Technology, 52(1):1{4. Devlin, J., M.-W. Chang, K. Lee, and K. Tou- at FakeDeS fa- ke with BERT Ibe- Lara, and 2021. Bribones Iberian Languages Evaluation Forum (IberLEF 2021).Montes, M., P. Rosso, J. Arag\u0013 on, R. Agerri, M. \u0013A.\u0013Alvarez-Carmona, E. \u0013A. Mellado, J. C. de Albornoz, Chi- ruzzo, H. G\u0013 omez-Adorno, Y. Guti\u0013 errez, S. M. J. Zafra, S. Lima, F. M. P. de Arco, and M. Taul\u0013 e, editors. G. J. J. M. Escobar. 2019. Detection of fake news in a new corpus for the spanish language. Journal of Intelli- gent & Fuzzy Systems, 36(5):4869{4876. Reyes-Maga~ na, J. Lusquino-Filho, F. M. G. Fran\u0018 ca, P. M. V. Lima, and E. de Oliveira. 2021. LCAD Tang, E. K., P. N. Suganthan, and X. Yao. 2006. An analysis of diversity measures. Mach. Learn. , 65(1):247{271. Wolf, T., L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. R. Louf, M. Funtowicz, J. Davison, S. Sh- leifer, P. von Platen, C. Ma, Y. Jerni- te, J. Plu, C. Xu, T. L. Scao, S. Gug- ger, M. Drame, Q. Lhoest, and A. M. Rush. 2020. Transformers: State-of- the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System October. Association for Computational Zhao, 2021: Spanish Shared Task 231Overview of the eHealth Knowledge Discovery Challenge at IberLEF 2021 Resumen de la Tarea de Descubrimiento de Conocimiento en Salud en fapiad, sestevez, yudyg@matcom.uh.cu 2University Institute for Computer Research, IIUI, University of Alicante, Spain 3Department of Language and Computing Systems, University of Alicante, Spain fygutierrez, montoyo, rafael g@dlsi.ua.es Abstract: This paper Discovery Challenge hosted at IberLEF 2021. We describe the task, resources, and participating systems, highlighting and discussing the main results achieved in the challenge. We analyse the best performing systems and present art\u0013 \u0010culo resume la Tarea de Descubrimiento de Conocimiento en Salud presentada en IberLEF 2021. Se describen la tarea, los recursos creados, y los sistemas que participaron. Se discuten los resultados principales obtenidos por estos sistemas, y se presentan recomendaciones para continuar la investigaci\u0013 on en esta tem\u0013 atica. Palabras clave: Reconocimiento de Entidades Nombradas, Extracci\u0013 on de Rela- ciones, Descubrimiento de Conocimiento, Tarea. 1 Introduction The accelerated growth of the Internet and the increased production of textual resources in all areas of human endeavour has cre- ated both new opportunities and new chal- lenges for the research community. On one hand, be collected as GPT-3 (Floridi and Chiriatti, 2020) and similar. On the other is becoming di\u000ecult to or- The ease of publication and consump- tion of textual information is of the increasingly up-to-date on information about topics of interest is crucial like \u0003Corresponding for search Scholar4). These re- of relevant information on a c topic. Tools like Connected Papers5and Papers with Code6are step documents, providing summarised and structured content in collection of documents. However, it remains an open problem to automatically combine, summarise, and present the relevant informa- tion in a collection of documents in a seman- tic structure (e.g., a knowledge a septiembre de 2021, pp. 233-242 del Lenguaje Naturalrequire methods to automatically detect in natural language the most relevant concepts and the factual statements in which they are normalise them into well- established taxonomies tures can linked with related concepts, e.g., knowledge graphs (Estevez- Velarde et al., 2018). The rst step of this process, i.e., the detection of relevant concepts complexity and language. To area, several academic as CLEF, SE- and more recently, IBERLEF. this context, the eHealth Knowledge Discovery Challenge is designed to foster the development of automatic knowledge discov- ery systems for natural language sentences in a cross-domain and multi-lingual setting. Concretely, 13 semantic relations is de- ned, and a corpus of 1800 sentences from di erent factual sources (i.e., A shared annotation was organised, where a total of 9 participants di systems which large data. A successful approach to the eHealth-KD leverage transfer domains vast majority of the training examples are provided in Spanish language and from the MedlinePlus domain, while only a small development set is available in the remaining settings. With this added complexity, we ex- pect to solutions that can be de- ployed in low-resource environments, where is unfeasible to language models over longs periods of time.The remaining of performance metrics. Section 3 describes the corpora and other resources created for this challenge and presents some qualitative analysis of their characteristics. Section 4 describes the dif- ferent systems presented in the challenge and summarises the main approaches and most common characteristics they share. Section 5 presents the main results of the challenge and discusses the most interesting insights of the challenge and model has been de ned, comprising 4 entity types and 13 re- lations, that attempts to capture a large part of in technical documents, including encyclopedias, news, and model is explained in detail in Piad-Mor\u000es et al. (2019). Figure 1 shows an illustrative example of the annota- tion model in three Spanish sentences. The challenge has been divided into two di erent The goal of this subtask is to identify all the entities per document and their types. These entities are all the relevant terms (single of one or more complete words (i.e., not a pre x or a su\u000ex of a word), and never include any surrounding punctua- tion symbols, identi es a relevant term, con- cept, the knowledge domain of sentence. Action: identi es a process or modi cation of other entities. It can be indicated by a verb or verbal de for Challenge. Predicate: identi es a function or lter of another set of elements, which has a se- mantic label in the text, and is applied to an entity with some additional argu- ments. Reference: identi es a textual element that refers to an entity of the same sentence or linking the entities detected and labelled in the input document. The pur- is entity instance, or member of the class identi- ed by the other. same-as: indicates that two entities are se- mantically the to indicate that con ned to a time-frame. in-place: to indicate that something exists, occurs or is con ned to a place or loca- tion. in-context: to indicate a general context in which something happens, like a mode, manner, or state. Action roles (2) indicate which role play entities related target: indicates who receives the e ect of the action. Predicate roles (2) indicate which role play the a an additional that spec- i es a value for the predicate to make sense. 2.3 Evaluation The predicted entities Discovery Challenge at IberLEF 2021 235overlap with the gold standard) are both are performed in sequence, which ultimately decides the win- ner of the challenge. In the subtask-speci c scenarios, the previous formulas are rede annota- tions for subtask. 3 Corpora and Resources The is based a corpus doc- uments composed of is divided into showing a Fur- thermore, the majority of sentences are in Spanish, but a small set of English sentences is also included, to evaluate generalisation across domains. The corpus with both anno- tated les in ANN format and ready-to-use les for training and evaluation is available online8. In the training collection, each sentence is labelled with its corresponding domain and language (Spanish or English), so that par- ticipants can potentially ne-tune and learn identify them in the text. In the develop- ment collection, sentences are from all di er- ent sources and no labelling is provided, so that participants can evaluate their systems in a similar environment to the testing set. In the testing set, a large number of unlabelled sentences is added along with a small batch 8https://github.com/ehealthkd/corporaCollection Source Language Size Training MedlinePlus Spanish 1200 h 300 MedlinePlus Spanish 25 Wikinews Span is h 25 CORD English 50 Testing MedlinePlus Spanish 75 Wikinews Span ish 75 CORD English 150 1800 Table of manually labelling the test set. As in previous edition, the corpus for eHealth-KD 2021 is based mostly on text extracted from MedlinePlus9sources, plus additional resources. First, the same cor- pus used in the 2020 edition will be pro- vided for training and development, while a new set of previously unlabelled sentences will be manually annotated sourced from be also provided for and development. Fi- nally, a small set of sentences from scienti c papers in the CORD-19 corpus (in English language) are selected, annotated, and dis- tributed in the development, and testing col- lections (Wang et al., 2020). The nal com- position of the corpus is presented in Table 1. 4 Systems Descriptions The challenge caught the attention of 8 par- ticipants from across the globe, who pre- sented a variety of approaches clustered around deep learning architectures. 2 pipeline trained with the spacy library, mod- i ed to t the eHealth-KD annotation model. They apply some preprocessing steps multi-token annotations, B Multi-span Codestrange Spacy Only A Sequence - Custom IXA ROBERTa Sequential Sequence Sequence BIO JAD BERT Joint Token Pairwise Relation PUCRJ-PUCPR-UFMG BERT Joint Joint Text2text Text2text Only A Sequence - BIO Yunnan-Deep BETO Only A Sequence - BIO Table 2: Summary of the approaches presented at the eHealth-KD IXA models both substasks are sequence labelling problems encoded with a BIO and pre-trained XLM-RoBERTa language model. Subtask A is solved with a standard NER architecture. For Subtask B, they solve a sequence labelling problem for es, pre-trained embeddings that relation labels. To deal with multi-span entities, they add a virtual re- lation that links tokens from the same en- tity (Navarro Comabella, Valle Diaz, and Helguera Fleitas, 2021). PUCRJ-PUCPR-UFMG proposes a joint model that outputs token models both standard NER with BILOUV Subtask A and a pair- wise model for Subtask B. As cation for Subtask As the Vicomtech presents two di erent models. The rst consists of a joint architecture for se- quence labelling and BERT The model is a text-to-text architecture, based on a T5 model, ne-tuned on entities and relations based and CRF layers contex- tual from CRF and Liu, 2021). Overview eHealth Knowledge Discovery Challenge at IberLEF 2021 237Main Task A Task B Team F1 P R F1 P R F1 P R - Yunnan-1 - - the eHealth-KD Challenge. The top result metric is highlighted. simple computational baseline are also reported. As it can be seen Table 2, predom- inant solution for feature extraction is a shared architecture) is used by more than half of the systems that tackle both sub- tasks. As in previous editions, a sentence. Two approaches stand out as distinctly novel in this edition of the challenge, pre- sented by IXA and Vicomtech, respectively. The former labelling problem, reusing the same architecture as in subtask A. The latter presents a text-to-text architecture which translates a raw sentence into a semi- structured representation that encodes all eHealth- Challenge. In the main scenario the best performing system was the joint architecture presented by Vicomtech, fol- lowed closely by a very similar architecture presented PUCRJ-PUCPR-UFMG. Both approaches are based on to the strength of these solutions for solving sub- task A, since in subtask B the best perform- ing system is the sequence labelling architec- ture presented by IXA. tasks, to be fun- solved up to human performance. tively. improvement, dealing with English sentences even though there is little to no training data (i.e., only 50 English sentences were provided in the development collection). However, there is a gap in performance across domains and languages, as shown by Table 4. Most participants have a better performance on the subset of the test collection that cor- responds to Wikinews CORD Task A T ask B Task A T ask B Task A T ask B Team F1 Di F1 Di F1 Di F1 Di F1 Di F1 Di subtasks A and B. The Di column corresponds to the absolute di erence Di indicates that team had a correspondingly better result on that speci c subset than in the overall corpus. mance on the CORD subset. On average, performance on MedlinePlus (the most com- source) is 25:2 points on CORD and B, respectively. This suggests that multilingual pre-trained models alone are insu\u000ecient learning method in low-resource erent subset collec- tions of the set. As it be ob- served, most teams present a signi cant drop in performance, in both subtasks, for the CORD subset with respect to the Medline- Plus and the Wikinews subsets. This high- lights the relative di\u000eculty of the transfer learning component of the challenge. IXA presented the most robust system in this re- spect. As an illustrative example, the system presented by best results on the MedlinePlus subset of the testing collection, but performance drops the remaining subsets, di\u000eculty of a speci annotation (e.g., a speci c entity or relation instance in testing for each of the annotations in the testing set. The gure suggests that entities are, on average, easier to compared to Subtask are form of shared architecture. The three best performing systems exploit in some manner the interrelation between both subtasks, ei- ther with sep- (thus sharing a large part of the feature extraction model), or by reusing the same architecture in both subtasks. In contrast, systems that attempt to subtask A have a signi cant drop in performance in this task. This suggests that subtask B also bene ts from direct ac- raw which result is that the best in uses a se- architec- ture (the same one used for subtask A) with a raw sentence, where two entities of interest Overview of 239Figure 2: of the performance by in each subset collection of the testing set. Each bar represents the scores obtained by each team minimum, and inter-quartile ranges. are marked. The model is trained to out- put a BIO sequence where tags are aligned with the marked entities and correspond to the label of the relation among them, if any. This requires O(jEj2) queries to the model for each sentence with jEjentities detected. We believe this approach can be improved by training the model with all entities at once, predicting one-to-many relations in a per sentence. ne-tuned a T5 language model on the task of translating from raw sentences to a semi-structured output structure consists of a of tuples all of entities detected and their corresponding re- lations. However, being a text-to-text model, it su ers from hallucination, which requires some to every relation in which they appear). However, we believe that this is a fruitful direction for future research that should be explored in greater depth. Moving towards a full solution for the eHealth-KD challenge, there are some funda- mental obstacles that still need to be tackled. The most salient one is e ectively capturing the interaction between entities and the se- mantic relations in which they appear. As we have not natural the sentence, but rather annotate the most salient entities rst (e.g., the main Action and related Concepts) and then add the con- textual relations and entities. Al- though it is approach designing an automatic an- form of purpose, it could be to analyse not only the nal an- notated sentence but the individual annota- including chal- lenge have shown that knowledge discovery in natural language text is still an open and challenging problem. Detecting the most rel- evant entities and relations is but a rst step in a hypothetical knowledge discovery system that could be used to intelligently explore the vast amount of information available in nat- ural language. Moving up in this pipeline, the next natural step is to create semantic structures (e.g., sentences. This could en- able using natural language queries that can be answered with precise factual information encoded in such a semantic structure. Future editions of the eHealth-KD challenge will fo- cus on those types of problems, in an attempt to continue fostering research in the full prob- lem of knowledge discovery in natural lan- guage. 6 Conclusions The eHealth Knowledge Discovery Chal- lenge, in its fourth edition, demonstrated that there is a growing interest in the NLPcommunity, both Spanish-speaking and in- ternational, related to research in knowledge discovery in natural language. While the top performing solutions are all based on state- of-the-art neural especially re- garding transfer learning to low-resource do- mains. New approaches not based on the standard NER pipeline seem promising, but still require further development before they sentences in three domains and two languages is published online with a permis- Creative Additionally, utility scripts for loading and manipulating the corpus data, training machine learning models, and evaluating their results. Fur- thermore, the o\u000ecial testing which can and for drawing further conclusions in future research. These resources are re- leased with the hope of enabling future re- searchers to build on these results, and con- tinue improving the state of the art in auto- matic knowledge discovery from natural lan- guage text. Acknowledgements This research has been supported by a Car- olina Foundation grant in agreement with University of Alicante and University of Ha- vana. Moreover, the research has been par- tially funded by the University of Alicante and the University of Havana, the General- itat Valenciana ( i Esport ) and the COST Actions: invaluable contribution of the Overview of the IberLEF 2021 241annotators who eHealth-KD Challenge 2021: Deep Ap- Knowledge Discovery at eHealth-KD Challenge 2021: Deep Learn- ing Approaches to Model Health-related Text in Spanish. In Challenge 2021: Deep Learning Model for A. J. Berinsky, K. M. Greenhill, F. Menczer, M. J. Metzger, B. Nyhan, G. Pennycook, D. Rothschild, et al. 2018. The science of fake news. Science, 359(6380):1094{1096. Monteagudo-Garc\u0013 \u0010a, L., (IberLEF 2021) .Navarro Comabella, J. G., J. D. Valle Diaz, and A. Helguera Fleitas. 2021. JAD at eHealth-KD Challenge 2021: Simple Neu- ral Network with BERT for Joint Classi- cation of Y. Bonescki Gumiel, T. Castro Ferreira, L. Ferro Antunes de Oliveira, J. V. Andrioli de Souza, G. P. Meneghel Paiva, C. M. Silva e Oliveira, Lucas Emanuel Cabral Moro, E. Cabrera Paraiso, E. Labera, and A. Pagano. Mu~ noz. 2019. A General- Purpose Annotation Model for Knowledge Discovery: Case Study in Spanish Clini- cal Text. In Proceedings of the 2nd Clin- ical Natural Language Processing Work- shop, pages 79{88. Wang, L. L., K. Lo, Y. Chandrasekhar, R. Reas, J. Yang, D. Eide, K. Funk, R. Kinney, Z. Liu, W. Merrill, et al. 2020. Cord-19: The covid-19 open re- search dataset. ArXiv. aplicado salud en normalizaci\u0013 atica de los pacientes, las ocu- paciones juegan un papel fundamental tanto desde el punto de vista de la salud laboral, accidentes laborales y exposici\u0013 on a t\u0013 oxicos y pat\u0013 ogenos como desde el de la salud f\u0013 \u0010sica y mental. Este art\u0013 \u0010culo presenta la tarea Profes- sion Recognition (MEDDOPROF), celebrada dentro de IberLEF/SEPLN 2021. La tarea se centra en el reconocimiento y detecci\u0013 on de ocupaciones en textos m\u0013 edicos en castellano. MEDDOPROF propone tres retos: NER (reconocimiento de pro- fesiones, situaciones laborales y actividades), CLASS (clasi car cada ocupaci\u0013 on en funci\u0013 on de su referente, como puede ser el paciente o un familiar) y NORM (nor- malizar menciones a las terminolog\u0013 \u0010as ESCO y SNOMED-CT). De un total de 40 equipos registrados, 15 han presentado un total de 94 sistemas. Los sistemas de mejor rendimiento se basan en tecnolog\u0013 \u0010as de aprendizaje profundo como trans- formers, llegando a conseguir una F-score de 0.818 en detecci\u0013 on de ocupaciones (NER), 0.793 en clasi caci\u0013 on de ocupaciones por su referente (CLASS) y 0.619 en normalizaci\u0013 on (NORM). Futuras iniciativas deber\u0013 \u0010an tener tambi\u0013 en en cuenta aspec- tos multiling\u007f ues y la aplicaci\u0013 on en otros dominios como servicios sociales, recursos humanos, an\u0013 alisis del mercado legal y laboral o la pol\u0013 \u0010tica. Palabras clave: tarea compartida, toxic/pathogenic agents, but also their impact on general physical and mental health. This paper presents the Medical Documents Profession Recogni- tion (MEDDOPROF) recognition and documents er in From the total of 40 registered teams, 15 submitted a total of 94 runs sub-tracks. Best-performing systems to other domains like social services, human resources, legal or job market data analytics and policy makers. Keywords: shared task, clinical domain, occupations, Spanish. Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 243-256 statistical Despite the relevance of our livelihood and economic health and lifestyles, occupations are systemat- or infectious strain (Stansfeld et al., 2011). Oc- cupational medical specialty, aims to de ne and prevent all health issues derived from our professional activity. tion, employment and activities and hobbies that while not usually structured can be very impactful (Vanotti et al., 2017). Recently, machine learning technologies have been applied to the characterization of occupational data. Occupational data min- ing is a eld within data mining that explores di erent occupational variables to of the causes of these accidents (Cheng, Yao, and Wu, 2013). Even if some studies show that predictive models return many openly available resources that allow a thorough analysis of this type of data. Automatic extraction of mentions of occu- pations and Named Entity (NER) while c were left for specialized systems.Although the Sixth Message Understanding Conference (muc, 1995) already included the detection of management posts (a very spe- ci c type of occupation), recent strate- gies only marginally of detec- mation item that had to be detected, hid- den from plain text or pseudo-anonymized. In English, the 2014 i2b2/UTHealth (Stubbs y in their datasets. For Spanish, the MEDDO- CAN track of IberEval 2019 et occupations were among the most dif- cult of Personal Health Information when examining the results obtained and Occupations Dataset (IPOD) (Liu et al., 2020) was released, which includes a corpus of 475,085 job titles in glish crawled from LinkedIn and a gazetteer. The limitation of these resources is that they only consider occupations when job ti- tles are directly the ma- terials used by the worker, a reference to the workplace and other. In order to detect all these occupational nuances, more exhaustive resources that re ect how speakers actually talk about occupations are needed. An interesting 244(ISCO) Labour Organization, 2021) and education and training\", as well as their the importance of these variables in medicine. To the best of our knowledge, there are no other available re- sources that describe a fam- ily member, or a healthcare professional The complexity of this task lies in the lack of pre- cursors, the wide range of expressions refer- ring to occupations, and the descriptive heterogeneous of medical documents. Systems able to address these issues are also crucial for social workers, the industry, policy makers and Human Resources departments. In order to nd solutions and following the success of previous e, and Krallinger, 2020) and MEDDOCAN (Marimon et al., 2019), we have organized the task (MEDical DOcuments PROFes- sion Recognition). MEDDOPROF aims foster the creation of resources and occupa- tions detection systems in the eld of occu- pational data mining. It is the second shared task on occupational text mining in Span- ish, after the social media-focused ProfNER (Miranda-Escalada et al., 2021). 2 Task Description 2.1 Shared Task Goal MEDDOPROF focused on the detection and teams were and (2) to classify them according to who they refer to in the text (patient, family member, health professional, someone else). to overview of the task. 2.2 Sub-tracks The shared task was divided into three sub- tracks, each of them associated in the text and label them according to the type of occupation: pro- fession, employment to MEDDOPROF-NORM track. Given a list of valid codes that includes all of ESCO and a of text mining systems. 2.3 Shared Task Setting The MEDDOPROF shared task had two dis- tinct phases: Training phase The leased in April 2021 and participants had al- most two to build their systems. Evaluation phase. In June test set was released without annotations. Partic- ipants had around two weeks to make their predictions and submit them. Each team was allowed only matches between predictions and the manually MEDDOPROF-CLASS, this means that both the text span and label had to be cor- rect. In sub-track lexical lookup system with a slid- ing window. The system uses the annotations from the training set and scans the input text to nd new matches. generated by the lexical lookup were compared to the data to pre- dict codes. The results are shown in Table Resources 3.1 MEDDOPROF Gold Standard The Gold Standard corpus for MEDDO- PROF is a collection of 1844 clinical cases, reports of individual patients published in medical journals. Unlike Electronic Health Records (EHRs), which might include a lot of personal information of patients and cannot usually be shared due to privacy The MEDDOPROF cor- pus was split into training (around 80%) and test (around 20%) sets. The complete Gold Standard corpus is available at Zenodo2. The corpus' entities MEDDOPROF occupations and another related to the ac- tual person it to. Three considered: patient ), ). linguist in col- laboration The step involves the creation and de nition of spe- ci c guidelines that aim to expressed in were internal annotator anno- tation tool brat was used (Stenetorp et al., 2012). MEDDOPROF corpus normal- isation, we rst tried to profession/activity in clinical reports. When we did not nd an ESCO code, as was the case for most employment statuses, we turned to SNOMED-CT codes, and some common codes are described in the guidelines. The nal version of the MEDDOPROF guidelines is 33 pages long and includes over 70 rules and exceptions. They are freely available at Zenodo3. 3.1.2 Corpus Format For the rst two sub-tracks (NER and CLASS), the MEDDOPROF corpus' clini- cal cases are provided as UTF-8 text les with the annotations as separate les in brat stando format (.ann les). For each clinical case, there is an associated .ann le with the same name in each sub-track. For the normalization sub-track (NORM), mappings valid shared. 3.1.3 Corpus The MEDDOPROF corpus is made up of 1,844 documents, with a total of 58,627 sen- tences A complete is provided in Table 2, while Table 3 breaks down the label distribution of the corpus. In total, there are 4,743 annotations, of which 2,058 are unique mention strings. On average, each document had between two and three annotations. Each process additional resources Complementary Entities Dataset. In or- der to connect the occupational and clinical aspects of the text, a version of the 1500 documents of the training set that includes automatic annotations for clinical and lin- guistic and used for the PharmacoNER scope. Participants were free to implement this extra knowledge in their systems. Table 4 gives a general view of the entities in this additional dataset, while Figure 3 provides an example an enti- ties dataset can be at Zenodo4. 4https://doi.org/10.5281/zenodo.4775741Occupations Gazetteer. A gazetteer of occupations in a tab-separated le. This resource includes over 25,000 terms and was constructed by ex- Stanford CoreNLP in a large collection of social media Spanish pro- les. The gazetteer can be found in Zenodo5. 4 sub-tracks: MEDDOPROF-NER, MEDDOPROF- CLASS and MEDDOPROF-NORM. the rst two tracks are independent, as NORM requires the output of at least one of the other tracks. Participants could choose to submit results for one, two or all three sub-tracks. Up to ve submissions were of 40 teams registered for the task, of which 15 submitted their tions. All 15 teams sub-track, the MEDDOPROF-CLASS sub-track and 8 in the MEDDOPROF-NORM sub-track. Due to the fact that up to 5 systems could be sub- mitted, the nal number of prediction runs is high, a total of 94: the NER sub-track re- ceived 39 Ten teams were from academia, three were from industry, one is a collaboration between academia an industry and one participant is a freelance. Even though most teams were from Spain, there were also MEDDOPROF participants from France, Germany, India, Mexico and the United Kingdom. Table 5 5https://doi.org/10.5281/zenodo.4524659 Salvador Lima-L\u00f3pez, Eul\u00e0lia Farr\u00e9-Maduell, Antonio Miranda-Escalada, Vicent 134 3,227 Empl. Status 1,047 119 0 203 1,369 Activity 122 7 0 18 147 Total 2,327 260 1,525 631 4,743 Table Example of a clinical shows sub-track was imple- mented by the team NLNDE, with an of Their precision was well as strategic data splits. The second best system was submitted by the MUCIC team, who obtained a 0.8 F1-score and employed MUCIC's submission was an F1-score of 0.764. Both teams employed the same . The TALP team using a pre-trained multilingual DistilBERT (Sanh al., an F1-score of 0.619, second best result with an F1-score of 0.603 with a BERT transformer. For a complete overview of the task's re- sults, please refer to the Annex at the end. 4.3 Methodologies Over the past few years, some of the biggest advancements in NLP have been achieved by large neural language models and their transformer architecture. This trend has been observed in MEDDOPROF, where the most common architecture used by pants are transformers-based language mod- els, mainly BERT (Devlin al., 2019) or its Spanish version BETO (Ca~ nete et al., 2020). In all three every team used them either directly { ne-tuning task data, or in the of embeddings. Conditional Random Fields were also Rovira i Virgili Spain A NE,CL,NO - Galiza Universitat Oberta de Catalunya Spain A NE,NO - gbali Independent France - NE,CL - HULAT-UC3M Universidad Carlos III de Madrid Spain A NE - ICC Instituto de Ingenier\u0013 \u0010a del Conocimiento Spain A NE,CL,NO - IITKGP Karaghpur Indian Institute of Technology India A NE (Harkawat y Vaidhya, 2021) KaushikAcharya Philips India Limited India I Adel, and Str\u007f SINAI Universidad A NE,CL,NO (Mesa-Murgado et al., 2021) SMR-NLP Siemens AG / Ludwig Maximilian University of MunichGermany I NE,CL - TALP Universitat Polit\u0012 ecnica de Catalunya (Medina Herrera y Turmo Borr\u0012 as, 2021) URJC-UNED Team Universidad Rey Juan Carlos / Universidad Na- cional de Educaci\u0013 on a DistanciaSpain A stands for academic or industry institution. In the Tasks column, NE stands for MEDDOPROF-NER, CL for MEDDOPROF-CLASS and NO for MEDDOPROF-NORM. NER CLASS NORM Team Name P R F1 P R F1 P sub-track is presented in best is underlined. A dash indicates that the team did not participate For reference, ral like NLNDE also exper- imented with variables like c models and splitting the data into strategic partitions with great success. There also some in- stance, one of the submissions of the Galiza team was a non-neural lookup system based on rules. Krallinger 250spaCy (Honnibal et 2020), HuggingFace (Wolf et al., 2020) or Flair (Akbik et al., 2019), either to build the entire system or at some point in their pipeline. As for the choice of systems for each sub-track, many teams re-used els for all challenges, either by train- ing the model to output multiple labels or via transfer learning. Even then, results for the NER sub-track are overall higher than for the CLASS sub-track, although the data used for both is the same. This suggests that the CLASS sub-track is somewhat harder, some- thing that is to be match- ing an occupation to of teams same systems other Some teams, like the Vicomtech NLP-team, opted to build more specialized systems that use semantic search. 4.4 Error analysis This section goes through some of the most frequent errors observed when comparing the Gold Standard tions of participants' terms or and can be clinical an adjective. This distinc- tion seems to be somewhat complex for current systems to understand and it is a common source of false positives. This type of error happens a lot mentions are highly context-dependent, and in the Gold Standard they were annotated only when they related to occupations. However, some systems la- beled all instances of the word regardless of the context. An interesting point is one sys- tem decided professions. are some frequent mentions that appear in more than ones are: \\Trabajador\" ). On its own, word is considered an employment status. How- ever, whenever it is accompanied by some type of job description (work- place, specialty), it is considered a pro- consider the context and, thus, not including description the prediction. The opposite is also true: some predictions together with an adjectival or prepositional phrase that role taken up by a family member, in which case it was annotated as employment status. This distinction requires a good understanding the context replicated Scope. NER systems is correct boundary detection (Li et al., 2020). Due to the variety in the cor- pus' annotations, predictions being too short or too long are often a source of errors. On the one hand, some common examples of a prediction being too short are: A\u000exes. Some pre xes like \\ex-\" were in- cluded part of important information. How- separated by a space rather than joined together with the x. to occupational etica la belleza\" ( aes- thetics and beauty professional ). There were systems who stopped at the rst specialty and did not include the coordinated part. On the other hand, some predictions were too long for reasons such the cor- pus, a good system needs the the in syntac- tically similar structures. This was not al- ways the case, as some predictions include irrelevant information due to their syntactic distribution. For instance, in the sentence \\[...] clasi ca al trabajador en tres categor\u0013 \u0010as: apto / no apto / apto en determinadas ( [...] classify the worker in three apt / under certain ), one a appear or activity, as in jubilado\" (retired farmer ). These cases were annotated separately, but were at times predicted as a single entity. In for the normalization task the main source of errors was code granularity Mul- tiple correct. nomenon may similarity between some of ESCO's concepts. 5 Discussion We present the MEDDOPROF shared task results and resources on the automatic detec- tion and normalization of occupations from medical documents written in Spanish. To the best of our knowledge, it is the rst at- tempt at characterizing that goes beyond what handle. Although doc- uments in Spanish, it that the release of the MEDDOPROF annotation guidelines and the use of multilingual terminologies can serve as base for similar ef- forts in other languages and other application domains. The following resources have been re- leased as part of MEDDOPROF: a normal- ized Gold Standard that language long annota- tion guidelines that describe how to anno- tate this phenomenon7, a version of the train- ing cur- rent be able to the manual and box corpus scenario often research. A total c sub-track, the results for the NER track are the highest overall. The CLASS sub-track's results are somewhat lower de- spite using the same data, which indicates that it is more complex and harder to learn. We anticipate that the systems resulting from especially in the clinical do- main, and help in their processing. Beyond the healthcare application domain, we expect that systems similar to those used for the MEDDOPROF track may be adapted and applied to heterogeneous elds such as social resources, legal NLP and even gender studies. European research projects like the Project for Health and Occupational Research (EPHOR) could also bene t from this type resources more exhaustively the to manage concept code granularity and concept ambiguity. Some of the entries in ESCO were highly similar, hindering the se- lection of the most appropriate code. For ex- ample, codes 01, 011 and 0110 are all named \\commissioned armed forces o\u000ecers\". The di erence between them is given as a short text description, which is di\u000ecult to exploit for automatic tools. In case of mentions that could potentially t was tised regularly (which we had to label SCTID: 228447005 \\Physi- cally active ( nding)\"). people regu- larly played instruments or of these activi- ties, for practice preventive healthy aging include the practice many activities like going to the gym, singing inchoirs, going for group walks and to improve the health, well-being and quality of life of patients. believe should be obtain com- prehensive of the health of the patient, to further personalised medicine and to investigate the most delicate nature of the enti- ties. Looking at the participant systems' correct. Sometimes, the dif- ference between Gold Standard and the predictions is simply a comma. Some other times, er that was not captured in the GS because it was not relevant. There are some of these approximate pre- dictions which might look perfectly okay to a human even if they do not exactly match the GS. Because of this, in future tasks we would tasks, Task 9 As for the normalization's evaluation, the results show that there were systems that had problems choosing the right granularity within a granularity some ESCO codes and use of more than one terminology, such met- rics could provide alternative to exploit the di erent levels of granular- MEDDOPROF was promoted through the collaboration between the Spanish Plan for the Advancement of Language Technology (Plan TL) and the BSC. We also want to acknowledge the 2020 Proyectos de I+D+i - RTI Tipo A (DESCIFRANDO EL PA- PEL DE LAS PROFESIONES EN LA SALUD DE LOS PACIENTES A TRAVES DE LA MINERIA DE TEXTOS (PID2020- 119266RA-I00)) for support. sci- enti c committee, in special Michelle Turner (ISGlobal) and Francisco Javier Sanz Valero (Escuela Nacional de Medicina del Trabajo, Instituto de Salud Carlos III), as well as Mar- vin Ag\u007f uero-Torales, Luis Gasc\u0013 o ing Conference (MUC-6): Proceedings of a Conference Held Bergmann, D. Blythe, K. Ra- sul, S. Schweter, and R. Vollgraf. 2019. Flair: An easy-to-use framework for state- of-the-art nlp. In NAACL 2019, 2019 Annual Conference of the North Ameri- can Chapter of the Association for Com- M. Conference Language Resources (LREC'14), pages G. Fuentes, J.-H. Ho, Kang, and J. P\u0013 erez. 2020. Span- ish pre-trained bert model of petrochemical the Process Industries, 26(6):1269{1278. Devlin, J., K. Lee, and K. Toutanova. Pre-training deep bidirectional transformers for lan- guage understanding. In Proceedings of the 2019 Conference of the North Amer- ican Chapter of the Association for Com- putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171{4186, Minneapolis, Minnesota, Junio. Association soft skills from any text. CoRR, abs/2101.11431. Fernandes, F. and A. Dias. 2019. Perspecti- vas do uso de minera\u0018 c~ ao sa\u0013 ude e seguran\u0018 no trabalho. Revista Brasileira de Sa\u0013 ude Ocupacional , 44, 01. Gambhir, R., G. Singh, S. Sharma, R. Brar, and H. Kakar. 2011. Occupational health hazards in current dental profession-a re- view. The Open Occupational Health and Safety Journal, 3, 12. Gasco, L., A. A. Krithara, Guneri, Y. Fatih, and O. C \u0018 elebi. 2016. Analysis of the relation and occupa- En- vironmental Medicine and Safety, 1:102{ 118, 04. Harkawat, J. and T. Vaidhya. 2021. Analysis of the Spanish Pre-Train Language Model for deghem, and A. Boyd. 2020. spaCy: Industrial-strength Natural Language Processing in Python. International Labour Organization. 2021. International standard classi cation of oc- Proceedings . Li, J., A. Sun, J. Han, and C. Li. 2020. A survey on deep learning for named en- tity recognition. IEEE Transactions on Knowledge and Data Engineering. Liu, J., Y. C. Ng, K. L. Wood, and K. H. Lim. 2020. Ipod: A large-scale indus- trial and professional occupation dataset. InProceedings the 2020 ACM Confer- ence on Computer Supported Cooperative Work and Social Computing M., A. Gonzalez-Agirre, A. In- txaurrondo, H. Rodriguez, J. L. Martin, M. Villegas, and M. Krallinger. 2019. Au- tomatic de-identi cation of re- sults. In IberLEF@SEPLN, pages 618{ 638. Medina Herrera, S. spanish, and M. Krallinger. 2021. shared In the Sixth Social Media Mining for Health (#SMM4H) Workshop and Shared Task , pages 13{20. Sanh, V., L. Debut, J. Chaumond, and T. Wolf. a Computational Seman- tics (*SEM), Volume 2: Workshop on Semantic Evaluation (SemEval 2013), pages 341{350, Atlanta, Georgia, USA, Junio. Association for Computational Linguistics. Stansfeld, S., F. Rasul, J. Head, and N. Sin- gleton. 2011. Occupation and mental health in a national uk survey. Social psychiatry and Pyysalo, Topi\u0013 T. Ananiadou, and J. 2012. a web-based for nlp-assisted text annotation. In Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages Biomedi- for Automatic ization of Yastremiz, A. Marinangeli, R. Alonso, B. Silva, A. Io- rio, F. C\u0013 aceres, and O. Garcea. 2017. Estudio del estatus laboral y el nivel so- cioecon\u0013 omico en personas con esclerosis m\u0013 ultiple en 2 centros de buenos aires. Neurolog\u0013 \u0010a Argentina, 10, 09. Wolf, T., L. Debut, V. Sanh, J. Chau- mond, C. Delangue, A. Moi, P. Cis- tac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush. 2020. Transformers: State-of-the-art natural language process- ing. In Proceedings of the Con- ference Language Octubre. modeling for occupa- tional safety outcomes and away from work analysis in mining operations. In- ternational Journal of Environmental Re- search Health , 17(19). Zotova, Garc\u0013 typology and the results of The University the results of HAHA at IberLEF 2021: Humor Analysis ba- sed on Human Annotation. This year's edition of the competition includes the two classic tasks of humor detection and rating, plus two novel tasks of humor logic me- chanism and target classi cation. We describe the corpus created for the challenge, the competition phases, the submitted systems and the main results humor, Spanish, Humor mechanism, resultados de HAHA en 2021: Humor Analysis based on Human Annotation. La edici\u0013 on de la competencia de este a~ no incluye las dos tareas cl\u0013 asicas de detecci\u0013 on y valoraci\u0013 on de humor, m\u0013 as dos tareas nuevas de clasi caci\u0013 on de mecanismo y objeto de humor. Describimos la creaci\u0013 on del corpus, las fases de la competencia, los sistemas enviados y los principales resultados obtenidos. Palabras clave: Humor computacional, Espa~ nol, Mecanismo de humor, Objeto de humor. 1 Introduction American author E. B. White once said: \\Explaining a joke is like dissecting a frog. You understand it better but the frog dies in the process.\" It is generally agreed upon that analyzing humor is a di\u000ecult endeavor all of the acti- However, we believe focusing on humor analysis is important and it is one way of lin- king current work on computational humor with a more theoretical background. The eld of computational humor has had a surge in recent years, as can be seen by the growing number of shared tasks related to the subject that have been organized. Most of the time these tasks focus on humor detection, and on occasions also humor rating, but a deeper analysis of the way humor works and the topics it deals with continues to be largely exceptions). Our ob- jective with the HAHA task is to go further in the direction of analyzing humor structure and content, while at the same time nuing explore the more established tasksof humor detection and rating. 1.1 Background The study of humor from a computational and machine learning perspective is relati- al., 2016), but a characterization of humor that allows its automatic recognition and gene- ration is far from being speci ed. Figurati- ve language, and in particular humor, has been a productive area of research as re- gards shared tasks for several years. SemEval- 2015 Task 11 (Ghosh et al., 2015) focused on the challenging aspects to predict the ranking that the producers the tweets. the Humor Analysis based on Human Annotation task, at IberEVAL 2018 (Castro, Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 257-268 Procesamiento del Lenguaje NaturalChiruzzo, and Ros\u0013 a, 2018; Castro et al., 2018) and IberLEF 2019 (Chiruzzo et al., 2019), consisted Funniness Score 2020 Task 7 (Hossain et al., 2020) proposed a task of humor rating in which participants had to predict how humorous an edited head- line was and to predict which of two edits to the same headline was funnier. More same subtasks as in HAHA 2018 and 2019, and adding two ad- ditional tasks: ense particular (and very common) class of jokes: puns. The rst task in this competition was the more usual ap- proach of detecting if a text in English con- tains a pun, in the second task the partici- pants had to detect exactly which word of the text is the pun, and the third task implied de- tecting what are the di erent senses the pun word can be interpreted as. We believe focu- sing on this type of analysis is a promising way of moving forward in the eld of compu- tational humor, but in our new tasks, instead of trying to explore one type of humor mecha- nism in we take a broader approach larger set of humor mechanisms, as well as exploring the most common targets associated to jokes. New Tasks Mirroring the growing interest in compu- humor with each research in tasks during the rst edition. Inter- est rose sharply in the second edition of the task, with 18 participants. However, the per- formance achieved by systems in these rst and second editions was still far from human- level for humor detection. For this reason, in this third edition we included the same two tasks of humor detection and rating from the previous some Firstly, in the dataset used for the previous edition, there were about 38.7 % of humo- rous tweets, but this time the new test setwas created with the aim of keeping it as We also aimed to advance the eld of computational humor by adding two new tasks which are directly inspired by one of the most well-known and comprehensive theories of humor, the General Theory of Verbal Hu- mor (GTVH) (Attardo and Raskin, 1991). This theory claims there are six Knowledge Resources (KRs) used in jokes, which charac- terize the type of humor contained therein. In particular, we propose to focus on these two: Logic Mechanism (LM) contemplates how the joke works, what are the means by which it conveys humor (e.g., analogy, exag- geration, Target (TA) if somebody is being laughed at (the butt of the joke) and who that entity is, which relates to the content of the text. Note that this is not the only possible ca- tegorization. (Tsakona, 2009) presents some practical examples of this theory, (Attardo, Hempelmann, and Di Maio, 2002) presents a deeper categorization, while (Reyes et al., 2009) describes another possible way of or- ganizing jokes in a taxonomy, (Berger, 2017) describes a comprehensive of 45 mecha- nisms that are used to convey humor in jokes, and (Buijzen and Valkenburg, 2004) follows the same path for analyzing audiovisual hu- mor used in television commercials. We used these ideas as a starting point for our cate- gories' de nition and, as we will see in Sec- tion 2.2, we adapted them to the types of mechanisms we found in our dataset. 2 Corpus For the 2019 edition of this task, we built a corpus of 30,000 crowd-annotated tweets (Chiruzzo et al., Chiruzzo, The labe- led to indicate they with a 5. non-humorous have at least three negative annotations. This corpus was split between 24,000 tweets for training and 6,000 for test. This year's edition of the corpus has Luis Chiruzzo, Santiago Castro, Santiago G\u00f3ngora, Aiala Mihalcea 25836,000 tweets total: the 2019 training and test sets were used this year as training and development sets, and we created a new test set of 6,000 tweets. We also annotated a sub- set of 20 % of each partition with information about humor mechanism and targets. 2.1 New Test Set The new test corpus was developed by crowd- sourcing in a similar way to an- notation, this year aimed to impro- by calculated coe\u000ecient for every pair of and built clusters of tweets with a similarity more than 0.5, then we manually every cluster and tagged them as near- duplicates or not. From the near-duplicate clusters, only one tweet was kept and the rest were discarded. We also repeated this analysis on the 2019 corpus to avoid inclu- ding tweets that were too similar to the vious ones. After this pruning, 13,032 tweets were left in humorous texts, also downloaded a ran- dom collection 11,353 tweets in Spanish. The aim was to create a test set of 6,000 tweets, expecting 5 annotations for each of them, with as balance as possible bet- ween the humorous and humorous, and if so how funny it is on a scale from 1 to 5 (see Fig. 1). The task took on average 30 minutes and the collection. each collection varied between batches to keep collection as lanced as possible. After each round, for ( humorous inten- ded to check if the users had understood the task and to gauge their attention level. If a user failed to label more than 60 % of these tweets their annotators. The result of the annotation process is a corpus of 6,000 tweets with exactly 50 % of the tweets classi ed as humorous. All of the ve annotations each (at least three positive ones, with their co- rresponding humor rating), while all of the non-humorous tweets have at least three ne- The countries with the most annota- tors were Mexico (72), Chile (45), and Spain (36); but there were also annotators from Ar- gentina, Bolivia, Colombia, and Venezuela, among others (see Fig. 2). The agreement between Country of origin of the annotators from Proli c. Our only lter for annotators was that they spoke Spanish as a rst lan- guage. alpha was agreement for humor ra- ting measured as while it was 0.224 for HAHA 2019 (Chiruzzo et al., 2019) and 0.124 for Hahackaton (Meaney et al., 2021). We believe one reason this could be happe- ning is the geographic diversity of the an- notators sourced from Proli c, which might have had e ect of increasing the diver- sity of opinions on the highly subjective mat- ter of humor rating. Due to the way we pro- moted the web annotation tool in previous editions, the distribution of annotators could have been biased with a large number of an- notators from Uruguay and fewer annotators the Spanish-speaking coun- tries. This over-representation of annotators from produced more homoge- opinions on jokes. To a lesser ex- tent, something similar might have happened in Hahackaton: although the annotators were carefully selected to cover many age groups, they were all from the United States. 2.2 Corpus for Logic Mechanism and Target The annotation of the corpus for the new tasks was more complex as we were dea- ling with uncharted territory. There were th- exploratory phase.Initial de nition of categories: We ai- med to de ne a suitable set of categories that would be comprehensive enough to ca- tegorize the texts for our dataset, but kee- ping in mind they were going to be used in the context of a machine learning com- petition. We started by sampling a set of 200 tweets from the 2019 corpus and having them annotated by four annotators (organi- zers of using the humor from (Berger, 2017). These action (e.g. slapstick). From the beginning it was clear that this number of categories would be too large, and many of them were not found in our dataset (for example, the action categories made no sense for the verbal humor in tweets) so the objective was to narrow it down to mana- also to that were not in this ori- ginal set but could be present in the corpus. The annotators were also asked to identify the individuals/groups which were the target of the jokes in the tweets. We discussed and iterated over until we reached an initial set of categories to use, which included 12 cate- gories for the mechanism (see Section 3.3 for the de nitions) and 22 categories for the tar- get. The target categories were organized in a tree with 12 superclasses, so new labels could be added as appropriate leaves of the tree. Annotation of the corpus: Using this initial set, Eleven annotators participated in the annotation process for the training and development sets, they were all Computer Science students that had taken at least one course in with credit. Each annotator labeled a total of 600 tweets, 500 of these were uni- que to the annotator, while 100 were shared with another annotator in order to calculate the inter-annotator agreement. Their instruc- were to use 12 categories for mecha- nism, but add new categories to the targets tree as necessary, as we knew the corpus could contain many more targets. Re nement: Once the annotation process was nished, a total of 58 target categories Luis Chiruzzo, Santiago Castro, Santiago G\u00f3ngora, (9) body shaming body shaming (224) mothers-in-law (1) family aunts (3), couples (87), ex (25), fathers (6), grandmothers (4), husbands (1), mothers-in-law (52), mothers (45), orphans (1), widows (1), wives (9)family/relationships (234) gender men (3), women (15), homosexuals (1)gender homosexuals (50), men (102), transgender (5), women (329), others (1)lgbt (57) men (105) women (345) health alcoholics (4), illness (1), mental illness (2)health (7), bankers (2), boxers (3), builders (6), doctors (49), engineers (6), entertainment - gures (2), footballers (47), law- yers (13), musicians (4), atheists jehovah (56) self-deprecating (20) social statuspoor (3), rich social statuspoor (60), rich (5) social status (68) organizations (77)technology annotation: in the rst round of 200 tweets we found 22 categories, in the second round of 6,000 tweets we found 58 categories, in the nal annotation round we uni ed and simpli ed classes to get to 15 categories. The numbers in parenthesis represent the number of instances found for each category. found in the corpus. Two more anno- (organizers of ying all targets into a tree of categories. We then analyzed the nodes of the tree loo- king for a set of categories that was managea- ble but also contained the most representati- ve ones. We ended up settling on a collection of 15 categories. Table 1 shows a summary of the categories and number of tweets found for each one of them in the three rounds of annotation. The nal step was annotating a subset of the new test set. Two annotators (from the organizing team) took part in this, annota-ting sha- this case, we considered the categories for mechanism and target as xed. inter-annotator and dev sets was 0.365. This agreement was a little better for the test set, with 0.449. We believe the higher inter-annotator agreement in test than in train and dev is due to the expertise of the annotators, but in any case we can a\u000erm that it is a very di\u000ecult task. On the other hand, we calculated agree- ment the F1 score between annota- tors (taking one annotator as gold and the other one as the candidate). This agreement is 0.375 on average for the train and dev sets, and slightly lower for the test set: 0.350. This way of calculating the agreement does not ta- ke into account the large number of labels there are, but at least it may give an idea of how di\u000ecult the task is even for humans. 2.3 Composition of the Corpus Table 2 shows a summary of the composition of the corpus split in train, development, and test sets. We include the number of tweets that are labeled with each the categories one 399 400 Mechanism labels absurd 566 142 136 analogy 319 84 53 embarrassment 301 72 28 exaggeration 476 103 75 insults 146 40 21 irony 371 90 100 misunderstanding 416 100 94 parody 255 59 65 reference 578 121 85 stereotype 230 68 35 unmasking 441 130 69 wordplay 701 191 439 Target labels age 105 16 15 body shaming 181 43 28 ethnicity/origin 69 24 41 family/relationships 177 57 55 health 58 12 24 lgbt 40 17 13 men 92 13 23 professions 263 65 63 religion 45 11 6 self-deprecating 212 45 36 sexual aggressors 13 5 8 social status 52 16 8 substance use 83 21 15 technology 51 12 10 women 287 58 74 Table 2: analogous to the tasks proposed in HAHA 2018 and 2019, and we proposed novel tasks for this ite- ration. We also created new baselines for the rst two tasks that stronger than the ones used in previous aiming challenge. 3.1 Detection Given a tweet, the task of humor detection is to determine if its content is humorous or not (intended humor by the author; i.e. a joke). The main metric for measuring performance for this task is the F1 score of the `humorous' class. In previous years we used a simple random baseline for this task, which was a very weak baseline meant to encourage participation in the task. This year we used a slightly stronger baseline, but still one of the simplest machine learning methods: of 0.6493 on the dev set, and 0.6619 F1 on the test set. 3.2 Humor Rating The humor rating task is to predict a funni- score a it is error (RMSE) ra- ting. In previous years the baseline for this task assigned the average rating found in the trai- ning corpus to all tweets. This year we trai- ned a SVM regression model with TF-IDF features { arguably the strongest the ba- selines used for competition, and 0.6704 the test set. 3.3 Humor Logic Mechanism Classi cation For a humorous tweet, this task predict the mechanism by which the tweet conveys humor from a prede ned set of classes. In this task, only one class per tweet is allowed. The possible categories for this task are the follo- wing: Absurd: Humor comes from a logical in- consistency reasoning. Luis Chiruzzo, Santiago Ros\u00e1, J. Meaney, comparison between the punchline one participants shames or embarras- ses another one. Exaggeration: There is a situation or comparison that is exaggerated. Insults: There are insults to the charac- ters in the joke or to real life people. Irony : They say something but mean the opposite, or they describe a contra- dictory situation. Misunderstanding : Humor comes from a participant understanding a question or a situation wrong. Parody : The text is similar to another known text or work (for example a song, a saying, or a movie dialog) but it is mo- di ed to make it humorous. Reference: It describes a real life situa- tion, generally mundane, that the reader might relate to or not, but when the reader does identify with the situation it results in a humorous e ect3. Stereotype: Humor comes from using a social profession to remark on a stereotypical from a cha- racter acting in a certain way and la- ter combinations of words to give humorous sense. The main metric for measuring perfor- mance in this task is the macro-averaged F1 score. The baseline for this task for dev set, and 0.1001 for the test set. 3This is the only mechanism category that does not correspond to at least one of the categories from (Berger, 2017), but it is a particular type of humorous text that is very common in the dataset.3.4 Humor Target Classi cation For a humorous classi ca- tion task consists in predicting the target of the joke based on its content (what/who it is making fun of) from a prede ned set of clas- ses. In this case, there may be many classes associated to a tweet, and also tweets that do not belong to any of the categories (it is a multi-label classi cation). In this case, each tweet can be labeled with zero or more categories: and women. speech. To measure performance in this consider baseline this could capture absolutely none of the targets in the corpus. We thus devised another method: assigning the label X to a tweet if it contains one of the top words for label X in the training corpus. The collec- tion of top words was the la- bel (thus discarding the words that were too common, and the ones that were too rare). This method obtained 0.0595 F1 score on the dev set, and 0.0527 on the test set. 4 Competition The competition ran between March 18 and June 10, 2021 on the CodaLab4platform. Du- ring that time, a total of 74 users registered to participate, and 18 of those users submit- ted at least one system for the development or the evaluation phase. 4.1 Phases The competition consisted of three phases: Development phase: from April 8 to May 26. At the beginning of this phase, we relea- sed the training and development sets. Parti- cipants could train their systems and compa- re their results for the development in Spanish 263participant could submit up to 200 systems. There were 276 submissions. Evaluation phase: from May 27 to June 9. At the beginning of this phase, we released the test set. Participants could run their al- ready trained systems on the tweets and submit their Each participant could submit re 140 Post-Evaluation phase: mark their system. It could be used in the future to advance the state of the art in the- se tasks or to test alternative methods that the users could send to obtained part of o\u000ecial results of the competition. 4.2 Systems Descriptions submitted to the competition used neural networks for their solutions, most cases pre-trained neural language models such as BERT (De- vlin et GPT-2 (Radford et al., 2019), or BETO (Ca~ nete et 2020), a Spanish texts. Some models (for example SVM or Deci- sion Trees) in order to make comparisons, but none of the participants that sent their system descriptions submitted any of these models. In what follows, we give a brief description of each system: Jocoso the humor reached very results in all four tasks, the every URL, username and laugh using a unique a neural One path models each sentence separately, and the other path models the whole text, capturing the incoherence of the nal line (punchline) with respect to the previous ones. BERT4EVER (Wang et al., 2021), user Neakail, used a model based on BERT, continuing its pre-training with the training data from the task. For Tasks 3 and 4, they used a pseudo-labeling technique, using model of the unlabeled tweets and keeping the with high con dence, creating 1940 more silver-standard examples. Training a model with this new data obtained the best results for Tasks 3 and 4 in the competition. RoMa (Rodriguez, Ortega-Bueno, and Rosso, 2021), and transformers. They use both the pre- processed data and the original one in order to obtain sentence embeddings, or misspellings, using UMUTextStats. They got the best result for Task 2 in this competition. skblaz used a model called autoBOT, \u0014Skrlj et al., 2021). The system was run for 8 hours, the default neurosymbolic model was used. They report this was one of the rst non-English attempts with autoBOT. kuiyongyi (Kui, Multilingual BERT and LSTM models for Tasks 1, 3 and 4, and a GPT-2 based model Luis Chiruzzo, Santiago Castro, Task 4 each task for teams in the competition. The numbers in parenthesis indicate the position of the team with respect of the other participants in that task. and used these embeddings as hidden layers in a neural (Nanda, and Gupta, on a ne-tuning of BERT adapted to Tasks 1 and 2. They use the BERT encoding of the whole text and also individual senten- ces, extracting features from all of Tasks 1 and 2. The network has three layers: an embeddings layer that uses pretrained Spanish word embeddings, a multi kernel CNN layer, and a BiLSTM layer. Besides these submissions, there were th- ree more that participated in the com- petition (they are located around the middle of the table) but did not send any description of their system. The teams noda risa, hum- BERTor androBERTocarlos did not send any description of their systems, but are still included in Table 3. 5 Results Table 3 shows the results including the top result for each task for each team on the test set. The best system obtained 88.5 % F1 for humor detec- tion, a great improvement over the best re- sult in 2019 (82.1 % F1) and 2018 (79.7 % F1). However, we must take in considerationthat the test sets for the three editions we- re di erent, so they are not directly rable. The same happens for humor rating: this year's top system got 0.6226 RMSE for Task 1, while in 2019 the best system achie- ved 0.736, and in 2018 the best system achie- ved 0.9784. The numbers for this task seem to be improving as well, again with the caveat that the test sets were di erent. The humor mechanism and humor target classi cation tasks were new this year, and in this case the best system got 33.96 % macro- averaged F1 for the mechanism and 42.28 for targets. Even though these harder than the baseli- nes by a large margin. Although there is still considerable room for improvement, we nd these initial results encouraging to keep in this direction and pushing the li- mits in humor analysis. 6 Conclusions We presented the third edition of the HAHA task at IberLEF, including two new subtasks { humor mechanism and humor target classi- cation { in addition to the two previous editions edi- tion, the existing dataset was extended with a new test set, and also a subset was enriched with annotations for the new challenges. Fourteen in 1 and 0.6226 RMSE in Task 2. For Tasks 3 and 4, even if the results were not very high, most of the teams were able to improve over the proposed baselines. We consider that these are encouraging results, and we believe that with a larger corpus the learning process could be improved. The labeled datasets compiled for this challenge are publicly available5. We datasets and the insights from the cu- rrent evaluation will humor detection, S., C. F. Hempelmann, and S. Di Maio. 2002. Script oppositions and logical Modeling incon- gruities and their resolutions. Humor, 15(1):3{46. Attardo, and V. Raskin. 1991. Script theory revis(it)ed: Joke similarity and joke representation model. Humor: Internatio- nal Journal of Humor Research. Berger, A. A. 2017. An anatomy of humor . Routledge. Buijzen, M. psychology , 6(2):147{ 167. Castro, S., L. Chiruzzo, and A. Ros\u0013 a. 2018. Overview of the HAHA resources/tree/master/humor/haha2021IberEval 2018. In CEUR Workshop Pro- ceedings, volume 2150, pages 187{194. Castro, S., L. Chiruzzo, A. Ros\u0013 a, D. Ga- rat, and G. Moncecchi. 2018. A crowd-annotated Spanish corpus for hu- mor analysis. In Proceedings of the Sixth International Workshop on Natural Lan- guage Processing for Social Media , Associa- tion for Computational Castro, S., M. Cubero, D. Garat, and G. Moncecchi. 2016. Is this a joke? de- tecting humor in Fuentes, J.-H. Ho, H. Kang, and J. P\u0013 erez. 2020. Spanish pre-trained bert model and evaluation da- ta. In PML4DC at ICLR 2020 . Chiruzzo, L., S. Castro, M. Etcheverry, D. Garat, J. J. Prada, and A. Ros\u0013 a. 2019. Overview of HAHA at IberLEF 2019: Humor CEUR-WS. L., Castro, and A. Ros\u0013 a. 2020. HAHA 2019 Dataset: A Corpus for Humor Analysis in Spanish. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5106{5112. Devlin, J., M.-W. Chang, K. Lee, K. Tou- Transformers for Analy- sing Spanish Humor. The What, the How, and to Whom. In Proceedings of anchez, and Vaca. 2021. BERT and SHAP for Humor Analysis based on Ros\u00e1, J. Meaney, Rada Mihalcea 266Ghosh, A., G. Li, T. Veale, P. Rosso, E. Shu- tova, J. Barnden, and A. Reyes. 2015. Semeval-2015 task 11: Sentiment analysis of gurative language in twitter. In Pro- ceedings of the 9th international workshop on semantic evaluation (SemEval 2015), pages 470{478. Grover, HAHA@IberLEF2021: Humor Analysis Ensembles Humor 2020. Crossing line: Whe- re do demographic variables t into hu- mor detection? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Re- search Workshop, pages 176{181. Meaney, J., S. Wilson, L. Chiruzzo, A. Lo- Magdy. Semantic Ma- Investigations in automatic humor recognition. In Procee- dings Conference on Human Lan- guage Technology and Empirical Methods in Natural Language Processing, pages 531{538, Stroudsburg, PA, USA. Association for Computational Linguis- tics. Miller, C. F. hash- nal Workshop on Semantic Evaluation (SemEval-2017), pages 49{57. A., J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et multitask P. \u0010, and M. Taul\u0013 e. 2009. Caracter\u0013 \u0010sticas y rasgos afectivos del humor: Un estudio de reconocimien- to autom\u0013 atico del humor en textos escola- res en catal\u0013 an. Procesamiento del lenguaje natural , 43:235{243. Rodriguez, M., R. Ortega-Bueno, and P. Ros- so. 2021. RoMa at HAHA-2021: Deep Reinforcement Learning to a Transformed-based Model without recognizing meaning. WILF , volume 4578 of Computer Science, pages 469{476. Springer. \u0014Skrlj, B., M. Martinc, N. in- teraction in cartoons: Towards a multimo- dal theory Humor in Spanish 267Wang, L., X. Lin, N. Lin, Y. Fu, K. Wu, and J. Wu. 2021. Humor Analysis in Spanish Tweets with Multiple in at IberLEF 2021 de detecci\u0013 Technological Development (CDTec), Federal University of Pelotas Pelotas, RS, Brazil 2Arti cial Intelligence Innovation Hub (H2IA), Federal University of Pelotas, Pelotas, RS, Brazil 3Sul-Rio-Grandense Federal Institute of Education, Science, and Technology (IFSul), Brazil 4University which on de Iron\u0013 en Portugu\u0013 es (IDPT), realizada en el IberLEF 2021. Les pedimos a los participantes que desarrol- laran sistemas capaces de identi car la iron\u0013 \u0010a en los textos. Creamos dos corpora que contienen tweets y art\u0013 \u0010culos de noticias. Doce equipos se inscribieron en la tarea, entre los cuales seis presentaron predicciones e informes t\u0013 ecnicos. El sistema con mejor rendimiento logr\u0013 o un valor de precisi\u0013 on equilibrada (Bacc) de 0,52 para los tweets (Equipo PiLN) y 0,92 para las noticias (Equipo BERT4EVER). clave: Detecci\u0013 on de Iron\u0013 \u0010a, Portugu\u0013 es, Tweets, Noticias. Introduction This is nothing new that, in recent decades, a large part of human communication has taken place on the internet. This way, so- networks are essential for disseminating opinions, positions, re many of As a result, these platforms are also a valuable source of information about public opinion. Therefore, a target of interest for companies, advertis- ing, politics, and research, as pointed out by (Pang and Lee, 2008). In this context, there has been an increase in research inter- est in text mining on social networks in recent years. Social networks coexist with other meansof online communication and information. If, on the one hand, the immediate communica- tion which characterizes social networks such important way among other are broad- cast language, as are texts on so- cial networks. In this sense, they include Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 269-276 linguistic resources and communica- these spaces, even based on and other types of gurative language. As a characteristic of human language, communication on social networks some de facto acts involved in this communicational operation can confuse. Developing Natural Language Processing (NLP) resources that aim to irony is crucial ing several co-located with theSociedad Espa~ del Lenguaje Natural (SEPLN) conference. The IDPT task aims to challenge di published in the microblog- ging platform Twitter. The present paper presents an overview of the task. First, we brie y present some theoretical re exions on irony concepts (Section 2) and describe the proposal of our task (Section 3). Section 4 presents the corpora description and the an- notation process. In Section 5, we describe the evaluation Participant systems and the results are discussed in Section 6. Fi- nally, the nal remarks are done in Section 7. 2 On Irony Even in works that focus on system devel- opment, it is important to noted that theoretical in- sights are not our focus in this article. The irony concept is usually understood as a linguistic resource used with the purpose of expressing the the literal mean- ing of an utterance (Cignarella et al., 2018). As discussed in Freitas Santos, and Deon, e orts on studies, there is no consensus on the de nition of this linguis- tic phenomenon. Searle (Searle, 1969) and Grice (Grice, 1975) an by alluding to expectations (failures not). According Reyes (Reyes, Rosso, and Bus- caldi, 2012), often with the of irony. It is com- mon to nd discussions about the di erences (or between same phenomenon, making no distinctions. They are part of the second study group, such as those by Attardo (Attardo, 2000), Reyes (Reyes, Rosso, and Veale, 2013), and Hee (Van Hee, Lefever, and Hoste, 2016). Ac- cording to what Gibbs that sarcasm, humor, to di general, cannot consider the con- text of production of an focuses on identifying irony to Ulisses Leonardo Coelho, Leonardo Santos, Larissa A. de Freitas 2703 Task Description Inferring ironic meanings is an easy humans, yet some of the speech acts in- operation al., 2014). Creating to text can be a challenging task au- tomatically. Still, it is crucial to improve the performance of other NLP's tasks, e.g., Senti- (Gupta and Yang, 2017), and Hate Speech Detection (Bosco et al., 2018). This task aims to instigate participants to apply the amount of research done for this language. This task will contribute to the progress of Portuguese NLP, as there is a demand in the area for the development of new methods (Bueno et al., 2019), IronITA 2018 (Cignarella et al., 2018), and SemEval 2018 Task 3 (Hee, Lefever, and Hoste, 2018), in- spired us to develop the same objective: systems should determine whether a message is ironic or not according a speci ed con- text (assigning a binary value 1 or 0). This task is tasks on irony detection (Italian) (Cignarella et al., 2018), and SemEval 2018 Task 3 (English) (Hee, Lefever, and Hoste, 2018). 4 Corpora previously developed by Freitas (Freitas et al., 2014), Silva (Silva, 2018), and Schubert (Schubert and de Freitas, 2020) for training purposes. Freitas (Freitas between October and June 10th. Still, in this collection, we removed all retweets. Schubert (Schubert and de Freitas, from public datasets of tweets4and news articles5. In summary, this competition through manual annotation of 300 tweets and 300 news articles. The test is The show Big Brother Brasil6, referred through the hashtag #bbb. Both sets of tweets were joined and shuf- ed in a single corpus . Then the corpus was split into three subsets, and each subset was assigned to three annotators. dataset news comprises 118 ironic news from ario IberLEF 2021 271known nationally and a source of real news. The extraction was divided into two steps, the collection of news articles from the Di\u0013 ario de Barrelas and the collection of the latest news articles from R7. Then both sets, ironic and were using the Python Random package9. The shu\u000fed dataset was split into three sets with 100 samples each. We did the documents from news and tweets sets. set of volunteers annotators (linguists and science students) manually classify the The vol- unteers, the meaning should Eu a favor da sa\u0013 \u0010da da atual Presidente. [I am in favor de- parture the current President.] Ironic text: one must consider ironic the text where there is an opposition of meaning between what is intended and written. ao. [They are very (marquee viaduct) is a luxury one... In (1) the speaker expresses his opinion without presenting, in the text, that indicate a contradiction between the explicit opinion and the intended message. On the other hand, we observe in sen- tences (2) and (3) elements that contradict the explicit message and the intended one. Thus, in it is possible to money from cor- ruption\". Since we know that being noble is good quality and using money from corrup- tion is illegal, we can infer that the sentence is ironic. In (3), there are at least two ele- ments that indicate irony: (i) the opposition the of `living under a marquee or viaduct' and `living in a luxury house' and (ii) the use of three trite expressions used in Brazil by president Bolsonaro and porters (` petezada ' and `Venezuela' for refer- ring to left-wing people, and ` '). annotation We have not requested teams or even our volunteers' team to found. Leonardo Leonardo Santos, Larissa A. de Freitas 272Characters Tokens Mean Min Max Mean 2,548 1,813 305 10,362 299 49 1,810 TweetsIronic 124 22 300 19 5 51 Non-ironic 97 4 259 15 1 44 Table 1: Documents Statistics per corpus . the ironic and 177 non-ironic tweets and 115 ironic and 185 non-ironic news. 5 Evaluation Measures The training set has been released on March 30th, and participants had sixteen days to train their systems. The test set has been released on April 16th, and each participant had twenty maximum of three runs. Participating teams will and test datasets. The was sent with- out the label of teams registered to the and technical reports. Participants are from universities and companies from four di er- ent countries: Brazil, China, Portugal, and Spain. Participants used either traditional learning methods (Transformers). Tables 3 and 4 present re- For each system, best run is highlighted in bold. Team BERT4EVER, from China, used transformers to achieve a Bacc of 0.92 for news dataset. For the dataset, PiLN, use BERT model pre-trained by Souza et al. (Souza, Nogueira, and Lotufo, 2020) Overview of the ne-tune BERT model separately eld. Strat- egy 2: adopt the Loss Weight imbalance. Strategy SiDi-NLP: the authors BERT model pre-trained by Souza et al. (Souza, Nogueira, and Lotufo, 2020) to classify irony sentences and UFPR: the total of nine strategies in the preprocess- ing step, four on the feature extraction step (CountVectorizer, T dfVectorizer, HashingVectorizer, and in the learning step (RF, Ulisses B. Corr\u00eaa, Leonardo classify the documents. From the six systems presented, four use classical machine learning methods. Just two systems use deep learning methods, BERT4EVER and based on focused on Por- we proposed a task within the Iber- LEF 2021. This paper overviews the rst detection Portuguese for tweets subsets is fair. In which teams, from four countries, submitted both technical that several systems performed with Bacc values around 0.50 for the tweets task. Without a robust statistical framework, we can not assure the superiority of any of them. The deep learning methods, based on Transformers and BERT, performed bet- ter when dealing with bigger inputs. The BERT4EVER Team achieved a Bacc of 0.92 for the news dataset, while approaches based on classical methods performed, at best, Bacc Acknowledgments The annotators for their essential work. to Oliveira, Carolina Gadelha, Felix Silva, ao Soares, Johnny Ferreira Birck, Luiz Ot\u0013 avio Hammes, and Rodrigo Rodrigues. Gabriel Schubert and F\u0013 abio Ricardo Ara\u0013 ujo da Silva by the corpora used to create the training dataset.Johnny Ferreira Birck and Leonardo Coelho by the collection of the samples used in the test Sanguinetti, and M. Tesconi. 2018. Overview of the EVALITA 2018 hate speech detection task. In T. Caselli, N. Novielli, V. Patti, and P. Rosso, ed- itors, Proceedings of the Sixth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Work- shop (EVALITA 2018) WS.org. Bueno, R. O., F. M. R. Pardo, D. I. H. Far\u0013 \u0010as, P. Rosso, M. Montes-y-G\u0013 omez, and J. Medina-Pagola. Overview of the task irony detection ish variants. In M. \u0013A. G. J. Gonzalo, E. M. C\u0013 amara, R. Mart\u0013 \u0010nez-Unanue, P. Rosso, J. Carrillo- Montalvo, L. Chiruzzo, S. Collovini, Y. Guti\u0013 M. R. 35th Conference of the Spanish Soci- ety for Natural Language Processing, IberLEF@SEPLN 2019, Bilbao, Spain, September 24th, 2019 CEUR-WS.org. Cignarella, A. T., S. Frenda, V. Basile, C. Bosco, V. Patti, and P. Rosso. 2018. Overview of the EVALITA 2018 task on irony tweets (ironita). In T. V. Patti, P. Rosso, editors, Proceedings of the Sixth Evaluation Campaign of Natu- ral Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2018) co-located with A. A. Vanin, D. N. Hogetop, M. N. Bochernitsan, and R. Vieira. 2014. Pathways for irony detection in InProceedings Applied Computing , SAC '14, page 628{633, New York, NY, USA. Association for Computing Machinery. Ghanem, B., J. Karoui, F. Benamara, V. Moriceau, and P. Rosso. 2019. Idat at re2019: of the track on irony arabic FIRE '19, 10{13, New York, NY, USA. Association for Computing Machinery. Gibbs, R. W. and H. L. Colston. 2001. The risks and rewards of ironic communica- tion. IOS Press. Grice, P. 1975. Logic and conversation . Academic Press. Gupta, R. K. and and V. task 3: English International Workshop on Seman- tic Evaluation, pages 39{50, New Orleans, Louisiana, June. Association for Compu- tational Linguistics. Kreuz, R. J. and S. to be sarcastic: The choice reminder of ogy, 118(4):374{386. Marchetti, A., D. Massaro, and A. Valle. 2007. Non dicevo sul serio. Ri essioni su ironia e psicologia. Franco Angeli.Pang, B. and L. Lee. 2008. Opinion min- ing and sentiment analysis. Foundations and Trends\u00ae in Information Retrieval , 2(1{2):1{135. Reyes, A., P. Rosso, and D. Buscaldi. From humor recognition to irony detec- tion: The gurative language of social me- dia. Data and Knowledge Engineering, 74:2{12. Reyes, A., P. Rosso, and T. Veale. 2013. A multidimensional approach for detect- ing irony in twitter. Language Resources and Evaluation, 47(1):239{268. Schubert, G. and L. A. de Freitas. 2020. A constru\u0018 c~ ao e sarcasmo cial e Computacional , pages 709{717, Porto Alegre, RS, Brasil. SBC. Searle, J. R. 1969. Speech Acts: An Essay in the Philosophy of Language. Cambridge University Press. Silva, F. R. 2018. Detec\u0018 c~ ao de ironia e sarcasmo pretrained BERT models for Brazilian Portuguese. In 9th Brazil- ian Conference on Intelligent Systems, BRACIS, Rio Grande do Sul, Brazil, Oc- tober 20-23. Sperber, D. and D. Wilson. 1981. Irony and the Use - Mention Distinction . Academic Press. Van Hee, C., E. Lefever, and V. Hoste. 2016. Guidelines for annotating irony in social media text, version 2.0. Technical report, LT3 Technical Report Series. Wick-Pedro, G. and O. A. Vale. 2020. Co- mentcorpus: Description and analysis of irony in a corpus of 276Overview of ADoBo 2021: Automatic Detection ofUnassimilated Borrowings in Press on autom\u0013 atica de pr\u0013 estamos l\u0013 en prensa espa~ nola Elena \u0013Alvarez Mellado1, Luis Espinosa Anke2, Julio Gonzalo Arroyo1, Constantine Lignos3, Jordi Porta Zamorano4 1NLP & IR group, UNED, Madrid, Spain 2School of Computer Science and Informatics, Cardi University, Cardi , UK 3Michtom School of Computer Science, Brandeis University, Massachusetts, USA 4Centro de Estudios de la Spain the ADoBo 2021 shared task, proposed in the context of IberLef (coming mostly Spanish newswire was as corpus of lexical borrowings which we split into training, development and test splits. We received submissions from 4 teams with 9 di erent system runs overall. The results, which range from F1 scores of 37 to 85, suggest that this is a challenging task, especially when 2021, la tarea compartida de IberLEF 2021 sobre detecci\u0013 on de pr\u0013 estamos l\u0013 exicos en la prensa espa~ nola. En esta tarea abordamos la detecci\u0013 on de pr\u0013 estamos como un problema de etiquetado de secuencias. A los participantes de la tarea se les proporcion\u0013 o un corpus de prensa espa~ nola anotado con pr\u0013 estamos l\u0013 exicos no asimilados (mayoritari- amente anglicismos) siguiendo el esquema BIO. Recibimos nueve sistemas distintos provenientes de cuatro equipos diferentes. Los resultados obtenidos oscilan entre los 37 y los 85 puntos de valor F1, lo que indica que la detecci\u0013 on de pr\u0013 estamos l\u0013 exicos es un problema no resuelto (sobre todo cuando se abordan pr\u0013 estamos no vistos anteri- ormente) y que el trabajo lexicogr\u0013 a co tradicional podr\u0013 \u0010a bene ciarse de incorporar process of into another 2007; Sanko , and guages. The proven to as well 2014) (Tsvetkov and Dyer, 2016). In a reader of French newspapers encounters a new lexi- cal borrowing every 1,000 words (Chesley and Baayen, 2010), English borrowings outnum- Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 277-285 Sociedad del Lenguaje Naturalbering all other borrowings to anglicisms (Gerding et al., 2014). In Euro- it was estimated that angli- 2% Gonz\u0013 alez, a num- ber that is likely to be higher today. As a result, the usage of the general public. For ADoBo 2021, a shared task on automatically detecting lexical bor- rowings Spanish newswire, with a special focus on unassimilated anglicisms. In this pa- per we describe the purpose and scope of the shared task, introduce the systems that par- ticipated in it, and share 2 French (Alex, 2008a; Chesley, 2010), Finnish (Mansikkaniemi and Kurimo, 2012), and Norwegian (Andersen, 2012; Losnegaard and Lyse, 2012), with a particular NLP literature for Iberian lan- guages in general and for Spanish in particu- lar, with only a few recent covers a wide range of linguistic phenomena, words, elements or patterns of one language (the into another language (the recipient language) (Haugen, 1950; 1963). In discourse is among bilin- and in fact both phenomena have sometimes described as a contin- uum with a fuzzy frontier between the two (Clyne, Clyne, and Michael, 2003). Con- sequently, disagreement on what borrow- ing is (and exists (G\u0013 omez Capuz, Tad- mor, 2009) and for anglicism ish 4 Task description For the ADoBo shared task we have fo- cused on unassimilated lexical borrowings, words from used in Spanish without orthographic modi cation and that have not (yet) been integrated the recipient language|for example, run- ning, smartwatch, time and lawfare . 4.1 Motivation for the task The task lexical it might appear to be at rst. To begin with, lexical borrowings can be either single or multitoken expressions (e.g., time, tie break learning ). Second, linguistic assimilation is a unassimilated lexical borrowings in Spanish at some point in the past, but have been around for so long in the Spanish language that the process of phonological and morphological adaptation is cannot borrowings anymore. speci c elements whose name entered via the language of origin decades ago (like jazz or whisky ) the annotation Consequently, in prior work on extraction from Spanish text, plain dictionary lookup pro- duced very limited results with F1 scores of Elena \u00c1lvarez Mellado, Luis Espinosa Anke, Julio Gonzalo Arroyo, Constantine Lignos, Jordi Porta Zamorano 278Team System Type Prec. Table 1: set. For each label, precision, recall and F1 score are Unique Train 231,126 1,493 380 Dev. 82,578 306 3,038 or always be deter- mined by plain dictionary lookup; after all, an expression such as social media is an an- glicism in Spanish, even when both social and media also happen to be Spanish words that are registered in regular dictionaries. This justi es the need for a more NLP-heavy ap- proach to the task, which has already Previous work produced an F1 score Spanish headlines ( Mellado, language sively on code-switched data (Solorio et al., 2014; Molina et al., 2016; Aguilar et al., 2018), which is close to borrowing but dif- ferent in scope and nature (see exists on borrowing de- tection in NLP so far. To the best of our knowledge, ADoBo is the rst shared task speci cally devoted to Spanish was distributed to the The corpus articles were sourced from various Spanish newspapers and online media best big data) and that those units should be treated as one borrowing and not as independent borrowings, BIO encoding was used to of Borrowings in the Spanish Press 279Team System Table 3: Results on the lower-cased version of the test the boundaries of each span. Two classes were used for borrowings: for English borrowings, and OTHER for lexi- cal borrowings from other languages. Tokens that were not part of a borrowing were anno- tated with The data was distributed in CoNLL for- mat. An additional collection of documents that was not evaluated (the background set) was released as a part of the test set. This was done to encourage scalability to larger data collections and manual examination of the evaluated part of the test set. The dataset contained a high number of more of tokens spans 4.3 Evaluation metrics The evaluation metrics used for the task precision, and F1 over spans: Precision: The ed. Recall: The percentage of test that nized classi o\u000ecial teams. Evaluation was done ex- clusively This means that only exact matches were considered, and no Elena \u00c1lvarez Mellado, Luis Espinosa Anke, Julio Gonzalo Arroyo, Constantine Lignos, Jordi Porta Zamorano 280Team System Type Prec. Table 4: Results on the unquoted version of the test set. credit was given to partial matches. For ex- to be cor- rectly labeled in to count as a true pos- itive. This makes the evaluation more rigor- the scores that can sometimes result from token result on a token-level evaluation. 4.4 during training: annotation was al- lowed for training. Given that the main purpose of the shared task was to evalu- ate how di erent models perform for the task fair evaluation of di er- ent model approaches. Although the usage of regular lexiconsand extraction) were allowed. The reason for this limitation that we were interested in a result, we have no description whatsoever for two of the participating systems, including the one that obtained the best results. We provide a brief summary of the two participating systems for which we received a submission, and refer the reader to their respective task description pa- pers for further 2021: in the Spanish Press 281Team System 463 337 ALL 72.75 419 304 ALL 73.17 Table 5: Results on the unquoted and lower-cased version of the test set. 5.1 BERT4EVER team: CRF model with data augmentation The BERT4EVER on di erent of the task's training data. The models were used to label a freely-available open corpus in Spanish, and individual models were then re-trained on the output. Results suggest that this strategy improves two F1 points on the test set when compared to a trained- on-task-data-only baseline. The paper com- ML tool- box, namely CRFs and data augmentation, and shows models evaluating multilingual BERT, RoBERTa, and models trained on small sets of languages. 6 Results Results Versae team (F1=84.80, P=88.77 and R=81.17). In order to get a better understanding of the systems that took part in the shared task, we performed some experiments on the out- put that was submitted by participants. 1https://github.com/bltlab/seqscore Elena \u00c1lvarez Mellado, Luis Espinosa Anke, Julio Gonzalo Arroyo, Constantine Lignos, Jordi Porta Zamorano 2826.1 Combining outputs In order to assess the complementarity of the submitted systems, an experiment was ried out combining their outputs. The com- bination consisted the union of all detected terms. Since the number of systems is not very high, all combinations of systems were explored. In terms of F1 score, the best per- forming combination was (1), (2), and with ortho-typographic cues Three variations of the test set were included in the background set (the additional collec- tion of documents released along with the test set): 1. A lowercase version, where all uppercase letters in the original test set were trans- formed to lowercase. 2. A no-quotation-mark version, where all quotation marks in the original test set (\\ \" ` ' ) were removed. 3. A lowercase no-quotation-mark removed. None of these versions were used to rank the systems but to observe the systems dif- ference in performance on di erent textual characteristics. The rationale for these ex- periments marks) were still a borrowing regardless of whether it is written with or without quo- tation marks and it would be of little use to have a model that systematically labeled any- thing between quotation marks as a borrow- or that only if they are between usually proper uppercase where no case distinction exists|and that these cues are not present in other textualgenres (e.g. social media), we were interested in assessing how well the models performed when no case cue was available. Results for these experiments are pre- sented in Tables 3, 4 and 5. Focusing on the best two performing systems, we observe a drop of global F1 due to a consistent drop on precision not compensated with the a slightly increase of recall for the lowercased versions of the test set. In general, the drop in sys- tem (2) is more pronounced than in system (1), which causes its repositioning in the cor- responding rankings. For the unquoted ver- sion of the test set, system (1) increases its F1 and system (2) decrements it slightly. Not having information on system (1), we can not attribute any of the di erences to any char- acteristics of the systems. 7 Conclusions In this paper we have presented the results of the ADoBo shared task on extracting similated for this topic, we have described the scope and nature of the proposed task, we have shared the obtained results and have the main ndings. Participants results from F1 scores of 37 to 85. These scores show that this is not a triv- ial task and that lexical borrowing detection is an open problem that requires further re- search. Our goal with this shared task was to raise awareness about a topic that, although within NLP. Although the participation for this rst edition was mod- est (nine systems submitted from four di er- ent teams), the response was positive and it seems to indicate there exists a moder- ate population within the community that is interested in borrowing as Spanish Press 283switched data: Overview of the CALCS 2018 shared task. In Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching , to thesis, of the Con- on Language Resources and Eval- uation L\u0013 of French Language Studies, 20(3):231{251. Chesley, P. and R. H. M. G. Clyne, and C. Michael. 2003. Dynamics of language contact: En- glish and immigrant languages. Cam- bridge University Press. Furiassi, C. and K. Ho and. 2007. The retrieval of false anglicisms in newspaper texts. In Corpus Years Brill Rodopi, pages 347{363. Furiassi, C., V. Pulcini, hip hop forum. In Proceedings Korea, July. Association for Computational Linguistics. Gerding, C., M. Fuentes, linguistic borrow- ing (illustrated with anglicisms in romance languages). Revista alicantina de estudios ingleses, 10:81{94. Haspelmath, M. and U. Tadmor. 2009. Loanwords a comparative handbook. linguistic borrowing. Language, 26(2):210{231. Leidig, S., T. Schlippe, and T. Schultz. 2014. Automatic detection of anglicisms for the pronunciation dictionary generation: a case study on our German IT corpus. In Spoken Language Technologies for Under- hisp\u0013 anica: Estudios Gredos. Losnegaard, G. S. and G. I. Lyse. 2012. A data-driven approach to anglicism identi - edi- Exploring Newspaper Language: Us- ing the web to create and investigate a large corpus of modern Norwegian. John Benjamins Publishing, pages 131{154. Mansikkaniemi, A. and M. Kurimo. 2012. vocabulary adaptation morph-based language NAACL-HLT 2012 Work- shop: Will We Ever Really Replace the N- gram On the perspec- G., AlGhamdi, M. Ghoneim, A. Hawwari, N. Rey-Villamizar, M. Diab, and T. Solorio. 2016. Overview for the Elena \u00c1lvarez Mellado, Luis Espinosa Anke, Julio Gonzalo Arroyo, Constantine Lignos, Jordi Porta Zamorano 284second shared task on language identi ca- tion in code-switched data. In Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 40{ 49, Austin, Association for Computational of social correlates Pratt, en el espa~ nol peninsular contempor\u0013 aneo , G\u007f orlach, editor, English in Europe. Ox- ford University Press, chapter 7, pages 128{150. Serigos, J. to Anglicisms in Spanish. Ph.D. thesis, The University of Texas at Austin. Solorio, T., E. Blair, S. Maharjan, S. Bethard, M. Diab, M. Ghoneim, A. Hawwari, F. J. Hirschberg, A. Chang, and P. Fung. 2014. Overview for the rst shared task on language identi cation in code-switched data. InProceedings of the First Workshop on Computational Approaches to Code Switching, pages 62{72, Doha, Qatar, October. Association for Computational Linguistics.Thomason, S. G. and T. Kaufman. 1992. Language contact, creolization, and ge- netic linguistics. Univ of and Cross- lingual bridges of cial Intelligence Research, 2021: Automatic Detection of Unassimilated Borrowings in the Spanish Press 285 Informaci\u00f3n General Informaci\u00f3n para los Autores Formato de los Trabajos La longitud m\u00e1xima admitida para las contribuciones ser\u00e1 de 10 p\u00e1ginas DIN A4 (210 x 297 mm.), incluidas referencias y figuras. Los art\u00edculos pueden estar escritos en ingl\u00e9s o espa\u00f1ol. El t\u00edtulo, resumen y palabras clave deben escribirse en ambas lenguas. El formato ser\u00e1 en Word \u00f3 LaTeX Env\u00edo de los Trabajos El env\u00edo de los trabajos se realizar\u00e1 electr\u00f3nicamente a trav\u00e9s de la p\u00e1gina web de la Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural ( http://www.sepln.org ) Para los trabajos con formato LaTeX s e mandar\u00e1 el archivo PDF junto a todos los fuentes necesarios para compilaci\u00f3n LaTex Para los trabajos con formato Word se mandar\u00e1 el archi vo PDF junto al DOC o RTF Para m\u00e1s informaci\u00f3n http://www.sepln.org/la -revista/informacion -para-autores Informaci\u00f3n Adicional Funciones del Consejo de Redacci\u00f3n Las funciones del Consejo de Redacci\u00f3n o Editorial de la revista SEPLN son las siguientes: Controlar la selecci\u00f3n y tomar las decisiones en la publicaci\u00f3n de los contenidos que han de conformar cada n\u00famero de la revista Pol\u00edtica editorial Preparaci\u00f3n de cada n\u00famero Relaci\u00f3n con los evaluadores y autores Relaci\u00f3n con el comit\u00e9 cient\u00edfico El consejo de redacci\u00f3n est\u00e1 formado por los siguientes miembros L. Alfonso Ure\u00f1a L\u00f3pez (Director) Universidad de Ja\u00e9n laurena@ujaen.es Patricio Mart\u00ednez Barco (Secretario) Universidad de Alicante patricio@dlsi.ua.es Manuel Palomar Sanz Universidad de Alicante mpalomar@dlsi.ua.es Felisa Verdejo Ma\u00edllo UNED felisa@lsi.uned.es Funciones del Consejo Asesor Las funciones del Consejo Asesor o Cient\u00edfico de la revista SEPLN son las siguientes: Marcar, orientar y redireccionar la pol\u00edtica cient\u00edfica de la revista y las l\u00edneas de investigaci\u00f3n a potenciar Representaci\u00f3n Impulso a la difusi\u00f3n internacional Capacidad de atracci\u00f3n de autores Evaluaci\u00f3n Composici\u00f3n Prestigio Alta especializaci\u00f3n Internacionalidad El Consejo Asesor est\u00e1 formado por los siguientes miembros: Manuel de Buenaga Universidad de Alcal\u00e1 (Espa\u00f1a) Sylviane Cardey -Greenfield Centre de recherche en linguistique et traitement automatique des langues (Francia) Jos\u00e9 Camacho Collados Cardiff University (Reino Unido) Irene Castell\u00f3n Universidad de Barcelona (Espa\u00f1a) Arantza D\u00edaz de Ilarraza Universidad del Pa\u00eds Vasco (Espa\u00f1a) Antonio Ferr\u00e1ndez Universidad de Alicante (Espa\u00f1a) Koldo Gojenola Universidad del Pa\u00eds Vasco (Espa\u00f1a) Xavier G\u00f3mez Guinovart Universidad de Vigo (Espa\u00f1a) Jos\u00e9 Miguel Go\u00f1i Universidad Polit\u00e9cnica de Madrid (Espa\u00f1a) Ram\u00f3n L\u00f3pez -C\u00f3zar Delgado Univers idad de Granada (Espa\u00f1a) Mariana Lara Neves German Federal Institute for Risk Assessment (Alemania) Procesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021 \u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural Elena Lloret Universidad de Alicante (Espa\u00f1a) Bernardo Magnini Fondazione Bruno Kessler (Italia) Nuno J. Mamede Instituto de Engenharia de Sistemas e Comput adores (Portugal) M. Teresa Mart\u00edn Valdivia Universidad de Ja\u00e9n (Espa\u00f1a) Patricio Mart\u00ednez -Barco Universidad de Alicante (Espa\u00f1a) Eugenio Mart\u00ednez C\u00e1mara Universidad de Granada (Espa\u00f1a) Paloma Mart\u00ednez Fern\u00e1ndez Universidad Carlos III (Espa\u00f1a) Raquel Mart\u00ednez Unanue Universidad Nacional de Educaci\u00f3n a Distancia (Espa\u00f1a) Leonel Ruiz Miyares Centro de Ling\u00fc\u00edstica Aplicada de Santiago de Cuba (Cuba) Ruslan Mitkov University of Wolverhampton (Reino Unido) Manuel Montes y G\u00f3mez Instituto Nacional de Astrof\u00ed sica, \u00d3ptica y Electr\u00f3nica (M\u00e9xico) Llu\u00eds Padr\u00f3 Universidad Polit\u00e9cnica de Catalu\u00f1a (Espa\u00f1a) Manuel Palomar Universidad de Alicante (Espa\u00f1a) Ferr\u00e1n Pla Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a) German Rigau Universidad del Pa\u00eds Vasco (Espa\u00f1a) Horacio Sa ggion Universidad Pompeu Fabra (Espa\u00f1a) Paolo Rosso Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a) Emilio Sanch\u00eds Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a) Kepa Sarasola Universidad del Pa\u00eds Vasco (Espa\u00f1a) Encarna Segarra Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a) Thamar Solorio University of Houston (Estados Unidos de Am\u00e9rica) Maite Taboada Simon Fraser University (Canad\u00e1) Mariona Taul\u00e9 Universidad de Barcelona Juan-Manuel Torres -Moreno Laboratoire Informatique d'Avignon / Universit \u00e9 d'Avignon (Francia) Jos\u00e9 Antonio Troyano Jim\u00e9nez Universidad de Sevilla (Espa\u00f1a) L. Alfonso Ure\u00f1a L\u00f3pez Universidad de Ja\u00e9n (Espa\u00f1a) Rafael Valencia Garc\u00eda Universidad de Murcia (Espa\u00f1a) Ren\u00e9 Venegas Vel\u00e1sques Pontificia Universidad Cat\u00f3lica de Valpara\u00eds o (Chile) Felisa Verdejo Ma\u00edllo Universidad Nacional de Educaci\u00f3n a Distancia (Espa\u00f1a) Manuel Vilares Universidad de la Coru\u00f1a (Espa\u00f1a) Luis Villase\u00f1or -Pineda Instituto Nacional de Astrof\u00edsica, \u00d3ptica y Electr\u00f3nica (M\u00e9xico) Cartas al director Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural Departamento de Inform\u00e1tica. Universidad de Ja\u00e9n Campus Las Lagunillas, Edificio A3. Despacho 127. 23071 Ja\u00e9n secretaria.sepln@ujaen.es M\u00e1s informaci\u00f3n Para m\u00e1s informaci\u00f3n sobre la Sociedad Esp a\u00f1ola del Procesamiento del Lenguaje Natural puede consultar la p\u00e1gina web http://www.sepln.org . Si desea inscribirse como socio de la Sociedad Espa\u00f1ola del Procesamiento del Lenguaje Natural puede realizarlo a trav\u00e9s del formulario web que se encuentra en esta direcci\u00f3n http://www.sepln.org/sepln/la -sociedad Los n\u00fameros anteriores de la revista se encuentran disponibles en la revista electr\u00f3nica: http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/issue/archive Las funciones del Consejo de Redacci\u00f3n est\u00e1n disponibles en Internet a trav\u00e9s de http://www.sepln.org/la -revista/consejo -de-redaccion Las funciones del Consejo Asesor est\u00e1n disponibles Internet a trav\u00e9s de la p\u00e1gina http://www.sepln.org/la -revista/ consejo -asesor La inscripci\u00f3n como nuevo socio de la SEPLN se puede realizar a trav\u00e9s de la p\u00e1gina http://www.sepln.org/sepln/inscripcion -para-nuevos -socios \u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalIberLEF 2021: Res\u00famenes de las tareas de evaluaci\u00f3n Overview of FakeDeS at IberLEF 2021: Fake News Detection in Spanish Shared Task Helena G\u00f3mez -Adorno, Juan Pablo Posadas -Dur\u00e1n, Gemma Bel Enguix, Claudia Porto Capetillo ...... 223 Overview of the normalization ing and Analyzing Humor Spanish Luis Chiruzzo, Santiago Castro, Santiago G\u00f3ngora, Ros\u00e1, J. ................................ Automatic Detection Unassimilated Borrowings in the Spanish Press Elena \u00c1lvarez Mellado, Luis Espinosa Anke, Julio Gonzalo Arroyo, Constantine Lignos, Jordi Porta Zamorano ................................ ................................ ................................ ................................ ................... \u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural "}