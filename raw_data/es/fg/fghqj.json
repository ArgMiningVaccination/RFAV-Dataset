{"title": "PDF", "author": "PDF", "url": "https://www.rae.es/sites/default/files/2023-05/Discurso%20de%20ingreso%20de%20Asuncion%20Gomez-Perez.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "id": "PDF", "license": "PDF", "body": "PDF", "comments": "PDF", "commentsbody": "PDF", "raw_text": "PDF", "text": "Inteligencia artificial \ny\u00a0lengua española\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   3 11/5/23   12:0411/5/23   12:0434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   4 11/5/23   12:0411/5/23   12:04REAL ACADEMIA ESPAÑOLA\nInteligencia artificial \ny\u00a0lengua española\nDISCURSO LEÍDO\nEL DÍA 21 DE MAYO DE 2023\nEN SU RECEPCIÓN PÚBLICA\nPOR LA EXCMA. SRA.\nD.\u00aa ASUNCIÓN GÓMEZ-PÉREZ\nY CONTESTACIÓN DEL EXCMO. SR.\nD. SANTIAGO MUÑOZ MACHADO\nMADRID\n2023\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   5 11/5/23   12:0411/5/23   12:04\u00a9 Asunción Gómez-Pérez y \nSantiago Muñoz Machado. 2023\nDepósito Legal: M-16895-2023\nImpreso por: Safekat, S. L.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   6 11/5/23   12:0411/5/23   12:04Discurso  \nde la  \nExcma. Sra. D.\u00aa Asunción Gómez-Pérez\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   7 11/5/23   12:0411/5/23   12:0434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   8 11/5/23   12:0411/5/23   12:049Señora Vicepresidenta primera del Gobierno, señoras y se-\nñores académicos, señoras y señores:\nM is primeras palabras emocionadas son de profunda \nsorpresa, gratitud y alegría por el gran honor que me han otorgado al elegirme para ocupar la silla q, vacante por el \nfallecimiento del Excmo. Sr. D. Gregorio Salvador Caja el\u00a026 de diciembre de 2020. Mi agradecimiento más since-ro hacia los académicos D. Pedro García Barreno, D. Luis Mateo Díez y D. Salvador Gutiérrez Ordóñez, que presen-taron mi candidatura y confiaron en mis méritos. \nCon gran atrevimiento me presento ante ustedes, selec-\ncionando algunas de las palabras del discurso leído el 31 de octubre de 1920 por el académico, catedrático en la Univer-sidad Politécnica de Madrid y polímata D. Leonardo Torres Quevedo, cuando manifestaba que era profano en achaques literarios y llegaba de tierras muy remotas. En mi caso, soy licenciada en Informática y doctora en Ciencias de la Com-putación e Inteligencia Artificial por la misma universidad. \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   934439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   9 11/5/23   12:0411/5/23   12:0410Desde hace 30 años, he sido testigo y partícipe del pro-\ngreso tecnológico digital en España. Enseño inteligencia \nartificial e ingeniería ontológica, y he dirigido numerosos proyectos internacionales y nacionales de investigación e innovación en la Universidad Politécnica de Madrid. Mi laboratorio está lleno de computadoras. En él imaginamos tecnologías en la frontera del conocimiento; realizamos proyectos que permiten avanzar a las Administraciones p\u00fa-blicas, al tejido empresarial y a la industria en multitud de dominios y sectores; proyectos que, en definitiva, son por y para la sociedad. Mis inquietudes lingüísticas, no obstante, se acercan más al uso de la semántica de las palabras por las máquinas y al multilingüismo computacional que a los sa-beres de esta docta casa.\nManifiesto, con profunda humildad, que el que me ha-\nyan elegido para formar parte de esta honorable academia representa para mí un inmenso privilegio que nunca soñé alcanzar, a la vez que un reto y una enorme responsabili-dad al ser la primera doctora en Informática, especialista en inteligencia artificial, que formará parte de vuestro Ple-no. Declaro p\u00fablicamente, en estas primeras palabras ante todos ustedes, el compromiso que adquiero para colabo-rar con mi conocimiento y esfuerzo en las tareas de esta real institución y, en particular, para contribuir en los tra-bajos que se me encomienden. Por un lado, mis aporta-ciones han de ir encaminadas a poner la inteligencia arti-ficial al servicio de la Lengua Española, pero también a que los materiales de calidad de la Lengua Española estén en formatos apropiados para las tecnologías de la inteligencia artificial. \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   1034439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   10 11/5/23   12:0411/5/23   12:0411No puedo finalizar sin referirme a las personas de mi \nentorno más cercano que, de un modo más directo, han \ninfluido, me han apoyado y me han animado en mi forma-ción y en mi carrera investigadora. Procedo, pues, de Azua-ga, pueblo de la provincia de Badajoz. Crecí en una familia dedicada a las ciencias de la salud. Soy afortunada; mis \n padres me han dado todo tipo de facilidades para elegir en la vida y me han enseñado los valores fundamentales para actuar en consecuencia: trabajo bien hecho, esfuer\nzo, rigor, \nresponsabilidad y constancia. Gracias a todos los integran-tes del grupo de Ingeniería Ontológica en la Universidad Politécnica de Madrid por vuestro empeño, dedicación, entusiasmo y generosidad, y por esa curiosidad intelectual para estar permanentemente en continua evolución, imagi-nando nuevos retos científicos y tecnológicos en la frontera de la computación y de la inteligencia artificial. Pero nada de esto sería posible sin la impagable ayuda de mi familia y, especialmente, de mi marido Manuel Pizarro y de mis tres hijas: Carolina, Asunción y \n María. Conciliar la vida perso-\nnal y profesional en la inv\nestigación, siendo mujer, no es \ntarea fácil. Gracias por vuestra comprensión y generosidad ante el tiempo no compartido y las renuncias a tantos mo-mentos. Sin vuestro apoyo constante no estaría yo hoy aquí, en este acto de toma de posesión del sillón q en esta Real Academia Española. Vosotros sois la u que siempre está junto a la q, y yo la q que no puede vivir sin la u. \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   1134439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   11 11/5/23   12:0411/5/23   12:0412La biografía de cada uno de nosotros es puro azar, desta-\ncaba D. Manuel Alvar López en su discurso de réplica a \nD.\u00a0Gregorio Salvador Caja 1 refiriéndose a la niñez de este. \nEse azar, como en los sorprendentes relatos de G. Salvador que forman parte de su libro Casualidades\n 2, ha hecho que \nme haya correspondido la silla designada con la letra q  mi-\nn\u00fascula y que recuerde al gran filólogo, experto en lexicolo-gía, dialectología y semántica estructural. Los méritos de Gregorio Salvador, su talento, su entrega y su pasión por la lengua fueron extraordinarios. Combinó su actividad do-cente con otras tareas literarias como escritor y articulista. En palabras de Manuel Alvar, era un hombre profundamen-te realista, con una gran carga de cultura a cuestas, y cuyos trabajos desprenden una constante agudeza y originalidad. \nOriundo de C\u00fallar\n 3, Granada, Gregorio Salvador Caja \n(1927-2020) fue elegido en el pleno de la Academia del 5 de junio de 1986, e ingresó en la institución el 15 de febre-ro de 1987. Ocupó la silla q min\u00fascula, que era de nueva creación, y, al no existir académico predecesor al que elo-giar, en su original discurso Sobre la letra \u00abq\u00bb\n 4 disertó, en-\n1 G. Salvador Caja (1987): Sobre la letra \u00abq\u00bb. Discurso leído el día \n15 de febrero de 1987, en su r\necepción p\u00fablica, por el Excmo. Sr. Don \nGregorio Salvador Caja y contestación del Excmo. Sr. D. Manuel Al-\nvar López. Madrid: Imprenta Aguirre. https://www.rae.es/sites/de-fault/files/Discurso_Ingreso_Gregorio_Salvador.pdf\n2 G. Salvador (1994): Casualidades. Madrid: Espasa Calpe.\n3 P . Álvarez de Miranda (2022): \u00abGregorio Salvador Caja (1927 \n-2020): In memoriam\u00bb. BRAE\n, tomo cii, cuaderno cccxxv , 341-347. \nhttps://www.rae.es/noticia/publicado-el-cuaderno-cccxxv-del-brae\n4 G. Salvador Caja (1987): Op. cit.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   1234439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   12 11/5/23   12:0411/5/23   12:0413salzó y reflexionó, desde el pasado, pero con la mirada \npuesta en el futuro, sobre el papel que le corresponde a esta letra en nuestra lengua. Ilustró su exposición hablando de \u00absus peculiaridades, de su origen y vicisitudes, de sus de-tractores y partidarios, de sus blasones y de sus flaquezas, de su evidente vulnerabilidad y de su posible razón de ser\u00bb. Su discurso fue contestado en nombre de la corporación por D. Manuel Alvar López, su director de tesis doctoral, maestro, amigo y también director del Atlas Lingüístico y Etnográfico de Andalucía\n 5, en el que Salvador tuvo una efi-\ncaz colaboración. \nSu actividad docente comenzó en un instituto de ense-\nñanza media. Posteriormente, ejerció como catedrático de lengua española en las universidades de La Laguna, Grana-da, Autónoma de Madrid y Complutense, de la que fue profesor emérito. \nHe seleccionado algunas frases del discurso de entrada \nque reflejan su amor por la lengua y su vocación de servicio a esta Academia. Gregorio Salvador se definía como \u00abaman-te de esta lengua que nos une a tantos pueblos y a tantas gentes\u00bb. Y seguía: \u00abMe enorgullezco de ella. A su enseñan-za, a su análisis, a su investigación, he dedicado una parte esencial de mi vida\u00bb. Añadía más adelante, refiriéndose a la Institución: \u00abHaré lo posible por no defraudar la confian-za\u00bb, \u00abPrometo que cumpliré las tareas que me encomen-déis\u00bb. Y terminaba diciendo: \u00abEspero con mi trabajo con-\n5 M. Alvar, con la colaboración de Antonio Llorente y Gregorio \nSalvador (1961-1973): \nAtlas lingüístico y etnográfico de Andalucía, 6 \ntomos. Granada: Universidad de Granada-CSIC.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   1334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   13 11/5/23   12:0411/5/23   12:0414tribuir a sus ineludibles obligaciones para con la lengua que \nnos une\u00bb. Y así fue: solo cinco días antes de fallecer, Grego-rio Salvador ocupaba el primer lugar en el escalafón de asis-tencias de la Real Academia, con la asombrosa cifra de 2385 asistencias\n 6. Asimismo, prestó inestimables servicios a la ins-\ntitución ocupando los cargos de bibliotecario (entre 1990 y 1998), vicedirector (entre 2000 y 2007) y director de la Es-cuela de Lexicografía (2002-2009); ocupó también la presi-dencia de la Asociación de Academias de la Lengua Españo-la entre 1992 y 1998. Como lexicógrafo, dirigió los trabajos preparatorios de la vigésima primera edición del Diccionario de la lengua española ( DRAE 1992), fue ponente de la comi-\nsión encargada de redactar las normas para la elaboración del Gran diccionario de americanismos (1996), y fue desig\n-\nnado como delegado para la preparación de la vigésima \n segunda edición del DRAE, publicada en 2001. A\u00a0todo ello\n \nse suman los encargos de ejercer la representación institucio-nal ante otras instancias oficiales o culturales.\nSus extensos méritos\n 7 le fueron reconocidos con hono-\nres en España y más allá de nuestras fronteras: miembro de honor de la Asociación de Hispanistas de Asia y de la Aso-ciación de la Prensa de Madrid; correspondiente de la Aca-demia Nacional de Letras de Uruguay, de las academias Chilena y Hondureña de la Lengua, y de la Academia Ar-\n6 \u00abIn memoriam Gregorio Salvador Caja\u00bb. Boletín de Información \nLingüística de la Real Academia E\nspañola, n\u00famero 17, 2021. http://\nrevistas.rae.es/bilrae/article/view/457/939 \n7 Real Academia Española: Gregorio Salvador Caja. https://www.\nrae.es/academico/gregorio-salvador-caja \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   1434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   14 11/5/23   12:0411/5/23   12:0415gentina de Letras. Asimismo, fue académico honorario de \nlas academias Colombiana y Nicaragüense de la Lengua. Fue presidente también de la Sociedad Española de Lingüís-tica (1990-1994). Recibió la Medalla de Honor de la Uni-versidad Internacional Menéndez Pelayo (2004) y los pre-mios de periodismo José María Pemán (1987), Mesonero Romanos (1995), César González Ruano (2001) y Mariano de Cavia (2004). Contaba, además, entre otros reconoci-mientos, con la Gran Cruz de la Orden Civil de Alfonso X el Sabio (1999) y la Medalla de Andalucía (2010).\nNo conocí personalmente a Gregorio Salvador, pero \nhubiera disfrutado aprendiendo del maestro y estudiando con él algunos aspectos que se derivan de la interpretación de la semántica estructural y de los procesos de construc-ción de diccionarios en un contexto interdisciplinar de lin-güística e inteligencia artificial. Para mí es una gran res-ponsabilidad tratar de relacionar el trabajo de mi predecesor en este sillón q  —experto en lexicología y dialectología, \nfilólogo, ensayista y crítico— con las futuras tareas de su sucesora —investigadora en inteligencia artificial, ontolo-gías y lenguaje—. A ello vengo, al menos, con la misma ilusión, entusiasmo y responsabilidad que Gregorio Salva-dor demostró.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   1534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   15 11/5/23   12:0411/5/23   12:0416Inteligencia artificial y lengua española\nErróneamente se piensa que muchos de los conceptos de \ninteligencia artificial son relativamente modernos. \u00a1Nada más lejos de la realidad! Desde el comienzo de los tiempos, el ser humano ha creado artilugios, herramientas y máqui-nas que emulan o simulan las actividades físicas o mecáni-cas, y le asisten en ellas; y, desde mediados del siglo xx, han aparecido máquinas y dispositivos digitales que emplean la electrónica, los semiconductores y las tecnologías de la in-formación y comunicaciones para ayudar al hombre en el procesado de la información y en las actividades cognitivas. Sin embargo, bien puede decirse que actualmente nos encon\n tramos ante una nueva revolución tecnológica sin \npr\necedentes, cuyo horizonte sobrepasa la digitalización y \nque cambiará drásticamente la forma en que vivimos, tra-bajamos, aprendemos y nos relacionamos. \nEn esta revolución, la inteligencia artificial está acompa-\nñada por otras tecnologías habilitadoras, que ya se han in-corporado en muchas industrias y sectores de la actividad económica, en las Administraciones p\u00fablicas, en la investi-gación y en las actividades cotidianas que realizamos en el tiempo de ocio y en el hogar. Al hablar de tecnologías ha-bilitadoras, me refiero a tecnologías como la Web y la Web 2.0, que suministra datos multilingües y multimedia en la transmisión de la información; la Internet de las cosas, que proporciona datos en tiempo real en cualquier momento y lugar; el almacenamiento masivo de datos y la computa-ción en la nube; las cadenas de bloques, tan empleadas en los ámbitos económico y financiero; y el metaverso, que \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   1634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   16 11/5/23   12:0411/5/23   12:0417nos llevará a una realidad híbrida, entre lo físico y lo vir-\ntual, y en el que disfrutaremos de la realidad virtual y de la realidad aumentada. Al mismo tiempo, otras tecnologías se encuentran en un estado más incipiente, pues no han sido a\u00fan ampliamente adoptadas; así, la computación cuántica, la computación neuromórfica, la neurotecnología o los chips implantados en humanos para incrementar sus capa-cidades físicas y cognitivas están contribuyendo también a esta revolución tecnológica.\nNo cabe duda de que cada vez con mayor ahínco los \nseres humanos tratan de dotar a las máquinas, artefactos y dispositivos de capacidades propias de las personas, como pensar, razonar, hablar, entender, escribir, traducir y dialo-gar. La inteligencia artificial es la tecnología habilitadora que dota a sistemas y dispositivos digitales de capacidades cognitivas, tales como razonar, emplear el lenguaje, ya sea para traducir entre idiomas, mantener una conversación oral o escrita con una máquina para resolver una tarea, cla-sificar documentos, crear imágenes a partir de descripcio-nes en lenguaje natural, reconocer objetos en fotografías e, incluso, aprender de nuestra conducta o costumbres, y de esa forma recomendar la compra de un producto o selec-cionar la siguiente canción, por mencionar solo algunas de esas m\u00faltiples posibilidades. La tecnología evoluciona tan rápidamente que, cuando ustedes lean este discurso, nue-vas invenciones estarán en nuestros bolsillos, hogares y lu-gares de trabajo, y, en lugar de hablar de DALL-E2, Chat-GPT, Bard, o MusicLM, tendremos otros \u00abjuguetes\u00bb tecnológicos que usarán la inteligencia artificial y que abri-rán portadas de periódicos.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   1734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   17 11/5/23   12:0411/5/23   12:0418Por ello, debemos comenzar preguntándonos: ¿qué es in-\nteligencia?; ¿puede medirse objetivamente?; ¿pueden repro-\nducirse fuera de su sustrato natural orgánico las funciones del cerebro humano?; ¿es posible la singularidad tecnológica?; ¿a qué distancia se está de conseguirla?; ¿entienden los modelos del lenguaje lo que escriben?; ¿razonan los modelos de len-guaje con el texto que escriben?; ¿qué impacto ambiental tie-ne la\u00a0inteligencia artificial?; ¿debe prevalecer la regulación o la ética?, y ¿es necesario realizar una parada tecnológica?\nPara responder a estas y otras preguntas, he dividido esta \nexposición en dos partes. En primer lugar, rescato las contri-buciones más relevantes de la inteligencia artificial y del pro-cesamiento del lenguaje natural, y describo los motivos por los que la inteligencia artificial es la tecnología del momento, a pesar de que sus orígenes datan de 1956. En la segunda parte, me adentro en el futuro para proponer con modestia cuatro retos, y un decálogo de propuestas que profundizan en la ne-cesidad de que la Academia ponga a disposición de la inteli-gencia artificial sus materiales y de que la inteligencia artificial ayude a mejorar los métodos de trabajo y las herramientas de\u00a0la Academia. Todo ello, sin perder de vista las actividades de observación, supervisión y verificación del uso de la lengua en la esfera digital y la necesidad de crear un ecosistema de inno-vación abierta de la economía de la lengua en español.\n1. Definición de inteligencia artificial\nComenzaré con un dato anecdótico que refuerza la impor-\ntancia social del tema que nos ocupa. Recientemente, la \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   1834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   18 11/5/23   12:0411/5/23   12:0419Fundación del Español Urgente (FundéuRAE), promovida \npor la Real Academia Española y la Agencia EFE, otorgó el título de palabra del año 2022\n 8 a la expresión compleja \ninteligencia artificial, que es una denominación com\u00fan y, por lo tanto, debe escribirse enteramente con min\u00fascula, si bien la sigla IA ha de escribirse con may\u00fascula.\nPero, antes de entrar en materia, es obligado revisar \ncómo define el diccionario las unidades léxicas inteligencia, artificial e inteligencia artificial, expresiones que han evolu-\ncionado en el mapa de diccionarios de la lengua española desde 1780 hasta la versión 23.6 del DLE, del año 2022. \nActualmente, el diccionario presenta ocho acepciones de la palabra inteligencia. Tomo como relevantes en este contex-\nto la primera, que se refiere a la \u00abcapacidad de entender o comprender\u00bb; la segunda, \u00abcapacidad de resolver proble-mas\u00bb; la tercera, \u00abconocimiento, comprensión, acto de en-tender; y la quinta, \u00abhabilidad, destreza y experiencia\u00bb. Además, el diccionario proporciona cuatro significados para la palabra artificial, de los cuales son pertinentes los tres primeros: \u00abhecho por mano o arte del hombre\u00bb; \u00abno natural, falso\u00bb; y \u00abproducido por el ingenio humano\u00bb. Fi-nalmente, la locución inteligencia artificial se incorpora al diccionario en 1992, definida como \u00abla atribuida a las má-quinas capaces de hacer operaciones propias de los seres inteligentes\u00bb; la definición se actualiza en 2001 para referir-se al \u00abdesarrollo y utilización de ordenadores con los que se \n8 FundéuRAE (2022): Inteligencia artificial es la expresión del 2022 \npara la FundéuRAE.\n https://www.fundeu.es/recomendacion/inteligen-\ncia-artificial-es-la-expresion-del-2022-para-la-fundeurae/ \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   1934439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   19 11/5/23   12:0411/5/23   12:0420intenta reproducir los procesos de la inteligencia humana\u00bb; \ny, actualmente, la inteligencia artificial se entiende como la \u00abdisciplina científica que se ocupa de crear programas in-formáticos que ejecutan operaciones comparables a las que realiza la mente humana, como el aprendizaje o el razona-miento lógico\u00bb.\nHagamos un breve recorrido ahora por la historia de la \ninteligencia artificial. Debemos retrotraernos a finales de agosto de 1955, cuando los investigadores John McCarthy (del Dartmouth College), Marvin Minsky (de la Universi-dad de Harvard), Nathaniel Rochester (de IBM) y Claude Shannon (de los laboratorios Bell) propusieron que diez investigadores llevaran a cabo, durante un periodo de dos meses en el verano de 1956, un estudio sobre inteligencia artificial en el Dartmouth College. Plantearon demostrar la siguiente conjetura\n 9: \u00abCualquier aspecto del aprendizaje o \ncualquier otra característica de la inteligencia puede, en principio, estar tan precisamente descrita que se puede construir una máquina para simularla. Se intentará descu-brir cómo hacer que las máquinas utilicen el lenguaje, for-men abstracciones y conceptos, resuelvan tipos de proble-mas hasta ahora reservados a los humanos y se mejoren a sí mismas\u00bb. La conferencia de Dartmouth se considera el evento fundacional de este campo de estudio, que surge como una disciplina científica de la informática, aunque \n9 J. McCarthy, M. L. Minsky, N. Rochester and C. E. Shannon \n(1955): A Proposal for the D\nartmouth Summer Research Project on Arti-\nficial Intelligence. http://www-formal.stanford.edu/jmc/history/dart-\nmouth/dartmouth.html \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   2034439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   20 11/5/23   12:0411/5/23   12:0421comparte métodos y técnicas con otras ciencias, como las \n matemáticas, la estadística, la psicología, la lingüística, la \nfilosofía y la economía, entre otras. Cuatro de los inv\nestiga-\ndores participantes obtuvieron con posterioridad el Premio Alan T uring, conocido como el Premio Nobel de la Infor-mática: Marvin Minsky en 1969, John McCarthy en 1971 y, en 1975, Allen Newell y Herbert Simon, quien también fue premio nobel de economía en 1978.\nDurante el siglo xx, el desarrollo tecnológico de la inte-\nligencia artificial estuvo limitado por la potencia de los pro-cesadores, por el espacio de almacenamiento de los orde\n-\nnadores y por la escasez de datos disponibles. Durante estos años, \nlos investigadores se afanaron en el diseño de algorit-\nmos y modelos ingeniosos que pudieran ejecutarse en in-fraestructuras muy limitadas, para así resolver problemas complejos y evitar la explosión combinatoria del espacio de b\u00fasqueda. A modo de ejemplo, el primer asistente virtual no se llamaba Siri, Alexa o ChatGPT; se llamaba ELIZA\n 10 \ny fue construido en 1964. Eliza se ejecutó en una de las primeras máquinas con procesador, el IBM 3090\n 11, que te-\nnía 32 kilobytes de memoria RAM, un procesador de 32 bits, velocidad de 0,5 megahercios, y realizaba 100\n 000 \n10 J. Weizenbaum (1966): \u00abELIZA. A Computer Program for the \nStudy of Natural Language Communication betw\neen Man and Ma-\nchine\u00bb. Communications of the ACM, volume 9, number 1, 36-45. ht-\ntps://dl.acm.org/doi/10.1145/365153.365168 \n11 IBM (1987): IBM 3090 Processor Complex M odels 200E and \n400E https://www.ibm.com/common/ssi/ShowDoc.wss?docURL=/\ncommon/ssi/rep_ca/3/897/ENUS187-003/index.html \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   2134439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   21 11/5/23   12:0411/5/23   12:0422operaciones en coma flotante por segundo; es decir, 100 \nkiloflops por segundo. El precio del IBM 3090 era de, aproximadamente, un millón de dólares, lo que equival-dría, aproximadamente, a unos 22 millones de dólares de hoy. En 2023, se puede adquirir por unos 200 euros un teléfono móvil de gama media con un procesador de seis n\u00facleos, 64 bits, una velocidad de 2,63 gigahercios, 3 gi-gabytes de memoria RAM y un rendimiento de 786 giga-flops. Por tanto, cualquier teléfono móvil de precio reduci-do tiene una velocidad 5300 veces mayor y una memoria 100\n 000 veces mayor, y r\nealiza 7.8 millones de operaciones \nen coma flotante por segundo más que el IBM3090. Pre-viamente, en 1962, el IBM 7090, que procesaba datos a una velocidad de 24\n 000 operaciones por segundo \ny ocupa-\nba una sala, fue el ordenador de la NASA que realizó los cálculos matemáticos que posibilitaron a John Glenn orbi-tar la Tierra. Katherine Johnson trabajaba en la División Segregada de Cálculo del Ala Oeste del Centro de Investi-gación Langley. Ella fue quien verificó que los cálculos rea-lizados por el IBM 7090 eran correctos. Como muestra la película biográfica dirigida por Theodore Melfi Hidden fi-\ngures (Figuras ocultas), Johnson formó parte del grupo de personas, conocidas como computadoras, que trabajaban en la NASA realizando cálculos matemáticos antes de la incorporación de los ordenadores. Comparto con ustedes que, durante la elaboración de este discurso, he vuelto a repasar muchos de los artículos científicos publicados en-tonces, y realmente he disfrutado leyéndolos. \u00a1Qué altura intelectual la de aquellos investigadores que, con infraes-tructuras tan limitadas, llegaron tan lejos! \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   2234439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   22 11/5/23   12:0411/5/23   12:0423El siglo xxi ha traído nuevas infraestructuras digitales con \nimportantes avances de ensueño en los procesadores, en la ca-\npacidad de almacenamiento y de comunicación de los ordena-dores y en el desarrollo de multitud de máquinas y dispositi-vos interconectados, como teléfonos móviles, tabletas, robots, coches, tractores, drones, electrodomésticos, equipos de diag-nóstico médico o cadenas de producción robotizadas, por mencionar solo algunos. Incluso se llega a ver al cuerpo humano como un \u00abdispositivo\u00bb más en el que implantar chips con fines terapéuticos o de monitorización, o para mejorar y aumentar sus capacidades y transformar así la condición humana.\nLos avances, como ustedes saben, requieren nuevas defi-\nniciones para términos ya existentes. Así, recientemente, la Comisión Europea ha definido los sistemas de inteligencia artificial, en el documento de guías éticas para una inteligen-cia artificial fiable\n 12, como \u00absistemas de software (y en ocasio-\nnes hardware) diseñados por humanos que, ante un objetivo \ncomplejo, act\u00faan en el mundo físico o digital percibiendo su entorno a través de la adquisición e interpretación de datos estructurados, semiestructurados o nada estructurados, razo-nando con el conocimiento, procesando la información de-rivada de estos datos y decidiendo las mejores acciones a rea-lizar para alcanzar el objetivo. Los sistemas de IA utilizan modelos simbólicos para razonar, aprender mediante mode-los numéricos y adaptar su comportamiento analizando cómo se ve afectado el entorno por sus decisiones previas\u00bb.\n12 European Commission (2019): Ethics Guidelines for Trustworthy \nAI. https://ec.europa.eu/digital-single-mar\nket/en/news/ethics-guideli-\nnes-trustworthy-ai \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   2334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   23 11/5/23   12:0411/5/23   12:0424Ahora bien, dependiendo de la complejidad de la tarea \nque la inteligencia artificial resuelve, la literatura distingue \ndos tipos de inteligencia artificial: general y específica. La inteligencia artificial general es aquella que puede imitar por completo todas las capacidades relacionadas con la \n inteligencia, como la r\nesolución de tareas complicadas y \nheterogéneas, la planificación, el aprendizaje, el razona-miento o la capacidad de abstraer y generalizar. Los inves-tigadores Dahane, Lau y Houder plantean, en un artículo publicado en la prestigiosa revista Science, que, si alg\u00fan día la máquina pudiera tener consciencia y ser consciente de sí misma, por ser capaz de monitorizarse y obtener infor\n-\nmación de este proceso de introspección, podría detectar sus debilidades y corregirlas, llegando así a la superinteli\n-\ngencia. Este es el término que se emplea para hacer referen-cia a la IA que supera con creces las capacidades humanas. En nuestros días, no existe esta superinteligencia artificial que sea capaz de resolver todas las tareas, simples o comple-jas, que se le planteen; al contrario, existen muchas inteli-gencias artificiales específicas que resuelven actividades concretas de forma muy satisfactoria. La tecnología avanza hacia la agregación de esas inteligencias artificiales para re-solver situaciones cada vez más complejas y generalizar las soluciones a otros tipos de escenarios; por ejemplo, un co-che autónomo capaz de conducir prácticamente sin inter-vención humana y, mientras conduce, realizar otras activi-dades, como calcular el camino más rápido entre dos puntos y reservar una mesa en un restaurante del gusto de los ocupantes. Esta inteligencia artificial especializada pue-de crecer a medida que interact\u00faa con otras inteligencias \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   2434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   24 11/5/23   12:0411/5/23   12:0425artificiales también especializadas, aproximándose con ello, \nmediante un proceso de agregación, a una inteligencia arti-ficial general. Thomas Cambpell\n 13 muestra en su artículo \n\u00abAnalogía de la escala de Kardashev: pensamiento a largo plazo sobre la inteligencia artificial\u00bb, publicado en 2017, cómo la agregación de inteligencias artificiales puede evo-lucionar en el futuro hacia una superinteligencia con con-ciencia, lo cual nos lleva a plantear si la inteligencia artifi-cial es motivo de preocupación o de optimismo. \n2. Máquinas con capacidades cognitivas\nComo ya he mencionado, el anhelo del siglo xxi de cons-\ntruir máquinas con capacidades cognitivas no es nuevo. En \nel siglo xix, los inventores británicos Charles Babbage y Ada Byron, y en el siglo xx Alan T uring y el español Leo-nardo Torres Quevedo, ya diseñaron artefactos mecánicos y eléctricos para realizar actividades cognitivas. \n2.1. \n Máquinas precursoras de tipo mecánico y \nelectr\nomecánico\nCharles Babbage (1791-1871) diseñó la máquina analítica, de naturaleza mecánica, capaz de elaborar de forma automá-\n13 T. Campbell (2017): Kardashev Scale Analogy: Long-T erm Thin-\nking about Artificial I\nntelligence. FutureGrasp, LCC. https://irp-cdn.\nmultiscreensite.com/9297f8c7/files/uploaded/The%20Kardas-\nhev%20Scale-An%20AI%20Analogy_FINAL.pdf \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   2534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   25 11/5/23   12:0411/5/23   12:0426tica las tablas de n\u00fameros —similares a las tablas de logarit-\nmos—, que en aquella época eran instrumentos que facilita-ban los cálculos numéricos; estas tablas eran usadas por personas denominadas \u00abcomputadoras\u00bb. El diseño de esa má-quina analítica incluía la mayoría de las partes lógicas de una computadora y un \u00ablenguaje de programación\u00bb que tomaba decisiones y ejecutaba bucles. A Ada Byron (1815-1852), también conocida como Ada Lovelace, que fue colaboradora de Babbage, se la reconoce como la primera programadora de la historia, al haber publicado el primer algoritmo destinado a ser procesado por una máquina. Más tarde, en 1910, el hijo de Babbage, Henry P . Babbage, construye una parte del dis-positivo para calcular una lista de m\u00faltiplos de pi.\nAprovecho este discurso para poner de relieve al acadé-\nmico Leonardo Torres Quevedo (1852\n-1936), precursor en \nEspaña de la automática y de la informática. Los inventos del ingenier\no Torres Quevedo no aparecen entre los escritos \nmás citados, pese a no ser un inventor desconocido. La tra-dición en lengua inglesa de la ingeniería y la técnica es, a mi entender, el principal motivo por el que se ignoraron los logros del ingeniero español. Sin embargo, es justo recordar que Torres Quevedo fue precursor de las máquinas mecáni-cas y electromecánicas de cálculo. Sus logros en computa-ción e inteligencia artificial no tuvieron parangón. Ejem-plos relevantes de sus invenciones son, entre otros, una máquina analógica que resolvía mecánicamente las raíces de un polinomio de grado 8, inventada en 1895 y fabricada en 1910, o la primera calculadora con consola y el arit\n-\nmómetro electromecánico, presentado en 1917. R\nesultan \ntambién destacables los dos prototipos de\u00a0ajedrecistas, de-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   2634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   26 11/5/23   12:0411/5/23   12:0427sarrollados en 1912 y 1920, máquinas  electromecánicas \ncapaces de responder a situaciones particular\nes del juego. \nPor \u00faltimo, me referiré al telekino, un autómata que ejecu-\nta órdenes transmitidas a distancia mediante telegrafía sin hilos para gobernar un dispositivo \n mecánico, que fue pre-\nsentado en la \nAcademia de Ciencias de París en 1903. En \nresumen, Torres Quevedo fue un precursor exitoso en la programación de máquinas, en el \n cálculo y en la creación \nde los primeros autómatas funcionales. T\nodos ellos se cus-\ntodian y muestran en el Museo Torres Quevedo 14, ubicado \nen la Escuela Técnica Superior de Ingenieros de Caminos, Canales y Puertos de la Universidad Politécnica de Madrid.\nPero fue Alan T uring (1912-1954), conocido por desci-\nfrar los códigos nazis de la máquina Enigma durante la Se-gunda Guerra Mundial, quien planteó en 1950 la pregun-ta\n 15: \u00ab¿pueden pensar las máquinas?\u00bb. En su artículo, T uring \nlimita el concepto de \u00abmáquinas pensantes\u00bb a las computa-doras digitales formadas por un almacén de información, por unidades de ejecución que llevan a cabo las operaciones individuales, y el control que cumple con las\u00a0instrucciones en el orden establecido, aunque puedan existir operaciones sustituibles por otras hasta que alguna condición sea satisfe-cha. Es en este contexto en donde T uring propone el juego de la imitación, que da lugar al conocido test de T uring, que \n14 Universidad Politécnica de Madrid: Museo Torres Quevedo. \n https://www.upm.es/UPM/MuseosUPM/MuseoT\norresQuevedo \n15 A. T uring (1950): \u00abComputing Machinery and Intelligence\u00bb. \nMind, 59, 433-460. https://www\n.jstor.org/stable/2251299?origin=JS-\nTOR-pdf \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   2734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   27 11/5/23   12:0411/5/23   12:0428consiste en plantear una pregunta a un humano y a una \nmáquina. Si el interrogador no es capaz de diferenciar la respuesta dada por el humano y la máquina, el sistema pasa con éxito el test de T uring, lo que supone que la máquina exhibe un comportamiento inteligente. T uring finaliza su artículo proponiendo que las máquinas finalmente compi-tan con las personas en actividades intelectuales como el juego del ajedrez o la comprensión y el habla en inglés. Pero A. T uring ya había introducido ideas y conceptos de inteli -\ngencia artificial en \u00abMaquinaria Inteligente\u00bb\n 16, un artículo \nanterior —a pesar de que no se publicó hasta 1967— con-siderado el primer manifiesto de\u00a0la inteligencia artificial. \n2.2. El lenguaje de los ordenadores \nFue durante la Segunda Guerra Mundial cuando se constru-\nyeron los primeros ordenadores digitales: el ABC y el \n16 A. T uring redactó \u00abIntelligent Machinery\u00bb en 1947; fue publica-\ndo en 1967 y está disponible en https://weightagnostic.github.io/pa\n-\npers/turing1948.pdf. En el capítulo tercero del libro de Pamela McCor-\nduck (2003) Machines Who Think: A Personal Inquiry into the History and Prospects of Artificial Intelligence, de la editorial A K Peters, la autora presta especial atención a A. T uring y le identifica como responsable de muchos de los primeros trabajos en inteligencia artificial. En dicho ca-pítulo también habla de John von Neumann, quien sostuvo la opinión, opuesta a la de T uring, de que las computadoras nunca serían capaces de pensar. J. Von Neumann (1903-1957) es conocido por la arquitectura de computadoras digitales que lleva su nombre, y que consta de un sis-tema de entrada/salida, una memoria principal, una memoria externa, una unidad de procesamiento y una unidad de control. \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   2834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   28 11/5/23   12:0411/5/23   12:0429ENIAC. Los pioneros de la tecnología informática adopta-\nron un sistema de codificación para traducir los n\u00fameros y las letras, las palabras, las frases, los párrafos y cualquier texto alfanumérico a un código binario que los ordenadores pu-dieran manejar para procesar la información. Este código binario, como su nombre indica, solo dispone de dos dígi-tos: el cero (0) y el uno (1). El código binario tiene sus orí-genes en la aritmética binaria, cuya formulación más cono-cida es la de Gottfried Leibniz\n 17,  18 (1646-1716), que incluye \nlas operaciones de adición, sustracción, producto y división. \nPara profundizar en el concepto de las computadoras \ndigitales y posteriormente discernir si las máquinas pueden pensar o mostrar habilidades cognitivas, es necesario re-flexionar sobre el uso del sistema binario, también conoci-do como codificación en base dos, para representar la infor-mación en la computadora. Frente al sistema binario, el sistema decimal, que es el que normalmente usamos, está compuesto por 10 dígitos (0, 1, 2, 3, 4, 5, 6, 7, 8, 9). Los dígitos del sistema binario se pueden combinar para repre-sentar todos los valores numéricos posibles en base 10. Por ejemplo, en una secuencia de cinco dígitos en el sistema binario, el 00001 se corresponde en el sistema decimal con \n17 B. C. Look (2013): \u00abGottfried Wilhelm Leibniz\u00bb. Stanford En-\ncyclopedia of Philosophy. https://plato.stanford.edu/entries/leibniz/\n18 Predecesores fueron el matemático hind\u00fa Pingala (https://es.ma-\nthigon.org/timeline/pingala); Francis Bacon, que en 1605 hiz\no referen-\ncia a un sistema para reducir las letras del alfabeto a secuencias de n\u00fa-\nmeros binarios; y Juan Caramuel Lobkowitz, autor de Mathesis bíceps  \n(1670), que también describe el sistema binario años antes que Leibniz.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   2934439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   29 11/5/23   12:0411/5/23   12:0430el n\u00famero 1, la secuencia 00010 con el n\u00famero 2, la se-\ncuencia 00100 con el n\u00famero 4, la secuencia 01000 con el n\u00famero 8, y la secuencia 10000 con el n\u00famero 16. La arit-mética de Leibniz propuesta en el siglo xvii permite reali-zar operaciones en base 2 de tal forma que producen su equivalente en el sistema decimal. \nDado que las computadoras digitales se sirven del siste-\nma binario y los seres humanos nos valemos del lenguaje para formar frases que contienen palabras, n\u00fameros y sím-bolos de un alfabeto, es necesario transformar cualquier tex-to alfanumérico en secuencias de ceros y unos, de forma que un computador pueda almacenarlo, procesarlo y extraerlo. En 1963 se aprueba el código ASCII, que es el primer están-dar para el intercambio de información con 8 bits y al cual seguirán otros sistemas de codificación. El formato estándar de nuestros días se llama UNICODE, y pauta los códigos en que los dispositivos electrónicos han de almacenar, inter-cambiar y visualizar datos y textos en multitud de idiomas. Permite numerosos alfabetos, distingue entre las min\u00fasculas y may\u00fasculas del alfabeto latino, los n\u00fameros del 0 al 9, el espacio en blanco para separar palabras, los caracteres de puntuación, los símbolos matemáticos, los símbolos de mo-nedas y los emoticones, entre otros. \nA modo de ejemplo, veamos cómo se representa la letra \nq. En el código ASCII, la letra q min\u00fascula se representa mediante el valor en base decimal 113 y la secuencia de ocho dígitos 01110001; la letra Q may\u00fascula se representa como 01010001, lo que corresponde en base decimal al n\u00famero 81; y la letra u min\u00fascula se representa como 01110101, cuya correspondencia en base decimal es el n\u00fa-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   3034439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   30 11/5/23   12:0411/5/23   12:0431mero 117. De esta forma, el código binario que representa \nla letra q min\u00fascula seguida de su vocal de escolta se repre-sentaría como 01110001 01110101, lo que se corresponde con la secuencia de dos n\u00fameros decimales: 113\n 117. Así, \ncualquier texto se puede transformar en un conjunto de ceros y unos. Si tomamos como ejemplo el comienz\no del \nQuijote, el texto \u00abEn un lugar de la Mancha, de cuyo \n nombre no quiero acordarme\u00bb se r\nepresenta, con el código \n ASCII en binario, de la siguiente manera:\n01000101 01101110 00100000 01110101 01101110 \n00100000 01101100 01110101 01100111 01100001 01110010 00100000 01100100 01100101 00100000 01101100 01100001 00100000 01001101 01100001 01101110 01100011 01101000 01100001 00101100 00100000 01100100 01100101 00100000 01100011 01110101 01111001 01101111 00100000 01101110 01101111 01101101 01100010 01110010 01100101 00100000 01101110 01101111 00100000 01110001 01110101 01101001 01100101 01110010 01101111 00100000 01100001 01100011 01101111 01110010 01100100 01100001 01110010 01101101 01100101 00101100   00100000\nEn decimal como\n69 110 32 117 110 32 108 117 103 97 114 32 100 101 32 108 97 32 77 97 110 99 104 97 44 32 100 101 32 99 117 121 111 32 110 111 109 98 114 101 32 110 111 32 113 117 105 101 114 111 32 97 99 111 114 100 97 114 109 101\nY en Unicode como la secuencia \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   3134439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   31 11/5/23   12:0411/5/23   12:0432\\u0045\\u006e\\u0020\\u0075\\u006e\\u0020\\u006c\\u0075\\\nu0067\\u0061\\u0072\\u0020\\u0064\\u0065\\u0020\\u006c\\u0061\\u0020\\u004d\\u0061\\u006e\\u0063\\u0068\\u0061\,\\u0020\\u0064\\u0065\\u0020\\u0063\\u0075\\u0079\\u006f\\u0020\\u006e\\u006f\\u006d\\u0062\\u0072\\u0065\\u0020\\u006e\\u006f\\u0020\\u0071\\u0075\\u0069\\u0065\\u0072\\u006f\\u0020\\u0061\\u0063\\u006f\\u0072\\u0064\\u0061\\u0072\\u006d\\u0065 \nPero, partiendo de los bits clásicos, uno puede llegar a los \nbits cuánticos. Cuando los ceros y los unos se representan en \nsistemas cuánticos, los bits se convierten en c\u00fabits y pasan de poder tener solo dos valores a tener una infinidad de ellos, fruto de la combinación lineal de los estados que representan el cero y los estados que representan el uno. Las propiedades del mundo cuántico hacen que los nuevos sistemas de proce-samiento de información basados en c\u00fabits tengan capacida-des imposibles de igualar en el mundo clásico\n 19. \n2.3. ¿Cómo dotar de capacidades cognitivas a las máquinas?\nAhora bien, como cualquier texto alfanumérico se transfor-\nma a código binario, procede preguntar: ¿cómo se dota de capacidades cognitivas a las máquinas si su lenguaje se basa en ceros y unos? Existen dos enfoques: el primero se funda-menta en el álgebra de Boole y en la lógica; el segundo, en cambio, se basa en la cibernética. Aprender y razonar son los dos grandes pilares de la inteligencia artificial. El apren-\n19 M. Nielsen and I. Chuang (2000): Quantum Computation and \nQuantum Infor\nmation. Cambridge: Cambridge University Press.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   3234439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   32 11/5/23   12:0411/5/23   12:0433dizaje se realiza principalmente utilizando técnicas estadís-\nticas mientras que el razonamiento se realiza con palabras y las leyes de la lógica. Estos dos enfoques, bien diferencia-dos, aunque complementarios, han dado lugar a la inteli-gencia artificial subsimbólica, que es la que aprende, y a la inteligencia artificial simbólica que es la que razona y expli-ca los resultados alcanzados.\nVeamos en primer lugar el álgebra de Boole\n 20, que fue \npropuesta en 1854 por el matemático y lógico George Boole (1815-1864), conocido como uno de los padres de la ciencia de la computación. Esta álgebra está formada por un conjunto de variables que solo pueden tomar dos valores, uno o cero, para representar los estados abierto o cerrado, encendido o apagado, todo o nada, cierto o falso. El álgebra booleana se define por tres operaciones básicas —que representan el complementario, la suma y el pro-ducto de las variables y que, en lógica, se corresponden con la negación, la disyunción y la conjunción, respectivamen-te— y por un conjunto de axiomas que permiten deducir y demostrar teoremas. Con las variables y las tres operacio-nes se pueden construir fórmulas lógicas, que son interpre-tadas al asignar los valores cero o uno a las variables. Ya a principios del siglo xx, entre 1910 y 1913, Alfred White-head y Bertrand Russell culminaron el trabajo de Boole en Principia Mathematica\n 21, que es la obra emblemática de la \n20 G. Boole (1854): An Inv estigation of the Laws of Thought on \nWhich are Founded the Mathematical Theories of Logic and Probabilities.\n21 B. Russell (1903): The Principles of Mathematics. Cambridge: \nCambridge University P\nress. \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   3334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   33 11/5/23   12:0411/5/23   12:0434lógica formal, pues define nociones matemáticas en térmi-\nnos de nociones lógicas y deriva principios matemáticos, así definidos, a partir de principios lógicos. La parte de la inteligencia artificial que está basada en la lógica recibe el nombre de inteligencia artificial simbólica. \nUnos años más tarde, en 1936, Claude Shannon (1916-\n2001) se sirvió del álgebra de Boole para describir cómo funcionan los circuitos eléctricos usando símbolos que re-presentan el estado y las conexiones de los circuitos y su correspondiente interpretación en el cálculo proposicional. Con los símbolos se modeliza un sistema de ecuaciones cu-yas variables representan los relés y los conmutadores del circuito. Shannon desarrolló un sistema de cálculo mate-mático para manejar las ecuaciones de forma similar a los algoritmos algebraicos. También demostró que el cálculo era exactamente análogo al cálculo proposicional empleado en el estudio simbólico de la lógica y que su método permi-tía diseñar circuitos sencillos y equivalentes. En palabras de Borrajo\n 22 y otros investigadores, Shannon estableció la co-\nnexión entre la expresión formal de la lógica y un medio para automatizar esa lógica con circuitos eléctricos. De acuerdo con McCorduck, no es descabellado plantearse que, si las leyes del pensamiento pueden expresar la con-ducta de circuitos electrónicos, los circuitos electrónicos podrían expresar el pensamiento. \nComo alternativa al álgebra de Boole surge la ciberné-\ntica, término acuñado por Wiener y que el diccionario de \n22 D. Borrajo, N. Juristo, C. Montes y J. Paz os (1993): Inteligencia \nartificial: métodos y técnicas. Ed. CEURA.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   3434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   34 11/5/23   12:0411/5/23   12:0435la Academia define como la \u00abciencia que estudia las analo-\ngías entre los sistemas de control y comunicación de los seres vivos y los de las máquinas\u00bb. En 1943, en el ensayo \u00abComportamiento, utilidad y teleología\u00bb\n 23, los investi -\ngadores Rosenblueth, W\niener y Bigelow estudiaron las \nanalogías y diferencias en el comportamiento, utilidad y teleología de dispositivos electrónicos y sistemas biológi-cos. Ese mismo año, McCulloch y Pitts\n 24, fundamentán-\ndose en el carácter de \u00abtodo o nada\u00bb de la actividad nervio-sa, pretendían que las computadoras pudieran imitar la cognición humana mediante lo que se conoce actualmen-te como células de McCulloch y Pitts. Este tipo de células (actualmente neuronas artificiales) recibían varias entradas con sinapsis excitadoras e inhibidoras. Este modelo no se basó en el conocimiento biológico detallado de la célula, sino en la correspondencia entre el comportamiento acti-vo-inactivo de la neurona y su similitud con el comporta-miento encendido-apagado de los interruptores, que su-puestamente sería suficiente para diseñar un modelo del sistema neuronal y así proporcionar un comportamiento inteligente basado en el código binario. Este modelo de célula artificial dio lugar a la rama de la inteligencia arti\n-\n23 A. Rosenblueth, N. Wiener and J. Bigelow (1943): \u00abBehavior, Pur-\npose and T\neleology\u00bb. Philosophy of Science, 10, 18-24. https://www.cam-\nbridge.org/core/journals/philosophy-of-science/article/abs/behavior-pur-\npose-and-teleology/73ACBBEC616CE78767088694F357D57B\n24 W. S. McCulloch and W. Pitts (1943): \u00abA Logical Calculus of \nthe Ideas Immanent in N\nervous Activity\u00bb. Bulletin of Mathematical \nBiophysics, 5, 115-133.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   3534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   35 11/5/23   12:0411/5/23   12:0436ficial denominada conexionista y puede considerarse como \nel precursor de las redes de neuronas artificiales y del aprendizaje profundo, que forma parte de las técnicas de la inteligencia artificial subsimbólica. \n3. \n Inteligencia artificial subsimbólica y simbólica\nPara entender la evolución y el impacto económico, social, \nmedioambiental, ético y legal de la inteligencia ar\ntificial en \nsus dos ramas, subsimbólica y simbólica, es conveniente recordar que esa evolución está ligada a los avances del hardware, al volumen de los datos y a los modelos y algorit-mos que representan el conocimiento y habilitan el razona-miento y el aprendizaje. Estos dos enfoques, bien diferen-ciados aunque complementarios, se distinguen no solo en la forma en la que representan los datos, la información y el conocimiento, sino también en el modo de usarlos.\nEl enfoque subsimbólico se centra en el aprendizaje auto-\nmático de modelos numéricos a partir de grandes cantidades de datos. Permite, entre otras tareas, realizar predicciones y resolver problemas de clasificación, mejorando en algunos casos el rendimiento humano en tareas de reconocimiento de imágenes, audio o procesamiento del lenguaje natural. A este enfoque pertenecen las técnicas que utilizan la estadísti-ca, las redes de neuronas artificiales, los modelos computa-cionales bioinspirados en la estructura y el funcionamiento del cerebro que emplean algoritmos, principalmente ba\n sados \nen el cálculo numérico, para intentar reproducir el pr\noceso \nbiológico del aprendizaje. Las redes de neuronas con m\u00falti-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   3634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   36 11/5/23   12:0411/5/23   12:0437ples capas de procesamiento aprenden a realizar una tarea a \npartir de cantidades ingentes de datos. Cuando se aplican al procesamiento del lenguaje natural, estos sistemas de apren-dizaje profundo no manejan palabras, sino representaciones numéricas de la información, como valores escalares, vecto-res o matrices multidimensionales. \nPor otro lado, las técnicas del enfoque simbólico em-\nplean símbolos alfanuméricos —es decir, palabras— y re-glas lógicas para representar los datos y los conocimientos que los humanos tienen acerca de un dominio para, poste-riormente, generar nuevo conocimiento mediante procesos de inferencia o de razonamiento. Bajo este enfoque, los modelos computacionales se valen de técnicas basadas en lógica formal, heurísticas, ontologías o grafos de conoci-mientos. Los motores de inferencia realizan normalmente razonamiento deductivo y aplican heurísticas para limitar las opciones de b\u00fasqueda y reducir la explosión combina-toria, así como para extraer nuevos datos y conocimiento a partir de los existentes.\nComo ustedes bien conocen, la palabra ontología proce-\nde del griego , , que significa 'ente' y 'ser', y de \n-, que significa 'tratado, estudio, ciencia'. La ontolo-gía es la rama de la metafísica que trata del ser en general y de sus propiedades trascendentales. De la misma forma que un diccionario recoge las palabras de una lengua acompa-ñadas de su definición, equivalencia o explicación en un lenguaje natural, que incluye relaciones semánticas con otras palabras, las ontologías en informática recogen los conceptos de un dominio dado, junto con sus propiedades y sus relaciones con otros conceptos, de forma que pueda \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   3734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   37 11/5/23   12:0411/5/23   12:0438entenderlos tanto una persona como un sistema de inteli-\ngencia artificial. \nVeamos todo ello con un ejemplo: el autor de una obra \nparticipa en ella como escritor, compositor, retratista o rea-lizador audiovisual. La creación intelectual, que es la obra, puede ser una novela, una canción, un retrato o un vídeo. Para describir al autor, utilizamos propiedades como el n\u00fa-mero de su documento nacional de identidad, el nombre y los apellidos, el lugar de nacimiento o la fecha de nacimien-to, entre otras. Para identificar la obra, también utilizamos propiedades, como el título o la fecha de creación, entre otras. Y así, al construir la ontología computacional, co-menzamos a pensar en las propiedades que describen al es-critor, al retratista, a la novela y al retrato, y, especialmente, nos fijamos en las propiedades que diferencian al escritor del retratista y a la novela del retrato. \nSiguiendo con esta comparación, vemos que, en la onto-\nlogía, los conceptos se conectan con otros conceptos me-diante relaciones de clasificación que permiten construir ta-xonomías en las que se manifiestan la hiponimia y la \n hiperonimia; por ejemplo, los conceptos \u00abescritor\u00bb y \u00abretra-tista\u00bb son más específicos que el concepto \u00abautor\u00bb, o los con\n-\nceptos \u00abnovela\u00bb y \u00abretrato\u00bb son más específicos que el\u00a0con-cepto \u00abobra\u00bb. Los conceptos también se relacionan mediante relaciones meronímicas o de agregación, que permiten re-presentar las relaciones de \u00abparte de\u00bb o \u00abcomposición\u00bb. Por ejemplo, un \u00ablibro\u00bb en papel está formado por la \u00abportada\u00bb, la \u00abcontraportada\u00bb y muchas \u00abhojas de papel\u00bb; o una \u00abcolec-ción\u00bb está formada por un conjunto ordenado de \u00abnovelas\u00bb, como los Episodios nacionales de Benito Pérez Galdós. Ade-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   3834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   38 11/5/23   12:0411/5/23   12:0439más, los conceptos de la ontología se relacionan entre sí me-\ndiante relaciones específicas del dominio. Por ejemplo, la relación \u00abautor de\u00bb se emplea para representar la relación entre el autor y la obra; la relación \u00abescribe\u00bb sirve para repre-sentar la relación entre el escritor y la novela, y \u00abescrito por\u00bb para representar la relación entre la novela y su escritor; por su parte, la relación \u00abretrata\u00bb sirve para representar la rela-ción entre el retratista y el retrato, y \u00abretratado por\u00bb para re-presentar la relación entre el retrato y el retratista.\nLa ontología computacional permite representar la evo-\nlución temporal de los conceptos. Por ejemplo: el boceto es previo al retrato; o una exposición temporal está compues-ta por retratos que son cedidos y no siempre tienen que ser los mismos. \nA esto se añade el hecho de que las ontologías permiten \nque los sistemas de inteligencia artificial puedan integrar datos procedentes de fuentes heterogéneas porque conocen lo que significa un dato, es decir, la semántica de ese dato. De esta forma, la ontología resuelve los problemas deriva-dos de la polisemia, porque elimina la ambigüedad de la expresión lingüística al clasificarla en el concepto al que se refiere. Pensemos ahora en Miguel de Cervantes. Mayorita-riamente se piensa en el ilustre escritor. Sin embargo, esta expresión lingüística da nombre a calles de municipios y ciudades, colegios, plazas, bibliotecas, hoteles, etc. Las per-sonas somos capaces de reconocer el significado de la expre-sión lingüística en función del contexto en el que aparece. También sabemos que la calle tiene un punto de comienzo y otro de fin; y que el colegio, la biblioteca o el hotel tienen una geolocalización, un código postal y un n\u00famero de telé-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   3934439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   39 11/5/23   12:0411/5/23   12:0440fono. Por otra parte, el colegio tiene un n\u00famero de estu-\ndiantes y una plantilla de profesores, la biblioteca un n\u00fa-mero de obras y el hotel un n\u00famero de habitaciones. En cambio, el escritor tiene una fecha de nacimiento y de de-función que no tienen los otros conceptos. Por ello, si un sistema de inteligencia artificial agrega datos de entidades que se llaman \u00abMiguel de Cervantes\u00bb utilizando ontolo-gías, no podrá asignar un n\u00famero de teléfono, un código postal y una geolocalización al escritor, o una fecha de na-cimiento y defunción a una plaza.\nEn otras palabras, lo que tienen en com\u00fan las ontolo-\ngías, tanto en filosofía como en informática, es el intento de representar cualquier concepto o abstracción que existe. Mientras que en filosofía la ontología trata del ser funda-mentalmente, en informática las ontologías son representa-ciones consensuadas, procesables por máquinas, para que los sistemas de inteligencia artificial puedan categorizar y entender los principales conceptos de un dominio, sus pro-piedades e interrelaciones con otros conceptos para poste-riormente razonar con ellos. En definitiva, las ontologías computacionales proporcionan a los sistemas de inteligen-cia artificial terminologías consensuadas procesables por máquinas que describen alg\u00fan aspecto de la realidad.\n4. \n Principales aporta\nciones de la inteligencia \nartificial\nDurante los primeros años de la inteligencia artificial, el \nenfoque simbólico, al que ya me he referido, irrumpe con \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   4034439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   40 11/5/23   12:0411/5/23   12:0441fuerza a causa de las limitaciones de los procesadores, la \nmemoria RAM y la capacidad de almacenamiento de las computadoras de la época. En los años cincuenta, científi-cos como Allen Newell, Herbert Simon y Cliff Shaw de-mostraron que un ordenador puede realizar más tareas que la mera realización de cálculos matemáticos (recuérdese que los primeros ordenadores sustituyeron a las computa-doras, personas que realizaban cálculos matemáticos), al almacenar y manipular símbolos y, de esta forma, simular aspectos de la inteligencia humana. Estos investigadores construyeron en 1956 el Teórico Lógico\n 25, que es el primer \nprograma de inteligencia artificial que realizaba un razona-miento automatizado. Este programa probó 38 de los pri-meros 52 teoremas de los Principia Mathematica de White-head y Russell, y encontró nuevas pruebas y demostraciones más elegantes para algunos de los teoremas. Esta contribu-ción introdujo tres conceptos fundamentales en la inteli-gencia artificial simbólica: el razonamiento deductivo como técnica de inferencia; el uso de reglas, también llamadas heurísticas, para reducir la explosión combinatoria del es-pacio de b\u00fasqueda a un tamaño manejable, y un lenguaje de programación llamado IPL, antecesor del lenguaje LISP\n 26, que procesaba listas simbólicas. Al Teórico Lógico \nle siguió en 1958 el Método General de Resolución de Pro-\n25 A. Newell and H. Simon (1956): The Logic Theory Machine: a \nComplex Information P\nrocessing System. The RAND Corporation. P-868.\n26 J. McCarthy (1960): Recursive Functions of Symbolic Expressions \nand Their Computation by Machine\n. Part I. Massachusetts Institute of \nTechnologies.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   4134439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   41 11/5/23   12:0411/5/23   12:0442blemas 27, desarrollado por los mismos autores. Otras con-\ntribuciones importantes en la década de los sesenta fueron \nel lenguaje de programación LISP , inventado por J. Mc-Carthy; la lógica difusa\n 28, de Lotfi Zadeh; el ya menciona-\ndo programa de comunicación persona-ordenador llamado Eliza, de Weizenbaum\n 29; las redes semánticas 30 de Quillian \npara representar conocimientos de dominio en modelos computacionales, utilizando palabras como las que em-plean los seres humanos; y el primer sistema experto, lla-mado Dendral\n 31, que realizaba razonamiento científico \nformulando hipótesis y automatizaba la toma de decisiones en el dominio de la química orgánica. En el ámbito de la robótica, el robot Shakey\n 32 integraba numerosas técnicas \n27 A. Newell, J. C. Shaw and H. A. Simon (1958): Report on a \nGeneral P\nroblem-Solving Program. The RAND Corporation. P-1584. \nhttps://digitalcollections.library.cmu.edu/node/32154?search_api_fu-\nlltext=\n28 L. A. Zadeh (1965): \u00abFuzzy Sets\u00bb. Information and Control, 8 \n(3), 338-353. doi:10.1016/S0019-9958(65)90241-X\n29 J. Weizenbaum (1966): \u00abELIZA. A Computer Program for the \nStudy of Natural Language Communication between M\nan and Ma-\nchine\u00bb. Communications of the ACM, volume 9, number 1, 36 -45. \ndoi:\n10.1145/365153.365168. http://www.universelle-automation.de/ \n1966_Boston.pdf\n30 M. R. Quillian (1968): \u00abSemantic Memory\u00bb. In Marvin L. \nMinsky (ed.), S\nemantic Information Processing. MIT Press, 227-270.\n31 B. G Buchanan and E. A. Feigenbaum (1978): \u00abDENDRAL \nand Meta-DENDRAL: \nTheir Applications Dimension\u00bb. Artificial In-\ntelligence, 11, 5-24.\n32 P . E. Hart, N. J. Nilsson and B. Raphael (1968): \u00abA Formal Basis \nfor the Heuristic Deter\nmination of Minimum Cost Paths\u00bb. IEEE Transac-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   4234439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   42 11/5/23   12:0411/5/23   12:0443de inteligencia artificial, porque percibía su entorno con \nuna cámara, era capaz de moverse autónomamente y razo-naba sobre sus propias actuaciones, gracias al algoritmo de b\u00fasqueda A* (leído como \u00aba estrella\u00bb), que calculaba el ca-mino más corto entre dos puntos, y al planificador STRIPS\n 33, que le permitía decidir qué acción debía realizar \nen cada momento. Así, Shakey abría y cerraba puertas, en-cendía y apagaba luces, empujaba objetos móviles que en-contraba a su alrededor o se movía de una habitación a otra. En febrero de 2017, la prestigiosa organización ame-ricana IEEE reconoció a Shakey como el primer robot mó-vil inteligente. \nEn cuanto al enfoque conexionista de redes de neuro-\nnas, uno de los pioneros fue Frank Rosenblatt (1928-1971), quien, basándose en los trabajos de Santiago Ra-món y Cajal, McCulloch y Pitts, así como en el contexto de las teorías cibernéticas de Wiener, desarrolló en 1958 una red de neurona artificial de una capa, junto con su algorit-mo de aprendizaje, a la que llamó Perceptron\n 34 y que pro-\ngramó en un IBM 704 de válvulas de vacío que realizaba \ntions on Systems Science and Cybernetics, 4 (2), 100-107. doi:10.1109/\nTSSC.1968.300136\n33 R. Fikes and N. J. Nilsson (1971): \u00abSTRIPS: A New Approach \nto the Application of Theorem P\nroving to Problem Solving\u00bb. Artificial \nIntelligence, 2, 189-208. https://ai.stanford.edu/users/nilsson/Online-Pubs-Nils/PublishedPapers/strips.pdf\n34 F . Rosenblatt (1958): \u00abThe Perceptron: A Probabilistic Model \nfor Information Storage and O\nrganization in the Brain\u00bb. Psychological \nReview, 65 (6), 386-408. https://doi.org/10.1037/h0042519\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   4334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   43 11/5/23   12:0411/5/23   12:0444inicialmente 40  000 flops 35. La máquina Mark I Percep-\ntron fue la primera implementación del algoritmo del Per-\nceptron. Este computador se destinó a reconocer imágenes, y fue también el primero en clasificar fotos de hombres y mujeres después de haber sido entrenado. Sin embargo, como la mayoría de los modelos subsimbólicos, carecía de una representación interna simbólica o explícita acerca de cómo realizar el proceso de clasificación. En 1959, el térmi-no aprendizaje automático\n 36 fue acuñado por Arthur Samuel \ncomo sinónimo de la capacidad de las máquinas para ense-ñarse a sí mismas o de aprender sin ser programadas por un humano. Para ilustrarlo se sirvió del juego del ajedrez. Sin embargo, en 1969, Marvin Minsky y Seymour Papert\n 37 \nponen de relieve las dificultades del Perceptron a la hora de resolver problemas de clases que no son linealmente o cla-ramente separables y, en consecuencia, la investigación y desarrollo en redes de neuronas artificiales queda estancada hasta mediados de los años ochenta. \nA la vista de esta evolución, podemos decir que, hasta \nlos años setenta, los investigadores crearon algoritmos y modelos ingeniosos que resolvían problemas sencillos que necesitaban pocos datos, principalmente debido a las enormes limitaciones de los procesadores de las computa-doras de aquellos años y a su elevado coste. Durante estos \n35 https://amturing.acm.org/info/corbato_1009471.cfm\n36 A. L. Samuel (1959): \u00abSome Studies in Machine Learning Using the \nGame of Checkers\u00bb. IBM J\nournal of Research and Development, 3 (3), 210-229.\n37 M. Minsky and S. Papert (1969): Perceptrons. The MIT Press. \nhttps://mitpress.mit.edu/9780262630221/perceptr\nons/\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   4434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   44 11/5/23   12:0411/5/23   12:0445años iniciales, algunos científicos realizaron predicciones \ndesmesuradas sobre los resultados a los que llegaría esta disciplina, fallando en sus pronósticos. Así, en 1958, \n Rosenblatt señalaba en el New Y\nork Times 38 que \u00abel Per-\nceptron sería el embrión de una computadora electrónica que caminaría, hablaría, miraría, escribiría, se reproduci-ría y sería consciente de su existencia\u00bb. El premio nobel de \n economía Herbert Simon 39 predijo en 1960 que \u00ablas má-\nquinas serían capaces, dentro de 20 años, de realizar \n cualquier trabajo que los \nhombres pueden hacer\u00bb. Marvin \nMinsky 40 anunció en 1967 que \u00aben una generación, el \nproblema de crear inteligencia artificial estaría sustancial-mente resuelto\u00bb.\nSin embargo, los años 1972 y 1973 fueron claves en el \ndeclive de esta ciencia incipiente, provocando lo que se ha llamado el primer invierno de la inteligencia artificial. En 1972, Dreyfus\n 41, en su libro Lo que los ordenadores a\u00fan no \npueden hacer: una crítica del razonamiento artificial, analiza-\n38 M. Olazaran (1996): \u00abA Sociological Study of the Official History \nof the Perceptrons Contr\noversy\u00bb. Social Studies of Science, 26 (3), 611-\n659. doi:10.1177/030631296026003005. JSTOR 285702. S2CID \n16786738\n39 H. A. Simon (1960): \u00abOrganizational Design: Man-Machine Sys-\ntems for Decision M\naking. Lecture III. April 7, 1960\u00bb. In The New Scien-\nce of Management Decision. New York: Harper & Row, p. 38. (Verified with scans).\n40 M. Minsky (1967): Computation: Finite and Infinite Machines. \nPrentice-H\nall.\n41 H. L Dreyfus (1972): What Computers Can't Do: A Critique of \nArtificial R\neason. The MIT Press.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   4534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   45 11/5/23   12:0411/5/23   12:0446ba que los principales problemas de la inteligencia artificial \nsimbólica se encuentran en la representación del conoci-miento de sentido com\u00fan y el cuello de botella que supone la ad\n quisición del conocimiento del entorno. Coetánea-\nmente, el informe Lighthill 42, encargado por el Consejo \nBritánico de Investigación Científica, presentó un pronós-tico muy pesimista de la inteligencia artificial. El informe afirmaba que, en los \u00faltimos 25 años, los descubrimientos realizados no habían producido el impacto prometido en los años 50, y criticaba que los investigadores se valían de pocos datos para evitar así la explosión combinatoria de los problemas del mundo real. El informe Lighthill originó una reducción en los fondos destinados a investigación en las universidades británicas y coincidió en el tiempo con el cambio estratégico de DARPA\n 43, que pasó a financiar pro-\nyectos en vez de financiar instituciones. \nPara evitar la explosión combinatoria y reducir los pro-\nblemas a un tamaño adecuado para que se pudieran ejecu-tar en las computadoras tan limitadas de aquellos años, se postuló la representación del conocimiento de dominio, y especialmente el uso de heurísticas, como el elemento clave en el desarrollo de sistemas expertos. Durante los años 70, se construyó el sistema experto Mycin, que aplicaba reglas con factores de certeza para representar el conocimiento \n42 J. Lighthill (1973): \u00abPart I: Artificial I ntelligence: A General \nSurvey\u00bb. In Artificial Intelligence: A Paper Symposium. Cambridge Uni-\nversity. https://www.aiai.ed.ac.uk/events/lighthill1973/lighthill.pdf\n43 E. M. Phillips (1999): If it Works, It's not AI: A Commercial Look \nat Artificial I\nntelligence Startups. Ph. D Thesis. MIT.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   4634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   46 11/5/23   12:0411/5/23   12:0447con incertidumbre en el diagnóstico y terapia médica; y \nMinsky, en 1974, propuso los marcos 44 como técnica para \nrepresentar conocimiento simbólico.\nEn la siguiente década, los años ochenta, surge la pri-\nmera primavera de la inteligencia artificial. La industria pasó de unos pocos millones de dólares en 1980 a miles de\u00a0millones de dólares en 1988, gracias a la popularidad de los sistemas expertos. Otros avances relevantes, con impac-to en nuestros días, son las redes bayesianas\n 45 de Judea \nPearl, las primeras ontologías del proyecto Cyc y el algorit-mo de retropropagación del gradiente del error para el en-trenamiento de redes de neuronas artificiales con capas ocultas. Este algoritmo fue desarrollado por el psicólogo David Rumelhart\n 46 y los investigadores Geoffrey Hinton \n(Premio T uring en 2018 y Princesa de Asturias 2022) y Ronald Williams, y en el segundo decenio del siglo xxi  \nresultó clave para el desarrollo del denominado \u00abaprendi-zaje profundo\u00bb. \n44 M. Minsky (1974): A Framework for Representing Knowledge. \nMIT-AI Laboratory M\nemo 306. https://web.media.mit.edu/~minsky/\npapers/Frames/frames.html\n45 J. Pearl (1985): Bayesian Networks: A Model of Self-Activated Me-\nmory for Evidential R\neasoning (UCLA Technical Report CSD-850017). \nProceedings of the 7th Conference of the Cognitive Science Society, \nUniversity of California, Irvine, CA., 329-334. Retrieved 2009-05-01. https://ftp.cs.ucla.edu/tech-report/198_-reports/850017.pdf\n46 D. Rumelhart, G. Hinton and R. Williams (1986): \u00abLearning \nrepresentations b\ny back-propagating errors\u00bb. Nature, 323, 533 -536. \nhttps://www.nature.com/ar\nticles/323533a0#citeas\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   4734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   47 11/5/23   12:0411/5/23   12:0448También en los años 80, comienza el proyecto Cyc 47, \nque pretendía construir un sistema que tuviera el sentido \ncom\u00fan humano. Para ello disponía de cuatro componentes: un lenguaje formal muy expresivo, basado en la lógica para representar el conocimiento; una ontología computacional para representar los conocimientos de sentido com\u00fan; una gran base de conocimientos para almacenar grandes canti-dades de datos que se describen con una ontología; y un motor de inferencia para razonar con la ontología y los da-tos. Este proyecto fue el precursor de las ontologías compu-tacionales tal como las conocemos actualmente. \nPero no fueron suficientes los avances de los años ochen-\nta. Poco después, llegó el segundo invierno de la inteligencia artificial, porque el hardware de los sistemas de inteligencia artificial eran máquinas LISP , que difícilmente se integraban con los sistemas que manejaba la industria. En esas fechas, a \nmediados de los años 90, también decae el interés por las redes de neuronas artificiales. Aunque, teóricamente, el\u00a0al-goritmo de retropropagación del gradiente se puede aplicar a redes de neuronas artificiales —con cualquier n\u00famero de capas ocultas— para resolver problemas complejos, la falta de capacidad de cómputo y disponibilidad de grandes vol\u00fa-menes de datos impide que se pueda llevar a cabo. \nT ras este periodo de desgaste, comienza una segunda pri-\nmavera. A ello contribuye el hecho de que los investigadores y la industria comienzan a abandonar las arquitecturas LISP \n47 D. Lenat and R. V. Guha (1990): Building Large Knowledge-Based \nSystems: Representation and Inference in the Cyc Project. Addison-Wesley.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   4834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   48 11/5/23   12:0411/5/23   12:0449en favor de las estaciones de trabajo de IBM y de Sun Mi-\ncrosystems, las cuales, al estar conectadas a Internet y dispo-ner de nuevos sistemas operativos, procesadores y memorias, retaban a los investigadores a que abandonaran los problemas \u00abde juguete\u00bb y se pasaran a ejecutar los algoritmos en estas nuevas plataformas, para resolver problemas del mundo real.\nEste resurgir se vio afianzado por un hito muy relevante \nen 1997, cuando Deep Blue ganó al campeón mundial de ajedrez, que en aquellos momentos era Kasparov. Deep Blue era una computadora de procesamiento paralelo ma-sivo construida por IBM, que utilizaba un algoritmo de fuerza bruta y era capaz de calcular 200 millones de posi-ciones por segundo. \u00a1Causó enorme alegría en la comuni-dad de la inteligencia artificial ver al campeón Kasparov renunciar a seguir jugando y abandonar la sala agitando con desesperación las manos sobre la cabeza! \nEn la década de los noventa, las ontologías computacio-\nnales se convierten en un área de investigación de gran rele-vancia, pues proporcionan piezas de conocimientos que se pueden reutilizar para favorecer el intercambio de datos en-tre aplicaciones heterogéneas que comparten el mismo mo-delo. Entre las contribuciones más relevantes de estos años cabe mencionar la metodología METHONTOLOGY\n 48 y \nel entorno de desarrollo de ontologías Protégé 49. \n48 M. Fernández-López, A. Gómez-Pérez y N. Juristo (1997): \n\u00abMETHONTOLOGY\n: from Ontological Art towards Ontological \nEngineering\u00bb. En AAAI-97 Spring Symposium Series, 24-26 March \n1997, Stanford University.\n49 Stanford University: Protégé. https://protege.stanford.edu/\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   4934439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   49 11/5/23   12:0411/5/23   12:0450En 1997, el aprendizaje automático adquirió gran im-\nportancia gracias al libro publicado por el científico Tom \nMitchell 50, que contribuyó en gran medida a sistematizar \nesta área. Las técnicas de aprendizaje automático se clasifi-can, principalmente, en aprendizaje supervisado, aprendi-zaje no supervisado y aprendizaje por refuerzo. La caracte-rística principal del aprendizaje supervisado es que el conjunto de datos o ejemplos a partir de los que el sistema aprende a realizar una tarea está etiquetado; es decir, para cada entrada, se conoce la salida correcta. En el aprendizaje no supervisado, por el contrario, los datos no están etique-tados; no se conoce la respuesta correcta para cada entrada. Este tipo de aprendizaje se emplea, por ejemplo, para la resolución de tareas de clasificación o agrupación (cluste-ring), de manera que el sistema divide los datos en grupos o clases en función de su grado de similitud. A modo de ilustración de este tipo de aprendizaje, cabe mencionar la clasificación de textos por su contenido o el agrupamiento de fotografías en álbumes seg\u00fan las personas que aparecen en las escenas. En el aprendizaje por refuerzo, en cambio, el modelo aprende, de forma continua, a realizar una tarea a través de una serie de acciones llevadas a cabo por el sistema y mediante las recompensas o penalizaciones obtenidas al evaluar la calidad o consecuencias de estas acciones.\nEl inicio del siglo xxi representó un gran paso adelante \npor la rapidez con la que las infraestructuras procesan los datos gracias a que las nuevas unidades de proceso gráfico, \n50 T. Mitchell (1997): Machine Learning. New York: McGraw-Hill. \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   5034439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   50 11/5/23   12:0411/5/23   12:0451conocidas como GPU, cuyo diseño estaba destinado al \nprocesamiento de imágenes, pasan a emplearse también en computación vectorial, que es el tipo de operaciones que se realizan principalmente en redes de neuronas artificiales. Simultáneamente, surge la Web 2.0, la Internet de las \n cosas, \ny con ello las personas se convierten también en grandes productores de contenidos. Asimismo, las empr\nesas, insti-\ntuciones y Administraciones p\u00fablicas comienzan a digitali-zar sus datos: documentos, sonido, imágenes, vídeos. Todo esto contribuye a que se comiencen a almacenar, procesar y tener disponibles grandes cantidades de datos. Aprove-chando este almacenamiento masivo, surgen los sistemas de recomendación que se aplican a cualquier actividad y tarea. En la Web y en la Web 2.0, las personas son las que realizan las actividades cognitivas: deciden qué paginas leer, interpretan la información y agregan datos, pues los pro-gramas no pueden entender y razonar con los contenidos de dichas páginas. Para resolver estas limitaciones, surge la Web semántica\n 51, que se concibe como una extensión de \nla\u00a0Web actual destinada a los sistemas de inteligencia artifi-cial. Con la Web semántica se transforma el contenido de las páginas webs a otros formatos para que los sistemas in-formáticos descubran, integren y reutilicen sus contenidos más fácilmente. La Web semántica se sirve de las ontologías para proporcionar la semántica computacional a las pala-bras de las páginas webs. Durante los primeros años del si-glo xxi, el Consorcio de la World Wide Web propuso nue-\n51 J. Hendler, T. Berners-Lee and E. Miller (2002): Integrating Appli-\ncations on the Semantic W\neb. http://www.w3.org/2002/07/swint.html\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   5134439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   51 11/5/23   12:0411/5/23   12:0452vos lenguajes, como OWL 52 y RDF(S) 53 para representar \nlas ontologías computacionales y el lenguaje RDF 54 para \nrepresentar los datos. \nFue en el año 2011 cuando la inteligencia artificial se \nhace visible para el ciudadano, gracias al conocido concur-\nso estadounidense de televisión Jeopardy. La empresa IBM presentó como concursante a Watson, que compitió contra los campeones del momento. Watson era un sistema de pregunta-respuesta que resultó ganador. \nPero la verdadera explosión de la inteligencia artificial y \nsu aplicación en muchos sectores empresariales y cotidianos se debe al aprendizaje profundo. ¿A qué se llama \u00abaprendiza-je profundo\u00bb? A los algoritmos que se inspiran en la estruc-tura y función del cerebro humano y que utilizan redes de neuronas artificiales profundas para aprender representacio-nes o patrones, a partir de grandes conjuntos de datos, para realizar, entre otras tareas, el reconocimiento de voz, el aná-lisis de imágenes y el procesamiento del lenguaje natural. Los creadores de esta nueva línea de investigación, Geoffrey Hinton, Yoshua Bengio y Yann LeCun, fueron galardonados con el Premio T uring en 2018. Ellos tres, junto con Demis Hassabis, recibieron además el Premio Princesa de Asturias de Investigación 2022 por sus contribuciones en esta área. \n52 World Wide Web Consortium (2012): Web Ontolog y Language \n(OWL). https://www.w3.org/OWL/\n53 World Wide Web Consortium (2014): RDF Schema 1.1. ht-\ntps://www\n.w3.org/TR/rdf-schema/\n54 World Wide Web Consortium (2014): Resource D escription Fra-\nmework (RDF). https://www.w3.org/RDF/\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   5234439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   52 11/5/23   12:0411/5/23   12:0453Otras investigaciones continuaron avanzando en esta \n línea. En 2012, AlexNet 55, un sistema de visión por com -\nputador desarrollado por Krizhevsky, Hinton y otr\nos inves-\ntigadores, superaba a los mejores algoritmos de reconoci-\nmiento de imágenes por un amplio margen, al proponer las redes neuronales convolucionales, lo cual supuso un salto espectacular sin precedentes en el reconocimiento de imá-genes. En 2015, Alphago, de la empresa DeepMind, co-fundada por Demis Hassabis, gana al campeón mundial de GO inventando movimientos ganadores hasta entonces desconocidos, y, en 2017, el sistema AlphaZero de Google aprende las nociones estratégicas del ajedrez sin interven-ción humana, analizando 5 millones de partidas en cuatro horas. Finalmente, en 2021, el grupo DeepMind constru-yó la base de datos de estructuras de proteínas AlphaFold\n 56 \ncon más de 350  000 proteínas, sacando con ello el máximo \nrendimiento del aprendizaje pr\nofundo.\nRecientemente, en el ámbito de la percepción computa-\ncional, Hinton ha prepublicado GLOM 57, una teoría a\u00fan \nno implementada computacionalmente en la que propone incluir los denominados vectores coincidentes, a modo de heurísticas, en redes de neuronas artificiales para intentar \n55 A. Krizhevsky, I. Sutskever and G. E. H inton (2012): \u00abImage-\nNet Classification with Deep Convolutional Neural Networks\u00bb. Ad -\nvances in Neural Information Processing Systems, 25, 1097-1105.\n56 \u00abDeepMind's Protein-Folding AI Has Solved a 50-Year-Old \nGrand Challenge of Biology\u00bb. \nMIT T echnology Review, 2020.\n57 G. Hinton (2021): How to Represent Part-Whole Hierarchies in a \nNeural N\network. https://arxiv.org/abs/2102.12627 \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   5334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   53 11/5/23   12:0411/5/23   12:0454simular el razonamiento analógico y modelar la intuición. \nGLOM aborda dos de los problemas más difíciles de resolver para los sistemas de percepción visual: comprender una es-cena completa en términos de objetos y sus partes natura-les, y reconocer los objetos desde un nuevo punto de vista. \nPara concluir este breve recorrido por la historia y los \navances de la inteligencia artificial, se puede afirmar que todos estos ejemplos demuestran que la conjetura de Dart-mouth enunciada en 1956 es prácticamente una realidad, pues las máquinas se comunican en lenguaje natural, for-man abstracciones y conceptos, resuelven tipos de proble-mas y realizan tareas, al igual que los humanos, en dominios no triviales, razonan, aprenden, perciben y se comunican. \n5. \n Inteligencia artificial y procesamiento \ndel\u00a0lengu\naje natural \nLenguaje natural es un término que se utiliza en computa-ción para referirse al lenguaje humano, en contraposición al lenguaje artificial que se usa en programación. El proce-\nsamiento del lenguaje natural (PLN) es una rama de la inteligencia artificial cuyo objetivo es la creación de méto-dos, técnicas y recursos computacionales que permitan a las máquinas \u00abtratar\u00bb el lenguaje humano; por ejemplo, métodos para la extracción de información a partir de cor-pus textuales, para la traducción entre lenguas, para el es-tablecimiento de un diálogo entre una máquina y un hu-mano, para analizar manifestaciones de emociones, las tendencias de opinión o de compra en redes sociales, o \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   5434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   54 11/5/23   12:0411/5/23   12:0455bien para anonimizar documentos o para clasificar auto-\nmáticamente documentos, mensajes y reclamaciones, en-tre otros posibles fines. \nSi bien, como apuntamos anteriormente, el lenguaje \nhumano envuelve todos los aspectos sociales que forman el entorno del ser humano, su formalización o \u00abtratamiento\u00bb informático mediante el procesamiento del lenguaje natu-ral debe nutrirse de otras disciplinas, como la lógica, la estadística, la lingüística, la ciencia cognitiva o la filosofía. Uno de los máximos defensores del estudio formal del len-guaje desde el punto de vista interdisciplinar fue Noam Chomsky, quien contribuyó notablemente en la teoriza-ción de la lingüística durante toda la segunda mitad del siglo xx. Sus hipótesis y teorías, como la jerarquía de gra-máticas formales, han sido determinantes en la introduc-ción de nuevos métodos de análisis formal de las lenguas naturales en los que se ha basado el procesamiento del len-guaje natural, hipótesis y teorías que ya fueron introduci-das por D. Antonio Colino López\n 58 en su discurso de en-\ntrada en enero de 1972. \nEl procesamiento de lenguaje natural consta de dos su-\nbáreas principales de investigación: la que está orientada a la \u00abcomprensión del lenguaje\u00bb y la que centra sus esfuerzos en la \u00abgeneración del lenguaje\u00bb. La primera crea métodos \n58 A. Colino (1972): Ciencia y Lenguaje. D iscurso leído el día 23 \nde enero de 1972, en su recepción p\u00fablica, por el Excmo. Sr. Don \nAntonio Colino López y contestación del Excmo. Sr. D. Julián Marías. Madrid: Talleres gráficos Vda. De C. Bermejo. https://www.rae.es/si-tes/default/files/Discurso_de_ingreso_Antonio_Colino_Lopez.pdf\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   5534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   55 11/5/23   12:0411/5/23   12:0456capaces de transformar el lenguaje natural en una represen-\ntación informática que sea procesable o tratable por la má-quina. La segunda se basa en dichas representaciones para producir textos en lenguaje natural. Los dominios de apli-cación de las tecnologías de procesamiento de lenguaje na-tural son m\u00faltiples; por ello, estamos ante una tecnología transversal que permea en cualquier dominio.\nLos programas que procesan lenguaje natural necesitan \nentender las dimensiones lingüísticas, a saber: fonología, morfología, sintaxis, semántica y pragmática de los textos orales y escritos (aunque no siempre lo consigan). Para re-solver la ambigüedad semántica, por ejemplo, los progra-mas informáticos recurren al contexto lingüístico en el que se encuentra la palabra en el texto, y así establecen la \n dependencia con otras palabras y reconocen la polisemia. Además, deben tener en cuenta el contexto extralingüís\n-\ntico: la relación emisor-destinatario, el canal de comunica-ción, el momento en el que la comunicación tiene lugar o el tipo de texto, entre otr\nos aspectos.\nLos métodos y técnicas que analizan el lenguaje y mode-\nlizan el contenido de los textos son parejos a la evolución de los métodos y técnicas de la inteligencia artificial, al vo-lumen de los datos disponibles, a la capacidad de cómputo de los ordenadores y a la creatividad a la hora de construir aplicaciones que lleguen al usuario final. Las técnicas sim-bólicas para el procesamiento del lenguaje natural se sirven de representaciones lógicas del texto, mientras que las téc-nicas subsimbólicas aplican representaciones numéricas del texto. Con ambos enfoques se construyen modelos de len-guaje.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   5634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   56 11/5/23   12:0411/5/23   12:0457Los modelos de lenguaje son modelos computacionales \nque tratan de representar las expresiones o secuencias de \npalabras de una lengua. Estos modelos comenzaron con lenguajes naturales en dominios o ámbitos muy acotados soportados, en una primera etapa, por sistemas \n basados en \nreglas que formalizaban dicha comunicación y, posterior-mente, por ontologías. S\nin embargo, en los \u00faltimos años, \nlos modelos de lenguaje han variado su definición debido a la aparición del aprendizaje profundo. Actualmente, se pueden definir como modelos que pueden obtener la pro-babilidad de una secuencia de palabras en un lenguaje y predecir las palabras que siguen a las que le preceden. \nComencemos con la aproximación simbólica. Desde los \ninicios del PLN y hasta finales de los años 90, las herra-mientas para analizar los textos, es decir, para obtener una representación que un programa informático pudiera ma-nipular, eran sistemas basados en reglas de reescritura que, combinadas con información léxica de diccionarios, se convertían en gramáticas del lenguaje en cuestión. Durante estos años, se anotaban corpus pequeños de forma manual, con gran calidad. Esta tarea implicaba el enriquecimiento del texto con descriptores lingüísticos a distintos niveles: morfológico (en general asociando el lema y la categoría a las formas), sintáctico (identificando los constituyentes y/o las dependencias), léxico-semántico (anotando la modali-dad, la polaridad o la correferencia, entre otros). \nA principios del año 2000, se desarrollaron sistemas \npara la creación de grandes gramáticas, basadas en la teoría de la gramática sintagmática nuclear (HPSG en inglés) y la teoría de las gramáticas léxico-funcionales (LFG en inglés). \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   5734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   57 11/5/23   12:0411/5/23   12:0458En esa época, destacan el proyecto de gramática paralela \nconocido en inglés como The Parallel Grammar Project 59, \nde 2002, o el marco de trabajo gramatical Grammatical Framework\n 60, activo desde 2004 hasta nuestros días. Tam-\nbién aparecieron los sistemas en los que se concatenan mó-dulos para realizar el análisis de textos, generalmente en inglés. El resultado fue la extracción de contenidos del tex-to y su representación en un lenguaje formal.\nLas principales ontologías lingüísticas surgieron en la \n\u00faltima década del siglo xx. Tenían como finalidad descri-bir la semántica de las unidades gramaticales de una len-gua para ser utilizadas en el procesamiento del lenguaje natural y en la traducción automática. Para definir la se-mántica de las palabras, describían en lenguaje formal sus propiedades y las relaciones semánticas entre ellas. Onto-logías lingüísticas relevantes de aquellos años fueron el modelo PENMAN\n 61, Mikrokosmos 62, Generalized Upper \n59 M. Butt, H. Dyvik, T.H. King, H. Masuichi and C. Rohrer \n(2002): \u00abThe Parallel Grammar P\nroject\u00bb. In COLING-02: Grammar \nEngineering and Evaluation. https://aclanthology.org/W02-1503.pdf\n60 A. Ranta (2004): \u00abGrammatical Framework\u00bb. Journal of Func-\ntional Programming, 14 (2), 145-189.\n61 J. A. Bateman, R. T. Kasper, J. D. Moor and R. A. Whitney \n(1990). A Gener\nal Organization of Knowledge for Natural Language \nProcessing: The Penman Upper Model. Technical Report. USC/Infor-\nmation Sciences Institute. Marina del Rey, California.\n62 K. Mahesh (1996): Ontology Development for Machine Transla-\ntion: Ideology and M\nethodology. Technical Report MCCS-96-292. Com-\nputing Research Laboratory, New Mexico State University, Las Cruces, New Mexico. http://citeseer.nj.nec.com/mahesh96ontology.html\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   5834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   58 11/5/23   12:0411/5/23   12:0459Model (GUM) 63 y SENSUS 64. Posiblemente, la más avan-\nzada fue GUM, de John Bateman, capaz de expresar la es-\ntructura de la lengua inglesa y representar el contenido de los textos escritos en un lenguaje formal que las computa-doras pudieran manejar para razonar sirviéndose de las le-yes de la lógica. El primer sistema que generó una cantidad significativa de textos en español siguiendo este enfoque fue Ontogeneration\n 65, circunscrito al dominio de los ele-\nmentos químicos. En esta década se construyó la base de datos léxica WordNet\n 66, uno de los recursos más reputados \nen el procesamiento del lenguaje natural, cuyo propósito fue organizar el lexicón del inglés basándose en el significa-\n63 J. A. Bateman, G. Fabris and B. Magnini (1995): \u00abThe Genera-\nlized Upper M\nodel Knowledge Base: Organization and Use\u00bb. In Mars \nN (ed.): Second International Conference on Building and Sharing of Very \nLarge-Scale Knowledge Bases (KBKS '95). University of T wente: Ensche-de, The Netherlands; IOS Press: Amsterdam, The Netherlands, 60-72.\n64 B. Swartout, P . Ramesh, K. Knight and T. Russ (1997). \u00abToward \nDistributed Use of Large-Scale O\nntologies\u00bb. In A. Farquhar, M. Gru-\nninger, A. Gómez-Pérez, M. Uschold and P . van der Vet (eds.): AAAI'97 Spring Symposium on Ontological Engineering. Stanford Uni-versity: California, 138-148.\n65 G. Aguado de Cea, A. Bañón, J. Bateman, S. B ernardos, M. \nFernández, A. Gómez-Pérez, E. Nieto, A. Olalla, R. Plaza and A. Sán-chez (1998): ONTOGENERATION: Reusing Domain and Linguistic Ontologies for Spanish T ext Generation. En la 13\nth European conference \non artificial intelligence.\n66 G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross and K. Miller \n(1990): \u00abIntroduction to \nWordNet: An On-Line Lexical Database\u00bb. \nInternational Journal of Lexicography, 3 (4), 235-244.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   5934439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   59 11/5/23   12:0411/5/23   12:0460do de las palabras mediante relaciones de sinonimia-anto-\nnimia, hiperonimia-hiponimia o meronimia, entre otras. A continuación, el proyecto EuroWordNet\n 67 tomó como \npunto de partida la estructura del WordNet y creó lexico-nes para otras lenguas europeas que se conectaban a través de una \u00abinterlengua\u00bb. Esta especie de conceptualización in-termedia permitía crear puentes entre lexicalizaciones en distintos idiomas.\nEn la primera década del siglo xxi, algunos lingüistas \ninteresados por la Web semántica plantean la pertinencia de aprovechar el conocimiento de dominio previamente estructurado en ontologías y validado por expertos para que las herramientas de procesamiento de lenguaje natural lo usen en mayor medida. Este aprovechamiento se plan-tea desde dos puntos de vista: en primer lugar, enriquecer las descripciones lingüísticas asociadas a los conceptos de\u00a0la ontología con información morfosintáctica emplean-do los modelos LingInfo\n 68, LexInfo 69 y LexOnto 70; en se-\ngundo lugar, traducir las descripciones a m\u00faltiples lenguas, \n67 Eurowordnet: https//archive.illc.uva.nl//EuroWordNet/ \n68 P . Buitelaar, T. Declerck, A. Frank, S. Racioppa, M. Kiesel, M. \nSintek and R. Engel (2006): LingI\nnfo: Design and Applications of a \nModel for the Integration of Linguistic Information in Ontologies. In \nProceedings of the OntoLex Workshop at LREC.\n69 P . Buitelaar, P . Cimiano, P . Haase and M. Sintek (2009): \n\u00abTowar\nds Linguistically Grounded Ontologies\u00bb. In Proceedings of the \n6th European Semantic Web Conference (ESWC09), 111-125.\n70 P . Cimiano, P . Haase, M. Herold, M. Mantel and P . Buitelaar \n(2007): \u00abLexOnto: A Model for O\nntology Lexicons for Ontology- \nbased NLP\u00bb. In Proceedings of the OntoLex07 Workshop at the ISWC07.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   6034439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   60 11/5/23   12:0411/5/23   12:0461para su uso en distintos contextos lingüísticos con el mo-\ndelo LIR 71. Estos modelos tenían el problema de estar \n restringidos a un subconjunto limitado de descriptores lin-güísticos y no pr\noporcionar mecanismos para su extensión \no ampliación. \nEl año 2012 es testigo de la creación de un grupo de \ntrabajo específico en el marco del Consorcio de la Web (el llamado Ontology-Lexica Community Group) que se en-cargaría de proponer un modelo que sirviera de puente \n entre la representación semántica de los conceptos que\n \n proponían las ontologías de dominio y las distintas ma -\nnifestaciones lingüísticas para nombrar a los conceptos. Para ello se tomaba como punto de partida el modelo Le\n-\nmon 72 (Lexicon Model for Ontologies) que acababa de ver \nla luz, resultado de un proyecto europeo, y que compartía esta misma filosofía. El modelo resultante se llamó Onto-lex y su objetivo era sentar las bases de un modelo que permitiera estructurar y representar la información lin-güística asociada a una conceptualización de dominio, pero que fuera lo suficientemente genérico como para dar cabida a diversas necesidades lingüísticas y lo suficiente-mente flexible como para incluir descripciones antes no \n71 E. Montiel-Ponsoda, G. Aguado de Cea, A. Gómez-Pérez and \nW. Peters (2010): \u00abE\nnriching ontologies with multilingual informa-\ntion\u00bb. Natural Language Engineering, vol. 17, n.\u00ba 3, 283-309.\n72 J. McCrae, G. Aguado-de Cea, P . Buitelaar, P . Cimiano, T. De-\nclerck, A. Góme\nz-Pérez, J. Gracia, L. Hollink, E. Montiel-Ponsoda, \nD. Spohr and T. Wunner (2012): \u00abInterchanging Lexical Resources on \nthe Semantic Web\u00bb. Language Resources and Evaluation, vol. 46.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   6134439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   61 11/5/23   12:0411/5/23   12:0462previstas. \nTodo ello dio lugar a un modelo que actualmente cons-\nta de varios módulos o ramificaciones, centradas en uno o \nvarios aspectos lingüísticos, y que está dedicado a la conver-sión de recursos lingüísticos (lexicones, tesauros, termino-logías) en los formatos de la Web semántica (o Web de datos). Las especificaciones del modelo inicial vieron la luz en 2016, y en 2019 se publica el módulo lexicográfico, con el objetivo de proporcionar un modelo que permita repre-sentar (modelar) el contenido y la estructura de dicciona-rios y otros recursos lexicográficos en formatos estándares. Hoy en día, el grupo de expertos de Ontolex sigue traba-jando de manera incansable para dar respuesta a las necesi-dades de representación computacional de los recursos lin-güísticos basada en técnicas simbólicas.\nContin\u00fao con la aproximación subsimbólica. Como ya \nse ha mencionado, todo empezó con el reconocimiento de caracteres y el Perceptron. Desde el principio de los años 90, las redes de neuronas artificiales, al igual que otros al-goritmos probabilísticos, fueron apareciendo para generar tanto modelos de lenguaje como modelos para el procesa-miento del lenguaje natural en diferentes tareas. Sin em-bargo, los límites de la computación de ese momento hicie-ron que dichos avances quedasen latentes hasta años más tarde. Por ejemplo, las redes neuronales recurrentes que aparecieron a mitad de los años 90 —como las memorias largas a corto plazo (en inglés, long short-term memory\n 73 o \n73 S. Hochreiter and J. Schmidhuber (1997): \u00abLong Short-Term \nMemory\u00bb\n. Neural Computation, 9 (8), 1735-1780.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   6234439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   62 11/5/23   12:0411/5/23   12:0463LSTM) o las unidades recurrentes cerradas (en inglés, gated \nrecurrent units 74 o GRU)— no tuvieron un verdadero im-\npacto en el procesamiento del lenguaje natural hasta 2016, \ncuando manifestaron su utilidad al capturar la relación en-tre las palabras y su contexto. Con estas técnicas se empezó a aprender el orden de las palabras en problemas de predic-ción de secuencias de texto. \nEs en 2017 cuando se produce un hito y un cambio de \nparadigma con la publicación de la arquitectura neuronal T ransformer\n 75 (T ransformador) de Google. Esta arquitec-\ntura neuronal, que se pensó concretamente para la traduc-ción, introduce mecanismos de atención como base \u00fanica y elimina la recurrencia. De hecho, el trabajo se titula Todo lo que necesitas es atención (Attention is all you need). Los mecanismos de atención permiten, a partir de grandes vo-l\u00famenes de datos, generar patrones de conexiones entre las palabras que suelen aparecer juntas o relacionadas, pero en diferentes partes de una oración. Por ello, mejoran las redes de neuronas recurrentes, que se fijaban más en las palabras inmediatamente anteriores y posteriores a una voz determi-nada. \nLa importancia del T ransformer se debió a que la comu-\nnidad científica descubrió que, mediante los principales \n74 J. Chung, C. Gülçehre, K. Cho and Y. Bengio (2014): Empiri-\ncal Evaluation of G\nated Recurrent Neural Networks on Sequence Mode-\nling. CoRR, abs/1412.3555.\n75 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, J. Llion, A. N. \nGómez, L. Kaiser and I. P\nolosukhin (2017): Attention Is All You Need. \nhttps://arxiv.org/abs/1706.03762.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   6334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   63 11/5/23   12:0411/5/23   12:0464componentes que constituyen un transformador, denomi-\nnados codificador y decodificador, o mediante arquitectu-ras derivadas de él se construirían modelos de lenguaje y que, además, este tipo de modelos podían llevar a cabo un proceso de adaptación, también denominado \u00abajuste fino\u00bb, para realizar tareas de procesamiento de lenguaje natural con todo el conocimiento aprendido. \nPara la creación de un gran modelo de lenguaje, se re-\nquiere un gran corpus de texto que permita aprender el vocabulario en él utilizado y la relación entre las palabras que lo conforman. Los ordenadores necesitan ingentes can-tidades de textos, con los que aprenden patrones de regula-ridad, de manera que aquello que es poco frecuente en el texto se pierde. Con más datos, los patrones estadísticos se calculan mejor y la elección de las palabras en cada contex-to será más precisa. Por tanto, un modelo de lenguaje tra-baja con una distribución probabilística sobre una palabra o secuencia de palabras y, en su versión más simplificada, predice qué texto sigue a una cadena de palabras determi-nada en función de los textos del corpus empleado en su construcción. En otras palabras, un modelo de lenguaje predice cuál es la siguiente palabra en la frase que se está escribiendo. \nGeneralmente, para construir un modelo de lenguaje se \ntiende a utilizar corpus genéricos de cada idioma. Sin em-bargo, para crear modelos de lenguajes especializados en diferentes ramas del saber —como el médico, el farmacéu-tico, el jurídico, el financiero o el del sector p\u00fablico—, se necesitan corpus en los lenguajes de especialidad de cada una de las materias, para cubrir la terminología o la fraseo-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   6434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   64 11/5/23   12:0411/5/23   12:0465logía propias de cada dominio, y la relación que existe entre \nlas palabras, mediante estos mecanismos de atención. A modo de ejemplo, un sistema conversacional que utilice un modelo de lenguaje construido con un corpus general po-dría completar la frase los estudiantes se reunieron en con la palabra clase, o con el comedor, la sala de lecturas o el patio; \npero, si el modelo de lenguaje fuera especializado en farma-cia, la frase se completaría con el laboratorio o, si fuera es-pecializado en informática, con el centro de proceso de datos. En definitiva, el sistema conversacional aplica el modelo de lenguaje y genera las frases usando la probabilidad de que cierta palabra aparezca en una frase. \nComo se ha mencionado antes, un modelo de lenguaje \nya entrenado (o, como se denomina en la comunidad, pre-entrenado) puede ser sometido a un proceso de ajuste fino para que realice tareas concretas de procesamiento del len-guaje natural, tales como la clasificación de documentos o sistemas de pregunta-respuesta, beneficiándose de todo lo que ha aprendido del lenguaje previamente. Esta tarea de ajuste fino requiere un corpus con las anotaciones necesa-rias para realizarla y modificar ligeramente la red de neuro-nas final. Esta posibilidad de reutilización de los modelos de lenguaje para otras tareas constituye el gran cambio de paradigma del procesamiento del lenguaje en la actualidad y ha revolucionado la investigación en este campo. \nDe hecho, los modelos de lenguaje se eval\u00faan atendien-\ndo a los resultados que obtienen en diferentes tareas me-diante su ajuste fino. Estas tareas de procesamiento de len-guaje natural se realizan a través de conjuntos de corpus con anotaciones, como GLUE para el inglés, que se toman \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   6534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   65 11/5/23   12:0411/5/23   12:0466como referencia para comparar los diferentes modelos, \ncomo BERT 76 y RoBERTa 77. Para el español, han apareci-\ndo corpus anotados para tareas de preguntas y respuestas o detección de nombres como SQAC\n 78 o CANTEMIST 79. \nSin embargo, estos corpus todavía son pocos y pequeños en comparación con los de la lengua inglesa.\nDesde 2017 se han creado arquitecturas de modelos de \nlenguaje cada vez más sofisticados, generados por gigan-tes tecnológicos, basadas en la arquitectura de T ransfor-mers, tales como BERT (2018), RoBERTa (2019) y la serie de modelos generativos GPT-1 (Generative Pre-  \ntrained T ransformer), GPT-2\n 80 (2019), GPT-3 81 (2020),  \n76 J. Devlin, M.W. Chang, K. Lee and K. Toutanova (2018): \nBERT: P\nre-training of Deep Bidirectional Transformers for Language Un-\nderstanding. https://arxiv.org/abs/1810.04805\n77 Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi et al. (2019): RoBER-\nta: A Robustly Optimiz\ned BERT Pretraining Approach. https://arxiv.\norg/abs/1907.11692\n78 A. Gutiérrez-Fandiño et al. (2021): MarIA: Spanish Language \nModels. https://ar\nxiv.org/abs/2107.07253\n79 A. Miranda-Escalada, E. Farré and M. Krallinger (2020): \u00abNamed \nEntity Recognition, Concept N\normalization and Clinical Coding: Over-\nview of the Cantemist T rack for Cancer Text Mining in Spanish, Corpus, \nGuidelines, Methods and Results\u00bb.  In Proceedings of the Iberian Languages \nEvaluation Forum (IberLEF 2020). CEUR Workshop Proceedings. \n80 A. Radford, J. Wu, R. Child, D. Luan, D. Amodei and I. Suts-\nkever (2019): Language M\nodels Are Unsupervised Multitask Learners. \nOpenAI blog, 1 (8), 9. https://openai.com/blog/tags/gpt-2/\n81 B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P . Dha-\nriwal, A. Neelakantan, P . S\nhyam, G. Sastry, A. Askell, S. Agarwal, A. \nHerbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   6634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   66 11/5/23   12:0411/5/23   12:0467GPT-4 82 (2023) de gran impacto en los medios por el Chat-\nGPT y ChatGPT plus de OpenAI, y recientemente Google \nBard 83 (2023). También hay notables modelos especializados, \ncomo SciBERT 84, para \u00abentender\u00bb el lenguaje de los artículos \ncientíficos. Todos estos modelos se han centrado preferente-mente en el idioma inglés e incluso algunos se han entrenado para varios idiomas al mismo tiempo. En español, el primer modelo de esta rama de investigación fue BETO\n 85 (2019), \nbasado en la arquitectura BERT, que fue desarrollado por la Universidad de Chile. En 2020, como resultado del Plan de Tecnologías del Lenguaje y de la colaboración del Barcelona Super Computing Center (BSC) y la Biblioteca Nacional de\u00a0España (BNE), se publica el modelo MarIA\n 86. Desde en-\ntonces, han aparecido otros modelos, como RigoBERTa 87  \nZiegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. \nGray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever and D. Amodei (2020): Language Models are Few-Shot Lear-ners. https://arxiv.org/pdf/2005.14165.pdf\n82 OpenAI (2023): GPT-4 System Card. https://cdn.openai.com/\npapers/gpt-4-system-card.pdf\n83 Google Bard. https://bard.google.com/\n84 I. Beltagy, K. Lo and A. Cohan (2019): SciBERT: A Pretrained \nLanguage Model for Scientific T\next. https://arxiv.org/abs/1903.10676\n85 J. H. Ho, H. Kang, J. Pérez et al. (2020) \u00abSpanish Pre-T rained \nBERT Model and E\nvaluation Data\u00bb. PML4DC ICLR 2020. https://\nusers.dcc.uchile.cl/~jperez/papers/pml4dc2020.pdf\n86 A. Gutiérrez Fandiño, J. Armengol Estapé, M. Pàmies, J. Llop \nPalao, J. Silv\neira Ocampo, C. Pio Carrino, & M. Villegas (2022). \u00abMarIA: \nSpanish language models\u00bb. Procesamiento del Lenguaje Natural, 68.\n87 A. V. Serrano, G.G. Subies, H.M. Zamorano, N.A. García, D. \nSamy, D.B. S\nánchez, ... A.B. Jiménez. (2022). RigoBERT a: A State-of-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   6734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   67 11/5/23   12:0411/5/23   12:0468o IXAes 88. El corpus  BETO 89 constaba de tres mil millo-\nnes 90 de palabras. Por otro lado, el volumen de los corpus \nMarIA y RigoBERTa es casi similar, 135 mil millones de \npalabras 91, aunque vienen de fuentes de datos totalmente \ndistintas. \nLos modelos de lenguaje se caracterizan por la arquitec-\ntura en la que se basan, el volumen del corpus, el n\u00famero de parámetros, el tiempo y el modo de entrenamiento y el n\u00famero de las GPU, que se refiere a la capacidad de com-putación destinadas al entrenamiento. Tomando como ejemplo la familia GPT se observa el crecimiento, pues GPT-3 necesitó 100 veces más potencia de cálculo que GPT-2, y este necesitó 1000 veces más potencia de cálculo que GPT. También hay un aumento en el tamaño de los corpus utilizados para crear los modelos (un factor 10 de GPT\n-2 a GPT-3, concretamente de 40GB a 570 GB de \ntexto). La constr\nucción de estos grandes modelos no está al \nalcance de cualquier empresa, y mucho menos para lenguas minoritarias, debido a los corpus necesarios, a la infraes-\nthe-Art Language Model For Spanish. https://arxiv.org/abs/2205.10233\n88 R. Agerri, E. Agirre, (2022). Lessons learned fr om the evaluation \nof Spanish Language Models. https://arxiv.org/abs/2212.08390\n89 J. Cañete, G. Chaperon, R. Fuentes, J.H. Ho, H. Kang, J. P érez \n(2020). \u00abSpanish pre-trained BERT model and evaluation data\u00bb. Pml4dc \nat iclr, 1-10.\n90 https://pml4dc.github.io/iclr2020/papers/PML4DC2020_10.\npdf\n91 https://pml4dc.github.io/iclr2020/papers/PML4DC2020_10.\npdf\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   6834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   68 11/5/23   12:0411/5/23   12:0469tructura de computación requerida, y a las enormes canti-\ndades de energía empleadas para su entrenamiento, ajuste y operación, cuando millones de usuarios lo utilizan con fre-cuencia, con el consiguiente aumento del impacto ambien-tal en la huella de carbono.\nLas arquitecturas de los grandes modelos de lenguaje se \nhan aplicado en la lengua de la química o la biología\n 92, dan-\ndo lugar, en este \u00faltimo caso, a la llamada revolución de la biología digital. Estos modelos aprenden cómo leer y escri-bir en el \u00abidioma nativo\u00bb de la física, la química o la biolo-gía. La empresa NVIDIA vende el modelo MegaMolBART con el \u00ablenguaje de la química\u00bb; los modelos ESM-1\n 93 y \nProtT5 94 contienen el \u00ablenguaje de las proteínas\u00bb. Al igual \nque los grandes modelos de lenguaje, los grandes modelos bioquímicos pueden ser reentrenados mediante técnicas de ajuste fino para que realicen tareas especializadas, como ge-nerar sustancias químicas con propiedades muy concretas o proteínas de un cierto tipo. Esto permite generar candida-tos para nuevos medicamentos, que, gracias al enorme \n92 C. Marquet, M. Heinzinger, T. Olenyi, T. et al. (2022) \u00abEmbe-\nddings from protein language models pr\nedict conservation and variant \neffects\u00bb. Hum Genet 141, 1629-1647 https://doi.org/10.1007/s00439-  \n021-02411-y\n93 J. Meier, R. Rao, R. Verkuil, J. Liu, T. Sercu, A. Rives (2021). \n\u00abLanguage models enable zer\no-shot prediction of the effects of muta-\ntions on protein function\u00bb. Advances in Neural Information Processing \nSystems, 34, 29287-29303.\n94 A. Elnaggar, M. Heinzinger, C. Dallago, G. Rehawi, Y. Wang, \nL. Jones, \nL. et al. (2021). \u00abProtT rans: towards cracking the language of \nlife's code through self-supervised learning\u00bb. bioRxiv.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   6934439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   69 11/5/23   12:0411/5/23   12:0470avance de los simuladores (en parte también gracias a la \nIA), pueden simular su efecto en el cuerpo humano sin necesidad de ensayos reales con humanos. Solo cuando el simulador haya validado una sustancia se iniciará el sistema tradicional de pruebas reales con humanos. El descubri-miento de nuevos medicamentos está a nuestro alcance gracias a la IA. \nAsí, un modelo de lenguaje que aplica una representa-\nción numérica del texto no es capaz de explicar el motivo por el que se sugiere una determinada palabra para com-pletar una frase, más allá de que es la más probable por-que aparece en muchos textos o de que los mecanismos de atención sugieren cierta relación entre palabras. En la actualidad, estos sistemas están muy limitados, porque no razonan con los datos y con el conocimiento extraído de los textos, de forma que estos sistemas a\u00fan no son capaces de deducir nuevos datos o extrapolar nuevo co-nocimiento a partir de la información disponible en el texto pues desconocen el significado del texto que han escrito.\nA modo de ejemplo, la versión actual de Chat-GPT, que \nes una de las tecnologías más utilizadas del momento, es a la vez brillante y necio. Es brillante porque produce re\n-\nsultados verbosos y complacientes, con un léxico, una sin-taxis y una gramática correctos, y un discurso educado\n. Por \notro lado, es necio porque la versión actual de Chat -GPT \nno piensa, no razona, no entiende lo que escribe, no predi\n-\nce lo que podría ocurrir. En otras palabras, Chat-GPT no es capaz de realizar explicaciones o razonamientos matemá-ticos temporales o de sentido com\u00fan relacionados con el \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   7034439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   70 11/5/23   12:0411/5/23   12:0471texto que ha escrito. Tampoco elabora críticas sobre un tex-\nto ni realiza conjeturas. Con la tecnología actual, el modelo solo puede explicar en términos estadísticos por qué eligió una palabra frente a otra. En palabras de Chomsky\n 95: \u00abLa \nmente humana no es, como ChatGPT y sus similares, un pesado motor estadístico que busca patrones, que se atibo-rra de cientos de terabytes de datos y extrapola la respuesta conversacional más probable o la respuesta más probable a una pregunta científica. Por el contrario, la mente humana es un sistema sorprendentemente eficiente, y hasta elegan-te, que opera con pequeñas cantidades de información; no busca inferir correlaciones brutas entre puntos de datos, sino crear explicaciones\u00bb. \nEn los \u00faltimos años ha aparecido el enfoque neurosim-\nbólico, que hibrida los modelos simbólicos y subsimbóli-cos. Se basa en la inyección de conocimiento procedente de grafos de conocimientos sobre los modelos de lenguaje ya entrenados para reforzarlos y realizar un ajuste fino. Nace con dos objetivos: el primero trata de solventar el problema de la falta de datos con los que se entrenan tanto los mode-los de lenguas minoritarias como los modelos de dominios concretos que adolecen de corpus voluminosos; el segundo se dirige a las tareas de ajuste fino que necesitan de datos anotados por humanos, lo cual conlleva procesos de lectu-ra, comprensión y anotación de los textos por más de un experto. La inyección de conocimiento se plantea, por aho-\n95 N. Chomsky, I. Roberts and J. Watumull (2023): \u00abThe False \nPromise of ChatGPT\u00bb. The New York Times. https://www.nytimes.\ncom/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   7134439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   71 11/5/23   12:0411/5/23   12:0472ra, de dos formas 96,  97: en la entrada de datos o en el propio \nmodelo. \nRecientemente se han identificado los posibles riesgos \nde los grandes modelos de lenguaje 98 en categorías que van \ndesde la discriminación, la exclusión y la toxicidad en el \nuso de un lenguaje que incite al odio, a la violencia o que cause ofensa, hasta la información falsa o engañosa que puede ser empleada en campañas de desinformación, ya sea para crear estafas personalizadas o fraudes a gran escala. Gran parte de estos riesgos se derivan de la elección de cor-pus que incluyen lenguaje dañino, que sobrerrepresentan algunas identidades y mayorías sociales en detrimento de\u00a0otras. Otros riesgos se encuentran en las llamadas \u00abalu-cinaciones\u00bb que ocurren, en palabras de Chomsky, porque los modelos de lenguaje se atiborran de cientos de terabytes \n96 Y. Yao, S. Huang, L. Dong, F . Wei, H. Chen and N. Zhang \n(2022): \u00abKformer: Knowledge Injection in \nT ransformer Feed-Forward \nLayers\u00bb. En Natural Language Processing and Chinese Computing. \nCham: Springer International Publishing, 131-143.\n97 D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang and F . Wei (2022): \n\u00abKnowledge neurons in \npre-trained transformers\u00bb. En Proceedings of \nthe 60th Annual Meeting of the Association for Computational Linguis-tics (Volume 1: Long Papers). Dublin, Ireland: Association for Compu-tational Linguistics, 8493\n-8502.\n98 L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P . Huang, \nM. Cheng, M. Glaese, B. Balle, A. Kasir\nzadeh, Z. Kenton, S. Brown, W. \nHawkins, T. Stepleton, C. Biles, A. Birhane, J. Haas, L. Rimell, L.A. Hendricks, W. S. Isaac, S. Legassick, G. Irving and I. Gabriel\n(2021): Ethical and Social Risks of Harm from Language Models. \nDeepMind. https://arxiv.org/pdf/2112.04359.pdf \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   7234439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   72 11/5/23   12:0411/5/23   12:0473de datos para generar los patrones estadísticos que combi-\nnan información frecuente, pero no relacionada, y pueden construir contenido erróneo, frases incongruentes y sin sentido. Sabemos que los grandes modelos de lenguaje son extremadamente fluidos y suelen generar textos con una gramática correcta, pero no siempre son veraces, en el sen-tido de que no siempre garantizan que el texto generado carezca de errores de contenido. Si los modelos se entrenan con textos de la Web que contienen errores, relacionados por ejemplo con fechas, personas, o lugares, las frases que generará el sistema contendrán también información falsa o engañosa. Además, para que el modelo de lenguaje no se quede obsoleto, conviene alimentarlo con nuevos textos de forma continuada.\nConsidero también importante referirme a los elevadísi-\nmos costes energéticos en los que se incurre debido a la energía requerida para alimentar el hardware en el que se entrenan y ajustan los grandes modelos de lenguaje, que utilizan miles de millones de parámetros. Existen estudios científicos\n 99 que ya cuantifican el elevado impacto ambien-\ntal que supone la construcción de esos modelos, medido en la huella de carbono. Si bien la reducción de la huella de carbono es una preocupación constante de gobiernos y em-presas, los sistemas conversacionales están siendo utilizados masivamente por millones de usuarios que, a veces, los em-\n99 E. Strubell, A. Ganesh and A.McCallum (2019): \u00abEnergy and \nPolicy Considerations for Deep Learning in NLP\u00bb. I\nn Proceedings of the \n57th Annual Meeting of the Association for Computational Linguistics. \nFlorence, Italy: Association for Computational Linguistics, 3645-3650.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   7334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   73 11/5/23   12:0411/5/23   12:0474plean como un juguete más. Aunque el coste energético \nindividual puede resultar irrelevante, el impacto agregado en la huella de carbono no lo es. Por ello, los ciudadanos debemos ser conscientes de que nuestro consumo energéti-co también suma y debemos contribuir, en lo posible, a esa reducción. \n6. Espacios de datos lingüísticos y principios FAIR\nEn 2017, The Economist\n 100 publicó un artículo titulado \u00abEl \nrecurso más valioso del mundo ya no es el petróleo, sino los \ndatos\u00bb. Desde su publicación, el tema ha generado un gran interés y \u00abLos datos son el nuevo petróleo\u00bb se ha convertido en un eslogan muy difundido.\nLa Comisión Europea, en el documento Una estrategia \neuropea de datos\n 101, apuesta por el desarrollo de un mercado \n\u00fanico europeo de datos en el que los productos, los servi-cios y las aplicaciones basadas en datos, ya procedan de fuentes p\u00fablicas o privadas, respeten las normas y valores europeos de protección de datos personales, derechos fun-\n100 \u00abThe World's Most Valuable Resource Is No Longer Oil, but \nData\u00bb. The Economist\n, 2017. https://www.economist.com/lea-\nders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-\nbut-data\n101 Comisión Europea (2020): Comunicación de la Comisión al \nP\narlamento Europeo, al Consejo, al Comité económico y social europeo y \nal Comité de las regiones. Una estrategia europea de datos. https://eur-lex. europa.eu/legal-content/ES/TXT/PDF/?uri=CELEX:52020D-C0066&from=EN\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   7434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   74 11/5/23   12:0411/5/23   12:0475damentales, seguridad, ciberseguridad y protección de con-\nsumidores, así como la legislación de sectores específicos. \nEsta estrategia se basa en cuatro pilares: el primer pilar \npropone crear un marco de gobernanza intersectorial para el acceso a los datos y su utilización, que tenga en cuenta las especificidades de cada sector y las de los Estados miembros; el segundo pilar es la inversión de dos mil millones de euros para desarrollar infraestructuras, herramientas de compar-tición de datos y arquitecturas de gobernanza en un gran proyecto de alto impacto europeo; el tercero refuerza los derechos que tienen las personas sobre los datos que ellas generan para que puedan decidir qué hacer con sus datos personales; mientras que el cuarto pilar identifica sectores estratégicos y de interés p\u00fablico en Europa. \nPara incrementar la soberanía tecnológica en espacios de \ndatos, el actual programa marco de investigación de la Unión Europea ya está avanzando en dos direcciones. La primera se encamina a definir una gobernanza participativa que establezca de forma clara las normas de acceso a los datos y su uso por parte de todos los actores. La segunda dirección persigue desarrollar las infraestructuras y las tec-nologías que implementen dicha gobernanza para fomen-tar la creación de repositorios de datos europeos bajo los principios FAIR que describiré a continuación. La gober-\nnanza de la estrategia también contempla que las organiza-ciones que aporten datos puedan obtener, de alguna mane-ra, un rendimiento por ello. Puede ser en forma de un mayor acceso a los datos de otros colaboradores, o bien el acceso a los resultados analíticos del repositorio de datos y a servicios como los de mantenimiento predictivo o cáno-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   7534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   75 11/5/23   12:0411/5/23   12:0476nes de licencia. Los espacios de datos fomentan un ecosis-\ntema de innovación abierta que genera valor y nuevas opor-tunidades de negocio y a la vez dinamizan la transformación digital del sector p\u00fablico y privado. \nDesde una perspectiva tecnológica, en los espacios de \ndatos cada proveedor mantiene siempre la soberanía sobre sus datos, decide si desea almacenarlos en un lugar central o mantenerlos en su entidad, define sus políticas de acceso y las condiciones de uso, puede compartir los datos de for-ma gratuita o remunerada, y puede cambiar las condicio-nes de uso a lo largo del tiempo. Además, estos espacios de datos proporcionan una infraestructura federada y servicios específicos de acceso a datos interconectados, con servicios interoperables, estandarizados y consensuados, todo ello conforme a la legislación europea en principios éticos y de protección de datos personales. También incluyen herra-mientas avanzadas para disminuir los tiempos y recursos económicos requeridos al construir nuevos sistemas de analítica de datos e inteligencia artificial. \nPara alcanzar este objetivo de construir un mercado \u00fani-\nco de datos europeos, uno de los problemas más acuciantes es que los datos de los espacios europeos están en diferentes idiomas; es decir, son multilingües. Por este motivo, junto a los más de diez espacios de datos de dominios específicos relacionados, entre otros, con el sector p\u00fablico, la sanidad, las finanzas, la movilidad, el turismo, la energía, la indus-tria, los medios, la agricultura, la seguridad y la aplicación de la ley, la Comisión ha incluido un espacio de datos para compartir recursos lingüísticos \n monolingües y multilin-\ngües. En España, la Estrategia Nacional de I\nnteligencia Ar-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   7634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   76 11/5/23   12:0411/5/23   12:0477tificial del año 2020, que está alineada con esta estrategia \neuropea, propone, en el eje estratégico tercero, el desarrollo de plataformas de datos e infraestructuras tecnológicas que den soporte a la inteligencia artificial y que incluyan recur-sos, entre otros, de la siguiente naturaleza: datos, corpus, lexicones, ontologías y modelos. Añade también que se debe asegurar la gestión eficiente y la gobernanza del uso de los datos seg\u00fan los principios de interoperabilidad, integri-dad, fiabilidad, calidad y legalidad. \nLos sistemas de inteligencia artificial consumirán los da-\ntos sectoriales de los espacios de datos junto con los mate-riales monolingües y multilingües para, por ejemplo, inte-grar datos que están en diferentes idiomas, realizar analítica de los datos integrados y así presentar los resultados en mul-titud de idiomas. Además, el procesamiento de lenguaje natural puede ayudarse de los espacios de datos para, por ejemplo, generar terminologías multilingües en sectores es-pecíficos y mejorar los procesos de traducción automática. \nAnteriormente he mencionado el término FAIR. ¿Qué \nsignifica? Un artículo de la revista Scientific Data propuso en el año 2016 el término FAIR\n 102, acrónimo del inglés \nfindable, accesible, interoperable and reusable, para calificar a los datos que habrían de satisfacer cuatro propiedades: ser fácilmente localizables, ser universalmente accesibles, estar operativos en diferentes contextos y con diferentes propó-\n102 M. Wilkinson, M. Dumontier, I. Aalbersberg et al. (2016): \n\u00abThe FAIR Guiding P\nrinciples for Scientific Data Management and \nStewardship\u00bb. Sci Data, 3, 160018. https://doi.org/10.1038/sda-\nta.2016.18\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   7734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   77 11/5/23   12:0411/5/23   12:0478sitos. La finalidad de estos principios es que las aplicaciones \ninformáticas y de inteligencia artificial reutilicen y compar-tan los conjuntos de datos directamente, sin necesidad de la intervención humana. Para ello, los datos han de estar organizados y descritos de manera estandarizada, en forma-tos digitales, de forma clara y sin ambigüedad. Estos prin-cipios han ganado rápidamente reconocimiento unánime, tanto por parte de la comunidad científica como de las au-toridades p\u00fablicas internacionales. \nProfundicemos en el significado del acrónimo FAIR \npara los materiales lingüísticos.\nComencemos con la f de FAIR, que se refiere a que los \ndatos deben localizarse fácilmente. Si la Academia decide publicar materiales lingüísticos para que la inteligencia ar-tificial o cualquier otro tipo de programa informático los utilice, quienquiera que los busque debería encontrarlos sin dificultad. Los materiales publicados en el espacio de datos lingüísticos tendrían que ser tan visibles en el mundo digital como un diccionario en una biblioteca. Así pues, al igual que un bibliotecario se sirve de los tejuelos y los códi-gos de los libros para encontrar un diccionario determina-do, se necesita también describir los materiales lingüísticos meticulosamente mediante descriptores que las máquinas entienden y que reciben el nombre de metadatos (palabra que no viene explícitamente en el Diccionario de la lengua española, pero sí sus formantes meta- y dato\n 103, y que el \ndiccionario de Oxford define como \u00abinformación que des-\n103 Real Academia Española: https://dle.rae.es/meta-?m=form y \nhttps://dle.rae.es/dato#Bskzsq5 \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   7834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   78 11/5/23   12:0411/5/23   12:0479cribe otra información para ayudar a comprenderla o utili-\nzarla\u00bb). Los metadatos son como las fichas bibliográficas de los libros: consignan los datos principales de una obra, ya sea un libro, una página web o una base de datos en la nube. Los metadatos tienen asignados unos identificadores globales, uniformes y perdurables que reciben el nombre de URI, acrónimo del inglés Uniform Resource Identifier. De la misma forma que las fichas bibliográficas se almace-naban en grandes archivos, los metadatos de los recursos se almacenan y registran en repositorios que posibilitan el descubrimiento de dichos recursos a los buscadores de In-ternet; es decir, los metadatos son visibles tanto para las máquinas como para los humanos que buscan informa-ción. Con este fin, las Administraciones p\u00fablicas europeas han respaldado ontologías como DCAT\n 104 o DCAT-AP 105 \npara catalogar sus recursos.\nLa a de FAIR alude a la accesibilidad; en otras palabras, \na la garantía de que cualquiera pueda acceder a un conjun-to de datos sin que exista la necesidad de utilizar tecnolo-gías privativas para su uso. Los metadatos que describen el recurso siempre están disponibles de manera gratuita. La accesibilidad no excluye el cobro por acceso; esto es, los datos que esta institución decidiera publicar podrían ofre-cerse bien de manera gratuita o remunerada, o bien con di -\nferentes tarifas, pero siempre utilizando protocolos y for-\n104 World Wide Web Consortium (2023): Data C atalog Vocabu-\nlary (DCAT). https://www.w3.org/TR/vocab-dcat-3/\n105 Secretaría de Estado de Digitalización e Inteligencia Artificial: \nDCAT-AP\n. https://datos.gob.es/es/doc-tags/dcat-ap\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   7934439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   79 11/5/23   12:0411/5/23   12:0480matos abiertos. Si los datos no son gratuitos, el protocolo \npuede incluir procedimientos de autentificación y autori-zación de usuarios, seg\u00fan indiquen las licencias estable\n-\ncidas para cada material, uso y tipo de usuario. \nLa i de \nFAIR indica interoperabilidad, un término po-\npular en el ámbito informático que describe el entendi-miento entre sistemas digitales heterogéneos en el plano léxico, sintáctico y semántico. Datos interoperables son aquellos que pueden utilizarse en diferentes contextos, y por aplicaciones informáticas dispares, que han sido cons-truidas con propósitos diferentes. La interoperabilidad ha-bilita la integración de datos y su intercambio entre siste-mas sin intervención humana. Es ciertamente una gran apuesta para cualquier sistema. Mejorar la interoperabili-dad de un conjunto de datos es un reto notable, que re-quiere superar dificultades técnicas y alcanzar consensos en la terminología —y no siempre son plenos—. Algunos ex-pertos en inteligencia artificial, organismos de estandariza-ción e incluso legisladores han consensuado el uso de des-criptores que abarcan diferentes aspectos en su descripción para garantizar el intercambio de los datos. El Consorcio de la World Wide Web, que es el organismo que publica \nestándares de la Web, ha propuesto las ontologías PROV-O\n 106 \ny ODRL 107. PROV-O se utiliza para identificar al creador \n106 World Wide Web Consortium (2013): PROV -O: The PROV \nOntology. https://www.w3.org/TR/prov-o/\n107 V. Rodríguez-Doncel and P . Labropoulou (2015): \u00abDigital Repre-\nsentation of Rights for Language Resources\u00bb. I\nn Proceedings of the 4th \nWorkshop on Linked Data in Linguistics: Resources and Applications, 49-58.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   8034439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   80 11/5/23   12:0411/5/23   12:0481y responsable del recurso; conocer la reputación del provee-\ndor del dato, cómo se ha generado, procesado, posiblemen-te combinado con otros datos y, finalmente, validado; en definitiva, estas descripciones son imprescindibles para au-mentar la confianza del usuario final en dicho recurso. La ontología ODRL, por otra parte, describe los derechos, condiciones y licencias de uso de los datos, que pueden ser gratuitos o de pago por uso. En el caso concreto de los da-tos lingüísticos, y bajo el paraguas del citado consorcio, un grupo de expertos en inteligencia artificial y lingüística ha trabajado en la ontología Ontolex\n 108, un modelo simbólico \ncon el que se representa la información de los recursos lin-güísticos —y que habilita para detallar cada entrada del diccionario— de forma \u00abentendible\u00bb para la máquina, y que es interoperable con otros recursos que sigan este mis-mo formato en español o en otras lenguas. Es decir, se trata de transformar la información gramatical, léxica o semán-tica que tienen las entradas en un diccionario y describirlas en este modelo. Otra ontología alternativa para explicitar la procedencia de los datos es Dublin Core\n 109.\nFinalmente, la r de FAIR significa reutilizable. Los datos \nque siguen los principios FAIR se pueden integrar con otros datos en otros escenarios, con o sin intervención humana. Por este motivo, las licencias de uso han de ser claras, inte-ligibles y disponibles en formatos digitales. \n108 World Wide Web Consortium (2016): Lexicon model for onto-\nlogies: community report. https://www.w3.org/2016/05/ontolex/\n109 World Wide Web Consortium (2005): DublinCore. https://\nwww.w3.org/wiki/D\nublinCore\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   8134439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   81 11/5/23   12:0411/5/23   12:0482En cierta manera, los principios FAIR ya se habían ma-\nnifestado antes, en un progreso tecnológico callado, pero \ndel cual yo he sido testigo: la creación y desarrollo de una nube de datos enlazados. De nuevo, Sir Tim Berners-Lee propuso en 2006 una nueva forma de publicar datos en la Web, que conectara los datos, no los documentos, como en una malla de datos enlazados. Si en la Web los usuarios navegan a través de los enlaces de los documentos, en la nueva Web los usuarios clican en los datos para navegar y así obtener nuevos datos. Desde entonces muchas institu-ciones gubernamentales y empresas privadas han descrito sus datos con ontologías, los han publicado en portales de datos y los han conectado con datos de otras instituciones para así incrementar su interconexión. Lo que se ha conse-guido es una nube de datos enlazados que las máquinas pueden explorar con la ayuda de algoritmos. Esta nube de datos puede llegar a ser un bien abierto y valioso, como la Web por la que navegamos cada día. Los diccionarios, glo-sarios, tesauros y corpus también tienen su lugar en ella, si se transforman a los estándares del antes mencionado Con-sorcio de la World Wide Web y se enlazan con otros datos lingüísticos. A esta parte de la nube se la conoce como nube de datos lingüísticos enlazados\n 110.\n110 J.P . McCrae (2018): Linguistic Linked Open Data. https://lin-\nguistic-lod.org/\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   8234439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   82 11/5/23   12:0411/5/23   12:04837. Inteligencia artificial y lengua española\n7.1. Situación de partida de la informática en la RAE La lengua y el habla evolucionan continuamente, pero exis-\nten acontecimientos políticos, económicos, tecnológicos y culturales, entre otros, que originan nuevas palabras y ex-presiones que aceleran este cambio. La popularización de Internet y de la Web, unida a los avances sin precedentes de\u00a0la inteligencia artificial, está propiciando cambios tanto en\u00a0la lengua en general como en los diferentes lenguajes de especialidad. El lingüista David Crystal\n 111 señala que \u00abla \nred ofrece un nuevo entorno para el lenguaje, más dinámi-co que la escritura tradicional y más permanente que el discurso tradicional y, de \n hecho, ha tenido como conse-\ncuencia un cambio revolucionario en el ámbito de la lin\n-\ngüística\u00bb.\nLas tecnologías digitales favorecen y fomentan la rápida \naparición y adopción de siglas, acrónimos, tecnicismos e incluso palabras \u00aben crudo\u00bb, que adoptamos tal cual del inglés y que surgen, por ejemplo, en documentos técnicos, revistas especializadas y ofertas de empleo, y han entrado en nuestra vida diaria desde hace ya mucho tiempo\n 112. El \n111 D. Crystal: El lenguaje, las lenguas e Internet. https://www.euska-\nra.euskadi.eus/contenidos/informacion/ar\ntik25_1_crystal_08_06/es_\ncrystal/adjuntos/David-Crystal-cas.pdf \n112 Torres Quevedo mencionaba en su discurso: \u00abAdemás, los neolo-\ngismos propiamente técnicos no aparecen de la misma manera, no son\n \ncreaciones arbitrarias que responden a una necesidad claramente perci-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   8334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   83 11/5/23   12:0411/5/23   12:0484primer diccionario detallado de la terminología informáti-\nca 113 se elaboró en el año 1994. ¿Quién se acuerda, por \nejemplo, de que bit es un acrónimo formado a partir del término binary digit, o de que PC se refiere a un computa-\ndor y no a un partido político? Así, la sigla GAFAM alude abreviadamente a las empresas americanas Google, Ama-zon, Facebook, Apple y Microsoft; y BATX  se refiere a las \nempresas tecnológicas Chinas Baidu, Alibaba, Tencent y Xiaomi. En ocasiones, la sigla utilizada en documentos téc-nicos procede de la lengua inglesa, a pesar de que ya exista una traducción en español para el término inglés, como es el caso de las memorias RAM y ROM. Por ejemplo, no tenemos una sigla para la Internet de las cosas y se sigue escribiendo IoT, del inglés Internet of Things. Otras siglas \nque aparecen en documentos técnicos, como la ya mencio-nada FAIR, no se traducen al español. Son numerosos los \nejemplos que nos han quedado desde la entrada de la infor-mática. Muchos referidos a lenguajes de programación: COBOL, FORTRAN, ALGOL. Algunos, como BASIC, han mantenido la forma, aunque en ocasiones se haya adaptado la pronunciación al genio de nuestra lengua. Los \nbida. Los hombres dedicados a la técnica no saben, por lo com\u00fan, de \nraíces griegas, ni pueden esperar a que otros les den ya formadas las pa-labras que necesitan para entenderse; sus neologismos nacen en el cam-po, en el taller, en la fábrica, en el arsenal, en todas partes donde hay obreros; también son debidos con frecuencia a extranjeros, que los apor-tan al aportar nuevas artes o nuevos procedimientos: son vulgarismos o barbarismos, que se extienden y se imponen a veces muy rápidamente\u00bb. \n113 G. Aguado de Cea (1994): Diccionario comentado de terminolo-\ngía infor\nmática. Madrid: Paraninfo.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   8434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   84 11/5/23   12:0411/5/23   12:0485neologismos —generalmente anglicismos— son tan nu-\nmerosos que citaré solo algunos más actuales, como big data, blockchain, blogger, phishing, doodle, hashtag, podcast, cuyas traducciones al español a\u00fan no han sido totalmente incorporadas a nuestra lengua, aunque en alg\u00fan caso se han adaptado a las normas de nuestro sistema lingüístico, como ha ocurrido con bloguero. Encontramos también inconta-bles tecnicismos o términos técnicos, como metadato, api-ficar, aprendizaje profundo, red de neuronas, refactorizar, token o tokenización, que están presentes en español, pero \nsin delimitación semántica en muchos casos. En cambio, otros, como tuitear y su conjugación, ya han sido incorpo-rados al diccionario de la Academia. \nEsta constante aparición de nuevos términos se ha visto \nfavorecida con la Web 2.0. Recordemos que, mientras en la Web 1.0 no existía interacción entre escritores y lectores, pues las instituciones generaban contenidos estáticos y las personas los leían, en la Web 2.0\n 114 los usuarios generan \ncontenidos de forma colaborativa, dinámica, descentraliza-da, a través de cualquier dispositivo y en m\u00faltiples idiomas y formatos que combinan texto, voz, imágenes y vídeos. Esta participación tan activa se hace con poca supervisión \n114 Bajo el título \u00abYes, You. You Control de Information Age\u00bb o, tra-\nducido, \u00abSí, t\u00fa. T\n\u00fa controlas la era de la información. Bienvenido a tu \nmundo\u00bb, la revista Time, que es famosa por seleccionar a la persona del \naño, eligió como personaje del año 2006 a los millones de personas que de forma anónima contribuían a generar contenidos en la Web en foros, blogs y plataformas como YouT ube, MySpace, Facebook, Wiki-pedia y otras plataformas que existían en aquellos años.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   8534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   85 11/5/23   12:0411/5/23   12:0486de la forma y del fondo de lo que se publica, pues no existe \nun editor que fije el estilo y que introduzca modificaciones en la ortografía, la puntuación y la gramática. \nTambién, en los asistentes conversacionales y en las apli-\ncaciones de mensajería instantánea, las personas se comu-nican con sistemas de inteligencia artificial de forma oral o escrita. Observamos con frecuencia que, en las comunica-ciones privadas, las \n personas establecen convenciones pro-\npias para el canal \ncuando emplean aplicaciones de mensaje-\nría instantánea como WhatsApp, WeChat o Telegram, y transforman el lenguaje escrito en aras de la brevedad y la economía de símbolos. Adapto textos de Gregorio Salvador en su discurso de entrada para mostrar como la letra q se utiliza en aplicaciones de mensajería sin su vocal de escolta (la u): cuando se opta por la letra q para referirse a que; o \ncuando se sustituye la letra q por la letra k o se prefiere la letra k para referirse a que o qui; o bien cuando se truecan \nlas letras en símbolos utilizando xk o xq  en lugar de las ex-\npresiones porque o por qué. Podría argumentarse que la ra-\nzón subyacente es el ahorro de letras en mensajes cortos, pues plataformas como T witter inicialmente limitaban los mensajes a 140 caracteres, aunque hace unos años amplia-ron ese límite a 280 caracteres, y a los suscriptores de T wi-tter Blue les permiten tuits de hasta 4000 caracteres. Otros fenómenos frecuentes son la escritura fonológica, el uso de logogramas, la escritura continua (sin espacios) o sin acen-tos, la abreviación, así como la combinación con voces in-glesas a veces o el olvido de las may\u00fasculas.\n\u00abDesde su fundación, en 1713, la labor de la Real Aca-\ndemia Española ha estado orientada hacia la elaboración \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   8634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   86 11/5/23   12:0411/5/23   12:0487de\u00a0los tres grandes códigos que regulan la norma de una \nlengua: el buen uso del léxico, el dominio de las reglas \n gramaticales y la correcta escritura. Por ello, los objetivos académicos, r\nenovados constantemente a lo largo de los si-\nglos, han quedado recogidos en tres publicaciones emble-máticas\n 115: el Diccionario, la Gramática y la Ortografía\u00bb. La \n115 Las obras académicas (https://www.rae.es/obras-academicas) \nmás destacadas disponibles en la web de la corporación son: \n Diccionario de autoridades (año 1726-1739); Diccionario de \nla lengua española (vigésimotercera edición sexta actualización, de 2022); los diccionarios académicos accesibles en la aplica\n-\nción Mapa de Diccionarios (ediciones de los años 1780, 1817, 1884, 1925, 1992 y 2001); Nuevo tesoro lexicográfico, que re\u00fane toda la lexicografía académica, desde el Diccionario de autoridades hasta la 21.\u00aa edición del diccionario usual; la ver-sión del diccionario usual que permite b\u00fasquedas complejas, denominada Diccionario avanzado (año 2018, 2021); Diccio-nario histórico de la lengua española (año 2013); Diccionario panhispánico del español jurídico (año 2022); Diccionario del español jurídico (año 2016); Diccionario de americanismos (año 2010); Diccionario panhispánico de dudas (año 2005); Diccionario esencial de la lengua española (año 2006); Diccio-nario práctico del estudiante (años 2005 y 2016); Tesoro de los diccionarios históricos de la lengua española (2021).\n\n Corpus del Español del Siglo XXI (CORP\nES), con 350 millo-\nnes de formas; Corpus del Diccionario Histórico de la Lengua Española (CDH), con 355 millones de formas; Corpus de Re-ferencia del Español Actual (CREA), con 160 millones de for-mas; el Corpus Diacrónico del Español (CDH), con 250 millo-nes de formas, y el Corpus Científico y Técnico (disponible en Enclave de Ciencia), con 112 millones de formas. \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   8734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   87 11/5/23   12:0411/5/23   12:0488década de los noventa fue decisiva para la corporación. En \n1993, los estatutos fundacionales se actualizan 116, rejuvene-\ncen y modernizan para afrontar nuevas situaciones y \u00abvelar por que los cambios que experimente la lengua española en su constante adaptación a las necesidades de sus hablantes no quiebren la esencial unidad que mantiene en todo el ámbito hispánico\u00bb. Cabe recordar que, en estos años, los libros se leían en papel, la Web acababa de crearse y la in\n-\nteligencia artificial estaba en su segundo invierno. Apar\ne-\ncieron en esas fechas los primeros reproductores de m\u00fasica MP3 en sustitución de los CD y DVD, surgen las televisio-nes privadas y comenzaba a usarse el correo electrónico. A\u00fan no existían la Web 2.0 ni las aplicaciones de mensaje-ría instantánea. \nCoincidiendo con la elaboración de los nuevos estatutos, \nel director don Fernando Lázaro Carreter y el vicedirector don Ángel Martín Municio impulsaron en la Academia un ambicioso plan para incorporar las nuevas tecnologías infor-máticas a sus actividades. El primer proyecto informático de \n Glosario de términos gramaticales, que pretende contribuir a \nla unificación de la terminología entre los docentes de gra-mática en el mundo hispanohablante; G\nramática y ortografía \nbásicas de la lengua española; Nueva gramática básica; los tres vol\u00famenes de la Nueva gramática de la lengua española, dedicados a la fonética y fonología, la morfología y la sinta-xis, y su versión manual; y Las voces del español. Tiempo y espacio.\n116 Real Decreto 1109/1993, de 9 de julio, por el que se aprueba los \nEstatutos de la Real A\ncademia Española. https://www.boe.es/buscar/\ndoc.php?id=BOE-A-1993-19893\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   8834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   88 11/5/23   12:0411/5/23   12:0489la corporación, ampliamente documentado por don Alonso \nZamora Vicente en su obra, tenía cuatro objetivos 117: el pri-\nmero era crear la infraestructura informática; el segundo, desarrollar el sistema de gestión de diccionarios para infor-matizar la vigésima primera edición del DRAE; el tercero, crear una base de datos de materiales léxicos y lexicográficos, y finalmente, el cuarto, confeccionar un gran banco de da-tos del español estructurado en dos grandes secciones, una diacrónica y otra sincrónica, que han dado lugar al Corpus Diacrónico del Español (CORDE) y al Corpus de Referen-cia del Español Actual (CREA). El uso de las tecnologías informáticas tradicionales ha puesto a disposición de los le-xicógrafos y estudiosos nuevas herramientas que han facili-tado su labor. Con ellas, la Academia ha elaborado numero-sos materiales lingüísticos de calidad que están disponibles en la web de la corporación\n 118, en Enclave RAE 119 y en En-\nclave de Ciencia 120. En diciembre de 2022, la dirección aca-\ndémica de la corporación tomó la decisión, a mi entender muy acertada, de facilitar un \u00fanico punto de acceso a los materiales lingüísticos desde su página institucional. Las aplicaciones de consulta que están disponibles en estas pági-nas webs acceden a los materiales lingüísticos mediante in-\n117 A. Zamora Vicente (2015): La Real Academia Española: 300 \naños. E\nd. Real Academia Española-Fundación María Cristina Masaveu \nPeterson.\n118 https://www.rae.es\n119 Enclave. https://enclave.rae.es/\n120 Enclave de ciencia. https://enclavedeciencia.rae.es/contenidos/\ninicio\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   8934439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   89 11/5/23   12:0411/5/23   12:0490terfaces de programación de aplicaciones privativas de la \ncorporación. Cualquier hablante puede realizar numerosas consultas en la web a los recursos disponibles de forma gra-tuita. Pero las respuestas se presentan generalmente a través de páginas webs y solo en limitadas ocasiones se pueden exportar al formato PDF . En definitiva, actualmente, los materiales lingüísticos de la RAE no pueden ser utilizados por sistemas de inteligencia artificial externos a la Academia sin intervención humana porque no están en el formato que usa la inteligencia artificial.\n7.2. Rumbo al nuevo mundo de la inteligencia artificial \nCasi treinta años después de la incorporación de la infor-\nmática en la Real Academia, el actual director, don Santia-go Muñoz Machado, idea e impulsa un nuevo plan para introducir la inteligencia artificial en la institución, un plan que se materializa en el proyecto Lengua Española e Inteli-gencia Artificial (LEIA)\n 121. LEIA tiene como fin principal \ncuidar el uso de un correcto español en los medios tecnoló-gicos y así evitar que se pierda la unidad que permite que más de 585 millones de personas puedan comunicarse en nuestra lengua sin dificultades. Así, en el año 2019, en el decimosexto\n 122 Congreso de la Asociación de Academias de \nla Lengua Española (ASALE), el director de la corporación \n121 Real Academia Española (2020): ¿Qué es LEIA? https://www.\nrae.es/noticia/que-es-leia \n122 Real Academia Española: Lengua española e inteligencia artifi-\ncial. https://www.rae.es/leia-lengua-espanola-e-inteligencia-ar\ntificial \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   9034439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   90 11/5/23   12:0411/5/23   12:0491y presidente de la ASALE declaraba: \u00abEstamos en un mo-\nmento crucial en el que tenemos que hacer algo que hicie-ron nuestros antecesores del siglo xviii (con los humanos): normativizar la lengua de las máquinas y de la inteligencia artificial. Su lengua tiende a diversificarse y hay que tomar medidas. La IA habla inglés, fundamentalmente, y tene-mos que procurar que, poco a poco, el español ocupe una posición eminente en el mundo de la IA, pero también en el mundo general de las redes\u00bb.\nUn año más tarde, en diciembre de 2020, el Gobierno \nanunciaba la Estrategia Nacional de Inteligencia Artifi-cial\n 123 (ENIA). La ENIA incluía entre sus objetivos estraté-\ngicos liderar, a nivel mundial, el desarrollo de herramien-tas, tecnologías y aplicaciones para la proyección y uso de la lengua española en los ámbitos de aplicación de la inteli-gencia artificial. La ENIA presenta treinta medidas agrupa-das en seis ejes estratégicos, que paso a enumerar y que es-tán relacionados con el impulso de la investigación, la innovación y el desarrollo tecnológico; el impulso de las capacidades digitales; la construcción de plataformas de datos e infraestructuras tecnológicas; la integración de la inteligencia artificial en las cadenas de valor para transfor-mar el tejido económico; el uso de la inteligencia artificial en las Administraciones p\u00fablicas, y, finalmente, el desarro-llo de un sandbox regulatorio, o \u00abespacio controlado de \n123 Ministerio de Asuntos Económicos y T ransformación Digital \n(2020). ENIA: Estrategia N\nacional de Inteligencia Artificial. https://\nwww.lamoncloa.gob.es/presidente/actividades/Documents/2020/\nENIA2B.pdf \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   9134439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   91 11/5/23   12:0411/5/23   12:0492pruebas de la regulación\u00bb, que esté alineado con el regla-\nmento específico 124 de la Comisión Europea para proteger \nlos derechos de los ciudadanos y que trate sobre el diseño, la implementación y el uso de los sistemas de inteligencia artificial en todos los países de la Unión. \nSi bien en todos los ejes de la ENIA están presentes las \ntecnologías del lenguaje y la nueva economía de la lengua, la ENIA incluye dos medidas específicas, de gran calado, para liderar los avances y el uso de la lengua española por parte de los sistemas de inteligencia artificial. En la medida decimotercera, se propone la creación de espacios com\n-\npartidos de datos \nsectoriales e industriales y de repositorios \nde datos descentralizados y accesibles que faciliten la crea-ción de servicios de valor añadido, basados en infraestruc-turas de datos. Los datos sectoriales de la lengua se encuen-tran en sus diccionarios, corpus, catálogos de entidades nombradas, terminologías especializadas, anotaciones, gra-máticas, etc. La otra medida de la ENIA, la decimocuarta, contempla dos actuaciones: el impulso al Plan Nacional de Tecnologías del Lenguaje y la incorporación de la ya men-cionada iniciativa de la lengua española en la inteligencia artificial (LEIA). Para finalizar, la Estrategia de España Di-gital 2026\n 125 incluye dos actuaciones relevantes, que están \n124 European Commission (2021): Proposal for a Regulation Laying \nDown \nHarmonized Rules on Artificial Intelligence. https://digital-strate-\ngy.ec.europa.eu/en/library/proposal-regulation-laying-down-harmo-\nnised-rules-artificial-intelligence\n125 Ministerio de Asuntos Económicos y T ransformación Digital: \nEstrategia de E\nspaña Digital 2026. https://espanadigital.gob.es/ \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   9234439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   92 11/5/23   12:0411/5/23   12:0493directamente relacionadas con la inteligencia artificial y la \nlengua: la primera es el nuevo 126 Plan Nacional de Tecnolo-\ngías del Lenguaje; la segunda se refiere al PERTE Nueva Economía de la Lengua y a sus catorce proyectos tractores, entre los que se encuentra LEIA. Para llevar a cabo estas actuaciones, se utilizarán los recursos europeos de los fon-dos de recuperación, transformación y resiliencia. \nEn LEIA se vinculan, con gran acierto, dos elementos \nsin establecer relación jerárquica entre ellos, lo cual permite reflexionar en dos direcciones, al crearse un círculo virtuo-so que beneficia tanto a la lengua española como a la inte-ligencia artificial, al tiempo que ayuda a difundir los cri\n-\nterios de propiedad y corrección en el mundo digital. \nEn este contexto de la N\nueva Economía de la Lengua en \nEspañol, caben hacerse algunas preguntas: ¿cómo puede la RAE normativizar la lengua de las máquinas cuando son producto de una inteligencia artificial?; ¿ayudan las tecno-logías digitales y la inteligencia artificial a identificar las formas no recogidas en el diccionario, a sostener sólida-mente el sistema lingüístico del español y a divulgar las \n126 En el año 2015, España fue el primer país europeo en crear un \nPlan de Impulso de las \nTecnologías del Lenguaje para fomentar el de-\nsarrollo del procesamiento del lenguaje natural, la traducción automá-\ntica y los sistemas conversacionales en lengua española y lenguas coo-ficiales. El plan ya vislumbró el impacto de las tecnologías del lenguaje en la internacionalización de las empresas e instituciones y en la mejo-ra de la cooperación con la comunidad iberoamericana. https://www.plantl.gob.es/tecnologias-lenguaje/PTL/Paginas/plan-impulso-tecno-logias-lenguaje.aspx \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   9334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   93 11/5/23   12:0411/5/23   12:0494normas de la lengua española tal como han ido consolidán-\ndose en el correr de los siglos?; ¿qué puede hacer la Acade-mia para que sus materiales, herramientas, tecnologías y aplicaciones sean de utilidad para la proyección y uso de la lengua española en el ámbito digital, de la inteligencia arti-ficial y del procesamiento del lenguaje natural en toda la comunidad hispanohablante?; ¿cómo puede ayudar la inte-ligencia artificial a la Academia en su misión de observar el cambio que experimenta la lengua española? \nEstas preguntas me llevan a realizar un ejercicio de pros-\npectiva para identificar unos retos que impactarán en el uso del español en el mundo digital. Este ejercicio es arriesgado y complicado, debido a la dependencia de las grandes em-presas tecnológicas. Además, es atrevido, pues las actuacio-nes requieren de un ecosistema de innovación abierta que actualmente no existe en la nueva economía de la lengua en español. El primer reto sería conseguir que los materiales de la corporación se convirtieran en el material de referen-cia para los sistemas de inteligencia artificial en español; el segundo reto sería incorporar más técnicas de inteligencia artificial para mejorar la metodología y las herramientas in-formáticas que los lexicógrafos utilizan en sus actividades diarias, al generar los materiales lingüísticos de la corpora-ción; el tercer reto perseguiría observar, supervisar, verificar y, quizá también, certificar el uso del español en el mundo digital; y, finalmente, el cuarto reto plantearía la necesidad de generar un ecosistema de innovación abierta en el que los recursos de la Academia, con las licencias que corres-pondan, se convirtieran en el referente de la lengua españo-la en el espacio europeo de datos lingüísticos. \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   9434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   94 11/5/23   12:0411/5/23   12:04957.3.  Incorporar los materiales en el espacio europeo de datos \nlingüísticos\nConviene que las máquinas que usan la lengua española sean entr\nenadas con materiales lingüísticos fiables que ga-\nranticen una comunicación o un entendimiento efectivo y eficiente con sus interlocutores y usuarios. Por este motivo, se sugiere que la Academia analice si se adhiere a los princi-pios FAIR anteriormente explicados y que decida la forma \nen la que algunos de sus materiales, de forma centralizada o federada, con las licencias de uso que corresponda a cada material, pasen a formar parte del espacio europeo de datos lingüísticos.\nEl proyecto LEIA contempla publicar algunos materia-\nles de la Academia siguiendo estos principios, para así favo-recer el buen uso de la lengua española en el universo digi-tal y, especialmente, en el ámbito de la inteligencia artificial y de las tecnologías de la lengua española. \nLas actividades técnicas que habría que realizar pasan \npor construir una ontología para la corporación, transfor-mar los contenidos de algunos materiales, identificar sus licencias de uso en lenguaje \u00abcomprensible\u00bb por la máqui-na, proveer de API semánticas que faciliten el uso a los de-sarrolladores, publicar la ontología y los materiales para que los sistemas de inteligencia artificial los puedan utilizar sin intervención humana, e interconectar los datos de la Academia con los datos del espacio europeo de datos lin-güísticos que convenga. De esta forma, se creará una tupida red de conexiones, formando un grafo holístico de conoci-miento lingüístico en español. Con todo este proceso, los \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   9534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   95 11/5/23   12:0411/5/23   12:0496materiales de la Casa se posicionarán en el espacio de datos \nlingüísticos como la referencia para las aplicaciones digita-les en la nueva economía de la lengua, incluidas las que están a\u00fan por inventar. \n7.4. Modelos de lenguaje en español\nLos materiales de la corporación también se podrían desti-\nnar a la construcción de nuevos modelos de lenguaje en español o a la mejora de alguno de los existentes. Se descri-ben a continuación tres posibles escenarios de decisión.\nEn el primer escenario, la Academia debería valorar \nla\u00a0conveniencia de construir un modelo de lenguaje nuevo a partir de recursos propios como el CORPES XXI y el CREA, que no son fuentes especialmente voluminosas, pero sí bien cuidadas. Los corpus de la Academia tienen la ventaja de ser de calidad contrastada, con revisiones y ano-taciones hechas por expertos. Como ya se ha mencionado antes, el volumen de un corpus no está siempre relaciona-do\u00a0con el buen rendimiento del modelo de lenguaje, por más que la abundancia conduzca en principio a modelos más ricos y que el resultado de un modelo se obtenga por el conjunto de los datos, las técnicas de entrenamiento y la arquitectura utilizada. \nEn el segundo escenario, la Academia podría liderar la \nagregación de los corpus más representativos panhispáni-cos en aras de un mayor volumen, diversidad y representa-tividad de las muestras de lengua con las que se entrenan estos modelos. \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   9634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   96 11/5/23   12:0411/5/23   12:0497En el tercer escenario, la Academia debería analizar si la \nopción más conveniente es mejorar, en colaboración con \notras entidades, alguno de los modelos de lenguaje que ya existen para el español. \nEn los tres casos, para realizar un ajuste fino, se necesita-\nría aportar datos procedentes de grafos de conocimiento validados, en formatos y lenguajes computacionales estan-darizados y abiertos. De esta forma se compensaría el gran esfuerzo humano que supone la anotación de textos y de datos. \nEn esta carrera tecnológica que persigue obtener mode-\nlos más ricos y productivos, el español progresa detrás del inglés. No existe todavía una metodología sistemática para evaluar modelos de lenguaje en español para diferentes ta-reas. Se convierte, pues, en otro desafío el desarrollo de una metodología de evaluación comparativa de los mode-los de lenguaje de propósito general y específico en nuestra lengua.\n7.5. Gobernanza de los materiales lingüísticos\nPara que los materiales lingüísticos de la RAE puedan ser \nconsumidos masivamente y de forma sencilla, es preciso explicitar las licencias y modos de uso de cada uno de ellos, con terminologías y formatos estandarizados e inte-roperables que puedan ser procesados y \u00abentendidos\u00bb por los sistemas de inteligencia artificial. De esta manera se favorece el uso y el mercadeo entre máquinas, sin inter-vención humana, en el mundo digital. La definición de \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   9734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   97 11/5/23   12:0411/5/23   12:0498licencias en un lenguaje computacional se aplicaría no \nsolo a los materiales lingüísticos, sino también a cualquier recurso software que tuviera la Academia o que pudiera \ntener en un futuro. \nEntre los ejemplos de uso cabe mencionar algunos que \nno requerirían la intervención humana: una aplicación que necesita conocer la evolución de la ficha técnica de una palabra en el mapa de diccionarios, o el significado de un conjunto de palabras del diccionario jurídico, o un conjun-to de obras que comiencen por una preposición seguida de un artículo indeterminado y un sustantivo, o las anotacio-nes de un documento concreto en CORPES XXI. \nEn el caso de que la Academia decida adherirse al espa-\ncio europeo de datos lingüísticos, sería necesario gestionar con especial esmero la propiedad intelectual de sus mate-riales y establecer alianzas estratégicas con terceros. En cuanto a la estrategia de licenciamiento seleccionada para publicar los resultados, habrán de considerarse, entre otros, tres aspectos fundamentales:\nPor un lado, los materiales con los que se construyen \nlos corpus y los modelos de lenguaje son obras que pudie-ran estar protegidas por los derechos de autor. No obstan-te, de conformidad con el artículo 67 del Real Decreto ley 24/2021, de 2 de noviembre, se aplicaría en estos casos la excepción relativa a la minería de textos y de datos —tér-mino que procede del inglés data mining y que el diccio-nario académico define como \u00abproceso en el que se anali-zan grandes vol\u00famenes de datos con el fin de hallar patrones que expliquen su comportamiento en un con-texto determinado\u00bb—, a pesar de que los datos con los \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   9834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   98 11/5/23   12:0411/5/23   12:0499que se construyan los modelos no puedan ser distribui-\ndos, y siempre que se satisfagan las condiciones impuestas en la Directiva 2019/790\n 127 de derechos de autor en el \nmercado \u00fanico digital, que, en resumen, se refieren al uso para investigación y a su ejecución por organismos de in-vestigación e instituciones responsables del patrimonio cultural.\nPor otro lado, tampoco baladí, está la elección de las li-\ncencias con las que se publicarán los materiales lingüísticos generados, y que determinarán cuándo se utilizarían, quién podría hacerlo y cómo y para qué se destinarían. A modo de ejemplo, la licencia debería incluir bajo qué condiciones un diccionario, un corpus o un modelo se podrían modifi-car con datos sintéticos, o con datos de otros proveedores; si se podrían transmitir, reproducir, publicar, distribuir, re-distribuir, etc.; o si sería posible transferir parte del modelo para construir otro nuevo.\nFinalmente, el tercer aspecto atañe a que la licencia debe \nanticipar y, si fuera el caso, impedir posibles usos inapro-piados de los modelos de lenguaje, que no contarían con el respaldo de la Academia, y que se refieren a riesgos o daños de naturaleza ética y social que la utilización de los materia-les pudiera provocar. De esta forma, la licencia incluiría aspectos relacionados con el uso responsable de los mate-riales lingüísticos. \n127 Directiva 2019/790 del Parlamento Europeo y del Consejo sobre \nlos derechos de autor y derechos afines en el mer\ncado \u00fanico digital y por la \nque se modifican las Directivas 96/9/CE y 2001/29/CE (2019). https://\nwww.boe.es/doue/2019/130/L00092-00125.pdf \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   9934439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   99 11/5/23   12:0411/5/23   12:041007.6. Plataforma software para la inteligencia artificial\nLa web de la Academia, con más de veinte millones de con-\nsultas mensuales 128, ha sabido atraer a hispanohablantes de \ntodo el mundo, procedentes principalmente de España, Mé-xico, Argentina, Per\u00fa y Colombia, en este orden, y de mu-chos otros países. Las personas conocen los materiales de la corporación, pero ahora toca que las grandes empresas tecno-lógicas y las empresas de base tecnológica comiencen a em-plearlos en sus desarrollos. Para que esto sea posible, se requie-re una transformación que facilite que las máquinas usen los materiales de la RAE directamente, sin intervención humana. \nDesde los tiempos del CREA, la corporación dispone de \ntecnología lingüística propia que le permite el procesa-miento y la anotación de sus corpus, y también dar soporte a los diferentes proyectos lexicográficos: segmentadores de texto, lexicones computacionales, redes de sinónimos, morfologías computacionales y lematizadores, etiquetado-res de clases de palabras y reconocedores de nombres pro-pios, entre otras tecnologías. \nLa Academia debería actualizar estas tecnologías infor-\nmáticas para que incorporen las técnicas más modernas, para que sus materiales sean interoperables en el espacio europeo de datos lingüísticos y con los espacios de datos sectoriales, y para que su software sea fácilmente accesible por sistemas software externos a la Academia. Con este fin \n128 Real Academia Española: Traffic and Engagement Analysis. \n https://www.similarweb\n.com/website/rae.es/#traffic (dato de marzo \nde 2023).\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   10034439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   100 11/5/23   12:0411/5/23   12:04101se propone la necesidad de construir, en una primera etapa, \nuna nueva plataforma que proporcione a desarrolladores de software externos unas interfaces de programación de apli-caciones (API) basada en estándares, muy detallada y bien documentada, con servicios básicos de acceso, que permi-tan la recuperación y la consulta de los materiales en for-matos propios de la casa, o siguiendo los principios FAIR o con las tecnologías de los modelos de lenguaje. Se ha de favorecer la interoperabilidad máquina-máquina para que los investigadores, los gigantes tecnológicos, las empresas de base tecnológica, las empresas dedicadas a la enseñanza de la lengua española y las empresas proveedoras de las interfaces de acceso a los modelos de lenguaje —por ejemplo siguiendo el modelo de la empresa estadounidense Hugging Face\n 129— \naprovechen los materiales de la Academia para construir nuevas aplicaciones.\nAdemás, en una segunda etapa, habría que pensar si la \nplataforma debería abrir otros servicios más avanzados, quizá relacionados con la anotación y la clasificación de textos; la verificación ortográfica, léxica, gramatical o de estilo; la limpieza o curado de corpus; la verificación y la validación de los textos en español que se destinan a cons-truir los sistemas de preguntas y respuestas, y otros relacio-nados con el uso del modelo Ontolex en la transformación de los diccionarios al formato de datos enlazados. \nLos datos y servicios de la plataforma deben ser publica-\ndos en el mundo digital con el mismo esmero y minuciosa \n129 Hugging Face: https://huggingface.co/\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   10134439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   101 11/5/23   12:0411/5/23   12:04102supervisión con que la Academia ha publicado siempre sus \nvol\u00famenes impresos. Igualmente, convendría habilitar un punto de acceso \u00fanico con un portal de acceso a datos, herramientas y servicios siguiendo los principios FAIR para catalogar y almacenar los materiales lingüísticos, las herra-mientas y los servicios. Cada recurso debería tener un con-junto de metadatos asociados para facilitar su descu\n-\nbrimiento, acceso, interoperabilidad y r\neutilización. Estos \nmetadatos estarían disponibles mediante licencia abierta, independientemente de la licencia correspondiente a cada recurso, que podrá ser abierta, remunerada o dual en fun-ción de razones legales o contractuales. Para lograr una ma-yor visibilidad, y siempre que fuera posible, se recomienda publicar los metadatos de los recursos lingüísticos en por-tales de datos abiertos gubernamentales de los países de las veintitrés academias, en el portal europeo de datos abier-tos\n 130 y en la infraestructura europea CLARIN ERIC 131, \nque agrupa metadatos y recursos tecnológicos relacionados con el uso y aplicación de las lenguas.\nEsta plataforma podrá contabilizar dentro de unos años \nel n\u00famero de consultas que realizan los sistemas de inteli-gencia artificial a los materiales de la Academia. Quizá no resulte aventurado imaginar que el n\u00famero de consultas de las máquinas pueda, en pocos años, superar a las de las per-sonas.\n130 European Commision: European Data Portal. https://data.eu-\nropa.eu/en\n131 Clarin: The research infrastr ucture for language as social and cul-\ntural data. https://www.clarin.eu/\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   10234439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   102 11/5/23   12:0411/5/23   12:041037.7.  Inteligencia artificial en los procesos pr\noductivos  \nde la RAE\nLas técnicas de inteligencia artificial pueden agilizar las ta-\nreas cotidianas de la Academia. Algunas de estas técnicas se pueden incorporar a las herramientas, sistemas y metodolo-gías de trabajo de la RAE para reducir los tiempos y recur-sos empleados en la construcción de los materiales lingüís-ticos y en su posterior mantenimiento. Entre las mejoras y el desarrollo de nuevas herramientas y funcionalidades que convendría incorporar, cabe mencionar la modernización de los sistemas de gestión de los diccionarios digitales y de los sistemas de construcción, anotación y explotación de los corpus, tanto los contemporáneos como los históricos. Todo ello, incluyendo en los procesos internos de la Acade-mia, la interacción humana para validar los resultados que proponga la inteligencia artificial, garantizando así que los recursos mantengan los criterios de calidad establecidos, con el fin de preservar el sistema lingüístico del español.\n7.8. Sistema de consultas lingüísticas\nOtro aspecto que podría ser beneficioso sería un sistema de \nconsultas lingüísticas avanzado para los usuarios. La Academia ya dispone de listados de preguntas y respuestas, y se conoce también la frecuencia con la que se realiza cada una de las pre-guntas. Al tiempo, la institución ofrece, a través de diferentes canales, un sistema exitoso que atiende consultas relacionadas con la ortografía, el léxico, la gramática o el estilo. \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   10334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   103 11/5/23   12:0411/5/23   12:04104Las técnicas de inteligencia artificial posibilitan el desa-\nrrollo de un sistema de consultas lingüísticas que guíe a \nnuestros visitantes hacia el par pregunta-respuesta que bus-caba de manera más eficaz, sacando mayor provecho de los listados de preguntas y respuestas que ya han atesorado la Academia y FundéuRAE. Además, mediante las tecnolo-gías de análisis de lenguaje natural, la colección de pregun-tas y respuestas podría enriquecerse con material extraído de otras publicaciones, tales como el Manual de español ur-gente\n 132, El español más vivo: 300 recomendaciones para ha-\nblar y escribir bien 133 o 1001 curiosidades, palabras y expre-\nsiones del español 134. Nuevas preguntas y respuestas se \nsumarían, pues, a las ya existentes, y todas ellas estarían disponibles en un \u00fanico punto de entrada en la web.\nEl sistema de consultas debería disponer de un buscador \ncon capacidades avanzadas de b\u00fasqueda semántica y auto-completado, para lo que se podría beneficiar del histórico de consultas realizadas. Adicionalmente, nuevas herra-mientas informáticas de carácter l\u00fadico permitirían ameni-zar el aprendizaje y el uso de las normas lingüísticas en las aulas docentes de los centros educativos para potenciar la \n132 Manual del español urgente. FundéuRAE. https://www.fundeu.\nes/Sobrefundeubb\nva/publicaciones/manual-de-espanol-urgente/\n133 El español más vivo: 300 recomendaciones para hablar y escribir \nbien. F\nundéuRAE. https://www.fundeu.es/noticia/fundeu-reune-el- \nespanol-mas-vivo-en-300-recomendaciones-para-hablar-bien/ \n134 1001 curiosidades, palabras y expresiones del español. F undéu-\nRAE. https://www.fundeu.es/blog/la-fundeu-reune-1001-curiosida-\ndes-de-espanol-ordenadas-de-10-en-10/\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   10434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   104 11/5/23   12:0411/5/23   12:04105motivación y la participación, y así facilitar la consecución \nde estos objetivos en los diferentes niveles formativos.\n7.9. Verificador lingüístico en línea para el español\nResultaría de gran interés contar con un verificador lingüís-\ntico en línea para el español, posiblemente integrado en las aplicaciones de mensajería instantánea, en los asistentes conversacionales y en los editores (en línea o instalados en local) de las grandes empresas tecnológicas, para ayudar a escribir correctamente. El verificador, además de avanzar en los aspectos de corrección ortográfica, léxica, sintáctica, gramatical y de estilo, debería sugerir sinónimos, redaccio-nes alternativas para mejorar el estilo, el acomodo a la va-riación lingüística de cada usuario, explicando siempre los motivos de la sugerencia o corrección. También debería adaptarse al tipo de texto, pues como todos saben, no se verifica igual un libro de poemas que una novela, o un \n documento técnico de informática que un BOE. Además,\n \ndebería permitir la incorporación de terminologías especia-lizadas, que no se encuentran en diccionarios de uso com\u00fan, así como catálogos de entidades nombradas específicas. \n7.10. \n Observatorio del sistema lingüístico del español \nen\u00a0I\nnternet\nPara examinar con atención el uso actual de las variedades del español (oral y escrito, en las diferentes zonas geográfi-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   10534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   105 11/5/23   12:0411/5/23   12:04106cas y en diferentes canales de comunicación), se necesitaría \nun sistema que observara cómo se emplea nuestro sistema lingüístico en prensa digital, plataformas colaborativas, fo-ros y blogs, por mencionar algunas posibilidades. Dado que el observatorio recopilaría, prácticamente en tiempo real, grandes vol\u00famenes de documentos y vídeos, la inteli-gencia artificial ayudaría a analizar y extraer, entre otros, términos y expresiones que se catalogarían como neologis-mos, tecnicismos, regionalismos, o que recibirían cual-quier otra clasificación que se considerara pertinente. El observatorio permitiría detectar posibles variaciones con respecto a la norma lingüística, variaciones que, a su vez, quedarían censadas en un corpus de nuevas palabras y ex-presiones no recogidas en el sistema lingüístico del espa-ñol. Este corpus resultaría de gran interés para realizar, al menos, dos actividades: la primera, monitorizar si el uso de una palabra o expresión persiste o decae en el tiempo, o si se amplía o se reduce en diferentes zonas geográficas; y la segunda, generar un catálogo vivo de palabras y expresio-nes no admisibles por zonas geográficas, que podrían servir también de entrada a la herramienta en línea de verifica-ción antes descrita. De esta forma, el verificador recomen-daría sustituir las palabras y expresiones no admisibles por otras que sí lo fueran, siempre de acuerdo con la variante del español del usuario. Todo ello ayudaría a difundir los criterios de propiedad y corrección del español en la esfera digital.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   10634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   106 11/5/23   12:0411/5/23   12:041077.11.  La necesidad de evaluar de forma comparativ\na \ny\u00a0consensuada\nEn informática, es ampliamente conocido que la calidad de \nla salida de un sistema viene determinada por la calidad de\u00a0los datos de entrada. Si los datos de entrada son inco-rrectos o de mala calidad, la salida será defectuosa. \nComo ya se ha mencionado, el Gobierno de España \npuso en marcha el proyecto estratégico Nueva Economía de la Lengua, que cuenta con un presupuesto total de 1100 millones de euros. En este contexto, es previsible que, en los próximos años, surjan una plétora de terminologías, diccionarios, corpus y modelos de lenguajes especializados para diferentes disciplinas que se utilizarán en numerosas aplicaciones. \nLas instituciones beneficiarias de los proyectos de la \nNueva Economía de la Lengua en español deben prestar especial atención a la evaluación de los corpus que generen, a los datos que incorporen al espacio europeo de datos, a los modelos de lenguaje que se alojen en repositorios y al software que se suba a las forjas de código abierto. Por este motivo, se sugiere la necesidad urgente de elaborar una me-todología consensuada que guíe este proceso de evaluación o auditoría de los materiales y tecnologías del lenguaje en español, dado que los actores son numerosos y los materia-les que se generen son diversos. La definición de una meto-dología consensuada que permita documentar y, posterior-mente, certificar los procesos facilitaría la evaluación comparativa y la toma de decisiones con garantías en un ecosistema puntero de innovación abierta. \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   10734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   107 11/5/23   12:0411/5/23   12:04108Especial atención merece la evaluación de los corpus \ncon los que se entrenan los modelos de lenguaje en la lla-\nmada inteligencia artificial generativa.\nSi los corpus utilizados en el entrenamiento presentan \nerrores repetitivos en el léxico, o en la ortografía o en la gramática, el modelo de lenguaje aprenderá dando por buenos los errores, y las frases que el modelo generará no seguirán las normas del idioma. De la misma forma, si los corpus contienen reiteradamente palabras o expresiones no admisibles, abreviaturas, siglas, neologismos, tecnicismos o sesgos racistas o sexistas, las aplicaciones que utilicen el modelo los darán por buenos, porque entienden que ese es el patrón estadístico, salvo que los desarrolladores de los modelos realicen una evaluación rigurosa de los textos con carácter previo al entrenamiento del modelo. \nEs conocido que los sesgos de sexo y raza son necesarios, \nen cierta medida, para algunos diagnósticos médicos y para el análisis de la genética humana. Pero el sesgo es inherente a los seres humanos e influye en la toma de decisiones. Existe un sesgo inconsciente, propio de cada uno, que está relacionado con prejuicios, discriminación, racismo, sexo, estereotipos o el nivel educativo, por mencionar solo algu-nos. Por ello, es imprescindible detectar, en etapas tempra-nas, los sesgos en los datos que se utilizan en el entrena-miento de los modelos de aprendizaje profundo. Así, si los datos de entrenamiento están sesgados de forma consciente o inconsciente, el algoritmo podría favorecer una respuesta frente a otra, o tomar una decisión potencialmente dañina que perjudicaría a un individuo o a un colectivo. Obsérve-se también que un sesgo no es un requisito, una cualidad, \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   10834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   108 11/5/23   12:0411/5/23   12:04109una circunstancia o algo que se requiera a priori para, por \nejemplo, dar un crédito o ser elegible para un puesto de trabajo. \nAl construir el corpus, conviene filtrar los datos perso-\nnales para evitar que el modelo de lenguaje revele informa-ción privada de una persona o de una empresa. También podrían inferirse datos de una persona que no están expli-citados en la fuente a partir de los datos de otras personas de similares características. Incluso se podría divulgar in-formación, por parte de algunos usuarios malévolos, relacio -\nnada con la seguridad nacional o con secretos comerciales o científicos.\n7.12. Ecosistema de innovación abierta\nComo ya se ha mencionado, la ENIA señala el tratamiento \ndel lenguaje natural como un componente básico para el desarrollo de la inteligencia artificial en España, que aspira a \u00abliderar a nivel mundial el desarrollo de herramientas, tecnologías y aplicaciones para la proyección y uso de la lengua española en los ámbitos de aplicación de la IA\u00bb. Para alcanzar este objetivo tan ambicioso, se necesita crear un ecosistema de innovación abierta que requiere de un marco de colaboración estable entre numerosos agentes. Junto a la Real Academia, deberían participar las Adminis-traciones p\u00fablicas, las grandes empresas tecnológicas, las pequeñas y medianas empresas, las universidades, los cen-tros de investigación e innovación, así como los centros de enseñanza media y de formación profesional. En los conte-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   10934439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   109 11/5/23   12:0411/5/23   12:04110nidos docentes de estudios preuniversitarios, se deben in-\ntroducir el pensamiento computacional, unas nociones básicas de inteligencia artificial y el uso de herramientas digitales en la enseñanza del español y de la literatura. Dada la importancia de la interdisciplinariedad, se debería mantener una visión multidisciplinar en la formación de los egresados y doctorandos de las universidades, para faci-litar niveles más altos de empleabilidad del personal for-mado en lingüística y lingüística computacional. De esta manera se contribuiría a satisfacer las necesidades de nu-merosos sectores empresariales y de la Administración p\u00fa-blica. Esta línea de formación a todos los niveles requeriría acuerdos bilaterales de la Universidad con las empresas y con las Administraciones p\u00fablicas para garantizar, a largo plazo, una relación más estrecha entre las necesidades de la sociedad digital y la preparación impartida en la universi-dad. Igualmente, el sector empresarial y las Administracio-nes p\u00fablicas pueden colaborar con la Universidad ofertan-do prácticas curriculares, trabajos fin de grado o de máster, o doctorados industriales. Los ministerios con competen-cias en Investigación, Innovación y Universidades deben financiar generosamente programas estables y ambiciosos de movilidad, atracción y retención de talento con el fin de crear una masa crítica de perfiles altamente cualificados que realicen una investigación puntera en la frontera del conocimiento. \nLa Academia debe situarse, junto a otros actores, en el \ncentro del ecosistema que se está creando. Un ecosistema que estará vivo y en continua evolución, que se tiene que retroalimentar y con la obligación de perdurar. \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   11034439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   110 11/5/23   12:0411/5/23   12:041118.  Hacia una regula\nción europea de la inteligencia \nartificial\nEn el año 2018, la comunicación de la Comisión Europea \ntitulada Inteligencia artificial para Europa 135 propuso una \ninteligencia artificial que beneficiara a las personas y a la sociedad en su conjunto, con el fin de incrementar la com-petitividad\u00a0de Europa frente a Estados Unidos y a China, y enfatizar la necesidad de mejorar las políticas educativas que permitan a los ciudadanos adquirir nuevas habilidades, todo ello basado en los valores europeos. Un año más tarde, el grupo de expertos de alto nivel sobre inteligencia artifi-cial presentó unas directrices éticas\n 136 para una inteligencia \nartificial fiable, basada en los principios de respeto por los derechos humanos, autonomía, prevención del daño, equi-dad y capacidad de explicación, que fuera respetuosa con las leyes y reglamentos, y robusta desde un punto de vista técnico y social. \nEn estos días, la Comisión Europea está finalizando un \nreglamento específico\n 137 sobre el desarrollo y el uso de los \n135 European Commission. Communication from the Commis-\nsion to the European P\narliament, the European Council, the European \neconomic and social committee of the regions (2018): Artificial Inte-\nlligence for Europe. https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:52018DC0237&from=EN\n136 European Commission. Independent high-level group on artifi-\ncial intelligence (2019): Ethics G\nuidelines for Trustworthy AI. https://di-\ngital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\n137 European Commission. Proposal for a regulation of the Euro-\npean Parliament and of the Council (2021): Laying D\nown Harmonized \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   11134439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   111 11/5/23   12:0411/5/23   12:04112sistemas de inteligencia artificial en todos los países de la \nUnión para proteger los derechos de los ciudadanos. De esta forma, la Comisión Europea reconoce que los mismos elementos y técnicas que potencian los beneficios socioeco-nómicos de la inteligencia artificial en el mercado europeo pueden también generar nuevos riesgos o consecuencias negativas para los individuos y para la sociedad. Por ello, el reglamento contempla cuatro objetivos específicos: prime-ro, garantizar que los sistemas de inteligencia artificial in-troducidos y utilizados en el mercado de la Unión sean \n seguros y respeten la legislación vigente sobre der\nechos fun-\ndamentales y valores europeos; segundo, garantizar la segu-ridad jurídica para facilitar la inversión y la innovación en inteligencia artificial; tercero, mejorar la gobernanza y la aplicación efectiva de la legislación existente sobre derechos y requisitos de seguridad aplicables a los sistemas de inteli-gencia artificial; y cuarto, facilitar el desarrollo de un mer-cado \u00fanico para unas aplicaciones de inteligencia artificial lícitas, seguras y fiables. El reglamento está estrechamente relacionado con la ley europea de protección de datos\n 138, \nRules on Artificial Intelligence (Artificial Intelligence Act) and Amending \nCertain Union Legislative Acts. https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-laying-down-harmonised-rules-artifi-cial-intelligence \n138 Official Journal of the European Union (2016): Regulation \n(EU) 2016/679 of the European P\narliament and of the council on the \nprotection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation). https://eur-lex.europa.eu/eli/reg/2016/679/oj \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   11234439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   112 11/5/23   12:0411/5/23   12:04113con la gobernanza del dato europeo 139, con la directiva de \ndatos abiertos 140 y con otras iniciativas que establecen me-\ncanismos y servicios de confianza para la reutilización, \ncompartición y puesta en com\u00fan de datos. \nEl reglamento identifica cuatro tipos de riesgos: riesgo \ninaceptable, alto riesgo, riesgo limitado y riesgo mínimo. Impone también cargas regulatorias solo cuando es proba-ble que un sistema de inteligencia artificial plantee riesgos elevados para los derechos fundamentales o para la seguri-dad. Así, las aplicaciones que manipulan el comportamien-to de las personas, las clasifican o las identifican biométri-camente de forma masiva, a distancia y en tiempo real serán consideradas de riesgo inaceptable y estarán prohibidas. Para los sistemas de inteligencia artificial que no son de alto riesgo, solo se imponen obligaciones de transparencia muy limitadas, en términos de suministro de información para avisar a las personas de que están interactuando con un software. Para los sistemas de alto riesgo, el reglamento es-tablece requerimientos que han de satisfacerse antes de que el sistema se comience a utilizar, requisitos que han de \n139 Official Journal of the European Union (2022): Regulation \n(EU) 2022/868 of the European P\narliament and of the council on Euro-\npean data governance and amending Regulation (EU) 2018/1724 (Data \nGovernance Act). https://digital-strategy.ec.europa.eu/en/policies/da-ta-governance-act \n140 Official Journal of the European Union (2019): Directive (EU) \n2019/1024 of the European P\narliament and of the Council of 20 June \n2019 on open data and the re-use of public sector information. https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%  \n3A32019L1024 \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   11334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   113 11/5/23   12:0411/5/23   12:04114mantenerse durante todo su ciclo de vida. En la aplicación \ndel reglamento, está previsto que España sea pionera con el primer proyecto piloto que probará estas guías. Si bien la ENIA fue de las \u00faltimas estrategias europeas en publicarse, en julio de 2021, España fue pionera al lanzar la carta de derechos digitales\n 141 para articular un marco de referencia \nque vela por los derechos de la ciudadanía en la nueva rea-lidad digital. También lo está siendo por el proyecto piloto que identificará buenas prácticas, que permitirán a las em-presas implementar la nueva regulación europea\n 142 en los \nsistemas de inteligencia artificial de alto riesgo.\n* * *\nPara concluir, la A\ncademia debe considerar cómo llevar a \ncabo su misión para conocer y supervisar el uso del español en el mundo digital, y a la vez actuar para ofrecer sus mate-riales, con las licencias que corresponda, para que las apli-caciones existentes, y las nuevas que están por construir, usen correctamente la lengua en un ecosistema de innova-ción abierta. Por ello, debo insistir en que lo que se haga, para que sea efectivo, tiene que ser localizable, accesible, interoperable, reutilizable, y muy fácil de utilizar por los \n141 Carta de derechos digitales. https://portal.mineco.gob.es/Re-\ncursosNoticia/mineco/prensa/noticias/2021/Car\nta_Derechos_Digita-\nles_RedEs_140721.pdf \n142 Sandbox de regulación de la inteligencia artificial. https://por-\ntal.mineco.gob.es/es-es/comunicacion/P\naginas/20220627-PR_AI_\nSandbox.aspx \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   11434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   114 11/5/23   12:0411/5/23   12:04115sistemas de inteligencia artificial. Tiene que cubrir la sensa-\nción de inmediatez que tenemos.  \nComo todos sabemos los avances de la tecnología van \nmuy por delante de cualquier legislación. No quisiera fina-lizar sin señalar que, cada día que pasa, se necesita más el reglamento específico sobre el desarrollo y el uso de los sis-temas de inteligencia artificial que la Comisión Europea está elaborando. Mientras llega, debemos pensar en que no todo lo que sea técnicamente posible es socialmente conve-niente, porque puede superar las fronteras de lo ético, medioambiental y legalmente aceptable.\nMe siento profundamente honrada y motivada por la \noportunidad de contribuir a la labor de la Academia. Pro-meto trabajar con gran ilusión, prudencia, entusiasmo, dis-posición y responsabilidad para aportar todo lo que esté en mi mano en este apasionante ámbito de aplicación de la inteligencia artificial.\nGracias por su atención y confianza.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   11534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   115 11/5/23   12:0411/5/23   12:0434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   11634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   116 11/5/23   12:0411/5/23   12:04Contestación \ndel \nExcmo. Sr. D. Santiago Muñoz Machado\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   11734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   117 11/5/23   12:0411/5/23   12:0434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   11834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   118 11/5/23   12:0411/5/23   12:04119Señora Vicepresidenta primera del Gobierno, señoras y se-\nñores académicos, señoras y señores.\nH ace ya noventa años que el director de la Academia \nno asumía personalmente la responsabilidad de responder al discurso de ingreso de un nuevo académico, función que es práctica delegar en alg\u00fan académico más sabio o más especializado que él en la materia sobre la que verse la ex-posición del recipiendario. He retenido, en esta ocasión, el honor y el riesgo de la contestación, porque la materia so-bre la que ha disertado la profesora Gómez-Pérez es radi-calmente nueva en esta Casa, como lo es la propia tecnolo-gía que sostiene la denominada inteligencia artificial. Sabemos todos muy poco sobre lo que sus aplicaciones pueden depararnos, pero tenemos la percepción de que abren un horizonte revolucionario para el uso y la regula-ción de la lengua, tan retador y apasionante para la Real Academia Española que me atrevo a afirmar que entramos en una segunda era de su vida institucional.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   11934439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   119 11/5/23   12:0411/5/23   12:04120Hace un par de semanas la revista The Economist ilustró \nsu portada con las dos letras que forman el acrónimo de \ninteligencia artificial en inglés (AI). Adornó la primera con coronas y signos de santidad y la segunda con los símbolos que identifican al diablo, con cuernos y rabo incluidos. Una simbología tan sencilla y conocida acertó a expresar esquemáticamente, con dos pinceladas, la ambigüedad y las dudas que todavía plantea a muchos el irresistible y rá-pido avance de la inteligencia artificial: puede ser un motor de cambio de nuestras sociedades de una potencia descono-cida en cualquier revolución tecnológica anterior, y, al mis-mo tiempo, ha generado el temor a que pueda tener efectos destructivos sobre el dominio humano de todas las cosas, sea el final del Antropoceno y el inicio de la era en que las máquinas gobernarán el mundo. \nLa inteligencia artificial puede ser un espacio abierto a la \nutopía, pero también la puerta de entrada a un mundo dis-tópico. En cuanto a lo primero Raymond Kurzweill (The singularity is near: when humans transcend biology, New York, \n2005) ha observado que podría conducirnos a un modelo de sociedad sin precedentes en el que nuestro mundo pudie-ra funcionar gracias a inteligencias calificadas de alg\u00fan modo como superiores que nos liberasen del trabajo repeti-tivo, que gestionasen el tráfico, redujesen la contaminación o eliminasen la enfermedad. Por lo que concierne a la pro-yección distópica de los cambios que se avecinan, lo que se plantea es la posibilidad de que la inteligencia artificial pue-da tomar decisiones contrarias a los intereses de los seres humanos. Una inteligencia con una capacidad computacio-nal ilimitada, superior a cualquier inteligencia humana, \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   12034439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   120 11/5/23   12:0411/5/23   12:04121puede asumir la responsabilidad de dirigir los destinos de una \nespecie inferior o incluso considerar que la acción de los seres humanos, la actuación antrópica, es un peligro para el plane-ta. De aquí la tendencia de esa inteligencia, artificial pero superior, a sojuzgar y controlar a los propios seres humanos.\nEn la actualidad estos riesgos son algo más que especu-\nlaciones literarias. El análisis de las ventajas e inconvenien-tes de la inteligencia artificial se ha convertido en uno de los temas recurrentes de cualquier exposición sobre esta materia, tanto desde una perspectiva ética como desde un punto de vista jurídico [en general, R. López de Mantaras y P . Messeguer González Inteligencia artificial (Madrid, Ca-tarata-CSIC 2017)]. \nLos perjuicios que acarrearía una inteligencia artificial \ndenominada \u00abfuerte\u00bb, o general, como la ha llamado Asun-ción Gómez-Pérez hace un momento, capaz de sustituir todas las capacidades de la mente humana y de superarlas, están todavía situados en los términos de la especulación. La inteligencia artificial que, por el momento, hay que te-ner en cuenta es la que se crea para desarrollar propósitos específicos. El prototipo que todavía tenemos en la cabeza es el de la máquina dotada de inteligencia artificial que es capaz de jugar y ganar al ajedrez. La otra clase, la inteligen-cia artificial fuerte, tendría que ser capaz de razonar como una persona y, por tanto, estar dotada de empatía, es decir de capacidad para ponerse en el lugar de otro, y habilidad para el aprendizaje emocional, lo que hoy por hoy no está al alcance de una máquina.\nEn todo caso, los progresos de la inteligencia artificial \nestán haciendo preocuparse al mundo. Hace pocas semanas \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   12134439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   121 11/5/23   12:0411/5/23   12:04122un grupo relevante de expertos, al que acompañaban muy \nconocidos filósofos, antropólogos, historiadores y popula-res hombres de negocios, propusieron una \u00abpausa\u00bb en la inteligencia artificial, una moratoria de pocos meses para determinar sus límites. Y solo hace unos días, expertos de todo el mundo en neurociencia, entre los cuales el español Rafael Yuste, advertían de las consecuencias para los dere-chos individuales de algunos de los progresos de la aplica-ción al cerebro humano de estas nuevas tecnologías, que pueden permitir ampliar sus capacidades, explorarlo y pre-determinar sus decisiones o conocer los pensamientos.\nQuiere esto decir, a mi juicio, que la distinción entre \nuna inteligencia artificial puramente instrumental y otra de fondo, que aspira a emular en todo a la inteligencia huma-na, tiene algunos bordes bastante difuminados, y el tránsito entre los modelos más utilitarios e inocentes y las herra-mientas de alto riesgo será un proceso difícil de advertir y evitar.\nPor lo que concierne a la importancia que cabe conceder \nal dominio por las máquinas de la lengua que hablamos, es necesario recordar el evangelio de San Juan (1:1) porque, se le otorgue o no carácter sagrado, seg\u00fan las inclinaciones de cada uno, es una realidad empírica que la lengua está en el centro de todo el sistema de nuestra cultura. En el princi-pio estuvo la palabra, el verbo. Aunque algunas veces se hayan considerado excesivamente especulativas las tesis del filósofo e historiador Yuval Noah Harari, pocas objeciones pueden oponerse a sus consideraciones recientes sobre las consecuencias de que la inteligencia artificial haya adquiri-do capacidades avanzadas para manipular y generar lengua-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   12234439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   122 11/5/23   12:0411/5/23   12:04123je, ya sea con palabras, sonidos o imágenes. Esto supone \nque la inteligencia artificial \u00abha hackeado el sistema opera-tivo de nuestra civilización\u00bb (The Economist, 24 de abril de 2023). Lo afirma porque el lenguaje es la materia de que está hecha toda la cultura humana, la definición de los de-rechos y todos nuestros artefactos culturales, el dinero, las religiones, las leyes, las historias... No sabemos cuánto tiempo tardará la inteligencia artificial en poder crear libros que podamos confundir con los que proceden de la imagi-nación humana, ni en construir nuevas creencias con pala-bras que podamos considerar propias de los humanos, ni en mantener conversaciones políticas que puedan influir en nuestras propias convicciones o contribuir a alterar la opi-nión de los ciudadanos. No sé cuánto tiempo falta, pero se ha iniciado el camino y el progreso en ese sentido no se hará esperar mucho.\nEl inicio está en la utilización por la inteligencia artifi-\ncial del lenguaje natural y esto ya es objetivo conquistado. Es evidente que habrá que establecer límites jurídicos y éti-cos para la protección de valores y derechos, sea por la vía de la autorregulación o por medio de la regulación. Sobre ello volveré luego.\n*\n * *\nUn paso más allá del hecho mismo de la irrupción de la inteligencia ar\ntificial en el dominio del lenguaje humano \nestá la cuestión de las repercusiones puramente lingüísticas de esa utilización, es decir la calidad y la accesibilidad de la lengua hablada por las máquinas y el riesgo de que produz-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   12334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   123 11/5/23   12:0411/5/23   12:04124ca perjuicios en su unidad, mantenida hasta hoy como una \nde las mayores conquistas de la ordenada expansión del es-pañol en el mundo. El problema que enuncio no es ningu-na cuestión de futuro, sino que describe una situación con la que la RAE ha empezado a enfrentarse. \nEstará bien subrayar que no es esta una cuestión baladí. \nLa lengua es el valor principal de la cultura de los pueblos y el español lo es de una comunidad que incluye a casi seiscien-tos millones de personas. Un deterioro de la calidad, la capa-cidad expresiva, la belleza o la unidad del español a cuenta de los desarrollos de la inteligencia artificial sería una lesión cul-tural de primer orden.\nTéngase en cuenta que la Real Academia Española, du-\nrante más de trescientos años, ha dedicado toda su relevan-te actividad al mantenimiento de normas léxicas y gramati-cales que aseguren la capacidad de comunicación del español y su unidad en todo el espacio hispanohablante. Las obras de la RAE han sido aceptadas siempre y conside-radas, a lo largo de los tres siglos de la institución, como normas de obligado acatamiento. No tiene la Academia a\u00a0su disposición un poder sancionador con el que reprimir a los infractores, pero su autoridad y prestigio determinan que sus reglas constituyan un singular \u00abderecho blando\u00bb cuyo acatamiento es imprescindible para cualquiera que desee ser miembro de una comunidad hispanohablante como persona alfabetizada. Es la sociedad misma la que repudia la utilización bárbara, inadaptada o incorrecta de la lengua com\u00fan. Este carácter normativo de las obras acadé-micas tiene, desde hace ya muchos decenios, el apoyo de las Academias de la lengua española, que empezaron a consti-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   12434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   124 11/5/23   12:0411/5/23   12:04125tuirse en América desde finales del siglo xix, y de la Asocia-\nción de Academias que se fundó en 1951, y ha mantenido regularmente su actividad hasta hoy.\nEl Diccionario, la Gramática y la Ortografía, que son las \nprincipales obras normativas de la Academia desde el si-glo\u00a0xviii, han ido adaptándose a la evolución del idioma. Los procedimientos de elaboración y reforma de los textos correspondientes han sido siempre cuidadosos y medita-dos, y basados en los cambios paulatinos del lenguaje. La relación de la Academia con la lengua puede decirse que se produce con la intermediación de los propios usuarios del idioma, siempre personas dotadas de inteligencia natural a los que la Academia puede dirigirse directamente porque tienen capacidades que les permiten entender los mensajes que la institución envía con sus obras y recomendaciones.\nLa utilización de la lengua por la inteligencia artificial \ncambia este panorama tradicional. La RAE no tiene rela-ción directa con las máquinas, ni posibilidad de corregir directamente la eventual barbarie con la que manejen el idioma. La lengua de las máquinas la determinan sus pro-gramadores. Siempre se han adaptado las obras académicas a los cambios, muy lentos, que los hablantes introducen en el idioma. Son muchos millones de agentes los propietarios de la lengua, a los que esta institución sigue con toda meti-culosidad apuntando sus mutaciones y variantes en toda la geografía universal del español. Desde hace pocos años y con una intensidad creciente, los cambios no son obra ex-clusiva de la comunidad de los hispanohablantes porque se han introducido en el sistema de la lengua agentes nuevos, las empresas tecnológicas que usan inteligencia artificial, \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   12534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   125 11/5/23   12:0411/5/23   12:04126que son potenciales reguladores o, al menos, prescriptores \nde nuestra propia lengua, con capacidades para imponer variantes que pueden no coincidir con los usos comunes de los humanos. Además esas prescripciones se establecen como reglas de uso a toda la extraordinaria legión de nue-vos usuarios de la lengua, que son cientos de millones de máquinas parlantes. \nCiertamente, los directivos de las grandes empresas tec-\nnológicas que fabrican máquinas dotadas de inteligencia artificial, capaces de usar nuestra lengua, tienen el máximo interés en que lo hagan en términos de calidad aceptable porque, de no ser así, lo mismo que las comunidades de hablantes repudian a quienes no manejan adecuadamente el idioma com\u00fan, los mercados no aceptarán productos de-fectuosos porque carecen de las habilidades idiomáticas mí-nimas.\nLa cuestión problemática radica, no obstante, en la po-\nsibilidad de que se establezcan estándares poco exigentes para el lenguaje de las máquinas, o también que se generen diferencias crecientes entre las lenguas programadas para sus máquinas por las principales empresas tecnológicas. Es posible, por ejemplo, lo más elemental: que el vocabulario con que se dota a la inteligencia artificial hablante sea me-nor en unos casos que en otros. Puede ocurrir que algunas máquinas parlantes acepten jergas o particularismos lin-güísticos desconocidos por otras, abunden en variantes fra-seológicas que no hayan sido asumidas por todos los pro-gramadores, o, en fin, que se utilicen de modo sesgado especialidades lingüísticas de algunos territorios y se use menos, o incluso se desplace, el tronco com\u00fan del idioma.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   12634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   126 11/5/23   12:0411/5/23   12:04127Estas especialidades pueden ser el germen de dialectos \ndigitales, llamémoslos así, que tensionen la unidad y esta-\nblezcan las bases de una fragmentación del uso del idioma que la normativa académica ha logrado evitar durante más de trescientos años. Sería un grave retroceso, desde luego, con un impacto cultural de enorme envergadura.\nNo es esta una cuestión que pueda banalizarse porque \nen la actualidad ya hay en el mundo más máquinas que manejan la lengua natural que humanos. Estas máquinas se han introducido por completo en nuestras comunidades y nos acompañan en todas partes y durante todo nuestro tiempo: teléfonos y relojes inteligentes, tabletas y computa-dores, traductores, correctores lingüísticos en los teclados que usamos para escribir, navegadores, asistentes de voz que colaboran eficazmente en las tareas domésticas y en la programación de nuestra vida diaria, juegos hablantes con los que aprenden los niños... En fin, la vida diaria de cual-quier ciudadano se desarrolla en compañía de la inteligen-cia artificial (dada la dependencia que generan todos los programas mencionados, seguramente son ya el miembro de la familia con el que más trato existe) e, indudablemen-te, influye en el uso de la lengua. En el caso de los niños y de las personas con menos formación porque se produce un más que probable aprendizaje directo del vocabulario, la fonética y la fraseología de las máquinas. Si de adultos formados se trata, porque los programas indicados impo-nen cambios paulatinos en el lenguaje que tienden a dete-riorarlo y acarrean peligros para su calidad expresiva.\nMencionaré alg\u00fan ejemplo de esto \u00faltimo. Los teclados \ninteligentes, que nos advierten sobre la incorrecta ortogra-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   12734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   127 11/5/23   12:0411/5/23   12:04128fía de alguna palabra o incluso marcan automáticamente \nque no existe, toman como referencia corpus léxicos que pueden ser insuficientes o incorrectos. El Diccionario de la lengua española de la RAE y ASALE contiene alrededor de 94 000 entradas y 189 000 acepciones. Otros diccionarios recogen un léxico más limitado. Y nadie obliga, hasta aho-ra, a las empresas tecnológicas a que entrenen sus máquinas con uno u otro diccionario o incluso que elaboren un vo-cabulario propio. La consecuencia de estas elecciones es que el teclado inteligente aceptará o no un vocabulario más rico dependiendo de la extensión del corpus que use. Pero, además, no es cuestión de n\u00famero, sino también de cali-dad: un corrector puede excluir una determinada expre-sión, aunque sea correcta, porque sus programadores así lo han decidido, y esto puede ocurrir al margen de las normas que la RAE ha establecido y que los humanos venimos aceptando como correctas desde hace tres siglos. Sucede, además, que si el usuario acepta las indicaciones de su te-clado inteligente, los algoritmos de que se vale el sistema viralizan la solución, sea correcta o incorrecta desde la pers-pectiva de las normas lingüísticas, generalizándola. La Aca-demia ha podido determinar que hay teclados con sistemas de corrección automática que dan por inexistentes casi el veinte por ciento de los vocablos que figuran en nuestro diccionario. \nProblemas semejantes se suscitan con los traductores au-\ntomáticos. Ha avanzado esa tecnología a una velocidad pas-mosa y no tardará en llegar el tiempo en el que los traduc-tores sean intermediarios habituales en las conversaciones entre individuos de diferentes lenguas o para la lectura y \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   12834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   128 11/5/23   12:0411/5/23   12:04129comprensión de obras y libros de cualquier clase. He escrito \nen otro lugar que el progreso de la traducción automática augura el final de Babel porque las diferencias lingüísticas acabarán siendo superadas, gracias a esos programas inteli-gentes, de modo que no tendremos que soportar la mal\n-\ndición bíblica de tener que aprender otros idiomas para mejorar nuestra capacidad de comunicación. P\nodremos ha-\nblar con cualquiera con absoluta simulta  neidad entre la ex-\npresión originaria y la deriv\nada de la traducción. Pero hay que \ntener en cuenta la enorme importancia de que las máquinas sean entrenadas con los vocabularios adecuados y que ma-nejen correctamente el sistema entero de la lengua.\nSe están tratando de preparar, con comprensible acelera-\nción, corpus para la inteligencia artificial de la lengua espa-ñola que, hasta donde son conocidos, plantean muchos problemas si se comparan con los corpus lingüísticos que la RAE ha elaborado desde hace años: los de la nueva genera-ción han sido extraídos de páginas web, y consisten en acu-mulaciones de datos simplemente recolectados, no selec-cionados. Y han recopilado la lengua española de España, pero parece que se dejan atrás la de América, que tanto peso tiene en nuestro tiempo. Por más que algunos de estos corpus hayan conseguido reunir, seg\u00fan anuncian, miles de millones de palabras, les falta un control de calidad, una documentación suficiente, y la aplicación de métodos que hagan \u00fatiles los resultados. La inteligencia artificial puede facilitar la elaboración, en pocas horas, de corpus lingüísti-cos multimillonarios. Lo que es seguro es que no tendrán la calidad ni podrán manejarse con la seguridad que ofrecen los elaborados en esta Casa.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   12934439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   129 11/5/23   12:0411/5/23   12:04130En el momento actual, la Real Academia Española es la \n\u00fanica institución que ha creado el gran corpus del español, \nconsultable, de todas las épocas y de todos los lugares en los que se habla o se habló español, documentado, anotado, del que es posible extraer datos lingüísticos con los que ali-mentar a la inteligencia artificial. La suma de palabras de todos los corpus (CORDE, CDH, CREA, CORPES, En-clave de Ciencia...) supera los 800 millones de formas se-leccionadas, que comprenden desde los inicios de la lengua española hasta la actualidad. Cada uno de los textos que conforman los corpus de la RAE están perfectamente do-cumentados, de modo que se conoce su origen geográfico, el tipo de obras literarias o medios de comunicación de los que procede. Son completos y evitan inclinaciones o sesgos hacia una mala aplicación de nuestra lengua por los algorit-mos de la inteligencia artificial.\nEs notorio que el objetivo fundamental de los corpus \nque se preparan para la inteligencia artificial es facilitar mo-delos de lenguaje que permitan \u00abcrear herramientas relacio-nadas con el lenguaje y capaces de clasificar documentos, realizar correcciones o elaborar herramientas de traduc-ción\u00bb (La información básica (oficial) está en https://datos.gob.es/es/blog/asi-es-maria-la-primera-inteligencia-artifi-cial-de-la-lengua-espanola). \nEl sistema habitual de construcción de estos recursos con-\nsiste en programas que recorren la red (o acuden a bancos de datos donde se ha producido una previa acumulación de pa-labras contenidas en sitios web) en busca de textos potencial-mente interesantes para, tras algunos filtros referidos a la lengua del texto, tamaño, eliminación de repeticiones y si-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   13034439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   130 11/5/23   12:0411/5/23   12:04131milares, integrarlos en esos conjuntos ingentes que son pro-\ncesados en busca de los modelos capaces de \u00abgenerar textos nuevos a partir de un ejemplo previo, lo que resulta muy \u00fatil a la hora de elaborar res\u00famenes, simplificar grandes cantida-des de información, generar preguntas y respuestas e, inclu-so, mantener un diálogo\u00bb.\nLos corpus de referencia, como los elaborados por la \nRAE, siguen un planteamiento totalmente distinto. Su ob-jetivo básico consiste en integrar materiales de muy diverso tipo y procedencias a efectos de obtener los datos necesa-rios para mejorar nuestro conocimiento acerca de la situa-ción de la lengua española, sus variedades y su historia y poder aplicar luego esos conocimientos a campos muy va-riados.\nSe recuerda con frecuencia el riesgo de los sesgos que \npueden presentar los algoritmos de inteligencia artificial, que puedan afectar a la igualdad de género, a la desigualdad por razón de la discapacidad, o ser contrarios a las políticas medioambientales. Como este acto está teniendo lugar en sede de la RAE, es pertinente añadir los sesgos lingüísticos consistentes en una mala utilización gramatical o léxica de nuestro idioma. \nEntre las diferentes causas de la aparición de sesgos en la \ninteligencia artificial, es más probable que los sesgos proce-dan de los big data. Los grandes corpus ya existentes en cualquier idioma han sido compendiados, en la mayor parte de los casos, mediante procedimientos automatiza-dos en los que no se incluyen instrumentos capaces de de-tectarlos. Puede haber discriminaciones geográficas debi-das a que las bases de datos hayan atendido al lenguaje \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   13134439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   131 11/5/23   12:0411/5/23   12:04132com\u00fan de una parte del mundo (hispanohablante en nues-\ntro caso) olvidando, si aquellas se han preparado en Espa-ña, el léxico y las expresiones americanas. Para luchar con-tra ello la fórmula más adecuada es la utilización de corpus lingüísticos bien marcados, en los que se haya depurado cualquier sesgo, contrastado las desviaciones de cualquier corpus de gran tamaño que se esté utilizado para entrena-\nmiento de la inteligencia artificial o a efectos de la prepara-ción de algoritmos.  \n*\n * *\nSiendo importante el desafío de defender la cultura y los der\nechos de los riesgos que plantea la inteligencia artificial, \nninguna duda me cabe de que la mejor opción que pueden seguir los Estados y la Unión Europea, y más bien esta que los Estados dada la escala del problema, es regularla lo antes que sea posible. En los ámbitos económicos angloamerica-nos se inclinan más bien por la autorregulación. La cultura europea ha preferido siempre la regulación de los nuevos inventos. Lo urgente ahora no es entretenerse en cambiar los principios en que se ha basado tradicionalmente la rela-ción entre lo p\u00fablico o privado en el área europea y en la angloamericana; ya habrá tiempo y maneras de confluir en principios y valores de puerto seguro para que las reglas de ordenación sean globales.\nUno de los principios generalmente admitidos de la in-\nteligencia artificial y de la robótica es el que exige que las máquinas siempre estén dominadas por los humanos. Este principio impone la necesidad de que los poderes p\u00fablicos \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   13234439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   132 11/5/23   12:0411/5/23   12:04133establezcan límites de la inteligencia artificial mediante una \nregulación de sus aplicaciones.\nDesde hace años se han estado aprobando directrices no \nvinculantes y criterios éticos sobre el uso de la inteligencia artificial, tales como las que fijó en 2018 el grupo indepen-diente de expertos creado por la Comisión Europea bajo el título Directrices éticas para una IA fiable. También la Co-\nmisión aprobó en 2020 un Libro Blanco sobre Inteligencia Artificial. En 2021 se ha elaborado en España la Carta de derechos digitales. También se ha aprobado una Carta ibe-roamericana de derechos en los entornos digitales en 2023. El \nParlamento, el Consejo y la Comisión Europea han apro-bado en 2023 una Declaración Europea sobre Principios y Derechos digitales para la Década Digital. Son, todos los ci-tados, documentos declarativos, sin valor vinculante.\nLa regulación de la IA en Europa tendrá un impulso \ndefinitivo cuando se apruebe la Propuesta del Reglamento del Parlamento Europeo y del Consejo por el que se establecen normas armonizadas en materia de inteligencia artificial (Ley de inteligencia artificial) y se modifican determinados actos legislativos de la Unión, publicada en Bruselas el 21 de abril de 2021.\nLa propuesta corresponde a un compromiso de la presi-\ndenta Von der Leyen, que anunció en sus orientaciones po-líticas para la Comisión 2019-2024 tituladas \u00abUna Unión que se esfuerza por lograr más resultados\u00bb, consistente en que la Comisión presentaría propuestas de legislación para un enfoque europeo coordinado sobre las implicaciones éticas y humanas de la inteligencia artificial. T ras dicho anuncio, el 19 de febrero de 2020, la Comisión publicó el \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   13334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   133 11/5/23   12:0411/5/23   12:04134Libro Blanco sobre la inteligencia artificial: un enfoque euro-\npeo orientado a la excelencia y la confianza. Se definen en este texto las opciones existentes para alcanzar el doble ob-jetivo de promover la adopción de la IA y de abordar los riesgos vinculados a determinados usos de esta nueva tec-nología.\nLa Propuesta de Reglamento pretende desarrollar un \necosistema de confianza mediante la proposición de un marco jurídico destinado a lograr que la IA sea fiable. La Propuesta se basa en los valores y derechos fundamentales de la UE y tiene por objeto inspirar confianza a los ciuda-danos y otros usuarios para que adopten soluciones basadas en la IA, al tiempo que se trata de animar a las empresas a que desarrollen este tipo de soluciones. La IA debe ser un instrumento para las personas y una fuerza positiva en la sociedad, y su fin \u00faltimo debe ser incrementar el bienestar humano.\nLa Comisión propone un marco reglamentario sobre in-\nteligencia artificial con los siguientes objetivos específicos:\n—\n garantizar que los sistemas de IA introducidos y usa-dos en el marco de la UE sean segur\nos y respeten la \nlegislación vigente en materia de derechos funda-mentales y valores de la Unión Europea.\n—\n garantizar la seguridad jurídica para facilitar la in-versión e innov\nación en IA;\n— mejorar la gobernanza y la aplicación efectiva de la legislación vigente en materia de derechos funda-mentales \ny los requisitos de seguridad aplicables a \nlos sistemas de IA.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   13434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   134 11/5/23   12:0411/5/23   12:04135— facilitar el desarrollo de un \nmercado \u00fanico para ha-\ncer un uso legal, seguro y fiable de las aplicaciones \nde IA y evitar la fragmentación del mercado.\nNo es momento de una explicación más detenida de los \npropósitos del borrador de reglamento europeo, sobre el que me limitaré a expresar mi opinión de que está en el buen camino.\nEs importante considerar que el canon de la corrección \nlingüística no puede limitarse actualmente a la utilización adecuada de las reglas de la gramática o al empleo del voca-bulario correcto. La inteligencia artificial está siendo utili-zada, a ritmo creciente, por las Administraciones p\u00fablicas y por las grandes corporaciones empresariales en sus relacio-nes con los ciudadanos. Cobra especial importancia la exi-gencia de buen uso del lenguaje en el sentido de que no se oscurezca, limite, pierda capacidad de comunicación por-que, de seguirse esta tendencia, se estaría vulnerando el de-recho de los ciudadanos a entender las normas, las resolu-ciones de las Administración que les afectan, las decisiones de los jueces y tribunales, y también su capacidad para usar los servicios de interés general, sean p\u00fablicos o privados. Existe un movimiento universal en favor del lenguaje claro e instituciones que lo defienden implantadas en geografías supranacionales, no solo de habla española. Las simplifica-ciones, jergas y dialectos que han introducido las redes, y que puede generalizar la inteligencia artificial, reclaman atención especial.\nEl deber de emplear un lenguaje claro está ampliamente \nrelacionado con la preservación de los derechos individua-\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   13534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   135 11/5/23   12:0411/5/23   12:04136les, que no pueden ejercerse adecuadamente ante comuni-\ncaciones oscuras o casi encriptadas para quienes carecen de habilidades digitales mínimas. Por esta razón a la idea de len -\nguaje claro hay que añadir la de lenguaje accesible. Hay que adecuar el lenguaje de la inteligencia artificial a las capaci-dades naturales de las personas. Cuando la normativa euro-pea o estatal sobre inteligencia artificial se refiere a la prohi-bición de discriminaciones como uno de los valores que el progreso en esta materia debe preservar, no se refiere solo al sexo, la raza o la religión, sino también a la discapacidad. Un porcentaje importante de la población es incapaz de entender el lenguaje complejo, y la inteligencia artificial (que ha de atenerse a la máxima de que no puede restringir o empeorar el régimen de los derechos) tiene que establecer alternativas \u00fatiles para el disfrute de sus ventajas por todas las personas, con independencia de su capacidad, sea por razón de nacimiento, accidente o edad. No es solo un pro-blema de claridad, sino de diseño de un lenguaje fácil, de accesibilidad universal, que hay que entender como uno de\u00a0los avances irreversibles de los derechos de las personas. \n*\n * *\nHace poco más de cuatro \naños (noviembre de 2019) que la \nRAE anunció su programa Lengua Española e Inteligencia Artificial (LEIA), que hemos venido desarrollando con la colaboración de las empresas tecnológicas globales. En una primera fase hemos centrado nuestra atención en el buen uso del español por estos gigantes. Con una gran receptivi-dad por su parte, que nos ha alegrado poder constatar. Ya \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   13634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   136 11/5/23   12:0411/5/23   12:04137usan nuestras obras para perfeccionar sus sistemas hablan-\ntes. Ahora entramos en la fase que Asunción Gómez-Pérez ha descrito, para cuyo desarrollo contamos con la ayuda que ha concedido a la RAE el Gobierno de España, a pro-puesta del departamento que dirige la Vicepresidenta 1.\u00aa del Gobierno y ministra de asuntos económicos, y con car-go al denominado \u00abPERTE nueva economía de la lengua\u00bb, cuyo apoyo agradecemos muy sinceramente, así como su presencia en esta sesión solemne. Para este periodo de aco-plamiento a la tecnología de la inteligencia artificial espera-mos contar también con empresas especializadas de España.\nTeníamos necesidad en la RAE de una experta como la \nprofesora Gómez-Pérez y hemos escogido, estoy seguro, a la más idónea por sus conocimientos y experiencia. Nos ha hablado de una disciplina cuyos contenidos materiales han sido hasta ahora ajenos a la actividad de la Real Aca-demia Española. La inteligencia artificial abre un nuevo horizonte y presenta retos que la profesora Gómez-Pérez ha explicado en su discurso. Estamos seguros de que su acreditada capacidad ayudará a su progresiva ejecución en los años inmediatos.\nLa profesora doctora Asunción Gómez-Pérez es Licen-\nciada en Informática (1991) y Doctora en Ciencia de la Computación de Inteligencia Artificial (1993) por la Uni-versidad Politécnica de Madrid (UPM). Máster en Direc-ción y Administración de Empresas (1994) por ICADE. Es catedrática de Universidad en el área de Ciencias de la Computación e Inteligencia Artificial (2007). Actualmente es Vicerrectora de Investigación, Innovación y Doctorado (2016- ) en la UPM. Fue Directora del Departamento de \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   13734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   137 11/5/23   12:0411/5/23   12:04138Inteligencia Artificial (2008-2016), es Directora del Grupo \nde Ingeniería Ontológica desde 1995, fue Directora Acadé-mica del Máster en Inteligencia Artificial (IA) (2009-2016) y también Coordinadora del Programa de Doctorado en Inteligencia Artificial (2009-2016) en la UPM.\nAparece en la lista del 2\n % de los científicos más citados \ndel mundo en todas las áreas del conocimiento publicada por la U\nniversidad de Stanford. Posición 15 en informática \nen el listado nacional del año 2022. https://research.com/scientists-rankings/computer-science/es .\nMiembro de la Academia Europea de Ciencias (EU-\nRASC) desde 2018. Cuenta con un importante n\u00famero de premios que reconocen su condición de profesional pione-ra y el alto nivel de su especialización. \nDesde el año 2016 es miembro del Consejo de Ciencia y \nTecnología de la región de Madrid. En 2018 fue nombrada miembro del grupo de expertos que asesora al Gobierno de España en materia de inteligencia artificial y Big Data. En 2019 \nparticipó en la Estrategia Española de I+D en Inteligencia Arti-ficial. Es miembro del Consejo Asesor de Inteligencia Artificial del Gobierno de España desde 2020. Representa a España en el grupo de trabajo GPAI - sobre Gobernanza de Datos (2021).\nEs miembro fundador de la Asociación EBRAINS (2019) \npara el proyecto Cerebro Humano y de ODISEIA (Observato-rio del impacto social y ético de la inteligencia artificial (2019). \nHa participado en la creación a nivel mundial del área \nde ontologías, Web semántica y datos enlazados desde sus comienzos, introduciéndola en España. Sus principales contribuciones científicas han sido las metodologías Me-thontology y NeOn, y su trabajo pionero en evaluación de \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   13834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   138 11/5/23   12:0411/5/23   12:04139ontologías en la Universidad de Stanford. En todos ellos ha \nconseguido una altísima proyección internacional. Cuenta con más de 300 publicaciones, más de 26 800 citas acumu-ladas y un índice h de 67. \nLa doctora Gómez-Pérez posee una excepcional y asom-\nbrosa experiencia investigadora tanto en el ámbito nacional como internacional, tanto con fondos p\u00fablicos como con recursos privados. Ha participado en 106 proyectos de los cuales 49 son internacionales y 57 nacionales. De estos 49\u00a0proyectos internacionales, ha coordinado 7 consorcios europeos, en otros 32 ha participado como investigadora principal (IP) por la UPM, y en 10 como miembro del equipo investigador. De estos 57 proyectos nacionales: 50\u00a0proyectos como IP y en 7 como miembro del equipo de la UPM. Además, es la directora de la primera sede del Instituto de Datos Abiertos en España: ODI-Madrid (des-de octubre de 2015). Los fondos de estos proyectos le per-mitieron crear y ahora mantener el Grupo de Ingeniería Ontológica (integrado actualmente por 61 miembros), y dirigir 28 tesis doctorales. Ocho de las tesis dirigidas han sido distinguidas por la UPM con \u00abhonor\u00bb, (premio que es competitivo), y 16 obtuvieron la mención Internacional en el Diploma de Doctorado. La profesora Gómez-Pérez tam-bién ha supervisado más de 20 postdoctorados financiados por programas españoles e internacionales. \nEn el campo de la IA, es responsable del centro de ope-\nraciones de innovación digital de IA y Robótica para los Objetivos de Desarrollo Sostenible, uno de los 30 AI DIH europeos seleccionados por la Comisión Europea en este campo. Además, fue IP en la UPM y miembro del consejo \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   13934439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   139 11/5/23   12:0411/5/23   12:04140asesor consultivo en el proyecto europeo Inteligencia Arti-\nficial para Europa y participa como profesora en el máster Inteligencia Artificial para el Sector P\u00fablico.\nAsimismo, es revisora y evaluadora de proyectos euro-\npeos de la Comisión Europea y de otras agencias europeas, nacionales e internacionales. \nHa sido conferenciante invitada en numerosas universi-\ndades y centros internacionales de investigación de Estados Unidos, del Reino Unido, Irlanda y Francia, entre otros.\nLa evolución y progresión profesional extraordinaria de \nla profesora Gómez-Pérez en el campo de la inteligencia artificial ha pasado, en fin, por cuatro etapas bien definidas:\nLa primera, que comprende los años 1993 a 1999, viene \nmarcada por su estancia postdoctoral en la Universidad de Stanford (EE. UU.) en 1994, mientras trabajaba con Tom Gruber (célebre investigador en el ámbito de la ingeniería ontológica y cofundador de Siri Inc.).\nSu primer artículo sobre Methontology fue publicado \npor la Asociación Americana de Inteligencia Artificial en 1997. Tiempo más tarde, se empleó esta metodología para desarrollar una ontología en el dominio de la química, y el artículo consiguió ser uno de los 15 más citados en la his-toria de la revista hasta el año 2008. El éxito de la metodo-logía y su aceptación por parte de la comunidad la llevaron a escribir en el año 2003 el primer libro del mundo en in-glés sobre ingeniería ontológica, que incluía ejemplos to-mados de las áreas de gestión del conocimiento, comercio electrónico y web semántica.\nParalelamente a la metodología, la Prof. Gómez-Pérez \nlideró el desarrollo de un software denominado WebODE, \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   14034439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   140 11/5/23   12:0411/5/23   12:04141cuyo objetivo era ayudar a los desarrolladores a crear onto-\nlogías en colaboración con equipos de ontólogos utilizando una interfaz web.\nEl impacto de Methontology no se puede medir tenien-\ndo en cuenta \u00fanicamente el n\u00famero de citas acumuladas. Recientemente (2015), se ha empleado para desarrollar otras ontologías importantes como CORA y, además, se ha recomendado en programas educativos en universidades como la de Edimburgo, en el Instituto de Tecnologías de Massachusetts, Universidad de Milán, Universidad Politéc-nica de Cataluña, Universidad de Zaragoza, Universidad Nacional de Educación a Distancia, Universidad Carlos III de Madrid, Universidad de Murcia, entre otras.\nLa segunda fase de la evolución profesional de Asunción \nGómez-Pérez comprende el periodo 2000-2010 y viene marcada por la expansión de su actividad en materia de ontologías por todo el mundo, así como por su participa-ción en la creación de la web semántica. Asumió la coordi-nación de tres proyectos europeos en el sexto y séptimo programa marco de la Unión Europea y fue investigadora principal de la UPM en otros seis, realizando actividades de coordinación científica en ellos. \nEn el transcurso de todos estos años, investigó en: reuti-\nlización de ontologías, integración de ontologías, reingenie-ría de ontologías, evaluación de ontologías, aprendizaje de ontologías, alineamiento de ontologías y bases de datos, ontologías multilingües y localización de ontologías. A modo de compilación de todos los trabajos se publicó el li-bro Ingeniería Ontológica en un mundo conectado, del que \nAsunción Gómez-Pérez fue coautora junto con otros cuatro \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   14134439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   141 11/5/23   12:0411/5/23   12:04142investigadores. En él se recogen las aportaciones más rele-\nvantes de los principales investigadores de aquellos años.\nLa tercera fase de la evolución profesional de la profeso-\nra Gómez-Pérez llega hasta 2016. En este periodo destaca su investigación orientada hacia la aplicación de datos en-lazados en cuestiones más amplias como los campos de la geografía, la meteorología, consumo energético en ciuda-des y el nuevo portal de datos enlazados de la Biblioteca Nacional de España. \nEn una Europa multilingüe, la investigación previa so-\nbre ontologías multilingües y localización de ontologías la llevó a centrarse de forma intensiva en los datos lingüísticos enlazados. Estos persiguen transformar los recursos lingüís-ticos en RDF y relacionarlos con otros recursos. Un artícu-lo de su equipo, que versaba sobre el enriquecimiento de una ontología con información multilingüe fue pionero en este tema. Esta investigación condujo al proyecto europeo Monnet, en el que el modelo LIR se tomó como input para el estándar W3C denominado Lemon-Ontolex. En el año 2013, la Prof. Gómez-Pérez coordinó el proyecto de la UE sobre datos lingüísticos enlazados con el objetivo de gene-rar recursos lingüísticos en formato de datos enlazados a través de Lemon-Ontolex.\nLa evaluación de ontologías fue su primer tema de in-\nvestigación en Stanford y, desde entonces, siempre ha esta-do presente en su trayectoria investigadora. Su amplia ex-periencia en este tema le permitió supervisar una tesis doctoral que resume y da a conocer las \u00faltimas novedades sobre el tema de evaluación de ontologías y el software OOPS! OOPS! es una aplicación online para la evaluación \n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   14234439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   142 11/5/23   12:0411/5/23   12:04143de ontologías que ha sido ampliamente aceptado por un \nelevado n\u00famero de usuarios en todo el mundo y se ha em-pleado más de 4 000 veces en unos 60 países diferentes. \nEn fin, el periodo actual de su actividad profesional se \ncaracteriza por su dedicación a tareas de gestión universita-ria junto a las docentes y de investigación que han marcado toda su trayectoria. Desde 2016 es vicerrectora de Investiga-ción, Innovación y Doctorado en la Universidad Politécnica de Madrid. Desde entonces, y tal y como se ha descrito an-teriormente, ha ampliado su actividad al trabajar como revi-sora y evaluadora internacional, nacional y regional. Tam-bién forma parte del panel evaluador de los proyectos de consolidación del Consejo Europeo de Investigación. En el ámbito de la IA, es responsable del intercambiador de inno-vación digital de IA y Robótica para los Objetivos de Desa-rrollo Sostenible, uno de los 30 intercambiadores de innova-ción digital europeos en materia de inteligencia artificial seleccionados por la Comisión Europea en este campo. Ade-más, fue IP en la UPM y miembro del consejo asesor con-sultivo en el proyecto europeo Inteligencia Artificial para Europa, y participa como profesora en el máster Inteligencia Artificial para el sector p\u00fablico.\n*\n * *\nAhora te esperan en la RAE, querida Asunción, retos muy importantes. \nTe acogemos confiados en tu trabajo y gran-\ndes conocimientos, y te deseamos muchos éxitos y felicidad al servicio de nuestra lengua y de los proyectos de esta cen-tenaria institución.\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   14334439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   143 11/5/23   12:0411/5/23   12:0434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   14434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   144 11/5/23   12:0411/5/23   12:04145Índice\nINTELIGENCIA ARTIFICIAL Y LENGUA ES-\nPAÑOLA  16\n1. DEFINICIÓN DE INTELIGENCIA AR\nTIFI-\nCIAL  18\n2. MÁQUINAS CON\n CAPACIDADES COGNI-\nTIVAS  25\n2.1. Máquinas precursoras \nde tipo mecánico y \nelectromecánico  25\n2.2. El lenguaje de los ordenadores   28\n2.3. ¿Cómo dotar de capacidades cognitivas a las \nmáquinas?  32\n3. INTELIGENCIA ARTIFICIAL SUBSIMBÓ -\nLICA\n Y SIMBÓLICA  36\n4. PRINCIPALES APOR\nTACIONES DE LA IN-\nTELIGENCIA ARTIFICIAL  40\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   14534439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   145 11/5/23   12:0411/5/23   12:041465. INTELIGENCIA ARTIFICIAL Y PROCESA-\nMIENTO DEL\u00a0LENGUAJE NATURAL   54\n6. ESPACIOS DE\n DATOS LING\u00dcÍSTICOS Y \nPRINCIPIOS FAIR  74\n7. INTELIGENCIA ARTIFICIAL \nY LENGUA \nESPAÑOLA  83\n7.1. Situación de partida de la informática en la \nRAE   83\n7.2. Rumbo al nuevo mundo de la inteligencia artificial  \n 90\n7.3. Incorporar los materiales en el espacio eu -\nropeo de datos lingüísticos  95\n7.4. Modelos de lenguaje en español  96\n7.5. Gobernanza de los materiales lingüísticos  97\n7.6. Plataforma software para la inteligencia arti\n-\nficial  100\n7.7. Inteligencia artificial en los procesos pr\no-\nductivos de la RAE  103\n7.8. Sistema de consultas lingüísticas  103\n7.9. Verificador lingüístico en línea para el es -\npañol  105\n7.10.  Observatorio del sistema lingüístico del es\n-\npañol en\u00a0Internet  105\n7.11.  La necesidad de evaluar de forma \ncompara-\ntiva y\u00a0consensuada  107\n7.12.  Ecosistema de innovación abier\nta  109\n8. HACIA UNA REGUL\nACIÓN EUROPEA DE \nLA INTELIGENCIA ARTIFICIAL  111\n34439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   14634439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   146 11/5/23   12:0411/5/23   12:0434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   14734439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   147 11/5/23   12:0411/5/23   12:0434439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   14834439_DiscursoDeIngresoDeAsuncionGomezPerez.indd   148 11/5/23   12:0411/5/23   12:04", "language": "PDF", "image": "PDF", "pagetype": "PDF", "links": "PDF"}