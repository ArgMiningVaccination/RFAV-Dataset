{"title": "PDF", "author": "PDF", "url": "escholarship.org/content/qt6nk2s73b/qt6nk2s73b_noSplash_8593d2054261393405b820e6f3875d8b.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "UNIVERSITY OF CALIFORNIA SANTA CRUZ CONSERVATIVE NUTRITION: THE INDUSTRIAL FOOD SUPPLY AND ITS CRITICS, 1915-1985 A dissertation submitted in partial satisfaction of the requirements for the degree of DOCTOR OF PHILOSOPHY In HISTORY by Martin Renner September 2012 The Dissertation of Martin Renner is approved: _____________________________________ Professor Edmund Burke III, _____________________________________ Professor Mark Cioc _____________________________ Tyrus Miller Vice Provost and Dean of Graduate Studies Copyright \u00a9 by Mart in Renner 2012 iii TABLE OF CONTENTS List of Abbreviations Abstract Acknowledgemen ts Introduction 1. \"Demineralized, Devitaminized, Devitalized\": Nutrition Science and the Attack on Processed Foods in the Interwar Period 2. \"The Mouth, Barometer of Health\": The Rise and Decline of Nutritional Dentistry 3. \"Health from the Ground Up\": Nutrition Science and Ecology Befo re Environmentalism 4. Better Bread Through Chemistry? Conservative Nutrition and the Enrichment Controversy 5. Food Faddism, Fat Fears, and the Decline of Conservative Nutrition After World War II Epilogue and Conclusion Bibliography iv v vii 1 17 100 188 259 315 407 427 iv LIST OF ABBREVIATIONS AAAN American Academy of Applied Nutrition ADA American Dental Association AMA American Medical Association BMA British Medical Association CO Colonial Office FDA Food and Drug Administration FNB Food and Nutrition Board GPO Government Printing Office HMSO His/Her Majesty's Stationary Office MRC Medical Research Council NAS National Academy of Sciences NIH National Institutes of Health NRC National Research Council SCS Soil Conservation Service USDA United States Department of Agriculture v ABSTRACT Mart in Renner Conservative Nutrition: The Industrial Food Supply and Its Critics, 1915-1985 Nutritional thinking has undergone several drastic shifts in the past century. Within two decades of the discovery of vitamins in the 1910s, nutrition experts had begun to argue that much of the ill health in the industrial West was caused by the consumption of too many nutritionally impoverished \"processed foods\"\u2014highly refined cereals, sugars, and fats\u2014and too few vitamin- and mineral-rich \"protective foods\"\u2014dairy products, eggs, organs, meat, and fresh fruits and vegetables. But between the 1950s and the 1980s, a growing number of nutrition authorities started branding many of these very same foods as artery-clogging killers. The key to avoiding atherosclerosis and other \"diseases of civilization,\" these experts told the public, was to restrict animal fat consumption and eat more carbohydrates. Yet recently, the low-fat, high-carbohydrate message has come under increasing scientific scrutiny. Nutritional thinking, it appears, is poised for another about-face. Some scholars have come to the conclusion that authoritative dietary advice keeps changing because nutrition science is inherently too uncertain to permit theories about the nature of the ideal diet to be definitively proved or disproved. Others have gone even further and claimed that the nutrient-centered approach to food that has defined the field since its origins over a century ago is at fault. This dissertation challenges these interpretations. It argues that scientific knowledge of the relationship between food and health, in fact, increas ed in depth and coherence between the 1910s and 1980s, when official dietary advice was going though its seemi ngly greatest period of flux. During that time, researchers working in a wide range of fields accumulated a vast amount of scientific evidence against vi the consumption of the highly processed \"industrial foods\" and in favor of what nutritionists used to call the \"protective foods.\" But this body of evidence failed to cohere into unified perspective, and as a result there were constant shifts in what health authorities considered the ideal diet. vii ACKNOWLEDGEMENTS This study arose from casual interest of mine. I was contemplating entering into nutrition science after an undergraduate experience mostly in the humanities and, for better or worse, I thought it would be worthwhile to study the history of the field first. There is absolutely no way that this casual interest could have turned into a bona fide dissertation, all these years later, without the unstinting support of my advisors, friends, and family. My first thanks go to my advisor, Edmund \"Terry\" Burke III. Not only while I was working on this dissertation but throughout my entire graduate career, I benefited imme asurably from his open yet discriminating mind. Terry has truly been a mentor and intellectual companion. I am also indebted to the other members of my reading committee, Jonathan Beecher and Mark Cioc, both of whom, in their own way, assured me that my project was worth pursuing at a time when I wasn't so sure. In addition, the archivists and librarians in charge of the many collections that I visited for this dissertation were exceedingly helpful. I am also deeply grateful to Gary Taubes, Philippe Hujoel, and Joseph Heckman for reading parts of my dissertation and providing insightful feedback. Special thanks go to Gary, whose work helped me to make sense of the immense and often confusing literature on heart disease, diabetes, and obesity that I encountered in the course of doing my research. I would like to express my appreciation to the History Department at the University of California, Santa Cruz for supporting my graduate work. My thanks also go to the Andrew W. Mellon Foundation (Council on Library and Information Resources Mellon Fellowship for Dissertation Research in Original Sources) and the Mellon W. Foundation viii and the American Council of Learned Societies and (Mellon/ACLS Dissertation Completion Fellowship) for providing financial support for both living and research expenses. My friends and family have been a huge source of encouragement and, at times, much-needed diversion. You are too numerous to name here, but you know who you are. I would be remiss, however, if I didn't give special thanks to my brother and sister-in-law, Ted Renner and Carrie Barjenbruch, and my friends, Matt Mariola and Debora Galaz, for opening their homes to me while I was out on the lonely road. I would also like to thank my boss and human dynamo, Paul Kaiser, for being so flexible in scheduling work, even when weeds threatened and crops desperately needed to get planted. Ever important have been my father and mother, Peter and Heather Renner, whose love and support sustained me in ways that cannot be described. Lastly, I want to express my deepest love and gratitude to my wife, Maja Sidzinska, who came into my life when I was starting on a particularly trying phase of this dissertation. I hope she will be even more forbearing if, at some future time, I finally decide to work toward that degree in nutrition science. 1 INTRODUCTION The history of nutrition affords a background that promotes a balanced if less enthusiastic evaluation of current discoveries. Careful study of history will reveal in each decade during the past two centuries some optimist who was convinced that his age knew almost the last word in nutrition with little hope for great advances. But history tends to inculcate a spirit of modesty in regard to our own time and to make us realize that we have made but a beginning in solving the intricate and difficult problems of feeding men. Clive McCay, Journal of the American Dietetics Association, 19471 The field of nutrition science has undergone several drastic shifts over the course of the past century. With the discovery of the first vitamins in the 1910s, scientists moved beyond the previous view of nutrition, which focused on carbohydrate, fat, and protein. In the following three decades, researchers discovered all the vitamins and minerals necessary for physical wellbeing. The \"newer knowledge of nutrition,\" as this body of discoveries was commonly termed, undoubtedly contributed to the lasting improvement of public health by setting the stage for the eradication of vitamin and mineral deficiency diseases. By the latter half of the 1940s, therefore, nutritionists could justifiably feel that they had made great progress, as Clive McCay put it, \"in solving the intricate and difficult problems of feeding men.\" But developments in subsequent decades appear to have amply borne out McCay's cautious outlook on nutrition science. Beginning in the 1950s, a growing number of researchers argued that the consumption of fat, and saturated fat in particular, was responsible for the rising rates of coronary heart disease in the affluent Western countries. Since consumption of saturated fat elevated blood cholesterol levels, and since blood cholesterol levels were positively associated in some studies with heart disease incidence, it 1 Clive McCay, \"Four Pioneers in the Science of Nutrition: Lind, Rumford, Chadwick, and Graham,\" Journal of the American Dietetics Association 23 (April, 1947): 401-02. 2 was inferred that saturated fat was to blame. The proponents of this \"fat-cholesterol hypothesis\" thus advised the public to eat more carbohydrate-rich foods and fewer of the very foods that nutritionists had previously argued should be eaten in liberal quantities on account of their high vitamin and mineral content\u2014whole milk, cheese, butter, eggs, organs, and meat. Of the fats that were eaten, advocates of the fat-cholesterol hypothesis recommended that they be mostly of the polyunsaturated variety, such as were found in vegetable oils and margarine spreads derived from maize, soybeans, and canola. Although there was some resistance to this hypothesis in the scientific and medical communities, by the 1970s and 1980s virtually all public health organizations in the affluent West had put their weight behind it and changed their dietary guidelines accordingly. More recently, though, some prominent nutritionists, medical researchers, and science writers have pointed out that the fat-cholesterol hypothesis has had little solid scientific backing from the very beginning. This hypothesis, it is becoming increasingly clear, was based on faulty reasoning and a number of methodologically flawed studies. The incidence of heart disease has not declined, despite the fact that Western populations have cut down on their consumption of saturated fat and cholesterol. Moreover, there is an ever-growing body of evidence indicating that high-fat, low-carbohydrate diets perform better than low-fat, high-carbohydrate diets with respect to weight loss and reducing risk factors for heart disease. Nutritional thinking, it seems, is in the throes of yet another sea change.2 2 See, e.g., Frank B. Hu, JoAnn E. Manson, and Walter C. Willett, \"Types of Dietary Fat and Risk of Coronary Heart Disease: A Critical Review,\" Journal of the American College of Nutrition 20, no. 1 (2001): 5-19; Gary Taubes, \"The Soft Science of Dietary Fat,\" Science 291, no. (March 2001): 2536-45; Sylvan Lee Weinberg, \"The Diet-Heart Hypothesis: A Critique,\" Journal of the American College of Cardiology 43, no. 5 (3 March 2004): 731-33; Adele H. Valerie Goldstein \"Low-Carbohydrate Diet Review: Shifting the Paradigm,\" Nutrition in Clinical Practice 26, no. 3 (June 2011): 300-08. 3 In the past couple decades, a growing number of scholars have sought to understand why nutrition scientists, after over a century of accumulating research, have been unable to answer the question that stands at the core of their discipline: What is the ideal diet? In the large body of literature that has consequently developed, explanations for this question have taken two general forms, which can be described as \"extrascientific\" and \"trans-scientific.\" Historians and sociologists of science have found that factors other than those related to methodology and experimental procedure are necessary for explaining why nutrition researchers have failed to reach a consensus on the nature of the ideal diet. These extrascientific factors include differing professional values among nutrition and medical experts, varied economic interests, individual agendas and political inclinations, and contrasting views on human educability. Scholars have also determined that major historical developments, such as the world wars and periods of economic depression and prosperity, also strongly influenced how nutrition scientists generated and interpreted their findings.3 3 See, e.g., Harvey Levenstein, Revolution at the Table: The Transformation of the American Diet (New York, Oxford: Oxford University Press, 1988), 158; Harvey Levenstein, Paradox of Plenty: A Social History of Eating in Modern America (New York, Oxford: Oxford University Press, 1993); Harmke Kamminga and Andrew Cunningham, eds., The Science and Culture of Nutrition, 1840-1940 (Amsterdam/Atlanta: Rodopi, 1995); Donna Maura and Jeffrey Sobal, eds., Eating Agendas: Food and Nutrition as Social Problems (New York: Aldine de Gruyter, 1995); Rima Apple, Vitamania: Vitamins and American Culture (New Brunswick, NJ: Rutgers University Press, 1996); David F. Smith, ed., Nutrition in Britain: Science, Scientists, and Politics in the Twentieth Century (London/New York: Routledge, 1997); David F. Smith and Jim Phillips, eds., Food, Science, Policy, and Regulation in the Twentieth Century: International and Comparative Perspectives (London/New York: Routledge, 2007); Charles Webster, \"Healthy (1982): 110-29; Madeline Mayhew, \"The 1930s Nutrition Controversy,\" Journal of Contemporary History 23, no. 3 (July 1988): 445-65; David F. Smith and of Paton, Findlay and Cathcart: Conservative Thought in Chemical Physiology, Nutrition and Public Health,\" Social Studies of Science 19, no. 2 (May 1989): 195-238; David F. Smith and Mark of Parturiunt Montes, Nascetur Ridiculus The BMA Committee 1947-1950 and the Political Disengagement of Nutrition Science,\" Journal of the History of Medicine and Allied Sciences 59, no. 2 (2004): 240-72; Karin Garrety, \"Social Worlds, Actor-Networks and Controversy: The Case of Cholesterol, Dietary Fat and Heart Disease,\" Social Studies of Science 27 (1997): 727-73; Karin Garrety, \"Science, Policy, and Controversy in the Cholesterol Arena,\" Symbolic Interaction 21 (1998): 401-24; Mark W. Bufton, David F. Smith, and Virginia Berridge, \"Professional Ambitions, 4 Some observers have also pointed out that nutrition scientists cannot conduct the kinds of studies needed to determine the exact nature of the ideal diet. In today's world of evidence-based medicine, the randomized, double-blind, placebo-controlled, clinical trial is the gold standard of scientific investigation. While research ers have used this type of study to assess the health effects of vitamin, mineral, and antioxidant supplemen ts, it is virtually impossible to do so in the case of whole diets. Altering the proportion of one class of foods in an experimental diet\u2014animal products containing large quantities of saturated fat and cholesterol, for example\u2014often requires altering the proportion of another class of foods, unless the total caloric intake is reduced. As a result, dietary trials cannot test the effect of changing just one variable while controlling for all the others. Another vexing problem for nutrition research ers is the so-called placebo effect. A significant percen tage of test subjects respond to a treatment or intervention regardless of whether they have actually received it. Whereas investigators in drug and supplemen t trials can correct for this effect by using a placebo such as a sugar pill, in dietary studies it is virtually impossible to get subjects to confuse a substitute food for the real thing. Nor have research ers had the time or the wherewithal to perform the large, controlled clinical investigations required to determine the effects of different dietary intakes on long-term, let alone multigenerational, health outcomes. In place of such studies, nutrition scientists have resorted to less reliable types of evidence. Researchers who have investigated the long-term impacts of diet on health have typically relied on feeding experiments with short-lived animals, since human subjects can only participate in this type of study for just a fraction of their lifetimes. But, as critics have Political Inclinations, and Protein Problems: Conflicts and Compromise in the BMA Nutrition Committee 1947-1950,\" Medical History 47, no. 4 (2003): 473-92; Michael Ackerman, \"Interpreting the 'Newer Knowledge of Nutrition': Science, Interests, and Values in the Making of Dietary Advice in the United States, 1915-1965\" (Ph.D. diss, University of Virginia, 2005). 5 pointed out, the relevance of animal-feeding experiments to humans is questionable, because the diets used in these experiments greatly differ from what people actually eat, and human beings are biologically very different from small animals like the mouse and the rat. Scientists have also depended on epidemiological studies to evaluate the healthfulness of various diets. These types of investigations, however, only suggest correlations between nutrient intakes and health outcomes, and typically cannot prove cause and effect. One type of epidemiological study, the large-scale dietary intervention trial, is viewed as the most reliable form of nutrition research. But these studies are labor-intensive, expensive, and must extend over long periods of time to detect relationships between eating habits and the development of chronic diseases. In addition, intervention trials tend to provide an inaccurate picture of dietary intake, since participants in treatment groups often misreport food consumption patterns to approximate the dietary advice that they receive from media sources or those conducting the trial. Finally, even if reliable general findings could be obtained from large-scale studies, human metabolic variability is potentially so great that they might have only limited applicability to individuals. Nutrition science, in short, is inherently too uncertain to permit experts in the field to make uncontroversial claims about the influence of food on health. In 1972, Alvin Weinberg, director of the Oak Ridge National Laboratory, aptly described this kind of problem as \"trans-scientific,\" since it cannot be resolved within the normal channels of scientific experimentation.4 In recent years, some critics have moved beyond pointing out the problems bedeviling the production, so to speak, of nutritional knowledge, and have questioned the very conceptual framework in which nutrition science has operated since its origins over a 4 Alvin M. Weinberg, \"Science and Trans-Science,\" Minerva 10 (April 1972): 'Newer Knowledge of Nutrition',\" 22-23, 702-03; Marion Nestle, Food Politics, rev. ed. (Berkeley: University of California Pres, 2007), 395-405; Gary Taubes, Good Calories, Bad Calories (New York: Knopf, 2007), 22-41. 6 century ago. Sociologist Gyorgy Scrinis has been particularly influential in this regard. He identifies what he calls the ideology of \"nutritionism,\" which he defines as a nutritionally reductionist or nutrient-centered approach to food rooted in biochemistry. According to Scrinis, in the past thirty years the paradigm of nutritionism has moved from professional and governmental circles into the realm of popular consciousness\u2014with far-reaching negative consequences. Nutritionism, he claims, has created the conditions for popular confusion about diet, unwarranted dependence on scientific expertise, susceptibility to dubious food mark eting strategies, and a general sense of anxiety about what to eat. In the process, nutritionism has undermined other, more healthful ways of engaging with and understanding foods, including what Scrinis calls \"traditional, cultural, sensual, and ecological approaches.\"5 A number of other scholars have attacked the historical legacy of nutrition science on similar grounds. Jane Dixon, for example, argues that an insidious process of \"nutritionalisation,\" wherein diets are promoted on the sole criterion of their nutritional qualities, has been progressing since the early twentieth century. This process, she contends, has given rise to the \"nutricentric citizen,\" whose life is ruled by abstract biomark ers such as \"good\" and \"bad\" cholesterol, daily calorie and protein requirements, and the body mass index (BMI). Dixon claims that nutrition scientists have played a crucial role in creating the nutricentric citizen, and in driving the widespread adoption of the \"industrial diet\" at the expense of putatively healthier \"peasant type plant-based diets.\" As a remedy to these two problems, Dixon proposes a \"pre-industrial ecological nutrition approach,\" which, she 5 Gyorgy Scrinis, Ideology of Nutritionism,\" Gastronomica 39 (Winter 2008): 39-48. 7 alleges, moves beyond the traditional biochemical orientation of nutrition science to include social, cultural, and environmental dimensions.6 Michael Pollan offers a similarly negative portrayal of the historical record of nutrition science in his widely read treatise, In Defense of Food: An Eater's Manifesto (2008). The nutrient-centered approach to diet and health that has been championed by nutrition authorities for the past century, according to Pollan, \"supplies the ultimate justification for processing food by implying that with a judicious application of food science, fake foods can be made even more nutritious than the real thing.\" But time and again, he points out, this approach has made people less, not more, healthy. Pollan maintains that his dietary advice\u2014\"Eat Food. Not too much. Mostly plants\"\u2014is informed just as much by the experience of traditional food cultures as it is by the findings of nutrition science.7 And so, it appears from the recent literature, nutrition scientists have failed to provide any definitive answers as to the nature of the ideal diet, either because they cannot conduct sufficiently rigorous experiments, or because they have been led badly astray by the nutrient-centered approach to food that has defined their field since its earliest days. However, the scientific tradition that I will describe in the following chapters\u2014which I will call \"conservative nutrition\"\u2014does not wholly concur with these interpretations. I use the term \"conservative\" in a sense that is not necessarily political. Rather, it describes a cautious attitude toward several of the major changes in food production and consumption that occurred coincident with the rise and spread of industrialization in the nineteenth and early twentieth centuries. Over the course of this period, highly refined flour and cereals, sugar, 6 Jane Dixon, \"From the Imperial to the Empty Calorie: How Nutrition Relations Underpin Food Regime Transitions,\" Agriculture and Human Values 26 (2009): 321-33. 7 Michael Pollan, In Defense of Food: An Eater's Manifesto (New York: Penguin, 2008), 32, 139. 8 and vegetable oils became dietary staples in the industrial Western countries and in locales brought into contact with them through trade, colonization, and settlement. The first sustained scientific attack on these novel foods\u2014and the starting point of this study\u2014occurred from the 1910s to the 1930s, and coincided roughly with the discovery of vitamins and minerals. The generation of nutritionists who made these discoveries became convinced that Westerners needed to return to diets consisting mostly of unrefined \"natural foods,\" though not constrained, as in the preindustrial past, by seasonal scarcity and outright shortage. They came to hold in particularly high regard what biochemist Elmer McCollum originally termed the \"protective foods\": dairy products, eggs, organ meat s, leafy green vegetables, certain vitamin-rich fruits, and to a lesser extent muscle meats and wholegrain cereals. Over the course of the interwar period, many leading figures in medicine and dentistry also became convinced that the excessive use of highly processed grains, sugars, and oils was a major threat to public health, and joined nutritionists in promoting the increased consumption of the \"protective foods.\" The scientific evidence against the \"industrial diet\" and in favor of the \"protective foods\" continued to accumulate in a wide range of fields in the decades following World War II, but much of it was disregarded or simply did not have an impact on the development of the conventional wisdom on diet during that time. It was the failure of many nutrition authorities to recognize this unique requirement for intrascientific agreement, I believe, that played a large role in leading them so badly astray in their efforts to improve public health in the latter half of the twentieth century. Here I differ somewhat from the scholars who have argued that it is the uncertainty inherent to nutrition science, in combination with extrascientific factors, that has propelled the seemingly unceasing vicissitudes in what experts in the field consider \"healthy eating.\" 9 This is not to say, however, that the field of nutrition science has been free from uncertainty or external influences. Indeed, as I will emphasize in the following chapters, contradictory impulses have long coexisted in tension in nutrition science, and even in the minds of individual nutrition scientists. These contradictory impulses have operated on several levels. On an epistemological level, there has been an abiding conviction among nutrition scientists in the power of \"reductionist science\" to uncover the chemical makeup of food and how it affects the body. Yet leading nutritionists have noted for decades how little science can know in the face of the immense complexity and dynamism of the natural world, including our bodies and the foods that go into them. On an institutional level, as historians and sociologists of science have repeatedly demonstrated, nutrition has been a field in which many powerful interests have had a stake: food processing companies, vitamin manufacturers, farmer groups, public health authorities, magazine and book publishers, and university science departments, to name just a few of the larger players. It is therefore unsurprising that the field has long been characterized by a messy give-and-take between the increm ental pursuit of truth and the more immediate motivations to maximize market share, exploit a new discovery, or advance a particular policy agenda. Nutrition science has also generated tensions, for lack of a better term, on an existential level. The main goal of nutritionists\u2014to expand our knowledge about diet and health\u2014is an admirable one, given people's understandable desire to avoid disease and prolong the productive years of life. The idea that eating should be foremost about physical health, however, can all too easily undermine the other important purposes of eating. \"Food,\" as Pollan eloquently puts it, \"is also about pleasure, about family and spirituality, about our relationship to the natural world, and about expressing our identity.\"8 8 Pollan, In Defense of Food, 8. 10 Notwithstanding these numerous tensions and contradictions, I have found that nutrition research ers have been successful over the long term in uncovering what, generally speaking, constitutes \"healthy eating.\" The scientific tradition that I will describe in the following chapters does not conform neatly with the portrayal of a field defined, in the end, by the extrascientific considerations of its participants or by perpetual uncertainty about the nature of the ideal diet. Nor is it consistent with the view that nutrition science is a field fundamen tally flawed by hubristic reductionism. In fact, the endeavor in the twentieth century to understand the relationship between food and health through the \"reductionist\" lens of the nutrient did not undermine so-called traditional, cultural, sensual, or ecological approaches, as is alleged in the literature on \"nutritionism,\" but as often as not reinforced them. It is a curious and unfortunate accident of history that these terms are now used as rhetorical bludgeons against \"mainstream\" nutrition science. My study will, I hope, shed some light on why this occurred. Dissertation Prospectus This dissertation is a study of the rise and decline of the conservative nutrition perspective between the 1910s and the 1980s. Chapter one explores the transformative effect that the discovery of vitamins had on scientists and physicians' understanding of the relationship between food and health in the first half of the twentieth century. I also examine how the growing knowledge of vitamins and minerals informed the development of the concepts of \"Western\" and \"primitive\" diets during the interwar period. Nutrition researchers at that time came to view the widespread adoption of a diet based on white flour, refined cereals and sugar, and vegetable oils in the industrial West as a historically unique 11 and potentially dangerous phenomenon. In fact, some of the critics of these foods came to view the dietary changes that were taking place through the prism of human evolution. Chapters two and three show how the nutritional critique of the industrial food supply spread to other scientific fields in the interwar period. In chapter two, I discuss the profoundly divisive effect that the vitamin and mineral discoveries had on the dental profession. Beginning in the 1920s, a vanguard of nutrition-minded dental researchers argued that the increasing consumption of processed foods was responsible for the rising rates of dental decay, gum disease, and even the degeneration of maxillofacial structure in the populations of the industrial Western countries. Many leading dental authorities, however, dismissed the notion that vitamin and mineral undernutrition played a role in these conditions, and insisted that diet was important to dental disease only insofar as it affected the bacterial environment in the mouth. Although the controversy between these two viewpoints was never fully settled, dental researchers quickly came to agreement on the type of diet that Westerners needed to consume to avoid tooth decay as well as nutrient deficiencies: one that was rich in natural foods, particularly animal products, and low in refined starches and sugar. Chapter three describes a network of scientists who developed an agricultural and ecological dimension of the nutritional critique of the industrial food supply in the interwar period. They came to the conclusion that trends in agriculture\u2014soil erosion, declining soil fertility, and the privileging of quantity over quality in the production of crops\u2014were compromising the nutritional quality of foods in much the same way as food processing. I argue that these scientists articulated a remarkably modern view of the food chain that connected human health with the health of plants, animals, and soils. In fact, by the late 12 1930s and early 1940s, this view of the food supply was beginning to influence the research agendas of some leading scientific institutions and the discourse of policymakers. In chapter four, I describe an episode in the history of nutrition science that throws doubt on the notion that advances in the understanding of the chemical makeup of food have abetted, as is often implied in the literature on \"nutritionism,\" the industrialization of the food supply. As the 1920s and 1930s progressed, nutrition scientists in the United States and Britain became increasingly alarmed by evidence that the consumption of white flour was contributing to widespread vitamin and mineral deficiencies. They realized that, before the near-total replacement of stone grinding mills by steel roller mills in the late nineteenth century, unrefined or nearly unrefined flour and bread had been a significant source of some B-complex vitamins, as well as iron. Following the successful chemical synthesis and mass production of some of the B-complex vitamins\u2014thiamin, riboflavin, and niacin\u2014in the late 1930s and early 1940s, some nutritionists and physicians began campaigning for laws mandating the addition of these vitamins to white flour up to the levels naturally found in whole-wheat flour. But the \"enrichment movement,\" as this camp aign came to be called, sparked a highly divisive controversy among nutrition scientists. Although the proponents of enrichment in the United States eventually succeeded in getting mandatory enrichment laws passed in the 1940s, some leading American nutritionists opposed enrichment when it was first proposed on the grounds that it contravened their longstanding belief that most people needed to eat more unprocessed, natural foods. Even many of the researchers who endorsed enrichment saw it as only a temporary means for combating several widespread vitamin and mineral deficiencies until Americans could be convinced to consume more wholegrain cereals and other natural foods. Enrichment was also proposed in Britain, but the country's nutrition and medical authorities turned increas ingly against it in the 1940s. 13 This stance drew them into conflict with the milling and baking industries, whose spokesmen insisted that vitamin enrichment was vastly prefer able to legislation restricting how flour was milled. The debate between science and industry was still unsettled when the government mandated the exclusive production of \"National wheatmeal\"\u2014a type of flour that was less refined than ordinary white flour but had most of the fibrous bran removed\u2014during World War II due to shortages of shipping space. Even after white flour became available with the official end of rationing in 1953, the British government continued to subsidize bread made from National wheatmeal in deference to the views of the country's nutrition experts. Chapter five describes the waning of the conservative nutrition viewpoint in mainstream scientific thought in the three decades following World War II. I ascribe this decline to two developments. First, a polarizing process took place in the 1950s and early 1960s between nutrition authorities, who maintained that vitamin and mineral deficiencies were no longer a major public health problem, and the advocates of what became known as the \"natural food movement,\" who continued to insist that malnutrition was widespread in the West. Second, the rise of the fat-cholesterol hypothesis between the 1950s and the 1970s threw into doubt much of the dietary advice that nutritionists had been giving in the first half of the twentieth century. In the space of just a few decades, traditional animal products such as milk, butter, eggs, and organ meats went from being vitamin- and mineral-rich dietary essentials to artery-clogging killers. Yet this shift in nutritional thinking did not go uncontested. Skeptics pointed out that the evidence used in support of the fat-cholesterol hypothesis failed to constitute scientific proof. Moreover, researchers working in the field of lipid metabolism were making findings during that time that pointed to modern refined 14 foods, rather than saturated fat and cholesterol, as the more likely culprit in the rising incidence of heart disease. Indeed, as I explain in the epilogue, investigators in a wide range of scientific fields continued to accumulate evidence during the latter half of the twentieth century showing the negative effects of highly processed grains, sweeteners, and oils on health. However, the focus of concern in this period was not on vitamin and mineral deficiencies, as it had been between the 1910s and 1940s, but on chronic conditions such as diabetes and obesity. This more recent research, when placed in the scientific tradition described in the previous chapters, indicates that nutrition science does not need to be seen as a field fated to unending reversals in what it views as a healthful diet. As I suggest in my concluding remark s, an understanding of the development of the conservative nutrition perspective over the course of the twentieth century might be important in creating a future for a more coherent and unifying approach to \"solving the intricate and difficult problems of feeding men.\" A Note on Terms I use the terms \"nutritionist\" and \"nutrition scientist\" throughout this dissertation to refer to individuals in professions requiring expertise in nutrition. This includes research ers with training in chemistry, biochemistry, physiology, pathology, as well as physicians with degrees in both science and medicine engaged in nutrition-related work. It should be noted that, while the term \"nutrition\" was first used to refer to a branch of science in the early 1900s, the first professional journals and associations devoted explicitly to nutrition science did not emerge until the interwar period, and the first departments and schools of nutrition were not founded until the 1940s. Who a nutrition scientist was therefore remained somewhat indeterminate well into the twentieth century. To avoid encumbering the flow of 15 the text, I also sometimes use the term \"nutrition researcher\" to refer to individuals who were not, strictly speaking, nutrition scientists, though the professional background of the researcher under discussion should be obvious from the context in which the term is being used. Another term that I use in this dissertation, \"Western diet,\" has meant different things to differen t commentators over the past century. Here the term refers to a diet consisting largely of refined flour, cereal products, and sugar, and to a lesser extent muscle meat s and refined vegetable fats and oils. This type of diet originated for the most part in the Anglo-American countries and, as I will explain in chapter one, was first identified as a distinctive eating pattern by British and American nutritionists in the first half of the twentieth century. In more recent decades, the consumption of large amounts of meat, eggs, and dairy products has also come to be associated with the \"Western diet.\" This criterion, however, is highly problemat ic, since researchers have provided ample evidence over the past century that the consumption of liberal quantities of these foods is not unique to the contemporary West. Two related terms that I use, \"industrial diet\" and \"industrial food supply,\" are also fraught with danger, since the word \"industry\" and its cognates have been used to encompass a wide range of meanings\u2014e.g., the use of steam and internal combustion engines and electricity, the concentration of workers into factories, the standardization of the production process and consumer goods, and as a descriptor for non-agricultural sectors of the economy\u2014that are only contingently related to each other. For my purposes here, the adjective \"industrial\" refers to the changes that took place in food and diet as a concomitant 16 to the transformation of societies around fossil-fuel-powered technologies in the nineteenth and early twentieth centuries.9 9 For discussions of the immense conceptual problems that have been created by the imprecise use of the word \"industry,\" see David Cannadine, \"The Past and Present in the Industrial Revolution 1880-1980,\" Past and Present 103 (1984): 131-72; Colin Duncan, The Centrality of Agriculture: Between Humankind and the Rest of Nature (Montreal, Kingston: McGill-Queen's University Press, 1996), 24-39. 17 CHAPTER ONE \"DEMINERALIZED, DEVITAMINIZED, DEVITALIZED\": NUTRITION SCIENCE AND THE ATTACK ON PROCESSED FOODS IN THE INTERWAR PERIOD1 \"The national dietetic sin of America and many parts of Europe has grown to be close adherence to a meat, bread, and potato diet, or other foods which have similar dietary properties,\" Elmer McCollum asserted in a 1921 paper in the Journal of Biological Chemistry.2 \"When it is remembered,\" he continued, \"that the cereal grains are now all but universally decorticated and degerminated for the purpose of producing products without commerci al hazard, and that these are decidedly poorer in their dietary properties than are the seeds from which they have been milled, the situation can easily be appreciated.\" Eight years before he raised these objections to the dietary norms of his day, McCollum had announced the discovery, along with his laboratory assistant Marguerite Davis, of one of the first vitamins. He initially designated the nutrient \"fat-soluble factor\" and then \"fat-soluble A,\" but several years later it was given the name still used for it today, \"vitamin A.\"3 Soon after, a deficiency of the vitamin was linked to xerophthalmia, a disease in humans affecting the eyes. His scientific reputation thus secured, McCollum embarked on what would become a long and illustrious career as a researcher, public lecturer, and author of popular books and magazine articles on nutrition. 1 The tile quotation comes from Victor E. Levine, \"Why We Should Be More Interested in Nutrition,\" Scientific Monthly 12, no. 1 (January 1926): 21. 2 E. V. McCollum, Nina Simmonds, and H. T. Parsons, \"Supplementary Protein Values in Foods: IV. The Supplementary Relations of Cereal Grain With Cereal Grain; Legume Seed With Legume Seed; and Cereal Grain With Legume Seed; With Respect to Improvement in the Quality of Their Proteins,\" Journal of Biological Chemistry 47, no. 1 (June 1921): 210. 3 Donald R. Davis, \"Some Legacies of Nutrition Pioneer E. V. McCollum,\" Transactions of the Kansas Academy of Science 82, no. 3 (1979): 134. 18 It might seem surprising to us in the present day that a widely respected scientist would have to point out the nutritional perils of a \"meat, bread, and potato diet,\" particularly one in which the cereal grains going into it have been refined in the milling process. But in the early 1920s, McCollum was still in a minority of scientists who disagreed with the assumption, as he put it, \"that if the diet affords sufficient calories and considerable variety, satisfactory nutrition will be assured.\"4 As a result of his own laboratory experiments and those of other contemporary researchers, McCollum became convinced that this complacent attitude toward eating had to be discarded. The rapidly growing knowledge of nutrition, he argued, was revealing that \"no matter how many cereals, tubers, and muscle meats such as steak, ham, chops, roasts, etc. may be taken, the diet will prove to be inadequate.\"5 To prevent the development of vitamin and mineral deficiency diseases, McCollum recommended including into the diet liberal quantities of what he called the \"protective foods\": milk, eggs, and leafy green vegetables. For McCollum, the health benefits of the \"protective foods\" went beyond just staving off deficiency diseases, which, in any case, were rare in the West. \"The results of our experimental studies on the rat,\" he reflected, \"have led us to an appreciation of the short-sightedness of the view that a diet is satisfactory if it is not sufficiently poor to cause the development of a 'deficiency' disease.\" McCollum and his research team conducted their feeding experiments with more demanding criteria in mind for what constituted illness and health. \"We have given a great deal of attention to the problem of demonstrating the effects of relatively slight defect s in the diet on the general health of the rat, and on its capacity to reproduce and rear young, and to remain vigorous to an advanced age,\" he noted. 4 E. V. McCollum, Nina Simmonds, and H. T. Parsons, \"Supplementary Protein Values in Foods: II. Supplementary Dietary Relations Between Animal Tissues and Cereal and Legume Seeds,\" Journal of Biological Chemistry 47, no. 1 (June 1921): 143. 5 McCollum et al., \"Supplementary Protein Values in Foods: II,\" 143. 19 After observing several generations of laboratory animals fed on a wide variety of diets, McCollum became convinced that poor nutrition was much more widespread in the human population than had typically been assumed. \"Low vitality, low resistance, and inefficiency, and a tendency to cumulative fatigue,\" he warned, \"are what we should expect in man to result from adherence to a faulty diet when the faults are of a lower order than would be necessary to bring about an attack of a 'deficiency' disease.\"6 For McCollum, moreover, proper eating did not just involve avoiding faulty diets that could place one in the \"twilight zone\" of malnutrition, where there were not yet any obvious symptoms of deficiency disease.7 His experimental work was making it abundantly clear to him that the standards then commo nly accepted by the medical commu nity for what constituted adequate health and development were too low. As McCo llum told an audience of social workers in 1921, \"We take as the 'normal' approximately the average physical development with which we are familiar, rather than the 'optimal' development as represented by the best developed members of the community.\" But he believed that it was the best-developed members who should be taken as the standard toward which to work in the development of all. For the population as a whole to attain this optimal level, McCollum asserted, physicians and public health workers had to give much more attention to assuring sound nutrition during the prenatal period and infancy. This was the period in a person's 6 E. V. McCollum, Nina Simmonds, and H. T. Parsons, \"Supplementary Protein Values in Foods: III. The Supplementary Dietary Relations Between the Proteins of the Cereal Grains and the Potato,\" Journal of Biological Chemistry 47, no. 1 (June 1921): 176-77. 7 McCollum borrowed the term \"twilight zone\" from Joseph Goldberger, a contemporary researcher who played a pivotal role in convincing the medical community that pellagra was a nutrient deficiency disease. See McCollum, The Newer Knowledge of Nutrition: The Use of Foods for the Preservation of Health and Vitality, 1st ed. (New York: Macmillan, 1918), 147; McCollum ea al., \"Supplementary Protein Values in Foods: III,\" 177. 20 physical development that was, in their words, \"fundamental in establishing a vigorous constitution.\"8 The arguments put forward by McCollum in 1921 did not remain unorthodox for very long. In the two decades that followed, a growing number of scientists and physicians began exploring the relationship between food and health, and their findings led them to the conclusion that diet played a much larger role in human wellbeing than had been previously thought. The scientific and medical commu nities became increasingly convinced that a large proportion of the population in the West was living in the so-called twilight zone of malnutrition, where vitamin and mineral deficiencies were not so extreme as to imperil life but nevertheless significant enough to lower health and vitality. Nutrition researchers also accru ed evidence that people who consumed generous quantities of the protective foods throughout their entire lifespan would enjoy improved physical development, lessened susceptibility to illness, prolonged youthfulness, and greater longevity relative to the norms of the day. In fact the twin notions of partial deficiency and optimal nutrition were even beginning to substantially influence the discourse on agricultural policy in the United States and Britain as the 1930s drew to a close. The expanding knowledge of vitamins and minerals also informed the first sustained scientific attack on \"processed foods\" and the \"modern\" or \"Western\" dietary habits based around them. As the understanding of nutrition improved in the 1920s and 1930s, this critique of the modern food supply gained wide acceptance in the scientific and medical communities. At the same time, a growing number of research ers in a broad range of disciplines became convinced of the superiority of unprocessed \"natural foods,\" and of the 8 E. V. McCollum and Nina Simmonds, \"The Place of Nutrition in Bringing the Undernourished Child Up to Normal,\" Nutrition Clinics for Delicate Children, Pamphlet no. 24 (1921): 6. 21 superior health and physical development of certain \"primitive\" societies that based their diets around these foods. The interwar period therefore represents an important phase in the historical development of the \"conservative nutrition\" perspective that I am following in this study. Advances and Anxieties: Nutrition Science from the 1910s to the 1930s When McCollum and Davis announced their discovery of the \"fat-soluble factor\" in 1913, the paradigm that had dominated the field of nutrition for the previous half-century was, in fact, already in a state of crisis. This paradigm was the chemical analysis of foods\u2014the identification and quantitative determination of the constituents of foods believed to be essential in the diet. Over the course of the nineteenth century, organic and physiological chemists separated from plant and animal materials many compounds whose existence was previously unsuspected and studied their properties. They determined that that the material basis of nutrition rested on four substances besides water: proteins, carbohydrates, fats, and minerals. Proteins were considered peculiar in that they contain about sixteen percent of the elemen t nitrogen, and are complex in structure. Carb ohydrates were found to include starch-like substances and simple sugars. Fats were grouped with a number of closely related oily organic compounds known collectively as \"lipoids.\" Scientists knew that minerals were needed in relatively small amounts for teeth and bones and to aid in digestion, but knowledge of the functions of particular minerals in the body, and quantitative requirements for them, remained rudimentary. Certain less ubiquitous organic compounds, such as 22 cellulose in plants and the waste products of the vital processes of animals, did not fall into these four main categories and received relatively less attention.9 The supposition that the substances identified by chemical analysis were the only ones worthy of consideration in formulating dietary advice accorded with the mech anistic view that guided thinking about physiology at the time. Nutrition investigators commonly refer red to the human body as a motor requiring food as fuel, both to generate power and energy and to build its productive capacity. C. F. Langworthy, a prominent nutrition investigator in the United States Department of Agriculture, summed up this narrow conception of the relationship between food and the body in 1898, writing, \"Foods have a dual purpose: Building and repair. Energy for heat and work.\"10 Scientists considered carbohydrates and fats the energy-producing fuel for the human mach ine, while they valued proteins as the substances needed to build and repair its working parts. Chemists had discovered that the skeleton and teeth consist of inorganic elements, primarily calcium and phosphorus, but they viewed these hard tissues as static components of the body, largely separate from the dynamic processes of anabolism and catabolism. Consequently, from the mid-nineteenth century into the 1910s, nutrition researchers concerned with formulating dietary recommendations focused almost exclusively on calorie and protein intake. Because the nutrients from all food sources were regarded as having equivalent value, it was considered economically wise to derive the diet from calorie-dense and cheap cereals, flour, peas, and beans, and to omit water-rich and more expensive milk, fruits, and garden vegetables. Even lean cuts of meat, eggs, and seafood were ranked rather 9 E. V. McCollum, A History of Nutrition: The Sequence of Ideas in Nutrition Investigations (1957), ch. 11 passim. 10 Quoted in McCollum, A History of Nutrition, 190. 23 low by the standards of the day. As sources of fat calories, marg arine and vegetable oils were held to be nutritionally equivalent to the more expensive butter.11 Had these dietary recommendations been followed seriously in an effort to economize on food, of course, many people probably would have developed deficiency diseases. But as it was, nutrition researchers up to the early 1910s were quite vague in their discussions of the relationship of diet to any particular disease. In keeping with the conventional metaphor of the human body as a motor requiring fuel, they saw malnourishment primarily in quantitative terms . Scientists assumed that as long as a person consumed an adequate number of calories and grams of protein, he or she was eating a healthful diet. They believed that a lack of calories and protein led to fatigue, weakness, and lethargy, whereas overeating taxed the body with excess deposits of fat and metabolic waste products. Moreover, even though a small number of physicians had become enthusiastic about therapeutic use of dietetics by the late 1800s, there was relatively little interact ion between nutrition researchers and the medical profession. Since scientists up to that time had offered no concrete proof that any actual disease was caused by improper nutrition, most doctors had little reason to think that any of the serious health problems patients presented to them could be dealt with through diet. And in any case, this was the time when the germ theory of disease dominated the thinking of the medical community.12 But a series of discoveries in the late nineteenth and early twentieth centuries began to challenge the prevailing wisdom. Physicians had long speculated that several diseases\u2014anemia, scurvy, beriberi, rickets, pellagra, and xerophthalmia\u2014were associated with diet. Some physicians beginning in the mid-nineteenth century had successfully treated a type of 11 Ibid., 191. 12 Ackerman, \"Interpreting the 'Newer Knowledge of Nutrition,\" 50-51. 24 anemia called \"chlorosis,\" a disease especially common in young women, with iron supplemen ts and iron-rich foods. It had been known for some time that scurvy, a condition characterized by lethargy, skin lesions, spongy gums and loss of teeth, bleeding from the mucous membranes, and eventually death, occurred in humans and some animals when the diet did not contain enough fresh vegetables, fruits, or animal foods. Soldiers on military camp aign and crews on sailing vessels, who were forced to live for extended periods of time on preserved meat and dried cereals and beans to the exclusion of fresh foods, were commonly afflicted by epidemics of scurvy. Diet was also suspected in the development of rickets, an illness affecting the bone development of infants and children, which had become increas ingly common in the cities of northern Europe and North America in the nineteenth century. Beriberi, an inflammation of the peripheral nerves that can cause fatigue, paralysis, and even death, had begun to appear with increasing frequency in East Asia in the late nineteenth century, where it was associated with the consumption of rice. Pellagra, on the other hand, was usually found in locations where maize had become the primary staple, including parts of Spain, Italy, the Balkans, Egypt, and the southern United States. Dietary deficiency was also suspected in the etiology of xerophthalmia, a disease affecting the eyes that could, if untreated, progress to total blindness. Investigators had found that animal liver and cod liver oil were effective in treating the condition. With each of these diseases, however, dietary hypotheses had to compete with alternative explanations, and none of them recei ved universal acceptance. And so, with the exception of some scattered dissenting voices, the field of nutrition up to the early 1900s was not influenced in any substantial way by the notion that specific diseases could be caused by deficiencies of substances in foods.13 13 Aaron J. Ihde and Stanley L. Becker, \"Conflict of Concepts in Early Vitamin Studies,\" Journal of the History of Biology 4 (Spring 1971): 3-6; Karl Y. Guggenheim, Nutrition and Nutritional Diseases: 25 Research into the cause of beriberi provided the breakthrough that ultimately led to the correct understanding of these diseases. Beriberi had been known to the medical world for centuries, but it was a relatively unimportant disease. By the last quarter of the nineteenth century, however, beriberi was widespread and growing at an extraordinary rate in East Asia, especially in prisons, hospitals, and military institutions. In 1883, the Dutch government sent a team of researchers to its colony in the East Indies to study the disease. A Dutch physician, Christiaan Eijkman, was tasked with finding the organism responsible for beriberi. It was while looking for an infectious organism for the disease that he serendipitously recognized its similarity with peripheral polyneuritis in his colony of experimental chickens. He found that polyneuritis developed in the birds when they were fed a diet of polished \"white\" rice, but that by adding the pericarp (or inner husk) of the rice grain, which had been removed during polishing, the disease could be prevented or cured. From the results of parallel feeding experiments using foods other than rice, Eijkman concluded that polyneuritis was due to a nerve toxin produced during the fermentation of starch in the chicken crop, and that the pericarp of rice contained an antidote to the toxin. But many studies, some of them with species other than the chicken, yielded inconclusive results. And Eijkman knew that further proof was required because others in the medical community doubted that peripheral polyneuritis in chickens had any real relation to beriberi in humans. His work, however, was cut short by repeated bouts of malaria, and he returned to the Netherlands in 1896 to recover his health. But the medical surgeon who succeeded Eijkman in Batavia, Gerrit Grijns, continued the research into polyneuritis. In one experiment, he successfully induced The Evolution of Concepts (Lexington, MA: Collamore Press), 173-271; Kenneth J. Carpenter, \"A Short History of Nutritional Science: Part 2 (1885-1912),\" Journal of Nutrition 133 (2003): 977-83. 26 polyneuritis in chickens by feeding them autoclaved meat\u2014that is, meat cooked under high temperature and pressure\u2014as the sole food. Since meat is free of starch, Grijns was able to discount Eijkman's theory that the toxic fermentation of starch was responsible for the disease. He determined instead that polyneuritis was caused by a deficiency of some unknown substance present in the pericarp of rice, several varieties of beans, and fresh meat , and that it appeared particularly susceptible to destruction by heat. \"There occur in natural foods, substances,\" Grijns wrote in the conclusion to a 1901 paper, \"which cannot be absent without serious injury to the peripheral nervous system.\"14 Mean while, Adolphe Vorderman, the medical inspector of prisons in the Dutch East Indies, had corroborated Eijkman's chicken-feeding experiments with an observational study of humans. Vorderman made a thorough investigation of prisons in which different kinds of rice were used, and found that the incidence of beriberi was vastly greater in the \"white rice\" prisons compared to the \"brown rice\" prisons. He found, moreover, that other factors that had been suggested as being conducive to the disease, such as overcrowding and poor ventilation, showed no evidence of having any adverse effect. Soon after, Vorderman's study was further supported by controlled feeding experiments with human subjects in a mental institution. Other researchers outside the Dutch East Indies working in areas where the diet consisted mainly of polished white rice or highly milled \"white\" wheat flour gathered additional compelling evidence that beriberi was caused by a lack of some unknown component in food, rather than by an infectious agent.15 By the early 1910s, it 14 Grijns quoted in Barbara Sutherland, \"A Micronutrient Deficiency in Chickens (Grijns, 1896-1901),\" Journal of Nutrition 127 (1997): 1025S. 15 Henry Fraser and A. T. Stanton, \"The Etiology of Beri-Beri,\" Lancet 176, no. 4555 (17 December 1910): 1755. 1757; John M. Little, \"Beriberi Caused by Fine White Flour,\" Journal of the American Medical Association 58, no. 26 (29 June 1912): 2029-30. 27 was widely recognized in the medical community that beriberi was a dietary deficiency disease.16 The success achieved by Eijkman and Grijns in uncovering the cause of beriberi also influenced the course of research on scurvy. Beginning in the mid-1890s, crews on Norwegian ships began to suffer from a disease whose symptoms were similar to those of beriberi, for which reason it was called \"ship-beriberi.\" In 1902, Axel Holst, a Norwegian professor of bacteriology and hygiene who had become concerned by the appearance of ship-beriberi, took an opportunity to visit Grijns in Batavia to observe his work on chicken polyneuritis. He was impressed with the work that Eijkman and Grijns had done, so he decided to adopt similar techniques in his own study of ship-beriberi. He attempted to induce the disease in chickens and pigeons, and later, after having been joined by pediatrician Theodor Fr\u00f6lich, in guinea pigs. Curiously, while Holst and Fr\u00f6lich were trying to study ship-beriberi, the guinea pigs regularly contracted a disease closely resembling scurvy. This unexpected discovery encouraged the two men to conduct further research, which eventually led them to conclude that the illness suffered by the guinea pigs was the same as human scurvy but distinct from beriberi. They found, moreover, that it was caused by a deficiency of a substance found in fresh fruits and vegetables, and that this substance was susceptible to destruction when foods containing it were stored or heated.17 All of this research was finally assembled in a landmark article published in 1912, \"The Etiology of the Deficiency Diseases,\" by Polish chemist Casimir Funk. Funk had been hired by the Lister Institute in London two years earlier to isolate the anti-beriberi substance found in the pericarp of rice. In the article, Funk announced\u2014somewhat prematurely\u2014that 16 K. Codell Carter, \"The Germ Theory, Beriberi, and the Deficiency Theory of Disease,\" Medical History 21 (1977): 119-36; Carpenter, \"A Short History of Nutritional 17 Scurvy: II. On the Etiology of Hygiene 7 (1907): 634-71; Carter, \"Germ Theory,\" 133-34. 28 he had successfully done so. But more importantly, he advanced the idea that beriberi, scurvy, pellagra, and possibly rickets were \"caused by a deficiency of some special substances in the food.\" To describe these substances, Funk coined the term \"vitamine,\" which suggested that they were all \"vital,\" and chemically nitrogenous \"ami nes.\" He proposed that the \"vitamines\" were required to prevent specific \"vitamine deficiency diseases.\"18 Funk's argument did not receive universal acceptance immediately, but his paper was widely read, and many research ers accepted the novel theory of deficiency diseases.19 Support for the vitamin hypothesis did not just come from investigations in prisons, hospitals, and military institutions far from the industrial West. At roughly the same time that researchers were coming to a correct understanding of beriberi and scurvy, agricultural scientists were also beginning to assail the chemical analysis paradigm. Toward the end of nineteenth century, some scientists became troubled by the repeated failure of chemical analysis to provide information capable of predicting the ability of foodstuffs to nourish livestock. There was increasing awareness by agricultural scientists of the unreliability of the advice given to farmers in bulletins and textbooks as a guide to the feeding of animals. Stephen Moulton Babcock, a highly respected professor of agricultural chemistry at the University of Wisconsin, reportedly irritated Wilbur Atwater, the \"father of nutrition science\" in the United States, by suggesting that it would be cheaper to feed pigs soft coal instead of farm crops. When such coal was analyzed by chemical analysis, Babcock jibed, 18 Casimir Funk, \"The Etiology of the Deficiency Diseases\" Journal of State Medicine 20 (1912): 341-68. 19 McCollum, History of Nutrition, 217. 29 the results indicated that it was a well-balanced food for the pig.20 Henry Prentiss Armsby, a leading American authority on animal nutrition, likewise became frustrated with the lack of progress being made in the field of nutrition. As he commented in a 1906 publication, \"We have come to question some of our standards. Some we have modified, and we hold them more flexibly than we once did, but protein, carbohydrates, and fats are still the nutritional trinity. Our theory of nutrition has become traditional, and has little pedagogic value and inspiration for the investigator.\" Furthermo re, between 1873 and 1906, more than a dozen researchers tried to feed small experimental animals diets of purified proteins, carbohydrates, fats, and minerals. Even though these diets contained all the nutrients that were then considered essential, in every case the animals weakened and died after a few weeks. Some of these investigators began to speculate that there were other required, yet still unknown, nutrients.21 The impasse, however, was soon overcome, due in part to new investigations in protein nutrition. Toward the close of the nineteenth century, chemists discovered that most proteins could be separated into simpler organic compounds called amino acids. By 1900, they had identified twelve of the twenty-two amino acids that are now recognized as the building blocks of protein. Soon after, improved analytical techniques disclosed the existence of other amino acids, and revealed, in many instances, great differences in the amino acid composition of proteins from different foods. These findings led researchers to wonder if proteins varied in their nutritive quality, and if animals had separate requirements 20 This story is recounted in Elmer Verner McCollum, From Kansas Farm Boy to Scientist: The Autobiography of Elmer Verner McCollum (Lawrence, KS: University of Kansas Press, 1964), 113-17. 21 McCollum, History of Nutrition, 150-54. 30 for individual amino acids. Chemical analysis could not readily provide the answer to these questions, so they turned to animal-feeding experiments.22 One of the pioneers in the field of amino acid research was Frederick Gowland Hopkins, the first professor of biochemistry at Cambridge University. In 1901, Hopkins and his colleague Sydney Cole showed that \"tryptophane,\" until then a substance of obscure chemical identity and physiological function, is an amino acid. (It was later renamed tryptophan.) The physiological importance of tryptophan was soon after demonstrated in the experiments of Edith Willock, published jointly with Hopkins, in which mice were fed a diet of purified food substances, using zein, a protein from corn that contains no tryptophan, as the sole source of protein. On this diet, the mice survived an average of only two weeks. When a supplemen t of tryptophan was given, the average survival period doubled. This outcome motivated Hopkins to begin another series of animal feeding experiments using diets composed of pure ingredients of known chemical composition, and systematically varying the amino acid content of these diets. In the course of his investigations, Hopkins discovered that diets of pure protein (even with an adequate amino acid composition), carbohydrate, fat, minerals, and water failed to support animal growth. (He had apparently been unaware of the earlier recorded feeding experiments demonstrating the failure of purified diets when he began his own.) When he added a very small quantity of milk to these artificial diets, the growth of the mice was excellent. Hopkins waited until 1912 to publish the results of his research, but in a lecture six years earlier he had already suggested that there were as-yet-unidentified substances in normal foods that animals needed in minute quantities for growth and survival. He called these hypothetical substances \"accessory food 22 Ibid., 57-60. 31 factors,\" and hinted at a possible link between a lack of such substances and diseases such as scurvy and rickets.23 Mean while, agricultural scientists were carrying out experiments to evaluate the practical value of chemical analysis in the feeding of farm animals. One of the most extensive of these studies led, indirectly, to Elmer McCollum's rise to public prominence as a pioneering vitamin researcher. The idea for the feeding experiment originated from Stephen Babcock, a longtime skeptic of the idea that the physiological value of a ration could be predicted from knowledge of its chemical composition. Upon his official retirement, Stephen Babcock challenged his successor as professor of agricultural chemistry at the University of Wisconsin, Edwin Hart, to feed breeding heifers ingredients all from a single cereal, and to compare the results with a diet made up from mixed cereals. Hart agreed, and in collaboration with G. C. Humphrey, began the heifer experiment in 1907. Hart hired McCollum, who previously had been doing postdoctoral work in physiological chemistry at Yale, to conduct the laborious chemical analyses of the foods, blood, and excrement of the animals. The objective of the experiment was to compare the performan ce of four groups of heifers fed rations composed entirely of the corn, wheat, or oat plant, or a mixture of the three. All the rations were carefully formulated using the grain, gluten, hay, and straw of the plants in proportions such that each mix rendered the same analysis for nitrogen, fiber, minerals, and moisture. The heifers receiving the all-wheat ration quickly lost condition and performed poorly, with none of their calves surviving and half of the cows also dying before the end of the trial. In contrast, the corn-fed heifers maintained their condition and birthed 23 Harmke Kamminga Mark W. I: Frederick Gowland Hopkins' Construction of Dynamic Biochemistry,\" Medical History 40 (1996): 274-75. 32 healthy, strong calves. The other two groups fared somewhere between the corn-fed and wheat-fed groups. The result of this experiment confirmed Babcock's suspicions, and was a major blow to the idea that identical chemical analyses, as it was then understood, could predict identical nutritional results.24 Although the experiment received a great deal of notoriety, McCollum became discouraged because he could not determine why corn was nutritionally superior to wheat. \"I became convinced,\" he later reminisced, \"that the project we were engaged in with cows fed highly complex chemical rations could not lead to the discovery of anything of importance and that I was wasting my time.\"25 So McCollum reviewed the available scientific literature to find a better way to evaluate the nutritional quality of foods. From this research\u2014particularly his reading of the abstracts in Richard Maly's Jahresbericht \u00fcber die Fortschritte der Tier-Chemie\u2014McCollum learned about the various failed attempts to nourish small animals on diets of purified foods. Since smaller species matured and reproduced much faster than cattle with much less feed, he decided that he would like to try this approach. McCollum proposed using rats as experimental animals because of their short life cycle and omnivorous habits, but his superiors were initially opposed to the idea of working with an economically unimportant species. But Babcock encouraged the young McCollum to persevere. So, in addition to his other pressing duties, McCollum set up a colony of white rats at his own expense in 1907.26 Owing to his meager resources and limited knowledge in dealing with small experimental animals, McCollum began to learn through trail and error. After several false 24 Howard A. Schneider, \"Rats, Fats, and History,\" Perspectives in Biology and Medicine 29, no. 3, Part 1 (Spring 1986): 395-98. 25 Elmer Verner McCollum, \"The Paths to the Discovery of Vitamins A and D,\" Journal of Nutrition 91, no. 2, Supplement 1, Part II (February 1967): 12. 26 McCollum, Autobiography, 114-33; Harry G. Day, \"E. V. McCollum: Doyen of Nutrition Science,\" Nutrition Reviews 37, no. 3 (March 1979): 66-67. 33 starts, he developed the approach that scientists would use for the next forty years to determine animal and human nutritional requirements. McCollum termed this method \"biological analysis,\" in order to contrast it with the chemical analysis of foods. Biological analysis worked by feeding experimental animals, usually white rats, a nutritionally inadequate diet of purified food materials, to which various natural foods were added. If the health of the animals improved, as measured by increased growth and greater fertility, the researcher chemically fractionated the natural food supplemen t in order to locate and identify the nutrient responsible for the enhanced performan ce. Biological analysis, in other words, placed chemical determination in a secondary role in experimental procedure. Using this new method, scientists eventually discovered all the vitamins, and provided a much more accurate understanding of dietary needs than the one that had been developed through chemical analysis.27 An unexpected outcome in the early phases of McCollum's work on the biological analysis of foods was his discovery of vitamin A. It had been speculated around the turn of the nineteenth century that animals needed to be supplied with phosphorus in the form of nucleic acids, but McCollum found in his first feeding trials that rats could manufacture their own nucleic acid without the need for either organic phosphorus or purines (a nitrogenous base compound). Having shown that casein could be used as a purified protein source in experimental diets without worrying about its phosphorus content, he then sought to improve his mineral mix. McCollum and Marguerite Davis, his laboratory assistant at the time, found that rats eating a diet of purified foodstuffs\u2014casein, lard, lactose, starch, and a mineral mixture\u2014would cease growing and in some cases lose weight unless fed some butterfat or 27 E. V. McCollum, The Newer Knowledge of Nutrition: The Use of Food for the Preservation of Vitality and Health (New York: Macmillan, 1918), ch. 1 passim; Schneider, \"Rats, Fats, 398-402. 34 the fatty fraction of egg yolk. They learned, moreover, that giving rats olive oil or cottonseed oil as a substitute for these substances did not have the same beneficial effect. So they extracted the small non-saponifiable fraction from butterfat, transferred it to olive oil, and fed the product to the rats. The animals survived and flourished, leading McCollum and Davis to conclude in 1913 that they had discovered a new \"fat-soluble factor\" essential for growth.28 Soon after, Lafayette Mendel and Thomas Osborne at Yale confirmed these results with their own rat-feeding experiments.29 Hopkins, in reply, argued that the two American groups had been able to obtain growth with their diets only because the casein and lactose were insufficiently purified from a water-soluble \"growth factor.\" Indeed, prior to his discovery of the fat-soluble micronutrient that became known as vitamin A, McCollum had failed to find any reference to the research and controversy surrounding the etiology of beriberi. When he finally learned of this work in 1913 or 1914, he and his colleagues began a feeding experiment to test the theory that beriberi was caused by a deficiency of a water-soluble growth factor. McCollum fed his rats a diet of purified foodstuffs, polished rice, and the recently discovered growth-promoting fat-soluble factor, but they ceased growing and became paralyzed. When he supplied the rats with a water-soluble extract of wheat germ or milk, their health revived, confirming the earlier findings of Grijns and Funk on fowls fed polished rice. McCollum and his colleagues therefore concluded in a 1916 paper that rats needed 28 E. V. McCollum and M. Davis, \"The Necessity of Certain Lipins During Growth,\" Journal of Biological Chemistry 15 (1913): 167-75. 29 E. V. Osborne and L. B. Mendel, \"The Relation of Growth to the Chemical Constituents of the Diet,\" Journal of Biological Chemistry 15 (1913): 311-26. 35 fat-soluble \"Factor A\" and \"Factor B\" for growth and maintenance of health.30 Here we see the beginning of the scheme for identifying vitamins by letters. The nomenclature of the newly discovered organic compounds, however, remained unsettled during the 1910s. Many research ers at the time were opposed to the use of Funk's term \"vitamine\" to describe these compounds, since it had become clear that not all the substances they were investigating were nitrogenous amines. But it was also recognized that \"vitamine\" was vastly preferable to the unwieldy \"accessory food factors,\" hormones,\" \"food hormones,\" or similarly awkward alternatives.31 Moreover, \"vitamine\" was entering into widespread use in less specialized literature, including medical journals.32 Disagreement over terminology subsided after British biochemist J. C. Drummo nd proposed in 1920 dropping the \"e\" on \"vitamine,\" to loosen the connection with the chemical category of amines, and adopting the letter system suggested by McCollum to denote order of discovery.33 Under the rubric of vitamins, previously disparate lines of investigation began cohering into a unified field. As the 1920s and 1930s progressed, a growing number of researchers entered into the nutrition field, resulting in an immense increase in the number of scientific articles on the subject. In 1933, for instance, an estimated 5,000 papers describing original nutrition work 30 E. V. McCollum and Cornelia Kennedy, \"The Dietary Factors Operating in the Production of Polyneuritis,\" Journal of Biological Chemistry 24 (1916): 491-502; McCollum, Autobiography, 131-36. 31 Edward B. Vedder, \"The Relation of Diet to Beriberi and the Present Status of Our Knowledge of Vitamins,\" Journal of the American Medical Association 67, no. 21 (1916): 1494-97. 32 Harmke Kamminga, \"'Axes to Grind': Popularising the Science of Vitamins, 1920s and 1930s,\" in David F. Smith and Jim Phillips (Eds.), Food, Science, Policy and Regulation in the Twentieth Century: International and Comparative Perspectives (London and New York: Routledge, 2000), 88-89. 33 J. C. Drummond, \"The Nomenclature of the So-Called Accessory Food Factors (Vitamins),\" Biochemical Journal 14 (1920): 660. 36 appeared in the world's literature.34 The first professional organizations and journals devoted specifically to nutrition science were founded in this period. In the United States, the Journal of Nutrition began publication in 1928, and the American Institute of Nutrition was founded in 1933. In Britain, Nutrition Abstracts and Reviews was begun in 1931 in order to provide nutritionists and research ers in related disciplines with summaries of the growing mass of published work scattered in more than 450 journals and bulletins around the world.35 While nutrition science was emerg ing as a distinct field in the interwar period, knowledge about vitamins and minerals was expanding rapidly. One of the most widely publicized advances was the discovery that rickets was caused by a deficiency of vitamin D. As previously mentioned, rickets, a defect of bone and joint formation, had become endemic in the cities of Europe and North America by the early twentieth century. Medical experts produced a mass of conflicting explanations for the cause of the condition, faulting lack of exercise or sunshine, infectious agents, changes in diet that accompanied industrialization, and the increase in the use of proprietary infant formulas in place of breast milk. Rickets was particularly serious in the industrial districts of Britain, where it was associated with slum dwelling. So, in 1914, the Medical Research Committee (the predecessor of the Medical Research Council) selected Edward Mellanby, a physiologist at King's College for Women, London, to sort out the competing claims for the cause of rickets. After a long series of experiments using dogs, Mellanby published his results in 1921, which clearly showed that the disorder was caused by a deficiency of a trace component in the diet. He was able to produce rickets in puppies by keeping them indoors and feeding them a diet 34 Figure cited in Madeline Mayhew, \"The 1930s Nutrition Controversy,\" Journal of Contemporary History 23, no. 3 (July 1988): 446. 35 H. H. Williams, \"History of the American Institute of Nutrition: The First Fifty Years,\" Journal of Nutrition 108 (1979): 125-97. 37 consisting of fat-free milk, lean meat , bread or other cereal, yeast, orange juice, and some vegetable fat such as olive oil\u2014that is, a diet that was considered completely adequate at the time except for a deficiency of fat-soluble vitamin A. However, Mellanby could prevent the occurrence of rickets using supplemen ts of cod liver oil, egg yolk, whole milk, butter, or suet, without allowing the dogs access to the outdoors or giving them exercise. He concluded in his 1921 report that rickets was a disease caused by a deficiency of vitamin A or a fat-soluble substance with similar properties.36 On learning of Mellanby's experiments, McCo llum, who had left the University of Wisconsin in 1917 to become head of the chemistry department at the new School of Hygiene and Public Health at Johns Hopkins University, decided to pursue them further. McCollum found that the rat could be used to provide a model for rickets by severely unbalancing the calcium-phosphorus ratio of its ration. He and his colleagues then discovered that cod liver oil could prevent rickets in the rat even after the oil had been heated and aerated to destroy its vitamin A content. McCo llum therefore concluded that an antirachitic vitamin existed that was fat-soluble but distinct from vitamin A. The newly discovered substance was dubbed vitamin D in 1922.37 Around the same time, medical researchers confirmed experimentally that irradiation of infants with ultraviolet light could also cure rickets. Lack of exposure to sunlight had long been suspected as a cause of rickets, and in 1919 a German researcher, Kurt Huldschinsky, carried out an experiment showing that artificially produced ultraviolet light from a lamp cured children of the condition. Then, in 1922, a team of scientists led by 36 Edward Mellanby, Experimental Rickets, MRC Special Report Series No. 61 (London: HMSO, 1921). 37 E. V. McCollum, N. Simmonds, J. E. Becker, and P. G. Shipley, \"Studies on Experimental Rickets. XXI: An Experimental Demonstration of the Existence of a Vitamin which Promotes Calcium Deposition,\" Journal of Biological Chemistry 53 (1922): 293-312. 38 Harriette Chick of London's Lister Institute published a landmark study confirming that rickets could be treated by both food sources of vitamin D and ultraviolet light. The idea for the experiment originated in early 1919, when reports had begun reaching Britain of severe food shortages in Central Europe during the later stages of World War I and after its end. The Lister Institute and the Medical Research Committee decided to send a group of investigators led by Chick to Vienna, where conditions were particularly grave, to ascertain whether the research in the previous decade with experimental animals on vitamin deficiency diseases might be applicable to human beings. While on their mission in Vienna, Chick and her team observed many cases of rickets among infants at a large foundling hospital. In a carefu lly controlled experiment, they were able to demonstrate, with the aid of X-ray photographs, that either the use of cod liver oil or irradiation of the children with ultraviolet light would cure rickets, which otherwise developed under very hygienic clinical conditions.38 In short order, a series of investigations tied together the nutritional research on rickets and the findings regarding ultraviolet radiation. In 1923, British researchers Harry Goldblatt and Katherine Soames discovered that the livers from irradiated rats, when fed to other rats, were growth promoting, whereas the livers from unirradiated rats were not. Soon after, two teams of American investigators\u2014Harry Steenbock and Archie Black at the University of Wisconsin, and Alfred Hess and Mildred Weinstock at Columbia University\u2014found almost simultaneously that irradiation of excised rat skin as well as foods such as vegetable oils, egg yolk, milk, lettuce, and rat chow gave them antirachitic power. Researchers then sought to determine the exact substance in food and skin that was activated 38 Harriette Chick and Margaret Hume, \"The Work of the Accessory Food Factors Committee,\" British Medical Bulletin 12, no. 1 (1956): 5-8; Harriette Chick, \"Study of Rickets in Vienna, 1919-1922,\" Medical History 20, no. 1 (January 1976): 41-51. 39 by ultraviolet radiation. In 1928, Adolf Windaus, a German chemist at the University of Gottingen, isolated three forms of the vitamin. He found that two forms were derived from irradiated plant sterols, which he called D1 and D2, and one derived from irradiated skin, which he called D3. In 1931, a team of British scientists successfully defined the chemical makeup of vitamin D2, which was derived from the precursor molecule ergosterol. Five years later, in 1936, Windaus synthesized the molecule 7-dehydrocholesterol and converted it by irradiation to vitamin D3, which, as other investigators found almost simultaneously, was identical to the antirachitic component in cod liver oil. During this period, other research groups found that vitamin D prevented and cured rickets by ensuring the proper deposition of calcium and phosphorus in developing bones. They also determined that people obtained most of their vitamin D from exposure to the sun, since only a few natural foods, namely fish liver oils, egg yolk, and butterfat, contained it in significant quantities. It was also discovered that women who were exposed to ultraviolet light or ate diets rich in vitamin D produced milk with a higher content of the vitamin; studies on experimental animals generated similar conclusions. These findings strengthened the growing emphasis that nutritionists and pediatricians at the time were placing on the importance of sound nutrition in the antenatal period and early infancy.39 During the 1920s and 1930s, animal-feeding studies revealed the existence of other fat-soluble vitamins besides vitamins A and D. In 1922, Herbert Evans and Katharine Bishop at the University of California, Berkeley developed a purified diet that supported good growth in female rats but nevertheless failed to support normal reproduction. Male rats 39 E. V. McCollum, Elsa Orent-Keiles, and Harry G. Day, The Newer Knowledge of Nutrition, 5th ed. (New York: Macmillan, 1939), 336-41; Carpenter, \"A Short History of Nutritional Science: Part 3,\" 3025-26; Kumaravel Rajakumar, Susan L. Greenspan, Stephen B. Thomas, and Michael F. Holick, \"Solar Ultraviolet Radiation and Vitamin D: A Historical Perspective,\" American Journal of Public Health 97, no. 10 (October 2007): 1746-54. 40 on the diet, moreover, showed impaired sperm formation and testicular degeneration. Evans and Bishop determined that these reproductive problems were caused by a deficiency of a third fat-soluble vitamin, which was designated vitamin E. Further studies on animals showed that a deficiency of this vitamin also led to a variety of other disturbances, including paralysis, defective growth in the later stages of life, and abnormal hormonal secretion. Researchers found that the oils from seeds, such as wheat, rice, cotton, and corn, were especially rich sources of vitamin E, although leafy vegetables also appeared to contain considerable amounts as well.40 Hemorrhaging in chicks, which responded to dosing with leafy vegetables, was another disorder that investigators thought was possibly caused by a deficiency of either vitamin C or vitamin E. But in a series of experiments, Danish biochemist Henrik Dam found that chicks on a purified diet suffering from vascular hemorrhaging failed to respond to additions of pure vitamin C, which had first been synthesized in 1933, or vitamin E-rich wheat germ oil. In 1935, Dam announced the discovery of yet another fat-soluble substance, which he named \"vitamin K,\" in recognition of its essential role blood coagulation (or \"koagulation\" according to the German and Scandinavian spelling). Research ers soon discovered that leafy green vegetables, certain fermentative bacteria, and animal liver were particularly rich in vitamin K, and that other foods such as tomatoes, eggs, cereals, and dairy products contained lesser amounts as well.41 Knowledge of the water-soluble vitamins also expanded rapidly at this time. By the late 1920s, researchers had demonstrated that \"vitamin B\" was in fact composed of at least 40 McCollum, History of Nutrition, 359-68; Carpenter, \"A Short History of Nutritional Science: Part 3,\" 3028-29. 41 Carpenter, \"A Short History Science: Part 3,\" 3028; Henrik Dam, \"Vitamin K: Its Chemistry and Physiology,\" in Advances in Enzymology and Related Areas of Molecular Biology, Volume 2, eds. F. F Nord and C. H. Werkman (New York: Wiley, 1942). 41 two vitamins: the original antineuritic or antiberiberi substance, which was designated vitamin B1, and another component responsible for promoting growth in experimental animals, which was designated vitamin B2 or G. For a time, most nutrition workers believed that vitamin B2 prevented pellagra, but in the late 1930s it was finally discovered that this disease was caused by a deficiency of yet another B vitamin, called nicotinic acid or niacin. During that decade, pantothenic acid, pyridoxine, and biotin were also added to the list of water-soluble vitamins, and researchers proposed the existence of even more.42 Knowledge about micronutrients other than vitamins also grew during the interwar period. Scientists not only learned more about the well-known inorganic elements calcium, phosphorus, and iron, but they found that the so-called trace elements, such as iodine, copper, manganese, zinc, and cobalt, played a role in animal and human nutrition as well. Experiments conducted during the 1920s showed that many animals and humans converted carotene, a pigment found in carrots and many yellow and green vegetables, to vitamin A, an almost colorless substance. The scientific understanding of different types of fats, which had long been viewed as more or less equivalent from a nutritional standpoint, underwent drastic modification as well. In the late 1920s, George O. Burr and Mildred M. Burr of the University of Minnesota produced a hitherto unidentified syndrome with rats on a fat-free diet. Soon after, they and their colleagues determined that both linoleic and linolenic acids, both unsaturated fatty acids, were effective in preventing or curing this fat-deficiency disease. Following upon the work of the Burrs, other research ers began investigating the so-called indispensable or essential fatty acids in the 1930s. By the end of that decade, they had discovered that mammals could synthesize all the saturated fatty acids as well as the 42 McCollum, History of Nutrition, Chaps. 25 and 27 passim; Carpenter, \"A Short History of Nutritional Science: Part 3,\" 3026-28. 42 monounsaturated oleic acid from carbohydrate, but not the more highly unsaturated fatty acids, which had to come from the diet.43 Although the list of known essential nutrients grew rapidly between the 1910s and the 1930s, the dietary advice that nutritionists offered to the public changed relatively little during this period. McCollum's early recommendations had an especially enduring influence in this regard. In his first decade of research using the biological method of analysis, he conducted about three thousand animal-feeding experiments that led him to reject existing dietary standards based on chemical analysis. McCollum fed rats a variety of diets composed of grains or potatoes plus supplemental foods such as alfalfa leaf meal, butterfat, purified protein, and mineral salts. As a result of his experiments, McCollum discarded the older view that an adequate diet consisted of an appropriate number of calories and grams of protein, and determined that proper nutrition also required the right amount of differen t kinds of amino acids, various minerals, and \"fat-soluble A\" and \"water-soluble B.\" The staple foods commonly consumed in the West, McCollum maintained, were especially deficient in \"fat-soluble A\" and calcium. After he discovered that milk and leafy greens were rich in these two nutrients, he began advising the public to eat generous quantities of these foods. McCo llum designated these items the \"protective foods\" in 1918, in order to emphasize their value in correcting the nutritional deficiencies of seeds, tubers, fleshy roots, fruits, and muscle meats. He also considered eggs and the glandular organs of animals to be protective foods, though to a lesser extent on account of their low calcium content.44 By the early 1920s, McCollum also accepted the existence of the water-soluble antiscorbutic substance that was eventually called vitamin C. He urged nursing mothers in particular to 43 E. V. McCollum et al., The Newer Knowledge of Nutrition, 5th ed., 55-59. 44 E. V. McCollum, The Newer Knowledge of Nutrition, 1st ed., 149-50. 43 include an abundance of fresh vegetables and citrus fruits in their diet to assure that their children would not develop scurvy.45 Other nutritionists around that time also advocated the liberal use of relatively expensive dairy products, eggs, green and yellow vegetables, tomatoes, and citrus fruits and berries on account of the vitamins and minerals that they contained.46 The list of protective foods underwent little alteration for the remainder of the interwar period, although, for reasons that will be explained in chapter four, nutritionists became more vocal in advocating the replacement of refined flour, bread, and cereals with their wholegrain counterparts. In addition, some nutritionists began including meat and seafood in the protective food category in the 1930s, after researchers revealed that these foods contain appreciable quantities of certain vitamins of the B complex. Some evidence also emerged at this time that the protein requirement of children for \"optimal\" physical development was higher than had previously been believed.47 Nutrition scientists did not just argue that consuming sufficient quantities of the protective foods would stave off vitamin and mineral deficiencies. They also maintained that these foods had a beneficial effect on health and physical development in general. Some leading nutritionists even went so far as to claim that the discovery of specific vitamin and mineral deficiency diseases was, in practical terms, of secondary importance for improving public health. McCollum, for instance, declared that \"the researches in the field of nutrition have a greater value in preventive medicine in relation to raising the vitality of mankind, with all that this implies, than they have in the prevention of the occurrence of the deficiency 45 E. V. McCollum, The Newer Knowledge of Nutrition: The Use of Food for the Preservation of Vitality and Health, 2nd ed. (New York: Macmillan, 1922), ch. 8 passim. 46 Henry Sherman and Sybil Smith, The Vitamins (New York: Chemical Catalog Company, 1922), 205-34. 47 See, e.g, Julian Boyd, \"The Nature of the American Diet,\" Journal of Pediatrics 12, no 2 (February 1938): 243-54. 44 diseases.\"48 McCollum's extensive experimentation with laboratory rats fed different diets convinced him early on that there was a significant difference between the minimally adequate and the optimal in nutrition. As he remarked in the first edition of The Newer Knowledge of Nutrition (1918): In the study of diets the author and his colleagues have kept constantly in mind the best results we have ever seen in the nutrition of animals, as exemplified in rapidity of growth, ultimate size attained, number of young produced, and the success with which these were reared, and have attempted to assign to every experimental group its legitimate place on a scale of performance, which has complete failure to either grow or remain long alive as the one extreme, and the optimum of which the animal is capable at the other.49 McCollum frequently emphasized that animals consuming an \"adequate\" diet would grow normally and appear to the casual observer to be normal, yet exhibit susceptibility to respiratory infections, low fertility, high infant mortality, irritability, fatigue, early onset of the signs of old age, and short lifespan. McCollum also observed that subsequent generations of these animals became smaller and less well developed, until the strain finally died out after several generations. He was able, on the other hand, to devise experimental diets upon which the rats grew rapidly, appeared vigorous and healthy, produced large litters that survived infancy, avoided infections, maintained the \"characteristics of youth\" when quite old, and had unusually long lives. Other nutrition researchers made similar findings. Edward Mellanby, for example, discovered that the addition of vitamin D to the diet of his experimental animals beyond the point where it produced any further increase in the absorption and retention of calcium nevertheless resulted in an improvement of bone density.50 But it was Henry Clapp 48 McCollum, The Newer Knowledge of Nutrition, 2nd ed., 369. 49 McCollum, The Newer Knowledge of Nutrition, 1st ed., 51. 50 Sir Edward Mellanby, A Story of Nutritional Research: The Effect of Some Dietary Factors on Bones and the Nervous System, Abraham Flexner Lectures, Series No. 9 (Baltimore, MD: Williams & Watkins Co., 1950), 345-46. 45 Sherman, a professor of chemistry at Columbia University, who carried out the most extensive series of studies in the 1920s and early 1930s to test \"the principle of the nutritional improvability of the norm,\" as he called it. He divided rats into four groups, and fed each group a different proportion of ground whole wheat and dried whole milk, ranging from five parts wheat to one part milk up to one part wheat to five parts milk, in addition to table salt and distilled water. Sherman then observed the effects of these simplified diets on several generations of animals. He found a positive correlation between the proportion of milk in the diet and the overall health and vitality of his subjects. The rats eating the low-milk diet were still alive and showed no symptoms of acute deficiency disease after many generations, so their nutrition was clearly \"adequate,\" as the word was then commonly used and understood. But, to Sherman's surprise, animals in the other groups attained a higher general level of health as shown by a whole series of criteria, such as more rapid and efficient growth, somewhat larger average sizes at all ages, earlier maturity, longer duration of reproductive life, greater success in the rearing of the young, and greater longevity. To assure certainty of interpretation, Sherman and his associates conducted many such experiments with large numbers of genetically uniform laboratory rats.51 Sherman then sought to identify which nutrients in food were responsible for the improvement in health and vitality of his animals. After conducting another series of experiments, he concluded that the enrichment of an already adequate diet in calcium and vitamins A, C, and G (or B2) were especially important in the improvement of health and longevity. Based on large-scale studies with rats at Johns Hopkins University and Columbia University, Sherman determined that the optimal level of calcium intake was about two to 51 For a summary of these experiments, see Henry C. Sherman, The Nutritional Improvement of Life (New York: Columbia University Press, 1950), 71-76. 46 three times higher than the minimal need, though the better balanced the diet in other respects, the less calcium was needed for best results. Another series of long-term rat-feeding experiments led Sherman to conclude that the optimal requirement for vitamin A was three to four times higher than the minimum need. A number of similar experiments with rats and guinea pigs, according to Sherman, afforded strong evidence that the optimal levels for vitamins B2 and C were several-fold higher than the minimally adequate requirements. Increasing the intake of high-quality protein from milk or meat also appeared to play a supplementary part in improving the health and longevity of rats.52 Like McCollum, Sherman expressed confidence that his research with experimental animals was highly relevant for public health. He pointed out that since the chemistry of human and rat nutrition had been shown to be strikingly similar in most respects, \"the possibilities of improvement revealed by experiments with rats are almost certainly within the scientific probabilities for us human beings.\" And his experience with experimental animals was making it clear, Sherman believed, that \"health is not merely the absence of disease; but, rather, it is a positive quality of life which can be built to higher levels.\"53 He speculated that the consumption of an optimal diet would extend the average human lifespan from 70 to 77 years, and would add ten to fifteen years to a person's \"period of the prime\" between the attainment of maturity and the onset of senility. Sherman saw far-reaching implications for human welfare and cultural development in this possibility. \"There can be no doubt,\" he declared in a 1937 talk, \"that to reach the prime of life a little earlier and to hold it a good deal longer may add very greatly to the individual's chance to achieve his ideals, and to render to others the full meas ure of his best service; for both these take time, 52 Ibid., 155-57. 53 Henry C. Sherman, Calcium and Phosphorus in Foods and Nutrition (New York: Columbia University Press, 1947), 112-13. 47 even with full opportunity.\" In Sherman's view, improving the general level of public health and vitality in this way was a relatively simple matter. The great majority of the population eating a suboptimal diet, he believed, had to take a larger proportion of their necessary calories in the form of the protective foods. Sherman placed particular emphasis on the need to increas e the production and consumption of milk and dairy products, green and yellow vegetables, and fruits.54 Another leading nutrition scientist, John Boyd Orr, conducted experiments similar to those of Sherman. He and his several of his colleagues at the Rowett Institute in Aberdeen, Scotland divided a large colony of laboratory rats in half, and fed one group a control diet and the other an identical diet supplemen ted with additional milk and leafy green vegetables. But unlike Sherman, Orr did not simplify the control diet down to just a couple foods. Rather, Orr's control diet was a close approximation to the average diet eaten by a working-class community in Scotland as ascertained by a dietary survey conducted in the early 1930s. Orr carried out the feeding experiment for four generations of rats and observed the differen ces in health that developed between the two groups. He found that the group of rats fed additional milk and fresh greens grew and thrived in roughly the same manner as rats fed the laboratory's highly nutritious stock ration. The group of animals without additional milk and fresh greens, in spite of similar heredity and environment, showed slightly impaired reproductive capacity, a markedly increased death rate and susceptibility to infection, a slower rate of growth, and a clinically poorer condition as judged by behavior and state of the coat. \"If our results are applicable, even to some extent, to human beings,\" Orr and his colleagues concluded, \"they suggest that a large section of the human population is still far 54 H. C. Sherman, \"The Bearing of the Results of Recent Studies in Nutrition on Health and on Length of Life,\" The Herman Biggs Memorial Lecture, April 1, 1937, in Selected Works of Henry Clapp Sherman (New York: Macmillan, 1948), 947-59. 48 from the optimum state of nutrition, and that much could still be done, by means of improved food supply, to raise resistance to infection, and to improve the physique of human beings.\"55 Long experience in observing experimental animals on various diets also convinced nutritionists that the distinction between adequate and deficient nutrition was not nearly so clear as had been assumed. McCollum, for example, maintained that the consumption of slightly unbalanced diets produced \"border-line states of malnutrition\" that were not as strikingly apparent as the well-known avitaminoses, such as beriberi, scurvy, and rickets. Sherman likewise contended that an insufficient amount of any one nutrient needed for optimal health had wide-ranging, though not easily diagnosable, negative consequences. \"The interrelationship of nutritional factors will have the effect,\" he argued, \"of a divided responsibility for many suboptimal conditions, so that these while really nutritional cannot readily be proved to be so.\"56 Orr, too, pointed out that \"It is possible to produce all gradations of disease from the first slight deviation from perfect health to death according to the degree to which the diet is deficient in certain nutrients.\"57 In experimental animals, the signs of these borderline or suboptimal nutritional states commonly included poor growth and bone development, deteriorating texture and appearance of the coat, heightened susceptibility to infections, digestive and skin disorders, fatigue and lethargy, irritability and apprehensiveness, and reproductive difficulties. In the view of nutritionists such as McCollum, Sherman , and Orr, these experimental findings on borderline nutritional states provided yet more evidence in support of the notion that most of the population in the West 55 John Boyd Orr, William Thomson, and R. C. Garry, \"A Long Term Experiment with Rats on a Human Dietary,\" Journal of Hygiene 35, no. 4 (December 1935): 476-97. 56 H. C. Sherman, \"Some Recent Advances in the Chemistry of Nutrition,\" Journal of the American Medical Association 97, no. 20 (14 November 1931): 1428. 57 John Boyd Orr, The National Food Supply and its Influence on Public Health, Chadwick Lecture, 1934 (Westminster: P. S. King & Son, 1934), 11. 49 would benefit by consuming the protective foods in quantities well above the level required to avoid the obvious symptoms of deficiency disease. One of the earliest and most influential researchers to explore the issue of mild or borderline states of deficiency was Robert McCarri son, a physician from Northern Ireland who served in the Indian Medical Service from 1901 to 1935. During the early years of his service in India, he was stationed as a medical officer in the remote northern frontier region of the country. McCarrison's attention was drawn to the fine physiques, powers of endurance, and relative freedom from disease of certain ethnic groups living there, notably the Pathans, Sikhs, and Hunzas. He noticed that the people belonging to these groups did not suffer the major diseases of Western civilization, such as cancer, peptic ulcer, appendicitis, and tooth decay. He also observed that, generally speaking, \"the physique of the northern races of India is strikingly superior to that of the southern, eastern, and western races .\" McC arrison became convinced that the only obvious differen ces among these groups were in their diets. He concluded that the noteworthy charact eristics of the northern Indians were related to their diets of coarsely ground whole-wheat flour and other wholegrain cereals, milk and milk products, legumes, root and leafy vegetables, fruits, and occasional eggs and meat . The diets of the southern, eastern, and western groups in India, by contrast, consisted largely of milled and polished rice or cassava, with relatively sparse additions of dairy products, legumes, fresh vegetables, fruits, and, due to religious convention, no meat. During this same period, McCarrison began researching the etiology of goiter, a commo n ailment in the foothill region of the Himalayas that marred the otherwise good health and physique of the people living there. These early experiences led him to pursue a career investigating the impact of diet on health. 50 After serving in World War I, McCarrison moved to the Pasteur Institute at Coonoor in southern India, where he devoted his remaining years in the country to testing his observations of human populations using controlled laboratory experiments. In addition to small animals such as rats, guinea pigs, and pigeons, he selected monkeys to be his subjects, because he wanted a species as closely related to humans as possible. And unlike many of his contemporaries, McCarrison did not feed his experimental animals diets of purified food substances in order to ascertain the function of individual nutrients. He thought that human diets were rarely or never wholly lacking in any single essential nutrient, and that much ill health might be caused by multiple or combined deficiencies. In one series of experiments, McCarrison fed rats on diets eaten by certain ethnic groups of eastern and southern India, and produced diseases in the animals similar to those that frequently occurred in these groups. The rats fed on diets approximating those of northern Indian groups, by contrast, remained healthy over succeeding generations. Of these, the Sikh diet produced the healthiest animals; they enjoyed remarkable freed om from disease, as well as from maternal mortality and infant mortality. Since McCarrison was not content merely to note the gross evidence of malnutrition in his experimental animals caused by faulty diets, he undertook microscopic examinations of their tissues, glands, and internal secretions to determine the minor manifestations of ill health. He was, in fact, one of the first researchers to carry out complete postmortem histological examinations of animals fed on deficient diets. He found that deficiency of vitamins A, B, and C in the diet produced derangements in the digestive, absorptive, and motor functions of the gastrointestinal system. He further determined that these degenerative changes greatly increased the susceptibility of his experimental animals to bacterial and other pathogenic agents, which he described as \"often but the weeds which 51 flourish in the soil made ready for them by dietetic defects.\"58 McCarrison encountered a number of other maladies in improperly fed animals, including diseases of the skin, blood, mucous membranes, urinary tract, reproductive system, nervous system, and endocrine glands, particularly the thyroid. In addition, weakness, lassitude, irritability, and similar vague ailments were common. He emphasized that these signs of malnutrition affected his experimental animals long before clinically recognizable evidence of vitamin deficiency appeared .59 This research with experimental animals, McCarrison believed, had a significant bearing on public health. As he remark ed in a 1927 paper, \"If we closely observe animals subsisting on faulty food\u2014even though the fault be not so great as to cause such wreckages of health as scurvy, beriberi, pellagra, rickets or keratomalacia\u2014we notice many signs of impaired well-being which have their counterpart in human subjects similarly situated with respect to the quality and balance of their food.\"60 To describe these less obvious manifestations of malnutrition, McCarri son formulated the concept of \"partial deficiency.\" This type of deficiency, he suggested, was much more widespread in the human population than the severe degrees of vitamin deprivation produced in laboratory animals on purified diets: It is rare that the food of human beings is totally devoid of any one vitamin; it is more usual for the deficiency to be partial, and for more than one vitamin to be partially deficient; it is more usual still for partial deficiency of vitamins to be 58 Robert McCarrison, \"The Effects of Deficient Dietaries on Monkeys,\" British Medical Journal 1, no. 3086 (21 February 1920): 252. 59 For overviews of McCarrison's work in India, see Sir Robert McCarrison, Nutrition and National Health, Being the Cantor Lectures Delivered Before the Royal Society of Arts, 1936 (London: Faber and Faber, 1953), chs. 1-3 passim; W. R. Aykroyd, \"The Nutritional Research Laboratories, Coonoor,\" in The Works of Sir Robert McCarrison, ed. H. M. Sinclair (London: Faber and Faber, Nutrition Research in India\u2014Pre-Independence Era,\" Indian Journal of History of Science 40, no. 1 (2005): 89-93. 60 Robert McCarrison, \"Diseases of Faulty Nutrition (1927),\" in Nutrition and Health (London: Faber and Faber, 1953), 98. 52 associated with deficiency of suitable proteins and inorganic salts and with an excessive richness of the food in carbohydrates. Consequently, the manifestations of disease resulting from faulty foods are compounded of the several effects of varying degrees of avitaminosis on the one hand, and of ill-balance of the food on the other.61 McCarrison also maintained that people rarely ate defective diets for protracted periods of time. As a result, the usual symptoms associated with acute nutrient deficiencies never appeared . His work with experimental animals convinced him that many of the less serious ailments plaguing Westerners were, in fact, the effects of an intermittently poor diet. The range of maladies included a variety of gastrointestinal disturbances, such as loss of appetite, constipation, dysentery, diarrhea, peptic ulcers, and colitis, as well as anemia, respiratory infections, unhealthy skin and mucous membranes, and endocrine dysfunction.62 In the mid-1920s, McCarrison conducted another experiment that confirmed his views on the relationship between diet and health. He fed one group of twenty rats a diet similar to that eaten by the Sikhs, and another group of the same size a diet similar to that eaten by many British people of the poorer classes. The Sikh diet consisted of whole-wheat chapattis, fresh whole milk, butter, sprouted legumes, raw vegetables and fruits, and occasional fresh meat . The diet designed to resemble that eaten by the poor British was comprised of white bread, a margarine substitute made of coconut oil, heavily sweetened tea with a small amount of milk, boiled vegetables, tinned meat and jam, and food preservatives. The group of rats eating the Sikh diet lived happily together, increased in weight, and flourished. The other group eating the poor British diet presented a whole range of problems: they did not increase in weight; their growth was stunted; they were badly proportioned; their coats were rough and lacking in gloss; they lived unhappily together and 61 Robert McCarrison, Studies in Deficiency Disease (London: Frowde and Hodder and Stoughton, 1921), 3-4. 62 McCarrison, Studies in Deficiency Disease, 4-5, 37-47. 53 began to kill and eat the weaker ones among them. Diseases of the respiratory tract and gastrointestinal system were much more common in the group fed the Western diet than in the group fed the Sikh diet.63 McCarrison frequently presented the practical implications of his research to a wider audience, including his fellow physicians. He rejected the notion that access to an abundance of foods protected the populations of Western countries from malnutrition, and he recommended that they consume more milk and milk products, fresh vegetables and fruits, eggs, and whole grains. McCarrison stressed that education of the public in nutrition and dietetics was urgently needed, and that municipalities and other institutions should concentrate on the provision of an abundance of protective foods to the population as a basic public health measure. \"It must be recognized that an optimum supply of all vitamins, in an otherwise well-balanced diet, is a prerequisite of optimum health; and that a minimum supply, while it may suffice for the prevention of certain specific 'deficiency diseases,' creat es the conditions precedent to the occurrence of a wide range of other sicknesses,\" he declared in a 1936 lecture..64 During the 1920s and 1930s, a growing number of physicians besides McCarrison became proponents of the notion that malnutrition was much more prevalent in the West than previously assumed. British pathologist William Cramer, for example, contended in 1925 that \"Up to the present the extensive work on vitamins has concerned itself almost exclusively with the effects produced by a more or less complete absence of vitamin, and too little attention has been paid to the effects produced by the incomplete degrees of restriction 63 Robert McCarrison, \"A Good Diet and a Bad One: An Experimental Contrast,\" British Medical Journal 2, no. 3433 (23 October 1926): 730-32. 64 Sir Robert McCarrison, Nutrition and National Health, 58. 54 in the intake or 'vitamin-underfeeding' as it may be called.\"65 Partly as a result of his own animal-feeding experiments, Cramer came to the conclusion that partial vitamin deficiency\u2014a condition that he termed \"the borderland between health and disease\"\u2014might actually be common in the West.66 James McLester, a professor of medicine at the University of Alabama, was similarly concerned that that a lot of illness in the United States was caused by subacute vitamin and mineral deficiencies. As he wrote in his medical textbook Nutrition and Diet in Health and Disease (1929), \"observation indicates that nutritional failure\u2014not absolute, but relative\u2014resulting in many vague, ill defined states of ill health, is of frequent occurrence.\"67 One of the most influential figures in the medical profession to argue that dietary changes would lead to significant improvements in public health was Edward Mellanby. After working as a research student under Frederick Gowland Hopkins at Cambridge University from 1906 to 1907, Mellanby went on to do his medical training at St. Thomas's Hospital in London, where from 1909 to 1911 he was a demonstrator in physiology. From 1913 to 1920, he held a Chair of Physiology at the King's College for Women in London. It was during his time there that he began his groundbreaking experimental work with dogs on the antirachitic effect of the \"fat-soluble fraction\" in certain animal fats. In 1920, Mellanby became the first occupant of the Chair of Pharmaco logy at the University of Sheffield. At the same time, he was appointed Honorary Physician at the Sheffield Royal Infirmary.68 65 William Cramer, \"Disturbances of Health due to Vitamin Deficiency,\" Proceedings of the Royal Society of Medicine 18 (1925): 10. 66 William Cramer, \"An Address on Vitamins and the Borderland Between Health and Disease,\" Lancet 203, no. 5248 (29 March 1924): 633-40. 67 James S. McLester, Nutrition and Diet in Health and Disease (Philadelphia: W. B. Saunders, 1929), 213. 68 For biographical details, see Henry H. Dale, \"Edward Mellanby, 1884-1955,\" Biographical Memoirs of the Fellows of the Royal Society 1 (November 1955): 193-292; and B. S. Platt, \"Sir 55 Holding both positions enabled Mellanby to study the dietary treatment of rickets in both a laboratory setting with experimental animals and a clinical setting with children.69 Mellanby also became a leading advocate of the theory that vitamin A could be used in \"anti-infective\" therapy. In 1925 an epidemic of bronchopneumonia, a type of respiratory infection, swept through his colony of experimental dogs. Mellanby found on postmortem examination that bronchopneumonia was largely restricted to the dogs that had been rendered vitamin A-deficient. This discovery led him to suspect that a lack of the vitamin increas ed susceptibility to respiratory infections, and possibly other infective conditions as well. Mellanby and Harry N. Green, another physician at Sheffield, confirmed this suspicion in a rat-feeding experiment, the results of which were published in 1928 in the British Medical Journal. Postmortem examinations revealed that in virtually all of the rats raised on a diet deficient in vitamin A, some and generally many of their organs were infected by microorganisms, whereas controls receiving foods containing vitamin A remained free from infection. Sites of infection included the salivary and lymphatic glands, lungs, nasal sinuses, and the alimentary and genitourinary tracts. Mellanby and Green concluded by suggesting that many of the infections commo nly found in Western countries, such as inflammation of the nasal sinuses, middle-ear disease, and pneumonia, were related to vitamin A deficiency. \"We rely almost entirely on milk, butter, egg-yolk, and green vegetables for our supply of this substance,\" they noted, \"and the consumption of these, especially among the poor, is notoriously low.\"70 Edward Mellanby, G.B.E., K.C.B., M.D., F.R.C.P., F.R.S. (1884-1955): Statesman,\" Annual Reviews of Biochemistry 25 (1956): 1-28. 69 See, e.g., Edward Mellanby, \"On Deficiency Diseases, with Special Reference to Rickets,\" British Medical Journal 1, no. 3308 (24 May 1924): 895-900. 70 H. N. Green and E. Mellanby, \"Vitamin A as 'Anti-Infective' Agent,\" British Medical Journal 2, no. 3537 (20 October 1928): 691-96. 56 Even though Mellanby and Green made the most explicit statemen t of the theory that vitamin A was an anti-infective agent, other researchers around that time came to similar conclusions. One of the most noteworthy studies on the pathological effects of vitamin A deficiency, published in 1925, was conducted by Harvard pathologists S. Burt Wolbach and Percy Howe. They fed two groups of albino rats a purified diet, with the control group recei ving an additional supplemen t of butterfat to supply vitamin A. Wolbach and Howe performed close histological examinations of the vitamin A-deprived rats, and found degenerative changes in the epithelial tissues of the respiratory, alimentary, and genitourinary tracts, as well as the eyes and glands. Since the epithelium is the tissue that lines the canals, cavities, and ducts of the body and surfaces exposed to air, their findings suggested the possibility that vitamin A protected the body from infection by preserving a healthy epithelium and preventing microorganisms from penetrating the bloodstream.71 Shortly after Wolbach and Howe reported their results, Henry Sherman and his associates at Columbia University contributed additional evidence that vitamin A played a role in resistance to infectious disease. In rat-feeding experiments, they found that heightened susceptibility to various types of infection was an early manifestation of a dietary low in vitamin A. This weakening of immunity showed a mark ed influence before the appearan ce of the classical physical signs of vitamin-A deficiency, such as xerophthalmia.72 They further demonstrated that providing newborn rats foods rich in vitamin A protected them from developing respiratory infections for many months after they had begun eating diets deficient in this vitamin. Translating these findings into human terms, Sherman 71 S. Burt Wolbach and Percy R. Howe, \"Tissue Changes Following Deprivation of Fat-Soluble Vitamin A,\" Journal of Experimental Medicine 42 (December 1925): 753-77. 72 H. C. Sherman and M. P. Burtis, \"Vitamin A in Relation to Growth and to Subsequent Susceptibility to Infection,\" Proceedings of the Society for Experimental Biology and Medicine 25 (1928): 649-50. 57 recommended that children drink a quart of milk per day in order to obtain quantities of vitamin A sufficient to ward off infection. 73 Sherman's extrapolations from laboratory animals to humans were not entirely speculative. A number of contemporaneous clinical trials with children and adults showed that concentrated preparations of vitamin A or fish oils rich in vitamin A were effective in improving resistance to a variety of infectious diseases. But because other studies failed to consistently confirm these positive results in human subjects, the theory that vitamin A was valuable in staving off infections remained highly controversial well into the 1930s. Nevertheless, the work of Mellanby and other supporters of this theory did not go unnoticed by large pharmaceutical firms, nearly all of which had become involved in the manufacture of vitamin products in the 1920s. These companies were quick to promote the use of vitamin A concentrates and cod-liver oil as \"anti-infective\" therapy in their advertisements.74 Pharmaceutical companies drew on the latest, though often inconclusive, nutrition research to mark et other vitamin products in the interwar period. The first of these products were natural foods that contained unusually large concentrations of the different vitamins, such as several kinds of fish liver oil, brewer's yeast, and wheat germ. After chemists succeed ed in synthesizing several of the vitamins in the 1930s, drug companies began selling purified vitamin preparations as well. While most of the major pharmaceutical firms advertised their vitamin products exclusively to the medical profession, some of the smaller drug companies began marketing vitamins directly to the public in the mid-1930s. Food growers and processors similarly started taking advantage of the newer knowledge of nutrition in the interwar period, after realizing that advertising the healthfulness of their 73 H. C. Sherman, \"Supplementary Relations Among the Nutritive Values of Foods,\" American Medicine 23 (November 1928): 767-70. 74 Richard D. Semba, \"Vitamin A as \"Anti-Infective\" Therapy, 1920-1940,\" Journal of Nutrition 129, no. 4 (April 1999): 783-91. 58 products was a good mark eting strategy. By the 1930s, in fact, claims about the beneficial effects vitamins and minerals had become a major selling point in food advertisements. Some companies also began marketing \"vitamized\" foods, such as vitamin D-enriched infant formulas and marg arine enhanced with vitamins A and D.75 A variety of professional groups\u2014home economists, dieticians, social workers, and academic nutritionists\u2014also became enthusiastic popularizers of the newer knowledge of nutrition in the interwar period. In books, magazine articles, radio broadcasts, publicity camp aigns, and school instruction programs , these groups taught children and adults alike about the benefits of the protective foods and the perils of consuming unbalanced diets lacking in vitamins and minerals. Since these health educators were primarily concerned with conveying their expansive view of the influence of diet on health to non-expert audiences, they often interpreted the latest findings of nutrition research ers in sensational and at times simplistic ways.76 Proponents of vegetarian, raw-food, and other unconventional dietary schemes also responded positively to the newer knowledge of nutrition. The advocates of these regimens had been contending since the nineteenth century that, contrary to orthodox medical opinion, the consumption of a diet composed of unrefined natural foods was the best way to prevent and treat disease. They therefore felt vindicated when scientists discovered in the 1910s and 75 Rima D. Apple, Vitamania: Vitamins in American Culture (New Brunswick, NJ: Rutgers University Press, 1996); Sally M. Horrocks, \"The Business of Vitamins: Nutrition Science and the Food Industry in Inter-War Britain,\" in The Science and Culture of Nutrition, 1840-1940, eds. Harmke Kamminga and Andrew Cunningham (Amsterdam/Atlanta: Rodopi, 1995), 235-58; Sally M. Horrocks, \"Nutrition Science and the Food and Pharmaceutical Industries in Inter-War Britain,\" in Nutrition in Britain: Science, Scientists, and Politics in the Twentieth Century, ed. David F. Smith (London: Routledge, 1996), 53-74; Ackerman, \"Interpreting the 'Newer 111-36. 76 Rima D. Apple, \"Science Gendered: Nutrition in the United States, 1840-1940,\" in The Science and Culture of Nutrition, 1840-1940, eds. Harmke Kamminga and Andrew Knowledge of Nutrition',\" 101-11. 59 1920s that natural foods were more nutritious than processed foods. They also approved of the views of mainstream nutrition researchers such as McCollum and McCarrison, who argued that dietary changes would lead to significant improvements in public health.77 The reaction of the medical community to the newer knowledge of nutrition was much more ambivalent. By the late 1920s, physicians had come to accept the deficiency theory of disease with respect to anemia, beriberi, goiter, pellagra, rickets, scurvy, pellagra, and xerophthalmia. Moreover, specialists in one branch of medicine\u2014pediatrics\u2014benefited greatly from discoveries relating to vitamins C and D. Beginning around the mid-nineteenth century, physicians in Europe and the United States began noticing with increasing frequency a painful disease in infants characterized by swollen limbs and gums. By the early 1920s, it had been determined that this illness, called \"Barlow's disease\" after the British doctor who first described the condition, was a form of scurvy caused by the use of vitamin C-deficient sterilized or condensed cow's milk and proprietary infant formulas. Researchers linked the rising incidence of this disease to two practices that had become increas ingly common in the late nineteenth and early twentieth centuries: the artificial feeding of infants, and the pasteurization of city milk supplies, which had played a large part in reducing outbreaks of typhoid fever, bovine tuberculosis, scarlet fever, septic sore throat, and other diseases. Pediatricians consequently began prescribing the routine feeding of vitamin C-rich orange and tomato juice to infants in the 1920s. About the same time, physicians were also coming to a correct understanding of rickets, a disease that was common among infants and children living in the temperate regions of the world, particularly in urban areas where tall buildings and industrial smoke reduced children's 77 L. Margaret Barnett, \"'Every Man His Own Physician': Dietetic Fads, 1880-1914,\" in The Science and Culture of Nutrition, 1840-1940, eds. Harmke Kamminga and Andrew of Nutrition',\" 176-91. 60 exposure to sunlight. After investigators had determined that the primary cause of rickets was lack of exposure to the ultraviolet rays of the sun, doctors began using ultraviolet lamps and cod liver oil to treat rachitic children, and prescribing daily doses of cod liver oil for all children as a preventive meas ure. By the early 1930s, irradiated foods and vitamin D concentrates also began appearing on the market as well. Owing in part to these developments, the incidence of infantile scurvy and rickets declined mark edly in the 1920s and 1930s.78 Yet, outside the \"pellagra belt\" in the southern United States and areas of endemic goiter in Britain and the northern interior of the United States, physicians in these countries encountered relatively few clinically diagnosable cases of vitamin and mineral deficiency in adults. They therefore had little inclination to worry about what their patients ate. There was, as noted above, a vanguard of nutrition-minded physicians who played a crucial role in promoting the concepts of partial deficiency and optimal nutrition early on in their development. Many leading doctors, however, doubted the relevance of the experiments with laboratory animals that appeared to support these ideas. These skeptical physicians insisted that clinical evidence of marg inal vitamin deficiency and human studies showing the benefits of a diet rich in protective foods were needed before recommending drastic changes in people's eating habits. They also charged that nutrition scientists who did not take great care in presenting the practical implications of their research to non-expert audiences were partly to blame for the spread of \"food faddism\" and \"quackery\" in the interwar period. Edward Cathcart, a physiologist at the University of Glasgow, captured the sentiment of many of his medical colleagues in his assessment of nutrition science in 1921: \"There is at 78 Leslie Harris, Vitamins in Theory and Practice, 3rd ed. (New York: Macmillan; Cambridge: Cambridge University Press, 1938), the 'Newer Knowledge of Nutrition',\" 81-83. 61 present a tendency to draw conclusions in a haphazard way, and attempts are being made to convert a valuable and interesting field into a happy hunting ground for the charlatan and the manufacturer of proprietary remedies.\"79 According to many of these nutrition skeptics, individual appetite and traditional ideas on diet provided better guidance to food selection than the latest recommendations of nutrition scientists. Cathcart, for instance, questioned the practical value of laboratory experiments with animals, and opposed any radical departure from established dietary habits. \"The object of dietary studies,\" he maintained in 1931, \"is to collect in a trustworthy fashion the essential information regarding the nature and amounts of food consumed and then to reduce to some sort of scientific accuracy these 'fruits of colossal experience'.\"80 Another defender of dietary custom was Logan Clendening, a professor of medicine at the University of Kansas and one of the best-known doctors in the United States during the interwar period. In his highly popular books and articles on health and diet, Clendening attacked nutrition scientists who called for drastic changes to existing dietary habits. He saw little evidence that adults needed more vitamins than could be found in the typical American diet, and dismissed the notion that the average person had much to learn from the science of nutrition. According to Clendening, nutritionists had not created a new and better diet despite all they had learned, but \"they have simply proved why the old one so long in use was effective.\"81 Nutritionists and nutrition-minded physicians, in response, strained to downplay their role in the sudden mania for vitamins, and were at the forefront in efforts to control and 79 E. P. Cathcart, The Physiology of Protein Metabolism (London: Longmans, Green & Co., 1921), quoted in David and Malcom Nicolson, \"The 'Glasgow School' of Paton, Findlay, and Cathcart: Conservative Thought in Chemical Physiology, Nutrition and Public Health,\" Social Studies of Science 19, no. 2 (May 1989): 204. 80 E. P. Cathcart, \"The Significance of Dietary Studies,\" Journal of the Royal Sanitary Institute 52 (1931): 182. 81 Logan Clendening The Care and Feeding of Adults (New York: A. A. Knopf, 1931), 160. 62 regulate the claims made for these substances by the food and drug industries. But they also maintained that the dismissive attitude of many medical authorities to vitamins was unwarranted. As Edward Mellanby declared in a speech at the annual meet ing of the British Medical Association in 1927: Many of the established physiologists and pathologists of this country have not only failed to see the scientific and practical significance of the new dietetics, but have been sceptical and often even hostile as to the existence of these substances. The popularity of the subject among the general population and the uncritical way in which vitamins have been discussed have only made the atmosphere which surrounds nutritional investigation worse.82 Leslie Harris, a nutritionist at Camb ridge University, also attributed the resistance to the newer knowledge of nutrition that arose in the medical community to intellectual inertia. \"Physiologists had grown so used to calculating diet values solely in terms of calories or fuel value that the uncertainty and complication of the vitamins was rather distressing,\" he remarked. On the other hand, Harris granted, \"It is an unfortunate fact that food science does lend itself very easily to imposition, to quackery, to crankiness, and to specious advertising claims of all kinds.\"83 Henry Sherman took a slightly differen t tack. He blamed the widespread skepticism of physicians on the \"pharmacological bias\" of their profession, which inclined them to view vitamins as substances to cure and prevent specific diseases rather than as nutrients essential to normal health. As Sherman asserted in a 1931 paper in the Journal of the American Medical Association, \"deficiency\" was still at that point a somewhat ambiguous term, whereas the health benefits of a liberal intake of the protective foods was \"a matter of more positive knowledge to all who will study the experimental evidence.\"84 82 Edward Mellanby, \"Duties of the State in Relation to the Nation's Food Supply: Research on Nutritional Problems,\" British Medical Journal 2, no. 3483 (8 October 1927): 634. 83 Harris, Vitamins in Theory and Practice, 5th ed., 32, 187. 84 Sherman, \"Some Recent Advances in the Chemistry of Nutrition,\" 1427. 63 And in fact, nutrition skepticism in the medical commu nity did not persist unabated throughout the interwar period. This shift in outlook occurred mainly because, as the 1920s and 1930s progressed, researchers accumulated several new types of evidence that proved more persuasive to physicians than animal-feeding experiments and observational studies. Shortly after World War I, investigators began carrying out dietary intervention studies in children that were crucial in convincing the medical commu nity of the health benefits of the protective foods, and of milk in particular. Harold Corry Mann, under the auspices of the Medical Research Council, conducted one of the foundational studies of this type at an institution for orphaned boys in London from 1921 to 1925. He fed groups of boys diets enhanced with milk, butter, sugar, vegetable margarine, casein, or watercress, with a control group fed the regular institutional diet, which was adjudged to be well planned and nutritionally adequate by the medical standards of the day. The most notable difference in weight and height gain over the period of the study was between the control group and those recei ving an extra pint of milk daily. Butter also exerted a mark ed growth-promoting effect, though to a lesser degree than milk. Observers in the study also reported that the milk-fed group remained relatively free from minor illnesses and showed higher spirits.85 Other milk-feeding tests were undertaken in the following decade in Britain and the United States, and most of them supported Corry Mann's conclusions. Some of these experiments were carried 85 H. C. Corry Mann, Diets for Boys During the School Age, MRC Special Report, Series No. 105 (London: HMSO, 1926). Celia Petty has criticized Corry Mann's study on the grounds that it recorded catch-up growth in chronically malnourished children and not optimal growth in already adequately nourished children, as its proponents claimed. But for our purposes here, it only needs to be noted that the greatest gains in height and weight were recorded in the butter and milk groups, which suggests that the nutrients in shortest supply were the fat-soluble vitamins, not, as a comparison with the margarine and casein groups indicates, calories or protein. See Celia Petty, \"Food, Poverty and Growth: The Application of Nutrition Science, 1918-1939,\" Society for the Social History of Medicine Bulletin 40 (1987): 37-40. 64 out on a much larger scale, with more attention paid to randomization of the subjects and to attaining a sampling frame representative of the population.86 Mean while, researchers began developing methods for determining human nutritional requiremen ts that won wide acceptance in the medical community. Nutrition scientists had been beset by difficulties when they first attempted to draw up vitamin requirements in the 1910s and 1920s. In the first place, researchers during this time had not been able to actually isolate or determine the chemical structure of any of the vitamins, so they could not provide definitive evidence that particular conditions of ill health were caused by the lack of a specific vitamin rather than some other substance in a foodstuff or vitamin concentrate. Moreover, the methods that nutritionists used to estimate vitamin requirements were of questionable relevance to humans. To determine an animal's need for a particular vitamin, researchers would feed it a basal diet containing adequate amounts of all the other nutrients, and then examine the effects of varying amounts of a supplement containing concentrated amounts of the vitamin in question on the animal's health. A quantity sufficient to prevent symptoms of disease and enable proper growth was considered the minimal requirement. This method, which endangered the life of the subject animals, could obviously not be used on humans. Nor was it possible to extrapolate with any certainty the nutritional requiremen ts for humans from figures derived for different animal species. But, 86 For Britain, see John Boyd Orr, \"Milk Consumption and the Growth of School Children,\" British Medical Journal 1, no. 3548 (5 January 1929): 202-03; Gerald Leighton and Peter L. McKinlay, Milk Consumption and the Growth of School Children: Report on an Investigation in Lanarkshire Schools (Edinburgh: HMSO, 1930); Milk Nutrition Committee, Milk and Nutrition: New Experiments Reported to Milk Nutrition Committee from the National Institute for Research in Dairying, Vols. 1-4 (Reading: Poynder, 1937-39). For the United States, see L. J. Roberts, L. Carlson, and V. MacNair, \"The Supplementary Value of Dry Skim Milk in Institution Diets,\" Journal of the American Dietetic Association 10 (1934): 317-324; L. J. Roberts, R. Blair, B. Lenning, and M. Scott, \"Effects of a Milk Supplement on the Physical Status of Institutional Children: I. Growth in Height and in Weight,\" American Journal of the Diseases of Children 56 (1938): 287-300; V. MacNair and L. J. Roberts, \"Effects of a Milk Supplement on the Physical Status of Institutional Children: II. Ossification of the Bones of the Wrist,\" American Journal of the Diseases of Children 56 (1938): 494-509. 65 lacking better methods, nutritionists up to the early 1930s based their dietary recommendations primarily on approximations derived from animal-feeding experiments. They advised people to eat a certain number of servings of the various \"protective foods\" that, according to their rough estimations, contained more than adequate amounts of the known vitamins.87 The shortcomings of this technique for determining dietary standards became particularly apparent in the controversies that arose in the wake of the Great Depression over the extent of malnutrition in Britain and the United States. Although average rates of mortality in these countries continued to decline in the 1930s, some public health workers and social scientists produced evidence that the health of people in certain hard-hit regions and income groups was worsening as a result the economic downturn. These critical assessments were bolstered by a number of dietary surveys conducted in the early 1930s, which reported that a large proportion of poor and working-class households were consuming barely adequate or below-minimum amounts of the protective foods. Investigators were particularly disturbed to find that few families, at all income levels, consumed as much milk as nutritionists recommended. But these findings were not compelling to many medical authorities, since the dietary guidelines against which the survey data were compared was based on the estimations of nutritionists working with animals, not humans. The other principal techniques used to assess the extent of malnutrition\u2014medical examination and anthropometric measurement\u2014proved little more persuasive. While anthropometric measurements were useful for gauging the nutritional status large groups of children or adults, they were faulted for ignoring the fact that individual children grew and gained weight at different rates and that ethnic stocks had 87 Ackerman, \"Interpreting the 'Newer Knowledge of Nutrition',\" 272-75. 66 differen t builds. And medical examinations produced notoriously discrepant results, since they were based on highly subjective criteria such as skin complexion, luster of the hair, color of the mucous membranes, mental alertness, muscle tone, and breathing, in addition to height and weight. Studies carried out in the 1930s, in which several medical examiners were asked to assess the nutritional status of the same group of subjects, demonstrated complete lack of uniformity.88 But several key research achievemen ts largely overcame these difficulties. One of the most important developments that enabled research ers to formulate quantitative nutrient requirements was the isolation, chemical identification, and synthesis of many of the vitamins. During the 1910s and early 1920s, chemists had succeeded in producing concentrated extracts of various vitamins, but no one had demonstrated that they had isolated a vitamin in its pure form. Then, in 1926, researchers isolated vitamins B1 and C, and by 1931 they had also isolated vitamin D. In 1932, scientists determined the chemical structure of vitamin D and achieved its chemical synthesis. The same year, the structure of vitamin C was also determined, and in 1933 chemists synthesized it as well. Shortly thereafter, most of the other vitamins followed suit. By 1940, in fact, chemists were able to synthesize eight of the eleven known vitamins. The purification and synthetic production of vitamins enabled researchers to provide clinical evidence that particular conditions of ill health were caused by the lack of a specific vitamin, and not some other substance in a foodstuff or food concentrate.89 88 W. R Dunstan, \"The General Assessment of Medical Officer 57 55; \"Mathew Derryberry, \"Reliability of Medical Judgments on Malnutrition,\" Health Reports 53 (18 February 1938): 263-88; Webster, of Nutrition,\" 249-2\\55. \"A Short History of Nutritional Science: Part 3,\" 3028. 67 During this same period, new diagnostic tools were developed that allowed medical clinicians to identify mild or subacute vitamin and mineral deficiencies in humans. Prior to the 1930s, there were a limited number of tools available to detect nutritional deficiencies. Even before scientists had found that rickets was caused by a deficiency of vitamin D, physicians had been using x-rays of children's bones to reveal if they suffered this condition. Doctors had also long relied on estimates of hemoglobin in blood samples to detect iron-deficiency anemia. But in the early 1930s, medical research ers discovered ways to identify minor deficiencies of vitamins A and C as well. Physicians had known for several years previously that people who consumed insufficient amounts of vitamin A or its plant-derived precursors, carotenes and cryptoxanthin, often experienced night blindness, a visual abnormality that makes the eyes less responsive to dim illumination. Philip C. Jeans, a professor of pediatrics at Iowa State, developed a technique using a photometer to identify subjects who had below-normal sensitivity to light, and demonstrated that such persons were likely suffering slight deficiencies of vitamin A. Jeans and his colleagues determined that a surprisingly large percentage of the population showed symptoms of this condition. Researchers subsequently performed similar tests and confirmed their findings. About the same time, the recognition that scurvy produced a weakening of the capillary system inspired a Swedish researcher, Gustav F. G\u00f6thlin, to invent a capillary resistance test, involving the application of pressure to a patient's skin, to detect minor deficiencies of vitamin C.90 90 Esther Peterson Daniel and Hazel E. Munsell, Vitamin Content in Foods: A Summary of the Chemistry of Vitamins, Units of Measurement, Quantitative Aspects in Human Nutrition and Occurrence in Foods, USDA Misc. Pub No. 275 (Washington: USDA, 1937); Milbank Memorial Fund, Nutrition: The Newer Diagnostic Methods (New York: Milbank Memorial Fund, 1938); C. C. Ungley, \"Some Deficiencies of Nutrition and Their Relation to Disease: I. Origin and Detection of Nutritional Deficiencies,\" Lancet 231, no. 5982 (23 April 1938): 925-32; Lela Booher and Elizabeth Callison, \"Vitamin Needs in Man,\" in Food and Life: Yearbook of Agriculture, 1939 68 New chemical tests also promised to provide physicians and nutritionists with more accurate ways of detecting marginal or subacute vitamin deficiencies. As a result of the isolation and chemical determination of vitamins in the 1930s, tests were developed that could determine the amount of these substances in people's blood, tissues, and urine, as well as food. During this same period, researchers were also beginning to understand the physiological function of some of the vitamins. In the process, they learned that insufficient intake or improper absorption of some vitamins caused metabolic failures that produced a buildup of certain undesirable compounds in the body. Tests were consequently developed to meas ure the amounts of these substances in people's blood and urine. A deficiency of thiamin, for example, was found to cause an abnormal accumulation of pyruvic acid in the blood and tissues. This discovery, in turn, helped to explain the cause of symptoms such as muscle stiffness and nervous system disturbances that had been associated with an insufficient intake of the vitamin since its discovery decades earlier.91 The development of tools that could diagnose subacute deficiencies of vitamins also allowed clinical researchers to conduct risk-free human feeding experiments that could establish useful standards. To calculate people's requiremen t for a particular vitamin, subjects were fed basal diets supplemented by a sufficient amount of the vitamin to avoid serious illness but not enough to prevent a slight deficiency that could be detected with the new diagnostic tools. The subjects were then provided additional quantities of the vitamin (Washington: GPO, 1939), 221-71; Dwight L. Wilbur, \"Disease of Metabolism and Nutrition: Review of Certain Recent Contributions. II: Nutrition\" Archives of Internal Medicine 63, no. 2 (February 1939): 385-427. 91 Milbank Memorial Fund, Nutrition: The Newer Diagnostic Methods; Ungley, \"Some Deficiencies of Nutrition and Their Relation to 925-32; Lela E. Booher and Elizabeth C. Callison, \"Vitamin Needs in Man,\" in Food and Life: Yearbook of Agriculture, 1939 (Washington: GPO, 1939), 221-71; Wilbur, of Metabolism \"Vitamin Needs in Man,\" 221-71; Robert R. Williams and Tom D. Spies, Vitamin B1 (Thiamin) and Its Use in Medicine (New York: Macmillan, 1938). 69 until their symptoms could no longer be detected. By using the photometer and the capillary resistance test in this manner, researchers determined minimal requirements for vitamins A and C respectively. Investigators similarly used x-rays to determine the minimum quantity of vitamin D required to protect infants and children from rickets. Meanwhile, other nutrition workers used measurements of the amount of certain vitamins and minerals in the blood and urine as another yardstick for estimating a person's state of nutrition. Quantitative requirements for vitamins B1 and C were determined by feeding patients supplemen ts of these vitamins until their blood content and excretion rates rose to levels that indicated tissue saturation. Researchers also used the amounts of calcium retained and excreted by the body as a criterion for judging the adequacy of a person's vitamin D intake.92 Soon after the requirements for several of the major vitamins had been determined, researchers began using them to assess the nutritional adequacy of diets consumed by large sections of the population. In the United States, one of the most extensive dietary surveys was conducted by the USDA Bureau of Home Economics between December 1934 and February 1937, the results of which were reported by Hazel K. Stiebeling and Ester F. Phipard in Diets of Families of Employed Wage Earners and Clerical Workers in Cities (1939). In the study, surveyors recorded information on the kinds, quantities and monetary value of foods consumed by over four thousand urban working-class and lower-middle-class families over a one-week period. The families were from cities in all regions of the country; while most were white, some Southern black families were also included. The groups examined represented a higher economic level than characterized urban wage earners as a whole. By comparing the nutritive content of the foods consumed by these families to the 92 Booher and Callison, \"Vitamin Needs in Man,\" 221-71. 70 latest quantitative nutritional requiremen ts, the investigators in the Bureau of Home Economics were able evaluate the quality of \"typical\" urban working-class diets. The home economists classified the nutritional quality of the diets they surveyed as \"good,\" \"fair,\" or \"poor,\" according to how much protein, iron, phosphorus, calcium, and vitamins A, B1, B2, and C that they contained in relation to both the average minimal requirement for these nutrients and a higher standard representing a wide margin of safety above minimal needs. The liberal standard was 50 percent higher for protein and minerals, and 100 percent higher for the vitamins. A diet was deemed \"good\" if its nutritional content met or exceeded the liberal standards set for all eight nutrients; it was designated \"poor\" if it contained less than the minimally required amount of at least one of these nutrients; the rest of the diets were graded \"fair.\" Based on these criteria, the bureau's home economists determined that less than 20 percent of all families ate a good diet, while 40 to 60 percent of the white families and over 60 percent of the black families ate a poor one. Moreover, they suspected that the problem might actually be worse than these figures indicated, since the study did not take into account food waste or loss of nutrients during cooking or storage. The investigators found that urban working-class diets were most likely deficient in calcium, vitamin A, and, to a lesser extent, vitamins B1 and C. They also found that the most significant factor determining the nutritional adequacy of a family's diet was its per capita food expenditure, which in turn was related to family income. The bureau's economists calculated that about half of the surveyed families were able to spend as much on food as they needed to obtain a good diet, yet less than four out of ten of these families selected foods wisely enough to achieve this standard. This finding suggested to the investigators 71 that lack of purchasing power and ignorance of the newer knowledge of nutrition both contributed to the widespread incidence of malnutrition.93 Mean while, dietary surveys of the British population presented a similarly bleak picture. One of the most influential of these surveys was John Boyd Orr's Food, Health and Income (1936). From 1935 to 1936, the staffs of the Rowett Institute and the Mark et Supply Committee of the Agricultural Marketing Boards, working under the supervision of Orr, collated all the data on dietary surveys that had been previously conducted, in order to obtain an estimate of the nutritional state of Britain. Orr used an \"optimum\" rather than a \"minimum\" nutritional standard to assess the diets of 1,152 families, which he divided into six income groups. He then applied his findings to the population as a whole and compiled data to estimate the average expenditure on food per week. Using the stringent standards for optimum diet, Orr determined that malnutrition was so extensive that it affected all but the highest income group. He estimated that, while most of the British population secured sufficient calories and protein, only the top three income groups met or exceeded the standard in vitamins. Even worse, only the two highest income groups had an ample margin of safety in minerals, and even the second-highest group was possibly below standard in calcium. Orr corroborated these calculations with British studies showing that the average rate of growth in children and final height attained in adults increas ed with rising incomes. He also cited the experiments demonstrating accelerated gains in weight and height in schoolchildren fed supplements of milk products, which suggested to him that children from the working classes were growing below their inherited potential. Orr tentatively concluded from his survey that a large proportion of British families had an income insufficient to 93 USDA, Diets of Families of Employed Wage Earners and Clerical Workers in Cities, by Hazel K. Stiebeling and Esther F. Phipard, USDA Circ. No. 507 (Washington: GPO, 1939). 72 support an adequate diet, however knowledgeable the housekeeper might have been in food preparation and dietetics. \"As income rises the average diet improves, but a diet completely adequate for health according to modern standards is reached only at an income level above that of 50 per cent of the population,\" he warned. By the time the second edition of Food, Health and Income went to press in 1937, the League of Nations Technical Commission had released dietary standards that largely accorded with those developed by Orr.94 The alarming findings reported in the Stiebeling-Phipard and Orr surveys were repeated in other studies in the following years.95 But the new diagnostic tools and biochemical and physiological tests used for assessing nutritional status were only starting to be applied on a significant scale by the outbreak of World War II. Moreover, the variety of methods used to formulate vitamin requirements resulted in differing estimates of the extent of malnutrition. The standards that had been determined by observing the effects of vitamin supplemen ts on mild cases of deficiency were, on the whole, less than those that had been based upon analyses of blood and urine. As a result, some skeptics continued to doubt the claim that mild or subacute malnutrition was widespread. Edward Cathcart, for example, pointed out in 1939 that medical examiners were still reporting widely varying rates of malnutrition in contiguous areas with populations of similar occupational and demographic 94 John Boyd Orr, Food, Health and Income: A Survey of Adequacy of Diet in Relation to Income, 2nd ed. (London: Macmillan, 1937), 44-54, 58-67, 77-79. 95 See, e.g., Sir William Crawford and Herbert Broadley, The People's Food (London: Heinemann, Coons, Diets in the United States,\" in Food and Life: Yearbook of Agriculture, 1939, 296-320; USDA, Are We Well Fed? A Report on the Diets of Families in the United States, by Hazel K. Stiebeling, USDA Misc. Pub. No. 430 (Washington: USDA, 1941). 73 makeup. The real cause of this sort of discrepancy, he insisted, was that \"there is no reliable objective measure of the state of nutrition.\"96 However, nutritionists accounted for the discrepancies in estimates of vitamin and mineral requirements by drawing on the distinction that researchers had made between adequate and optimal nutrition. Animal-feeding experiments had shown that the quantity of vitamins and minerals needed to protect laboratory animals from disease and promote adequate growth was usually a fraction of the amount that produced the \"optimal\" state of health. It was thus suggested that the lower values, which marked the point at which symptoms of deficiency disease disappeared, represented minimum vitamin and mineral needs, while the higher figures, which indicated when the tissues had become saturated, approximated optimal needs. By the close of the 1930s, therefore, nutrition investigators had successfully produced the type of evidence in humans that proved convincing to the medical community.97 As British physician Cyril Donnison sardonically remarked in 1937, \"the medical profession appears to be under the influence of a wave of enthusiasm on the subject of nutrition. The list of diseases which are attributed to faulty nutrition seems to grow rapidly, but not rapidly enough for some of the enthusiasts who seem to see deficiency or imperfect balance of diet as the basis of nearly all pathology.\"98 96 E. P. Cathcart, \"Medical Aspects of Nutrition,\" in Nutrition and the Pubic Health: Medicine, Agriculture, Industry, Education, Proceedings of a National Conference on the Wider Aspects of Nutrition (London: BMA, 1939), 18. 97 Ackerman, \"Interpreting the 'Newer Knowledge of Nutrition',\" 276-77. 98 C. P. Donnison, Civilization and Disease (Baltimore, MD: William Wood & Company, 1938), 71. 74 \"A Marriage of Health and Agriculture\": The Newer Knowledge of Nutrition and the Farming Crisis of the 1930s It was not just the scientific and medical commu nities that supported the notion that a large proportion of the population would benefit from eating more of the \"protective foods.\" Some agricultural experts argued that increasing the consumption of these foods would not only improve public health, but also help alleviate the problem of food surpluses, and the low prices consequent upon them, that reached critical proportions in the 1930s. Even before the onset of the Great Depression, farmers had been unable to fully adjust to the convulsions in agricultural mark ets caused by World War I. The outbreak of the war in 1914 brought the entire edifice of global capitalism crashing down in a very short time, and with it went the more or less smoothly functioning global trade in foodstuffs. World War I also brought into sharp relief the dangers of international agricultural specialization that had been rapidly intensifying in the preced ing half-century. This was particularly the case for the most industrially advanced nations such as Britain and Germany, which had come to depend on imports for significant amounts of their food, animal feed, fibers, and industrial fats. In the years just preceding the outbreak of the war, the world's agricultures reached a peak of international cooperation and division of labor. Despite existing tariff obstacles to imports in various countries, farmers in the western and central parts of continental Europe had become accustomed to the trend of an ever-increasing demand for animal products and the availability of low-priced grain and high-protein animal feeds from overseas and Russia.99 The disruption in supply chains and increase in demand caused by World War I was a boon to farmers , at least in areas that were not in chaos. To capitalize on high prices and 99 Karl Brandt, The Reconstruction of World Agriculture (New York: W. W. Norton & Company, 1945), ch. 1 passim. 75 aid in the war effort, the United States and the British Dominions in particular increased their food production enormously. Governments guaranteed a high minimum price for wheat as part of their war effort. British farmers, reversing the trend of the preceding half-century, converted thousands of acres from pasture to arable for domestic grain consumption. The United States saw a rapid expansion in the use tractors and mech anization generally in order to more efficiently use scarce labor. Grain farming was carried further into the semiarid zones of the Great Plains and the eastern slope of the Rockies. With minimum prices guaranteed by the government, it became profitable to purchase arid and marg inal grazing land and to put it to wheat. In Canada the wheat boundary was pushed further north for the same purpose. In Australia bush land was cleared and in Argentina more pampas plowed.100 The forthcoming prosperity contributed greatly to the expansion of acreage, as farmers took out loans in expectation of future profits. During the boom years of the war, land values skyrocketed, and these values found expression in proportionately higher mortgages. But soon after the war, it became apparent that the agricultural situation was unbalanced. The \"Roaring Twenties\" largely bypassed the world's farmers. In the early 1920s, as economies in the war-torn regions recovered and good harvests were made, the bottom fell out of agricultural commodities markets. Farmers faced another round of ruinously low prices for their crops. After the years of World War I, during which expansion of physical output was the paramount necessity, a period began in which net profit had to again justify expenditures. Public budgets were curtailed and major efforts made to balance them. For instance, in the 1920s Britain strained greatly to restore its status quo ante bellum by cutting budgets, restoring free mark et conditions, and returning to the gold 100 Ibid., 45. 76 standard.101 In the case of the country's agriculture, this mean t the elimination of guaranteed grain prices in 1921, only a year after they had apparently been enshrined in government policy by the Agriculture Act of 1920. This policy reversal came to be known as the \"Great Betrayal,\" though, as historians have determined, it was more a betrayal of farm laborers than farm owners.102 Agricultural leaders disliked government \"interference\" with regulating wage levels so intensely that they agreed to the elimination of guaranteed commodity prices in exchange for the abolition of minimum wages for farm laborers. World War I also drastically impacted rural societies. In Britain, the final straw that led to the disintegration of many of the estates that had dominated the countryside for centuries was the imposition of death duties and controls on land rents during the war. In the immed iate postwar years, in Britain as elsewhere in Europe, there was a massive transfer of land. Many large British estates were broken up and sold off to farmer tenants as well as financiers and merchants who had grown rich off war contracts. But the owner-occupiers soon found themselves struggling under high mortgages, in many cases having to let fields, buildings, and drainage systems subside into dereliction. Farmers regarded wages as too high and sought labor-saving forms of production, with the result that rural population continued to decline. The acreage of plowed land, which had been increased from 5.8 million to 6.2 million hectares during World War I, was reduced to 5.3 million hectares. At the same time, there were significant advances in market gardening, in milk, egg, and fruit production, in the sugar beet industry, in canning, and in agricultural technology. Notwithstanding expansion in these high-value areas, Britain remained the world's greatest mark et for foods from everywhere. In 1930, with less than 3 percent of the world's 101 Ibid., ch. 2 passim. 102 Jeremy Burchardt, Paradise Lost: Rural Idyll and Social Change in England Since 1800 (London: I. B. Taurus, 2002), 107. 77 population, the British market accounted for 99 percent of the world's exports of bacon and ham, 96 percent of mutton and lamb, 62 percent of eggs, 59 percent of beef, 46 percent of cheese, and 28 percent of wheat and wheat flour. When World War II began in 1939, Britain's dependence on overseas supplies was even more pronounced than it had been at the outbreak of World War I.103 Elsewhere, the decade of the 1920s was one of stabilization and uneven progress, though farm ers viewed the situation as one of semi-depression. Following the sudden collapse of prices in 1920-21, the terms of trade for world agriculture partially recovered to their prewar levels. In the United States, grain production was carried on at 16 or 17 percent above prewar levels. The process of tractorization, motorization, and mechanization in general, which had been stimulated by the war, came into full stride, with the United States leading the way. By 1930, there were 4.1 million automobiles, 900,000 trucks, and 920,000 tractors on American farms. As cars, trucks, and tractors moved in, horses went out. From 1920 to 1930 the stock of horses on farms was reduced from 20.1 to 13.7 million. The six million fewer draft animals freed roughly 22 million acres, about eight percent of the cropland of the United States, for uses other than animal feed. In other words, petroleum began replacing pastures, which meant that fewer farmers could feed more people. At the same time, direct human consumption of grain fell off while the per capita intake of dairy and poultry products, fruits, and vegetables gained.104 This dietary shift was matched in agricultural markets, where the real prices farmers got for cereals and livestock never 103 John Martin, The Development of Modern Agriculture: British Farming since 1931 (London: Macmillan, 2000), 10. 104 Brandt, Reconstruction of World Agriculture, 53-54. 78 regained their prewar levels, while those of dairy products and fruits exceeded them by the late 1920s.105 Despite the partial agricultural readjustment and recovery of the 1920s, there were signs of a gathering storm. The largest problem for the world's farmers at that time was usually not overproduction, but indebtedness. This was more the case the more completely linked a country's agriculture was with the market economy. In the United States, to use the economically largest example, the real value of farmer debt soared between 1918 and 1921 on the expectation of continued high profits. Farmers borrowed for a variety of purposes, but mainly for land. The collapse in commodity prices beginning in 1920 dramat ically worsened the financial conditions of farmers . When the speculative frenzy ended, interest rates rose and farmers incurred huge capital losses when the price of land fell. By the end of the decade, the debt burden on farmers had decreased, but it was brought about almost exclusively by foreclosures. Farmers who managed to stay in business were generally heavily indebted, which left them badly exposed with the onset of the Great Depression, when the international credit system all but ceased to function. Interest rates increased and gobbled up a growing share of farmers ' declining incomes. Real prices of agricultural commodities in the early 1930s fell faster than interest on mortgage debts and taxes, resulting in a wave of foreclosures of unprecedented magnitude. Rural protests became violent in especially hard-hit areas.106 Although the agricultural depression reached its nadir in the early 1930s, it persisted through the rest of the decade. The slowness of agriculture to bring down supply to match deman d was largely due to the perverse fact that, for indebted farmers suffering from high 105 Giovanni Federico, \"Not Guilty? Agriculture in the 1920s and the Great Depression,\" The Journal of Economic History 65, no. 4 (December 2005): 961. 106 Brandt, The Reconstruction of World Agriculture, ch. 3 passim. 79 interest rates, the remedy for lower commo dity prices seemed to lie in increased production. In other sectors of the economy, by contrast, demand shocks caused a fall in output. In short, farmers tried to meet their financial obligations by selling more at lower prices, and by working harder with the land, labor, and tools available. Such behavior in aggregate only exacerbated the oversupply problem, which at the very least benefited economically strained urban populations with cheaper food. But where agriculturalists were well organized, great pressure was put on governments to break the grip of the agricultural depression by raising prices, relieving debt burdens, and adjusting supplies. During the 1930s, farmers were swept up in the pell-mell retreat from multilateral efforts to restore free-trade conditions, as the world's major powers slid toward economic nationalism and autarky. In a signal development, Britain abandoned the gold standard in 1931. And, following the Ottawa Conference in 1932, the metropole established a scheme of \"Imperial Preference,\" which included limited tariffs to farmers in the dominions and colonies of the British Empire and high tariffs with the rest of the world. Notwithstanding defensive interventions of this sort, the situation of many farmers would not mark edly improve until the arrival, yet again, of world war in 1939.107 It was in this context of persistent food surpluses and rural social turmoil that agricultural authorities began joining nutritionists and nutrition-minded physicians in calling for the dietary improvement of a large proportion of the population. One of the first proposals to link better nutrition to public health as well as agricultural policy was made at the assembly of the League of Nations in September 1935. The delegates at this gathering came to general agreement that the output-restriction policies being pursued by most national governments at that time in order to raise the prices of agricultural products were 107 Ibid., ch. 3 passim. 80 wrongheaded. They recommended that the expenditure and efforts of these governments should instead be directed toward improving the nutritional status of their constituent populations by increas ing the consumption of many of the very agricultural products whose supply was being restricted. Such an approach, the delegates pointed out, would benefit both farmers and consumers . Stanley Bruce, the Commissioner for the Commonwealth of Australia, illustrated this suggestion by famo usly describing the proposal as \"a marriage of health and agriculture.\"108 Soon after, other agricultural commentators echoed the recommendations drawn up at the League of Nations assembly. Viscount Astor and Seebohm Rowntree, for example, suggested in their 1938 analysis of British farming that dietary reform would soon become a priority in the formulation of the country's agricultural policy. \"It seems probable,\" they suggested, \"that the improvement of the standards of nutrition will come increasingly to be regarded as an important aim of national policy analogous to improvement of sanitation in the pre-war and of housing standards in the post-war period.\"109 And in April 1939, the British Medical Association organized a conference composed of representatives of medicine, agriculture, industry, and education to discuss the wider aspects of nutrition. Although disagreements arose between the delegates at the conference on some issues, they unanimously agreed that the government should formulate \"a long-term food policy in which the requirements of health, agriculture, and industry shall be considered in mutual relation.\" They also resolved that \"meas ures to secure the more ready availability to all sections of the 108 League of Nations, Final Report of the Mixed Committee on the Relation of Nutrition to Health, Agriculture and Economic Policy (Geneva: League of Nations, 1937). 109 Viscount Astor and Seebohm B. Rowntree, British Agriculture: The Principles of Future Policy (London/New York: Penguin, 1938), 25. See also Economic Reform Club, Health, Agriculture and the Standard of Living (London, 1939). 81 community of foodstuffs which are held to be desirable on nutritional grounds should be accompanied by an educational campaign to encourage their increased consumption.\"110 John Boyd Orr, one of the foremost proponents of the so-called marriage of health and agriculture, believed that improved nutrition would not only help to solve Britain's domestic problems, but also draw it into a closer relationship with its Empire. After reminding his audience in a 1936 speech that Britain would continue to depend on food imports from its overseas territories for the foreseeable future, he remarked that The development of agriculture would involve increased settlemen t on the lands, and fortunately the protective foods are those which give the maximum employment per acre. In the last few years we have always had somewhere between one and two million unemployed, with the result that we have families living in the slums badly housed, badly fed and leading a sub-human existence, contributing nothing to the commonwealth, while at the same time there is available land at home and even better land in the Dominions on which some of these could produce the additional protective foods which such a large proportion of our population including themselves need to enable them to lead healthy, happy lives.111 Orr was claiming, in short, that Britain could address its persistent urban and rural woes and improve its public health once the country's policymakers adequately grasped the importance of the protective foods. Arguments linking better nutrition to agricultural recovery were aired in the United States at roughly the same time. Hazel Stiebeling, for example, contended in 1939 that From the standpoint of health better diets could mean smaller outlays for illness, less loss of working time, greater physical efficiency, and longer, more productive life. For agriculture, better diets would mean increased production of fruits, succulent vegetables, butter, and milk, and hence more cows and the production of more feed.112 110 BMA, Nutrition and the Public Health: Medicine, Agriculture, Industry, Education. Proceedings of a National Conference on the Wider Aspects of Nutrition (London: BMA, 1939), 136. 111 John Boyd Orr, \"Nutrition and Physical Fitness in Relation to Empire Development, Agriculture, Trade and Emigration,\" Conference on Empire Development, 1936, 3, JBO 9/1-10, Reid Library, Rowett Research Institute, Aberdeen, UK. 112 Hazel K. Stiebeling, \"Better Nutrition as a National Goal,\" in Food and Life: Yearbook of Agriculture, 1939, 381. 82 Indeed, by the time that the United States began mobilizing its resources in preparation for war in 1940, many of the country's agricultural and public health authorities had become proponents of the idea that increasing the consumption of the protective foods\u2014through some combination of education, reduction of production and distribution costs, and government subsidies to farmers and low-income households\u2014would go a long way to improving public health while ameliorating the oversupply problem afflicting farmers.113 The Newer Knowledge of Nutrition and the Emergence of the Concept of the \"Western Diet\" By the close of the 1930s, as we have seen, nutrition science had attained an unpreced ented level of prestige. The development of new, more sensitive diagnostic tools, along with the chemical identification and synthesis of many of the vitamins, enabled researchers to formulate quantitative requirements for these substances for the first time. By comparing these values to the data obtained in food-consumption surveys conducted in North America and Britain, researchers determined that the diets of a significant proportion of the populations in these countries were far from optimal. In response to this evidence, a growing number of medical and agricultural authorities began voicing concerns about the prevalence of vitamin and mineral undernutrition. Surprisingly, the response of many nutritionists to these developments was somewhat ambivalent. Gastroenterologist and nutrition researcher Dwight Wilbur, for example, noted in a 1939 article that \"there is too much of a tendency to discuss nutrition in terms of vitamins, essential amino acids, carotene, thiamin chloride, ascorbic acid, nicotinic acid, dicalcium phosphate, calcium gluconate, units of various vitamins in foods and units of 113 Food and Life, Part I passim; Proceedings of the National Nutrition Conference for Defense (Washington: GPO, 1942). 83 various vitamins needed daily by individuals, whereas to prevent unnecessary confusion the question of a balanced nutrition must be discussed in terms of milk, meat, potatoes, carrots, oranges, and cod liver oil.\"114 A couple years after Wilbur made these remarks, John Boyd Orr expressed a similar sentiment in the British Medical Journal. \"In planning for a perfect diet we should depend not so much on attempts to provide a sufficient amount of any specific nutrient as on ensuring that the intake of the protective foods is ample,\" he argued. \"We do not yet know all the dietary constituents which are essential for health, but we do know that a sufficiency of the protective foods will supply everything which the body needs.\"115 Robert McCarrison also warned that the rapidly increasing complexity of nutrition science was leading researchers and public health workers to lose sight of the essential principles that had guided the field since the 1910s. As he noted in a 1939 talk, \"Biochemical and other facts accumulate with such rapidity, cover so wide a field, and their mass has become so great, that for some amongst us it may be difficult to see the wood for the trees.\" McCarrison enumerated three guiding principles that he thought were particularly important: first, that nutrition, which he defined as the sum of the processes that keeps the body in health, was a fundamental function of the body; second, that food was the paramount influence in determine a person's general physical makeup, powers of endurance, and resistance to disease; and third, that a well-constituted diet, made up of fresh, natural foodstuffs, contained all things needed for normal nutrition, insofar as food was capable of supplying them. \"Certain races of my acquaintance,\" McCarrison remark ed, referring to the groups that he had observed in northern India, \"discovered the last of these three principles 114 Wilbur, \"Diseases of Metabolism and Nutrition,\" 426. 115 Sir John Boyd Orr, \"Trends in Nutrition,\" British Medical Journal 1, no. 4176 (18 January 1941): 75. 84 for themselves centuries ago, and have demonstrated its truth in their own persons, despite their ignorance either of Calories, proteins, or vitamins.\" McCarrison, for his part, was less concerned with these components than with their sources, since he had become convinced that if these sources were \"fresh natural foodstuffs of proper kinds,\" a healthy appetite would take care of the quantities\u2014both of the essentials that research ers had discovered and of those they still had not\u2014required to satisfy physiological needs.116 Elmer McCollum also became concerned that nutrition science was undergoing a dangerous conceptual drift. Although McCollum approved of what he described in a 1942 article as \"a growing tendency to make people vitamin and mineral conscious,\" he nevertheless feared that \"the placing of undue emphasis on certain of these which are available for sale, rather than on the health significance of a complete and satisfactory diet, appears to be creat ing wrong impressions in the public mind concerning the wise course to follow in selecting food.\" He reminded his readers that nutrition investigators had clearly shown over two decades earlier that many Americans and Europeans were making serious mistakes in their selection of foods, and that this was reflected in their poor standards of health. McCollum further asserted that the advice given by dieticians since the early 1920s to those without technical knowledge of nutrition\u2014to build their daily menus around an adequately large nucleus of the protective foods in order to make up for the deficiencies of the \"white bread, lean meat, potato, and sugar diet\"\u2014was just as sound in 1942 as it ever had been. \"Instead of so much emphasis on the deficiencies of this and that food, and of the daily requirements of this or that vitamin in units which are meaningless to ordinary persons, we should devote our efforts whole-heartedly to instructing every one that a well-selected 116 Sir Robert McCarrison, \"Medical Aspects of Nutrition,\" in Nutrition and the Public Health, 25. 85 diet of natural foods is wholly adequate for the maintenance of optimum health,\" McCollum contended.117 The misgivings voiced by nutritionists such as Orr, McCarri son, and McCollum were not entirely unfounded, given that nutritionists had been repeatedly emphasizing the superiority of natural foods over processed foods since the 1910s. Indeed, from the very beginning of the newer knowledge of nutrition era, investigators had known that vitamins and minerals were often destroyed or remo ved during the processing of food. Even before Funk announced his \"vitamine hypothesis\" in 1912, clinicians and medical officers had determined that the outbreak of beriberi in East Asia had been caused by the introduction of mach ine-milled polished rice in the region. At roughly the same time, researchers found that other cereal grains lost most of their vitamin and mineral content when heavily refined. This discovery was especially disturbing to nutritionists, because white flour made from mach ine-milled wheat had almost entirely replaced coarsely stone-ground flour in the United States and Britain by the end of the nineteenth century. But whereas the average per capita consumption of white flour in these countries declined as the twentieth century progressed, the consumption of another highly processed food that contained even fewer vitamins and minerals\u2014refined or white sugar\u2014increased substantially. By the 1930s, each person in Britain and the United States was consuming, on average, approximately 100 pounds of sugar per year, or about one-sixth to one-fifth of the total calorie intake.118 Moreover, the consumption of marg arine, which lacked the fat-soluble vitamins A and D contained in the more expensive butter that it was intended to substitute, grew markedly in the first half of the 117 E. V. McCollum, \"What Is the Right Diet?\" New York Times (13 September 1942), SM10. 118 Orr, Food, Health and Income, 2nd ed., 23-25; Stiebeling and Coons, \"Present-Day Diets in the 86 twentieth century, particularly in lower-income households.119 Even worse, research ers discovered that vitamins were often destroyed when the foods containing them were heated or exposed to air, as commonly occurred during cooking, processing, and storage. On the other hand, the improving knowledge of vitamins and minerals enabled some food processors to improve the nutritional quality of their products. After concentrated preparations of vitamin A and D became available, some margarine manufacturers began adding them to their products so that they contained these vitamins in quantities approximating those found in high-quality butter from pasture-fed animals. And following the discovery that vitamin C was destroyed by exposure to oxygen rather than by high temperatures, commercial canners figured out how to preserve this vitamin by removing oxygen during the canning process. Furthermore, a handful of commercial bakers, stimulated in part by the growing body of research favorable to milk, started incorporating more milk and dried milk solids into their bread formulas in the 1920s and 1930s. Despite these advancements, nutritionists advocated an increas e in the consumption of \"natural foods\" in place of \"refined foods\" throughout the interwar period. McCollum's promotion of the protective foods, as mentioned above, was rooted in his belief that the typical Western diet contained too much white flour and refined sugar, as well as excessive amounts of processed cereal foods, potatoes, and lean muscle meats. In a similar vein, J. C. Drummond noted anxiously in a 1921 article that the food supply \"is becoming more and more artificial in character, and this process must necessarily continue as long as the populations of the towns increase and the web of their interdependent lives become more 119 Johannes Hermanus van Stuijvenberg, Margarine: A Social and Economic History, 1869-1969 (Liverpool: Liverpool University Press, 1969), 159-61. 87 and more complex.\" Under these perilous circumstances, Drummond argued, the key to improving public health was providing people with \"good natural foods at cheap prices.\"120 Other nutritionists echoed McCollum and Drummond's arguments. Robert and Violet Plimmer, for instance, commented in their popularly oriented nutrition book, Food, Health, Vitamins (1935): Civilisation has made it too easy to get wrong foods of all kinds and difficult to get the foods we ought to eat. Natural foodstuffs form but a small part of the present-day diet, because they have for convenience been replaced by less perishable foods. As we walk down any street of shops we are continually being tempted by displays of groceries, sweets, and cakes. Whole shop-fronts are dressed artistically with the foods we should not eat.121 Leslie Harris likewise faulted the typical working-class diet for containing \"too little vegetables, fresh fruit, milk, butter and eggs,\" and too much food that was \"tinned, preserved, refined, dried or compressed.\" He was especially worried that growing children and expectant and nursing mothers were not getting enough dairy products, eggs, meat, and seafood, which he termed the \"body-building foods.\"122 Nutrition-minded physicians seconded these views. Robert McCarrison was one of the most outspoken critics in the medical profession of what he called the \"white-bread-marg arine-tea-sugar diet.\" This type of diet, he warned, was dangerously unbalanced. \"Civilized man,\" he commented in 1922, \"applies the principles of his civilization\u2014the elimination of the natural and the substitution of the artificial\u2014to the food he eats and the fluid he drinks. With such skill does he do so that he often converts his food into a 'dead' fuel mass, devoid of those vitamins which are to it as the magneto's spark to the fuel mixture 120 J. C. Drummond, \"Vitamins and Certain Aspects of Their Relation to Public Health,\" American Journal of Public Health 11, no. 7 (July 1921): 593-97. 121 R. H. A. Plimmer and Violet G. Plimmer, Food, Health, Vitamins, 7th ed. (London: Longmans, Green and Co., 1935), 135. 122 Harris, Vitamins in Theory and Practice, 3rd ed., 209. 88 of a petrol-driven engine.\" 123 Luther Emmett Holt, a leading pediatrician and professor of medicine at Columbia University, also expressed concern about food processing. Like many of his colleagues in the medical profession, he voiced dismay at the sudden craze for vitamins and insisted that much more research was needed to accurately determine the relationship between nutrition and health. But he nevertheless believed that there was sufficient evidence to warrant a critical view of the industrial food supply. As he remarked in his book Food, Health and Growth (1922): Many of our foods we eat are no longer in their natural state. The conditions of modern life have made necessary the transportation of foods for long distances and the preservation and storage of foods in immense quantities for long periods. A certain amount of injury is done to our vegetables, fruits, milk, meats and grains by the processes to which they are subjected in preparation, preservation and storage. Holt added, however, that advances in nutrition science were helping to elucidate the problem. \"The study of vitamines has helped us to understand, to some degree at least, the nature of the harm that has been done,\" he noted.124 Other scientists and physicians shared Holt's belief that the newer knowledge of nutrition could act as a guide for correcting the unhealthful dietary practices that had become established in the West. Robert and Violet Plimmer, for example, feared that \"Machines and commerci al processes have denatured commo n foodstuffs in such a way as to jeopardize health,\" but they were confident that, because advances in nutrition science had revealed the predicament, \"it should not be a difficult matter to rectify mistakes.\"125 John R. Murlin, a physiologist at the University of Rochester, went so far as to suggest in a 1921 speech that the government should pass an amendment to the pure food laws making it illegal to employ food-manufacturing processes that reduced the vitamin content of foods. \"We cannot afford 123 McCarrison, \"A Good Diet and a Bad One,\" 732. 124 Luther Emmett Holt, Food, Health and Growth: A Discussion of the Nutrition of Children (New York: Macmillan, 1922), 192. 125 Plimmer and Plimmer, Food, Health, Vitamins, 7th ed., 5. 89 to let whole sections of the population become victims of undernutrition or malnutrition, either through their own ignorance, or through the cupidity of food manufacturers,\" he declared.126 The notion that dietary custom had manifestly failed as a guide to sound eating habits in the West was a recurring theme in the writings and speeches of nutritionists. McCollum concisely conveyed this line of argument in the second edition of The Newer Knowledge of Nutrition (1922): \"The development of the modern industrial era in Europe, America, and the other parts of the world that have been colonized by the Europeans, has seen gradual and progressive changes in the character of the human diet, which represents an experiment that was blindly entered into, and which is now proving a grievous failure, as attested by the pronounced tendency to physical deterioration.\"127 Frederick Gowland Hopkins also harbored doubts about the value of dietary custom. \"It is often felt that concerning matters so urgent as our own nutrition, humanity, with all the experience of the ages behind it, can have little to learn from modern science, yet, as in the case of so many other established traditions, an assumption of this kind is wholly unjustified,\" he stated in a 1931 article in Nutrition Abstracts and Reviews. \"Tradition accumulates prejudices quite as often as truths, and the former are apt to be more potent in their influence.\"128 Louise Stanley, Chief of the USDA Bureau of Home Economics, took a less condemnatory, though still critical, view of dietary custom. Nutrition science, she noted in 1939, \"is not a substitute for tradition and race experience; it supplements them and corrects them where they need correcting.\"129 126 John R. Murlin, \"The Need of Further Investigation of the Effect of Commercial and Household Processes on the Vitamine Content of Foods,\" Journal of Home Economics 13 (1921): 292-94. 127 E. V. McCollum, The Newer Knowledge of Nutrition, 2nd ed., 421. 128 F. G. Hopkins, Nutrition Abstracts and Reviews 1 (1931): 4. 129 Louise Stanley, \"From Tradition to Science,\" in Food and Life: Yearbook of Agriculture, 1939, 99. 90 The ambivalent attitude of nutrition scientists and nutrition-minded physicians toward dietary custom can be seen most clearly in their assessments of the eating patterns of \"primitive\" societies. For McCollum, observation of the widely varying dietary habits of non-industrialized populations around the world corroborated the findings that nutrition researchers were making in the laboratory with animals. He was especially impressed by the health of populations that consumed large quantities of dairy products, such as the pastoral peoples in northern Africa, the Middle East, the Balkans, and central Asia. \"Wherever dairy animals are abundant in proportion to the population,\" he claimed, \"fine physical development is seen without exception.\"130 McCollum also maintained that highly carnivorous primitive peoples were remarkably healthy compared to the populations of the industrial Western countries. But he did not attribute this differen ce to the consumption of large quantities of muscle meat, which was the part of the animal that the populations of the United States and Britain consumed almost exclusively. McCollum stressed that highly carnivorous groups, such as the Eskimos, Plains Indians, and the inhabitants of Iceland and the Hebrides before their exposure to processed foods through modern trade, included the vitamin- and mineral-rich glandular organs, blood, and soft bones of animals in their diets. McCollum was arguing, in essence, that the habit of consuming only the muscle meat portions of animals was an unhealthy innovation in the modern diet, akin to the adoption of other overly fractionated foods such as white flour and sugar. McCollum also believed that observation of the mostly vegetable-based diets of southern and eastern Asian populations substantiated the laboratory findings of nutrition researchers. He considered the \"Oriental diet\" similar, for the most part, to the \"unsatisfactory\" diet commonly consumed in the industrial West\u2014that is, both consisted 130 McCollum, Newer Knowledge of Nutrition, 2nd ed., 407. 91 principally of cereal grains, legume seeds, tubers and fleshy roots, and some meat. McCollum emphasized, however, that moderate variations in this otherwise inadequate diet could produce substantial improvemen ts in wellbeing: The better diets of Eastern peoples consist of much rice, supplemented with tuber and root vegetables, and a higher quota of leafy vegetables than is commo nly eaten in Europe and America. Such diets also contain moderate amounts of fish, pork and eggs. Diets of this character are superior to the one derived largely from white flour, potatoes, peas, beans, excessive sugar and moderate amounts of lean meats, such as is commonly taken by low income families in Western industrial countries.131 McCollum also maintained that the refining the cereal grain component of the already nutritionally marg inal \"Oriental diet\" resulted in a noticeable deterioration of health and physique. In a discussion of regional dietary variations in China, he remark ed: \"The Chinese are smaller in the south than they are in the north, where wheat and millet replace rice in the diet. In southern China polished rice is the most desired cereal.\" He concluded that this regional difference in physique was probably due to the lower mineral content of refined white rice as compared with whole wheat and millet. The reason for the development of this nutritionally counterproductive dietary habit in southern China, in McCollum's view, was not altogether uncommon in human experience. \"There is but one explanation for their liking of polished rice,\" he asserted, \"and that is custom.\"132 131 E. V. McCollum and J. Ernestine Becker, Food, Nutrition and Health, 5th ed. (Baltimore, MD: The Lord Baltimore Press, 1940), 99. McCollum's view of the diet of so-called \"Oriental peoples\" was not entirely consistent over the course of his career. In the first edition of The Newer Knowledge of Nutrition (1918), for example, he wrote: \"Those people who have employed the leaf of the plant as their sole protective food are characterized by small stature, relatively short life span, high infant mortality, and by contended adherence to the employment of simple mechanical inventions of their forefathers. The peoples who have made liberal use of milk as a food, have, in contrast, attained greater size, greater longevity, and have been much more successful in rearing their young. They have been more aggressive than the non-milk using peoples, and have achieved much greater advancement in literature, science, and art\" (150-51). By the late 1930s, he appears to have abandoned these sorts of sweeping claims. 132 McCollum, Newer Knowledge of Nutrition, 2nd ed., 400-01. 92 Robert McCarrison, as discussed above, likewise had an ambivalent view of \"primitive\" dietary customs. He ascribed the excellent health and physique of certain groups in northern India to their consumption of freshly ground whole-wheat, milk and milk products, legumes, fruits and vegetables, and occasional meat, while pointing to the inhabitants of eastern and southern India, whose diet consisted mainly of polished rice or tapioca meal, by way of contrast. The findings of other medical personnel working in India largely corroborated those of McCarrison. In the mid-1930s, for example, a team of researchers led by physician Dagmar Curjel Wilson conducted a dietary survey of Hindu, Muslim, and Sikh families in northern India, of which half in each case were urban and half rural. Although the staple food of all three groups was coarsely ground and baked whole-wheat flour chapattis, the survey revealed certain differences in the diets. Hindus, except among the lower castes, did not usually include meat in their dietary. They ate a wide variety of legumes, vegetables, and fruits, and as much cow's milk and clarified butter as they could afford. Muslims ate meat, but usually not in any large amount, and they tended to use more rice and less milk and clarified butter than Hindus. Sikhs, where economically possible, combined, in Wilson's words, \"the best of Hindu and Muslim diets,\" and preferred buffalo milk with its high fat content in preference to cow's milk. She concluded that \"the diets studied and the physical condition of those consuming them are by no means unsatisfactory, and that McC arrison's experimental demonstration with rats of the excellence of the Sikh's wheat-containing diet is fully borne out.\"133 133 Dagmar Curjel Wilson, \"Nutrition and Diet in Northern India: A Comparative Dietary Survey,\" Lancet 230, no. 5964 (18 December 1937): 1445-48. 93 Contemporaneously with this work in India, researchers and medical officers in other parts of the British Empire were carrying out studies of a similar nature.134 One of the most well-known and widely cited of these studies was a comparative survey of the diet and health of two ethnic groups in Kenya, the Masai and the Kikuyu. The study, the results of which were published in 1931, originated and was supervised by John Boyd Orr, then the director of the Rowett Institute at Aberdeen, and John Gilks, the director of Medical and Sanitary Services in Kenya. As described by Orr and Gilks, the diet of the pastoral Masai consisted to a large extent of milk, meat , and blood, while that of the agrarian Kikuyu consisted mainly of cereals, tubers, plantains, legumes, and green leaves, with very little animal food. The Masai measured substantially taller, heavier, and stronger than the Kikuyu. Among the Kikuyu tested, the blood chemistry showed signs of calcium deficiency and slight iron-deficiency anemia. Rickets-like conditions, owing probably to lack of calcium, were observed in 63 percent of the Kikuyu children, but were very rare among the Masai. The investigators found that 80 percent of Masai complained about constipation and arthritis, but few suffered from the four main maladies afflicting the Kikuyu: tropical ulcers, intestinal disorders, malaria, and bronchial infection. Orr and Gilks also noted that Kikuyu women on the whole were mark edly healthier and stronger than the men. This difference appeared to be due to the fact that the women added green leaves and minerals from salt licks, special spring water, and ashes that they produced from burning swamp plants to their mainly vegetable-based diets, whereas the men did not.135 134 See, e.g., Great Britain, Nutrition in the Colonial Empire: Report of the Colonial Advisory Council, Parts I and II (London: HMSO, 1939). 135 J. B. Orr and J. L. Gilks, Studies of Nutrition: The Physique and Health of Two African Tribes, Medical Research Council, Special Reports Series No. 155 (London: HMSO, 1931). Cynthia Brantley has more recently pointed out that the Orr and Gilks study suffered from \"faulty assumptions about the nature of African diet and the impact of colonialism on various aspects of African diet.\" While Brantley provides convincing evidence that Orr and Gilks overlooked the negative impact of 94 W. E. McCulloch, a physician working at the Dietetic Research Laboratory in Katsina, Northern Nigeria, also concluded from his research that there was a close association between diet and the health of \"primitive\" ethnic groups. He collected data on the different foods eaten by the Hausa and town-dwelling Fulani in northern Nigeria. McCulloch observed that, while a large variety of foodstuffs were grown, nearly all the population of these two groups lived mainly on millet porridge, sour milk, and a soup made from the leaves of the baobab tree, which analysis showed to have a particularly high calcium content. Among the Hausa, he noted that the birthrate was low, which he attributed to infertility among the women. To test his hypothesis that this high rate of infertility was caused by nutritional deficiency, McCulloch fed laboratory rats a diet approximating that of the Hausa. Almost invariably, a reduction in fertility resulted, which he ascribed largely to a deficiency of calcium. Certain villages that were well known by the Hausa for their fertile women, McCu lloch pointed out, were those where the baobab tree grew in profusion. As a result of his research in Nigeria, McCulloch, echoing McCarrison, came to disagree with the idea that deficiencies in the diet were only observable in the form of readily diagnosable diseases such as beriberi, scurvy, and rickets. McCu lloch surmised, rather, that the majority colonialism and land dispossession on the \"tribal diet\" of the Kikuyu, her other critique\u2014that nutrition scientists in the interwar period rejected the notion that a vegetarian diet could be healthful\u2014is based on a misreading of their research. She relies on the assertion by Elmer McCollum in the third edition of The Newer Knowledge of Nutrition (1925) that \"no known combination of grains alone can support growth and reproduction\" to substantiate her claim that nutritionists erroneously believed that the diet of the sort consumed by the Kikuyu was unhealthful. Yet, McCollum's argument regarding an all-grain diet has been amply confirmed in subsequent studies. And McCollum, like many of his contemporaries, explicitly argued that a vegetarian diet could support a high state of health, provided it contain liberal quantities of milk, eggs, and leafy vegetables (see, e.g., McCollum, The Newer Knowledge of Nutrition (1918), ch. 3 passim). Nor, contrary to Brantley's claim, did nutritionists in the interwar period consider a monotonous diet per se \"risky.\" See Cynthia Brantley, \"Kikuyu-Maasai Nutrition and Colonial Science: The Orr and Gilks Study in Late 1920s Kenya Revisited,\" International Journal of African Historical Studies 30, no. 1 (1997): 49-86. 95 of nutrient deficiencies tended to result in a general lowering of vitality and resistance to infection.136 A League of Nations committee tasked with analyzing the dietaries of a \"primitive\" population closer to the industrialized West\u2014rural Europeans\u2014also presented a mixed picture. The committee members found that the modernization of rural areas in Europe, whatever its putative economic benefits, often resulted in dubious changes in dietary habits. \"It is sometimes a nutritional disadvantage when improved means of transport enable a primitive rural area to sell its produce to the towns, for then the milk, butter and eggs, which the peasant families used to consume themselves, are zealously kept for sale to the towns,\" they noted. The lack of industrial methods of food processing in the more primitive areas of Europe also appeared to afford a significant degree of protection from malnutrition: There is one special advantage of rural dietaries over urban dietaries, and that is that rural people usually eat their food in a fresh and natural state. This is particularly important with regard to cereals, which are usually eaten as highly refined white flour in the towns, while, in most parts of Europe, the rural populations still eat whole-grain cereal s. The committee members nevertheless saw much room for improvement of rural diets. They determined that the diet of the average European peasant contained excessive amounts of cereal grains and suffered from a lack of protective foods, except in the short season of plenty in late summer and autumn. But they also emphasized that there were nutritionally significant regional deviations from this norm. The consumption of milk and vegetables, for example, varied enormously from one part of Europe to another. The committee found, moreover, that these variations were not solely due to differen ces in environmental or economic conditions. Differing dietary customs played a part as well. For example, 136 W. E. McCulloch, \"An Enquiry into the Dietaries of the Hausa and Town Fulani,\" West African Medical Journal 3 (1930): 1-75; E. B. Worthington, \"On the Food and Nutrition of African Natives,\" Africa: Journal of the African Institute 9, no. 2 (April 1936): 154-55. 96 peasants in many areas of southeastern Europe considered milk an unsuitable food for adults, while peasants of all ages drank milk in the Baltic countries. Differences in dietary customs also seemed to influence variations in the prevalence of vegetable gardens that surveyors observed between rural ethnic groups.137 In addition to their high regard for the diets of certain \"primitive\" groups, nutritionists also drew on evolutionary concepts to buttress their contention that natural foods were superior to modern processed foods. The discovery of the vitamins, as biologist Beverly Kunkel argued in a 1923 article, had made it clear that in the long years of human evolution \"the body has been most accurately adapted to the food stuffs which occur in nature and which have not been purified by artificial means.\"138 Henry Sherman seconded these views. In his book Food and Health (1934), he contended that \"we are 'flying in the face of nature' and shutting our eyes to one of the plainest implications of the evolutionary point of view when we take our nourishment too largely in artificially refined forms\u2014in forms from which man has removed parts and those wholes to which we are attuned by evolution.\" Sherman called for the use of \"reasonably natural foods\" in order to meet all the nutritional requiremen ts that were known at the time, as well as \"any substances which may be essential to our nutritional well-being though still scientifically unknown to us.\"139 McCollum also put an evolutionary angle on his critique of processed foods. He pointed out that, in contrast to the inhabitants of the modern industrialized countries, Primitive man ate everything he could secure which was edible. His animal food included the flesh of such game as he could catch, and also fish, eggs, birds, shell-fish, insects, etc. Among the vegetable products he doubtless ate were fruits, 137 League of Nations Health Committee, Rural Dietaries in Europe, Series of League of Nations Publications No. 26 (Geneva: League of Nations, 1939). 5-12. 138 Beverly Kunkel, \"Calories and Vitamines,\" Scientific Monthly 17, no. 4 (October 1923): 372. 139 Henry C. Sherman, Food and Health (New York: Macmillan, 1947), 164. 97 berries, fleshy roots, nuts, and a few other seeds of those grasses which have since developed into our cereal crops.140 Traditional agriculturalists, according to McCollum, used leafy green vegetables and dairy products to compensate for the nutritional deficiencies of the grains, tubers, and legumes that made up the bulk of their diets. They also derived some benefit from consuming these \"energy foods\" in an unrefined form, which provided a modicum of B-complex vitamins and minerals. He warned, however, that Americans and Europeans had strayed dangerously far from the diet of \"primitive man\" over the course of the nineteenth and early twentieth centuries. Of the changes in the diet that had occurred during this period, he contended that \"probably the most important because so insidious, is the great extension of the consumption of cereal grains, and the changes in the process of preparing these for human food.\"141 Edward Mellanby also cautioned that the consumption of cereal grains in many parts of the world had exceeded the margin of safety. While conducting research into the etiology of rickets in the late 1910s and early 1920s, he found that oatmeal and whole-wheat flour interfered with the calcification of bones and teeth of his experimental puppies. White flour, on the other hand, appeared to interfere to a lesser extent. Mellanby later determined that the chemical substance in whole grains responsible for this harmfu l effect was phytic acid, and that it could be rendered innocuous by the inclusion into the diet of more foods containing vitamin D and calcium. He tentatively termed phytic acid a \"toxamin,\" in order to emphasize its antagonizing effect on vitamins and minerals.142 Mellanby never claimed that cereal grains had to be completely excluded from the diet, and he certainly did not 140 E. V. McCollum, Nina Simmonds, and H. T. Parsons, \"Supplementary Protein Values in Foods. IV. The Supplementary Relations of Cereal Grain With Cereal Grain; Legume Seed With Legume Seed; and Cereal Grain With Legume Seed; With Respect to Improvement in the Quality of Their Proteins,\" The Journal of Biological Chemistry 47, no. 1 (June 1921): 208. 141 McCollum, Newer Knowledge of Nutrition, 2nd ed., 416. 142 Edward Mellanby, Nutrition and Disease: The Interaction of Clinical and Experimental Work (Edinburgh and London: Oliver and Boyd, 1934), ch. 4 passim. 98 condone continuing the widespread habit of consuming white flour. His research nevertheless suggested that any innovations in the human diet\u2014even those coincident with the development of agriculture\u2014had to be approached conservatively.143 Nutritionists repeatedly emphasized that their criticism of novel foods did not imply a wholesale rejection of modern civilization. They argued, rather, that nutrition science could function as a guide\u2014one more reliable than custom or instinct\u2014to healthful dietary habits. McCo llum was clear in his rejection of the notion that the \"primitive\" lifestyle had to be embraced in its entirety to ensure good health: It is not essential that we retrogress to a state of social, educational or ethical inferiority in order to enjoy the physical fitness frequently seen in half savage peoples. Too much emphasis has been placed upon the beneficial effects from the standpoint of health, of hardship and exposure to which primitive man was of necessity subjected. Hardship and exposure never did anyone any good. Protection from the elements is conducive to health, as is also freedom from excessive exertion. The factors which have tended to reduce civilized man to a state of physical inferiority as compared with his barbarous forebears are in great meas ure due to changes in the character of his diet.144 Sherman likewise maintained that he was not advocating a \"return to nature; but only an intelligent application to the problem of food and health in one of the simplest and most fundamen tal implications of the general evolutionary and scientific points of view of today.\"145 Leslie Harris also stressed that relying on \"natural instinct\" in food selection or returning to a \"natural life\" did not guarantee freedom from nutritional deficiencies. He pointed out that babies in countries located in temperate climates, such as Britain, often 143 As Mellanby commented in 1935, \"Cereals are undesirable, though not entirely so. However, when diets are made up of 60 per cent. of cereals, as is often the case with poor people, this is an undesirable amount. Potatoes should be more largely eaten than cereals; they contain vitamin C even when cooked. They are a valuable source of iron, they have good protein content, they are cheap, and have other advantages.\" See idem, \"Four Lectures on A Survey of Modern Views of Nutrition,\" 19. 144 McCollum, Newer Knowledge of Nutrition, 2nd ed., 435. 145 Henry C. Sherman, Food and Health (New York: Macmillan, 1934), 159-60. 99 developed some degree of rickets unless given artificial antirachitic treatment in the form of cod liver oil or ultraviolet radiation. Harris also noted that scurvy in adults and infants was not eradicated in Britain, where little of the fresh salad vegetables and fruits were available the winter months, until the \"artificialities\" of modern transport enabled imports of these essential foods in large quantities.146 Nutritionists were arguing, in short, that Westerners could take advantage of the greater abundance and variety of foods that modern mean s of production and transportation made available to them, while still avoiding the unhealthful dietary habits that had become established in the decades before the discovery of vitamins and minerals. For many nutrition researchers, one of the clearest indications that the populations of the industrial Western countries had deviated from the diets for which they were evolutionarily suited was the much higher rates of dental decay, gum disease, and misaligned teeth that they suffered relative to \"primitive\" peoples. In fact, as the next chapter will show, the dental profession became deeply divided during the interwar period over the question of whether nutrition had anything to do with these conditions. 146 Harris, Vitamins in Theory and Practice, 3rd ed., 196-97. 100 CHAPTER TWO \"THE MOUTH, BAROMETER OF HEALTH\" : THE RISE AND DECLINE OF NUTRITIONAL DENTISTRY On March 27, 1934, 1,500 doctors, chemists, dentists, and nutrition experts packed into the grand ballroom of the Hotel Pennsylvania in New York City. They were there, of all things, to hear a public debate between two differing schools of thought concerning the cause of tooth decay. On one side of the podium were Dr. Thaddeus P. Hyatt, director of the dental division of the Metropolitan Life Insurance Company and Clinical Professor of Preventative Dentistry at New York University; Dr. Alfred Walker, associate professor of the New York University College of Dentistry; and Dr. Maurice William, former chairman of the Oral Hygiene Committee of Greater New York. This august trio argued for the \"older, conservative theory of mouth hygiene:\" clean teeth do not decay, so the surest protection against dental troubles requires keeping tooth surfaces as free as possible from destructive bacteria and the food debris that feeds them. On the opposing side were Elmer V. McCollum, Professor of Chemical Hygiene at Johns Hopkins University who, as we saw in the previous chapter, was one of the leading nutrition scientists of the day; Dr. Arthur H. Merritt, winner of the prestigious 1932 Fauchard Medal; and Dr. Weston A. Price, director of the Dental Research Laboratories in Cleveland, Ohio. This group championed the new \"nutritional dentistry\": a proper diet, composed of plenty of milk, green vegetables, fruits, meat, eggs, and cod liver oil was the best method for maintaining sound, healthy teeth. Even though McCollum approved of mouth hygiene on general principle for the same reason that he approved of body 101 cleanliness, he insisted that \"any effective program of preventive dentistry must be based upon a dietary reformation by the nation.\" Any diet of a type optimal for the calcification of bone and for the optimal development of tooth structure, McCollum stated, would be highly successful in reducing susceptibility to dental caries. A proper diet also influenced salivary chemistry, he argued, creating an oral environment hostile to tooth decay. Both Price and Merritt concurred in their presentations, contending that brushing teeth and other sanitary meas ures were not \"Nature's method\" for healthy teeth. This trio was arguing, in other words, that good dentistry begins inside the body, since the teeth and salivary glands are connected via the bloodstream to the digestive system, and the food that goes into it.1 The New York Times reporter covering the story informed readers on the following day that \"several thousand persons were turned away owing to lack of room.\" 2 And this episode in 1934 was just one instance of a wider scientific debate that was raging in the interwar period. \"The dental profession,\" as nutrition research er Russell Wilder wryly noted in 1941, \"has been creditably active in nutrition, sometimes with greater zeal than wisdom.\" 3 Clearly, dentistry at that time was garnering a degree of interest and controversy that would seem remark able today. Most of us today view dentistry as a necessary evil, a modern advance with many benefits, chief among them keeping our teeth in good order despite the defect s of our genetic inheritance and oral hygiene regimen. Dentistry has unfortunately come to be associated with such agonizing procedures as fillings, crowns, bridges, braces, root canals, and wisdom tooth extractions. We view dentists with varying degrees of respect or dread depending on the results of the checkup, but normally we don't flock to 1 For a report of the debate, see \"Resolved: 'That a Clean Tooth Does Not Decay and that Mouth Cleanliness Affords the Best Known Protection Against Dental Caries',\" Dental Cosmos 76 (August 1934): 860-87. 2 \"Scientists Clash Over Dental Ills,\" The New York Times (28 March 1934), 8. 3 Russell M. Wilder, \"Mobilizing for Better Nutrition,\" in Proceedings of the National Nutrition Conference for Defense, 15. 102 amphitheaters to hear their latest theories. The most dietary advice we are likely to hear from dental authorities is the admonition to avoid too many sticky sweets and soft drinks. And we certainly don't hear dentists likening a thorough tooth cleansing in their offices to a skin-rejuvenating visit to the spa. It is therefo re reasonable to assume that the \"nutritional dentistry\" perspective failed to hold up to scientific scrutiny and has been discarded and forgotten for good reasons. However, much of the science indicating that decay, gum disease, and even orthodontic problems are linked to what we eat has never been disproved. The nutritional discoveries of the first half of the twentieth century, discussed in the previous chapter, simply failed to make a lasting mark on modern dentistry, and researchers in other scientific fields for the most part failed to notice. Teeth Under the Microscope: The Beginnings of Dental Science Discussions like the one held in 1934 at the Hotel Pennsylvania attracted so much attention because the rise in dental degeneration was such an obvious and alarming trend in all the industrial Western nations. In the midst of the notable advances in public sanitation, surgery, medicine, and food safety, it seemed incongruous and disturbing that the most \"advanced\" countries in the world had the worst teeth. Although dentists had been aware of this trend for decades, by the interwar period conclusive statistical evidence had been compiled showing worsening rates. Dentists and public health workers, quite naturally, were eager for explanations for why this was happening and how to reverse it. The theory that \"a clean tooth does not decay\" gained a wide following in the late nineteenth century, during the heyday of the bacteriological revolution. In the 1880s and 1890s, Willoughby Dayton Miller developed the first theory of tooth decay to be founded on 103 experimental investigation. Miller studied mathematics and physics at the University of Michigan, Ann Arbor, from 1871 to 1875, before traveling to Europe for additional studies. He became interested in dentistry after befriending an American dentist, Frank Abbot, who was living and working in Berlin. Miller worked for Abbot and married his daughter, Caro line. With Abbot's support, Miller graduated from the Philadelphia Dental College in 1879 as a Doctor of Dental Surgery. He then returned to Berlin to work for his father-in-law while studying natural and medical sciences, especially microbiology, which at that point was in the process of remarkable advance under the influence of Robert Koch. After performing extensive studies of the oral microflora and its relationship to dental caries in his own small laboratory, Miller developed his \"chemico-parasitic theory\" of dental decay. He set this theory out in detail in his classic Die Mikroorganismen der Mundh\u00f6hle (1889), which was translated into English a year later as The Micro-Organisms of the Human Mouth (1890). (The chemico-parasitic theory was also called the \"chemico-bacteriological,\" \"bacterio-chemical,\" \"acidogenic,\" and \"oral environment\" theory.) Although Miller did not invent the theory, he put it on a scientific basis. His investigations indicated that fermentation of carbohydrates by oral microorganisms resulted in acid formation. The progress of carious lesions, Miller argued, consisted of the decalcification of the hard tooth enamel by these acids, followed by the destruction of the remaining organic residue and underlying tissues by other microorganisms.4 Although Miller recognized that microorganisms formed a \"film\" on the teeth, he assumed that fermentation of impacted carbohydrate foodstuffs, particularly starches, occurred in situ by salivary organisms. This view corresponded well with the fact that 4 Klaus G. K\u00f6nig, \"W. D. Miller and His Contributions to Dental Science,\" introductory essay in 1973 ed. (Basel, New York: S. Karger) of W. D. Miller, Micro-Organisms of the Human Mouth (Philadelphia: S. S. White Publishing Co., 1890). 104 cavities most commonly form in the pits and fissures of the teeth (occlusal caries), or on the tooth surfaces juxtaposing the narrow gaps between them, where food debris can easily lodge (proximal caries). It was not until G. V. Black published his findings in 1898 that \"dental plaque\" and its implications were realized. Both Black and a contemporary researcher, J. Leon Williams, described the \"gelatinous microbic plaques\" found on teeth, and both believed that caries was due wholly to attack from acids produced by bacteria in these plaques. It was the eventual merger of Miller's chemico-parasitic theory and Black and Williams's plaque concept that provided the basis for what is still the central paradigm of oral biology regarding the etiology of dental caries.5 The chemico-parasitic theory was not the only one being bandied about at the time. Reflecting the preoccupations of the era, dental practitioners proposed other causative mech anisms for tooth decay, such as heredity, race-mixing, excessive protein consumption, dyspepsia, autointoxication, and lack of exercise of the jaw and masticatory apparatus due to the refinement and softness of \"civilized\" foods. As late as 1919, one dentist even speculated that dental caries was an epidemic introduced into Europe by \"Asiatics\" in the same way as cholera and the bubonic plague had been earlier.6 But in time, the chemico-parasitic theory, with its strong scientific grounding and elegant chain of causation, gained the loyalty of most dental researchers. The theory gave a sense of respectability to the dental profession in an era that increasingly recognized 5 Richard P. Suddick and Norman O. Harris, \"Historical Perspectives of Oral Biology: A Series,\" Critical Reviews in Oral Biology and Medicine 1, no. 2 (1990): 145-48. 6 Alyssa Picard, Making the American Mouth: Dentists and Public Health in the Twentieth Century (New Brunswick, NJ: Rutgers University Press, 2009), ch. 2 passim. For more on the \"racial mixture\" hypothesis, circulated for a time among physical anthropologists, see Ales Hrdlicka, \"Human Dentition and Teeth from an Evolutionary and Racial Standpoint,\" Dominion Dental Journal 23 (1911): 403. The \"epidemic\" hypothesis of Mihaly Lenhossek is described in Sidney Finn, \"Prevalence of Dental Caries,\" in A Survey of the Literature of Dental Caries (Washington: NAS, 1952), 128. 105 laboratory research as an agent of progress\u2014and the germ theory as one of its most valuable products. The overwhelming acceptance of the chemico-parasitic theory was just one aspect of the professionalization of dentistry in the late nineteenth and early twentieth centuries. The period saw dentistry in North America and Europe undergoing many of the hallmark developments as other emerging professions: the growth of self-policing associations and journals; the establishment of legislation and licensing standards; the stamping out of heterodoxy; increas ing funding of institutions for training and research; the refining of technologies and techniques; and the growth of subsidiary industries.7 Investigators following Miller, Black, and Williams added complexity and detail to the theory, but they did not fundamentally challenge it. As dental researchers obtained a more sophisticated understanding of different species of acid-producing bacteria in the oral environment, some disagreement arose as to which specific organism or group of organisms were responsible for dental decay. Different strains of bacillus acidophilus were initially favored as the main culprits, and most of the interest in the bacteriology of dental caries in the early decades of the twentieth century centered on identifying these lactobacilli. But some dental researchers observed phenomena that did not fit the view that caries was initiated by decalcification of enamel. It was readily demonstrated experimentally that acid alone, in whatever concentration, acting upon mature enamel failed to produce the type of \"true caries\" actually seen in people's mouths. Furthermo re, as Theodore Beust 7 Eric G. Forbes, \"The Professionalization of Dentistry in the United Kingdom,\" Medical History 29 (1985): 169-181; Richard P. Suddick and Norman O. Harris, \"Historical Perspectives of Oral Biology: A Series,\" Critical Reviews in Oral Biology and Medicine 1, No. 2 (1990): 135-151; Stanley Gelbier, \"125 Years of Development in Dentistry, 1880-2005,\" British Dental Journal 199, Nos. 6-12 (2005); and James L. Gutmann, \"The Evolution of America's Scientific Advancements in Dentistry in the Past 150 Years,\" Journal of the American Dental Association 140 (September 2009): 8S-15S. For similar trends in American medicine in general, see Paul Starr, The Social Transformation of American Medicine: The Rise of a Sovereign Profession and the Making of a Vast Industry (New York: Basic Books, 1982). 106 demonstrated in 1925, when a tooth with incipient caries was immersed in acid, the sound enamel was decalcified while the decayed part remained. He concluded that something else must be necessary for caries to progress besides acids from acidogenic bacteria\u2014possibly the direct action of enzymes. Beust's findings accorded with those of Bern hard Gottlieb, who was one of the first researchers to suggest that the carious process of the enamel begins as a degradation of its organic structures by proteolysis, with acid only secondarily involved.8 Beust and Gottlieb were early contributors to what became known as the \"proteolytic theory,\" which emerged slightly later than the \"acidogenic theory.\" 9 According to the proponents of the proteolytic theory, tooth decay was typically initiated not by acid-producing bacteria acting on the inorganic mineral portion of the enamel, but by protein-degrading microorganisms whose enzymes attack the organic cementing material (the enamel lamellae or prism sheaths). The proteolytic theory was also thought to explain a commonly observed cavitation process, wherein decay progresses down a narrow channel in the enamel before bacteria undermine much of the underlying dentin. Despite its crucial role in tooth formation, this \"organic matrix\" composes only about one percen t of the total mass of the enamel. Dental researchers well into the twentieth century widely believed that a fully erupted tooth was essentially an inert mineral mass, but this view became less tenable as methods and technologies improved in histology, chemistry, and biology. In any case, by the 1940s, it was commonly accepted that different forms of mouth organisms, including 8 B. Gottlieb, M. Diamond, and E. Applebaum, \"The Caries Problem,\" American Journal of Orthodontics and Oral Surgery 32, No. 6 (June 1946): A365-A379. 9 For a comprehensive survey of the early literature contributing to the proteolytic theory from one its major proponents, see Charles F. B\u00f6decker, \"Pathology of Dental Caries,\" in A Survey of the Literature of Dental Caries, 175-241. 107 lactobacilli, streptococci, yeasts, and fungi, probably act together symbiotically to form plaques and advance tooth decay.10 But it is notable that in the oral environment framework, there is little the body as a living organism can do to combat the carious process. The influence of diet on dental decay was seen as purely negative: once food is eaten, the tooth becomes a stationary target for bacterial adhesion and chemical attack. According to the conventional wisdom, the best one could do was assiduously brush and clean, avoid sticky sugars and starches, and eat of plenty of the so-called \"detergent foods,\" such as raw fibrous fruits and vegetables, to help keep the teeth free of food particles. But once caries started, the oral environment theory offered no solace to the victim, except to get into the dentist's chair as promptly as possible for cleaning and reparative work. And despite the fact that more and more people were getting into the dentist's chair and brushing their teeth as the twentieth century progressed, it was widely acknowledged that dental decay was not abating in the industrial Western countries. Dentists were getting better at conserving already decayed teeth, but there seemed to be factors causing the decay that were not being addressed on a broad scale. The principal preventive treatment conceived to protect the teeth and gums against destruction was the promotion of oral cleanliness, even though there was little evidence that it was beneficial. It was in this atmosphere of relative helplessness that the White House Conference on Child Health and Protection in 1930 concluded regretfully that no method of treatment had up to that point 10 See, e.g., Gies, L. S. Fosdick and H. L. Hansen, \"Theoretical Considerations of Carbohydrate Degradation in Relation to Dental Caries,\" Journal of the American Dental Association 23 (March 1936): 401; and J. L. T. Appleton, \"The Problems of Oral Microbiology as Problems in Ecology,\" in George M. Anderson ed., Proceedings, Dental Centenary Celebrations (Baltimore: Maryland State Dental Association, 1940). 108 been devised that could prevent the ravages of dental caries.11 Thus, the oral environment theory, though capable of explaining the process of dental caries, left dentists and public health workers looking for other, more promising approaches to keeping it from happening in the first place. Furthermore, the oral environment theory was dogged by findings that left it vulnerable to attack. On its own, the theory did not account for the fact that the caries process would suddenly halt in some individuals, or that the dentin layer of the teeth would commonly show signs of hardening or sclerosis (\"ebonization\") in response to decay. A very small percentage of fortunate individuals seemed to be completely immu ne to caries, despite consuming a supposedly highly cariogenic diet. Dentists had also long observed that the teeth appeared to be responsive to metabolic changes that affected the whole body. The times of greatest dental caries activity seemed to occur during childhood, adolescence, pregnancy and lactation, and episodes of disease such as tuberculosis. Nutritional requirements for normal development and health were found to be similarly very high during the first months of rapid growth in infancy, high throughout the early years of growth, increas ing to a maximum during adolescence and maturation, then decreasing in adult life to a lower level which is relatively constant, except at times of markedly increased need such as reproduction and disease. In other words, the development of dental caries did not appear to be a constant throughout the human life span, nor did it appear to be in direct progressive relation to the length of time that the teeth have been exposed to the destructive byproducts of bacteria, as would be logically deduced from the oral environment theory. Although the association between tooth decay and metabolic demand was based mainly on educated 11 White House Conference on Child Health and Protection (Washington, DC, 1930). 109 speculation into the 1930s, by the end of that decade careful statistical compilations demonstrated that it held true.12 Aside from these suggestive findings, advances in the rapidly growing field of biochemistry began forcing a revision in conventional views of tooth formation. As we saw earlier, the rising star of biochemistry in the first half of the twentieth century owed much to the vitamin and mineral discoveries. It is therefore not surprising that some of the scientists who were at the leading edge of the \"newer knowledge of nutrition,\" such as McCo llum, played a significant part in this revision in dentistry. Between the 1910s and 1940s, researchers equipped with the new biochemical techniques found that the tooth, though a unique organ in certain respects, suffered from metabolic derangements caused by vitamin and mineral deficiencies in much the same way as the rest of the body. To better understand how the work on vitamins and minerals came to challenge the predominant views on dental degeneration, it is worth briefly exploring the biochemistry of tooth development and structure, insofar as it was known to scientists at that time.13 The tooth differs from all other bodily structures in its anatomical location, which exposes it to the influence of a number of unique external factors, and in its predominantly mineral nature, which prevents it from responding immediately to stimuli in the same way as the soft tissues. This is seen in the fact that, rather than reforming in response to growth like other tissues in the body, permanent teeth entirely replace deciduous teeth. The tooth's lack 12 J. W. Knutson, H. Klein, and C. E. Palmer, \"Studies on Dental Caries: VIII. Relative Incidence of Caries in Different Permanent Teeth,\" Journal of the American Dental Association 25 (1938): 1923-34; and F. Hollander and J. M. Dunning, \"A Study by Age and Sex of the Incidence of Dental Caries in Over 12,000 Persons,\" Journal of Dental Research 18, no. 1 (1939): 43-60. The relationship between pregnancy and dental caries was, however, much debated. 13 Cf. J. C. Forbes, \"Dental Caries from a Biochemical Standpoint,\" Journal of Dental Research 11, no. 4 (1931): 591-98; Wallace D. Armstrong, \"Biochemical and Nutritional Studies in Relation to the Teeth,\" Annual Review of Biochemistry 11 (1942): 441-64; and Henry Leicester, Review of Biochemistry 15 (1946): 361-74. 110 of responsiveness is the tradeoff for a surface hard enough\u2014indeed, the hardest in the human body\u2014to tear and pulverize food for an entire lifetime. Since the final form of the erupted tooth is so unresponsive, dental investigators were intrigued by how this form is reach ed and whether the unresponsiveness is always charact eristic during its developmental period. Actual formation of the tooth as a distinct organ may be said to begin when the embryonic tooth reaches what dental histologists term the \"bell stage.\" At this time, the future tooth crown is outlined by two lines of cells in contact with each other. The outer line of cells, called ameloblasts, is of epithelial origin and will form enamel (epithelial refer ring to the outside layer of cells that covers all the free, open surfaces of the body that communicate with the outside of the body). The inner line, of mesenchymal origin, will lay down dentin and form pulp (mesenchymal referring to cells that develop into connective tissue, blood vessels, and lymphatic tissue). Dentin lies between the enamel and the pulp chamber, which contains blood vessels and nerve trunks. The line of contact between enamel and dentin becomes the \"dentino-enamel junction.\" There is a subtle relation between the ameloblasts and odonotoblasts, for each must begin to develop normally if the other is to do the same. Ameloblasts and odontoblasts develop simultaneously in opposite directions, so that enamel and dentin develop at roughly the same time, in each case away from the dentino-enamel junction. There are two stages to enamel formation. First, the ameloblast begins to function by laying down an organic matrix, composed mostly of keratinous protein, as well as some mineral salts. 14 The soft and acid-insoluble material of this organic matrix grows 14 The proponents of the \"proteolytic theory\" of dental decay, mentioned above, were among the first to elucidate the role of the \"organic matrix\" in the process of tooth formation. 111 outward from the dentino-enamel junction and downward from the cusp. It is composed of small fibers, and these fibers, as they form in the ameloblast, are oriented perpendicular to the dentino-enamel junction. As the ameloblast grows outward, the protein fibers lengthen and form an intermeshing network with small, fluid-filled spaces between them. After this protein network has grown out for a short distance, precipitation of the hard mineral part of the enamel begins in the fluid between the meshes. This is the second stage of enamel formation, usually referred to as calcification, mineralization, or maturation. It is during this stage that calcium hydroxyapatite, a phosphate mineral, is laid down in the organic matrix that has already formed. There is not only an influx of calcium and phosphorus at this stage, but water and organic matter from the matrix is removed. Researchers knew by the 1940s that at least phosphatase and proteolytic enzymes are necessary for mineralization to take place, though many of the details of the process remained obscure.15 The complete growth and deposition of protein and mineral matter by the ameloblast results in the formation of enamel prisms or rods, composed of tightly packed and highly organized apatite crystals. When an ameloblast's function has ended, the cell atrophies. Fully formed tooth enamel is therefore inert in the cellular biological sense. Nevertheless, enamel is always in close chemical contact with internal hard tissue fluids and external salivary excretions. In other words, fully formed enamel, though less sensitive to substances from within and without than it is during development, is certainly not an impermeab le structure. Studies carried out in the 1920s and 1930s showed that enamel 15 For the discovery of phosphatases in calcification, see R. Robinson, \"Possible Significance in Biochemical Journal 286-93. Cf. P. Weinmann, George D. Wessinger, and Gerald Reed, \"Correlation of Chemical and Histological Investigations on Developing Enamel,\" Journal of Dental Research 21, no. 2 (1942): 171-82. 112 continues to slowly harden after the tooth has erupted, even though it was unclear whether the organic or inorganic portions, or both, were involved.16 And by the 1940s, investigators using dyes and isotopic tracers produced conclusive evidence that the enamel is permeable to substances from outside and within the tooth.17 These discoveries opened the possibility that nutrition could continue to influence the fate of the tooth after eruption, either through saliva or the bloodstream. In the case of dentin, the mech anism of formation is similar to that of enamel. The protein of the matrix is different from that of enamel, since dentin, like connective tissue generally, is mostly collagenous and fibrous in character. The odontoblasts lay down this dentin matrix, which then calcifies in a manner similar to the enamel matrix.18 The orientation of the fibers and crystals is different than that of enamel, but the general principle of a matrix that calcifies progressively is the same. Unlike ameloblasts, the odontoblasts do not atrophy when they have laid down the full complement of dentin. They merely go into a resting state, and can later be reactivated to lay down secondary or sclerotic dentin. Such dentin does not repair imperfectly formed primary dentin, but it fills the dentin tubules or grows out into the pulp chamber in response to external stimuli. Like enamel, dentin was found to typically harden with age. 16 See, e.g., S. Karlstr\u00f6m, Physical, Physiological and Pathological Studies of Dental Enamel with Special Reference to the Question of Its Vitality, Supplement N: R 1 (Stockholm: Svensk Tandlakare-Tidskrift, 1931); Frank F. of Solubility of Various Samples of Dental Enamel,\" Journal of Dental Research 14 (1934): 21-28; and Edmund Applebaum, \"Grenz Ray Studies of the Calcification of the Enamel,\" Journal of Dental Research 17 (1938): 17 B. Nygaard Ostby, \"The Problem of Human Dental Enamel,\" Acta Odontologica Scandinavica 3, no. 2 McCauley, \"Significance of Radioactive Isotopes in Dental Research,\" Journal of the American Dental Association 22 (1935): 2089; and G. C. Hevesy, \"Applications of Isotopes in Biology,\" Journal of the Chemical Society, Part 2 (1939): 1213-23. 18 Some studies suggested that the dentin matrix is derived, not exclusively from odontoblasts, but from other cells of the pulp as well, either from fibroblasts or amorphous intercellular substances. See Balint Orban, \"The Development of the Dentin,\" Journal of the American Dental Association 16 (1929): 1547-86. 113 It should be stressed that tooth formation is a sequential process. Protein is laid down ahead of the calcified area, and the latter increases steadily in density as more calcium phosphate is precipitated. Unlike bone, which has some capacity for resorbtion and reformation in response to changed nutritional conditions, with teeth there is no going back for repair or improvement. Teeth are morphologically fixed, except for the terminal resorbtion of deciduous teeth and the minor apposition of dentin and cementum in response to wear and tear, which is a one-way process in any case. If a normal tooth is to be formed, all conditions for normal growth and mineralization must be operative throughout the developmental period. Thus, during a person's formative years, extending roughly from the antenatal period to adolescence, the teeth are sensitive to nutritional disturbances. One of the first investigators to bring this fact into the scientific mainstream in the 1920s and 1930s\u2014and in the process redefine the boundaries of proper dental research\u2014was British biochemist May Mellanby. \"How to Grow Good Teeth\": May Mellanby's Vitamin Investigations The route May Mellanby took to becoming a dental research er was unconventional in several respects. The fact that she was a woman and a scientist was, of course, unusual in that period. Later in life, she recalled the time when, as a graduate student at Cambridge University, she walked into the physiology laboratory and her future husband, Edward Mellanby, looked up and exclaimed, \"I do not approve of women working here!\"19 May's father, George Tweedy, prospered as a Liverpool shipper, which allowed for a comfortable upbringing for her and her sister. But it was a highly unconventional one. 19 But, she further commented, \"I later paid him out for that comment!\" May Mellanby, Speech at Cambridge University, January 19, 1938, PP/MEL/L2/36, Wellcome Library Archives and Manuscripts, London, UK. 114 After George took part in a debate in Liverpool with the famous Darwinian, Thomas Huxley, on the higher education of women, he decided that if ever he had a daughter, she would be educated as a boy, and if he could afford it, that she should go to Cambridge. In the 1880s and 1890s, May's parents shocked friends and family by permitting her and her sister to bicycle around in bloomers , ride ponies bareback, cut their hair short, and forego corsets. They also never restricted what their daughters could read, as was often the case at the time, and they encouraged them to join in family discussions on any topic. May traveled widely with her family in her school days, visiting much of Europe.20 Because of her upbringing, May got into a good deal of trouble at Girton College, the women's college she attended as an undergraduate, by breaking a number of written and unwritten rules.21 Her path into the dental world was also unconventional, in that she never trained to become a Doctor of Dental Surgery. Her graduate training was in physiology and biochemistry at Cambridge, where she worked with Frederick Gowland Hopkins in the early 1900s on his groundbreaking tryptophan investigations. Her interest in the role of nutrition in dental health arose as an offshoot of her husband Edward's work on rickets in the 1910s. Around 1917, May noticed that the teeth of the puppies in his rickets experiments varied widely in character. There seemed to be an intimate relationship between the structure of the developing teeth and the calcification of the animals' bones. The teeth of rickety puppies were irregularly set in malformed jaws, and were often rough, soft, and pigmented, while the teeth of those puppies completely free from rickets were white, shiny, and perfectly aligned. Concurrently with her husband's investigations on the role of the \"antirachitic vitamin\"\u2014vitamin D\u2014in bone calcification and the prevention of rickets, May developed a diet on 20 Undated talk, PP/MEL/L2/65, Wellcome Library Archives and Manuscripts, London, UK. 21 May Mellanby, Speech at Girton Roll Weekend Dinner, July 26, 1958, PP/MEL/L2/52, Wellcome Library Archives and Manuscripts, London, UK. 115 which puppies grew well while their teeth and orthodontic structure were of poor quality. By making small, and apparently insignificant, additions or alterations to this diet, Mellanby found, the structure of the teeth and jaws could be completely altered. Mellanby did not consider dental disease per se in her early experiments, but rather how to produce normal and abnormal dental structures. She began considering the pathology of teeth and dental tissues only as a mean s, as she put it, \"to unravel the mysteries of the normal or perfect.\" As part of her initial research, she consulted with the leading figures in British dentistry, such as John Howard Mummery , and immers ed herself in the dental literature. One of the most consistent themes in this literature, Mellanby found, was that the high incidence of dental caries and malocclusion in modern Britain was not typical for the human record as a whole. This fact led Mellanby to wonder \"why certain primitive races in their natural surroundings had such white, shining, non-carious teeth, well arranged in fine jaws, and why, when these same people were brought into contact with civilization their teeth deteriorated.\" She unsurprisingly drew the connection between these changes in human dentition accompanying the \"advance of civilization\" and her observations of rachitic puppies. 22 The basic lineaments of the \"nutritional dentistry\" perspective that Mellanby would defend for the rest of her career can be seen in her earliest dog studies. In an experiment published in 1918, Mellanby found that puppies fed on animal fats such as cod-liver oil, butter, and suet had the best teeth, whereas those fed on vegetable fats such as flax oil had the worst. Better-formed teeth, in turn, provided a more resistant medium against caries. \"If the enamel on all parts of the crowns of the teeth is abundant and sound, and if the teeth are 22 May Mellanby, Speech at Cambridge University, January 19, 1938, PP/MEL/L2/36, Wellcome Library Archives and Manuscripts, London, UK. 116 adequately spaced,\" she argued, \"then such teeth are less likely to be attacked by caries and other disease.\" However, in her early experiments Mellanby mistakenly ascribed this differen ce in tooth structure to \"fat-soluble A.\" Soon after she conducted her first dog studies, researchers succeeded in identifying fat-soluble vitamin A and vitamin D as separate substances. Edward Mellanby's experiments, in which he demonstrated that dogs could grow well and yet have badly calcified bones, contributed substantially to this discovery, though he hesitated for some years to differentiate between the two vitamins. Although May Mellanby also eventually drew the distinction between vitamins A and D in her research, her view of what kind of dietary improvements were needed for better dentition changed little over the years, since the foods that contain vitamin A tend to contain vitamin D as well. She concluded that the British diet of her day, particularly that of the poorer classes, was far too high in cereals and vegetable fats like margarine and too low in animal fats and milk products to ensure adequately formed dentition. A perusal of the list of the substances containing the fat-soluble vitamins, she argued in her 1918 study, \"makes it clear that civilised conditions, and more particularly those conditions met with in urban life, exaggerate the part played in the dietary by just those substances which are deficient in this type of accessory factor.\" To reinforce her argument, Mellanby pointed to the excellent teeth of \"the Esquimaux in his own country, where flesh and blubber are the staple articles of the diet\" and the much less sound teeth of \"the inhabitants of Chili, who live chiefly on cereals.\" She also felt that, \"with the development of civilization,\" infants were all too often not breast-fed for sufficiently long periods of time, if at all, which had deleterious effects on tooth development and health in general.23 23 May Mellanby, \"An Experimental Study on the Influence of Diet on Teeth Formation,\" Lancet 192, no. 4971 (7 December 1918): 767-70. 117 In subsequent animal studies in the1920s, Mellanby and her coworkers in Sheffield repeated and extended her initial animal experiments. She found that when pregnant and nursing animals under laboratory conditions were fed on diets deficient in vitamin D, the eruption of the teeth of the offspring was delayed, and enamel and dentin were often defect ively formed. This indicated to Mellanby that the mother had been unable to supply a sufficiency of factors necessary to ensure perfect calcification from their own bodies. But she observed that these defects of the deciduous teeth were less marked than those found in the permanent teeth when the puppies themselves were fed on the same deficient diet. This strongly suggested to Mellanby that the mother sacrifices her own stores of essential substances in rearing her young, and may in the process subject herself to harm if the dietetic conditions are severe. She conducted further vitamin D studies on other species of laboratory animals, the rabbit and the rat, in which the importance of vitamin D during the formation of the teeth was again demonstrated.24 Mellanby also investigated the effects of diet on the response of the tooth to harmful stimuli such as caries or attrition. She produced attrition in the teeth of dogs by rubbing them with a file at intervals of two or three days and observed what happened under differing conditions. She found that the quantity and quality of the dentin produced by the tooth to defend itself against this kind of abrasion varied with the diet. Namely, the teeth of an animal on a diet containing an adequate quantity of vitamin D responded by producing abundant and well-formed secondary dentin. Conversely, a diet containing little vitamin D resulted in the production of either no secondary dentin, or of defective dentin containing \"interglobular spaces\"\u2014that is, small irregular spaces on the outer layer of the dentin. In 24 May Mellanby, Diet and the Teeth: An Experimental Study, Medical Research Council Special Reports, Series No. 140 (London: HMSO, 1930). 118 Mellanby's view, these findings provided solid evidence that the tooth was responsive to nutritional influences even after eruption, and that diet could play a major role in halting the progression of caries. Whether vitamin D heightened resistance to caries through the bloodstream or by changing the nature of the saliva, or both, was not clear. One dietary factor that appeared to mark edly influence the production of normal or abnormal teeth in laboratory animals was the quantity of cereals, such as oats, wheat, and maize, that were included in the diet of her experimental animals. Edward Mellanby, as mentioned in the previous chapter, published several papers in the 1920s concerning the exaggerating effect of cereals on the production of rickets in young dogs and rats. He hypothesized that cereals might contain some \"anti-calcifying substance,\" which he tentatively called a \"toxamin.\" (Subsequent researchers discovered that this anti-calcifying substance, termed phytic acid in 1939, could be partially destroyed by the use of dilute acids in the preparation of grains.) Similarly, May Mellanby found in her animal studies that the greater the quantity of cereals eaten, other factors being equal, the greater was the tendency to produce badly calcified teeth. Animals fed on diets containing differing amounts of cereals would grow and put on weight at the same rate, but the quality and structure of their dentition varied. Surprisingly for Mellanby, cereals that had the worst effect on teeth, such as oatmeal, contained the most calcium and phosphorus. She discovered that the addition of a calcium carbonate supplemen t to a diet containing cereals improved tooth formation to some extent, but the addition of vitamin D without the calcium carbonate completely antagonized any of the cereal's anti-calcifying effect. These findings suggested to Mellanby that the intake of vitamin D and the minerals required for bone and tooth formation were not independent variables. Vitamin D appeared to increase the efficiency with which minerals were utilized by the experimental animals. This explained to Mellanby why \"primitive\" 119 groups that had what appeared by conventional standards to be rather low intakes of calcium nevertheless had well-built teeth and jaws. As her frequent comparisons between \"primitive\" and \"civilized\" diets suggest, Mellanby began to think of her work from an evolutionary perspective. \"Since paleolithic times,\" she stated in a 1928 paper, echoing Elmer McCollum, \"one of the greatest changes affecting human life has been the introduction of cereals into the diet of man and the gradual increas e in their consumption.\" With the increase in cereal consumption, she continued, \"there has been a corresponding diminution in the eating of animal products, including meat, fish, milk and eggs.\" In places such as India and the Hebrides in Scotland, where the consumption of cereal s was high but the teeth were noticeably good, there was also either abundant sunshine or a high consumption of animal foods such as marine life or dairy products rich in fat-soluble vitamins and minerals to counteract the negative effects of the cereals.25 In addition to her studies on tooth structure, Mellanby also studied the relationship between the fat-soluble vitamins and the supporting gingival and periodontal tissues of the mouth. In a series of about thirty animal experiments that lasted from three to eight years, she varied the quantities of vitamin A and vitamin D she fed to dogs, as well as the timing during the dog's growth and development at which she fed adequate or deficient quantities of the vitamins. Vitamin D deficiency affected mainly the hard tissues of the periodontal area, producing defective and osteoporotic tissue in the lamina dura and alveolar bone\u2014the thickened ridge of bone that contains the tooth sockets. Vitamin A, on the other hand, appeared to principally influence the development of the soft dental tissues. Chronic 25 May Mellanby, \"The Influence of Diet on the Structure of Teeth,\" Physiological Reviews 8 (1928): 574. 120 deficiency of vitamin A produced swollen gingival regions and hyperplastic and keratinized epithelial tissue, as well as malposed teeth and malformed roots. Periodontal tissue appeared abscessed and easily invaded by bacteria. With a deficiency of vitamins A and D, both the hard and soft tissues were abnormally developed. Mellanby further observed that a deficiency of either vitamin during early life was of much greater significance than the same deficiency after the periodontal tissues had already developed. For Mellanby, this phenomenon in dogs accounted for a \"condition often seen in man, namely, the presence of periodontal abnormalities in a mouth where there is little or no caries, a condition which would be explained by the consumption of a diet poor in fat-soluble vitamin A but rich in D during development and eruption of the permanent teeth, but containing a liberal supply of both vitamins later in life.\"26 It was not long before Mellanby began to make clinical observations and human experiments side-by-side with her animal studies. She and her coworkers conducted a series of studies from 1923 to 1934 on children between approximately five and twelve years of age in a Sheffield tuberculosis hospital. Mellanby described the hospital diet as \"generous and by all ordinary standards would be considered very good.\" The results of the first two experiments clearly indicated that with the addition of milk, eggs, and cod liver oil to this standard hospital diet, there was less caries initiated and less increas e in caries.27 In a third experiment in 1928, Mellanby and her coworkers gave a group of children purified vitamin D in the form of irradiated ergosterol, and another purified vitamin A, in order to distinguish between the possible effects of the vitamins. They tested the outcome against a control 26 May Mellanby, \"Periodontal Disease in Dogs (Experimental Gingivitis and \"Pyorrhoea),\" Proceedings of the Royal Society of Medicine 23, no. 11 (September 1930): 1503-10. 27 May Mellanby, C. Lee Pattison, and J. W. Proud, \"The Effect of Diet on the Development and Extension of Caries in the Teeth of Children. (Preliminary Note),\" British Medical Journal 2, No. 3322 (30 August 1924): 354-355; and May Mellanby and C. Lee Pattison, \"Some Factors of Diet Influencing the Spread of Caries in Children,\" British Dental Journal 47 (1926): 1045-47. 121 group on the standard hospital diet receiving neither of the vitamins. Although the vitamin A group provided inconclusive results, they found that the addition of the pure preparation of vitamin D checked the initiation of new carious lesions, diminished the spread of old carious lesions, and arrested the infective process in many carious teeth.28 In a final study, Mellanby and C. Lee Pattison gave a group of hospitalized children averaging five-and-a-half years of age a cereal-free diet rich in vitamin D and calcium for a period of six months. The teeth of the children were defective in structure and had much active dental caries at the beginning of the investigation. The cereal-free dietary regimen, which contained ample potatoes as a replacement, nearly eliminated the initiation and spread of caries. But the results were even better than those of the previous investigations in which the vitamin D or animal fats were increased in a diet containing bread, porridge, and other cereals. The \"anti-calcifying substance\" seemed to be the factor producing these variable results.29 Mellanby's experiments on tuberculous children in Sheffield were criticized because the subjects were either suffering from a disturbance of calcium and phosphorus metabolism or were otherwise not in good health.30 Furthermore, Mellanby admitted later, many dentists simply did not believe the results of the Sheffield survey because she was a biochemist, not a dentist.31 So, from 1928 to 1931, she supervised a more extensive investigation on putatively healthy subjects in three children's institutions in Birmingham. This survey bore 28 May Mellanby and C. Lee Pattison, \"The Action of Vitamin D in Preventing the Spread and Promoting the Arrest of Caries in Children,\" British Medical Journal 2, no. 3545 (15 December 1928): 1079-82. 29 May Mellanby and C. Lee Pattison, \"The Influence of a Cereal-Free Diet Rich in Vitamin D and Calcium on Dental Caries in Children, British Medical Journal 1, no. 3713 (19 March 1932): 507-10. 30 Franklin C. Bing, \"Diet and Dental Caries,\" Journal of the American Medical Association 19 (October 1932): 1845. 31 May Mellanby, Talk given to dental surgeons at the British Medical Association meeting on December 11, 1946, PP/MEL/L2/39, Wellcome Library Archives and Manuscripts, London, UK. 122 the imprimatur of scientific authority, since it was carried out under the auspices of the Committee for the Investigation of Dental Disease of the Medical Research Council. To insure reliable conclusions and quiet her skeptics, Mellanby's research team included a dental surgeon to examine the teeth and a statistician to interpret the data. The children were divided into four groups with practically identical calorie intakes: two controls, olive oil and sugar syrup, and two vitamin groups, cod liver oil and irradiated ergosterol. Due to administrative difficulties, the olive oil group at the end of the trial was too small to qualify as a usable control, and the decision to use treacle, or partially unrefined sugarcan e syrup, as a source of calories for a control group in a caries experiment was questionable. But the data were nevertheless suggestive on certain points. There was significantly less caries, both in incidence and spread, in the irradiated ergosterol and cod liver oil groups than in the controls. The number of first permanent molars that became fully erupted during the period of the investigation was too small to be of statistical value, but there were indications that the additional vitamin D or cod liver oil during the developmental period had tended to improve the structure. It appeared that vitamin D exerted a systemic, long-term effect on developing teeth.32 Despite these findings, Mellanby remained mired in controversy. One of main reasons for this was that her methods and criteria for determining enamel \"hypoplasia\"\u2014that is, malformation or underdevelopment\u2014differed from the conventional view of most dental workers. In her early microscopic examinations of deciduous teeth of English children, she observed that about 85 percent were \"hypoplastic\" and about 84 percent were carious. Macroscopic examination of these deciduous teeth using normal dental instruments and the 32 Committee for the Investigation of Dental Disease in Children, assisted by A. Deverall and M. Reynolds, The Influence of Diet on Caries in Children's Teeth, Special Report Series No. 211 (London: MRC, 1936). 123 naked eye indicated \"gross hypoplasia\" in only three percent of the teeth. But by applying her microscopic criteria for hypoplasia, which became known in the literature as \"Mellanby hypoplasia,\" she found a strong positive correlation between imperfect tooth structure and caries.33 In studies on permanent teeth, Mellanby elaborated further on the difference between \"gross hypoplasia\" and \"hypoplasia\" as revealed by microscopic examination. She found gross hypoplasia in only ten percent of permanent teeth, but moderate and severe microscopic hypoplasia in over 90 percent.34 She also examined the secondary dentin of the teeth that were exceptions to the correlation she identified between caries susceptibility and microscopic hypoplasia. She observed that most of these exceptions could be explained by the fact that either the secondary dentin in defectively formed teeth that resisted decay was well calcified, or the well-formed teeth that did decay had poorly formed secondary dentin. Mellanby's work on hypoplasia was possibly inspired by an examination made by New Zealand dentist H. P. Pickerill in the early 1900s on the permanent teeth of Maoris living under primitive conditions and those of \"civilized races.\" Pickerill observed that \"imbrication lines\" or \"furrows\" running horizontally along the teeth, which indicated checks in the secretion function of the ameloblasts, were much less common and distinct on the teeth Maoris living under primitive conditions than on those of the \"civilized races.\"35 Confirming this connection between diet and tooth structure, Mellanby found in a 1928 33 May Mellanby, \"The Effect of Diet on the Resistance of Teeth to Caries,\" Proceedings of the Royal Society of Medicine 16 (1923): 74-82. 34 May Mellanby, \"The Structure of Human Dental Journal 48 (1927): 737; May Mellanby, \"The Structure of Human Teeth in Relation to Caries,\" British Dental Journal 48 (1927): 1481; May Mellanby, Diet and the Teeth: An Experimental Study, Part II, MRC Special Reports, Series No. 153 (London: MRC, 1930); and May Mellanby, Diet and the Teeth: An Experimental Study, Part III, MRC Special Reports, Series No. 191 (London: MRC, 1934). 35 H. P. Pickerill, The Prevention of Dental Caries and Oral Sepsis (London: Balliere, Tindall, and Cox, 1912). 124 study that the deciduous teeth of children from well-off\u2014and presumably better-fed\u2014families were less severely hypoplastic and carious than those of the poor. In both social classes, however, it was evident to Mellanby that \"the deciduous teeth of children in England are very defect ive in structure and that views previously held as to the average good structure of these teeth are erroneous.\"36 But much to Mellanby's exasperation, the dental investigators who found neither widespread hypoplasia nor the correlation between hypoplasia and caries rates failed to take note of the distinction that she drew between \"gross\" and \"microscopic\" hypoplasia.37 Those who did make such a distinction came to conclusions largely in accordance with hers.38 For example, J. D. King, who collaborated with Mellanby on her hypoplasia work and later became head of the Medical Research Council's Dental Research Unit in London, defended her methods. He commented in a talk in the late 1930s that the number of people who \"have even bothered to test out Mrs. Mellanby's methods of judging hypoplasia... can almost be counted on the fingers of one hand.\" King argued that, in spite of all that Mellanby had said and written, \"her critics still appear to think that she refers to 'text-book' (gross) hypoplasia when she states that tooth structure and caries susceptibility are related.\"39 36 May Mellanby, \"The Influence of Diet on the Structure of Teeth,\" Physiological Reviews 8 (1928): 551. 37 Geoffrey Taylor and C. D. Marshall Day, \"Relation of Vitamin D and Mineral Deficiencies to Dental Caries,\" British Medical Journal 1, no. 4087 (6 May 1939): 919-21; and \"D. C. Wilson, \"Rickets and Dental Caries,\" The Journal of Physiology 96 (1939): 8P. 38 See, e.g., J. H. Davies, \"An Investigation into the Relationship Between Dental Structure and Dental Caries in Children Attending Public Elementary Schools,\" British Dental Journal 67 (1939): 66; Basil G. Bibby, \"The Relationship Between Microscopic Hypoplasia (Mellanby) and Dental Caries,\" Journal of Dental Research 22 (1943): 218; and Marvin Sheldon, Basil S. Bibby, and Michael S. Bales, \"The Relationship Between Microscopic Dental Defects and Infantile Debilities,\" Journal of Dental Research 24, no. 2 (1945): 109-16. 39 J. D. King, Talk with May Mellanby in Birmingham, 1937/38, PP/MEL/L2/37, Wellcome Library Archives and Manuscripts, London, UK. 125 For Mellanby, the running disagreement over hypoplasia came to represent a larger mental divide between the clinician and the scientist in their approaches to dental pathology. By the late 1930s, she had become acutely aware of the critical or indifferent attitude of many dental practitioners toward her work. Mellanby greatly respected dentists as \"healers,\" whereas she saw her work as dealing with the discovery of \"preventive means\" through basic scientific research. As she grew more confident that the application of her findings could improve public dental health, she became increasingly vocal in expressing her views. In 1938, Mellanby confessed that, although she began her work on dental tissues \"simply as a piece of physiological research which might ultimately prove of some value in the attack on dental disease,\" she \"soon became interested in the dental health of the nation.\"40 Aside from her fear that she could not express her work clearly enough in oral presentations, Mellanby voiced frustration to her friends and colleagues about the difficulty she continually faced as a woman and professional \"outsider\" in overcoming the resistance of the dental community to her findings.41 Nevertheless, Mellanby's ideas found a receptive audience among those researchers who were intrigued by the possibility that the newer knowledge of nutrition might help to turn back the tide of dental degeneration in the West. For instance, Percy Howe, while sitting as president of the American Dental Association in 1929, wrote her, \"I think your 40 May Mellanby, Speech at Cambridge University, January 19, 1938, PP/MEL/L2/36, Wellcome Library Archives and Manuscripts, London, UK. 41 In an undated speech to dental surgeons entitled \"Nutrition in Relation to Dental Disease,\" Mellanby commented: \"As you know, I am not dentally qualified, moreover I am a woman; for these and other reasons the dental profession in England has paid little attention to my work. I admit the contrary is the case in America and on the continent\" (PP/MEL/L2/74, Wellcome Library Archives and Manuscripts, London, UK). She expressed similar sentiments to Martin Rushton, a researcher for the British Dental Association, in a 1938 letter: \"By the way, you made me realize more than I have ever done before the futility of a pure research man or woman attempting to really interest the average clinician. I appreciate the stupidity (perhaps a big strong!) of ever accepting invitations to talk to clinicians about my work\" (PP/MEL/F42, Wellcome Library Archives and Manuscripts, Lonon, UK). 126 work has done more to turn the thoughts of the profession into the correct channel than any previous studies have done.\"42 Howe, unsatisfied by what the oral environment theory left unanswered, was himself conducting dietary intervention studies that were roughly contemporaneous with those of Mellanby. Indeed, Howe and Mellanby were just a few of the participants at the time in a growing wave of research exploring the relationship between nutrition and the teeth. \"A Healthy Tooth Resists Decay\": Nutritional Dentistry in the Interwar Years Even before the emerg ence of the newer knowledge of nutrition, Percy Howe had been interested in the role of diet and nutrition in the health of oral tissues. He agreed that Miller's chemico-bacteriological theory explained the mech anism of tooth decay once it was initiated, but he felt that too few dental researchers had appreciated the role of nutrition in building \"the defense of the system against invading pathogenic organisms.\"43 Howe hypothesized that dietetic changes could influence the overall microbial makeup of the oral environment and dental plaques, and thus the susceptibility or resistance of the host to caries.44 After becoming Chief of the Research Laboratory at the prestigious Forsyth Dental Infirmary for Children in Boston, Howe began focusing his research efforts on the role of vitamins in the dentition of rats, guinea pigs, and monkeys. Like many of his contemporaries, he was puzzled by the fact that many \"primitive races\" seemed to almost 42 Percy Howe to May Mellanby, May 4, 1929, PP/MEL/F16, Wellcome Library Archives and Manuscripts, London, UK. 43 Percy R. Howe, \"Dietetics,\" Dental Cosmos 52, no. 4 (April 1910): 441-44. In his speculations on what sort of diet might confer resistance to caries, Howe cobbled together several ideas that were popular at the time, such as the belief that the diet should be alkaline rather than acid in reaction, and low rather than high in protein. He also entertained the notion that \"autointoxication\" had dental and oral ramifications. These ideas eventually disappeared from his writings in the 1920s. 44 Percy R. Howe, \"The Saliva,\" Dental Cosmos 55, no. 11 (November 1913): 1112-17. 127 completely avoid dental decay until they came into frequent contact with civilization. Moreover, repeated attempts by researchers to produce caries in living animals by bacterial mean s and by feeding flour and sugar had largely failed, which indicated to Howe that the \"bacterial flora of caries is to be regarded as of secondary importance and that Miller's theory is not an adequate explanation of the full phenomenon of dental caries.\"45 For Howe, the dental research community's growing realization of the limits to the chemico-bacterial explanation for dental disease was part of a larger shift in focus in preventive medicine away from infective agents and toward chronic degenerative diseases. As he put it in a 1928 article, \"We hear much of the lengthening span of human life and of the achievements of medicine in overcoming tuberculosis and typhoid fever, but we also hear that caries and cancer are on the increas e, and that our young people have many structural defects of the jaws and other parts of the body framework.\"46 Howe felt that, in order to address this emerg ing problem, much more work had to be done on the subject of diet and nutrition. Indeed, soon after the discovery of the \"antiscorbutic\" vitamin C, Howe set out to test its effect on teeth. He fed a scorbutic diet to guinea pigs, rhesus monkeys, and Java macaques\u2014animals that, like humans, cannot synthesize their own vitamin C\u2014and carious lesions developed in their molars and incisors. Conversely, he failed to consistently produce tooth decay in animals that had been fed otherwise adequate diets containing \"considerable amounts of sugars and white flour,\" even though their teeth were covered with adhesive 45 Percy R. Howe, \"The Interpretation of Dental Lesions in Light of Recent Research,\" Journal of Dental Research 7 (1927): 146. In following decades, dental researchers succeeded in breeding strains of animals more or less susceptible to tooth decay, which indicated a genetic component to caries susceptibility. 46 Percy R. Howe, \"The Importance of the Diet during Prenatal and Infant Life on the Development of Human Teeth,\" Dental Cosmos 70, no. 1 (January 1928): 9. 128 masses of sugars and starches.47 Howe hypothesized from this animal research that dental caries appeared to result as a consequence of disordered calcium metabolism and not from some specific agency. Slightly later, Howe and S. Burt Wolbach of the Harvard University Medical School conducted some of the pioneering histological investigations, discussed briefly in chapter one, into the effects of vitamin A deficiency on animals under laboratory conditions. They found that the function of vitamin A is concerned primarily with epithelial cells and, in these cells, with the process of differentiation. As explained earlier, whenever vitamin A is lacking in the diet or is provided in insufficient amount, the epithelial tissues everywhere suffer injury. The highly specialized epithelial tissues lining the ducts, glands, and cells that produce secretions such as tears, saliva, and digestive fluids, tend to keratinize\u2014that is, they convert to a hard, skin-like or horn-like substance where they shouldn't. In dental tissues, this mean t that a vitamin A deficiency resulted in atrophy or morphological change of the epithelial cells responsible for governing the formation of tooth enamel and dentin.48 Howe and Wolbach found that a deficiency of \"fat-soluble A\" caused the odonotoblasts to law down bone instead of dentin, resulting in less dense and resistant calcification of the tooth structure. In experimental animals, vitamin A deficiency also produced enamel hypoplasia and the invasion of the pulp chamber by dentin, as well as retarded tooth eruption and alveolar bone malformation. 47 Percy R. Howe, \"Studies on Dental Disorders Following Experimental Feeding of Monkeys,\" Journal of the American Dental Association 11 (1924): 1149; and idem, \"Some Experimental Effects of Deficient Diets on Monkeys,\" Journal of the American Dental Association 11 (1924): 1161. For similar studies undertaken at the Forsyth Dental Infirmary, see Guttorm Toverud, \"The Influence of Diet on Teeth and Bones,\" Journal of Biological Chemistry 58 (1923): 583-600. 48 S. B. Wolbach and Percy. R. Howe, \"Tissue Changes Following Deprivation of Fat-Soluble A Vitamin,\" Journal of Experimental Medicine 42 (1925): 753-77; and idem, \"The Incisor Teeth of Albino Rats and Guinea Pigs in Vitamin A Deficiency and Repair,\" American Journal of Pathology 9, no. 3 (1933): 275-294. 129 Howe and Wolbach were publishing these vitamin A studies at a time when many other researchers were rushing into the dental field to assess the effect of vitamins and minerals on teeth and oral tissues. For example, in 1916, Leila Jackson and J. J. Moore published a paper reporting pulp changes in the teeth of scorbutic guinea pigs, which they described as the processes of \"necrosis.\"49 A few years later, S. S. Zilva and F. M. Wells of the Biochemistry Department at the Lister Institute in London investigated these processes and described them as \"fibrosis\" or \"fibroid degeneration.\"50 Subsequent studies on scorbutic guinea pigs provided a more detailed and comprehensive view of the effects of vitamin C deficiency on dental tissues.51 Vitamin C was found to play a primary role in the differen tiation of the connective tissue cells, and therefore in the capacity of these cells to form and maintain intercellular substances. The oral tissues that are mesenchymal products (dentin, bone, and periodontal membrane) or whose integrity depends upon their vascular supply (the gingiva) were primarily affected. Animal experiments demonstrated that a vitamin C deficiency resulted in the production of irregular and porous dentin and \"calcified scar\" in the pulp, as well as weakening and degeneration of the alveolar bone and periodontal tissues. G\u00f6sta Westin of Stockholm, Sweden conducted a series of significant investigations in the 1920s showing that the pathological changes in the teeth and gingiva of scorbutic adult human beings were nearly identical with those observed in experimental animals. However, Westin did not find any evidence that scurvy increased susceptibility to dental caries. His failure to identify a positive correlation between the scorbutic state and 49 Leila Jackson and J. J. Moore, \"Studies on Experimental Scurvy in Guinea-Pigs,\" Journal of Infectious Diseases 19, no. 3 (September 1916): 478-510. 50 S. S. Zilva and F. M. Wells, \"Changes in the Teeth of the Guinea-Pig, Produced by a Scorbutic Diet,\" Proceedings of the Royal Society of London: Series B, Containing Papers of a Biological Character 90, no. 633 (15 May 1919): 505-12. 51 A. H\u00f6jer and G. \"Jaws and Teeth in Scorbutic Guinea-Pigs: Histopathological Study,\" Dental Cosmos 67 (1925): 1-24 130 caries susceptibility was repeated in later investigations.52 On the other hand, the importance of vitamin C in maintaining the integrity of gingiva and supporting structures of the teeth was amply demonstrated in both experimental animals and humans.53 Research into the effects on the dental tissues of deficiency of the B-complex vitamins lagged behind that of the other vitamins. Since most of the findings in this area were based on clinical observations in humans rather than animal experimentation, the understanding of many of the microscopic changes brought about by vitamin B deficiencies, and the biological processes involved, remained rudimentary. Nevertheless, it became clear quite early in the process of discovery that vitamin B deficiencies primarily affected the oral soft tissues (gingiva, tongue, and mucous membranes). In fact\u2014much to the benefit of medical clinicians\u2014some of the earliest symptoms of vitamin B deficiencies appear in the oral cavity. One of the first signs of niacin deficiency, for example, is painful gingivitis and inflamm ation of the mucous lining of the structures in the mouth. Interestingly, though, victims of pellagra and other vitamin B deficiencies were found to have a relatively low incidence of dental decay, in spite of their generally poor oral hygiene and high-carbohydrate diet. Researchers in the 1940s and 1950s partially unraveled this mystery by showing that certain B vitamins, which are present in the saliva, are essential for bacterial growth and carbohydrate metabolism.54 But, complicating this picture somewhat, clinical and animal studies in the 1950s and 1960s showed that pyridoxine (vitamin B6) 52 G. Westin, \"Scorbutic Changes in the Teeth and Jaws of Man,\" Dental Cosmos 67 (1925): 868-72. 53 Isaac Schour and Mauray Massler, \"The Effects of Dietary Deficiencies upon the Oral Structures,\" Physiological Reviews 25, no. 3 (1945): 450-56. 54 A. W. Mann, Samuel Dreizen, Tom D. Spies, and F. M. Hunt, \"A Comparison of Dental Caries Activity in Malnourished and Well Nourished Patients,\" Journal of the American Dental Association 34 (1947): 244; and Samuel Dreizen and Tom D. Spies, \"Observations on the Relationship Between Selected B Vitamins and Acid Production by Microorganisms Associated with Human Dental Caries,\" Journal of Dental Research 32, no. 1 (February 1953): 65-73. 131 supplemen tation lowered caries incidence by altering the oral flora.55 Different B vitamins, then, seemed to influence the species composition of the oral environment. By the 1950s, the overall picture of the relationship of the B-complex vitamins to dental health had become quite clear. Deficiencies of B-complex vitamins, as with vitamins A and C, were shown to result in degenerative disorders of the soft oral structures and the periodontal tissues, and in heightened susceptibility to gingivitis and other infectious diseases of the oral cavity. It was suggested, quite reasonably, that much of the increase in tooth loss, periodontal disease, and root caries associated with aging was caused by chronic and sub-acute vitamin deficiencies in diets consisting largely of refined flour, sugar, and fats. The most-researched\u2014or at least the most-discussed\u2014vitamin in the dental field was vitamin D. Although May Mellanby was undoubtedly the leader in this area, other investigators explored the connection between vitamin D, tooth structure, and caries incidence as well. In the early 1930s, for example, a group in the Department of Pediatrics at the University of Toronto conducted a controlled study of several hundred orphanage children to determine the effect of a vitamin D supplement on the rate of dental caries. In planning this investigation, a survey was made of the supply of the various known factors necessary for adequate nutrition. The researchers determined that even under excellent dietary conditions, the average Canadian child did not receive any vitamin D for many months of the year unless it was specifically administered. The vitamin D value of sunshine in Toronto, they found, takes a very mark ed drop in mid-October and remains at an extremely low level throughout the winter months. This, combined with the necessity for 55 Robert W. Hillman, Philip G. Cabaud, and Roger A. Schenone, \"The Effects of Pyridoxine Supplements on the Dental Caries Experience of Pregnant Women,\" American Journal of Clinical Nutrition 10 (June 1962): 512-15. 132 bundling up in cold weather, mean t that very little vitamin D synthesis could take place in Canada and the northern part of the United States for approximately half of the year.56 With these troubling findings on hand, the Toronto researchers conducted their orphanage study to show whether a lack of vitamin D had any effect on the development of tooth decay. They assessed the dental caries increase in the children of two orphanages, one group receiving 250 IU of added vitamin D in the form of viosterol (irradiated ergosterol), for one year. When the results were tabulated, they determined that in the group given the standard institutional diet, which was deficient in vitamin D, the incidence of caries in the deciduous teeth were more than double that found in the other group of children receiving exactly the same diet but with the added vitamin D. Moreover, these investigators observed a substantial reduction in the dental caries increment of the children in the experimental group between three and ten years of age, whereas there was no significant reduction in the experimental group between eleven and sixteen years of age. This difference could have been due to the use of a relatively low does of vitamin D at this critical period of growth, or to the lower responsiveness of permanent teeth at that age to nutritional influences.57 A group of researchers at the School of Dental and Oral Surgery at Columbia University, led by Ewing C. McBeath, also tested the effect of vitamin D on children in the 1930s and 1940s. In one study, McBeath administered vitamin D concentrate in evaporated milk and dairy milk, as well as ultraviolet radiation of the skin, to 425 boys and girls, aged eight to ten years of age in four children's institutions in New York. There were two control groups in each of the homes, one on the regular institutional diet and one on the same diet 56 Frederick F. Tisdall and Alan Brown, \"Seasonal Variation in the Antirachitic Effect of Sunshine,\" American Journal of Diseases of Children 34, no. 5 (1927): 721-36. 57 P. G. Anderson, C. H. M. Williams, H. Halderson, O. Summerfeldt, and R. G. Agnew, \"Influence of Vitamin D on the Prevention of Dental Caries,\" Journal of the American Dental Association 21 (1934): 1349. 133 with the daily addition of one pint of plain unevaporated milk with no extra vitamin D. The children of the experimental group received daily 100, 150, or 300 IU of vitamin D in a pint of reconstituted evaporated milk (which, unsurprisingly, they had to flavor with chocolate to get the children to drink). The increase in percen tage of carious surfaces during the seven-month experimental period was much greater on the control regimens than on the vitamin-supplemen ted ones, and the children with the highest doses of the vitamin experienced the fewest caries.58 Later studies by McBeath and his coworkers led them to believe that differen t forms of vitamin D provided varying degrees of protection from caries. The vitamin D supplied by cod liver oil, their findings indicated, was more effective per unit administered than irradiated ergosterol in reducing the dental caries increment of children.59 The third main research group in North America that studied the effects of vitamin D on human subjects was based at the State University of Iowa. In the mid-1930s, Julian Boyd, Charles Drain, and Genevieve Stearns in the Department of Pediatrics studied the effect of daily supplemen ts of 155 and 600 IU of vitamin D to an otherwise adequate diet of children in an institution. In the group of children who recei ved 155 IU for five months, there was still minimal but definite caries activity. When the supplement was increased to 600 IU of vitamin D per day, there was a cessation of caries activity. However, the \"Iowa group\" found that the effect of the 600 IU of vitamin D per day was no greater than that produced by them in earlier experiments with 350 IU.60 58 E. C. McBeath, \"Vitamin D Studies, 1933-1934,\" American Journal of Public Health 24, no. 10 (October 1934): 1028-30. 59 E. C. McBeath and T. F. Zucker, \"The Role of Vitamin D in the Control of Dental Caries in Children,\" Journal of Nutrition 15, no. 6 (1938): 547-64; E. C. McBeath and W. A. Verlin, \"Further Studies on the Role of Vitamin D in the Nutritional Control of Dental Caries in Children,\" Journal of the American Dental Association 29 (1942): 1393. 60 Julian D. Boyd, Charles L. Drain, Genevieve Stearns, \"Nature of Diet and Its Relationship to Dental Caries,\" Proceedings of the Society for Experimental Biology and Medicine 36 (1937): 645-46. 134 Studies exploring the relationship between mineral intake and dental caries incidence provided more ambivalent results than those on vitamin D. A number of investigators tested the use of calcium therapy in dentistry, but none of them produced convincing results showing any benefit on the dental caries incidence. Experimental animals placed on a diets deficient in minerals such as calcium, phosphorus, and magnesium showed disturbances in the formation of the growing dentin, but with little effect upon the growing enamel. Researchers discovered that the effect of a calcium-deficient diet was much less severe in the growing dentin than in the bone, since dentin has a distinct priority for the available calcium and cannot resorb like bone in response to deficiency. The changes in the growing dentin and bone observed in humans on a calcium-deficient diet were found to be similar to those in experimental animals. May Mellanby's observation that vitamin D therapy greatly lessened the effects of a diet deficient in calcium were confirmed by other researchers.61 Other dietary intervention studies supported the nutritional approach to dentistry, albeit with less emphasis on any single nutrient, and more on the diet in general. Even before their work on vitamin D, the Iowa group led by Boyd conducted investigations on children with diabetes mellitus under strict dietary supervision for a series of months. Most of the diabetic children, they observed, had definitely worsening dental caries prior to the establishment of dietary control. The main constituents of the prescribed diets were the highly regarded \"protective\" foods: milk, cream, butter, eggs, meat, vegetables and fruits, supplemen ted with cod liver oil. Contrary to the commonplace that oral hygiene was necessary to combat decay, the occurrence of dental calculus\u2014a technical term for tartar 61 For a review of the early literature on minerals and oral tissues, see Isaac Schour and Maury Massler, \"The Effects of Dietary Deficiencies upon the Oral Structures,\" Physiological Reviews 25, no. 3 (1945): 459-464. 135 buildup\u2014was almost universal on the teeth of the children with arrested caries. Also, the degree of oral hygiene varied greatly among the children, yet the researchers found unquestionable evidence of arrested caries even in the most poorly cleaned mouths.62 Intrigued by these findings, Boyd and his colleagues expanded their research to groups of children where diabetes and insulin therapy were not factors. They prescribed typical high-fat, low-carbohydrate diabetes-controlling diets to children living both at home and in orphanages. In their orphanage studies, the regular institutional diet was fortified with ample milk, butter, and eggs. In one of their dietary interventions in otherwise healthy pre-school children living at home, they prescribed, in addition to whatever other food was served, one quart of milk, one teaspoon of cod liver oil, one ounce of butter, one orange, and two or more servings of vegetables and fruits.63 In all their studies, the Iowa group achieved nearly total control of dental decay, except in cases that could be traced to increasing laxity in self-adherence to the prescribed diets. There were also cases in which the caries activity was unresponsive to dietary changes. However, Boyd and his coworkers managed to arrest dental caries activity in these children by further increasing the supply of the \"protective\" foods. Their findings suggested that children have individual variations in their capacity to absorb minerals, and they provided strong evidence of a relationship between dental caries and the retention of calcium and phosphorus. Children whose retention levels showed a positive balance experienced a halt in their caries experience.64 In addition, they observed arrest of decay in areas where 62 J. D. Boyd, C. L. Drain and M. V. Nelson, \"Dietary Control of Dental Caries,\" American Journal of Diseases of Children 38 (1929): 721-23. 63 J. D. Boyd and C. L. Drain, \"The Arrest of Dental Caries in Childhood,\" Journal of the American Medical Association 90 (1928): 1867-69. 64 J. D. Boyd, C. L. Drain, and G. Stearns, \"Metabolic Studies of Children with Dental Caries,\" The Journal of Biological Chemistry 103, no. 2 (1933): 327-37; C. L. Drain and J. D. Boyd, \"The Control and Arrest of Dental Caries: An Institutional Study,\" Journal of the American Dental Association 22 136 self-cleansing by fibrous \"detergent\" foods could not occur, eliminating for them the possibility that it was the tooth-cleansing nature of the diet that was responsible for the reduced activity of decay. Echoing May Mellanby's findings with experimental animals, Boyd and Drain argued that the arrest of caries \"is dependent on sclerotic changes in the dentin and enamel, which occur normally in the healthy tooth in response to physiologic or pathologic irritation.\" They concluded that the \"degree of sclerosis seems dependent on the adequacy of the diet.\"65 Workers at the Forsyth Dental Infirmary in Boston were also getting promising results from dietary interventions. In the early 1930s, a team under the supervision of Howe gave daily diet recommendations to the parents of the infirmary 's outpatients. The dietary additions were similar to those of the Iowa group: one quart of milk, one raw vegetable, at least two cooked vegetables, two servings of fruit with special emphasis on oranges, one egg, meat or fish five times a week, and butter on vegetables and bread. Cod liver oil was not given routinely but was prescribed by the pediatrician when indicated. Candy was allowed only at the end of meals. The average number of new carious lesions found in 104 \"cooperative\" subjects was a much lower 0.73 compared to the average number of 3.51 prior to dietary supervision, while the caries rate of the \"uncooperative\" children continued to worsen.66 Yet more support for the benefit of dietary change came from a series of intervention studies conducted by McBeath and his coworkers at Columbia University. In three New York orphanages and one state institute for \"mentally deficient children,\" (1935): 155-58; G. Stearns, \"Recent Investigations in Mineral Nutrition,\" Journal of the American Dietetic Association 18 (1942): 366-69. 65 J. D. Boyd and C. L. Drain, \"The Arrest of Dental Caries,\" American Journal of Diseases of Children 44 (1932): 691. 66 P. R. Howe, R. L. White, and M. Rabine, \"Retardation of Dental Caries in Outpatients of a Dental Infirmary. Preliminary Study,\" American Journal of Diseases of Children 46, no. 1 (1933): 1045-49. 137 McBeath supplemented the diet with milk, eggs, meat, vegetables, oranges, butter and cod liver oil daily. The first results, published in 1932, showed a definite reduction in the dental caries activity of the children on the supplemen ted diet. McBeath did not ascribe the improvement to any one nutrient. He felt, rather, that it was the result of the overall nutritional superiority of the supplemen ted diet compared to the unsupplemented diet.67 Aside from animal and intervention experiments, some the strongest evidence employed by the advocates of \"nutritional dentistry\" came from a growing number of anthropological studies that were carried out in the 1920s and 1930s. In 1926, for example, E. H. Mars hall, a researcher at the University of California School of Dentistry, published his findings on the dental condition of the roughly 140 inhabitants of the remote islands of Tristan da Cunha, located about 1500 miles west of the Cape of Good Hope, and about 4000 miles east of Cape Horn. Befo re the time of steam vessels for hunting whales in the South Seas, sailing ships frequently stopped there for supplies and provided the inhabitants with some food from the outside world. But with the advent of steam trawlers in the nineteenth century, the natives rarely saw anyone from the outside world. Tristan da Cunha, being so isolated, presented itself as a nearly ideal \"laboratory\" to test the nutritional theory of tooth decay. The inhabitants ate mainly potatoes, turnips, and other starchy root vegetables, with the remainder of the diet consisting of fish, crawfish, meat, birds, and very small amounts of eggs and milk. In spite of the relatively high carbohydrate content of the diet and utter lack of tooth brushing or professional dental care, Mars hall observed virtually no tooth decay.68 Slightly later, in utter contradiction to the conventional assumption that oral hygiene was 67 E. C. McBeath, \"Experiments on the Dietary Control of Dental Caries in Children,\" Journal of Dental Research 12, no. 5 (1932): 723-47; E. C. McBeath, \"Nutritional Control of Dental Caries in Children Over Two-Year Period,\" Journal of Dental Research 13, no. 3 (1933): 243. 68 E. H. Marshall, \"Report on a Visit to Tristan da Cunha,\" British Dental Journal 47 (1926): 1099. 138 required to prevent caries, some investigators found that dental plaques under certain conditions might actually be protective against caries.69 Mars hall was not the only \"dental anthropologist\" at that time. In the late 1920s, New York dentist Leuman Waugh journeyed to Labrador and Alaska to study the effect of changing diet on the teeth of the local inhabitants. In both places, he found that \"the old Eskimos who in childhood had lived on the primitive native foods of their ancestors, even though of small stature, had very large well formed and regular jaws, and decay of the teeth was almost absent.\" Waugh noted, as would many other later investigators, that the teeth of older Eskimos were typically covered in film and worn down almost to the pulps, due to the sandy grit ingested with their food and other tooth-reducing practices, such as the prolonged chewing of hides to make leather. But they showed no evidence of tooth decay; the dentin appeared to continue forming a hard mineral surface throughout life.70 However, in the settlemen ts where \"white man's foods\"\u2014primarily refined sugar, white flour, and canned goods\u2014were readily available, Waugh observed \"deplorable mouth conditions,\" including even abscessed deciduous teeth among infants who had not yet begun eating solid foods.71 Field researchers were also developing a more nuanced view, in light of what they knew about nutrition at the time, of the relationship between the diets of \"primitive races\" and their dental health. For example, John Boyd Orr and J. L. Gilks collected data on the 69 Basil G. Bibby, \"A Study of Pigmented Dental Plaque,\" Journal of Dental Research 11, no. 6 (1931): 855-72; E. C. Dobbs, \"Local Factors in Dental Caries,\" Journal of Dental Research 12, no. 6 (1932): 853-62. 70 Charles B\u00f6decker made a similar finding in his examination of several hundred teeth of the skeletal remains of Pueblo Indians living more than 900 years ago. He found that although deep pits and fissures were common on the enamel, the caries incidence appeared to be exceedingly low. See C. F. B\u00f6decker, \"Concerning Defects in the Enamel of Teeth of Ancient American Indians,\" Journal of Dental Research 10, no. 3 (1930): 313-22. 71 L. M. Waugh, \"Health of the Labrador Eskimo with Special Reference to Mouth and Teeth,\" Journal of Dental Research 10 (1930): 387. Quotations from Waugh in E. V. McCollum, \"Relationship Between Diet and Dental Caries,\" Journal of Dental Research 11, no. 4 (1931): 571. 139 rate of tooth decay in their survey, discussed in chapter one, of the physique and health the pastoral Masai and the agrarian Kikuyu in Kenya. Even though the incidence of caries was much higher in the Kikuyu than in the Masai children, the incidence in the Kikuyu, at approximately 13 percent, still fell far below the high incidence in the industrial Western countries, which was between 85 and 100 percen t. That is, \"primitive\" diets low in so-called protective foods also correlated with higher rates of tooth decay, though they were still far better than those produced by \"modern\" diets.72 Field researchers such as Waugh, Mars hall, and Orr and Gilks were adding explanatory depth to what had been common knowledge among physical anthropologists and archaeologists for some time: that rates of tooth decay among \"civilized races\" were much worse than those of \"ancient man\" and existing \"primitive races,\" particularly hunter-gatherer groups. It is therefore not surprising that during the 1934 debate at the Hotel Pennsylvania, McCollum, Price, and Merritt supported their nutritional argument by pointing to \"the Indians, the Eskimos, and other races that had enjoyed almost complete immu nity from dental troubles 'until they forsook their primitive diets and took to brushing teeth and eating modern foods,'\" in the words of the New York Times reporter.73 It is even less surprising given the fact that, in 1934, Weston Price himself was in the midst of a decade-long project to study the dental health of the \"isolated primitive races\" that had not yet come into steady contact with what he called the \"displacing foods of modern commerce.\" On the Margins of \"Modern Degeneration\": Weston A. Price's Dental Anthropology 72 J. B. Orr and J. L. Gilks, Studies of Nutrition: The Physique and Health of Two African Tribes, Medical Research Council Special Reports, Series No. 155 (London: HMSO, 1931). 73 \"Scientists Clash Over Dental Ills,\" The New York Times, 28 March 1934, 8. 140 Seen from the early years of his career, Price seems an unlikely candidate to become one of the most vociferous advocates of the nutritional approach to dentistry. After graduating in 1893 from the University of Michigan with a D.D.S. degree, Price set up his first practice in Grand Forks, North Dakota. Shortly thereafter, he became stricken with typhoid fever and nearly succumbed to the disease. His older brother, Albert, dropped everything and went to Weston's aid. When Weston was well enough to travel, Albert brought him back to the Price family homestead in Eagle Rock, Ontario, to recover his health. During his illness, Weston's teeth had decayed alarmingly, but his health improved and dental deterioration reversed when he began eating the foods available on the farm. The following spring, his father-in-law, William Delmage, took Price into the backcountry, where the two of them camped for an extended period. While convalescing, Price found that the wild berries, fish, and game in his diet worked wonders to rejuvenate his health. This experience likely led Weston\u2014eventually at least\u2014to consider diet as a prime factor in dental problems as well as overall health.74 Price, however, did not engage in nutrition-related research until quite a bit later. He followed his brother Albert to Cleveland and set up his next practice. Price's early contributions to the literature show the signs of a highly successful, albeit conventional, career in dental research. He published papers on cataphoresis, tooth desensitizations, skiagraphs made with radioactive salts, inlay materials, and a pyrometer-controlled furnace for fusing porcelain. In 1915, the National Dental Association (soon after re-named the American Dental Association) made Price the Director of its Research Institute in Cleveland. Price devoted much of his time and patient fees to the determining whether or not a 74 Donald Delmage Fawcett, \"Weston A. Price: Truly a 'Great' Uncle. The Eagle Rock Homestead, Life and Family,\" Price-Pottenger Nutrition Journal 14, no. 1 (1994): 7; and transcript of interview with two Delmage family members (10 May 1994), unsorted file, Price-Pottenger Nutrition Foundation, La Mesa, CA. 141 relationship existed between dental infections and systemic diseases.75 He suspected that teeth and oral tissues infected with microorganisms, particularly the streptococci, could produce acute or chronic disturbances, such as rheumatism, in other parts of the body. Price's hypothesis was essentially a reaffirmation of the focal infection theory then popular among dentists and physicians. He drew heavily on the work of leading proponents of the focal infection theory at that time, such as E. C. Rosenow, Frank Billings, and Sir William Hunter. Over a period of about 25 years, Price led a team of researchers consisting of bacteriologists, physicists, chemists, histopathologists, and serologists, to test if chronic degenerative diseases could arise from deeply infected teeth. He decided to implant extracted infected teeth under the skin of test animals, because if bacteria were present and carry ing illness, Price believed that their presence in a tooth might offer the same kind of proof physicians found when they injected bacterial cultures to produce diseases in animals. He argued that by implanting the tooth, the disease of the patient, or one that showed similar symptoms, could be transferred to a test animal. Price carried out thousands of experiments along these lines using rabbits. In 1923, Price compiled his years of research into a massive two-volume work, Dental Infections.76 It appears to have been well received, though one otherwise approving reviewer gibed that \"the vivisectionist will be pained to learn that Dr. Price's researches have 75 A path Price possibly took because of a personal tragedy. He and his wife Florence's only son, Donald, had been afflicted with endocarditis, an infection of the heart, for about four years before he died at age sixteen. Donald's condition quickly worsened following root canals that Price gave him when he was fifteen years old, despite their superb workmanship. The dedication to the second volume of Oral Infections is revealing on this score: \"My Wife and Our Deceased Son, Donald, who, at sixteen, paid with his life the price of humanity's delayed knowledge regarding these heart and rheumatic involvements this volume is dedicated.\" 76 Weston A. Price, Dental Infections, Volume I: \"Dental Infections, Oral and Systemic,\" Volume II: \"Dental Infections and the Degenerative Disease,\" (Cleveland, OH: Penton Publishing Co., 1923). 142 required the use of upwards of five hundred rabbits a year for several years.\"77 Price concluded from his experiments that \"an infected tooth is a fortress for bacteria within the tissue of the host,\" from which dissolved substances could pass to the outside fluids and affect the body. For Price, the infected tooth was an especially dangerous focus of infection because, unlike most other tissues in the body, it \"furnishes a condition and environment that is tremendously in favor of the invading organism inhabiting it, as compared with the host, since the latter may only rid itself of the menace by exfoliating or absorbing it.\"78 He contended that if the decay is neglected or not discovered until after it spread into the root canal of the tooth itself, then the nerve and blood vessels become infected from the microorganisms that are part of the tooth decay process. These rapidly travel the entire root canal and from there discover the tubules of the dentin, in which they grow and reproduce. Price held that streptococci and perhaps other microorganisms from infected teeth could alter their biological characteristics, allowing them, or toxins produced by them, to produce local disturbances in other parts of the body or metabolic disturbances without localization Price further argued that \"root canal fillings rarely fill pulp canals sufficiently perfect ly to shut out bacteria.\" Apparently successful root canal operations that brought local comfort to the patient were particularly dangerous, he warned, since the filled teeth could still harbor microorganisms or their toxic byproducts.79 In Price's view, inflamed and painful tooth pulps were a sign of resistance by the host, and desensitizing the tooth with a root canal operation did not eliminate the focus of infection. In debates following the 77 Wallace Seccombe, \"Editorial,\" Oral Health 13, no. 12 (December 1923): 489-90. 78 Price, Dental Infections, vol. II, 32. 79 Ibid., 29. 143 publication of Dental Infections, he called for the removal of practically all pulpess and root-infected teeth instead of saving them with root canals and other endodontic treatments.80 Many other leading physicians and dentists at the time shared Price's views. The theory of focal infection was widely taught as the cause of a wide range of illnesses, with infected teeth and glands as the principal sources. Millions of tonsils, adenoids, and teeth were removed in an \"orgy of extraction,\" in the words of one the focal infection theory's early critics, Louis Grossman. But in the 1930s, focal infection as an explanation for myriad infective and degenerative diseases began falling out of favor as a growing body of research accumulated against it. One of the principal problems with the studies that appeared to support the theory, including Price's animal experiments, was that they did not use models or doses of microorganisms that closely resembled actual human pathological conditions.81 But even before he completed his experiments for Dental Infections, Price was moving into new areas of research that had more to do with prevention of dental decay than extraction of already-decayed teeth. One of the major realizations that he came to in the course of his focal infection studies was that \"dental caries, deficient dental and osseous calcification, disturbed metabolism, and deficiency diseases are, fundamentally, expressions of a disturbed physical chemistry of the system.\"82 For Price, susceptibility to these 80 Weston Price, \"Is a Pulpless Tooth Healthy because it is Comfortable?\" Dental Digest 31 (1925): 477-78; \"Resolved that Practically All Infected Pulpless Teeth Should Be Removed (Debate),\" American Dental Journal 12 (December 1925): 1468-1469. 81 Thomas J. Pallasch and Michael J. Wahl, \"Focal Infection Theory: New Age or Ancient History?\" Endodontic Topics 4 (2003): 32-45. It is interesting to note that more recent critics of root canal treatments still cite Price's Dental Infections in support of their views: see, e.g., George Meinig, Root Canal Cover-Up (Ojai, CA: Bion Publishing, 1986). There is, however, an overwhelming body of evidence showing that endodontic procedures produce less likelihood of focal infection than exodontic, or extraction, procedures, and that the absolute risk of bacteremia due to endodontic procedures is very low in any case. See, e.g., C. A. Murray and W. P. Saunders, \"Root Canal Treatment and General Health: A Review of the Literature,\" International Endodontic Journal 33 (2000): 1-18. 82 Dental Infections, vol. II, 399. 144 conditions seemed to be based on heredity and metabolic conditions, the latter of which could be greatly influenced by nutrition. Between the time of his typhoid episode in the 1890s and the publishing of Dental Infections in the early 1920s, knowledge of nutrition, as we have seen, rapidly advanced. Price remained current to the latest developments.83 His recep tiveness to these new concepts appears to have been partly related to the growing dissatisfaction he felt working inside the conventional boundaries of the dental profession. \"The dentistry of the past and of the present,\" Price opined in one of his articles, \"has been chiefly concerned with repair and replacement, and while this has constituted a great blessing to humanity, it is very far short of the ideal service to be rendered by dental science.\"84 The increase in dental caries in the industrialized countries, as well as his own accumulating experience as a practitioner in Cleveland, were reaffirming to Price that conventional reparative work and prophylaxis were failing to deal with the worsening condition of patients' teeth and oral tissues. He knew that other degenerative diseases were concomitantly on the rise as well, and his public addresses evince a growing awareness that the healing arts, and \"modern civilization\" more generally, were entering uncharted territory. In a 1928 speech, for example, Price marveled at the rapid improvement in the average lifespan with the elimination of \"such tragic infections as small pox, yellow fever, typhoid fever, and a host of other maladies such as diphtheria.\" But he also noted that \"heart disease, cancer, kidney and bowel disorders, nervous break-downs are possibly more prevalent today than they were 83 Price's personal library (\"Library of Dr. Weston A. Price,\" unsorted file, Price-Pottenger Nutrition Foundation, La Mesa, CA) contained a number of books published during the 1910s-1930s on subjects relating to nutrition, particularly vitamins, animal metabolism, biochemistry, and ultraviolet light. 84 Weston A Price,\" Some New Fundamentals for the Prevention of Dental Disease, with Special Consideration of Calcification and Decalcification Processes,\" Dental Cosmos 71 (February 1929): 145. 145 twenty and fifty years ago.\" Price suggested that \"people are not dying of these degenerative diseases so much as they are getting the degenerative diseases because they are dying,\" due to inadequate nutrition.85 Price's research projects in the 1920s evince his growing concentration on the role of nutrition and diet in dental decay, and in disease processes more generally.86 He chemically analyzed large numbers of saliva samples to which he had added powdered bone or tooth. The results revealed that, in the saliva of those immune to caries, the inorganic phosphorus and calcium moved from the saliva to the powdered bone or tooth, whereas in cases with active dental caries, the inorganic phosphorus and sometimes the calcium moved from the chips to the saliva. These findings indicated to Price that the physical and chemical state of the saliva, which provides the immediate environment of the tooth, determines the susceptibility to caries. Price believed that his saliva experiment also threw light on common clinical observations, such as the progressive hardening of the teeth after eruption and the sclerotic change in dentin in response to attrition or caries. But Price also considered whether or not susceptibility to dental caries reflected a more generalized state of metabolic dysfunction in the body. Using methods similar to those he used for saliva, he conducted analyses of the blood serum or plasma of hundreds of his patients. In normal cases, there was a movement of inorganic phosphorus and calcium from the serum to the bone chips. But in cases showing evidence of decalcification of the skeleton, such as in degenerative arthritis, the movement of inorganic phosphorus and 85 \"The Relation of Vitamines to Health and Disease, Lecture read before the Cleveland Sorosis, Thursday Afternoon, December 20, 1928,\" File W213, Price-Pottenger Nutrition Foundation, La Mesa, CA. 86 Cf. Weston A. Price, \"Newer Knowledge of Calcium Metabolism in Health and Disease with Special Consideration of Calcification and Decalcification Processes, Including Focal Infection Phenomena,\" American Medical Association Journal 13 (December 1926): 1765-94. 146 occasionally that of calcium was in the opposite direction, off the bone or tooth powder. 87 Although the tests were crude by current standards, susceptibility to dental caries appeared to Price to be one manifestation of a larger systemic problem. Beyond developing methods to test for caries susceptibility, Price also sought out the mean s to restore normal conditions in the saliva and blood serum, in order to possibly halt the caries process and decalcification disorders in the body. Like May Mellanby, he focused on the role of the fat-soluble vitamins and minerals, particularly calcium and phosphorus, in the development of teeth and bones. In the 1920s, Price conducted several animal and clinical studies to test the effect of ultraviolet radiation and cod liver oil on the calcification of bones and teeth. His work could thus be regarded as another of the early investigations into the effects of vitamin D, the so-called \"sunshine vitamin.\" But Price, for his part, was not comfortable ascribing the process of calcification to one isolated vitamin. He believed that \"other factors are involved that are related directly and indirectly to radiant energy,\" even though they had not yet been identified. 88 It was for this reason that Price remained skeptical throughout his caree r to the use of purified vitamin preparations, such as the irradiated ergosterol form of vitamin D.89 Price frequently referred to the undetermined factors, as well as the known fat-soluble vitamins, as \"activators,\" so as to emphasize their role as biological catalysts in the building and repairing of body tissues. The fact that these 87 Weston A. Price, \"Newer Knowledge of Calcium Metabolism in Health and Disease with Special Consideration of Calcification and Decalcification Processes, Including Focal Infection Phenomena,\" American Medical Association Journal 13 (December 1926): 1765-94. 88 Price, \"Some New Fundamentals for the Prevention of Dental Disease,\" 2. Later on, Price proposed the existence of a fat-soluble substance in butterfat that he termed \"activator X.\" He believed it could not be considered identical with vitamins A, D, or E. Chris Masterjohn has plausibly suggested that this \"activator X\" is, in fact, vitamin K2. See his \"On the Trail of the Elusive X-Factor: A Sixty-Two-Year-Old Mystery Finally Solved\" (11 August 2012), http://www.westonaprice.org/fat-soluble-activators/x-factor-is-vitamin-k2. 89 Price was not alone in this respect. See, e.g., Charles E. Bills, \"New Forms and Sources of Vitamin D,\" Journal of the American Medical Association 108, no. 1 (2 January 1937): 13-15. 147 \"activators\" appeared necessary for the development of the epithelial and calcified tissues was of special concern for a dentist such as Price, who was searching for ways to prevent tooth decay and periodontal disease. Price's experiments with laboratory animals and human subjects in the 1920s and early 1930s were similar to others being undertaken by dental researchers at that time. In one experiment, Price selected a group of about thirty children with very active dental caries from a poor district of Cleveland, where the parents had been \"seriously suffering from the industrial stress since the closing of the mills.\" In addition to whatever they ate at home, the children were given one especially well-reinforced though inexpensive meal, as designed by Price and prepared by the staff of a church. The children were given the meal six days a week for a period of five months. Preliminary examinations of each child included complete X-rays of all the teeth, chemical analysis of the saliva, a careful plotting of the position, size, and depth of all cavities, a record of height and weight, and even a record of school performance. These tests were repeated every four to six weeks for the period of the test. Like the diets devised by the Iowa, Forsyth, and Columbia groups, the meal designed by Price was rich in the protective foods, with a special emphasis on providing the fat-soluble vitamins. It consisted of tomato or orange juice; a teaspoonful of very high-vitamin cod liver oil and butter; soups made largely from bone marrow and meat, seafood, or organs as well as chopped vegetables; slightly sweetened cooked fruit; rolls made from freshly ground whole wheat and spread with more high-vitamin butter; and two glasses of whole milk. By Price's measurements, a single helping of this meal well exceeded the known daily requirement for minerals. Although the home meals and the home care of the teeth had not changed, almost all of the children experienced a cessation of tooth decay 148 under the nutritional program. Many of the teeth that had even decayed to the pulp built up a hard dentinal defensive wall within the pulp chamber.90 In his clinical work, Price also sought out the most extreme cases of active tooth decay in order to test his nutritional program. He recommended meal s to his carious patients that were low in sugars and refined starches, consisting of freshly ground whole grain breads and cereals, liberal quantities of seafood and animal organs, and whole milk for children as well as some adults. He also prescribed a small amount of high-vitamin butter and cod liver oil to augment the fat-soluble vitamin content of the diet. Price found by physical examination and X-ray that dental caries was completely controlled in well above 90 percent of the hundreds of patients he treated with his nutritional program. Moreover, he observed an improvement in immunity and health in general. His patients reported suffering less from flu and colds in the winter and early spring. Price came to believe, echoing researchers such as McCollum, Mellanby, and Howe, that the oral environment theory did not adequately explain susceptibility and immu nity to dental decay. He interpreted the accumulating evidence from his and others' experiments as strongly indicating that caries was caused by \"the absence of some essential factors from our modern program, rather than the presence of injurious factors.\"91 He was confident by the early 1930s that the fat-soluble \"activators\" and minerals were central in the prevention of tooth decay. Since modern processing methods produced foods low in these nutrients, Price naturally shifted his focus from dentistry to the food supply as a whole. His intended audience therefore started to widen, and he became more explicitly critical of sugary and 90 Weston A. Price, \"New Light on the Etiology and Control of Dental Caries,\" Journal of Dental Research 12, no. 3 (June 1932): 121-24; Price, Nutrition and Physical Degeneration, 260-61. 91 Price, Nutrition and Physical Degeneration, 5. 149 processed foods. Besides dental professionals, he began addressing milk dealers, cereal chemists, public health workers, pediatricians, and lay audiences.92 Like some of his contemporaries, Price decided to turn to \"dental anthropology\" for conclusive evidence in support of his views. As he pointedly argued in a 1933 article, It is strange that so little effort has been made to inquire of remaining primitive peoples as to their habits of living and the selection of foods as standards of comparison; instead, the research has been largely conducted by searching the disintegrated tooth for the causative factors, which, alas, are not there. It is as though we would find the nature of lightning by studying the tree that had been killed by the bolt.93 Price resorted to the study of \"remnants of primitive racial stocks\" as a result of his failure to find adequate caries-free \"control groups\" close at hand in the populations of the industrial countries. He sought out places, first and foremost, that had not yet been impacted by what he called \"the foods of modern commerce\"\u2014white sugar, refined flour and rice, jams, confections, canned goods, refined vegetable oils, and so on. This usually meant that he had to travel to isolated locations that were not easily accessible by highway, railroad, or ship port, and where the populace still relied almost totally on locally available foods. \"Physical isolation,\" Price pointed out, \"provides practically the only condition for compelling human 92 Weston A. Price, \"Some Means for Improving Life by Increasing Vitamin Content of Milk and Its Products,\" The Association Bulletin: International Association of Milk Dealers, no. 10 (29 January 1931); Weston A. Price, \"Why the Annual Pilgrimages to the Hospitals?\" Your Health (March 1930); idem, \"New Light on the Dependence of Human Life Upon Plant Minerals and Activators (Vitamins) with Particular Consideration of Cereal Foods\" (Read before American Association of Cereal Chemists, Chicago, Illinois, February 2, 1931); Weston A. Price, \"Calcium Metabolism Studies: A. The Raising of Serum Calcium by Topical Applications of Raw and Activated Cod Liver Oil. B. Disturbances Associated with the Active Dental Caries of Childhood and Pregnancy,\" American Journal of Diseases of Children 33 (January 1927): 78-95. 93 Weston A. Price, \"Why Dental Caries with Modern Civilizations? I. Field Studies in Primitive Loetschental Valley, Switzerland,\" Dental Digest 39 (March 1933): 94. 150 beings to live on natural foods.\"94 He relied heavily on the assistance of locally knowledgeable ministers, doctors, and colonial officials to gain access to these remote areas and interact with the people. Wherever he went, Price observed three groups: \"isolated primitives\" who still subsisted on their traditional foods; \"primitives\" who had recently come into contact with modern foods and had made them a major part of their diets; and the first generation of offspring of the \"primitive racial stocks\" whose parents consumed mostly modern foods. One crucial modern invention, the portable camera, allowed Price to make a visual record of the people he studied by taking thousands of photographs.95 In every location, he employed the same method of systemat ically examining and photographing the teeth of isolated and modernized \"primitives.\" He also recorded the primitive dietaries and took numerous samples of their foodstuffs, which he preserved and later analyzed for their vitamin and mineral content. He paid particular attention to the foods that the \"primitives\" considered especially valuable for maintaining health and for pregnant and nursing mothers. Taken as a whole, the number of places around the world to which Price ultimately journeyed was truly impressive, especially in light of the fact he financed his research entirely with the income from his own practice. He visited some locations that had already been studied by dental workers searching for a connection between diet and the teeth, as well as others that had not. He initially journeyed to remote villages in the high Alps in Switzerland in 1931 and 1932, where the diet was found to be made up largely of dairy products and whole rye used chiefly as rye bread, meat occasionally, and a little fruit and 94 Weston A. Price, \"Why Dental Caries with Modern Civilizations? II. Field Studies in Primitive Valais (Wallis) Districts, Switzerland,\" Dental Digest 39 (April 1933): 147. 95 His wife, Florence, was particularly skilled at tinting color plates of the black-and-white photographs they took, which greatly added vividness to the slide presentations Price and his colleagues used in educational lectures. He initially attempted to construct a portable X-ray device as well, but it proved to be impracticable. 151 vegetables that were restricted largely to the summertime. The diet of the residents of Switzerland in the lower plains country and cities consisted of \"modern foods,\" including large quantities of white flour products, canned goods, and sweetened goods such as jams and chocolate milk. The incidence of dental caries of the children in the isolated Swiss mountain valleys was found to be 4.6 percent of the teeth studied, whereas for the highly modernized communities, the incidence in children was from 24.7 to 29.8 percent.96 In 1932, Price studied the people of the Outer Hebrides, a group of islands off the west coast of Scotland. In those still living, as their ancestors had, very largely on oat products, a little barley, and abundant fish, lobsters, crabs, oysters, and clams, the incidence of caries was only 1.2 percent of the teeth examined. An important and highly relished article of the diet, considered especially important in the nutrition of children, was baked cod's heads stuffed with chopped cod's liver and oatmeal. In those living in the ports in the Outer Hebrides with access to modern foods, Price found 30 percent of the teeth examined were to be carious.97 The following year, Price studied the Eskimos of western Alaska and the Indians of the far north of Canada and along the Pacific coast. The native diet of the Eskimos consisted of liberal amounts of organs and other special tissues of the large animal life of the sea, as well as fish and fish eggs. Caribou meat and organs were sometimes available, as were a few berries and plant foods that were gathered in summer and stored. The diet of the Indians living in the Rocky Mountain Range in the north of Canada was composed mostly of moose 96 Weston A. Price, \"Why Dental Caries with Modern Civilizations? I. Field Studies in Primitive Loetschental Valley, Switzerland,\" Dental Digest 39 (March 1933): 94-100; Weston A. Price, \"Why Dental Caries with Modern Civilizations? II. Field Studies in Primitive Valais (Wallis) Districts, Switzerland,\" Dental Digest 39 (April 1933): 147-52; and Weston A. Price, \"Why Dental Caries with Modern Civilizations? III. Field Studies in Modernized St. Moritz and Herisau, Switzerland,\" Dental Digest 39 (May 1933): 205-09. 97 Weston A. Price, \"Why Dental Caries with Modern Civilizations? IV. Field Studies in Primitive and Modern Outer Hebrides, Scotland,\" Dental Digest 39 (June 1933): 225-29. 152 and caribou, as well as plants during the summer months. They showed a particularly avidity for the organs and marrow. Caries attacked only 0.09 percent of the teeth of isolated Eskimos and 0.16 percent of the teeth of the Indians in the far north of Canada. For the groups of Indians living near the Hudson Bay trading posts and along the Pacific Coast, in contact with white flour and sugar, the incidence of dental caries was about 25 percent.98 In 1934, Price ventured to several archipelagos in the South Pacific and in the islands north of Australia to study the Melanesians and Polynesians. The foods used by the isolated islanders consisted of large quantities of fish and shellfish, eaten with the various native plant foods, vegetables, and fruit. The incidence of dental caries among the Melanesians who were sufficiently isolated to be dependent on native foods was 0.38 percen t of the teeth studied, while for those living at the ports using trade foods in considerable quantity, 29 percent of the teeth had been attacked by tooth decay. For the isolated Polynesians still living on native foods, the incidence of caries was a mere 0.32 percen t of the teeth. In the groups in contact with modern civilization, 21.9 percent of the teeth showed decay. Price discovered that the almost universal rule for exchange at the few large ports required white flour and sugar products, 90 percen t, and clothing, 10 percent, as payment for the local products purchased.99 98 Weston A. Price, \"Why Dental Caries with Modern Civilization? VII. Field Studies of Modernized American Indians in Ontario, Manitoba, and New York,\" Dental Digest 40 (February 1934): 52-58; Weston A. Price, \"Why Dental Caries with Modern Civilizations? VIII. Field Studies of Modernized Indians in Twenty Communities of the Canadian and Alaskan Pacific Coast,\" The Dental Digest 40 (March 1934): 81-84; Weston A. Price \"Why Dental Caries with Modern Civilizations? IX. Field Studies among Primitive Indians in Northern Canada,\" The Dental Digest 40 (April 1934): 130-34; Weston A. Price, \"Why Dental Caries with Modern Civilizations? X. Field Studies among Primitive and Modernized Eskimos of Alaska,\" Dental Digest 40 (June 1934): 210-13. 99 Weston A. Price, \"Why Dental Caries with Modern Civilizations? 12. Field Studies Among the Polynesians and Melanesians of the South Sea Islands,\" Dental Digest 41 (May 1935): 161-64; Weston A. Price, \"Why Dental Caries with Modern Civilization? 13. Field Studies Among the Polynesians and Melanesians of the South Sea Islands,\" Dental Digest 41 (June 1935): 191-94. 153 In early 1935, Price made investigations of Seminole Indians living in southern Florida for comparison with studies of the skull material available in various museums that had been taken from burial mounds. The Seminoles living in relative isolation in the cypress swamps ate fish, wild game, birds, alligators, turtles, shellfish, roots, and fruits. Owing to the activities of white hunters and the draining of the swamps for cultivation, some modern foods had to be purchased. The incidence of caries in this mostly isolated group was 4 percen t, higher than the virtually caries-free skulls Price examined. In the far larger group of Indians living along roads and in the vicinity of Miami, who lived very largely on modern foods, 40 percent of all teeth examined had been attacked by decay.100 Later in 1935, Price traveled over a large swathe of eastern and central Africa to study the groups living on widely differing diets. His studies in Africa were directed particularly toward the possible sources of fat-soluble vitamins. The livestock-raising tribes, such as the Masai in Kenya and the Muhima in Uganda, lived largely on milk, blood, and meat . For the people inhabiting the vicinity of the Upper Nile, Lake Victoria, and other inland lakes, freshwater fish were consumed, including all of the organs as well as the muscle meat. Some groups, such as the Nuers in Sudan, consumed fish in addition to a diet of milk and meat, while other groups, such at the Dinka, supplemented fish with cultivated cereals, tubers, fruits, and leaves. Some groups Price studied, such as the Kikuyu in Kenya, lived almost entirely on vegetable foods supplemen ted with small amounts of animal products. Price found that the African groups consuming the greatest amounts of foods high in fat-soluble vitamins had the highest immunity to dental caries, whereas those consuming the least had the greatest amount of dental caries. Those groups using the fat-soluble 100 Weston A. Price, \"Studies of Relationships Between Nutritional Deficiencies and (a) Facial and Dental Arch Deformities and (b) Loss of Immunity to Dental Caries Among South Sea Islanders and Florida Indians,\" Dental Cosmos 77, no. 11 (November 1935): 1033-45. 154 activators in liberal quantity, such as the Nuers, had not more than 0.5 percent of the teeth attacked by caries, while those using them less liberally, such as the agrarian Kikuyu, had up to 12 percent of the teeth attacked by caries. Price also examined the teeth of workers on plantations and children in mission schools who lived mostly on modern foods, and he found much higher rates of decay, up to about 50 percent.101 To study yet more \"races\" living in different environments on differen t foods, Price traveled to Australia, the Torres Strait Islands, and New Zealand in 1936. Along the coast of Australia, the Aborigines consumed the abundant fish and seafood, together with wild plants and land animals. In the interior, they ate large quantities of wild animal life, particularly wallaby, kangaroo, small animals, rodents, and insects. Price emphasized that all of the edible parts, including the walls of the viscera and internal organs, were eaten. The various native groups inhabiting the Torres Strait islands north of Australia consumed a great variety of large and small fish and shellfish, as well as taro, bananas, and other tropical crops. The native Maori along the coast of New Zealand that Price observed ate large quantities of seafood and birds, seaweeds, fern root, as well as cultivated vegetables and fruits. In Australia and New Zealand, the caries rate of the groups still subsisting on the primitive diets was exceedingly low, and the Torres Strait islanders had virtually none whatsoever. Those located at mission schools, government stations, and trading ports, however, presented Price with caries rates sometimes in excess of 50 percent.102 101 Weston A. Price, \"Why Dental Caries with Modern Civilizations? 14. Field Studies in Kenya, Uganda, Belgian Congo, Sudan, and Egypt,\" Dental Digest 42 (March 1936): 89-96; Weston A. Price, \"Field Studies Among Some African Tribes on the Relation of Their Nutrition to the Incidence of Dental Caries and Dental Arch Deformities,\" Journal of the American Dental Association 23 (May 1936): 876-90. 102 Weston A. Price, \"Changes in Facial and Dental Arch Form and Caries Immunity in Native Groups in Australia and New Zealand Following the Adoption of Modernized Foods,\" Dental Items of Interest 60 (February 1938): 107-25. 155 For his final voyage, Price traveled to South America in 1937 to study the isolated inhabitants along the Peruvian coast, the high Andean Plateau, the eastern watershed of the Andes, and the Amazo n Basin. As with his African expedition, he surveyed primitive groups living in widely differing environments subsisting on highly varied diets. One of the principal purposes of his trip was to study the effects of the Humboldt Current, with its immen sely abundant supply of marine life, on the ancient cultures that were buried along the coast. He also traveled to several locations in the Andean highlands to compare the dentition of the agriculturists living in isolation and those that had come into contact with modern foods. The isolated groups subsisted mostly on potatoes, maize, and beans, but Price also emphasized the inclusion of llama meat and guinea pigs into their diets, as well as kelp and fish eggs, which were imported from the coast in large amounts. Price was most impressed by the dentition of the Indian groups living at the lower altitudes in the Amazon Basin, who lived on yucca, fish, small birds and game, and fruits.103 Through most of the 1930s, Price published reports of his field studies in professional dental journals. These reports garnered positive attention and requests from patients and medical and dental clinicians for a concise statemen t of what he thought useful as preventative procedures. So, in an effort to provide a synopsis and interpretation of his years of research in a form that was accessible to the non-specialist, Price wrote his magnum opus, Nutrition and Physical Degeneration: A Comparison of Primitive and Modern Diets and Their Defects (1939). He devoted most of the chapters of the book to summarizing each of his voyages, in order to clearly demonstrate that \"the greater success of the primitives in 103 Weston A. Price, \"Nature's Intelligent American Indians,\" America Indigena 3, no. 3 (July 1943): 253-64. 156 meet ing Nature's laws has been based primarily on dietary procedures.\"104 In succeeding chapters, Price evaluated the diets of the isolated primitive groups in terms of what was known about nutrition at that time, and he then laid out suggestions for applying the equivalents of \"primitive nutritional programs\" to modern families. Price argued that the foods that allowed people of every \"race\" and environment to be healthy were relatively unprocessed and nutrient-dense. He emphasized the universal consumption of animal fats carrying the fat-soluble \"activators\"\u2014organs, dairy products, fish, seafood, and insects\u2014as well as whole grains, tubers, vegetables, and fruit rich in minerals and vitamins.105 From the samples of native foods that he sent or took home with him to analyze, Price found that the diets of the primitive groups that showed a very high immu nity to dental caries and freedom from other degenerative processes contained at least four times the phosphorus, calcium, magnesium, and iron as the recommended daily intakes of his day, which a substantial proportion of the population in the United States wasn't even attaining. Most notably, all the primitive diets he examined contained at least ten times the fat-soluble vitamins than the recommended daily intake. They also provided a larger amount of the known water-soluble vitamins than \"the displacing modern diets.\"106 104 Weston A. Price, Nutrition and Physical Degeneration: A Comparison of Primitive and Modern Diets and Their Defects (New York: Hoeber, 1939), 229. 105 Price believed that a strictly vegetarian dietary regimen, which had been particularly popular among some natural foods advocates since the mid-nineteenth century, was harmful. Not one \"primitive\" group he studied was totally vegetarian, and those who relied heavily on vegetable foods made great efforts to obtain animal products whenever possible. The more that an isolated primitive group relied on vegetable matter and cereals, Price repeatedly observed, the more dental decay and malocclusion they had, even if it was negligible compared to modernized groups. See Nutrition and Physical Degeneration, 278-79. Price also rejected the \"acid-base balance\" theory that had become popular by the 1920s and 1930s: see Weston A. Price, \"Acid-Base Balance of Diets which Produce Immunity to Dental Caries Among the South Sea Islanders and Other Primitive Races,\" Dental Cosmos 77, no. 9 (September 1935): 841-46. 106 Price, Nutrition and Physical Degeneration, 246-48. 157 Seen in the wider context of dental research at that time, Price's most original contribution was not his argument that diet could prevent and arrest caries, nor his use of \"remnants of primitive racial stocks\" to substantiate his claims. Although the number of groups around the world he studied was indeed large, he was mainly confirming what most dental researchers had already accepted as a given. Rather, Price's work was most unique in the emphasis he placed on the relationship between nutrition and the structure of the dental arch, jaw, face, and physique in general. Although less easily quantifiable than caries incidence, in hundreds of photographs Price vividly captured the degenerative process in oral and facial structure in the first generation of offspring raised on modern foods. All the isolated primitive groups, regardless of genetic background, presented Price with jaws and dental arches wide enough to accommodate rows of well-arran ged and straight teeth. Children raised on modern foods, aside from suffering from high caries rates, also commonly had narrowed dental arches and underdeveloped facial structure, resulting in the crowding and malocclusion of teeth. This kind of structural deformity was akin to what May Mellanby observed in her experiments on puppies deprived of vitamins A and D through the gestational and nursing periods. No primitive groups that Price studied had entirely \"perfect\" dental arch structure, but he found the lowest percen tage of irregularities in the position of the teeth in the primitive groups living very largely on dairy products, meat, and marine life, while the groups with highest rates of irregularities tended to be agriculturists.107 For Price, modern concoctions made with refined sugar, flour, and vegetable oils were the enemies lurking in the distance, forming the advanced guard of the assault on \"isolated primitives,\" as \"modern civilization\" spread its degenerative tentacles via outpost 107 Weston A. Price, \"Field Studies Among Some African Tribes on the Relation of Their Nutrition to the Incidence of Dental Caries and Dental Arch Deformities,\" Journal of the American Dental Association 23 (May 1936): 884. 158 trade, missionary settlements, and colonial ventures. These modern foods not only destroyed teeth, but faces and physiques as well. Because mothers who had long consumed modern foods did not have adequate stores of fat-soluble \"activators\" and minerals during pregnancy and the nursing period, Price argued, their children were not only more susceptible to tooth decay, but their whole physical makeup was compromised.108 He lamented the fact that mothers in modern society were rarely advised to go on special diets high in nutrient-dense foods before and during pregnancy and lactation\u2014a nearly universal practice that he had observed among \"primitives.\" Price also repeatedly emphasized that childbirth was quicker and less dangerous among primitive mothers. Modern fathers, too, were censured. Citing animal experiments to support his view, Price suggested that nutrition could influence the soundness of the genetic material passed on paternally.109 He even criticized modern production methods for certain \"protective\" foods, such as butter. Dairymen would frequently feed cows rations high in cottonseed meal and cereals to produce a hard butter, which shipped and stored better. Cows feeding on rapidly growing green pasture, on the other hand, produce a softer, yellow butter with poor shipping qualities but a much higher concentration of the fat-soluble \"activators.\"110 Price's efforts to repeatedly demonstrate that deformation of facial and dental arch structure occurred regardless of \"racial stock\" was rooted in the fact that one of the most widely accepted explanations for the prevalence of this anatomical pattern in \"modern civilization\" was increasing racial intermixture. Price, an enthusiast of Mendelian genetics, appears to have been receptive to this viewpoint early on in his career. But he discarded it\u2014and his belief in eugenics\u2014as he grew increasingly convinced that nutrition was the 108 Price, Nutrition and Physical Degeneration, 395-421. 109 Price, Nutrition and Physical Degeneration, 342-49. 110 Ibid., 291. 159 deciding variable. Even though Price drew heavily on the physical anthropology of his day and persisted in categorizing the groups he studied by \"racial type,\" he also emphasized that the characteristics of the degenerative process caused by modern diets were similar for all humans. The conclusion he drew from his discovery of the universality of this process was in fact quite optimistic: facial and dental arch deformities were the result of \"intercepted heredity,\" as he termed it, not of heredity itself. But Price was not just concerned about dental and physical deterioration. Particularly as he neared the end of his career, he became increasingly strident in blaming modern foods for mental, moral, and social degeneration as well. Price made the case that \"delinquents are primarily defectives, produced by parents one or both of whom was nutritionally unprepared for reproduction.\"111 In a chapter entitled \"Physical, Mental, and Moral Degeneration\" in Nutrition and Physical Degeneration, Price included pictures of murderers from newspapers and photographs that he took of institutionalized boys in Cleveland showing abnormal facial and dental development. Price also cited statistics noting the alarming size and expense of the jailed, institutionalized, and hospitalized proportion of the population in the industrialized countries.112 To rescue modern civilization from this \"abyss,\" as he described it, Price argued that expectant parents needed to be given the information necessary for proper conception and easy access to the foods required. Moreover, society a whole had to \"disdain to capitalize on Nature's fundamental instincts for selfish gain.\"113 In contrast to his excoriations of \"modern civilization,\" Price's writings on the \"isolated primitives\" presented a picture in which proper nutrition, physical excellence, 111 Weston A. Price, \"The Causes of Juvenile Delinquency and Modern Degeneration,\" Journal of the American Academy of Applied Nutrition (Summer 1947): 34. 112 Price, Nutrition and Physical Degeneration, 353-81. 113 Price, \"The Causes of Juvenile Delinquency and Modern Degeneration,\" 37. 160 communal solidarity, and moral uprightness were all connected. In his studies in the picturesque high Alps, for example, he noted that in the summer, Swiss villagers would place a bowl of exceptionally high-vitamin butter, produced from cows grazing on the summe r pasture, on the church alter and light a wick in it. Neither the \"two thousand inhabitants of the Loetschental Valley in Switzerland\" nor the \"Ruanda tribes estimated to number two and half millions\" required any prisons.114 Among Australian Aborigines, the difficult rites of passage to manhood for boys tested their ability to successfully obtain foods rich in fat-soluble vitamins; elders were accorded a great deal of respect for their knowledge of good hunting grounds. The ancient Chimu of coastal Peru, whose skulls revealed not one defect ive tooth or dental arch of the many hundreds that Price examined, were buried with the nets they used to harvest fish and seafood from the incredibly rich Humboldt Current in the Pacific Ocean.115 In a chapter of Nutrition and Physical Degeneration indicatively titled \"Application of Primitive Wisdom,\" Price summed up one of the profound lessons he learned from his journeys: As I have sojourned among members of primitive racial stocks in several parts of the world, I have been deeply impressed with their fine personalities, and strong characters.... Fundamentally they are spiritual and have a devout reverence for an all-powerful, all-pervading power which not only protects and provides for them, but accep ts them as a part of that great encompassing soul if they obey Nature's laws.116 In other words, the primitives' methods for obtaining nourishment and physical wellbeing were inextricably connected to their customs and spiritual beliefs in a virtuous whole. Given that Price was already conscious of the fact that his nutrition-based explanation for dental arch and facial degeneration was unorthodox, one wonders why he chose to stretch his argument even further into the realm of moral and social degeneration. 114 Price, Nutrition and Physical Degeneration, 360-62. 115 Price, Nutrition and Physical Degeneration, 217-39. 116 Ibid., 419. 161 One likely reason is that his journeys merely reinforced convictions that he had developed earlier in order to reconcile his deeply religious upbringing with the quickening pace of scientific discovery. From his public addresses in the 1920s and 1930s, it appears that Price underwent a metaphysical reorientation of sorts. The newer knowledge of nutrition played a crucial role in this shift. Price was an active member of the Epworth-Euclid United Methodist Church in Cleveland. In his lectures to his fellow congregants, Price showed an acute awareness of the strain that the traditional Christian worldview was under by modern theories. As he stated in one of his talks in 1928, \"This is not the same world as you were born into and I was born into. We find ourselves even in this decade required to re-orient ourselves.\" The Bible story, Price argued, \"was written at a time when it was not known that all of the things that we think of as knowledge were different expressions of the same thing\u2014truth. Consequently, everything that was understood was natural and everything that was not understood was supernatural.\"117 He emphasized that the supernatural realm was shrinking due to the quickening pace of scientific discoveries. One of the most thrilling inventions for Price was the spectroscope, which revealed that all the stars in the universe, including the sun, are made of the same elements as are present on Earth and in the human body. The spectroscope could also detect forms of radiant energy not visible to the human eye that could be absorbed by chemicals in animals and stored in the fatty tissues as a \"vitalizing force\"\u2014in other words, the \"activators.\" Price felt that, due to these sorts of discoveries, the modern understanding of the natural realm was becoming more unified and coherent. \"Mechanics and sound and heat and 117 Weston A. Price, Lecture at Pilgrim Congregational Church, 13 November 1928, 1-2, File W229, Price-Pottenger Nutrition Foundation, La Mesa, CA. 162 electricity and magnetism and light were all though of as different expressions of different things,\" Price stated in a 1928 speech, but \"now we know they are different expressions of the same thing.\" These leaps in knowledge, in turn, influenced Price's thinking about nutrition. Humans, like other omnivores, obtain energy and matter from both plant and animal life, but all animals ultimately rely on plants and the soil\u2014the interface between the nonliving chemical elements of the Earth and the radiant energy of the sun. As he told his fellow churchgoers in 1928: Indeed, it is only the last twenty years perhaps that we have come to recognize through great research work that the energy that comes from the sun and from the stars and from our electric lights and any other source of artificial light is an expression as energy of ultimate units which may come together in form and be matter or material.... The dinner we have eaten tonight was a part of the sun but a few months ago. We have actually eaten tonight a substance that was in the sun and came to the earth as energy.118 For Price, then, everything was \"ecologically\" interrelated: the sun, the soil, animal life, and the human body. He viewed nutrition as the way that all life obtains the radiant energy and elemen tal matter of the universe, and in this way all of existence was connected. Moreover, in Price's view this emerg ing picture of life and universal forces had spiritual ramifications. As he argued in one of his talks, perhaps startling fellow churchgoers, \"I believe that science has done more to help us believe in God in the last few years than religion has done within the last fifty years.\" Price was pained that campaigns against teaching evolution in public schools were succeeding in the 1920s, and he repeatedly criticized Christians who refused to embrace it despite the preponderance of evidence in its favor.119 He also reminded his audiences that chemicals produced by organs such as the 118 Ibid., 3-4. 119 Price, Lecture at Pilgrim Congregational Church, 13 November 1928, 5-7. It appears that Price accepted Darwinian evolution early in life: \"My father had a farm, and I am thankful that I was brought up on a country farm. But I am particularly thankful that I had a father and mother who were not afraid to let me have any book I wanted....I remember a young preacher of the district saying at 163 thyroid, pituitary, and adrenal glands greatly influence personality and development, so that, for instance, some animals evolved to produce more \"flight\" chemicals when in the presence of danger while others produce \"fight\" ones in similar circumstances.120 Along related lines, Price maintained that the basic urges to eat, protect one's young, and procreate drove human behavior more than \"modern civilization\" was generally willing to grant. He never tried to explain why life or speciation came about, but he clearly believed that Nature had established laws via evolution that each species has to follow to avoid degeneration. What was becoming abundantly clear to Price was that \"the most important thing that we can do is to take our proper place and be humble and willing to spend our lives putting ourselves in harmo ny to the laws that govern our universe.\"121 The line dividing God from Nature in Price's writings is not always clear\u2014in a sense, for him, Nature was God. In other words, he resolved the tension in his mind between the natural and supernatural realms by completely collapsing the latter. At times, moreover, he professed a belief that the material and spiritual realms could be more closely united if \"Nature's laws\" were obeyed. For Price, the Bible was at best only metaphorically true and at worst preventing Christians from accepting the truth. Striving for a spiritual afterlife in Heaven, if it involved rejecting scientific truth, was the ultimate act of selfishness, a guarantee that a more harmo niously united material and spiritual world would never be obtained. His vaguely materialist and the supper table one Sunday evening, 'Mr. Price, I read a new book that I would have given a thousand dollars if I had not seen and that is The Origin of the Species by a man named Darwin.' Father said, 'Why, young man, are you afraid of the truth?' 'No, but if that is true then there is nothing in religion.' Father said, 'I will take a chance on truth taking care of itself.' I had that book and father bought it for me. It was worth more than a thousand dollars to that man that he read that book.\" 120 \"The Newer Knowledge of the Nature of Personality,\" Lecture by Dr. Weston A. Price at Euclid-Epworth Church, 26 January 1929, File W216, Price-Pottenger Nutrition Foundation, La Mesa, CA. 121 \"Evidence of an Orderly Universe Controlled by Universal Law,\" Lecture by Dr. Weston A. Price at Euclid-Epworth Church, 25 November 1928, 12, File W222, Price-Pottenger Nutrition Foundation, La Mesa, CA. 164 pantheist version of Methodism was obviously unconventional, but he seems to have felt confidently liberated by it. As he counseled his fellow parishioners, \"Don't be afraid, there isn't any conflict at all between religion and science. Science is revealing God's laws.\"122 Therefore, when Price extolled the \"isolated primitives\" during his journeys in the 1930s, he already possessed a type of universal humanism that was not entirely relativistic. The more that primitives obeyed \"Nature's laws,\" whether or not they \"scientifically\" understood why they did so, the closer they were to a heaven on Earth, and it would show in their dental, physical, mental, and moral excellence. Price's nutritional reductionism was clearly extreme, and his arguments likely would have had a more lasting impact on the dental and medical research commu nity had he limited his already-broad claims and focused more on substantiating the quantitative aspects of his research.123 However justified Price was to be impressed by the primitive groups he encountered, the highly romanticized picture he presented lacked the depth and nuance of the more careful anthropological studies being conducted around that time. And in his field reports and Nutrition and Physical Degeneration, Price's intriguing dental and nutritional observations all too often shaded into social critique, moralization, and amateurish ethnography. Harvey Sapolsky's observation that \"Innovators can be expected to be somewhat monomaniacal about their innovations\" aptly describes Price's outlook.124 122 \"The Newer Knowledge of the Nature of Personality,\" Lecture by Dr. Weston A. Price at Euclid-Epworth Church, 26 January 1929, 21, File W216, Price-Pottenger Nutrition Foundation, La Mesa, CA. 123 For contemporary critiques of Price's work on these grounds, see Warren T. Vaughn, \"Effects of Dietary Deficiencies,\" Scientific Monthly 50, No. 5 (1940): 463-464; and \"Nutrition and Physical Degeneration: A Comparison of Primitive and Modern Diets and Their Effects,\" Journal of the American Medical Association 114, no. 26 (29 June 1940): 2589. 124 Harvey M. Sapolsky, \"Social Science Views of a Controversy in Science and Politics,\" American Journal of Clinical Nutrition 22, no. 10 (October 1969): 1405. 165 Nevertheless, Price's research drove home a message that was becoming hard to refute by the 1930s: that food was at the center of the dental degeneration afflicting industrialized populations. However, major disagreement persisted within the dental profession over the extent to which diet prevented tooth decay systemat ically, by affecting metabolism, and locally, by affecting the oral environment. It was also unclear how much of a role nutrition played in building teeth and periodontal tissues resistant to decay. This debate, in turn, touched on the larger question of what the proper boundaries of the dental profession should be. Should dentists be concerned only with oral hygiene and repairing decayed and misaligned teeth, or should advising on diet and nutrition be within their ambit as well? Was it the responsibility of the dentists to wade into the debate over the merits and flaws of the modern food system? Dentists, Not Dieticians: The Eclipse of Nutritional Dentistry Between about 1920 and 1940, as we have seen, much of the foundational research on the role of nutritional factors in the prevention of dental caries was carried out. The findings prompted much speculation and debate about the relative importance of individual nutrients, as well as the mechanisms responsible for immunity to caries and normal dental structures. Nonetheless, by the end of this first flush of studies, a sizeable contingent in the dental research community was confident enough to claim that nutrition had transformed the field. In 1936, for example, Percy Howe stated, \"While there is much that is obscure concerning dental conditions and, while the need for protracted investigation is self-evident, 166 sufficient evidence has been disclosed to indicate that the dental disaster of today is largely due to vitamin and mineral deficiencies.\"125 The defenders of the \"theory of mouth hygiene\" were not so easily convinced. It should be reemphasized that both the oral environment and systemat ic-metabolic viewpoints accep ted that food played a role in the etiology of dental caries. But the oral environment theory focused on the local effects of diet, while the systemat ic theory was concerned with nutrition. Diet deals with the physical or chemical effect of the food on the surface of the teeth, whereas nutrition deals rather with food after it has passed from the mouth into the alimentary canal and is digested. One of the primary difficulties in determining the relative importance of local and systemic effects was that the diets found in clinical trials and field studies to produce immunity to caries\u2014high in dairy products, eggs, organ meat, vegetables, fruits, and whole-grain cereals\u2014were also low in refined sugar and flour. Contariwise, a diet high in refined sugar and flour would tend to reduce the consumption of the protective foods. As Irwin Mandel of Columbia University put it, \"When one has a situation in which the so-called protective foods are introduced, often at the expense of the highly cariogenic foods, it is difficult to determine whether it is the absence of the cariogenic foods or the presence of the protective foods, that is responsible for the desired reduction in caries.\"126 In practical terms, that is, neither the cariogenic foods that affect the local oral environment nor the protective foods that affect the metabolism in general can be very easily made into independent variables, even under closely controlled conditions. 125 Percy R. Howe, \"The Relation of Avitaminosis to Oral Pathology,\" Symposium on Nutrition and the Deficiency Diseases at the Tercentenary Session of the Harvard Medical School (14 September 1936): 3. 126 Irwin D. Mandel, \"Effects of Dietary Modifications on Caries in Humans,\" Journal of Dental Research 49, Supplement to no. 6 (1970): 1201. 167 A good illustration of this problem can be seen in a series of experiments carried out by Russell Bunting and his coworkers in the School of Dentistry at the University of Michigan, Ann Arbor. In the late 1920s, Bunting and his team conducted a study on orphanage children to test the effects of a well-fortified, low-sugar diet in the reduction of dental caries, as well as an antibacterial mouthwash. The mouthwash resulted in no apparent reduction in caries rates, while the dietary improvements halted active caries in 75 to 80 percen t of the children. It would therefore appear that the Michigan group's research supported the other nutritional intervention studies being carried out in that period, and Bunting himself was initially enthusiastic about the use of diet to improve caries resistance.127 But the group ultimately interpreted their findings as supporting a strict interpretation of the oral environment theory, not the nutritional-systemat ic theory. In following studies, Bunting and his colleague Martha Koehne observed an increas e in susceptibility to dental caries when large amounts of sucrose as candy or table sugar were added to the diet of orphanage children. Befo re the experimental period, 70 to 80 percent of the children had no active dental caries, which represented a much lower dental caries incidence than in the average group of children of similar age. Bunting and Koehne believed that the factors responsible for the low incidence of decay were all related to the local oral environment: low sugar, uniformity of diet, and hard fibrous fruits after meal s. 51 of the orphanage children were given approximately three pounds of candy a week for five months. At the end of the test period, 44 percent showed evidence of active dental caries. But the experiment was criticized for several reasons. The orphanage diet was admittedly inadequate in calcium, phosphorus, and vitamin D, and contained no milk or butter. If the 127 Letter (10 May 1938) from Russell W. Bunting to May Mellanby, PP/MEL/F.7, Wellcome Library Archives and Manuscripts, London, UK. 168 regular diet consumption decreased because of the caloric increase from large quantities of sugar, as would be expected, the overall nutritional adequacy would be reduced even further. Moreover, the investigators later admitted that despite all reasonable precautions, an effective bootlegging system soon developed to provide the control children with an amount of candy that could not be estimated. The children's ingenuity essentially destroyed the control group.128 Bunting and his coworkers also studied the effect of sugar ingestion on the dental caries incidence of 23 metabolically normal girls, six to thirteen years old, in the University Hospital at Ann Arbor. Of the 14 who received the basal diet only, which was believed to be adequate nutritionally, nine had arrested tooth decay while five had very few carious lesions. Of the 13 girls who received the basal diet and 100 grams of sucrose, nine had extensive active carious lesions and four had no active decay. Since the researchers believed that the subjects ate the usual portions of the hospital diet in addition to the candy, they stated that no reduction in the intake of nutrients could be detected. But the calcium retention of both groups was poor, probably because the girls fed the \"nutritionally adequate\" basal diet did not recei ve cod liver oil or sunlamp radiation, even though they were hospitalized for extended periods. In any case, the Michigan group became increasingly convinced as the 1930s progressed that the claims that nutrition could increas e resistance of the teeth to caries were mostly groundless. The fact that dental caries was completely arres ted in a large majority who received a diet that was low in calcium, phosphorus, and vitamin D, but also low in sugar, as Bunting argued, \"does not coincide with many present-day theories regarding 128 Philip Jay, \"The Problem of Dental Caries with Relation to Bacteria and Diet,\" Journal of Pediatrics 8, no. 6 (June 1936): 73 169 dietary control of dental disease.\"129 Bunting and his coworkers concluded that the determining factor in caries susceptibility, aside from a very small percentage of people who possess inherited immu nity, was the consumption of sugar. To strengthen this viewpoint, Faith Hadley of the Michigan group refined the technique for estimating the lactobacillus acidophilus count in the saliva, which had previously been somewhat unreliable. Even though the group granted that a causal relationship between lactobacilli and dental caries had not yet been found, they nevertheless observed that an increas e in the lactobacillus content of the saliva was followed by active dental caries within a year in 85 to 90 percent of cases. \"If the lactobacilli are numerous,\" Michigan researcher Philip Jay stated, \"cari es will invariably follow, regardless of the quality of the tooth structure.\"130 Bunting made a similar argument in a 1933 piece titled \"Facts and Fancies in Our Concepts of Dental Caries.\" He granted that the rate and extent of tooth decay \"depend somewhat on the integrity and quality of the enamel substance,\" but he maintained that it had much more to do with \"the intensity of the attacking force and the environmental conditions under which it operates.\" Bunting further contended that the belief that susceptibility to dental caries is determined or largely controlled by the quality or resistance of the tooth \"is based solely on hypothetical opinion unsupported by scientific experimentation or evidence and is definitely refuted by so great a preponderance of clinical and experimental observations that it can no longer be considered tenable.\" He concluded that \"there is practical agreement among the students of the subject that all teeth, irrespective 129 Russell W. Bunting, \"Bacteriological, Chemical, and Nutritional Studies of Dental Caries by the Michigan Research Group: A Summary,\" Journal of Dental Research 14 (1934): 103. 130 Philip Jay, \"The Problem of Dental Caries with Relation to Bacteria and Diet,\" Journal of Pediatrics 8, no. 6 (June 1936): 733. 170 of their quality, are susceptible, in varying degrees, to dental caries and that the forces which determine this disease are resident not in the tooth itself, but in its environment.\"131 Bunting was correct in a very general sense, in that nobody in the nutrition and dental research fields by that time denied the fact that the local cause of caries was chemical and bacteriological in nature, and that carbohydrate-containing foods were the exciting factor in the decay process.132 But there was hardly the unanimity of opinion that he claimed regarding the importance of predisposing factors. Mellanby and her supporters, for example, critiqued the Michigan group for failing to differen tiate between gross and microscopic hypoplasia in their dismissal of any strong correlation between tooth structure and caries.133 But the conflict wasn't confined only to the Michigan and Mellanby groups. C. D. Mars hall Day and H. J. Sedwick of the University of Rochester conducted experiments on institutionalized teenagers that showed no beneficial effect on caries rates or the resistance of newly erupted teeth by the addition of a daily vitamin A and D concentrate to the diet.134 But as critics of the study pointed out, Day and Sedwick did not closely replicate the tests that showed a positive result from vitamin D. The period of tooth development in the experimental group had already largely passed, and the dosage of vitamins in the study was truly massive. The amount of vitamin D given daily was moderately toxic, and probably decreased, rather than increased, calcium retention. Other investigators, on the other hand, reaffirmed Mellanby's experiments showing that deficient diets and ensuing defective 131 Russell W. Bunting, \"Facts and Fallacies in Our Concepts of Dental Caries,\" Journal of the American Dental Association 20 (1933): 773. 132 E. V. McCollum, \"The Diet in Relation to Dental Caries,\" in Dental Caries (Philadelphia: University of Pennsylvania Press, 1941), 45. 133 See, e.g., Martha Koehne, \"Relation of Diet to Oral Health,\" Journal of the American Dental Association 25 (November 1938): 1767-80. 134 C. D. Marshall Day and H. J. Sedwick, \"The Fat-Soluble Vitamins and Dental Caries in Children,\" Journal of Nutrition 8, no. 3 (1934): 309-28. 171 nutrition resulted in formation of teeth more susceptible to decalcifying acids.135 John Mars hall of the University of California College of Dentistry in San Francisco, for instance, found that carious lesions could be produced more easily in the second and third generation of experimental animals raised on high-sugar, deficient diets than the first generation.136 Additionally, Mellanby, McCollum, and other nutrition researchers never argued that well-formed teeth showing no hypoplasia were completely immune to caries regardless of diet, nor did they argue that imperfectly formed teeth could not remain caries-free. Price's field studies, for example, clearly showed that \"primitive\" groups with putatively well-formed teeth suffered greatly from caries after adopting a modern diet high in refined sugar and flour. The principal area of contention was over the extent to which nutrition could influence susceptibility to caries and periodontal disease after tooth eruption. The post-eruptive hardening of enamel suggested that even if no cellular activity occurred, the process of diffusion and ion exchange through saliva or extracellular fluid did. The many studies by Mellanby, Boyd, Price, and other researchers also suggested that nutrition influenced decay susceptibility whether or not dentin underwent \"protective metamorphosis\" in response to carious attack. This was observed even when the dentin was not in direct contact with the abrasive action of foods and the cleansing action of saliva, which was the mech anism of dentinal hardening proposed by the proponents of the strict interpretation of the oral environment theory.137 As the above discussion of Price and Boyd's work indicated, various methods for determining chemical differences in the saliva, blood, and extracellular fluid of caries- 135 E.g., J. J. Enright, H. E. Friesell, and M. O. Trescher, \"Studies of the Cause and Nature of Dental Caries,\" Journal of Dental Research 12, no. 5 (1932): 759-851. 136 John A. Marshall, \"Dental Caries,\" Physiological Reviews 19, no. 3 (July 1939): 405. 137 Charles B\u00f6decker, \"Pathology of Dental Caries,\" in A Survey of the Literature of Dental Caries 210-20. 172 susceptible and caries-immune subjects had periodically been proposed and tested since the early decades of the twentieth century. Most of these studies sought to identify a correlation between caries susceptibility and the alkalinity, or the calcium and phosphorus content, of saliva or blood serum, but none produced completely unambiguous results.138 Similarly, the reason that the administration of foods containing vitamin D or ultraviolet light to deficient subjects resulted in the lowering of caries rates in already-erupted teeth was not clear, though there was speculation that some bacteriological or chemical alteration of the saliva occurred.139 Overall, then, the question of whether nutrition could positively affect resistance to caries via metabolic channels was hardly closed. A factional struggle of sorts emerged and intensified in the 1930s and 1940s, as dental research groups aligned themselves over the issue of nutrition. Those who focused on oral bacteriology and the local causes of caries saw themselves as defenders of the Millerian legacy against interlopers from nutrition science who did not respect their carefully accumulated knowledge. Theodor Rosebury, a bacteriologist at Columbia University, captured this sentiment well in a 1932 discussion: \"There has been an apparent tendency in the recent literature to overlook clinical facts, and to treat the subject of dental caries academically, or in complete disregard of clinical experience.\"140 F. E. Rodriguez of the Dental Corps of the US Army likewise lamented the sudden enthusiasm for nutrition in his field. \"In but few other periods in the history of science,\" he stated, \"have factual basic 138 Gerald J. Cox, \"Oral Environment and Dental Caries,\" in A Survey of the Literature of Dental Caries, 290-319. 139 McCollum, \"The Diet in Relation to Caries,\" 47. 140 In J. J. Enright, H. E. Friesell, and M. O. Trescher, \"Studies of the Cause and Nature of Dental Caries,\" Journal of Dental Research 12, no. 5 (1932): 845. 173 concepts been thrown aside in an effort to influence quickly, and to dominate, the clinical mind.\"141 The nutrition investigators, on the other hand, believed that they were injecting a much-needed \"scientific\" mindset into a field that historically privileged narrow technical expertise. In a 1954 letter to Charles B\u00f6decker, for instance, May Mellanby defended her research by portraying it as part of the Millerian legacy: \"It seems almost impossible to believe that not very long ago teeth were looked upon rather like bits of marble stuck into the jaws! I wonder what people like G. V. Black would think now? I am sure Miller would have been a believer today; he was scientific in outlook.\"142 The divisions that developed within the dental field can be seen in the lack of \"official\" consensus in the United States over the question of vitamin D in the control of dental caries. The Council on Dental Therapeutics of the American Dental Association refused advertising claims regarding the relation of vitamin D to dental caries. \"Evidence that this vitamin aids in the maintenance of the fully formed tooth or in the prevention or retardation of dental caries at any stage in its progress,\" the Council stated in 1945, \"is not supported by observations that have passed beyond the controversial stage.\"143 On the other hand, the Council on Foods and Nutrition of the American Medical Association and later the Council on Pharmacy and Chemistry accepted the following statement in 1946: \"There is clinical evidence to justify the statement that vitamin D plays an important role in tooth formation. Likewise experimental evidence justifies the statement that vitamin D is a beneficial factor in preventing and arresting dental caries when the intake of calcium and 141 In J. J. Enright, H. E. Friesell, and M. O. Trescher, \"Studies of the Cause and Nature of Dental Caries,\" Journal of Dental Research 12, no. 5 (1932): 838. 142 Letter (16 February 1954) from May Mellanby to Charles B\u00f6decker, PP/MEL/F.5, Wellcome Library Archives and Manuscripts, London, UK. 143 Council on Dental Therapeutics, American Dental Association, \"The Current Status of Vitamin D,\" Journal of the American Dental Association 32 (1945): 224. 174 phosphorus is liberal and the diet is adequate with respect to other nutrients.\"144 The fact that a group representing nutrition research ers held a different position than one representing the dental profession is indicative of a larger paradigmatic divide at that time. As the controversy over the Michigan group's research suggested, the definition of \"nutritional adequacy\" became one of the biggest points of contention between the bacteriological and nutritional camps. For example, J. D. King, a prot\u00e9g\u00e9 of May Mellanby, conducted a two-year experiment during World War II wherein groups of infants in two British institutions were given known amounts of boiled sweets or a chocolate biscuit each evening after cleansing their teeth, in addition to the usual monthly wartime allotment of 336 grams of sweets per week. King observed no increase in dental caries activity over periods from six months to two years. The basal diet of the children was determined to be very high in vitamins A and D and fairly adequate in calcium and phosphorus; the fluorine content of the drinking water was low. The carbohydrate content of the diet in the form of starches and cereals was also high. King interpreted his findings to suggest, contrary to the Michigan group, that the feeding of small amounts of so-called \"fermentable carbohydrates\" did not increas e the dental caries activity of the deciduous teeth of young children. Furthermore, resistance to decay was more mark ed in older children at one of the institutions with the greatest period of residence.145 Julian Boyd of the Iowa group came to conclusions in accordance with those of King. In the 1940s and 1950s, Boyd conducted a long-term experiment on teenaged girls in a custodial institution, in which he altered the amount of refined sugar he fed with meals to test groups. Meticulous dental studies failed to indicate any significantly different rate or 144 Council on Pharmacy and Chemistry, American Medical Association, New and Non-Official Remedies (Chicago, IL: American Medical Association, 1946), 610. 145 J. D. King, \"Dental Caries: Effect of Carbohydrate Supplements on Susceptibility of Infants,\" Lancet 247, no. 6401 (4 May 1946): 646-49. 175 pattern of caries advance among the girls fed four ounces of sugar daily than was evident among the control subjects whose diet were not modified, or among the experimental subjects during the fore- and post-periods of observation when they received the usual institutional diet. Moreover, Boyd found that among the whole group of subjects studied, more than 2000 lactobacillus counts from saliva samples failed to show significant correlation between the magnitude of the counts and the net rate of caries advance. Boyd emphasized that the nutritional content of the institutional diet well exceeded the diet of the average American child. He concluded from his experiment that dietary advice from dentists \"should be directed toward the inclusion of essential foods, not toward the exclusion of food products that might be considered as caries-promoting.\"146 Clinical and field studies such as those carried out by King, Boyd, and Price were raising the possibility that the criteria for determining a \"nutritionally adequate diet\" had to consider dental conditions as well. As James Shaw of the School of Dental Medicine at Harvard University stated in 1952, \"the exact specifications of what constitutes an adequate diet under all circumstances are not known completely today, nor do we know whether the maintenance of normal growth, reproduction and lactation are complete diagnostic criteria of an adequate diet.\" He suggested that the maintenance of intact teeth \"might in the future be shown to be just as essential a criterion of the adequacy of a diet.\"147 A decade later, Reidar Sognnaes of the School of Dentistry at the University of California, Los Angeles echoed a similar opinion: \"If we can unravel the deeper secrets of the teeth, their optimal structure, 146 Julian D. Boyd, \"Epidemiologic Studies in Dental Caries: VI. A Review of Intrinsic Factors as They May Affect Caries Progression,\" The Journal of Pediatrics 44, no. 5 (May, 1954): 578-90. 147 James H. Shaw, \"Nutrition and Dental Caries,\" in A Survey of the Literature of Dental Caries, 418. 176 and composition, and build them accordingly during primary development, it may become possible to maintain the teeth during function despite an unfavorable oral environment.\"148 Yet, research into the positive role of nutrition on dentition curiously faded to the marg ins in the postwar decades.149 There were several large-scale epidemiological studies conducted in Britain, Scan dinavia, and elsewhere in Continental Europe during and after World War II, when blockade and rationing created a mass involuntary experiment in dietary change. To some researchers, these studies appeared to support the argument that nutrition during pregnancy and childhood could influence the soundness of the teeth metabolically.150 But the statistical methods used to support this argument were contested and the issue remained inconclusive.151 This sort of back-and-forth demolition of arguments left little that anyone could defend with much certainty, but the long-term positive upshot was that it forced dental investigators to improve the quality of their experiments. Over time, more attention was paid to creating proper control groups, assuring statistical significance, combining cross-sectional with longitudinal studies, and standardizing diagnostic criteria and experimental conditions between research teams. Even though the dental research field between the 1920s and 1950s was characterized by running debate over the merits of hygiene and nutrition, the general dietary 148 Reidar F. Sognnaes, \"The Present Status of Caries Research,\" Journal of Prosthetic Dentistry 13, no. 5 (September-October 1963): 928. 149 The majority of studies in the postwar years, such as the famous Vipeholm Study in Norway, focused on the well-established relationship between sugar consumption and caries, though disagreement persisted over whether it was the total quantity or frequency of sugar consumption that was to blame, and whether forms of \"sugars\" besides sucrose were also cariogenic. For a summary of the classic studies on sugar in the etiology of caries and the persistent debates, see D. T. Zero, \"Sugars\u2014The Arch Criminal?\" Caries Research 38, no. 3 (2004): 280. 150 G. Toverud, \"Decrease in Caries Frequency in Norwegian Children During World War II,\" Journal of the American Dental Association 39, No. 2 (August 1949): 128-46; May Mellanby, Helen Coumuoulos, and Marion Kelley, \"Teeth of Five-Year-Old London Schoolchildren (1955),\" British Medical Journal 2, no. 5040 (10 August 1957): 318-22. 151 Gilbert J. Parfitt, \"The Apparent Delay Between Alteration in Diet and Change in Caries Incidence: A Note on Conditions in Norway Reported by Toverud,\" British Dental Journal 97 (2 November 1954): 235-37. 177 guidelines for arresting caries and building sound teeth and periodontal tissues became uncontroversial relatively quickly. Clinical and field studies made it clear that a tooth-friendly diet could vary greatly, but had to contain little or no refined sugar and flour and ample amounts of some combination of the \"protective foods\"\u2014milk, meat , organs, seafood, eggs, vegetables, fruits, and unrefined cereals.152 The widespread adoption of such a diet throughout life would satisfy the proponents of both the oral environment and systemat ic-metabolic theories, as well as almost any nutrition scientist or public health worker of that era. McCollum and Orent-Keiles Day nicely summarized this \"hybrid\" position in The Newer Knowledge of Nutrition (1939): It seems that were we to turn to a low sugar, high fat type of diet, such as is prescribed for diabetic patients, we might expect a prompt and marked reduction in caries susceptibility. This type of diet is practicable in many countries, but fats are in many regions considerably more expensive to produce than are starches and sugars. At any rate, we now know how to produce good teeth as respects structure and how to preserve them in considerable meas ure from decay. We may confidently expect that further researches will within a few years see complete unanimity of opinion as to the factors which operate to cause caries susceptibility.153 Likewise, M. A. Rushton, a dental researcher Guy's Hospital in London, succinctly described the mutually shared goal of the different factions in a 1936 lecture: \"Although the protagonists of various schemes for preventing dental decay are in many cases strongly opposed to one another, it is a fortunate thing that their remedies are not incompatible.\"154 Parad oxically, this certainty regarding the efficacy of diet in preventive dentistry was probably one of the reasons for the postwar decline of interest in nutritional research. 152 As mentioned above, field researchers found that undernourished populations eating little sugar and refined carbohydrate had very low rates of decay (e.g., I. Schour and M. Massler, \"Dental Caries Experience in Postwar Italy (1945),\" Journal of the American Dental Association 35, no. 1 (July 1947): 1-6). Although these studies helped elucidate the etiology of dental caries, they could hardly be considered as guidelines for a feasible dietary program in the developed countries. 153 McCollum, Orent-Keiles, and Day, The Newer Knowledge of Nutrition, 5th ed., 629. 154 M. A. Rushton, \"The Birmingham Investigations and Some Others,\" Guy's Hospital Gazette 51, no. 1264 (2 January 1937): 5. 178 And perhaps more importantly, the dental profession was divided and unsure about what role its memb ers were to play in the application of dietary and nutritional research to the public at large. At least according to anecdotal evidence, some dentists felt that, since very little was taught in either the medical or dental schools about diet and nutrition, any dentist or physician who had taken the pains to become informed on the subject should be allowed to give advice. Other dentists argued that the physician and the dentist should work together on dietary matters, while still others believed that the responsibility for prescribing diets should rest with the physician. In response to the question of whether members of the dental profession should take upon themselves the responsibility of prescribing diets for their patients, one dentist opined, \"I believe it to be a problem not so much of prescribing, as one of passing along basic dietary information. This seems to me wholly within the province of the dentist.\"155 But even researchers who were optimistic about the role of nutrition in preventive dentistry were pessimistic about the possibility of drastically changing Western dietary habits on a large scale, especially toward anything resembling the relatively unprocessed, high-fat, restricted-sugar \"diabetic\" diet that offered the best results in clinical trials and field studies of \"primitives.\" For instance, Nina Simmonds, a dietician who co-authored several important works with McCollum, confessed in 1938, \"Long experience in studying dietary problems makes me question whether entire freedom from dental caries, even if this were possible, would be sufficient to induce the average person to refrain entirely from sweet foods.\"156 Reidar Sognnaes also saw many obstacles in the way of achieving dietary control of caries: \"individual preferences and biases, economic status of the family, the greater 155 John Albert Marshall, \"Dietary Prescriptions by Dentists,\" Dental Cosmos 73 (September 1931): 894. 156 Nina Simmonds, \"Present Status of Dental Caries in Relation to Nutrition,\" American Journal of Public Health 28 (December 1938): 1386. 179 productivity of carbohydrate foods per unit of land, and last but not least, the difficulty in maintaining the interest of the individual in the faithful continuation of prolonged procedures.\"157 Simmo nds and Sognnaes were merely repeating the conventional wisdom that, except under conditions of isolation from modern foods or in carefully controlled trials, the prevention of caries and periodontal disease through diet was a rather academic endeavor. The sense of resignation on the part of the dental profession toward popular eating habits can be seen in the rather limited scope of the research into preventive dentistry in the postwar decades, which focused for the most part on improving oral hygiene and adding anti-caries substances to the food supply. The development and commercialization of the first effective fluoride-containing toothpaste in the late 1950s and early 1960s represented perhaps the most important breakthrough in preventive dentistry. In addition, researchers in the 1960s were excited by the possibility that the addition of phosphate supplements to foods containing sugar and flour could reduce their cariogenic effect. Experimental results on this score, however, were ultimately inconclusive.158 There was similar enthusiasm for various antibacterial substances and artificially sweetened products, as food chemists turned out new sugar substitutes. But the best-known postwar intervention into the food system was the addition of fluoride compounds to municipal drinking water supplies. Seen from the perspective of the known science in the 1920s and 1930s, the sudden adoption of public water fluoridation in the postwar years was highly unusual, since most of the early research on fluorine was 157 Reidar Sognnaes, \"Caries-Inhibiting Agents,\" Pharmacological Reviews 11, no. 9 (December 1959): 735. 158 For a review, see Irwin D. Mandel, \"Effects of Dietary Modifications on Caries in Humans,\" Journal of Dental Research 49, Supplement to no. 6 (1970): 1207-09. This is still an active area of research: G. C. Forward, \"Non-Fluoride Anticaries Agents,\" Advances in Dental Research 8, no. 2 (1994): 208-14. 180 concerned with its toxicity rather than any possible beneficial effects.159 Farm animals and people living in areas with concentrations of fluorine in excess of one parts per million in the drinking water were found to suffer not only from \"mottled teeth,\" but, at higher concentrations, other physiological disturbances as well. Since fluorine was found to be toxic in relatively low concentrations and had no known essential function in plant and animal growth and development, agricultural scientists in the 1930s advised that the quantity in animal feeds, commercial fertilizers, and spray insecticides should be kept to a minimum.160 In 1939, the United States Department of Agriculture even advised that \"it is especially important that fluorine be avoided during the period of tooth formation, that is, from birth to the age of 12 years.\"161 Fluoride had also become necessary in key industrial processes. Metal smelters, brickworks, glass, and enamel manufacturers, and superphosphate fertilizer producers all used raw materials that included enormous quantities of fluoride. In 1937, Danish scientist Kaj Roholm published his monumental study on the myriad health effects of industrial fluoride pollution.162 Farm animals and humans afflicted with fluorosis were found to suffer 159 See, e.g., E. V. McCollum, Nina Simmonds, J. Ernestine Becker, and R. W. Bunting, \"The Effect of the Addition of Fluorine to the Diet of the Rat on the Quality of the Teeth,\" Journal of Biological Chemistry 63 (1 April 1925): 553-62; and Floyd DeEds, \"Chronic Fluorine Intoxication: A Review,\" Medicine 12, no. 1 (February 1933): 1-60. 160 J. E. McMurtrey and W. O. Robinson, \"Neglected Soil Constituents That Affect Plant and Animal Development,\" in Soils and Men: Yearbook of Agriculture, 1938 (Washington: GPO, 1938), 820. 161 Margaret Cammack Smith, \"Mineral Needs in Man,\" in Food and Life: Yearbook of Agriculture, 1939, 212. 162 Kaj Roholm, Fluorine Intoxication: A Clinical-Hygienic Study, with a Review of the Literature and Some Experimental Investigations (Copenhagen: Nyt nordisk forlag; London: H.K. Lewis & Co., 1937). 181 a wide range of symptoms: osteosclerosis and skeletal deformations, gastric and central nervous system disorders, and even weakened and brittle teeth.163 But this largely negative view of fluoride quickly changed after studies in the United States and Britain in the 1920s and 1930s suggested that an inverse relationship existed between caries rates of children and mottled enamel prevalence. Indeed, there had been scientific speculation since the nineteenth century that because ingested fluoride was deposited in teeth and bone, it might be necessary for healthy teeth. Namely, the work of H. Trendley Dean of the United States Public Health Service was foundational in building accep tance for water fluoridation. Dean characterized dental fluorosis into four levels of severity: very mild, mild, moderate and severe. He conducted a series of studies in the 1930s and 1940s that showed that as the fluoride in the water increased up to about one part per million, dental decay rates went down in children. He also noted that at one part per million in drinking water, mild dental fluorosis affected only ten percent of the population, giving birth to the idea that the \"optimal\" level of fluoride in drinking water should be about one part per million. Following these suggestive findings, trials of artificial fluoridation began in 1945 in Newburgh, New York, Grand Rapids, Michigan, and Brantford, Ontario using sodium fluoride.164 In 1950, before any of these trials were completed, the Public Health Service reversed its former position and endorsed fluoridation. Other professional and governmental organizations in the United States soon followed suit, and the campaign for water fluoridation gained traction internationally, particularly in the English-speaking countries. 163 Floyd DeEds, \"Chronic Fluorine Intoxication: A Review,\" Medicine 12, no. 1 (February 1933): 1-60; F. J. McClure, \"A Review of Fluorine Effects,\" Physiological Reviews 13 (July 1933): 277-300. 164 \"Achievements in Public Health, 1900-99: Fluoridation of Drinking Water to Prevent Dental Caries,\" Morbidity and Mortality Weekly Report 48, no. 41 (22 October 1999): 933-940. 182 But fluoridation was never universally accepted as a public health measure in North America and Europe, and from the beginning, it has been dogged by acrimonious debate.165 Water fluoridation has been questioned on ethical grounds, since it violates a basic principle of modern pharmaco logy: fluoridation represented the first instance in which a medication was administered for a non-communicable disease by majority vote or administrative decision to an entire community, and not by the informed consent of the individual.166 Although the weight of evidence continues to show that water fluoridation is beneficial and safe for the population as a whole, overall exposure to fluoride has been increas ing in the developed countries in the past several decades. Dental fluorosis has accordingly been on the rise in both fluoridated and unfluoridated communities, though prevalence of the condition tends to be higher in the former.167 The margin between safe and deleterious doses of fluoride is quite small compared to other trace elements, and estimates for the safe upper limit of fluoride intake has remained subject to scientific disagreement.168 Moreover, recent evidence has indicated that the caries-protective effect of fluoride is predominantly due to its topical effect on the surface of the tooth rather than to any systemic 165 The persistence of this debate in the popular press is evidenced by the continued publication of books on both sides of the issue. See, e.g., Christopher Bryson, The Fluoride Deception (New York: Seven Stories Press, 2004); and R. Allan Freeze and Jay H. Lehr, The Fluoride Wars: How a Modest Public Health Measure Became America's Longest Running Political Melodrama (Hoboken, NJ: John Wiley & Sons, 2009). 166 Caries is, strictly speaking, an infectious disease, but fluoride's mode of action is on the chemical composition of the enamel surface, and not against specific oral microorganisms. Furthermore, the longstanding claim that fluoride is a \"necessary\" trace element because it reduces caries rates, and therefore cannot be considered a drug, is spurious, since it has no known beneficial function in human metabolism. Teeth do not require fluoride for complete caries immunity. 167 Marian S. McDonagh et al., \"A Systematic Review of Public Water Fluoridation,\" British Medical Journal 321, no. 7265 (7 October 2000): 855-859. The authors of this review found that the overall quality of evidence on water fluoridation is low, and that caries reduction from fluoridation was lower than previously reported. 168 This topic was reopened in the United States with the publication of the National Research Council's Fluoride in Drinking Water: A Scientific Review of EPA's Standards (Washington: National Academies Press, 2006). 183 effect during tooth formation.169 These reassessments, along with the longstanding ethical concerns, continue to fuel skepticism toward public water fluoridation. But in any case, the early effort to push fluoridation in the 1950s was hardly an edifying spectacle in light of the fact that the control of caries through diet was grounded in a much stronger, cross-disciplinary scientific base. On the one hand, the fact that public health and dental organizations managed to convince many commu nities to introduce a form of medication into their drinking water can be interpreted as a sign of the growing popular accep tance of the dental profession's claims to scientific authority and expertise. On the other hand, the campaign to fluoridate water, when seen in the context of the more substantial body of research on diet and nutrition that had developed in preceding decades, can also be interpreted as an admission of dentistry's impotence in the face of entrenched dietary habits and food processing interests. Rather than staking its reputation on attacking the industrial food system at its foundations, the dental profession largely chose instead to push for small, faute de mieux adjustments to it. Fluoridation was urged as a \"necessary\" public health intervention on the assumption that people would continue to make poor dietary decisions. It should be noted that caries rates in the past several decades have dropped mark edly in the developed countries, due to improvements in dental hygiene and treatment, increas ing fluoride use, and better nutrition. But the decline in dental decay, while certainly an admirable public health achievement, does not exclude the possibility that a thoroughgoing change in dietary patterns could have more effectively and more completely accomplished this goal. This alternative approach might still be worth considering, given 169 E. Hellwig and A. M. Lennon, \"Systematic versus Topical Fluoride,\" Caries Research 38 (2004): 258-262. 184 recen t indications that the decrease in caries prevalence over the last half-century has come to a halt and even begun to reverse in the developed countries.170 And even though surprisingly little research has been carried out along these lines since Weston Price's day, dietary change could possibly have lessened malocclusion and dental arch problems, which are still commo n afflictions requiring expensive orthodontic treatments. In a certain sense, the focus in the postwar decades on improving oral hygiene, widening access to professional dental care, increasing fluoride usage, and tempering sugar consumption signified a redrawing of the traditional boundaries of dentistry\u2014a return, after a period of flux, to regarding the oral cavity rather than the whole body as the proper province of the dental worker. Given the limited resources of the dental profession in relation to the enormity of the decay problem, this narrowing of scope was perhaps an inevitable strategic concession. Yet nutrition has been and remains a persistent undercurrent in the dental field, and the sense of continuity with the first wave of nutrition research has not been entirely lost.171 In fact, in the mid-twentieth century, dentistry was widening its scope to include not just human metabolism, but nature in a more general sense as well. Dental researchers in that period contributed substantially to an emerg ing body of science that was opening up a remarkably forward-thinking \"ecological\" view of the linkages between nutrition, health, and the environment. The early observational studies on fluoride, for instance, was one part of a larger body of geographical investigations into the relationship between caries rates and the chemical content of drinking water. James M. Dunning, in his 1953 statistical analysis of a number of these studies from the United States, South Africa, Australia, and New 170 Paula Moynihan and Poul Erik Petersen, \"Diet, Nutrition and the Prevention of Dental Disease,\" Public Health Nutrition 7, no. 1A (2004): 204. 171 Recent publications on nutrition and dentistry include, e.g., Riva Touger-Decker, David A. Sirois, and Connie C. Mobley, eds., Nutrition and Oral Medicine (Totowa, NJ: Humana Press, 2005). 185 Zealand, noted that there was a stronger correlation between total \"water hardness\" and caries rates than between naturally occurring fluoride and caries rates. Dunning suggested that the components of water hardness, such as calcium and magnesium, deserved more study, as did trace elements besides fluorine.172 John Myers likewise critiqued the overemphasis on fluoride in his analysis of the water supply of Deaf Smith County, Texas. Some of the early interest in water fluoridation developed from a study by Edward Taylor of the Texas State Department of Health on the dental conditions in that county. When Taylor's study was published in 1942, the schoolchildren of Deaf Smith County revealed approximately one decayed, missing, or filled tooth per child. This was by far the lowest rate of dental decay ever reported in an industrialized country.173 Following this report, the US Public Health Service made an extensive survey on water from wells in the High Plains area. From these findings, it was inferred that this unusually low rate of tooth decay was due to fluoride in the drinking water, and that the water of Deaf Smith County had the optimum fluoride concentration for good dental health. But it had also been observed for many years that cattle raised in the county gained weight faster and had larger skeletons than cattle from surrounding areas. There were also some anecdotal reports from physicians that residents of Deaf Smith County had stronger bones, higher mineral density, and faster bone healing ability compared to surrounding areas , even in old age.174 Myers observed that, in comparison with Dallas County, Texas, the water of Deaf Smith County was not only higher in fluorine, but iodine and magnesium as well, and lower in calcium. He hypothesized that iodine and magnesium, 172 James M. Dunning, \"The Influence of Latitude and Distance from Seacoast on Dental Disease,\" Journal of Dental Research 32 (December 1953): 811-29. 173 Edward Taylor, \"Preliminary Studies of Caries Immunity in the Deaf Smith County (Texas) Area,\" Journal of the American Dental Association 29, no. 3 (March 1942): 438. 174 Lewis B. Barnett, \"New Concepts in Bone Healing,\" Journal of Applied Nutrition 7 (1954): 318-23. 186 by affecting bone and tooth development and the function of the thyroid and submaxillary glands, protected against caries more than did the fluoride in the water.175 Abraham Nizel, a dentist at Tufts University, and Robert Harris, a nutritionist at MIT, came to similar conclusions in a series of experiments using caries-susceptible hamsters. Nizel and Harris were intrigued by surveys carried out during World War II that identified some correlation between soil types and caries prevalence in soldiers from several geographic areas in the United States.176 Nizel and Harris found that cariogenic diets containing milk and corn grown in Texas and fed to hamsters resulted in only 40 percent as much dental decay as otherwise identical diets containing corn and milk produced in New England. This effect did not seem to be due to fluoride, since the fluoride content was made equal by the addition of sodium fluoride to the New England diet up to the levels found in the Texas diet. Nizel and Harris initially concluded that the difference between these diets was due to the presence of a cariogenic factor in the New England diet rather than a protective factor in the Texas diet.177 But they found in subsequent studies that phosphorus, and perhaps other trace elements, were evidently cariostatic, while some elements appeared to be cariogenic.178 These and other studies pointed up the possibility that chemical variations in 175 John A. Myers, \"The Role of Some Nutritional Elements in the Health of Teeth and their Supporting Structures,\" Annals of Dentistry 22, no. 2 (June 1958): 35-47. 176 Abraham E. Nizel and Basil G. Bibby, \"Geographic Variations in Caries Prevalence in Soldiers,\" Journal of the American Dental Association 31 (1944): 1919-26. 177 Abraham E. Nizel and Robert S. Harris, \"The Caries-Producing Effect of Similar Foods Grown in Different Soil Areas,\" New England Journal of Medicine 244, no. 10 (8 March 1951): 361-62. 178 Abraham E. Nizel and Robert S. Harris, \"Effect of Ashed Foodstuffs on Dental Decay in Hamsters,\" Journal of Dental Research 34, no. 4 (1955): 513-15; and Robert S. Harris and Abraham E. Nizel, \"Effects of Food Ash, Phosphate, and Trace Minerals upon Hamster Caries,\" Journal of Dental Research 38, no. 6 (1959): 1142-47. 187 the composition of not only water, but also of food, could influence the dental decay process.179 Other environmental conditions were also found to influence caries rates. In a number of statistical analyses made in the 1930s and 1940s, a high correlation was observed between a decreas e in the hours of sunlight exposure per year and an increase in the dental caries incidence. In the population as a whole, rates of decayed, missing, and filled (DMF) teeth showed a clear rise with increasing latitude and cloud cover and cooler average winter temperatures. Investigators also found an association between the season of the year and dental caries development, with the greatest incidence occurring in fall and winter and the lowest in the spring and summer. These observational studies buttressed the clinical studies that suggested the protective role of vitamin D on teeth, since variation in exposure to ultraviolet rays was considered the most probable explanation for the geographical and seasonal differences in dental decay.180 Thus, for many dental researchers in the mid-twentieth century, the teeth were a reflection not only of people's diet and nutrition, but also of the larger environmental forces in which they lived. This broad-ranging \"ecological\" perspective on health and disease, as the next chapter will illustrate, was not exceptional in the field of nutrition at that time. 179 Another notable research project along these lines was Reidar F. Sognnaes and James H. Shaw, \"Experimental Rat Caries. IV: Effect of a Natural Salt Mixture on the Caries-Conduciveness of an Otherwise Purified Diet,\" Journal of Nutrition 53 (1954): 195-206. 180 For a review of this literature, see James H. Shaw, \"Nutrition and Dental Caries,\" in A Survey of the Literature on Dental Caries, 465-67, 478-79. 188 CHAPTER THREE \"HEALTH FROM THE GROUND UP\": NUTRITION SCIENCE AND ECOLOGY BEFORE ENVIRONMENTALISM1 In 1927, four years before he set out on his journeys around the world as a \"dental anthropologist,\" Weston Price began collecting thousands of samples of butterfat every one to four weeks from hundreds of localities in North America. He divided the United States and Canada into sixteen geographic districts of several thousand square miles each, and compared seasonal variations in the average vitamin A and D concentration of the butterfat produced in these districts with official statistics on deaths from pneumonia and heart failure. Within a few years, Price definitively determined that there was an inverse correlation.2 While he was collecting data for this massive study, he noticed an anomaly. As butter and cream samples were coming in, one series was comparatively higher in vitamins than others of the same district. His curiosity piqued, Price decided to take the short trip from his laboratory in Cleveland to the farm in western Pennsylvania producing this exceptional butterfat, to collect data firsthand. But he went to the pastures, not the milking parlor. It might have been a curious sight for the farmer. Why would a dentist be trudging 1 The title phrase comes from a booklet by Karl B. Mickey, Health from the Ground Up (Chicago, IL: Public Relations Department, International Harvester Company, 1946). 2 Weston A. Price, \"Seasonal Variations in Butter-Fat Vitamins and Their Relation to Seasonal Morbidity, Including Dental Caries and Disturbed Calcification,\" Journal of the American Dental Association 17 (May 1930): 850-873; Price, \"Some Contributing Factors to the Degenerative Diseases, with Special Consideration of the Role of Dental Focal Infections and Seasonal Tides in Defensive Vitamins,\" Dental Cosmos, Bulletin 107 (October and November 1930). The concern with seasonal variations in vitamin D intake and rates of disease has been experiencing a recent revival. See, e.g., Armin Zitterman, \"Vitamin D in Preventative Medicine: Are We Ignoring the Evidence?\" British Journal of Nutrition 89 (2003): 552-72; J. J. Cannell et al., \"Epidemic Influenza and Vitamin D,\" Epidemiological Infections 134 (2006): 1129-40 189 around his acres digging up plant and soil samples? What connection could possibly exist between people's teeth and his fields? But Price, attentive to the latest scientific research, had a hunch. He suspected that there was an association between the nutrients available to the plants from the soil and the vitamin content of the milk coming from the grazing animals. Price found that the cows producing the favorable product were grazing on river bottomland and showed evidence of being in especially fine physical condition. He watched carefully to see what grass they were eating. Price noticed that, unusually, the cows would \"wade through a luxuriant growth that was up to their knees, showing evidence of being in search for something.\" They were frequently found grazing on knolls where there was little available grass, for it had been eaten almost into the ground. Price observed that the cows sought out a particular plant, locally called \"Iron Weed\" because it was very difficult for a plow to cut through its deep roots. He was also told that cows and horses had such a fondness for the lowland pasturage that they would voluntarily swim across the river transecting the farm during the spring floods, in very cold water, just to get at it.3 Price widened his survey to the whole dairy district of northwestern Pennsylvania. He ultimately collected six different kinds of pasture plants from three different soil types. He found that the plants from the river bottom soils\u2014a deep and rich alluvium, underlain by fine sand and coarse gravel\u2014had the highest concentrations of calcium, phosphorus, iron, potassium, and magnesium. The plants from the upland soils, derived from a sandstone base, scored lowest. The plants from the hillside soils\u2014loamy and fertile but without the 3 Price recounts the field study and presents his data in \"Mineral Deficiencies and Animal Diseases,\" File W227A-C, File W227A-C, Price-Pottenger Nutrition Foundation, La Mesa, CA. This document is perhaps a manuscript chapter he wrote in the early 1930s for a never-published book, The Etiology and Control of Dental Caries. See Weston A. Price, \"Why Dental Caries with Modern Civilizations? VI. Practical Procedures for the Nutritional Control of Dental Caries,\" The Dental Digest 40 (August, 1933): 314 190 benefit of decaying organic matter and silt from annual flooding\u2014rated in mineral content between those of the upland and the bottomland. The deep-rooted \"Iron Weed\" from the bottomland knolls scored very high in most of the minerals (including, incidentally, iron), which explained to Price why cows ate it with such relish. His vision attuned to minerals, Price believed he could see nutritional differences written on the landscape that grazing animals seemed to instinctively taste. This foray into northwestern Pennsylvania confirmed patterns that started emerging from his larger continent-wide analysis. Price believed that the most potent factor in determining the fat-soluble vitamin concentration in butterfat was the availability of rapidly growing grass which was mainly available to grazing animals from late spring to early fall, the peaks in growth varying with latitude and temperature. He furthermore found that the seasonal curves for these vitamins were the highest in regions with the most fertile soils.4 Price felt that these findings had major implications for public health. He concluded from his own and others' research that variations in environmental conditions\u2014season, climate, and especially soil charact eristics\u2014brought about significant differences in the nutritional quality of foods, which had an effect, in turn, on the animal eating them, be it a cow or person. In Price's view, the variability in the nutritional quality of foods called into question the nutrition scientists' efforts to fashion accurate dietary advice based on \"standard food values.\" They further suggested to Price that the prevention of tooth decay and other degenerative diseases involved more than merely avoiding \"the foods of modern commerc e.\" Dentists, and the public at large seeking to avoid their ministrations, had to pay 4 Weston A. Price, Nutrition and Physical Degeneration (Redlands, CA: The Author, 1945), 387. 191 attention not only to what kinds of foods to eat, but also to the conditions under which they were grown.5 It might seem extraordinary that a dentist would feel comfortable trespassing into the territory of the agronomist. But Price was, in fact, not conjecturing far from the accepted science of the time. In the early 1930s, he voiced frustration that \"exceedingly little\" research had been done on the variability of vitamin and mineral content in foods grown under different conditions.6 But over the following two decades, investigators in government and academic institutions produced a sizeable body of science concerning the connections between the environment, food, and health. Awareness of these relationships was not peripheral to nutrition science in general. In 1947, for instance, William Darby, a professor of medicine at Vanderbilt University, wrote an article entitled \"The Nutritionist's Interest in Soils and Agriculture\" in the journal Nutrition Reviews. Darby recognized that Graham Lusk's 1920s-era definition of nutrition as the \"sum of the processes concerned in the growth, maintenance, and repair of the living body as a whole or of its constituent organs\" had given way \"to a broader one including the composition of the substances acted upon by the body (foodstuffs) and the science and production of these foodstuffs.\"7 In fact, for many researchers, the body of scientific discoveries known as the \"newer knowledge of nutrition\" became more than just a means for combating deficiency diseases and improving public health. These discoveries played a major\u2014though now mostly overlooked\u2014part in a wider transformation in the scientific understanding of the relationship between humankind and nature. The first half of the twentieth century saw a remark able accel eration in knowledge regarding the manifold connections between climate, soils, plants, 5 Price, \"Mineral Deficiencies and Animal Diseases,\" 85. 6 Ibid., 86. 7 William J. Darby, \"The Nutritionist's Interest in Soils and Agriculture,\" Nutrition Reviews 5, no. 3 (March, 1947): 65. 192 animals, and humans. As these connections became clearer, an extensive group of researchers in the natural sciences, agriculture, nutrition, and medicine began to perceive their fields as being strongly connected, giving rise to a scientific tradition that I will call \"nutritional ecology.\"8 The nutritional ecology outlook developed, to some extent, from a logic of its own within the scientific community. But scientists, of course, do not act apart from larger social and political forces. Indeed, nutritional ecology arose at a time in the late 1920s and 1930s when a growing international network of scientists, public intellectuals, and government officials began loudly raising the alarm on the threat of soil erosion to agriculture, and to civilization more generally.9 Researchers with an eye toward both soil degradation and nutritional ecology developed a powerful \"proto-environmentalist\" critique of the 8 The term \"ecology,\" as I use it here in relation to nutrition, is more closely related to its original, and more limited, meaning in the natural sciences than that found in, e.g., Joan Dye Gussow, The Feeding Web: Issues in Nutritional Ecology (Palo Alto, CA: Bull Publishing Co., 1978). Gussow defines the subject of \"nutritional ecology\" as the \"examination of the biological, technical, social, scientific, and commercial matrices in which the production, processing, and consumption of food are embedded\" (xiii). In a similar vein, a group of nutritionists and scholars at the University of Giessen have defined \"nutrition ecology\" as an interdisciplinary science that examines the food chain from the four dimensions of \"health, the environment, society, and the economy.\" See Claus Leitzmann, \"Nutrition Ecology: Origin and Definition,\" Forum of Nutrition 56 (2003): 220-21. It is questionable whether these \"systems\" approaches to nutrition should be labeled as a form of \"ecology,\" which as a matter of disciplinary identity is a natural science concerned, at its most general level, with interactions among organisms and between organisms and the atmospheric, hydrological, and geological spheres. For this reason, I have separated \"nutritional ecology\" from \"nutritional conservation,\" the latter term referring to a proto-environmentalist perspective informed by nutritional ecology. 9 There is a copious secondary literature on the increasing concern about soil erosion in the interwar years. See, e.g., Randal Beeman and James Pritchard, A Green and Permanent Land: Ecology and Agriculture in the Twentieth Century (Lawrence, KS: University of Kansas Press, 2001); Philip Conford, The Origins of the Organic Movement (Edinburgh: Floris Books, 2001); Sarah T. Phillips, This Land, This Nation: Conservation, Rural America, and the New Deal (Cambridge, UK: Cambridge University Press, 2007); Joseph Morgan Hodge, Triumph of the Expert: Agrarian Doctrines of Development and the Legacies of British Colonialism (Athens, OH: Ohio University Press, 2007); Kate Showers, Imperial Gullies: Soil Erosion and Conservation in Lesotho (Athens, OH: Ohio University Press, 2007); William Beinart, \"Soil Erosion, Conservationism and Ideas about Development: A Southern African Exploration, 1900-1960,\" Journal of Southern African Studies 11, no. 1 (October 1984): 52-83; David Anderson, \"Depression, Dust Bowl, Demography and Drought: The Colonial State and Soil Conservation in East Africa during the 1930s,\" African Affairs 83, no. 322 (July 1984): 321-43; Norman Hudson, \"A World View of the Development of Soil Conservation,\" Agricultural History 59, no. 2 (April 1985): 326-59. 193 predominant trends in food production. At the same time, they envisioned an alternative, agriculturally oriented path to better health, higher living standards, and \"ecological perman ence,\" which I will term \"nutritional conservation.\" Nutritional Ecology: Development of an Outlook At least on a general level, the connections between nutrition and nature seem rather intuitive. Animals and plants have certain chemical requiremen ts, those of animals being supplied by the plants on which they feed, either directly or indirectly. The plant is the great intermed iary by which certain mineral elements of the rocks, usually after their conversion into soil, are assimilated and made available to animals and humans. The mostly inorganic constituents of the atmosphere and the soil, with the energy provided by the sun, are selected and built up by the plant into complex organic substances such as carbohydrates, amino acids, and vitamins. These substances may be further elaborated, with selection and removal of elementary components, by the animal into flesh, blood, bones, and other bodily materials. This process is \"nutrition\" in its widest sense. But as investigators have long recognized, the task of understanding the relationships between environment, plants, and animals using the scientific method is by no mean s easy or straightforward. One of the most consistent themes in studies conducted between the 1920s and 1950s dealing with these relationships was the recognition by researchers of the great complexity of the work. Then as now, these kinds of investigations involve an immense number of variables, and controlling experimental conditions in the field is nearly impossible. Furthermore, the difficulties in identifying nutritional disorders in plants or animals multiply as the mobility of the subject increas es. The plant, being stationary as far as its range of activity is concerned, represents a relatively easy subject for 194 investigation. Domesticated animals are more difficult but not impossible to study, since they obtain their nourishment from pastures plants in a confined area or from feedstuffs that can be assayed and controlled. Humans, with the partial exception of those living under more \"primitive\" conditions in restricted localities, represent the most difficult \"subjects,\" due to their regionally varied diets and modern methods of food processing. In addition, it is not possible in the case of humans to separate economic, cultural, and other circumstances from natural factors as causes of good or poor nutrition. Nevertheless, scientists agreed that the less complicated task of investigating soils and other environmental conditions in relation to plant and animal nutrition was indirectly useful for improving human health. Learning how to increas e the nutritional quality of plants grown for food and to raise greater numbers of more healthy and productive animals, they believed, would contribute to a higher level of human nutrition. And this was not just an abstract endeavor. Improving the nutritional quality of foods was a pressing matter because, as Cornell soil scientist Kenneth Beeson argued in 1941, \"it is probably true that not only are a large portion of our people existing on a diet deficient in the protective foods such as milk eggs, fruits, and vegetables, but also a large portion are obtaining only a minimum amount of these foods.\" He feared that \"the value of a minimum diet of protective foods may be reduced significantly below the minimum requirement through the use of inferior foods.\"10 Frank Gilbert, a botanist and plant pathologist at the Battelle Memorial Institute in Columbus, Ohio, had similar concerns. He contended that the domestic animal \"serves as a screen , although by no means a perfect one, so that persons who are heavy consumers of 10 Kenneth J. Beeson, \"The Mineral Composition of Crops with Particular Reference to the Soils in which They Were Grown: A Review and Compilation,\" USDA Miscellaneous Publication, No. 369 (March 1941), 4. 195 eggs, dairy products, and red meat are less likely to be troubled by mineral deficiencies than those whose consumption of these products is limited.\" But because these foods were not cheap, many families in the low-income brackets frequently \"live almost entirely on food of the lowest cost per pound\u2014a diet generally far too high in carbohydrates, frequently low in protein, and usually much too low in minerals and vitamins.\" Rural populations living in regions of poor soils and subsisting almost entirely on what foods could be grown locally, Gilbert argued, were apt to be more malnourished than those living in cities with access to a wider variety of foods or in regions with more fertile soils.11 Other scientists broadly shared Bees on and Gilbert's concern about the insufficient quantity and quality of protective foods at the time, and it affected the development of nutritional ecology significantly. In fact, as early as the 1920s, the conviction among many scientists and some physicians that mild nutritional deficiencies were widespread in even the most economically advanced countries was dovetailing with a similar concern among agronomists regarding crops and animals. One of the first researchers to perceive this shared anxiety was John Boyd Orr. Roughly contemporaneous with the studies that he led in the 1920s and early 1930s on the effect of milk consumption on the growth of British schoolchildren, as well as the dietary surveys of the Kikuyu and Masai and Kenya, Orr was also overseeing research on animal nutrition and pasture improvement. Since his work in nutrition therefo re spanned nearly the entire process of food production and consumption, it is perhaps unsurprising that an early expression of the \"nutritional ecology\" perspective is found in a book that he 11 Frank A. Gilbert, Mineral Nutrition of Plants and Animals (Norman, OK: University of Oklahoma Press, 1948), 97-98. 196 authored with the assistance of Helen Scherbatoff, Minerals in Pastures and Their Relation to Animal Nutrition (1929).12 Minerals in Pastures was, on the surface, a review of the scattered studies from agricultural research stations all over the world, particularly those in the British Empire. But it can also easily be read as a challenge to the boundaries of specialization that had grown in the field of agricultural science between plant and animal breeders, pathologists, soil chemists, and veterinarians. The basic line of argument running through the whole survey was that problems confronting these branches in isolation could be resolved by understanding in detail what minerals do as they pass from soil to plant to animal.13 Orr believed that, despite some promising exceptions, research on the relationship between pastures and animal health had been pursued quite haphazardly until the 1910s and 1920s. Scientists in the nineteenth century widely recognized that the various \"ash\" (or mineral) constituents, such as calcium, phosphorus, sulfur, and iodine, were essential to the life of organisms as much as the energy-giving foodstuffs themselves. But knowledge of how particular minerals in the soil affect plant growth, and how the minerals in plants in turn affect animal health, remained rudimentary. Most attention in animal and human nutrition was devoted to the problem of intake and output in terms of energy, or in terms of protein, carbohydrate, and fat. The concern was for anabolism and catabolism rather than metabolism. 12 John Boyd Orr and Helen Scherbatoff, Minerals in Pastures and Their Relation to Animal Nutrition (London: H. K. Lewis & Co Ltd., 1929). Much of the Rowett Research Institute's pasture research was presented previously as a series of papers in The Journal of Agricultural Science 16 (1926). 13 Consistent with his forceful personality, Orr wrote Minerals in Pastures while his views on the centrality of mineral deficiency in animal and human disease were still being contested in his own research fiefdom. He eventually managed to drum his opponents out of the Rowett Research Institute. See David Smith, \"The Use of 'Team Work' in the Practical Management of Research in the Inter-War Period: John Boyd Orr at the Rowett Research Institute,\" Minerva 37 (1999): 259-280. 197 One reason for the relative underdevelopment of mineral nutrition was that most of the work in the nineteenth century was conducted from a purely economic point of view. The center of interest was maximizing the yield of the plant rather than its nutritional value for the animal. But, Orr argued, the sudden rise of nutrition science in the 1910s and 1920s aroused the interest of the physiologist and biochemist in pasture problems. These researchers differed from previous investigators since, in Orr's words, \"They are interested not in pasture, as such, but in the effects of different types of pasture on the nutrition of the grazing animal.\"14 As we have seen in previous chapters, discoveries in biochemistry and physiology in the early decades of the twentieth century began clearly demonstrating that minerals were not just the relatively inert materials that made up bones and teeth. They were constantly on the move, and played a necessary part in many dynamic metabolic processes involving the macro nutrients, vitamins, hormones, and enzymes. Orr was suggesting, in short, that pastures were increasingly being seen from a \"nutritional\" and \"veterinary\" rather than a purely \"economic\" point of view. From Orr's \"nutritional\" viewpoint, the world's pasturelands left much room for improvement. Minerals in Pastures challenged the previous assumption that pathological conditions in grazing animals due to mineral deficiencies in pastures were localized problems of little more than local importance. \"It is only in certain areas,\" Orr contended, \"that the deficiency of one of the elements is so extreme that the grazing animal develops symptoms and lesions sufficiently acute to warrant the term 'disease.' In much wider areas there is a general poverty in minerals which, while less extreme than that causing disease, is still sufficient to affect the feeding value of the pasture.\"15 Orr emphasized that, aside from 14 Orr, Minerals in Pastures, 3. 15 Ibid., 139. 198 the deficiency diseases caused by the absence or unavailability of trace elements such as iodine or iron in the soils of certain regions, the most widespread mineral deficiencies were of those needed in the greatest quantities by animals, namely calcium and phosphorus. These deficiencies, he contended, limited the rate of growth and productivity of the animal and rendered it more susceptible to certain diseases of bacterial origin.16 Orr, reflecting the heightened focus in the era of the newer knowledge of nutrition on growth and reproduction rather than sheer caloric expenditure, framed his argument in a somewhat novel way. Rather than focusing on maximizing animal weight gain, he used the mineral demands of lactation as a baseline for assessing the feeding value of pasture herbage. His rationale was that Milk is the food especially prepared by nature to supply all the essential constructive materials required for growth. It can be assumed, therefore, that the mineral content of milk corresponds closely with the mineral requirements of the young animal. It is obvious also that it will bear a close relationship to the requirements of the lactating animal.17 Orr mars haled evidence showing that, of all the feeds used at that time, only \"good pasture\" closely resembled cow's milk in terms of mineral content required per calorie. The other feedstuffs commonly given to cows at the time, such as maize, wheat, turnips, and cottonseed cake were found notably deficient in one or more of the minerals that were meas ured in the studies (calcium, phosphorus, sodium, potassium, chlorine, and nitrogen). Good pasture plants, either in the field or harvested and dried as forage, were considered the best feed for assuring that the needs of animals could be met at times of highest demand. But what exactly did Orr mean by \"good pasture\"? By the 1920s, investigations of the mineral content of plants confirmed the association that graziers had long empirically 16 Ibid., 4. 17 Ibid., 11. 199 understood between high-quality pasturelands certain plant namel and certain Leguminous plants such as clover, alfalfa, and vetch tend to have higher concentrations than grasses of all minerals, especially calcium, whereas grasses are relatively rich in phosphorus; deep-rooted plants are capable of surviving drought and drawing on supplies of minerals from the subsoil. Plant breeders, accordingly, had historically sought to promote these desirable species on cultivated pastures through selective breeding and the improvement of seed mixtures. Although Orr duly recognized the contribution of plant breeders, he argued that the most important variable in determining whether desirable plant species would thrive on a pasture, aside from grazing practices, was the fertility of the soil. Moreover, fertilization of pastures exerted an effect on the composition of the plants themselves. From the time that the methods to reliably meas ure the major minerals had been developed in the latter half of the nineteenth century, many different workers had found that the application on soils of fertilizers\u2014e.g., animal manure, bone meal (calcium phosphate mostly), agricultural lime (calcium carbonate), dolomite (calcium and magnesium carbonate), gypsum (calcium sulfate), potash (potassium carbonate), and various forms of phosphates\u2014improved the mineral content of the plants growing in them. Mineral concentrations in plants generally increased mark edly after the application of fertilizers onto poor and depleted pastures. Applications to already fertile fields did not have nearly as much beneficial effect, suggesting the existence of an \"ideal threshold\" for the mineral concentration of plants.18 Numero us studies cited by Orr demonstrated that fertilization of a pasture deficient in one mineral would frequently trigger a virtuous cycle, increasing the overall mineral 18 Orr, Minerals in Pastures, 47-48. 200 content of the herbage. Plants with a low percentage of one mineral tend to have low percen tages of others, owing to the fact that the available amount of one element becomes a limiting factor for the utilization of others, either by limiting the absorption of the others, or by limiting the growth and spread of the plant species with a high genetic requirement for minerals. As Orr put it, \"A rich soil tends to favor the spread of those species which are naturally rich in minerals and also tends to enrich the individual plant whatever its species.\"19 In this respect, the research on pastures was only reinforcing the \"Law of the Minimum\" that German agricultural chemist Justus von Liebig had popularized in the mid-nineteenth century.20 Another crucial discovery that Minerals in Pastures highlighted was that mineral content was not generally an independent variable in the overall composition of a plant. Analyses conducted in the 1920s showed a definite positive correlation in plants between their content of protein and minerals\u2014the \"constructive\" nutrients. On the other hand, comparatively little differen ce was found between the calorie content of the same weight of vegetation from good pasture and poor pasture, even though the mineral and protein content of the former was higher. Analysis of cultivated pastures that had been heavily fertilized and manured showed that the amount of the \"constructive\" nutrients in the herbage was actually too high for fattening animals, even though it was more than adequate for meet ing the needs of growing and lactating animals. On inadequate pastures or feeds, conversely, animals 19 Ibid., 46. 20 Popularized but did not invent: Carl Sprengel played an important role in the development of the Law of the Minimum, though Liebig never acknowledged Sprengel's discoveries and passed them off as his own. See R. R. van der Ploeg, W. B\u00f6hm, and M. B. Kirkham, \"On the Origin of the Theory of Mineral Nutrition of Plants and the Law of the Minimum,\" Soil Science Society of America Journal 63 (1999): 1055-62. 201 could literally be starved of minerals and protein despite consuming more than enough calories in the form of carbohydrate and roughage.21 For Orr, it was not only plant breeders and animal nutritionists who could benefit from being mineral-minded. The improving knowledge of mineral requirements in nutrition revealed that animal breeders were working at cross-purposes with nature. Animal breeders had historically been concerned with improving farm stock in a variety of ways: higher and better yields of milk, eggs, and wool; faster rates of growth; greater power output; and so on. By the 1920s, modern breeds of cattle were growing and producing milk at a much higher rate than their forebears. Such breeding improvements over time, Orr argued, could not have occurred without either an improvement in the nutritive value of pastures and forages or the placem ent of stock on naturally fertile pastures. On the other hand, when animals of an improved type, bred on mineral-rich pasture, would be transferred to a district with relatively poor pastures, the low mineral content was liable to be insufficient to support the rate of growth or the capacity for milk production. Consequently, malnutrition due to a deficiency of minerals would likely occur.22 As Orr found repeatedly in reports from around the world, this was precisely what happened when high-grade bulls were imported to \"grade up\" native cattle in many parts of the British Empire\u2014a growing trend in colonial \"improvemen t\" schemes at that time\u2014without insuring that the feed was sufficient to maintain the improved breed. The offspring from these crossbred cattle suffered higher rates of disease and mortality than the native cattle. Similarly, pathological conditions attributed to deficiencies of calcium, phosphorus, and other minerals were common among stock animals introduced onto natural, unfertilized 21 Orr, Minerals in Pastures, 11-18. 22 Ibid., 51-53 202 pastures. In this respect, the small wiry livestock of colonial Africa and the Scottish Highlands, for example, were not so much the result of backwardness in breeding, but of natural adjustment to the limits imposed by the local soils and vegetation. Stated in more contemporary terms, Orr was arguing that any attempt to improve an area's animal husbandry\u2014and therefore dietary standards\u2014required a wider \"ecosystemic\" understanding of the relationships between soil fertility, plants, and animals. This view gained increasing traction in the 1930s and 1940s, particularly in the United States, where interest became widespread in academia and government.23 For instance, in the 1938 USDA Yearbook of Agriculture, Soils and Men, Gove Hambidge contended that \"The whole subject of the effect of soil composition on plant composition, and through this on animal health, deserves the increas ing attention it is getting today.\"24 A year later, the chief of the USDA Bureau of Plant Industry, Eugene Auchter, wrote a similar plea for more research in the journal Science entitled \"The Interrelation of Soils and Plant, Animal and Human Nutrition.\"25 He conceded that agricultural scientists up to that point had \"more or less neglected the interrelationship between soil, plants, animals and man, although in nature this a fact, a reality.\" Echoing Orr, Auchter argued that this neglect was due to the fact that agricultural scientists had thought too largely in terms of quantity, including factors that interfere with quantity production, like plant diseases and insects. \"Surely it is just as much 23 Early surveys were Leonard A. Maynard, \"The Role of Pasture in the Mineral Nutrition of Farm Animals,\" American Society of Agronomy Journal 21 (1929): 700-08; Kenneth C. Beeson and L. A. LeClerc, \"The Significance of the Soil to Human and Animal Nutrition,\" Soil Science Society of America Proceedings 2 (1937): 335-341; C. A. Browne, \"Some Relationships of Soil to Plant and Animal Nutrition: The Major Elements,\" in Soils and Men: Yearbook of Agriculture, 1938 (Washington: 1938), 777-806. 24 Hambidge, \"Soils and Men\u2014A Summary,\" in Soils and Men: Yearbook of Agriculture, 1938, 36. 25 E. C. Auchter, \"The Interrelation of Soils and Plant, Animal and Human Nutrition,\" Science 89, no. 2315 (12 May 1939): 421-26. In the article, Auchter recognized Orr and Scherbatoff's Minerals in Pastures as a significant work in establishing the field of study. 203 our responsibility,\" Auchter suggested, \"to further the production of foods of the highest nutritional quality\u2014in other words, to dovetail agricultural production with human physiological needs; to move toward the ideal of a better nourished nation.\"26 Indeed, by the late 1930s, the links between nature and nutrition had become sufficiently suggestive for the US government to support a committed research institute to study them. In 1935, Congress passed the Bankhead-Jones Act to provide increased funding for the land-grant colleges suffering through the Depression. The Act, similar to the Development Act passed three decades earlier in Britain, was intended to provide for research into basic laws and principles relating to agriculture. Accordingly, forty percent of the total appropriation by Congress was designated a \"special research fund\" under the control of the Secretary of Agriculture, one-half of which was to be used for establishing and maintaining regional research laboratories. It was under this latter provision that the United States Plant, Soil, and Nutrition Laboratory was established in 1939 at Cornell University in Ithaca, New York. Leonard Amby Maynard, a highly regarded professor of Animal Nutrition at Cornell, was appointed the first director.27 The distinguishing feature of the laboratory's research program, in Maynard's words, \"was that soil scientists, plant scientists, and nutrition scientists would pool their knowledge and techniques in integrated experiments.\" The prime mover in establishing the laboratory was Auchter, a Cornell alumnus who had earlier consulted with Maynard and considered him the best choice for a director. Under the conditions of the Bankhead-Jones Act, the Cornell facility was established as a laboratory of the northeastern region, and thus required a memorandum of understanding with the directors of the twelve 26 Auchter, \"The Interrelation of Soils and Plant, Animal and Human Nutrition,\" 426. 27 Maynard's textbook Animal Nutrition, first published in 1937 (New York: McGraw-Hill) and reprinted and revised several times with co-author John K. Loosli, was a standard in the field for decades. 204 northeastern agricultural Experiment Stations. But the directors agreed to Auchter's more ambitious proposal that the laboratory be considered national in scope. Procedures were therefo re based on recommendations of an advisory committee of leading researchers in nutrition, many of whom were not located in the northeast and had no connection with the land-grant colleges.28 Aside from the Plant, Soil, and Nutrition Laboratory, many other research stations and universities inside and outside the United States studied environment-plant-animal relationships. One of the most significant discoveries was that Liebig's Law of the Minimum did not uniformly apply to plant nutrition. For example, in certain cases nitrogen fertilization was found to depress the concentration of calcium in plant tissues, even if yields were increased. Liming of the soil seemed to depress the concentration of phosphorus in plants under some conditions, while no effect was evident under others. On the other hand, the response of phosphorus concentration in the plant to application of phosphate fertilizers was found to be rather resistant to change, but the change that could be elicited was dependent to a large degree on the soil type and to some extent the supply of available magnesium.29 Some scientists even concluded that a lack of balance in the supply of nutrient elemen ts in the soil was more detrimental to plant growth than a deficiency of all the variable nutrients.30 From this growing number of seemingly conflicting findings on the effect of fertilization on plant composition, researchers came to realize, as agronomist George Ward put it, that \"inorganic nutrient absorption in the plant is not orderly in the sense of a 28 L. A. Maynard, \"The U.S. Plant, Soil and Nutrition Laboratory\" (1963), Box 9, LAM; H. H. Williams, \"Leonard Amby Maynard,\" The Journal of Nutrition, Vol. 104, no. 1 (January 1974): 7. 29 Kenneth Beeson, \"The Effect of Mineral Supply on Mineral Concentration and Nutritional Quality of Plants,\" Botanical Review, Vol. 12, no. 7 (July 1946): 424-55. 30 S. H. Wittwer, R. A. Schroeder and W. A. Albrecht, \"Interrelationships of Calcium, Nitrogen, and Phosphorus in Vegetable Crops,\" Plant Physiology 22, no. 3 (July 1947): 244-56. 205 manufacturing process.\"31 By the 1950s, crop and forage scientists had developed a much more nuanced view of plant nutrition than had existed at the beginning of the century. They found, in keeping with the Law of the Minimum, that a certain minimum concentration of each element was necessary to obtain any plant growth, with this concentration varying depending on the particular requirements of the plant species and variety. Increased amounts of the nutrient in the soil yielded more of a crop having the same concentration of elemen ts\u2014up to a point. But beyond that point, an increase in the supply of a nutrient elemen t resulted in increases both in crop yield and concentration of that nutrient\u2014what was termed the \"zone of poverty adjustment.\" Beyond this zone, the additional supply of elemen ts only increased the concentration of those elements in the crop\u2014the \"zone of luxury consumption.\"32 Thus, the quantity of minerals consumed by an animal or human from a plant could vary a great deal, depending on where the plant was along the spectrum of nutrient consumption. Examining these variations in nutrient uptake would be relatively simple if only one nutrient were involved. But as investigators realized, it is extremely complex when many nutrients are considered. Fertilization could change the status of one nutrient from poverty adjustment to luxury consumption, while reversing this situation for some other element due to increas ed yield. Moreover, aside from the known macro elements such as phosphorus and calcium, the list of the so-called trace elements considered essential for plant and animal health grew rapidly between the 1920s and the 1950s. To the previously known trace elemen ts such as sodium, iodine, zinc, and iron were added boron, cobalt, copper, 31 George M. Ward, \"Effect of Soil Fertility upon the Yield and Nutrition of Forages: A Review,\" Journal of Dairy Science, Vol. 42, no. 2 (1959): 277-97. 32 A. G. Norman, \"Influence of Environmental Factors On Plant Composition,\" in Centennial Symposium: Nutrition of Plants, Animals, Man (Michigan State University, 1955), 14-19. 206 manganese, molybdenum, selenium, and a number of others with possible biological functions.33 The growing body of research on the essential elements and the nutritional effect s of fertilization was, paradoxically, casting some doubt on the reductionist methods that had enabled agricultural chemists and plant and animal breeders since Liebig's day to dramat ically increas e yields. In the process of pushing up crop yields\u2014and therefore indirectly the output of animal products\u2014modern farming methods were increas ing the risk of creating deficiencies and imbalances. For instance, research ers found that soils intensively fertilized with the macro elements such as calcium and phosphorus produced forages low in iron, cobalt, copper, magnesium, and manganese, which when fed to cattle resulted in poor growth, anemia, and degenerative illnesses.34 In the late 1930s, to take another significant example, agronomists J. E. McMurtrey and W. O. Robinson raised concern that the development of highly purified fertilizer preparations in an effort to provide \"standardized\" products to farmers was actually increasing the likelihood of sulfur deficiencies in crops. Sulfur was known by that time to be especially important for the production of high-protein grains and legume forages. Sulfur had historically been applied unconsciously with phosphorus in the form of superphosphate and acid-treated bone meal. (In superphosphates, in fact, the sulfate content commo nly exceeds that of phosphates.) However, McMurtrey and Robinson feared that the use of \"highly concentrated fertilizers in which the phosphates carry no sulphates will create a sulphur deficiency unless sulphates are 33 Eric J. Underwood, Trace Elements in Human and Animal Nutrition (New York: Academic Press, 1962, 2nd ed.); and Howard B. Sprague and Stanley Arthur Barber, Hunger Signs in Crops: A Symposium (New York: McKay, 1964). Toxicity symptoms caused by an excess of \"trace elements\" in the soil, such as fluorine and selenium, also occupied the attention of plant and animal nutritionists at that time, but it will not be considered here. 34 H. A. Keener, F. E. Allen, H. A. Davis, K. C. Beeson, F. J. Thacker, \"Trace Mineral Deficiencies in Cattle Resulting from Heavy Fertilization of the Soil,\" Journal of Dairy Science 38 (1955): 626. 207 added.\"35 It was becoming clear, in other words, that an incomplete understanding of plant and animal nutrition, when applied to actual agricultural practices, could intensify nutrient deficiencies.36 Investigations into environment-food-nutrition relationships were not confined to minerals. A growing number of studies from the 1930s onward considered vitamins as well, and uncovered yet another set of variables that could influence the nutritional qualities of foods. Ascorbic acid (vitamin C) and the carotenes (vitamin A precursors) were two of the most intensively studied compounds. In both greenhouse and open field studies, light exposure was found to have a definite influence on the vitamin C content of plant tissue, the content increas ing with rising light intensities and duration of exposure. This meant that, under natural conditions, the ascorbic acid content of fruits and vegetables of the same variety could vary greatly depending on season, location, and weather conditions.37 Interestingly, investigators found in the 1940s that a high rather than low vitamin C concentration in leafy greens and fruits was associated with a reduction in yield due to nutrient deficiencies, particularly nitrogen. This finding accorded well with the discovery in the 1930s and 1940s that ascorbic acid, along with certain trace minerals (e.g., iron, copper, and manganese), played roles as catalysts in plant metabolism.38 Plant biochemists suggested that a rise in the concentration of these catalysts indicated a natural mechanism to make more effective use of elements deficient in the soil, much as the thyroid enlarges in the 35 J. E. McMurtrey and W. O. Robinson, \"Neglected Soil Constituents That Affect Plant and Animal Development,\" in Soils and Men: Yearbook of Agriculture, 1938 (Washington: GPO, 1938), 826-27. 36 For similar concerns about, e.g., nitrogen usage, see K. C. Berger, \"Increase Use of Nitrogen Fertilizer Accentuates the Need for Both Major and Minor Elements,\" in Anhydrous Ammonia Conference: Summary of Proceedings (University of Wisconsin, Department of Soils, 1954), 25-27. 37 Leonard A. Maynard and Kenneth C. Beeson, \"Some causes of Variations in the Vitamin Content of Plants Grown for Food,\" Nutrition Abstracts and Reviews 13 (1944): 155-164. 38 See William H. Schopfer, Plants and Vitamins (Waltham, MA: Chronica Britannica, 1949). 208 goiter-stricken humans in an attempt to produce more thyroxin when iodine is deficient, and as parathyroid glands increase their hormone production under calcium deficiency.39 A good deal of the research into the effect of environmental factors on the carotene content of plants focused on animal forages, since it was an important nutrient for reproduction and lactation in livestock.40 Young green plants high in protein and minerals, such as grasses, legumes, and other leafy herbage, were found to also contain relatively high levels of carotene; the concentration of these nutrients generally increased as the plant approached seeding stage and then decreasing thereafter. Some studies showed that the availability of boron in the soil exerted a pronounced effect on the quantity of carotene in plants. The influence of environmental factors\u2014e.g., soil fertility, sunlight exposure, climate\u2014on vitamins B, D, and E in plants was also studied, though research in this area was also mostly focused on animal feeds and forages.41 Investigations into the effect of fertilization on the B-complex vitamin content of plants showed varied results.42 39 S. H. Wittwer, R. A. Schroeder, and W. A. Albrecht, \"Vegetable Crops in Relation to Soil Fertility: II. Vitamin C and Nitrogen Fertilization,\" Soil Science 59, No. 4 (April 1945): 329-336. 40 However, studies of carotene content were also conducted on vegetables and fruits for human consumption. See, e. g., Leon Bernstein, K. C. Hamner, and R. Q. Parks, \"The Influence of Mineral Nutrition, Soil Fertility, and Climate on Carotene and Ascorbic Acid Content of Turnip Greens,\" Plant Physiology 20, No. 4 (October 1945): 540-72. There was also some evidence that boron fertilization could increase the carotene content of apples. 41 For a compilation of the data, see Donald F. Miller, Composition of Cereal Grains and Forages, National Academy of Sciences, National Research Council, Publication 585 (Washington: GPO, 1958). 42 Much of the early research on the B vitamins was conducted in the late 1940s and early 1950s at the Samuel Roberts Noble Foundation in Ardmore, Oklahoma. See, e.g., Thomas A. McCoy, Spencer Michael Free, Ruble G. Langston, Joseph Q. Snyder, \"Effect of the Major Elements on the Niacin, Carotene, and Inorganic Content of Young Oats,\" Soil Science 68, no. 5 (November 1949): 375-80; Thomas A. McCoy, David G. Bostwick, and A. Charles Devich, \"Some Effects of Phosphorus on the Development, B Vitamin Content, and Inorganic Composition of Oats,\" Plant Physiology 26, no. 4 (October 1951): 784-91; David G. Bostwick and Thomas A. McCoy, \"The Effect of Calcium on the B Vitamin and Inorganic Content of Oats,\" Proceedings of the Oklahoma Academy of Science for 1950, 112-15; Thomas A. McCoy, \"The Effect of Potassium on the B Vitamin and Inorganic Content of Oats,\" Proceedings of the Oklahoma Academy of Science for 1951, 96-99. 209 Meat and animal products were found to show little or no variation in vitamin, mineral, or amino acid content. Differences in soil fertility and the nutritional qualities of pasture plants and animal feedstuffs affected the stocking density, growth, productivity, and disease-susceptibility of animals, rather than the nutrient composition of animal tissues or animal products themselves.43 The exceptions to this rule, confirmed amply by investigations between the 1920s and 1950s, was the content in dairy products, eggs, and organ meats of fat-soluble vitamins A, D, and E, as well as certain trace elements such as iodine and selenium. Notwithstanding differences due to animal breed, the concentration of these nutrients could vary greatly depending on what the animal ate and its level of exposure to ultraviolet light. These studies on the nutrient content on animal products buttressed the generally growing favor among agronomists for fresh and dried green forage plants and exposure of livestock to the outdoors.44 \"Food Is Fabricated Soil Fertility\": Soil Science Meets Nutrition Science Although questions concerning the nutritional quality of foods were closely bound up with research into fertilization practices, crop improvemen t, and animal husbandry, the 43 There was a great deal of speculation on this score. Workers in the Departments of Soil Science, Farm Crops, Dairy, Foods and Nutrition, and Agricultural Chemistry at Michigan State University decided to definitively settle this question in a series of collaborative studies conducted from the mid-1940s to the mid-1950s. A summary of their research, along with contributions from such figures as Kenneth Beeson, William Albrecht, and E. Neige Todhunter is found in Centennial Symposium: Nutrition of Plants, Animals, Man (Michigan State University: 1955). 44 The literature from the period on this subject of animal products is vast. Cf. R. M. Bethke, D. C. Kennard, and H. L. Sassaman, \"The Fat-Soluble Vitamin Content of Hen's Eggs as Effected by the Ration and Management of the Layers,\" Journal of Biological Chemistry 72 (1927): 695-706; H. Ernest Bechtel and C. A. Hoppert, \"A Study of the Seasonal Variations in the Vitamin D Content of Cow's Milk,\" Journal of Nutrition 11, no. 6 (1936): 537-49; J. E. Campion, K. M. Henry, S. K. Kon, and J. Mackintosh, \"The Source of Vitamin D in Summer Milk,\" Biochemical Journal 31, no. 1 (January 1937): 81-88; J. W. Lord, \"Seasonal Variation of Carotene and Vitamin A in Butter-Fat and in Serum,\" Biochemical Journal 39, no. 4 (1945): 372-74; A J. W. Hibbs, W. E. Krauss, and C. F. Monroe, \"The Relation of the Caretenoid and Vitamin A Content of Summer Milk to the Caretenoid Content of the Pasture Herbage,\" Journal of Dairy Science 32, no. 11 (1949): 955-60. 210 nutritional ecology outlook was not rooted solely in applied agronomy. The development of nutritional ecology was also part of a contemporary conceptual revolution in soil science, a sprawling field that straddled many of the disciplines in the natural and applied sciences. In the first half of the twentieth century, workers in a variety of fields made rapid advances in understanding how physical, chemical, and biological forces interact in dizzyingly complex ways to form soils of varying fertility and nutritional potential. To understand the origins of nutritional ecology, then, it is necessary to delve briefly into the details of some of these advances. The paradigm shift in soil science was most drastic and probably had the greatest impact on the nutritional ecology perspective in the United States.45 The United States was initially a laggard in soil science, due largely to the fact that there was such an abundance of new land to use that Americans did not feel a pressing need to study it systemat ically.46 But in the early decades of the 1900s, American soil science experienced a productive synthesis of two schools of thought that had emerg ed earlier in Europe. In Western Europe, German chemist Justus von Liebig and the \"balance-sheet\" theory of plant nutrition that he popularized heavily influenced agricultural thinking. The main premise of this theory, in Liebig's words, was that \"the crops on a field diminish or increase in exact proportion to the diminution or increas e of the mineral substances conveyed to it in manure.\"47 The soil was considered a more or less static storage bin of chemical nutrients, with variations in natural fertility arising primarily from the composition of underlying geological formations. Little attention was paid to the role of living organisms in soil generation and structure. 45 But it also registered an effect in Britain and the British Commonwealth, of which more below. See, e.g., Plant and Animal Nutrition in Relation to Soil and Climatic Factors (London: HMSO, 1951). 46 Charles Kellogg, The Soils That Support Us (New York: Macmillan, 1941), 9. 47 Quoted in Charles E. Kellogg, \"Soil and Society,\" in Soils and Men: Yearbook of Agriculture, 1938, 879. 211 But beginning in 1870s, the Russian school of soil science under the leadership of V.V. Dokuchaev and N.M. Sibirtsev developed a new concept of soil formation. While investigators in Western Europe were thinking about soils mostly in terms of getting bigger yields of crops to feed a rapidly growing population, those in Russia were thinking about soils in terms of a great, undeveloped empire. Whereas scientists in Western Europe were confined to a region having about the same general features of climate and vegetation, the extensive Russian Empire contained strongly contrasting landscapes. There were also differen ces in method. While agricultural chemists in Western Europe mainly studied specimens of soils in laboratories and small experimental plots, the Russian scientists concentrated on the examination of soils in the field. The Russian scientists, working in this less controlled context, developed a view of soils as independent natural bodies distinct from the underlying geological formations. In their view, each soil possessed unique properties resulting from a combination of parent geological material, climate, plants and animals, topography, and time. Topography and parent material were seen as more or less passive and of local influence, whereas living matter and climate were active factors responsible for broad regional characteristics and for the great soil belts running through the Empire. The early soil surveys in the United States were not influenced in any substantial way by the Russian concepts. Milton Whitney, the first head of the USDA Soil Survey begun in 1899, did not incorporate them into his soil classification scheme. But he also rejected the balance-sheet theory. Agricultural scientists in Whitney's time knew that plants responded differently to equal applications of fertilizers in different soils. Whitney came to the conclusion that most soils contained sufficient and inexhaustible amounts of nutrients for crop growth. He insisted that differences in productivity arose from differences in soil texture, which was mostly influenced by temperature and moisture, and also by fertilizers. 212 But Whitney's views were not by any means universally accepted. Whitney sparred with the early \"permanent\"\u2014or what we would today call \"sustainable\"\u2014agriculture luminaries Franklin Hiram King, an agricultural physicist at the University of Wisconsin, and Cyril Hopkins, an agricultural chemist at the University of Illinois.48 King and Hopkins adhered for the most part to the balance-sheet theory, but also emphasized the value of soil organic matter; both scientists achieved local success using intensive \"European\" methods of fertilization and crop rotation. Whitney had a similar conflict later with George Nelson Coffey, a Bureau of Soils employee who had the opportunity to see a wide variety of soils in the United States. While working as a soil surveyor, Coffey encountered the Russian school's theory regarding the multiplicity and interdependence of factors in soil formation. Although he faulted the Russians for basing their classifications too much on climate, he applied their concepts to his analysis of the soils he had studied across the country, which was published as a USDA Bulletin in 1912. Coffey criticized his colleagues for overemp hasizing geology in soil formation and soil classification, but he failed to shake the Bureau of Soils\u2014or rather Whitney, whose personal theories were \"official\" bureau policy\u2014from its position.49 Whitney also publicly argued with agricultural chemist Eugene W. Hilgard of the University of California. Contemporary with the Russian school, Hilgard came to somewhat similar 48 Their best-known works are: Franklin H. King, Farmers of Forty Centuries, or Permanent Agriculture in China, Korea, and Japan (Madison, WI: Mrs. F. H. King, 1911), which became part of the canon of the early organic farming movement, and Cyril G. Hopkins, Soil Fertility and Permanent Agriculture (Boston, New York: Ginn, 1910). 49 Eric C. Brevik, \"George Nelson Coffey, Early American Pedologist,\" Soil Science Society of America Journal 63, no. 6 (November-December 1999): 1485-93. 213 conclusions regarding the correspondence among soil regions, biological regions, and climatic belts.50 But soon after Coffey's initial failure, the Russian theory succeed ed in making an enormous impact on soil science in the United States. Whitney's successor as head of the Soil Survey, Curtis Marbut, was previously a geologist at the University of Missouri and initially shared Whitney's views. But in 1915 he encountered the Russian concepts through a German translation of Konstantin Glinka's Pochvovedenie Verbreitung (1914). Marbut subsequently shifted his thinking in line with the Russian ideas, and in fact developed a strong antipathy concerning the influence of bedrock on soil characteristics. He wrote a series of influential soil science works in the 1920s and 1930s, including a partial English translation of Glinka's 1914 book entitled The Great Soil Groups of the World and Their Development (1927). Many of the terms invented by the Russians to classify soil types, such as podzol, chernozem, sierozem, and solonetz, were imported whole-cloth into the new American system. By the time of his death in 1935, Marbut's ideas had been received with overwhelming approval inside and outside the United States.51 Soil scientists' improving grasp of what caused broad geographical variations in soil fertility intersected in highly interesting ways with the newer knowledge of nutrition. The work of University of Missouri soil scientist William Albrecht perhaps best illustrates the way in which this intellectual crossover occurred. Albrecht was raised on a farm in central Illinois, the seventh of eight children, and throughout life he retained an abiding interesting 50 Eugene W. Hilgard, Soils: Their Formation, Properties, Composition, and Relation to Climate and Plant Growth in the Humid and Arid Regions (New York: Macmillan, 1906). 51 For the early history of the Soil Survey, see T. R. Paton and G. S. Humphreys, \"A Critical Evaluation of the Zonalistic Foundations of Soil Science in the United States. Part I: The Beginning of Soil Classification,\" Geoderma 139 (2007): 257-67; E. Cartography: 1924-1974,\" Geoderma 12 (1974): 347-62. 214 in all things agricultural. Since Albrecht, like John Boyd Orr, initially intended on entering medical school, his coursework at the University of Illinois was primarily in the basic sciences and liberal arts rather than the applied agricultural sciences. But he eventually realized that his real interest was in life and biology, and he returned to the University of Illinois to study agriculture. His background and roundabout education produced a distinct attitude. He saw agricultural problems mainly within the context of a larger \"master design\" in nature and life, which required an understanding of the basic sciences as well as practical skills in agronomy.52 In 1916, Albrecht joined the research and teaching staff at the University of Missouri College of Agriculture. He began by studying legumes, which had been the subject of intensifying scientific interest since the late nineteenth century. Farmers had known the special value of legumes in maintaining soil fertility since at least Roman times. By the nineteenth century, agricultural practices in Europe and Asia had progressed to the point that \"green manuring\" and intercro pping using legumes had become common. Some scientists suspected that legumes enriched soils because they could obtain nitrogen from the atmosphere and incorporate it into their tissues. But the mechanism whereby this took place was unknown until the end of the nineteenth century, when German scientists discovered the rhizobium bacteria, soil microorganisms that accumulate atmospheric nitrogen after becoming established inside the root nodules of legumes. These bacteria provide legumes with nitrogen to build their tissues and seeds, and in exchange get carbohydrate synthesized by the plant\u2014a symbiotic relationship. Liebig's balance-sheet theory of the soil was dealt a serious blow by the discovery of the nitrogen-fixing activity of this symbiosis. At least 52 Charles R. Koch, \"William Albrecht Sums up a Career in Soil Research,\" The Farm Quarterly 14, no. 4 (Winter 1960): 65, 112-113; William Albrecht, Charles Walters, Jr., ed., The Albrecht Papers: Volume I (Kansas City, MO: Acres U.S.A.), ix-x. 215 regarding nitrogen, soils under proper crop management did not resemble an input-output system requiring fertilizers. But legumes, in order to thrive, seemed to have special nutritional deman ds of the soil. When Albrecht began his career in the late 1910s, farmers and agricultural scientists widely accepted that liming soils was a valuable, and often necessary, treatment for establishing good leguminous crops. The line of reasoning was that since limestone applied on the soil lessens its acidity, and since limestone applied on the soil helps to grow legumes, the change in acidity caused the growth of legumes. Albrecht led a series of investigations in the 1920s that challenged this idea. He came to the conclusion that it was the calcium in the lime, as a nutrient, that improved the capacity for legumes to grow and the symbiotic rhizobia to accumulate nitrogen. Albrecht was able to promote legume and rhizobia growth in pot experiments by applying sources of calcium, such as calcium chloride or calcium sulfate, which did not substantially change or actually increas ed soil acidity. In Albrecht's view, too little science had led agronomists astray, since the neutralizing activity of lime on acidic soils was only contingently related to the beneficial role of its calcium in plant and microbial nutrition.53 Albrecht argued that, because the instruments to easily meas ure hydrogen ion activity (pH) came along before those to meas ure calcium ion activity (pCa) in a soil, agricultural scientists unfortunately paid more attention to the alkalinizing property of limestone than to the nutritional value of its calcium ion.54 53 According to a more recent assessment, the design and interpretation of Albrecht's experiments were flawed in certain respects. See Peter M. Kopittke and Neal W. Menzies, \"A Review of the Use of the Basic Cation Saturation Ratio and the 'Ideal' Soil,\" Soil Science Society of America Journal, Vol. 71, no. 2 (March-April 2007): 260-61. The attention Albrecht and his team drew to the calcium needs of legumes, however, was an original and lasting contribution. 54 A summary of his early work on calcium and legumes can be found in W. A. Albrecht, \"Nitrogen-Fixation as Influenced by Calcium,\" in Proceedings and Papers of the Second International Congress of Soil Science, Leningrad-Moscow, USSR, July 20-31, 1930 (1932); idem, \"Physiology of Root 216 Albrecht's work on legumes and calcium accorded well with what contemporary soil scientists had determined regarding broad regional differences in soil types. In his 1928 classification system, for example, Marbut used variations in the concentration of calcium carbonate in the horizons of the soil profile as his first and foremost criterion of soil differen tiation.55 He divided the great soil groups of the world into those with (\"Pedocals\") and those without (\"Pedalfers\") a horizon of calcium carbonate accumulation. The availability or unavailability of calcium to microbes and plants was central to the soil formation processes that American soil scientists had identified by the late 1930s. They termed these processes calcification, podzolization, laterization, salinization, solonization, solidzation, and gleization. Although soil scientists subsequently discarded most of these terms and adopted new classification systems, the broad-scale processes identified earlier have largely stood the test of time. I will briefly describe the processes most relevant to the development of the nutritional ecology perspective: calcification, podzolization, and laterization.56 The process termed as calcification is typically maintained under natural grasslands with restricted rainfall. Precipitation in grasslands causes sufficient leaching to move calcium to just below the solum (subsoil layer), but not so far that plant roots cannot reach them. The grassland plants bring large amounts of these bases (or cations) to the surface, and by incorporating them into their tissues keep them in a relatively immobile state in the form of organic matter. Since there is not enough percolating water to leach the positively Nodule Bacteria in Relation to Fertility Levels of the Soil,\" Soil Science Society of America Proceedings, Vol. 2 (1937): 315-27. 55 Curtis F. Marbut, \"A Scheme for Soil Classification,\" Transactions of the First International Congress of Soil Science, Proceedings, Vol. 4 (1928) in The Life and Work of C. F. Marbut (Soil Science Society of America, 1942), 143-169. 56 The description of these processes here is summarized from H. G. Byers, Charles E. Kellogg, M. S. Anderson, and James Thorp, \"Formation of Soil,\" in Soils and Men: Yearbook of Agriculture, 1938, 948-78; and Kellogg, The Soils That Support Us. 217 charged calcium cations out of the soil, the negatively charged soil colloids\u2014the fine portion of the soil particles, both organic and mineral\u2014adsorb much of it. These tiny colloidal particles influence the ability of the soil to hold nutrients and water, and to furnish them to growing plants. The soil colloids also affect the physical characteristics of a soil. Clay particles saturated mostly with calcium cations do not puddle easily, but rather cling together in soft clumps, crumbs, or granules. (Farmers had long known that applying lime to heavy, acidic soils would them more friable.) The high mineral and protein content of grassland organic matter promotes strong bacterial activity, which is favorable for the production of humus. Decomposition by soil microorganisms such as bacteria, actinomycetes, fungi, and animals is the reverse of the constructive process of plant growth. Growing plants, using the energy of the sun, bring carbon, nitrogen, and all other elements together into complex compounds such as sugars, cellulose, lignin, sterols, and amino acids. Microorganisms, along with small animals such as worms and insects living on and in the soil, eventually break down the complex constituents of dead vegetation into carbon dioxide, mineral salts, and nitrogen in the form of ammo nia, which is converted into the soluble nitrate form. This process makes nutrients available for new plant growth, starting the cycle afresh. The bodies of dead organisms and the residues of living matter on and in the soil form the material known as soil organic matter. Humus, a vital component for good soil structure and steady nutrient release to plants, represents an intermediate stage in the decomposition of soil organic matter between its raw state and complete mineralization. The microbial breakdown process also provides organic acids in the soil water, which have a much stronger solvent effect than rainwater on mineral compounds. 218 All soil microorganisms require carbon as \"fuel,\" but bacteria have higher requirements than fungi for nitrogen as well as bases such as calcium. Grasslands provide enough of these materials for strong bacterial activity. Fresh organic matter is usually characterized by a large amount of carbon in relation to nitrogen; or so far as bacteria are concerned, a wide ratio of fuel to building material. Fresh organic matter (straw, leaves, woody debris) may have a ratio that is very wide, so that it decomposes very slowly. If turned into the soil by cultivation, fresh, un-decomposed organic matter can actually temporarily reduce the amount of nitrogen available for growing plants. But if the carbon-nitrogen ratio is narrower, decomposition may take place more quickly. As Albrecht had demonstrated, in calcium-rich soils excellent legume growth results, and correspondingly large nitrogen additions can be made and retained. This process results in heavier and more proteinaceous plant growth, including greater root development, the decomposition of which is the most effective means of introducing organic matter into the soil. As Albrecht put it, \"Liberal calcium supplies and liberal stocks of organic matter are inseparable.\"57 It was this interact ion between chemical, physical, and biological forces that created the immensely fertile \"chernozem\" soils of the world's grassland regions, as for example on the Great Plains of North America, the Black Earth region running through Ukraine and southern Russia, and the pampas of South America. In progressively drier climates the soils become lighter in color, the calcium carbonate layer gets closer to the surface, and plant growth decreases, resulting in what soil scientists labeled chestnut soils, brown soils, sierozems, and desert soils. Under arid conditions with low rainfall and high rates of evaporation, salts are not leached from the surface layer, resulting in basic or alkaline soils. On the other hand, in 57 William Albrecht, \"Loss of Soil Organic Matter and Its Restoration,\" in Soils and Men: Yearbook of Agriculture, 1938, 357. 219 moving from grasslands to the more humid prairies and mixed prairie-forest landscapes, the horizon of carbonate accumulation becomes thinner and lower down in the soil horizon, and slightly acidic soils tend to develop. Whereas semiarid grassland regions are characterized by calcification, the process of podzolization occurs in humid regions under forest vegetation, typical in the eastern half of North America and much of Western Europe. Podzolization is one of the processes in the formation of the so-called Pedalfer soils. (\"Pedalfer\" was Marbut's contraction of Ped = soil, Al = aluminum, and Fe = iron, indicating the elements dominant in the solum; \"Ped ocal\" suggested the presence in the solum of Ca = calcium.) Podzolization occurs where there is sufficient rainfall to remove the more soluble mineral elements like calcium and magnesium and where not enough of them are returned to the surface by the vegetation to prevent the development of acid soil conditions. Podzols tend to be dominated by coniferous forests, which have much lower requirements for the readily leached minerals than grassland plants. Lacking these minerals, forest vegetation is composed almost entirely of carbonaceous bulk in the form of cellulose, starch, and lignin\u2014the products of sunlight, air, and water, but with relatively little protein \"biosynthesis,\" as Albrecht termed it, carried out by plants beyond that. Moreover, the high carbon-to-nitrogen ratio and low mineral content of the organic material results in the dominance of fungi over bacteria in forest decomposition processes. Forests accumulate carbonaceous and acidic organic matter on the soil surface in the form of leaves, twigs, branches, and trunks, which unlike organic material in grasslands and prairies, goes through a much slower cycle of decomposition and mineralization before it becomes living matter again. Under forests, the humus layer is meas ured in inches rather than feet. The soil microorganisms also compete with plants for the available nitrogen, slowing 220 decomposition and protein synthesis. Since trees and forest vegetation return but little calcium, the fine clay or colloid particles become saturated with positively charged hydrogen. Clay particles dominated by hydrogen cations do not gather readily into crumbs or clusters, but rather disperse in water and move with it, creating a dense clay-rich horizon under the organic matter layer on the surface. As conditions become less moist and humid, the normal podzols are replaced by \"gray-brown podzolics,\" where the same process of podzolization operates but under more lime-rich conditions congenial the growth of deciduous trees, which as a rule have higher mineral requiremen ts than conifers. In a few places in Europe and the United States under broadleaf trees with calcium-rich parent geological material, neutral chernozem-like soils developed. But most soils developed under forests and ample precipitation and humidity are light colored, acid, and leached of calcium. Higher temperatures intensify the fertility-reducing effects of high rainfall and humidity. With increasing moisture and temperature, bedrock weathering intensifies, as does the breakdown of organic matter by microorganisms. For this reason, soils in the United States tend to have a decreasing proportion of organic matter as one moves from north to south. And under warm and wet conditions, silicon is weathered from mineral particles, leaving behind clays dominated by iron and aluminum with a low capacity for cation exchange. This process created the reddish or\u2014under conditions of poor drainage\u2014yellowish color of tropical soils. Accordingly, the humid subtropical southeastern United States is charact erized by humus-poor and easily exhaustible reddish-brown and yellowish-brown soils, which occupy a transitional state between the red laterite soils of the humid tropics and the brown podzols of the humid temperate regions. From these premises, Albrecht developed what he called a \"biotic geography.\" The process of soil calcification involved a positive ecological cycle\u2014positive at least from an 221 agriculturally utilitarian perspective\u2014between climate, minerals, nitrogen, bacterial activity, and organic matter. Podzolization and laterization, on the other hand, represented a negative cycle. Albrecht focused on the role of these soil formation processes in animal and human nutrition. He reasoned that since an abundance of calcium in the soil layer resulted in vegetation higher in other minerals as well as protein, climatic variations also indirectly determined what kind of animal life could exist on a landscape: The calcareous soils serve to represent proteinaceous and mineral-rich crops, while leached soils represent carbonaceous crops. In terms of animals and their nutrition the former are the regions of growth or of 'grow' foods, the latter are the regions of energy or 'go' foods.58 Albrecht often pointed out that, before the arrival of European settlers, it was the grasslands of North America that had supported the massive herds of bison and other large grazing animals. In comparison, the forested soils that dominated the humid eastern half of the continent, with the exception of certain abnormally fertile areas such as the Kentucky Bluegrass region, could only support smaller and more scattered wildlife.59 In Albrecht's view, differences in soil fertility also mapped onto the regional and global specializations in agriculture that had developed by the first half of the twentieth century. In the United States, the mid-continental grasslands sustained the reproduction and growth of cattle, which were sent eastward to the gray-brown podzolic soils of the more humid Corn Belt to be fattened on carbonaceous feeds. It was the lime- and humus-rich chernozem of the Great Plains, once broken by the plow and connected to distant mark ets by railroad, that provided the high-protein \"hard\" wheats prefer red for bread making. Wheats grown further east tended to contain a lower proportion of protein and were therefore 58 William Albrecht, \"Soil Fertility and National Nutrition,\" Journal of the American Society of Farm Managers and Rural Appraisers (April, 1944): 60, File A128, PPNF. 59 William Albrecht, \"Soil Fertility and Wildlife\u2014Cause and Effect,\" Transactions of the Ninth North American Wildlife Conference (Washington: American Wildlife Institute, 1944), 19-28. 222 \"softer.\" Valuable animal feeds such as alfalfa grew readily in Kansas, Nebraska, Colorado, and in other regions with less leached soils. Similarly, it was widely recognized that diseases and reproductive problems associated with nutritional deficiencies in dairy and livestock herds were more prevalent in the eastern and southern areas of the country, particularly on the sandy coastal plains soils of the Atlantic and Gulf States, than in the mid-continent. With the exception of such areas as alluvial bottomlands and the Appalachian limestone valleys, the highly weathered soils of the Southeast were known for producing mainly carbonaceous and fibrous crops such as cotton, corn, and sugar.60 Since European settlers had tried to superimpose agricultural systems on soils that could not sustain them, Albrecht concluded that \"deficiencies are the expectable, not the unusual.\"61 Albrecht was arguing, in other words, that the most verdant and amply watered environments, despite outwardly visible signs of abundant life, did not generally have the most potential for animals with relatively high nutritional requirements\u2014including humans. Albrecht corroborated his \"biotic geography\" with statistics gathered in the United States during World War II showing an inverse relationship between regional soil fertility and rates of dental decay and draft rejection due to ill health. In both cases, the inhabitants of the mid-continental grassland soils fared best.62 60 This line of argument has recently been revived. See Douglas Helms, \"Soil and Southern History,\" Agricultural History 74,no. 4 (Autumn 2000): 723-58. 61 William Albrecht, \"Nutrition Via Soil Fertility According to the Climatic Pattern,\" in Plant and Animal Nutrition in Relation to Soil and Climatic Factors, Proceedings (London: HMSO, 1951), 384-97. 62 For a synopsis of his perspective, see William Albrecht, \"Soil Fertility and Biotic Geography,\" Geographical Review 47, no. 1 (January 1957): 86-105; William Albrecht, Soil Fertility and Animal Health (Webster City, IA, 1958). For his work on soil fertility-dental decay relationships, see William Albrecht, \"National Pattern of Dental Troubles Points to Pattern of Soil Fertility,\" Journal of the Missouri State Dental Association 28, no. 6 (June 1948): 195-201; and William Albrecht, \"Soil Fertility and Its Health Implications,\" American Journal of Orthodontics and Oral Surgery 31, no. 5 (May 1945): 279-86. 223 In Albrecht's view, these ecological and nutritional patterns applied not just to the United States, but also to the entire planet. He saw the fertility pattern in Europe as more or less a mirror of the pattern in North America, with the concentrated urban populations on the podzolic soils of northwestern Europe. In the course of its rise to global preeminence in the nineteenth century, Britain reached across the oceans to the fertile wheat and meat belts of North and South America, South Africa, and Australasia. And, as Albrecht pointed out in 1945, \"the hard wheat belt of the Russian chernozem soils has been the fertility goal under the Hitlerite move eastward.\"63 For Albrecht, then, soil-food-nutrition relationships played a major but often unrecognized role in global geopolitics and history. Although Albrecht carried the nutritional geography idea unusually far, he was working in the same conceptual territory as other figures in the field. Kenneth Beeson, writing about the United States in the early 1940s, was concerned that Most of our large cities are located on the well-leached Podzols and podzolic soils that are acid in reaction and lower in the bases. The truck farms are of necessity located on the same soils, and large quantities of truck crops and fruits are shipped to these cities from localities where nutritional disorders due to mineral deficiencies in the soils have been noted in both humans and animals.64 Bees on's fears were not based entirely on speculation. Using data compiled from several hundred published foreign and domestic reports and from unpublished data submitted by agricultural experiment stations and various bureaus of the USDA, Bees on found that the mineral concentration of fruits, vegetables, and forage plants ranged widely. For example, the calcium content of cabbage, one of most commo nly eaten \"protective foods,\" varied from approximately 0.4 to 1.6 percen t of dry weight. 63 \"Food Is Fabricated Soil Fertility,\" in Nutrition and Physical Degeneration (Redlands, CA: The Author, 1945). 64 Kenneth Beeson, \"The Mineral Composition of Crops with Particular Reference to the Soils in which They Were Grown: A Review and Compilation,\" United States Department of Agriculture Miscellaneous Publication No. 369 (March 1941), 4. 224 Albrecht's nutritional geography was also corroborated shortly after World War II in a study directed by Rutgers University soil scientist Firman Bear called \"Reg ional Variations in the Mineral Composition of Vegetables.\" Samples of lettuce, cabbage, snapbeans, and tomatoes were collected from commercial farms in Georgia, South Carolina, Virginia, Mary land, New Jersey, New York, Indiana, Illinois, and Colorado. The analysis showed that as one moved from east to west and south to north across the United States, the percentage of minerals in the vegetables generally increased. Bear's study and others emphasized, however, that local differences in soil parent material, plant type, climatic factors, and the fertilization practices on particular farms produced a large degree of variability in the mineral content of crops.65 For example, the phosphorus concentration of vegetables in Colorado was often lower than those from the East and South, where phosphate fertilization in preparation for planting was much more commonplace. On the other hand, the calcium concentration of vegetables in Colorado was found to be higher, even though farmers there did much less liming than those further east. The interpretation of landscapes according to their nutritional potential was also buttressed by increas ingly detailed quantitative knowledge of the flow of minerals from soil to plant to animal. \"Plants possess a very evident faculty,\" as agronomist C. A. Browne remarked in 1938, \"of assimilating certain elements, as calcium, potassium, magnesium, phosphorus, and sulfur, in much greater quantities than their abundance in the soil might lead us to suppose, and of more or less effectively rejecting other elements such as 65 Firman E. Bear, S. J. Toth, and A. L. Prince, \"Variations in Mineral Composition of Vegetables,\" Soil Science Society of America Proceedings 13 (1948): 380-84; Kenneth Beeson, \"The Effect of Mineral Supply on the Mineral Concentration and Nutritional Quality of Plants,\" Botanical Review 12, no. 7 (July 1946): 424-55. 225 aluminum, the second most abundant mineral constituent of soils.\"66 Chemists found that animals have a similar but less pronounced ability to further segregate the mineral elements assimilated from the soil by plants. They determined, for example, that the average ratios of calcium in the mineral elements of soils, plants, and humans are approximately 1:8:40, those of phosphorus 1:140:200, and those of sulfur 1:30:130, respectively. On the other hand, potassium, a macronutrient in the plant, was less valuable to animals and thus present in lower concentrations in their tissues. As the above description of soil formation processes indicated, these ratios could vary greatly from region to region, and within regions. Soil and vegetation arrays differed in their capacity to provide the minerals most highly concentrated by\u2014and thus usually in scarcest supply for\u2014livestock and humans. It was not just the growing knowledge of mineral metabolism that affected geographic perceptions of nutrition. Price, for example, argued in 1930 that \"one of the most universal stresses of life is produced by the difficulty in obtaining the fat-soluble activators essential for mineral utilization. There are only two considerable sources for the factors, namely the butter fats of milk and some of the fish oils.\" From this premise, Price hypothesized that \"all the civilizations of the past and present were built around these two sources of fat-soluble activators. It is probable that in the past the difficulty in obtaining the fat-soluble activators has constituted one of the most exacting limitations on the development of mankind.\"67 66 C. A. Browne, \"Some Relationships of Soil to Plant and Animal Nutrition\u2014The Major Elements,\" in Soils and Men: Yearbook of Agriculture, 1938, 777-806. Contemporary investigations into plant metabolism were also uncovering why this \"selective concentration\" took place: see R. W. Thatcher, \"A Proposed Classification of the Chemical Elements with Respect to Their Functions in Plant Nutrition,\" Science 79, no. 2065 (25 May 1934): 463-66. 67 Weston Price, \"Some Means of Improving Human Life by Increasing the Vitamin Content of Milk and Its Products\" (Read before International Association of Milk Dealers at Cleveland, Ohio, October 22, 1930), The Association Bulletin, no. 10 (29 Jan. 1931): 36. 226 The upshot of all this research into the formation and flow of nutrients was that both wild and human-altered environments could be evaluated for their capacity to supply the substances most valuable for human nutrition. \"Fertile\" soil and \"nutritious\" food were linked to an overall vision of \"healthy\" landscapes. Leonard Maynard conveyed such an outlook in a 1951 address: The soil which is either naturally of high fertility or kept in such a state by fertilizer additions and appropriate management provides the opportunity for a wider choice in the kind of crops that can be grown, and thus, for the introduction of plants having a higher nutritional quality. For example, if a soil is suitable for growing a legume such as alfalfa instead of a grass such as timothy, the forage produced on this soil can be markedly richer in protein, calcium and carotene.... Correcting the mineral deficiencies of the soils in certain areas has had a large effect in increasing animal production and thus the supply of animal products which provide the \"protective foods\" in man's diet.68 As Maynard's comments indicate, the nutritional ecology perspective was underpinned by an optimistic sense that what was good for the soil was also good for public health, and that at least some progress had already been made in improving both. Restoring Eroded Bodies: Soil Conservation and Nutritional Ecology This sort of optimism notwithstanding, the nutritional ecology perspective emerged in an atmosphere of increasing concern over the threat to society posed by land degradation and declining soil fertility. Given the strong connections that had grown between the sciences of nutrition, agronomy, and soil by the mid-twentieth century, it is not altogether surprising that soil erosion was also seen as a nutritional threat. Frank Gilbert was 68 Leonard A. Maynard, \"Effect of Soil Fertility and the Use of Commercial Fertilizers on the Nutritive Value of Food Crops and thus on Food and Animal Health,\" Delaney Committee 1951-52, Box 9, Leonard Amby Maynard Papers, Carl A. Kroch Library, Cornell University, Ithaca, NY. 227 expressing a widely held view when he warned in 1948, \"Human health goes with the soil and its fertility.\"69 The fears of environmental and nutritional degradation that seemed to suddenly plague thinking about agriculture in the 1930s and 1940s were not solely the fabrication of alarmist scientists excited by a novel viewpoint. There was also widespread unease toward the long-term prospects of agriculture during that period. As discussed in chapter one, farmers recovered precariously and unevenly through the 1920s and then suffered badly through the economic depression of the 1930s. The overall state of despair was heightened by a growing body of evidence reporting extensive soil erosion in many regions of the world. In the United States, warnings of the threat to society posed by erosion and soil-destroying farmi ng practices had periodically appeared since the nineteenth century, but they became louder and more persistent in the 1920s. 70 Hugh Hammo nd Bennett, a researcher at the USDA Bureau of Soils, grew increasingly alarmed by the high rates of erosion he found in his soil surveys of the South. After launching a national campaign to publicize his work, Bennett succeeded in 1929 in gaining government funding for a nationwide program of erosion surveys and soil and water conservation research.71 But public concern about 69 Gilbert, Mineral Nutrition of Plants and Animals, 98. 70 For the first half of the nineteenth century, Steven Stoll, Larding the Lean Earth: Soil and Society in Nineteenth-Century America (New York: Hill and Wang, 2003). For the late nineteenth and early twentieth centuries: Douglas Helms, \"Early Leaders of the Soil Survey,\" in Douglas Helms, Anne B. W. Effland, and Patricia J. Durana, eds., Profiles in the History of the U. S. Soil Survey (Ames, IA: Iowa State University Press, 2002), 19-64; and Stuart W. Schulman, \"The Business of Soil Fertility: A Convergence of Urban-Agrarian Concern in the Early Twentieth Century,\" Organization & Environment 12, no. 4 (December 1999): 401-24. 71 In The Soils and Agriculture of the Southern States (1921), Hugh Bennett drew attention to his distressing discovery of widespread sheet and gully erosion in the South. He advised that some soil types were unsuitable for agriculture or required conservation measures if used for agriculture. Bennett later coauthored, with W. R. Chapline, Soil Erosion: A National Menace (1928) for the USDA. 228 erosion surged with the arrival of the Dust Bowl, a period of catastrophic droughts and dust storms on the Great Plains during the 1930s. Drought, agricultural crisis, and dust storms on the Great Plains were not new, but their severity and the demographic situation in that period were. The population of the Great Plains stood at only 800,000 in 1880; it was seven times that, at 5.6 million, in 1930. Particularly during World War I and the 1920s, settlers moved into the area to cash in on the speculative boom in wheat. A drought that was unprecedented in the twentieth century began on the northern plains in the early 1930s and gradually spread south, covering the entire region through 1940. Strong winds, characteristic of the area, blasted across dry, cultivated fields, generating dust storms of unparalleled number, duration, and scale. Abundant rains returned in 1941, bringing an end to the dust storms. But for eight years crops failed, soils blew, and rural people, unable to meet financial obligations, suffered through tax delinquency, foreclosure, bankruptcy, and emigration. Many dust storms were local, but very intense weather systems also brought immense blasts that covered hundreds of miles and lasted hours. Clouds of windblown soil reached as far as New York and Washington, DC, and deposited into the Atlantic Ocean. What was most different about the Dust Bowl compared to past episodes, besides its magnitude, was the response of the government. In previous crises, when dust storms blew and farmers went bankrupt, the federal government stood aloof. The opposite was the case in the 1930s.72 The Dust Bowl provided an additional justification for agricultural and land use policies that were already being advanced as part of Franklin Delano Roosevelt's New Deal. 72 Geoff Cunfer, On the Great Plains: Agriculture and Environment (Texas A&M University Press, 2005), ch. 6 passim. 229 Roosevelt assumed the presidency in 1933 with a style of conservationist thinking that had developed for the most part in the decade before the New Deal. Although soil erosion was already a major concern for a cadre of soil surveyors and agricultural reformers in the early 1900s, conservationists in the Progressive era had essentially confined their efforts to waterways, forests, and recreational lands. Conservationist thought substantially expanded in the 1920s, generating a fresh set of concepts and policy proposals labeled the \"New Conservation\" by regional planning advocate Lewis Mumford in 1925. A loose network of engineers, intellectuals, politicians, and government workers voiced frustration with the nation's failure to meet the social and environmental needs of rural regions, which seemed to be falling increasingly behind in the industrializing economy. Sarah Phillips has identified two principal groups that contributed to the New Conservation in her recent study of the movement. One group, consisting of planners and politicians, believed that the modern industrial and financial system exploited the human and natural resources of the countryside and pushed impoverished rural people into overcro wded and unhealthy urban centers. These planners maintained that bringing electricity to the countryside, decentralizing industry, and classifying land according to its proper use could improve farmers ' living standards and protect their natural surroundings. In their view, regional land planning and publicly developed infrastructure could raise living standards, stem the rural exodus, halt soil degradation, and promote a prosperous \"permanent agriculture.\"73 A contemporaneous second group came to similar conclusions. A \"land utilization\" movement emerg ed from an influential group of farm experts and land economists seeking a way out of the agricultural troubles of the 1920s. Farmer advocacy groups such as the Farm Bureau argued that the dilemma could be resolved by equalizing the terms of trade between 73 Phillips, This Land, This Nation, ch. 1 passim. 230 farming and industry through what amounted to a protective tariff. The tariffs industry enjoyed, the case went, unfairly discriminated against American farmers , who had to pay higher prices for domestic manufactured goods yet compete in the international market to sell their crops. Through the use of tariffs, farm advocates aimed for a return to \"parity prices,\" or the terms of trade between agriculture and industry that had existed in the golden era of peacetime commodity farming between 1909 and 1914. But the \"land utilization\" movement, led by M. L. Wilson of the Montana State Agricultural College and L. C. Gray in the Department of Agriculture's Bureau of Agricultural Economics, offered a different analysis. They believed that the farm parity problem reflected social and environmental imbalances within farming, which could be remedied through proper land survey research and planning. They sought to classify farmland as good, poor, marg inal, and submarg inal, and to retire the latter category from production. Such planning aimed to reduce rural poverty, curtail chronic overproduction of farm crops, and protect land vulnerable to erosion.74 The land utilization movement was buttressed by soil scientists working in the Bureau of Soils, who had earlier begun considering susceptibility to erosion and preexisting erosion as elements in their classification of soil types.75 According to Phillips, both the regional planners and land utilization advocates who comprised the New Conservation movement connected social with environmental goals. In the view of these two groups, proper land use and fair distribution of natural resources would improve rural living standards and promote a healthy \"rural-urban balance.\" 74 Albert Z. Guttenberg, \"The Land Utilization Movement of the 1920s,\" Agricultural History 50, no. 3 (July 1976): 477-90. 75 Helms, \"Early Leaders of the Soil Survey,\" 29-34. 231 Many of the luminaries of the New Conservation rose to prominent positions under Roosevelt's presidency. Hugh Bennett, for instance, was appointed the first director of the Soil Erosion Service under the Department of the Interior in 1933, which was renamed the Soil Conservation Service (SCS) under the Department of Agriculture in 1935. Roosevelt also succeed ed in bringing Henry Wallace into his camp, and after victory he appointed Wallace as Secretary of Agriculture. Wallace, an agrarian intellectual and entrepreneur, was an influential agricultural editor who became convinced in the 1920s that only government intervention could solve the problem of farm overproduction. Rexford Tugwell, a Columbia University economist, was an articulate \"permanent agriculture\" advocate and one of the main architects of Roosevelt's farm strategy. In 1934, Tugwell was appointed Assistant Secretary of the Department of Agriculture, and then became the head of the Resettlement Administration (RA), a federal agency that sought to buy out, relocate, and retrain impoverished rural people who lived on land that had been deemed \"submarginal.\" Morris Cooke, one of the foremost contributors to the New Conservation movement, became the first director of the Rural Electrification Administration (REA). M. L. Wilson and L. C. Gray, the leading minds of the land utilization movement in the 1920s, also became influential figures in New Deal conservation apparatus. The New Deal's broad and oft-changing plans for agriculture were a combination of short-term political opportunism and long-term soil conservation ideas. The Agricultural Adjustment Administration (AAA), established in 1933, mark ed the first major government intervention into farm commo dity markets in American history. With the creation of the AAA, the Secretary of Agriculture was authorized to enter into contracts with farmers who, in return for a cash benefit payment, agreed to retire a stated percentage of his acreage from the production of particular commodities. The revenue for these payments was to be derived 232 from the levy of taxes on the processing of the commo dities involved. Conservation-minded proponents of the AAA hoped that it would provide farmers with the cash to replace the most destructive field crops with soil-restoring grasses and legumes. Other unemployment relief and public works measures were also geared to achieving a \"permanent agriculture\" in the longer term. Agencies such as the Civilian Conservation Corps (CCC), the Federal Emerg ency Relief Administration (FERA), and the Public Works Administration (PWA) were intended by the New Dealers to provide the labor and monetary resources to develop and electrify rural areas, reforest and restore marg inal lands, and halt soil erosion. In the state of Wisconsin, for instance, various work-relief agencies such as the CCC took up the mining and grinding of agricultural limestone in 1934. This made lime cheap for agricultural purposes and immediately increased its consumption five or six times, even though the Depression was still very much a reality for cash-strapped farmers .76 The proponents of agricultural lime saw it as a way to balance the \"calcium deficit\" in farm soils created by decades of export cropping and erosion. By the 1920s, lime and other mineral amendments such as phosphates were widely regarded as necessary on depleted or naturally infertile soils to produce the more nutritive but fertility-deman ding crops. Meas ures for farm relief, rural social equity, and soil conservation became even more tightly yoked to each other in the latter half of the 1930s. In 1936, the Supreme Court struck down the Agricultural Adjustment Act of 1933 as unconstitutional. This decision gave New Dealers such as Henry Wallace and Howard Tolley, who headed the AAA's Program Planning Division, the opportunity to push for new legislation that prioritized soil conservation practices over production control and farmer income relief. Their efforts 76 Emil Truog, \"Soil Acidity and Liming,\" in Soils and Men: Yearbook of Agriculture, 1938, 566-68. 233 succeed ed, resulting in the passage of the Soil Conservation and Domestic Allotment Act of 1936. This legislation attempted to correct some of the problems with the previous Act, most notably its failure to protect sharecroppers and tenant farmers. The 1936 Act was followed by a new Agricultural Adjustment Act in 1938, which balanced soil conservation goals with farm income assistance and put government involvemen t in agriculture on a lasting basis. More direct and coercive intervention into land use came with the inauguration of the soil conservation districts in 1937. Conservation planners had long understood that successful erosion control required not just individual farm er initiative, but cooperation on the community or watershed level. Under Hugh Bennett, the SCS encouraged the states to pass legislation permitting farmers to establish special soil conservation districts by refer endum. Although these districts were not units of local government, they had the authority to enforce farmer compliance with mutually agreed upon land use regulations. The SCS helped districts acquire conservation mach inery and seeds, and also assisted farmers in drawing up individual conservation plans, planting trees, filling gullies, laying fencing, and developing small dams.77 In addition, there were numero us financial incentives for farmers to abandon erosion-promoting practices. A farmer's ability to recei ve Agricultural Adjustment Administration payments, Agricultural Conservation Program (ACP) funds, and Farm Secu rity Administration (FSA) loans were all tied to compliance with soil conservation practices. The drought and dust storms that swept across the Great Plains were the most well known of these climatic events, and the centrality of soil conservation in American policy 77 Z. K. Hansen and G. D. Libecap, \"Small Farms, Externalities, and the Dust Bowl of the 1930s,\" Journal of Political Economy 112, no. 3 (2004): 665-94. 234 experimentation during the Depression was exceptional. But European settler societies elsewhere reckoned with environmental crises that they had apparently unwittingly unleashed. Across the British Empire, accumulating evidence of extensive land degradation prompted an \"erosion scare\" among government workers and specialists. In South Africa, a major step toward raising more widespread concern about erosion was taken by the South African Drought Investigation Commission of 1922-23. The commission's report drew attention to the pastoral farming areas of the semiarid mid-Cape, arguing that the veldt's destruction was due to exhaustion of the vegetation and erosion of the soil brought on by burning, overstocking, improper grazing methods, and intensively farmi ng for short-term gain. The Commissioners feared that the \"Great South African Desert\" was in the making. As William Beinart has argued, the growth of awareness about soil erosion on settler farms in South Africa facilitated heightened government involvemen t in land use during the 1920s and 1930s, and informed similar interventions into southern African peasant agriculture beginning in the 1930s.78 And in the mid-1930s, dust storms engulfed eastern Australia. By the 1940s, most Australian states had introduced soil conservation laws and established soil conservation institutes. In 1938, New Zealand experienced disastrous floods and soil erosion in Hawke's Bay, leading to the passage of Soil Conservation and Rivers Control Act in 1941.79 Scientists and advisers in the tropical colonies also contributed to the growing international discourse on soil erosion and conservation. Colonial foresters in Asia, Africa, and the Caribbean had long been concerned that deforestation would lead to droughts, soil 78 Beinart, \"Soil Erosion, Conservationism and Ideas about Development: A Southern African Exploration, 1900-1960,\" 52-83; William Beinart, The Rise of Conservation in South Africa: Settlers, Livestock, and the Environment, 1770-1950 (Oxford University Press, 2008). 79 Libby Robin and Tom Griffiths, \"Environmental History in Australasia,\" Environment and History 10 (2004): 446-447. 235 erosion, declining yields, food shortages, and, ultimately, social unrest. In West Africa in the 1920s and 1930s, British and French foresters and botanists expressed fears that excessive deforestation, bush burning, and cultivation were contributing to the \"desertification\" of the region. Surveyors in East Africa came to similar sobering conclusions regarding the environmental dangers of the intensification of agriculture that was taking place in response to population growth and mark et opportunities. By 1929, the director of agriculture in Tanganyika, E. H. Harrison, was describing soil conservation as a top priority, and in 1931 a Standing Committee on Soil Erosion was set up. From the mid-1930s onward, as Joseph Morgan Hodge has noted in his study of scientific expertise in the British colonies, \"the problem of soil erosion increas ingly caught the attention of officials and experts in London as the concerns of local authorities were lifted onto the imperial stage.\"80 Frank Stockdale, for example, spent most of his career in the colonial agricultural services battling deforestation and soil erosion in Ceylon. After his appointment as agricultural advisor for the Colonial Office (CO) in 1930, Stockdale remained a strong proponent of soil conservation. He used his many tours of the British Empire to gain a wider perspective on the soil erosion problem, and he also traveled to South Africa and the United States to learn about the pioneering soil conservation work being done there. Stockdale and other CO advisers were apparently awed by the capacity of Bennett's SCS for scientific planning and coordination, deployment of technical experts, and large-scale interventions into land use.81 To mobilize public support for conservation efforts, the \"erosion apostles\"\u2014as Randal Beeman and James Pritchard term them\u2014raised the specter of a \"soil crisis\" in 80 Hodge, Triumph of the Expert, 164. 81 Ibid., 165-66. 236 writings, films, and speeches.82 In the United States, a spate of books appeared in the 1930s: to take just a few, Henry Wallace's New Frontiers (1934), ecologist Paul Sears's ominously titled Deserts on the March (1935), economist Stuart Chase's Rich Land, Poor Land: A Study of Waste in the Natural Resources of America (1936), and Hugh Bennett's Soil Conservation (1939). One of the more controversial erosion jeremi ads was Edward Faulkner's Plowman's Folly (1943), which blamed overuse of the moldboard plow and the resort to chemical fertilizers for the erosion crisis in the United States. The government, for its part, sought to popularize soil conservation by supporting publications such as Russell Lord's To Hold This Soil (1938) and the lavishly illustrated Behold Our Land (1938). Perhaps the most penetrating analysis of the causes and consequences of American soil abuse came from the USDA itself, in the form of the 1938 Yearbook of Agriculture, Soils and Men. The erosion message was also delivered through the medium of film: Pare Lorentz's government-backed The River and The Plow That Broke the Plains both appeared in the late 1930s, followed shortly by Robert Flaherty's The Land (1941). Warnings about soil erosion extended beyond the United States as well. Graham V. Jacks and Robert O. Whyte of the Imperial Bureau of Soil Science wrote an influential world survey of soil erosion, entitled The Rape of the Earth (1939), which was republished in the United States as Vanishing Lands (1939). According to the erosion apostles, the commo n culprits in creating the soil crisis were bad farming and land use practices, misguided government policy and agricultural science, and a widespread exploitative attitude toward nature. In the American context, the Dust Bowl was interpreted as only the latest and most catastrophic episode in a long history of poor land stewardship. The individualism, entrepreneurship, and technological prowess 82 Beeman and Pritchard, A Green and Permanent Land, 11. 237 that had rapidly made America the wealthiest country in the world was also, paradoxically, responsible for the speedy exhaustion of its natural resources. Erosion apostles presented the story of the independent American pioneer, moving ever westward, in tragic rather than heroic tones. Paul Sears depicted the westward expansion of white settlement as a \"lustful march\" across the virgin continent, while agrarian luminary Louis Bromfield described it a \"plague of locusts.\"83 Geographer J. Russell Smith was similarly condemnatory: \"Indeed we Americans, though new upon our land, are destroying soil by field wash faster than any people that ever lived\u2014ancient or modern, savage, civilized, or barbarian.\"84 Bennett and Lowdermilk, exemplifying the nature-centered view of history typical of the erosion apostles, argued that the \"pioneering ax and plow rapidly upset the interplay of natural forces that had formed and preserved rich soils through ages of undisturbed development. The same tide that rolled the frontier forward from the Atlantic rolled back nature's stabilizing mantle of trees and grasses and bared virgin soil to wind and rain.\"85 According to Bennett, this callous attitude to the land had its root \"in the national illusion of abundance.\" The seemi ngly endless supply of resources allowed what he called \"a national philosophy of exploitation\" to gradually evolve. Erosion critics in other European settler societies assailed the \"pioneer mentality\" and the obsession with short-term economic gains at the expense of long-term land stewardship. The South African Drought Commissioners, for instance, critiqued those \"who start farming with the set purpose of wringing out the life blood of the farm in order to make 83 Cited in Beeman and Pritchard, A Green and Permanent Land, 16-17. 84 J. Russell Smith, Tree Crops: A Permanent Agriculture, rev. ed. (New York: The Devin-Adair Company, 1950), 4. 85 Hugh H. Bennett and W. C. Lowdermilk, \"General Aspects of the Soil-Erosion Problem,\" in Soils and Men: Yearbook of Agriculture, 1938 (Washington: GPO, 1938), 591. 238 a quick profit.\"86 Colonial administrative and technical officers similarly blamed the frantic push for increased commercialization and expansion of agricultural production for encouraging \"selfish individualism\" and overtaxing the soil.87 The assessment of \"primitive\" or \"peasant\" agricultures in regard to erosion was more varied, depending on the context and the observer. Whereas some \"primitive\" societies were assailed for practicing husbandry that was considered backward, inefficient, and destructive, other societies were esteemed, even romanticized, for maintaining productive systems of land husbandry for centuries without loss of fertility. Critiques of erosive land use practices from the 1920s onward were deeply informed by the science of ecology, which at that point was still a relatively young discipline. German zoologist Ernst Haeckel coined the term ecology in 1866 to describe the science dealing with the interrelations between life forms and between life forms and their environments (e.g., climate, topography, geological material). In the early decades of the twentieth century, ecology was dominated by the climax theory of vegetation, which was principally developed by two American plant ecologists, Henry Cowles and Frederic Clements. Clements suggested that the process of vegetation could be understood as a sequence of stages analogous to the development of an individual organism. Plant commu nities eventually developed from simple units to complex systems that reached a stable or \"climax\" state, which describes the form vegetation best suited to some idealized set of environmental conditions. Clements used the term complex organism to describe the character of climax vegetation. His method of dealing with ecological complexity was to define an ideal form of 86 Cited in Beinart, \"Soil Erosion, Conservationism, and Ideas about Development: A South African Exploration, 1900-1960,\" 59-60. 87 Hodge, Triumph of the Expert, 158. 239 vegetation\u2014the climax community\u2014and describe other forms of vegetation as deviations from that ideal. Although the climax school dominated ecological investigations during the first decades of the century, starting in the 1920s critics questioned and modified this explanatory structure. Namely, Henry Gleason became increasingly critical of the idea that assemblages of plants were the result of an orderly process of succession. Even though he largely agreed with Clements on what controls vegetation processes, Gleason argued that the characterization of plant commu nities as whole organism-like entities oversimplified the complex relationship between individual plants and the environment. Arguing from a slightly differen t angle, Arthur George Tansley of Oxford University disputed the notion of a \"monoclimax,\" suggesting that many different types of vegetation appeared more or less perman ent and deserving of the label \"climax community.\" These disputes over the usefulness of organism similes and the idea of climax were largely rooted in the limitations inherent in the study of whole, complex systems. 88 Furthermore, critics of Clements felt uncomfortable with the way he brought organicist notions into his discussion of plant associations. Though he employed the organism concept only metaphorically, it struck uncomfortably close to old arguments over vitalism and the idealist notion that some questions cannot not be answered by science. In 1935, Tansley proposed a way to get past some of these semantic and epistemological difficulties. He advanced the idea of an \"ecosystem\"\u2014a term coined by Roy Clapham in 1930 to describe the combined physical and biological components of an environment\u2014as a mean s to bridge the gulf between the holism of Clements and the reductionism of his 88 Christopher Eliot, \"Method and Metaphysics in Clements's and Gleason's Ecological Explanations,\" Studies in History and Philosophy of Biological and Biological Sciences 38 (2007): 85-109. 240 critics.89 The virtually identical concept of \"biocoenosis\" (or \"geobiocoenosis\") was already well established in 1877 by German zoologist Karl M\u00f6bius. His findings were consolidated by the Russian scientist V. V. Dokuchaev and his followers, who, not incidentally, inspired the \"ecological\" shift in American soil science in the interwar years.90 Indeed, the erosion apostles regarded ignorance of regional and local ecology as a major factor in the genesis of the soil crisis. In the United States, they drew attention to the transfer of European farming techniques that were maladapted to novel American crops and the continent's more extreme climatic conditions.91 The clean cultivation of fields every year, especially across sloping and hilly land, exhausted soil fertility and left it exposed to erosive forces. Europeans, historically, grew small grains\u2014wheat, barley, rye, and oats\u2014that covered the ground thickly and held the soil with their mat of roots. The novel American crops\u2014corn, cotton, and tobacco\u2014required extensive intertillage during their growth, leaving the soil between the widely spaced plants exposed. These cropping and cultivation practices exacerbated another factor of destruction new to European settlers in America, the torrential rainstorm. \"When the American heavens open and pour two inches of rain in an hour into a hilly cornfield,\" as J. Russell Smith put it, \"there may result many times as much erosion as results from two hundred inches of gentle British or German rain falling on the wheat and grass.\" To erosion critics, the failure of European settlers to match agricultural technique to environment became especially obvious in the semiarid Great Plains, where they stocked cattle heavily and busted open sod for grain crops during 89 Beeman and Pritchard, A Green and Permanent Land, 38-39. 90 Rapha\u00ebl J. Manlay, Christian Feller, and M. J. Swift, \"Historical Evolution of Soil Organic Matter Concepts and Their Relationships with the Fertility and Sustainability of Cropping Systems,\" Agriculture, Ecosystems and Environment 119 (2007): 224. 91 See, e.g., Part 2 in USDA, Climate and Man: Yearbook of Agriculture, 1941 (Washington: GPO, 1941.) 241 exceptionally moist years and then suffered badly with the arrival\u2014quite in keeping with the region's long-term climatic pattern\u2014of exceptionally dry ones.92 A greater sensitivity to ecology also contributed to a reassessment of tropical and subtropical environments and agricultural schemes in the British Empire. Helen Tilley has found that British colonial agricultural departments in the 1920s and 1930s showed a new awareness of the dangers of soil erosion and became increas ingly interested in incorporating and applying concepts from the emerging science of ecology.93 The accumulating body of research carried out by colonial technical personnel and research officers presented a picture that challenged the earlier cornucopian assumptions about tropical fertility and abundance, as well as the superiority of European farming practices in tropical environments. 94 This more chastened view of the colonies was matched by similar reassessments of the soils and agricultural potential of tropical and subtropical environments in Australia and the southern United States.95 The ecological critique of conventional agricultural methods also had a nutritional component. Scientists grounded in nutrition and agriculture\u2014anticipating the later formulation of the \"nutrient cycling\" concept in ecosystem science\u2014warned of the consequences of soil mineral depletion. In Minerals in Pastures, for example, Orr was deeply troubled by the gradual worsening of pastures due to the remo val of animals or animal products without any compensating return to the soil of the minerals that went into them. The one-way traffic of these products from the world's agricultural export regions to 92 C. Warren Thornthwaite, \"Climate and Settlement in the Great Plains,\" in Climate and Man: Yearbook of Agriculture, 1941, 177-87. 93 Helen Tilley, \"African Environments and Environmental Sciences: The African Research Survey, Ecological Paradigms and British Colonial Development, 1920-1940,\" in Social History and African Environments, eds. William Beinart and JoAnn McGregor (Oxford: James Currey, 2003), 109-30. 94 Hodge, Triumph of the Expert, 151-52. 95 C. S. Christian and N. H. Shaw, \"Protein Status of Australian Tropical and Sub-Tropical Pastures,\" in Plant and Animal Nutrition in Relation to Soil and Climatic Factors, 225-40. 242 the metropoles meant that the mineral content of pastures was bound to decrease over time. Attempts by breeders to maintain or improve the animal stock would create an increasingly large gap between the requirements of the animals for nutrients and the supply of these in the plants and soils. Orr was implying that the depletion of pastures by the export of agricultural commodities was adding to the already vast areas of the world naturally incapable of supporting the more nutritionally deman ding animal breeds. As with other arguments in Minerals in Pastures, Orr was not speculating far in advance of the evidence in this regard. Other investigators working in districts that had long exported animal products came to similar conclusions. In 1924, for instance, Hugh Munro reported that it had been becoming increasingly difficult over the previous two decades to rear lambs and other animals for export in the Falkland Islands.96 Robert McCarrison, among other researchers, ascribed the generally high mortality, sterility, and low milk yield of cattle in certain areas of India to the lack of phosphorus and other minerals in the soil. He believed that this phosphorus deficiency was caused, at least in part, by the long-continued export of bones from India to fertilize British farm fields. Similar observations of worsening pasture fertility surfaced in journals from all over the world. Orr laid out the implications of this process in stark terms: \"Every cargo of beef or milk products, every ship-load of bones, leaves the exporting country so much the poorer. In many of the grazing areas of the world this depletion has become a serious economic problem.\"97 By drawing together these scattered case studies, Orr was giving shape to a kind of global \"ecological imperialism,\" to repurpose a phrase coined more recently by 96 Hugh Munro, Report of an Investigation into the Conditions and Practice of Sheep Farming in the Falkland Islands (London: Waterlow, 1924). 97 Orr, Minerals in Pastures, 141. 243 environmental historian Alfred Crosby.98 Orr was arguing, in essence, that the broken mineral cycle, which started in the British Empire's pastures and ending in the metropole's sewage water, threatened wealthy and poor agricultural economies alike. Weston Price came to conclusions similar to those of Orr at roughly the same time. Price believed that \"modern civilization\" had not only heedlessly manipulated its food supply with novel processing methods, but had also developed a dysfunctional relationship with the soil that was leading to a decline in the nutritional quality of food. As early as 1930, he warned that \"probably few of the problems concerning the coming generations are more exacting and difficult to meet than the problem of reduction in the minerals and other chemical content of the soil.\"99 In the late 1930s and early 1940s, Price became increasingly preoccupied with the nutritional ramifications of mineral depletion and soil erosion. Reinforcing the emerg ing view that farming had to mimic natural ecosystems, Price commented in a 1938 article that In Nature's management of the animal and plant life, each animal and plant borrows enough of the various minerals to build its body, and when it is through with the loan returns these chemicals to the soil. This has been the policy of the surviving primitive races. There is accordingly no depletion associated with this usage. Our modern civilization returns exceedingly little of what it borrows. Vast fleets are busy carry ing the limited minerals of far flung districts to distant mark ets.100 Price devoted an entire chapter to the nutritional consequences of the broken mineral cycle and soil erosion, entitled \"Soil Depletion and Plant and Animal Deterioration,\" in the first 98 Alfred Crosby, Ecological Imperialism: The Biological Expansion of Europe, 900-1900 (Cambridge University Press, 2004). These concerns about the failure to follow the \" Rule of Return\" were not new. See Barton Blum, \"Composting and the Roots of Sustainable Agriculture,\" Agricultural History 66 (Spring 1992): 171-88; and Erland Marald, \"Everything Circulates: Agricultural Chemistry and Recycling Theories in the Second Half of the Nineteenth Century,\" Environment and History 8 (2002): 65-84. 99 Weston Price, \"Some Means of Improving Human Life by Increasing the Vitamin Content of Milk and Its Products\" (Read before International Association of Milk Dealers at Cleveland, Ohio, October 22, 1930), The Association Bulletin, no. 10 (29 January 1931): 14. 100 Price, \"Dentistry and Race Destiny,\" Dental Items of Interest (October 1938): 4-5. 244 edition of Nutrition and Physical Degeneration (1939). In the chapter, Price lambasted soil erosion as a terrific waste of nutrients. Echoing other permanent agriculture proponents, he advocated the conservation of soil organic matter and vegetative cover, practices he commonly encountered among the \"isolated primitives.\" He also observed that the parsimonious farming methods of the \"primitive races\" ensured the maintenance of relatively closed mineral cycles in the soil, by returning to them all vegetable and animal wastes, including even human excreta. \"In my studies of primitive races,\" remark ed Price in a radio interview shortly after the publication of his book, \"I have found them exceedingly thrifty with regard to the conservation of the precious life giving power of the lands and sea.\"101 Implicit in this praise, as with so many other aspects of \"primitive living\" that Price examined, was an attack on the practices of \"modern civilization.\" Looking over the \"departed civilizations of historic times\" to see the wreckage and devastation caused by soil destruction, Price warned that \"the rise and fall in succession of such cultures as those of Greece, Rome, North Africa, Spain, and many districts of Europe, have followed the pattern which we are carving so rapidly with the rise and fall of the modernized culture in the United States.\"102 In the second edition of Nutrition and Physical Degeneration (1945), Price added a supplemen t that included yet another chapter on the soil-food-health problem contributed by William Albrecht entitled \"Food is Fabricated Soil Fertility.\" Albrecht was also becoming increas ingly vocal with respect to the nutritional consequences of mineral depletion in the 1940s and 1950s. Summing up a troubling trend in the USDA crop statistics, he pointed out in a 1948 paper that 101 Price, \"Travelogues in Health,\" WTAM Radio Program (March 31, 1940), 8, File W225, Price-Pottenger Nutrition Foundation, La Mesa, CA. 102 Price, \"Travelogues in Health,\" 9. 245 While the bushels per acre of both wheat and corn have been going upward, the concentration of the protein within each of these grains has been going downward. Corn, which had a protein concentration ten years ago of nearly 9.5 percent, has an average figure of 8.5 percent today. While our crops have been yielding bushels per acre bountifully, those bushels have consisted mainly of the photosynthetic product, starch.103 In other words, Albrecht saw more than a metaphorical link between eroding soils and eroding bodies: impressive increases in crop yield were covering up an insidious process of declining nutritional quality. As the previous chapters demonstrated, nutrition researchers between the 1910s and 1930s became concerned about the \"starchification,\" \"sugarization,\" and \"demineralization\" of the diet that had taken place in the industrial West. Nutrition-minded soil scientists and agronomists such as Orr and Albrecht were also raising the alarm on the starchification, sugarization, and demineralization of entire landscapes due to soil depletion and erosion. They were, in effect, bringing the \"newer knowledge of nutrition\" to bear on the ecologically informed critiques of agriculture that arose in the 1920s and 1930s. But the nutritional ecology perspective also reinforced a note of optimism. Because the soil crisis was a product of human behavior, the erosion apostles argued, it was also avoidable. They typically expressed confidence that the prudent application of science, education, and planning could help to restore and maintain soil fertility at a high level of productivity. As part of this renewal process, nutritional ecology merg ed with a wider effort to apply ecological methods to soil conservation measures. Within the field of ecology, some scientists were uncomfortable with the way the climax concept seemed to imply that a primordial state of nature represented an ideal standard against which human activity should be meas ured. Tansley suggested the notion of 103 William Albrecht, The Land 7, no. 2 (Summer 1948): 195. 246 an \"anthropogenic climax,\" or a landscape shaped by humans yet as balanced and stable as natural grassland and forest climax commu nities. Similarly, Paul Sears advanced the idea of \"directed\" ecology, in which knowledge of \"climax commu nities\" and \"balance\" in nature could be applied to farming systems to simulate nature.104 In 1938, Herbert Hanson, president of the Ecological Society of America, called for the \"invasion\" of ecology into agriculture and resource conservation. To Hanson, this meant not only teaching people to \"use all available scientific information in order to adapt their modes of living to the environment,\" but also maintaining \"natural areas as checks, or standards, by which the values and effects of tillage, irrigation, drainage, grazing, lumbering, and other uses may be meas ured.\"105 For scientists and experts imbued with an awareness of ecology, it taught that society\u2014and particularly its foundation, agriculture--had to become \"symbiotic\" with other biological commu nities. Moving toward this goal meant, at least on a technical level, that farming methods had to adjust to the particular limitations of local environments and mirror nature's process of building and maintaining soil. The new agroecological perspective highlighted the need for detailed botanical, topographical, geological, and soil surveys to determine what types of land use would be appropriate to a particular region. This outlook also stressed that the realization of a \"permanent agriculture\" on any significant scale hinged on the widespread adoption of well-known methods for maintaining and improving soil fertility. One of the most fervently promoted measures for soil and water conservation was the inclusion of grasses, legumes, and other thick-growing vegetation into farming systems. These dense greenswards, once established, would protect the soil surface by breaking the 104 Beeman and Pritchard, A Green and Permanent Land, 39-45. 105 Herbert Hanson, \"Ecology in Agriculture,\" Ecology 20, no. 2 (April 1939): 111-17. 247 battering force of falling rain, thereby preventing run-off water from becoming muddy or silty. Thick plant roots would also bind soil particles together and build organic matter, which in turn would increase yields of future crops, improve the rate of water penetration, and slow run-off. Soil conservationists applied a similar logic, where there was sufficient precipitation, in their promotion of reforestation on very hilly or otherwise agriculturally submarg inal lands. Grasses and thick-growing vegetation, and to some extent trees, formed the basis of a variety of soil conservation methods. Contour strip cropping, to take one of the most significant examples, involves the production of farm crops in long, relatively narrow strips of variable width, on which dense erosion-control crops alternated with clean-tilled or erosion-permitting crops. In contrast to what had been the common practice of cropping in square-shaped fields laid out with little regard for the slope of the land, strip cropping involved the use of parallel strips crosswise of the slope. The strips of dense vegetation would slow run-off water and catch soil particles, preventing erosive forces from gaining momentum. In the United States, Rexford Tugwell and J. Russell Smith called for the wider adoption of \"two-story agriculture\" systems, in which interspersed tree and field crops provided diversified farm production while conserving soil, particularly on sloping lands.106 Some colonial foresters and agronomists pushed for similar forestation and diversification meas ures as a replacement for erosive cash crop monocultures. Even on level lands not prone to gross forms of erosion, soil conservationists advocated the use of grasses, legumes, and other sod-forming plants. When chopped and 106 Rexford Tugwell, \"Farm Relief and a Permanent Agriculture,\" Annals of the American Academy of Political and Social Science 142 (March 1929): 274; and Smith, Tree Crops: A Permanent Agriculture. Smith called for an all-out program of research into the breeding of highly productive nut- and fruit-bearing trees, especially for animal feed, which he felt had been a neglected area of sustained scientific inquiry in preceding decades. 248 turned into the soil to decompose, these crops become a fertility building \"green manure\" that would improve soil structure and replace some of the nutrients lost by cultivation. Agronomists also promoted the use of grasses and legumes in crop rotation systems as an alternative to year-on-year monocropping or bare-fallowing practices. And particularly in semiarid areas, where the denudation of pasturelands had become a major concern, the growing appreciation of the conservation potential of dense-rooted plants underpinned early efforts to develop rotational grazing systems, reduce stocking rates, and replace lost minerals.107 Although the conservation methods that were considered appropriate varied from region to region, and even from farm to farm, their general aim was to improve the structure and productivity of the soil by increasing its fertility and organic matter content. Sears summed up the overall \"ecological\" vision of permanent agriculture in a 1947 speech, declaring: \"Good land practices follow the model of nature. As much of the ground is kept in continuous cover as possible. Minerals which leave the farm in the form of finished products are returned to the soil in equivalent amounts. A balance between plants and animals is maintained, so that organic material is kept on the farm.\"108 In sum, then, improved soil fertility was seen as the commo n currency underpinning environmental, economic, and nutritional renewal. Reducing the acreage devoted to erosion-promoting field crops and increasing that devoted to soil-conserving grasses, legumes, and trees would also reduce the chronic glut of grain that had been saddling commercial farmers for years. This sort of \"permanent agriculture\" would ideally allow farmers to stay on land prone to erosion under tillage, and, by increasing land productivity, permit the retirement of 107 These soil conservation techniques are described throughout Soils and Men: Yearbook of Agriculture, 1938. 108 Paul Sears, paper read before a general session of the Ohio State Medical Association, cited in Soil, Food and Health, 42. 249 the most erosion-prone areas from cultivation. Moreover, reflecting the strongly biogeographical vision of soil science in that era, agronomists widely recognized that mineral amendments were necessary in the vast regions that were degraded or did not contain enough nutrients in their native condition for building soil fertility and growing many of the \"protective foods\"\u2014meat, dairy, eggs, and garden vegetables. Though these foods were the most demanding on the soil, they also held out the most promise, if produced properly, for soil conservation and restoration. In a broad sense, as the work of scientists like Albrecht and Bear implied, the ecology of the mineral- and humus-rich \"chernozem\" soils had to be built up and expanded by humans in order to halt soil erosion while feeding modern civilization at a high nutritional level with plenty of the \"protective foods.\"109 And in fact, nutrition science directly influenced government soil conservation meas ures in the United States. During the 1930s, Hazel Stiebeling of the USDA Bureau of Home Economics translated the nutrient requirements that had been developed by nutrition scientists into concrete dietary recommendations. Stiebeling's work provided agricultural economists with a means of bridging the gap of uncertainty between food production and consumption. In 1935, the Program Planning Division of the AAA endeavored to interpret the dietary requirements of Stiebeling's study in terms of crop acreages and number of livestock as national goals and in terms of regional adjustment in agricultural production. There was a large-scale investigation involving the land-grant universities, the State extension services and experiment stations, and some federal agencies. This study indicated the need for significant shifts in agricultural production toward the \"protective foods,\" and in particular, major increases in the production of leafy green and yellow vegetables and dairy 109 Albrecht laid out such a view numerous times. See, e.g., William Albrecht, \"Reconstructing the Soils of the World to Meet Human Needs\" (1951), File A281, Price-Pottenger Nutrition Foundation, La Mesa, CA. 250 products. According to M. L. Wilson, \"the shifts in production needed from a dietary standpoint worked in very well with the conservation objective, and further emphasized the advantages of shifting away from the surplus crops toward a better balanced agriculture.\" This nutritional line of thought informed the planning and development of the many farm programs administered by the Department of Agriculture into the World War II years.110 Therefore, for some scientists and leading government officials, the increas ed production of the \"protective foods\"\u2014provided there was a ready market for their consumption\u2014would also be protective of the soil and farmers' livelihoods. But as I will describe in chapter five, interest among nutritionists, agronomists, and government officials in this \"nutritional conservation\" vision faded rapidly in the years following World War II. At the same time, however, a \"natural food movement\" was emerging whose supporters claimed that impoverished soil was a major contributor to widespread malnutrition. These natural food advocates also diverged from postwar-era health authorities in their opposition to the \"enrichment\" of white flour with some of the vitamins and minerals that had been removed from wheat in the course of milling. Yet, as the next chapter will show, the antipathy of the natural food movement to enrichment was shared by many nutritionists when the measure was first proposed in the late 1930s and early 1940s. 110 M. L. Wilson, \"Nutritional Science and Agricultural Policy,\" Journal of Farm Economics 24, no. 1 (February 1942): 188-205. 251 CHAPTER FOUR BETTER BREAD THROUGH CHEMISTRY? CONSERVATIVE NUTRITION AND THE ENRICHMENT CONTROVERSY In 1942, an article by University of Wisconsin biochemist Conrad Elvehjem entitled \"Natural Foods in the American Dietary\" appeared in the Journal of the American Dietetics Association. The ultimate goal of nutrition education, Elvehjem contended in the piece, was to convince the public to increas e its consumption of unprocessed \"protective foods\" and decrease its consumption of \"energy foods\" lacking in vitamins and minerals, such as white flour and refined sugar. But the timing of the article seems unusual, given the historical context in which it was published. Why would Elvehjem feel the need to reiterate an argument that by that time had gained widespread acceptance among not only nutritionists and dieticians, but also physicians and public health officials? A defense of natural foods was especially crucial at that point, Elvehjem believed, because so much professional and popular attention had been suddenly fixated upon the \"enrichment\" of white flour with three of the vitamins and one of the minerals that were mostly remo ved in the milling process. In the late 1930s, a group of nutrition research ers became alarmed by the evidence of widespread deficiencies of thiamin, riboflavin, niacin, and iron in the American population. They realized that, before the near-total replacement of stone grinding mills by steel roller mills in the late nineteenth century, unrefined or nearly unrefined flour and bread had been a major source of these nutrients, particularly in lower-income groups that depended largely on cheap cereals and tubers for their calories. These researchers consequently began campaigning for laws mandating the addition of thiamin, riboflavin, niacin, and iron to white flour up to the levels found in whole-wheat flour. 252 Although Elvehjem approved of the \"enrichment movement,\" as this campaign came to be called, he believed that the addition of vitamins and minerals to refined foods should only be regarded as a stopgap public health measure. Elvehjem reminded his readers that the use of natural foods, unlike enrichment, \"will tend to prevent a lack of unknown factors as well as correct a deficiency in the factor originally recognized.\" Elvehjem also expressed the hope that the nutritional shortcomings of white flour highlighted in the run-up to the enrichment movement had provided a cautionary lesson. \"If we had followed a little more carefully the changes in our dietary intake as a result of using some of our processed foods,\" he wrote, \"we would not have to cope with so many deficiencies at one time.\" To further emphasize the potential pitfalls of enrichment, Elvehjem pointed out that researchers were just then discovering that the refining of fats and oils, as was common in the food industry, destroyed choline, and that ordinary canning methods destroyed folic acid\u2014two B-complex vitamins about which very little, besides their essentiality for experimental animals, was known.1 Elvehjem's conflicted attitude was not uncommon in the nutrition science community when the concept of flour enrichment was initially proposed in the late 1930s and mandatory enrichment laws were passed in the 1940s. In fact, a vocal contingent of nutritionists at the time opposed enrichment on the grounds that too little was known about human nutritional requirements and the composition of foods, and instead favored efforts to increas e the consumption of wholegrain foods or incorporate more nutrient-dense natural ingredients into commercial bread formulas. Even many of the researchers who backed enrichment, like Elvehjem, were aware of the dangers of taking a nutrient-centered approach 1 Conrad Elvehjem, \"Natural Foods in the American Dietary,\" Journal of the American Dietetics Association 18, no. 5 (May 1942): 279-84. 253 to food. In the early stages of the debate at least, supporters of enrichment considered it only an emerg ency meas ure to combat the most prevalent nutritional deficiencies while the slower process of educating the public to increase its consumption of wholegrain and other natural foods was being accomplished. The failure of the enrichment movement in Britain well into the 1950s illustrates the ambivalent response of the nutrition science community toward the addition of purified vitamins and minerals to refined foods. Shortly after the outbreak of World War II, the newly reestablished Ministry of Food ordered the fortification of white flour with thiamin. But many British nutrition experts strongly criticized this meas ure, and put pressure on the government to suspend the milling of white flour and mandate the exclusive production of \"National wheatmeal \"\u2014a type of less-refined flour that contained most of the vitamin-rich germ of the wheat grain but not its branny outer layer. Since Britain had to import most of its wheat, the government was eventually pushed to prohibit the production of white flour and bread in 1942 on account of wartime shortages in shipping space. This shift in policy was not the result of shipping considerations only, but saving of shipping space tilted the balance in favor of National wheatmeal . In fact, the government continued to subsidize the production of National wheatmeal bread after the end of rationing in 1953, when white flour again became available to the British population. Yet interestingly, present-day critics of \"nutritionism\" have largely overlooked the highly mixed response of the nutrition science community to the use of vitamins and minerals in refined flour and cereals. Drawing on recent research implicating the refining of grains in a variety of chronic diseases, they have held up the story of enrichment as a 254 cautionary tale about the limits of \"reductionist science\" when applied to the food supply.2 A close examination of the early years of the enrichment movement, however, underscores the importance of separating the reductionist approach to food from the manner in which it has actually been applied historically. The Emergence of the Enrichment Movement in the United States The sudden enthusiasm for flour enrichment in the late 1930s and early 1940s arose from a conjuncture of several developments. Most importantly, there was mounting evidence indicating that the consumption of white flour was not as innocuous as it had been previously portrayed. It was well known by the 1920s that the milling of wheat down to about 70 percent extraction, as was most commo n after the introduction of steel roller mills in the 1870s and 1880s, resulted in substantial reductions in vitamins, minerals, and protein quality. (The \"extraction rate\" refers to the proportion of milled flour obtained from a given quantity of wheat grains. Generally speaking, the lower the extraction rate, the whiter and more refined the flour.) This loss of nutrients deeply concerned some nutritionists and nutrition-minded physicians. During the 1920s, for example, Robert McCarrison and Henry Sherman demonstrated in animal-feeding experiments that whole-wheat flour was essential for good health whenever wheat products constituted a large portion of the diet. Only when protective foods, and milk in particular, made up a substantial proportion of the experimental diets did the growth of rats fed white flour match that of rats fed whole-wheat flour. These sorts of findings led many nutritionists and dieticians to promote the consumption of wholegrain foods throughout the interwar period. Sherman, for instance, suggested in his 2 Pollan, In Defense of Food, 109-10; Dixon, \"From the Imperial to the Empty Calorie,\" 321, 325. 255 book Food and Health (1934), \"Of whatever breadstuffs and other cereal and grain products are eaten, let at least half be in the 'whole grain,' or 'dark,' or 'unskimmed' forms.\"3 Confusing the picture, however, was the fact that some prominent nutritionists and physicians continued to assure the public of the wholesomeness of white bread well into the 1930s. Elmer McCollum believed that it was acceptable for people to eat white bread so long as they consumed sufficient quantities of dairy products and vegetables. The leaders of the medical establishment also defended white flour throughout most of the interwar period. Since the one disease associated with the consumption of white flour, beriberi, was quite rare in the United States, most physicians saw little reason to recommend wholegrain foods. They also associated the advocacy of whole-wheat flour with a long line of food faddists and advocates of unorthodox health doctrines who had blamed the consumption of white flour on a wide variety of diseases. Supported by these endorsements from nutrition and medical authorities, millers, bakers, and wheat farmers advertised the wholesomeness of white flour and bread extensively in the medical and scientific press.4 But the case for white flour and bread became more or less untenable toward the end of the 1930s. In a 1938 paper, Norman Jolliffe, a physician at Bellevue Hospital in New York City, calculated the effect that the roller milling of flour and the increasing consumption of refined sugar since the nineteenth century had on the vitamin B1 (also called thiamine, thiamin, or aneurin) content of the American diet. Jolliffe pointed out that these two sources of calories\u2014white flour and refined sugar\u2014made up slightly more than half the calories consumed by the American people. This total source of calories provided no more than 50 International Units of vitamin B1, well below the established minimum requirement. 3 R. McCarrison, \"White and Brown Bread,\" British Medical Journal 2, no. 3593 (16 November 1929): 913-14; Henry Sherman, Food and Health (New York: Macmillan, 1934), 163-64. 4 McCance and Widdowson, Breads White and Brown, 77-84; \"Interpreting the 'Newer Knowledge of Nutrition',\" 323-33. 256 Jolliffe warned that \"a 55 per cent fraction of the calories in the American diet of 1840 containing a minimum of 600 I.U. of vitamin B1 has been replaced in the contemporaneous American diet by a like fraction containing only about 50 I.U.\" He calculated that it would require the daily consumption of 625 grams of fruit, 600 grams of potatoes, 880 grams of other vegetables, and over a liter of milk to make up the difference\u2014a combination impossible because of its bulk.5 Jolliffe was not alone in raising concern about the health consequences of the American penchant for white flour and sugar. Robert Williams and Tom Spies conveyed a similar message in a book that played a large role in the emergence of the enrichment movement, Vitamin B1 (Thiamin) and Its Use in Medicine (1938). Williams, though neither trained nor employed as a nutrition researcher, was the first person to successfully synthesize thiamin in 1936 and patent the process. Spies, a physician who spent his career in the South, had developed highly successful clinical methods for treating pellagra victims with special diets and vitamin-rich food supplements in the early 1930s. Later in that decade, he pioneered the therapeutic use of niacin and other B-complex vitamins for pellagra and associated deficiencies.6 In Vitamin B1 (Thiamin) and Its Use in Medicine, Williams and Spies warned that lesser deficiencies of thiamin were much more common in the United States than was commonly believed. Physicians rarely recognized a subacute deficiency of the vitamin, they maintained, because the symptoms were usually general and vague, and patients often suffered from shortages of more than one of the B-complex vitamins. To substantiate their argument, Williams and Spies pointed out that the average American diet did not contain much more thiamin than Asian diets that induced beriberi. They explained 5 Norman Jolliffe, \"A Clinical Evaluation of the Adequacy of Vitamin B1 in the American Diet,\" International Clinician 4 (1938): 46-66. 6 Thomas H. Jukes, Douglas Spies,\" Journal of Nutrition 102 (1972): 1395-1400. 257 that the relative absence of beriberi in the United States compared to Asia was probably due not to an abundance of thiamin in the diet, but to the greater proportion of fat that Americans consumed. In reaching this conclusion, Williams and Spies drew on recent research that had revealed the function of thiamin in the body. In 1937, organic chemist Karl Lohmann reported that thiamin was a coenzyme involved in the breakdown of pyruvic acid, one of the intermed iate products of carbohydrate metabolism. Around the same time, Oxford biochemist Rudolph Peters observed that pyruvic acid accumulated in the tissues of experimental animals deprived of thiamin, and most adversely affected the central nervous system. These findings on the physiological effect of thiamin deficiency explained to Williams and Spies why peripheral neuritis was the most recognizable symptom of beriberi in humans.7 A year after the publication of Williams and Spies's book, Hazel Stiebeling and Ester Phipard of the USDA Bureau of Home Economics published their landmark study of four thousand urban families that confirmed the growing evidence of widespread nutritional deficiencies, and of the marginal nature of the American diet as a whole. Outstanding among the deficiencies identified in the report were those of calcium and thiamin. Iron, vitamins C and D, and other fractions of the B complex were also questionable and in none of the nutritional constituents was there apparently any generous marg in of safety.8 This and other investigations led by Stiebeling on American dietary patterns revealed nutritional inadequacies at all socioeconomic levels, but they were particularly prevalent in low-income groups. Because white flour was a cheap source of calories and protein relative to other 7 Robert R. Williams and Tom D. Spies, Vitamin B1 (Thiamin) and Its Use in Medicine (New York: Macmillan, 1938). The relationship between carbohydrate intake and thiamin requirement had been indicated earlier in such works as George Raymond Cowgill, The Vitamin B Requirement of Man (New Haven, CT: Yale University Press, 1934). 8 USDA, Diets of Families of Employed Wage Earners and Clerical Workers in Cities, by Hazel K. Stiebeling and Ester F. Phipard (Washington, DC: USDA, 1939). 258 kinds of foods, its proportion in the diet tended to increase as family income decreased. Stiebeling maintained that, while poor food habits played a part, an alarmingly large percen tage of the American population could simply not afford a sufficient amount of the protective foods that would make it possible to build an adequate diet around white bread and sugar.9 In the same year that the Stiebeling-Phipard study was published, three doctors at the prestigious Mayo Clinic, Raymond Williams, Harold Mason, and Benjamin Smith, began thiamin deprivation studies on small groups of test subjects. In their first study, the doctors put four young women on a diet deficient in thiamin but adequate vitamins A and C, riboflavin, and niacin for 21 weeks. They found that the women became sluggish, moody, fearful, and mentally fatigued.10 But the delayed development of symptoms and the failure of the subjects to show the typical signs of acute thiamin deficiency\u2014beriberi\u2014prompted a repetition of the study with other subjects. They repeated the experiment with six female patients at the Rochester State Hospital, aged 21 to 46, who \"were active physically and were engaged in hospital housework.\" All the women were chosen for \"absence of physical defect s, absence of any history of abnormal nutrition and quiescence of associated mental illness.\" On a basal diet low in thiamin, there was no clear evidence of nutritional deficiency, but it provided less than a sufficient amount of the vitamin for what the investigators termed \"the best nutritional state of the patient.\" The women developed a host of symptoms, including depressed mental states, generalized weakness, dizziness, backaches, soreness of muscles, heart palpitation, insomnia, nausea, vomiting, loss of weight, and greatly decreas ed capacity for physical activity. The early stages of the disease 9 Hazel Stiebeling, \"Food Habits, Old and New,\" in Food and Life: Yearbook of Agriculture, 1939, 125-130. 10 R. D. Williams, H. L. Mason, and B. F. Smith, \"Induced Vitamin B1 Deficiency in Human Subjects,\" Proceedings of Staff Meetings, Mayo Clinic 14 (13 December 1939): 787-93. 259 induced by restricting the intake of thiamin closely resembled what the doctors called \"neurasthenia,\" a term for chronic nervous fatigue and exhaustion, which they distinguished from hysteria and anxiety neuroses. The later stage of thiamin deficiency simulated \"anorexia nervosa.\" Moreover, the doctors observed that the symptoms induced by restricting thiamin intake differed from classic beriberi, which led them to hypothesize that a \"deficiency of factors of the vitamin B complex other than thiamine may be more important in the production of such features than thiamine itself.\"11 Just as the Mayo Clinic trials were being undertaken, George Cowgill of Yale University submitted a report for the AMA Council on Foods and Nutrition on the need for restoring vitamin B1 to the American diet. Citing the Jolliffe and Stiebeling-Phipard studies, Cowgill emphasized the disturbing fact that the white flour produced by modern roller-milling processes contained only one-eleventh of the thiamin content of the older stone-ground flour.12 Cowgill's argument in favor of adding thiamin to flour relied only on estimates of thiamin consumption rather than on an assessment of the actual disease burden on the population due to a deficiency of the vitamin\u2014that is, Cowgill's evidence, like that of all the other early studies purporting the existence of widespread subacute or latent thiamin deficiency, was indirect rather than direct. Cowgill's report nevertheless provided the buttressing that the AMA's Council on Foods and Nutrition needed for the position it had taken several years earlier on the restorative addition of nutrients to flour. In 1936, the AMA's Committee on Foods (later 11 R. D. Williams, Harold L. Mason, Russell M. Wilder, and Benjamin F. Smith, \"Observations on Induced Thiamine (Vitamin B1) Deficiency in Man,\" Archives of Internal Medicine 66, no. 4 (October 1940): 785-89. They extended and confirmed their findings in Williams, Mason, and Wilder, \"Evaluation of Nutritive Contribution of Enriched White Flour,\" Journal of the American Medical Association 121, no. 12 (20 March 1943): 943-45. 12 George R. Cowgill, \"The Need for the Addition of Vitamin B1 to Staple American Foods,\" Journal of the American Medical Association 113 (9 December 1939): 2146-51. 260 named the Council on Foods and Nutrition) held a meet ing to discuss \"fortified foods.\" The committee had previously approved the addition of vitamin D to milk and of iodine to table salt, but it had not developed a formal policy on food fortification. Although no exact definition was attempted at the 1936 meeting, the committee came to a general understanding of a fortified food as \"one in which the percen tage of mineral elements or the unitage of vitamins has been made significantly beyond that of the same food as it exists in nature.\" The committee announced its policy as follows: If in exceptional cases a general need for vitamin (or inorganic salt) intake above that afforded by the usual mixed diet of commo n foods is indicated, the Council shall require (a) acceptable and convincing evidence that there is a need for enhanced amounts of vitamins (or inorganic salts) in the general food supply, and (b) that the food vehicles proposed for the distribution of such vitamins (or inorganic salts) are suitable and appropriate.13 Then, in December 1938, a joint committee of the Council on Foods and Nutrition and the Council on Pharmacy and Chemistry favorably considered the use of vitamins in certain cheap staple foods, including white flour. On the basis of the report received from the joint committee of the two councils, the Council on Foods and Nutrition proceeded to take action. On March 18, 1939 it accepted, with minor reservations, a resolution that the joint committee had submitted. The resolution encouraged the restorative addition of nutrients to staple foods, on the condition that \"a wider distribution is considered by the council to be in the interest of the public health.\" In keeping with the AMA's longtime aversion to food faddism and quackery, the council emphasized its opposition to \"the indiscriminate fortification of general purpose foods with vitamins or minerals or other dietary essentials.\" Crucially, the council drew a distinction between \"fortification\" and \"restoration\" of nutrients. Fortification was understood as \"the 13 \"Annual Meeting of the Committee on Foods,\" Journal of the American Medical Association 107, no. 1 (4 July 1936): 39. 261 addition to a food of such an amount of a vitamin or other dietary essential as to make the total content larger that that contained in any natural (unprocessed) food of its class.\" The addition of vitamin D to milk and iodine to table salt were early examples of fortification meas ures approved by the AMA. Restoration, on the other hand, was defined as \"the addition of vitamins and minerals or other dietary essentials, in such amounts as will raise the content of vitamin or mineral or other dietary essential of general purpose foods to recognized high natural levels.\" At this meeting, the council specifically encouraged the restorative additions of thiamin, riboflavin, and iron to refined cereal products such as white flour and bread.14 It was known by that time that the thiamin content of whole-wheat flour was about four, the riboflavin about two to three, the niacin about five, and the iron about five times that of white flour. The AMA's statement of policy soon stimulated several millers to experimental production of flours with restorative additions of thiamin. In addition, bread brought up to whole-wheat levels of thiamin and riboflavin by the use of synthetic thiamin and defatted milk solids began appearing on the market as a specialty product. The AMA policy on restoration also served as a guide to subsequent action by the Food and Nutrition Board of the National Research Council (NRC), and influenced later actions taken by the Food and Drug Administration (FDA) of the Federal Secu rity Agency with respect to enriched flour and bread.15 The discussion leading up to the issuing of the policy statemen t of the Council on Foods and Nutrition also prompted consideration of the subject of food fortification at a convention of the American Institute of Nutrition in Toronto on April 26, 1939. At the 14 \"Annual Meeting of Council on Foods and Nutrition,\" Journal of the American Medical Association 113, No. 8 (19 August 1939): 680-681. 15 Russell M. Wilder and Robert R. Williams, Enrichment of Flour and Bread: A History of the Movement, NRC Bull. No. 110 (Washington: NRC, 1944), 1-3. 262 meet ing, a formula for the restoration of white flour with vitamins and minerals was presented formally for the first time\u2014and was received by the nutritionists in attendance with little enthusiasm. The contributors to the discussion at the convention included such high-profile nutritionists as Agnes Fay Morgan of the University of California-Berkeley, Lydia Roberts of the University of Chicago, William Sebrell of the United States Public Health Service, E. M. Nelson of the FDA, and Alonzo Taylor, Research Director of General Mills and Director Emeritus of the Food Research Institute of Stanford University. Agnes Fay Morgan's approval of fortification (here meaning both \"fortification\" and \"restoration\" by the AMA's then-newly established standards) was grudging and hedged with warnings. For Morgan, human experience and animal experimentation demonstrated beyond any doubt that \"whole fresh foods properly selected can support abundant health without addition or sophistication.\" She granted that if \"chemical fortification is cheaper than return to the manufacture and distribution of the whole fresh precursors, then let us have thiamine and the B2 complex added to sugar, white flour, polished rice, liquors and soft drinks, ascorbic acid to the beverages and jellies, but without fanfare and exorbitant profit.\" She argued that fortification should be considered as a merely temporary expedient, while the slower process of consumer education regarding the impoverished state of these refined foods took place. Morgan considered the supplementation of the diet with vitamin and mineral concentrates to foods to be \"a dangerous, expensive, and a passing phase in our struggle toward optimum nutrition for all the people.\"16 Lydia Roberts voiced similar misgivings in her approval of fortification. She argued that a long and vigorous program of popular education about the desirability of increasing 16 Agnes Fay Morgan, \"Fortification of Foods with Vitamins and Minerals: The Basic Nutrition Principles,\" Milbank Fund Quarterly 17, no. 3 (July 1939): 221-29. 263 the consumption of protective foods, including less-refined cereal products, was the ideal solution to improving the American diet. But she also accep ted that \"we are faced with the immed iate need to do something to better the nutritional status of our people.\" She conceded that fortification of flour and milled cereals would be in the interest of immed iately bettering nutrition \"if it could be done at no additional cost to the consumer\u2014for the lower-income groups are the ones that most need to have it provided in this way\u2014and if all advertising for the resulting product could be properly controlled.\" To all these pointed ifs, Roberts added an additional warning that fortification \"might tend to give a false sense of confidence that all the deficiencies of refined cereal s have been overcome. It must be remembered that cereals have lost far more than just thiamin or any other single vitamin in the process of milling and that some of these other factors may be equally important in human nutrition.\"17 Alonzo Taylor granted that the addition of concentrated vitamins and minerals might be effective under certain circumstances and at sufficiently low cost, as exemplified by the addition of iodine to table salt in goiterous regions and vitamin D to milk. But he maintained that in attempting to improve nutritional standards, \"we should first seek retention of native vitamins, then restoration when advantageous, then fortification when warranted, leaving medication to the last, applicable to regions and groups where ingestion of vitamins as food components encounters exceptional difficulties.\"18 William Sebrell was most emphatic in his critique of fortification. Although he was, like his colleagues, alarmed by the findings of Stiebeling and her associates at the USDA, he emphasized the lack of thorough research on nutrient deficiencies, particularly in terms of 17 Lydia Roberts, \"Fortification in a General Program for Better Nutrition,\" Milbank Fund Quarterly 17, no. 3 (July 1939): 237. 18 Alonzo E. Taylor, \"Retention, Restoration, and Fortification of Vitamins in Foodstuffs,\" Milbank Fund Quarterly 17, no. 3 (July 1939): 261. 264 their extent and geographic and socioeconomic distribution. He also pointed out that \"there is absolutely no adequate information available on the prevalence of the so-called subclinical stages of the various vitamin deficiency diseases.\" In addition to the lack of reliable knowledge required for an effective food fortification policy, Sebrell attacked the concept of flour fortification on \"naturalistic\" grounds: To me it does seem a little ridiculous to take a natural foodstuff in which the vitamins and minerals have been placed by nature, submit this foodstuff to the refining process which removes them, and then add them back to the refined product at an increased cost. Yet this seems to be the thing that is being proposed. If this is the object, why not follow the cheaper, more sensible, and nutritionally more desirable procedure of simply using the unrefined, or at the most, slightly refined natural food? Moreover, Sebrell saw in the fortification proposal a deeply disturbing process at work: the medicalization of food and eating. \"The association between disease and medicine is so firmly fixed in the mind of the public,\" he stated, \"that one is looked on with a certain degree of suspicion if you say that all the treatment that is necessary is a variety of nice, fresh vegetables, eggs, meats, milk, and so forth.\" Sebrell concluded by defending the superior nutritional value of natural foods: \"As a public health meas ure, I think we should actively oppose the fortification of foods with vitamins and minerals and, instead, we should do all in our power to destroy this misconception in the public mind that such things are necessary to prevent the deficiency diseases.\"19 Leading nutritionists had few illusions about the difficulty involved in altering the well-established popular preference for white flour and bread, particularly among the lower socioeconomic groups that consumed the most of it. But action of some sort was obviously in the wind. In the May 1939 issue of Cereal Chemistry, thiamin researcher Robert R. 19 W. H. Sebrell, \"The Public Health Aspects of the Fortification of Foods with Vitamins and Minerals,\" Milbank Memorial Fund Quarterly 17, no. 3 (July 1939): 241-47. 265 Williams declared that the cereal and sugar processing industries faced the problem of making their staple products \"more nearly the equivalent in nutritive value of the whole seed or cane stalk, as it was once consumed by primitive man. Whether this is to be done by additions of synthetic materials or by retention of the original nutritive components of the crude foodstuffs is a question for industry to decide.\" He ended his article on an ominous note: \"To blink at the scientific facts, which will presently become commo n knowledge, will be suicidal for the commercial enterprises concerned.\"20 Williams was, in effect, threatening the milling and baking industries with the prospect of a conflict with the nutrition science community. A year later, Williams and Sebrell\u2014who somewhat mysteriously became a proponent of restoration despite his earlier denunciation of it\u2014made a similar threat at the annual convention of the Millers National Federation, hinting at the possibility of federal legislation that would force the exclusive production of wholegrain flour and meal if the milling industry did not support vitamin restoration.21 Thus, while nutritionists had largely become convinced by the late 1930s that pushing white flour out of the American diet was an important long-term goal, the question of what role restoration was to play in the immediate future remained open. Several key events in 1940 and 1941 led to the acceptance and adoption of restoration of white flour in the United States, rather than the retention of nutrients by regulation of the flour extraction rate to a higher, whole-wheat or nearly whole-wheat level. \"No One Need Apologize for Advocating Something Less Than Perfection\": The Success of the Enrichment Movement in the 1940s22 20 Robert R. Williams, \"Cereals as a Source of Vitamin B1 in Human Diets,\" Cereal Chemistry 16, No. 3 (May 1939): 301-09. 21 \"Millers Endorse Program of Flour Enrichment,\" Food Industries 14 (1942): 77. 22 Heading quotation from Robert R. Williams, \"Refined Foods and Nutritional Deficiency,\" North-Western Miller 209, no. 7 (18 February 1942): 9-11. 266 In July 1940, the British government declared its decision to fortify their white flour with thiamin. Although the measure was never acted upon, except for expeditionary military forces, it came at a time when the eyes of Americans were turned to the Battle of Britain that was just then beginning. The United States itself was preparing for possible involvemen t in the war. In the summer of 1940, the Subcommittee on Medical Nutrition, established under the Division of Medical Sciences of the NRC, recommended the addition of thiamin to all white flour purchased for the Army and Navy of the United States. Although the Subcommittee confined itself primarily to military problems, it also expressed an opinion regarding the need for similar flour for the civilian population: \"It is generally agreed that the diet of the civilian population is minimal in its provision of vitamin B1 and in case of war the deleterious effect of an inadequate supply of vitamin B1 is likely to manifest itself (unfavorably), especially in men doing heavy labor.\" Coincidentally, the FDA hearings on flour were scheduled at the same time. Flour, as defined in the preliminary announcement of the hearings, was ordinary white flour. Unless testimony could be introduced into the record at the hearings showing the benefit of having in flour an amount of thiamin or other nutrients greater than that contained in the white flour conventionally in use, it would become impossible to legally making additions of such nutrients. A number of organizations were aroused to action by the FDA announcement. The NRC Subcommittee on Medical Nutrition became involved, as did the AMA Council on Food and Nutrition and several milling companies that were experimenting with restoration procedures in accordance with the recommendations of the Council. In addition, the Surgeon General of the United States Public Health Service, Thomas Parran, had been impressed by the Stiebeling-Phipard and other surveys revealing 267 the inadequacies in American diets. Parran , as well as the Director of the USDA Extension Service, M. L. Wilson, who was also concurrently heading a nutrition committee within the government, were convinced that improvement of the quality of bread was basic to any nutrition program that might be undertaken for national defense.23 With these government, professional, and industry groups active in the FDA hearings, standards for restoration of flour were developed, and after a series of smaller meet ings, harmony of opinion was obtained by mid-November 1940. In terms of technical knowledge, the milling industry felt prepared only for addition of thiamin to flour, whereas nutrition scientists believed that the prevalence of pellagra in the South and the increasing evidence of the prevalence of riboflavin deficiency called for addition of niacin and riboflavin as well. Riboflavin at that time was not commercially available in more than meag er quantities. The weight of testimony favored the immediate addition of thiamine and niacin and a later addition of riboflavin. The testimony also stressed the prevalence of iron-deficiency anemia. Restoration of flour with iron up to the level found in whole wheat was thus strongly recommended. A week after the conclusion of the FDA flour hearings, the Food and Nutrition Board of the NRC assembled for the first time. This Board had been established to provide scientific direction for the National Nutrition Program, which was then in its planning stages. The Board members considered the problem presented by white flour and bread, and it proposed to encourage the use of whole-wheat or high-extraction flours insofar as this was feasible. But they were also sympathetic to the testimony at the FDA flour hearings that was favorable to restoration. Flour restoration thereafter became a central component of the Board's broad program to improve the nutritional status of the American people. 23 Wilder and Williams, Enrichment of Flour and Bread, 4. 268 One of the first undertakings of the Food and Nutrition Board was to assist in organizing the National Nutrition Conference for Defense in Washington, DC, which was convened at the request of President Roosevelt in May 1941. At the meeting, a number of invitees voiced their opposition to the use of synthetic vitamins to correct the nutritional deficiencies of white flour. By that point, however, flour restoration was for all intents and purposes a fait accompli. Standards that had been fixed and established earlier were first issued at the conference. Thiamin, riboflavin, niacin, and iron were announced as required nutrients in \"enriched\" flour and bread. Producers were also allowed the option of including specified amounts of calcium and vitamin D as well.24 It was also at the National Nutrition Conference for Defense that the word \"enriched\"\u2014which had been first suggested by Frank Gunderson, a researcher at the Quaker Oats Company, at the FDA flour hearings in September 1940\u2014was decided on.25 It is still the term used today. But it bears repeating that, from the earliest discussions on \"restoration\" of flour and bread, many nutritionists had raised grave concerns about the possibility that the milling and baking industries would exploit the addition of vitamins and minerals for commercial gain. To these wary observers, the use of the term \"enriched\" instead of \"improved\" or \"restored\"\u2014or even more accurately, \"partially restored\"\u2014to describe white flour with added vitamins and minerals appeared to confirm their fears of profiteering. The term \"enriched,\" they argued, could easily give the consumer the impression that the flour was rich and wholesome, when in truth it was only somewhat better nutritionally than plain white flour and still inferior to whole-wheat flour. Elmer 24 Ibid., 5-7. 25 \"The Joseph Goldberger Award to Russell M. Wilder and his address before The Food and Nutrition Board, November 5, 1954,\" National Academy of Sciences, National Research Council, Box 4c, Folder 53, Franklin C. Bing Collection, Eskind Biomedical Library, Vanderbilt University, Nashville, TN. 269 McCollum's criticism of the term was fairly representative of many of his colleagues: \"In the manufacture of wheat flour a score or more of essential nutrients present in significant amounts in the wheat kernel are removed. To give such flour, supplied with three extra vitamins and iron, so good a name as 'enriched' is misleading.\"26 Critics also expressed fear that governmental support for \"enriched\" flour and bread would set a precedent for the accep tance of enrichment of other foodstuffs in a haphazard fashion suitable for sales promotion purposes.27 For the core group of nutrition researchers and physicians who were advocating the addition of vitamins and minerals to white flour, the fact that the term \"enriched,\" rather than any of the less appealing alternatives, achieved the status of a slogan word was crucial in the success of their movement. Two of the leading proponents of enrichment, Russell Wilder and Robert Williams, argued that \"without some such term as an aid to the program it is doubtful whether the practice could have been popularized.\" Wilder and Williams also defended the use of the word \"enriched\" on legal grounds. From the standpoint of the Food, Drug, and Cosmetic Act of 1938, the term \"flour\" was synonymous with ordinary 70-75 percen t extraction \"patent\" flour, since it was by far the most commonly used type of flour at that time. \"As compared with ordinary white flour, under the legal definition,\" Wilder and Williams maintained, \"there can be no doubt that the new product is genuinely enriched. There is no other term which commands more general support.\" They also reminded their critics that \"in any public movement of this sort one must deal with the situation as it exists, 26 E. V. McCollum, \"Bread 'Enrichment',\" Science 102, no. 2642 (17 August 1945): 181. 27 Williams and Wilder, Enrichment of Flour and Bread, 29. 270 though he may wish that embarrassing precedents were absent.\"28 In other words, advocates of whole wheat had to fight on unfavorable legal terrain. By the end of 1941, the main organizations representing the American nutrition and medical professions, as well as the milling and baking industries, had gotten behind the enrichment campaign. By that point, most of the technical barriers to the addition of vitamins at very low cost\u2014one of the main objections aired in the early deliberations on flour enrichment\u2014had been overcome. While most flour was enriched at the mill by continuous feeding of an \"enrichment mixture,\" bakeries were as a whole hesitant to adopt this flour. The main reason was that bakers used various quantities of defatted milk solids and certain grades of flour that could contribute measurable amounts of thiamin to the bread, and they naturally desired to economize on flour costs while still meeting the standards for \"enriched\" bread. The practice of enrichment at the bakery was enabled by the invention in 1938 of a process for producing yeast unusually high in thiamin, which could be used readily in conjunction with ordinary white flour. Yeast-making companies offered this high-thiamin yeast at a lower cost than synthetic thiamin, and bakers quickly developed a preference for it. Technical problems arising from the incorporation of riboflavin into this yeast\u2014which became a required enrichment component in October 1943 after it became certain that adequate quantities could be produced\u2014led many yeast companies to cease promotion of the use of high-vitamin yeast. Consequently, however, leading yeast-producing firms developed enrichment tablets that could be introduced into doughs along with ordinary yeast.29 The timing of the emergence of the enrichment movement played a highly significant role in how it was transmitted to doctors, health educators, and the wider public. 28 Wilder and Williams, Enrichment of Flour and Bread, 10 29 Ibid., 38-39. 271 Both during the period when the United States was anxiously rearmi ng and after it had formally entered the war in December 1941, proponents of enriched bread emphasized its role in providing vigor and fortitude in the nation's defense. An editorial in the Journal of the American Medical Association, for example, drew a connection between the Mayo Clinic's thiamin deprivation experiments and the country's susceptibility to invasion. Neglecting to mention the small number of test subjects involved, the editorial ominously noted: Restriction of the intake of this vitamin in a group of healthy subjects, otherwise adequately fed, provoked, among other signs and symptoms, moodiness, sluggishness, indifferen ce, fear, and mental and physical fatigue. The states of mind and body observed in these subjects were such as would be least desirable in a population facing invasion, when maintenance of stamina, determination and hope may mean defeat or successful resistance.30 Milling and baking firms also employed war-related themes to promote enrichment. An advertisemen t for the \"Savory Toasting Oven\" in the Journal of the American Dietetic Association featured the picture of an advancing American infantryman next to the caption, \"What has toast got to do with winning the war?\" The answer, according to the advertisers, was that \"the extra vitamins and minerals in enriched white bread help restore energy... help combat fatigue.\" Toasting with their device, they claimed, contributed to the war effort by making enriched bread \"more appealing\" and\u2014strangely, given the utter lack of evidence for the claim\u2014\"easier to digest.\"31 Nor did the nutrition researchers at the head of the enrichment movement hesitate to take advantage of the heightened patriotism and anxiety of wartime. In an article in the Science News-Letter in 1941, Russell Wilder wrote, \"Rumor has it that the Nazis are making 30 \"Vitamins for War,\" Journal of the American Medical Association 115, no. 14 (5 October 1941): 1198. 31 Paid advertisement in Journal of the American Dietetic Association 19, no. 1 (January 1943): 51. 272 deliberate use of thiamin starvation to reduce the populations of the occupied countries to a state of depression and mental weakness and despair which will make them easier to hold in subjection.\" Wilder also reported \"a story received from Canadian medical circles\" that some Canadian soldiers enlisted from relief rolls \"were defiant, others were depressed to the point where they seemed useless to the Army.\" But after satisfactory attention was paid to eliminating nutrient deficiencies, \"they became perfectly manageable and effective.\" Wilder even argued that thiamin deficiency was linked with labor unrest and strikes as well. He suspected that many industrial workers were led to make what he called \"unreasonable deman ds\" because of the inadequacy of this particular vitamin in the diet. On the other hand, he suggested, \"many middle-aged industrialists, getting paunchy and trying to keep down their weight, unconsciously restrict their diets in such a way as to fail to get enough of this vitamin, and, consequently, become hyper-irritable.\"32 But it wasn't just editorials and advertisements in professional journals that pushed the limits of scientific respectability. The release of enriched bread and flour was also accompanied by a vigorous publicity campaign in large-circulation magazines and newspapers, as well as brochures, dietary tables, and radio broadcasts. For instance, Robert Williams wrote \"Enriched\" Flour and Bread from the Housewife's Perspective: A Manual for Women Patriots. The pamphlet, printed and distributed by the American Institute of Baking, was intended to explain, \"in simple, factual language,\" the purpose of enrichment and to encourage the use of enriched white bread.33 The January 1941 issue of Readers' Digest, to take one more example, included an article by popular science writer Paul de Kruif entitled \"Supercharged Flour\u2014an Epochal Advance.\" Neglecting to mention the 32 Russell M. Wilder, \"Hitler's Secret Weapon is Depriving People of Vitamin,\" Science News-Letter 39, no. 15 (12 April 1941): 231. 33 Advertisement in the Journal of the American Dietetics Association 18, no. 10 (October 1942): 711. 273 veiled threats made against the milling industry by nutritionists, de Kruif praised the Millers National Federation for voluntarily calling a meet ing of leaders in the milling, baking, and chemical industries, \"vitamin-hunting\" scientists, physicians, and government experts, whose \"commo n purpose was to find ways and means of restoring to American bread the life-sustaining elemen ts which the staff of life once contained\u2014and more besides.\" Dismissing the previous generation of white flour critics as \"food faddists and whole-wheat crack pots,\" de Kruif contended that millers refined wheat for fifty years before scientists knew of its role in contributing to vitamin and mineral deficiencies. But, he argued, nutrition research ers suddenly learned better in the late 1930s, and millers and bakers were only too ready to aright the situation in response to the new evidence. \"The supercharged flour of tomorrow will not only restore to bread the chemicals hitherto milled out, but will carry an extra ration to help allay our hidden vitamin hunger,\" de Kruif concluded the article. \"Bread will again become, in truth, the staff of life.\"34 Despite this sort of hyperbolic promotion by leading professional, government, and industry groups, the enrichment program experienced little initial success. Even following the National Nutrition Conference for Defense in May 1941, where something approaching consensus had been obtained, many nutritionists soft-pedaled enrichment because they felt that their profession should undertake the promotion of whole wheat instead. Robert Williams, frustrated by the lack of progress being made on the enrichment program, charged in a February 1942 speech that \"this great reform is being sabotaged or damned with faint praise by half the nutritionists of the country on the ground that it would be better still if we could arran ge breakfasts of ham and eggs, whole wheat buns and a glass of milk for 34 Paul de Kruif, \"Supercharged Flour\u2014an Epochal Advance,\" Reader's Digest, January 1941, 111-14. 274 everybody.\"35 There was also a belief among nutritionists and dieticians that, since the milling and baking industries were destined to profit from the enrichment program, the costs of promoting enrichment should be borne by them. Bakers and millers, for their part, criticized the government for having started a project and then having failed to support it. Other food industries were jealous of the attention that was apparently being given to flour and bread in the emerging National Nutrition Program. Economic considerations were also significant in the early troubles of the enrichment movement. Since the government had only established official standards for enriched flour, and had not ordered the enrichment of all ordinary flour, there was still a slight price difference between enriched and unenriched products. The price difference appeared exaggerated by the fact that enrichment was largely confined to the brands that had always been higher-priced because they were sold on a quality basis to a select trade, while the brands that had always been cheaper maintained their old prices because they were not enriched.36 A related problem was that returns to scale in flour enrichment made the marginal cost per bag of enriched flour only negligibly higher than the unenriched product for large mills, but not for the smaller ones. Although the large mills could produce enriched flour without having to pass the cost onto the consumer, the small mills avoided doing so, because it would have destroyed their ability to compete with nationally advertised brands from the larger mills on the basis of price. The small mills opted to wait for the demand for enriched products to increas e to such an extent that consumers would be willing to pay the additional cost for enriched flour.37 And it certainly didn't help that the medical establishment and the 35 Robert R. Williams, \"Vitamins in the Future,\" Science 95, no. 2466 (3 April 1942): 338. 36 Wilder and Williams, Enrichment of Flour and Bread, 45-56. 37 David Bishai and Rita Nalubola, \"The History of Food Fortification in the United States: Its Relevance for Current Fortification Efforts in Developing Countries,\" Economic Development and Cultural Change 51 (October 2002): 43. 275 food industry had assured the public of the wholesomeness of white flour and bread for years before the enrichment campaign began. The enrichment of white bread and flour started off at the beginning of 1942 at a level of only about 30 to 35 percent of the entire output of these products. Crucially, military authorities initially delayed in coming to a decision on the use of enriched flour in place of ordinary flour in the armed services. They maintained that the varied and liberal rations in the Army and Navy made enrichment of flour products unnecessary. They also felt that the military services should not adopt enrichment merely as an example to civilians. But enrichment proponents pressured the military authorities, pointing out that their approval of the meas ure might bring \"advertising benefits\" that would help the National Nutrition Program and result in a healthier civilian population\u2014the source of military recruits.38 This pressure, in addition to the release of studies revealing extensive losses of vitamins in the preparation and cooking of camp and cafeteria rations, led military authorities to change their opinion on enrichment. In February 1942, the military decided to adopt enriched flour and bread, just as the civilian enrichment program was reach ing its lowest point.39 The favorable decision of the Army and Navy added an important impetus to the rising sentiment in favor of the enrichment program. All over the country, millers and bakers rushed to get on the bandwagon. Within the first six months of 1942, the extent of enrichment of white bread and family flour rose dramatically to about 75 to 80 percen t. But compliance in the milling and baking industry still did not reach 100 percent by the end of the year. In areas beyond the reach of large-scale food producers, the small mills and bakeries could still undercut their competitors' prices by producing unenriched products. 38 Ibid., 43. 39 Wilder and Williams, Enrichment of Flour and Bread, 48. 276 This all changed on January 18, 1943, when the first Food Distribution Order of the then-newly created Food Distribution Administration became effective. This order mandated, among other standards, that \"all white bread shall be enriched.\" The rationale for requiring enrichment of all bread was that measures justified from the standpoint of public health and the \"working efficiency\" of the population should not in wartime take the slow course that would be tolerable in peacetime, since national survival was at stake. Moreover, wartime shortages of the dried milk solids commonly used in commercial bread making, as well as of other protective foods, led government authorities to assume that B-complex vitamin deficiencies would likely increase unless enrichment became universal. By 1945, about 75 percen t of the total production of \"family flour\" destined for home use was enriched on a voluntary basis.40 Although the Food Distribution Administration order was only the temporary law of the land for the duration of the war, it contributed to a legislative climate that was becoming increas ingly favorable to enrichment. By the end of the 1940s, 26 states had enacted mandatory enrichment laws for flour. Five states in the South, where pellagra was still prevalent, had also passed enrichment laws for cornmeal and corn grits. Efforts to secure obligatory enrichment laws failed in some states with influential dairy industries. The opposition of dairy farmers was based on the belief that synthetic vitamins were being substituted for the addition of milk and milk solids in bread.41 But because nearly all the large producers of flour were engaged heavily in interstate commerce, the consumption of 40 F. C. Bing and A. C. Ivy, \"Bread Enrichment,\" Gastroenterology 5, no. 6 (December 1945): 526-27. 41 Robert R. Williams, \"Food Enrichment: Progress and Controversial Issues,\" Agricultural and Food Chemistry 2, No. 15 (21 July 1954): 771. 277 enriched flour eventually became more or less universal regardless of individual state laws.42 In the years after the war, the FDA also established formal standards of identity for enriched pasta (1946), white bread (1952), cornmeal and grits (1955), and white rice (1958).43 The proponents of enrichment claimed, with some justice, that it satisfied all the parties involved. Enrichment gave consumers the white flour and bread that they wanted and expected without appreciably raising its cost or changing its flavor and baking qualities, while preventing the deficiencies of thiamin, riboflavin, niacin, and iron that worried nutrition research ers. The milling and baking industries, already beleaguered by the long-term trend of declining per capita flour and bread consumption, were spared a potentially devastating public conflict with the nutrition science commu nity. And even though millers and bakers did not make money directly from adding enrichment ingredients, they could use enrichment to burnish the public image of their products and, it was hoped, maintain or even increas e sales. Nevertheless, seen in retrospect it is remark able that the enrichment program succeed ed at all. Even the scientists who supported the enrichment movement\u2014despite their enthusiastic public statemen ts and tactful praise for the \"progressive and patriotic bakers and millers\" who supported the effort\u2014saw enrichment as a band-aid solution.44 As Norman Jolliffe put it, \"All the authorities who recommend the enrichment program recognize that whole-grain flour, bread, and cereals are preferable.\"45 Shortly after the entry of the United States into World War II, Robert Williams and Russell Wilder premised their argument for 42 NRC Committee on Cereals, Cereal Enrichment in Perspective (Washington, DC: NAS-NRC Council Food and Nutrition Board, 1958), 3. 43 Jeffrey R. Backstrand, \"The History and Future of Food Fortification in the United States: A Public Health Perspective,\" Nutrition Reviews 60, no. 1 (January 2002): 17. 44 W. H. Sebrell, quoted in Robert R. Williams and Russell M. Wilder, \"Why Enriched Bread?\" Journal of the American Dietetic Association 18, no. 4 (April 1942): 229. 45 Norman Jolliffe, \"Nutritional Failures: Their Causes and Prevention,\" Milbank Memorial Fund Quarterly 20, no. 2 (April 1942): 122. 278 enriching bread on the grounds that the Soviet Army, fed on whole grains, was \"the only army to match successfully the whole-grain eating army of the Nazis.\" Those on the American home front, Williams and Wilder suggested, could also draw a lesson from the Russian diet: \"The endurance of the Russian citizen equals the vigor of the Russian soldier. The Russian people eating-whole grain bread receive important nutrients denied to people who depend on ordinary white flour for their bread.\" Even though enrichment backers such as Joliffe, Williams, and Wilder considered improving the nutrient quality of white bread an urgent need, they maintained that it was only a temporary measure until more gradual education in the fundamentals of good nutrition, including the consumption of wholegrain foods, could take effect.46 The triumph of the enrichment movement in the United States depended\u2014perhaps to a greater extent than the persuasiveness of the arguments in its favor\u2014on a group of scientists who happened to occupy key positions in the apparatus of nutritional expertise and policymaking that was just beginning to emerg e in the 1930s and 1940s. Wilder, for example, was a member of the AMA Council on Foods and Nutrition and the NRC Committee on Medicine in the 1930s when the \"restoration\" of flour and bread with vitamins and minerals was first discussed. In 1940, he organized and became the first chairman of NRC Food and Nutrition Board, where he played a crucial role in increasing accep tance of enrichment. Wilder was also aided by his immense personal enthusiasm and striking appearan ce\u2014dietician Mary Swartz Rose of Columbia University once referred to him as \"that man sitting there like a Greek god.\"47 Similarly, enrichment supporter Conrad Elvehjem simultaneously sat on the AMA Council on Foods and Nutrition and the NRC 46 Williams and Wilder, \"Why Enriched Bread?\" 225. 47 Cited by Franklin C. Bing, in Marie Balsley, \"Transcriptions of Recordings of Franklin C. Bing, November 24, 1980,\" Box 1e, 2, Todhunter-Bing Collection, Eskind Biomedical Library, Vanderbilt University, Nashville, TN. 279 Food and Nutrition Board, and liaised with the Scientific Advisory Committee of the American Institute of Baking. One of the pivotal, if more obscure, figures in bringing about the success of enrichment was Franklin Bing. He began a promising career in nutrition by pursuing graduate study in biochemistry under Lafayette Mendel at Yale in the late 1920s. While at Yale, he spent the summer of 1927 working with a fellow graduate student, Clive McCay, to complete a study on the nutrition of farmed trout. While still finishing his dissertation, Bing joined the faculty of Western Reserve University in Cleveland, Ohio, where he established a research program specializing in mineral nutrition. But he was unimpressed with the financial rewards and opportunities available in Depression-era academia. So, when the opportunity arose in 1936 to become the first executive secretary of the AMA Council on Foods (later the Council on Foods and Nutrition), he took it. Bing, like many other members of the AMA Council on Foods and Nutrition who supported enrichment, also became a member the NRC Food and Nutrition Board when it was established in 1940.48 In 1943, Bing decided to accept the position of director of the American Institute of Baking, a nonprofit industry organization founded in 1919 to promote education and research in baking. He hoped that, by taking the directorship, he could instill in the baking industry a greater appreciation for nutrition science. Bing believed, like many of his colleagues, that the newer knowledge of nutrition was of little practical use if consumers were expected to radically alter their dietary habits to conform to its findings. While serving as the institute's director, Bing wrote articles and made speeches advising the members of the baking industry to support enrichment in order to improve both their public image and 48 William J. Darby and Patricia B. Swan, \"Franklin Church Bing (1902-1988),\" Journal of Nutrition 131 (2001): 713-16. 280 public health. He also threatened the food industries with the prospect of losing public favor if they continued to view nutrition as a merely academic subject. In a 1944 talk before the members of the Institute of Food Technologists, for example, Bing urged food manufacturers to make their own investigations into the nutritional value of foods \"while they still have the opportunity of being in the forefront in this field which so vitally concerns their future.\"49 Bing also sought to quiet the nutrition scientists who remained critical of enrichment and continued to promote whole wheat exclusively. He advised bakers to abandon their well-worn claims regarding the superior digestibility of white bread and the irritating effects of wheat bran, which had by the 1940s been rather definitively disproved. Bing told them to instead emphasize the \"consumer inertia\" argument against whole wheat; he pled with bakers to compile and publicize the many specific instances in which attempts by baking companies to promote whole-wheat bread had failed.50 However, he appears not to have succeed ed at convincing them to do so. In any event, the directorship of the American Institute of Baking did not turn out to be as satisfying as Bing had hoped, and he resigned in 1949 to become a private nutrition consultant. Bing's concern about the lack of support from nutrition scientists for enrichment was not unfounded. Even after enrichment had gained official approval, the debate did not die down entirely. A handful of nutritionists remai ned critical of enrichment, much to the annoyance of its proponents. The sides of the enrichment debate were, in a sense, divided along generational lines. The older cohort of researchers that had been involved in the 49 Franklin C. Bing, \"Nutrition and the Future of the Food Industries,\" presented before the Institute of Food Technologists, 22 December 1944, Box 4c, Folder 33, Franklin C. Bing Collection, Eskind Biomedical Library, Vanderbilt University, Nashville, TN. 50 Minutes of the first meeting (27 May 1944) of the Scientific Advisory Committee of the American Institute of Baking, Box 5b, Folder 9, Franklin C. Bing Collection, Eskind Biomedical Library, Vanderbilt University, Nashville, TN. 281 pioneering stages of the newer knowledge of nutrition tended to favor the more idealistic \"natural foods\" approach. Their younger prot\u00e9g\u00e9es, who had made the first major breakthroughs in the synthesis and mass-production of vitamins in the 1930s, tended to be more willing to work pragmatically with public eating prefer ences and the interests of the food industry in mind.51 Moreover, the older generation had developed its perspective on the nature of a healthful diet without the kind of exact quantitative knowledge of the vitamin content of foods and of daily vitamin requirements that was beginning to emerg e in the late 1930s and early 1940s. For example, John Murlin, who had begun his career in nutrition under Graham Lusk in the early 1900s, greeted the official announcement of standards for enriched flour with dry skepticism. After discussing the rapid growth in knowledge of the vitamins of the B complex in a 1941 speech, Murlin remark ed that \"without the achievements of the synthetic chemists working with the nutritionist we could not embark on the new program of vitamin enrichment of flour, shall I say for better or for worse?\" Murlin pointed out that in animal experiments, the addition of the six B vitamin factors then available in synthetic form failed to clear up deficiency symptoms. Only supplements of yeast succeeded in both clearing up these symptoms and improving digestion in the animals. Murlin gave his consent to the enrichment proposal only because he was reassured that the NRC Committee 51 Russell M. Wilder later confessed that the exclusion of Elmer McCollum, Henry Sherman, Mary Swartz Rose, and John Murlin from the NRC Food and Nutrition Board \"was my mistake.\" He did so because he felt, at the time when he was chair, that they were \"too advanced in years to assume the vigorous responsibilities facing the members of the Board.\" Their opinions on enrichment, of course, might also have weighed into his decision. See \"The Joseph Goldberger Award to Russell M. Wilder and his address before The Food and Nutrition Board, November 5, 1954,\" National Academy of Sciences, National Research Council, Box 4c, Folder 53, 76, Franklin C. Bing Collection, Eskind Biomedical Library, Vanderbilt University, Nashville, TN. As a result of his anti-enrichment stance, McCollum was demoted from being a full member of the FNB and never again invited to attend their meetings. See E. V. McCollum, From Kansas Farm Boy to Scientist: An Autobiography of Elmer Verner McCollum (Lawrence, KS: University of Kansas Press, 1964), 199-200. 282 on Food and Nutrition was working to gradually increase the extraction rate of grain to retain more of its nutrients\u2014a promise that never materialized.52 Similarly, Leonard Maynard, who by the early 1940s had risen to the highest echelons of the scientific establishment in academia and government, publicly consented to the enrichment program while privately expressing strong misgivings. Maynard deviated from his general preference for natural foods over fortification, as he explained in a letter to Elmer McCollum, only because of the wartime emergency and his willingness to see \"one or two proposals for fortification tried out.\" Maynard admitted that he agreed to flour enrichment \"perhaps also because of expediency in connection with the activities of the Food and Nutrition Board\"\u2014that is, he was dealing in the realpolitik required of a committeeman . Although Maynard supported the fortification of marg arine with vitamin A, because of the rising prices for butter and dairy products caused by the outbreak of war, he claimed that he was never very enthusiastic about the enriched flour and bread program. Maynard did believe that enrichment was a step in the right direction, but, as he confided to McCollum, \"I did not like the way the program was set up in advance by a small group for the rest of us to rubber stamp.\" Maynard also felt that \"a definite bad effect of the enriched flour and bread program has been the impetus to the sale of vitamin pills.\" The emphasis placed on thiamin in raising morale and providing \"pep\" in the publicity for the enrichment program, he angrily noted, \"has given the pill vendors just the basis they need for the mark eting of their own products.\"53 Maynard preferred the addition of milk solids, soybean flour, and other natural ingredients to white bread rather than synthetic vitamins. But he was disappointed that the 52 John R. Murlin, \"Nutrition in the First World War and Now,\" in Proceedings of National Nutrition Conference for Defense (Washington: GPO 1942), 23-29. 53 Letter (8 December 1942) from Maynard to McCollum, Box 4c, Folder 54, Franklin C. Bing Collection, Eskind Biomedical Library, Vanderbilt University, Nashville, TN. 283 millers, who were primarily concerned with selling as much wheat flour as possible, had opposed the addition of these supplemen ts to bread and insisted that \"enriched\" be used to describe only one particular product containing synthetic vitamins and minerals. Maynard was further annoyed that the milling industry, after conceding that thiamin should be added to flour, had hesitated in consenting to the addition of niacin, riboflavin and iron. \"The millers have wanted to ride along in the advertising and publicity and yet make as small a contribution as possible,\" he remark ed. Maynard was also suspicious of the baking industry's motives in supporting enrichment. From analyses conducted by the School of Nutrition at Cornell University, he knew that there had been a definite decline in the amount of dried skim milk used in bread making that had started before the prices for it went up due to war mobilization. Even worse, in Maynard's view, was that the amount of milk solids in commerci al loaves fell even lower with the advent of the enrichment program, presumably because bakers saved on milk in order to purchase the additional synthetic vitamins and special yeasts needed to meet enrichment regulations.54 Henry Sherman also responded to the enrichment program guardedly. In Modern Bread from the Viewpoint of Nutrition (1942), Sherman and his Columbia University colleague, Constance Pearson, acknowledged that enriched bread was better than ordinary white bread. But they also approved of the development of breads that were \"nutritionally modernized,\" as they put it, in other ways. They were particularly favorable to breads made with milk and vitamin-rich yeast or wheat germ, and noted positively that millers were beginning to devise techniques for eliminating roughage from flour while retaining its natural iron content. Sherman and Pearson also emphasized that questions relating to the possible health threat posed by ordinary white bread had to take into consideration the 54 Ibid. 284 overall composition of the diet. The popular taste for white bread would not have become such a pressing public health problem in the United States and Britain, they noted, had the decline in the consumption of bread in these countries over the course of the nineteenth and twentieth centuries not been largely offset by an increase in the consumption of refined sugar. In countries where the consumption of large quantities of white bread did not typically result in widespread vitamin deficiencies, such as France, sugar made up a smaller part of the diet and vitamin-rich vegetables were eaten in greater quantities.55 Although many nutritionists responded equivocally to the flour and bread enrichment movement when it was being pushed forward in the early 1940s, few of them actually opposed it publicly. But the emergence of new research around that time began weakening the scientific underpinnings of enrichment. Agnes Fay Morgan, for example, conducted an animal-feeding study in 1941 with various fractions of the vitamin B complex that made her wary of adding purified vitamins to foods. She found that fortification of experimental diets with large quantities of synthetic vitamins such as thiamin and niacin precipitated disease conditions worse than the subacute deficiency states produced by the control diet, which was balanced in its inadequacies.56 Shortly after the publication of Morgan's results, other research groups reported that eight synthetic vitamins were insufficient as a source of the vitamin B complex for the monkey and the puppy.57 Around the same time, a team of nutritionists at the University of Illinois published the results of a 55 Henry C. Sherman and Constance S. Pearson, Modern Bread from the Viewpoint of Nutrition New York: Macmillan, 1942), 82-86, 101-104. 56 Agnes Fay Morgan, \"The Effect of Imbalance of 'Filtrate-Fraction' of the Vitamin B Complex in Dogs,\" Science 93, no. 2411 (14 March 1941): 261-262. See also Marion B. Richards, \"Imbalance of Vitamin B Factors,\" British Medical Association 1, no. 4395 (31 March 1945): 433-36. 57 Harry A. Waisman, A. F. Rasmussen, Jr., C. A. Elvehjem, and Paul F. Clark, \"Studies on the Nutritional Requirement of the Rhesus Monkey,\" Journal of Nutrition 26 (1943): 205-18; J. P. Lambooy and E. S. Nasset, \"The Inadequacy of Eight Synthetic B Vitamins for the Nutrition of Puppies\u2014Unknown Factor (Factors) in Yeast and Probably Liver,\" Journal of Nutrition 26 (1943): 293-302. 285 rat-feeding experiment that cast doubt on the efficacy of enrichment. In the study, different types of breads were fed as the sole diet to growing rats, except for a supplemen t of cod liver oil to supply vitamins A and D. The investigators found that the growth rate of rats fed whole-wheat bread was superior to that of rats fed white bread enriched with synthetic vitamins, and that the difference in rates was exaggerated when the rats were allowed to feed ad libitum. Enriched white bread with nonfat milk solids was the equal of whole-wheat bread in the promotion of growth and was distinctly superior to it in the promotion of bone calcification. The group of rats with the best growth rate and bone formation was fed whole-wheat bread containing six percent of nonfat milk solids.58 In addition to these animal-feeding experiments, a controlled study of low-thiamin diets carried out at the University of Minnesota using healthy young men failed to confirm the Mayo Clinic trials that had been used by enrichment proponents as the basis for their claim that \"hidden\" or \"latent\" thiamin deficiency was widespread.59 These findings galvanized some nutritionists to openly criticize mandatory enrichment laws just as they were being enacted across the United States. In a 1944 article in the journal Physiological Reviews, University of California, Berkeley biochemist Samuel Lepkovsky contended that the enrichment formula was premised on an overestimate of the importance of thiamin in the human dietary. Enrichment proponents had no evidence, he further pointed out, that other factors in the vitamin B complex besides thiamin supplied by whole wheat were unimportant. After discussing the numero us feeding experiments that had shown the nutritional superiority of whole-wheat flour, Lepkovsky concluded that it was far 58 H. H. Mitchell, T. S. Hamilton, and J. B. Shields, \"The Contribution of Non-Fat Milk Solids to the Nutritive Value of Wheat Breads,\" Journal of Nutrition 25, no. 6 (June 1943): 585-603. 59 Ancel Keys, Austin F. Henschel, Olaf Mickelsen, and Josef M. Brozek, \"The Performance of Normal Young Men on Controlled Thiamin Intakes,\" Journal of Nutrition 26, no. 4 (October 1943): 399-415. 286 easier to build an adequate diet around whole wheat bread than enriched white bread, particularly during wartime when many of the protective foods were becoming scarcer. He also questioned the claim, often repeated by enrichment advocates, that Americans could never be convinced to eat whole-wheat bread. Lepkovsky noted that wholegrain bread and cereals had not been considered for inclusion in the \"protective food\" category until 1936, so nutritionists had not been given any real opportunity to encourage the consumption of these foods before enrichment was first proposed.60 Shortly after Lepkovsky's article appeared, his Berk eley colleague Agnes Fay Morgan came out against enrichment as well. Insisting that nutrition educators had been successful in the preceding years in educating the public about the benefits of natural foods, Morgan contended that conceding to the enrichment of white flour would unnecessarily dilute their message. In support of her claim, she pointed to health-conscious California, where nearly one-third of all bread sold was either of the \"wheat\" or \"whole wheat\" variety.61 Elmer McCollum also publicly voiced his opposition to enrichment. Apparently savoring the opportunity for a pun, he derided the enrichment program as \"half-baked\" in a 1945 article.62 In the past, McCo llum had defended the consumption of white bread on the grounds that people could secure adequate amounts of vitamins, minerals, and high-quality proteins if half of their diet was comprised of the protective foods. He continued to defend this position, maintaining that people who built their diets around dairy products, eggs, meat, 60 Samuel Lepkovsky, \"The Bread Problem in War and in Peace,\" Physiological Reviews 24, no. 2 (February 1944): 239-76. For the stir created by Lepkovsky's article, see, e.g., the correspondence between Lela Booher, Chief Nutritionist of General Mills, Inc. and Franklin C. Bing, Box 2a, Folder 20, Franklin C. Bing Collection, Eskind Biomedical Library, Vanderbilt University, Nashville, TN; and the minutes of the first meeting of the Scientific Advisory Committee of the American Institute of Baking, Box 5b, Folder 9, Franklin C. Bing Collection, Eskind Biomedical Library, Vanderbilt University, Nashville, TN. 61 Agnes Fay Morgan, \"Education the Key,\" Journal of Home Economics 37 (September 1945): 401. 62 E. V. McCollum, \"Half-Baked Program,\" Journal of Home Economics 37 (September 1945): 397-99. 287 fruits, and vegetables did not need to eat enriched bread to ensure adequate nutrition. Moreover, McCollum remained convinced that, due to the centralization of cereal growing and milling in the American mid-continent far from the centers of urbanization, the majority of the population could not realistically be fed on whole-wheat flour because of its poor storing qualities. But he contended that less affluent Americans, who could not afford sufficient amounts of the protective foods, would not have all their nutritional needs met by bread enriched with just three vitamins and one mineral. To overcome this problem, McCollum recommended the use of low-cost and widely available natural materials\u2014milk solids, defatted wheat and corn germ, and dried brewer's yeast\u2014as bread ingredients. These additions, he claimed, changed the volume, color, and texture of bread very little, so the problem of consumer acceptance faced with whole wheat bread did not apply. The defenders of enrichment were quick to rebut these claims. Williams and Wilder maintained that the use of enriched flour did not prevent bakers from adding dry milk and other natural ingredients to their products. They also criticized McCollum for ignoring the fact that about half of the flour consumed by Americans went into home baking and could therefo re not be regulated in the same way as commercial bread production to include milk solids, wheat and corn germ, and special yeast. Moreover, they pointed out that most of the \"brown\" breads on the mark et were not whole-wheat or anything near it, but admixtures of white and whole-wheat flour or bran in varying proportions. Williams and Wilder also insisted that consumer tastes and industry economics made it very unlikely that McCollum's naturally fortified bread would quickly obtain a large market.63 63 Russell M. Wilder, \"Better to Light a Candle,\" Journal of Home Economics 37 (September 1945): 399-400; Wilder and Williams, Enrichment of Flour and Bread, 47. 288 In an exchange with Williams in the journal Science, McCollum conceded that he had ignored sociological, industrial and legal precedent in his recommendations concerning bread improvement. But, he rebutted, \"It has long been my belief that eventually industry must adjust itself in matters involving foods to the physiological needs of consumers .\"64 McCollum asserted that he offered his critical view of enrichment \"with no other objective than to acquaint the public with facts which are supported by scientific investigations.\"65 McCollum was implying, in effect, that Williams had based his argument for enrichment on shaky science and an excessively accommodating attitude toward industry. McCollum probably felt that his firm stance toward the food industry and people's eating habits was justified by experience, given the rapid expansion in the consumption of milk, fresh vegetables, and fruit that had occurred since he and his fellow nutritionists began promoting their health benefits in the late 1910s and early 1920s. Lepkovsky, Morgan, and McCollum were not the only nutritionists who drew attention to the lack of concrete evidence in favor of enrichment. Although the editors of Nutrition Reviews favored the meas ure, they nevertheless remarked in 1943: \"It is a curious fact that the enrichment of white flour and white bread was promulgated with little direct experimental evidence to demonstrate the value of such a proposal for the nourishment of human beings.\"66 Indeed, even in the years following the adoption of enrichment laws in many states, little effort was made by either government or industry to systemat ically 64 Why McCollum did not apply this line of logic to the milling industry and call for its decentralization in order to supply freshly ground whole-wheat flour to the entire population is somewhat perplexing. It is possible that he based his stance on the belief, which he voiced as early as the first edition of The Newer Knowledge of Nutrition (1918), that \"water-soluble B\" deficiency was very rare in the West anyway. 65 Robert R. Williams and E. V. McCollum, \"Bread 'Enrichment',\" Science 102, no. 2642 (17 August 1945): 180-182. 66 \"Nutritive Value of Enriched White Flour and Bread,\" Nutrition Reviews 1, no. 10 (August 1943): 295. 289 determine whether enriched white flour did what it was supposed to do. The principal study designed to evaluate the effects of enrichment on public health was not conduced in the United States, but in Newfoundland, where conditions could be more easily monitored. Initial dietary surveys of the area had shown evidence of vitamin A and thiamin deficiencies, and diets generally low in numerous nutrients. At the time the Newfoundland enrichment study was begun in 1945, extensive clinical and laboratory surveys had led to the conclusion that nutritional deficiencies were widespread. Vitamin A, thiamin, riboflavin, niacin, and ascorbic acid deficiencies were thought to be the most common, but there was relatively little unambiguous evidence of the \"classic\" deficiency diseases. A re-survey in 1948, three years after mandatory enrichment of white flour, showed considerable improvement in nutritional status. Although no improvement in signs and symptoms attributed to vitamin C deficiency was noted, those apparently related to thiamin, riboflavin, niacin, and vitamin A had markedly decreased. Crude mortality rate, tuberculosis death rate, and infant mortality rate had decreased considerably.67 However, as critics of the Newfoundland enrichment study pointed out, there was no control group fed on unenriched white flour. Moreover, during the study period, the investigators themselves also identified a general improvemen t in economic conditions, a widespread distribution of milk powder, and the enrichment of margarine with vitamins A and D. Considering these difficulties in establishing a direct effect of enrichment to the exclusion of other factors upon the health of Newfoundland's population, the greater difficulty of showing any such effect upon a relatively better-fed and more heterogeneous American population was obvious. And in fact, the reduction in deficiencies of the vitamin 67 Grace A. Goldsmith, William J. Darby, Ruth C. Steinkamp, Anne Stockell Beam, Ellen McDevitt, \"Resurvey of Nutritional Status in Norris Point, Newfoundland,\" Journal of Nutrition 40, no. 1 (January 1950): 41-69. 290 B complex observed in the United States during and after World War II coincided not just with the implementation of the enrichment program, but also with higher employment and rising wages, which made possible the purchase of more of the protective foods.68 Defenders of enrichment countered that, as a practical matter, direct evidence of particular nutrient deficiencies was nearly impossible to obtain. They argued that basic controlled studies upon the nutritional requirements of small numbers of humans, coupled with mass dietary surveys to determine whether the population was consuming diets containing lesser amounts than the estimated requirements, was the only plausible approach that could be taken.69 This was largely how the matter stood in the mid-1950s, by which point, as a major American nutrition textbook happily reported, \"frank deficiency disease have been virtually eliminated from this country, due largely to the work of medical, public health and nutrition authorities.\"70 \"Enriched\" flour, like \"iodized\" salt and \"vitamin D\" milk, quickly became an unremarkable feature of the American food supply\u2014adjectives the consumer habitually sees on food packaging without taking much notice. More remarkable, and even less noticed, was the long-running tension within nutrition science between the nutrient-centered and foods-centered approaches to diet that the enrichment controversy brought to the surface. With the synthesis of vitamins and the development of methods for determining quantitative requiremen ts of these substances in the 1930s, nutrition experts were suddenly confronted with the possibility of correcting deficiencies without caring about their dietary origin. In fact, some enrichment proponents 68 See, e.g., Agnes Fay Morgan and Lura M. Odland, \"The Nutriture of People,\" in Food: Yearbook of Agriculture, 1959, 197-98. 69 D. M. Hegsted, Martha F. Trulson, and Fredrick J. Stare, \"Role of Wheat and Wheat Products in Human Nutrition,\" Physiological Reviews 34, no. 2 (April 1954): 246. 70 Margaret Stella Chaney, Nutrition, 5th ed. (Cambridge, MA: Houghton Mifflin, 1954), 416. 291 claimed that it made little sense to use the nutritional composition of a natural food as the basis for calculating how much of a vitamin or mineral to add to its refined counterpart. But many of the physiologists and biochemists who were responsible for bringing about the \"golden age of vitamin research\" were not willing to go this far. The chemically reductionist approach to nutrition, they believed, had to be balanced by a recognition of the immen se complexity of biological processes. The development of Elmer McCollum's dietary advice illustrates the tension between chemical reductionism and biological holism that characterized nutrition science in the first half of the twentieth century. It was the failure of the chemical method of analysis to determine the composition of an adequate diet that inspired McCollum to develop and refine the biological method of analysis, in which natural foods were the starting point in experimental procedure. Using this method, he and other nutrition researchers discovered all the vitamins required for normal health between the 1910s and the 1940s. But just as importantly, in McCollum's view, the biological method of analysis had revealed that foodstuffs could be classified on the basis of their function in nature. As a result of his many animal-feeding experiments, he determined that foods could be categorized into groups according to whether they represented principally deposits of reserve food material (seeds and roots), functioning active protoplasm (leafy vegetables, the germ of seeds, and organ meats), highly specialized contractile tissues (muscle meats), or complex substances for the nourishment of the young (milk and eggs). \"From their biological function,\" McCollum concluded, \"their dietary properties can be fairly accurately predicted.\" McCollum believed that it was this aspect of his laboratory 292 research that was most significant for improving public health\u2014vitamins and minerals played a contributory, albeit large, role in the elucidation of these biological categories.71 Henry Sherman also attempted to straddle the borderland between chemistry and biology. On the one hand, he discussed the concept of \"optimal nutrition\" in terms of vitamins and minerals, and he devoted considerable effort to determining the quantities of these substances that would afford the highest state of health. But on the other hand, Sherman translated these experimental findings into terms of natural foods when he offered dietary advice. Indeed, as he argued in The Nutritional Improvement of Life (1950), one of the basic concepts that guided the work of nutrition scientists in the first half of the twentieth century was \"the principle of natural wholes.\" By this he meant that \"the body as an organism is (in some important respects) something more than merely the sum of its parts because it is an organized whole and functions as such in nature.\" Sherman further pointed out that the principle of natural wholes applied to food and nutrition as well. \"It is a significant scientific fact that we have evolved in nutritional adjustment to the wholes which we found in nature and until recently ate as we found them,\" he stated. \"So wisdom lies in nourishing ourselves at least largely on natural foods.\"72 In other words, researchers like McCollum and Sherman believed that the expanding knowledge of individual nutrients was greatly improving the ability of science to describe the nature of the ideal diet, but this knowledge could still not explain all the properties of foods and their effects on the body. As a consequence, they maintained a cautious attitude toward any changes in eating habits that did not entail increas ing the consumption of natural, unrefined foods. 71 McCollum, Newer Knowledge of Nutrition, 1st ed., 149-50. 72 Sherman, Nutritional Improvement of Life, 129-30. 293 The restorative addition to flour of several of the vitamins and one of the minerals up to the levels naturally found in whole wheat represented an unusual compromise between the nutrient-centered and foods-centered approaches to diet. On the one hand, defenders of the so-called restoration principle in the pro-enrichment camp contended that they approved of the addition of a few synthetic vitamins to flour in order to make it as nutritionally equivalent to the natural product as possible. Robert Williams, for example, hypothesized that human nutritional needs had become adapted over the course of evolution to the foods that were naturally available in the environment. However different the actual diets of prehistoric and modern man, he argued, the nutritional content of the natural foods that comprised these diets should roughly be the same. The presence of thiamin in plant seeds, where it helped metabolize the carbohydrates that sustained the young seedling until it germinated and could begin producing its own nourishment via photosynthesis, was a case in point. \"More and more,\" Williams stated in an article,\" it seems that man commits a crime against nature when he eats the starch of the seed and throws away the mech anism necessary for the metabolism of that starch by the plant.\"73 On the other hand, advocates of the restoration principle were more confident than the defenders of whole wheat that nutrition scientists had uncovered and synthesized the nutrients present in natural foods that were significant for improving human welfare. As Williams declared in 1942, \"I think we may say that we have already discovered and produced commercially the vitamins which are required to check the great nutritional plagues of mankind.\"74 73 Robert R. Williams, \"The Chemistry and Biological Significance of Thiamin,\" Science 87 (24 June 1938): 562-63; Robert R. Williams, \"The Evolution of Man's Dietary Requirements,\" Journal of the American Dietetic Association 17 (May 1941): 423-27; Robert R. Williams, \"Synthetic Vitamins and Human Nutrition,\" Industrial and Engineering Chemistry 39, no. 4 (April 1947): 464-66. 74 Robert R. Williams, \"Vitamins in the Future,\" Science 95, no. 2466 (3 April 1942): 336. 294 \"A Little Knowledge Is a Dangerous Thing\": The Enrichment Controversy in Britain75 During World War II and for nearly a decade after, the \"bread problem\" was handled differently in Britain than in the United States. Although this divergence was determined to some extent by the particular exigencies of Britain's wartime and immediate postwar experience, differences in the resolution of the scientific debate over the merits of adding vitamins and minerals to refined flour also played a role. Nevertheless, the conceptual tensions between chemical reductionism and biological holism that helped drive the enrichment controversy in the United States were also present in the debate over the merits of mandating the production of \"wheatmeal\" flour in Britain. In the years just preceding the war, enrichment efforts in Britain appeared to be taking a similar course as they had in the United States. John Boyd Orr's widely read dietary survey, Food, Health and Income (1936), revealed that the intake of 50 percent of the British population fell below the minimal daily requirement for vitamin B1. Orr's survey showed that the average daily consumption of bread and flour was just over one-half pound per capita, but the distribution of the consumption was very uneven. Better-off people averaged about four ounces a day, whereas in World War I it was found that the average daily consumption in some poor individuals doing heavy manual labor was as high as a pound and a half. It was just these people with diets low in the expensive protective foods\u2014milk, cheese, eggs, fruit, and vegetables\u2014and highest in carbohydrates who had the highest requirements for vitamin B1. Additional studies in the late 1930s, as well as the mass dietary surveys being carried out in the United States and Dominions at roughly that time, confirmed to British 75 Quotation from J. A. Scott, \"Bread as Human Food,\" Proceedings of the Nutrition Society 4 (1946): 44 295 nutrition scientists that something had to be done to address the shortcomings of white bread. \"The immediate pre-war period was a very interesting one,\" J. C. Drummo nd recalled in a speech just after the war, \"because, after many years of controversy about the relative merits of white and brown breads there had at least come about a general agreement, both inside milling circles and outside among medical and scientific experts, that there was something wrong with white bread.\" By about 1938 or 1939, according to Drummond, some of the British millers were seriously contemplating a project for adding vitamin B1 to white flour.76 There was also a growing agreement at that time that calcium intake was deficient in many British diets, especially in the lowest-income families that could not afford a sufficiency of dairy products, and that bread might be a proper vehicle for calcium fortification. Soon after the outbreak of the war in 1939, the extraction rate was controlled under the Defence Regulations. The rate was initially set at 70 percent in September 1939, and raised in the following month to 73 percent. No enrichment policy had yet been formulated when the war began. But on July 18, 1940, the Ministry of Food announced that the standard white loaf would be fortified with a supplement of thiamine and a calcium salt. The government decided not to suspend the manufacture of \"straight-run\" white flour on the grounds that the great majority of consumers preferred white bread, and that the keeping qualities of white flour were greater than those of wholemeal (that is, whole-wheat or wholegrain) flour. The government also ordered that wholemeal bread be available at the same price as white bread, in order to remove the grievance of those who claimed that the poor could not eat wholemeal because of its price. 76 J. C. Drummond, \"Scientific Approach to Food Problems during the War,\" International Journal of Food Sciences and Nutrition 1, no. 2 (Summer 1947): 57. 296 In a leading article in Nature soon after the announcement, Jack Drummo nd, who had been appointed Scientific Adviser to the Minister of Food, and Thomas Moran, his deputy, endorsed the flour reinforcement program. They pointed out that the alternative\u2014mandating the production of bread made from high-extraction wholemeal flour\u2014would reduce the availability of \"milling offals\" for the livestock industry and therefore jeopardize Britain's milk supplies. Drummo nd and Moran further argued that the introduction of the reinforced white flour \"will undoubtedly stultify the controversy of white versus brown bread, since it is mainly in respect of vitamin B1 that the white loaf has been open to attack. There is not the same evidence that we are deficient in vitamin E or members of the vitamin B2 complex which are present in whole meal flour.\" They granted that the government decision \"will probably not satisfy the advocates of brown or wholemeal bread,\" but they were nevertheless confident that \"it is a compromise which is sound on nutritional grounds and inevitable from the practical point of view.\"77 Others in the nutrition science commu nity, however, were not convinced that such a compromise was worthwhile. Indeed, contrary to Moran and Drummond's prediction, the controversy over white bread intensified more than ever following the Ministry of Food's \"reinforcement\" announcement. This debate, carried out in medical journals, on the floors of Parliament, and in newspapers, often became acrimonious and bitter. An editorial in The Lancet written shortly after the announcement of the flour reinforcement order was for the most part praiseful, but it concluded on a cautionary note. The medical and scientific workers responsible for discovering vitamin and mineral deficiencies, the editorial noted, \"are not satisfied that the addition of vitamin B1 to white bread makes it as good a food as 77 T. Moran and J. C. Drummond, \"Reinforced White Flour,\" Nature 146, no. 3691 (27 July 27 1940): 117-18. 297 wholemeal ; the extent to which some other nutrients which are absent in white bread are present in wholemeal bread, and the degree to which the human body requires them, are not yet fully established.\"78 This was precisely the argument that the Accessory Food Factors Committee of the Medical Research Council (MRC) made in a special memorandum. The members of the committee recommended that, rather than reinforcing white flour with thiamine, the flour extraction rate should be increased from 70 percent to 80-85 percent, and should contain the germ of the wheat grain and as much of the nutrient-rich aleurone layer and the finer portions of the bran as possible. They pointed out that requirements for other members of the vitamin B complex besides thiamine contained in the wheat grain could not be ignored, even though knowledge on these vitamins was \"less exact than for vitamin B1.\" The members of the committee also called attention to the loss of the essential fatty acid linoleic acid, as well as vitamin E and minerals, in the refining and bleaching of flour. Their final recommendation was for the addition of calcium to 80-85 percent extraction flour, in order to raise intake of calcium, particularly in the poor who consumed little in the way of dairy products, and to counteract the possible calcium-blocking effect of phytic acid, found in greater quantities in wholegrain and high-extraction flours. Even after the release of the MRC flour memorandum, The Lancet still considered the thiamin content of the wheat grain to be the significant factor in the choice between white and brown flours.79 But the editors changed their position after the publication in October 1940 of an animal feeding experiment by Harriette Chick at the Lister Institute. Chick observed that the nutritive value of white flour on a small number of young growing rats was inferior to that of wholemeal flour, even when the defects of the former in protein, 78 \"A Reinforced Loaf,\" Lancet 236, no. 6100 (27 July 1940): 79 \"Second Thoughts Bread,\" Lancet 236, no. 6102 (10 August 1940): 167-77. 298 minerals and vitamin B1 were corrected. The rats eating a ration consisting mostly of wholemeal flour gained about twice as much weight as those on \"reinforced\" white flour, and they utilized their food for growth more efficiently.80 Chick suggested that the differen ce between the two groups was perhaps due to other members of the vitamin B complex. Soon after the publication of Chick's rat study, The Lancet turned against the flour and bread reinforcement program. A leading article under the heading, \"Superiority of Wholemeal ,\" pointed out that considerations of the inferiority of white flour could no longer be concerned only with thiamin, but with other factors as well, the nature of which was not certain. \"If we accept that these findings in rats are applicable to man,\" the editors argued, \"we may argue that the signpost points to wholemeal as the high-road to better health and greater food economy.\"81 The Ministry of Food, for its part, did not promptly change its stance in response to Chick's study, which only added to a rising crescendo of criticism.82 In hearings and letters-to-the-editor, Members of Parliament had been attacking the Ministry of Food ever since the announcement of the flour reinforcement program in the summer of 1940. Sir Ernest Graham-Little, a particularly vociferous proponent of wholemeal bread in the House of Commons, challenged the reinforcement program on the grounds that it was statistically demonstrated that the health of the nation was better in the last part of World War I and in the period that immediately followed, when the British people were obliged to eat wholemeal bread. He also pointed out that claims regarding the indigestibility and irritating 80 Hariette Chick, \"Nutritive Value of White Flour with Vitamin B1 Added and of Wholemeal Flour,\" Lancet 236, no. 6113 82 Reinforced Loaf,\" Lancet 236, no. 6117 (23 November 1940): 668. 299 effects of the bran in wholemeal flour had been amply disproved during World War I. 83 Adding to the disquiet were rumors that the head of the English branch of a large vitamin-manufacturing firm, Hoffmann-La Roche, was responsible for promoting the reinforcement of flour with thiamin, but that there were efforts to keep the matter quiet.84 Gradually, however, the British government modified its bread policy as a result of the controversy. Lord Woolton, the Minister of Food, had been impressed by the unanimity of scientific opinion on the nutritive value of high-extraction flour. He announced in the House of Lords in December 1940 that he had decided to adopt an 85 percent extraction rate as the basis for \"wholemeal flour,\" which would be supplied at the same price as \"straight-run\" white flour of 70-75 percent extraction. The Lancet reported soon after the announcement that Lord Woolton \"was afraid that his decision would disappoint those people who were anxious that he should adopt a more dictatorial attitude, but he was sure that the advocates of wholemeal bread would not abate in their efforts to persuade the public to adopt this form of diet.\"85 The MRC naturally welcomed the Ministry of Food's decision to promote the type of high-extraction bread recommended in its 1940 memorandum.86 In response, the MRC's Accessory Food Factors Committee issued a second memorandum in May 1941 that incorporated the results of further research and defined more exactly the character of their recommended 85 percen t extraction flour. The Accessory Food Factors Committee, with the assistance of the Cereals Research Station of the Association of British Flour Millers at St. Albans (which had been taken over by the Ministry of Food as a wartime measure) 83 \"Fortifying the National Diet,\" Lancet 236, no. 6100 (27 July 1940): 117. 84 Letter from E. M. Nelson to Franklin C. Bing (5 April 1941), Franklin C. Bing Collection, Box 16, Folder 29, Eskind Biomedical Library, Vanderbilt University., Nashville, TN. 85 \"Wholemeal and Fortified Bread,\" Lancet 237, no. 6123 (4 January 1941): 29. 86 \"The M.R.C.'s View,\" Lancet 237, no. 6123 (4 January 1941): 29. 300 developed what became known as \"National bread\" or \"National wheatmeal .\" This National bread was made from a type of 85 percen t extraction flour that contained a maximum amount of the B vitamins and protein as little of the bran as possible.87 The memorandum also detailed the two methods that millers could use to produce this type of flour. They could do so either by rearrangement of the milling machinery, or by adding to the white flour obtained by ordinary milling processes combinations of other fractions of the wheat that did not include the 15 percent consisting of the \"coarse bran\" and \"pollards,\" or \"shorts.\"88 Unfortunately, after the second MRC memorandum was issued, the Ministry of Food's Cereals Division issued regulations that made no distinction as to selection of the most nutritious part of the grain, and permitted National wheatmeal to consist of ordinary white flour mixed with coarse bran and pollards and virtually no germ. It was this type of \"milling offal\" that contributed to the dark color and roughage disliked by those who did not care for wholemeal breads. Yet without enforcement of the MRC's exact specifications, customers bought National bread that varied from a coarse brown to a fine-ground off-white, depending on what the mill had produced.89 And as 1941 progressed, National bread consisting of a mixture of white flour and coarse branny material was becoming more and more the rule.90 Unsurprisingly, this unpopular loaf captured only a small percentage of the total bread mark et. As the National wheatmeal situation deteriorated, nutritionists and politicians became more explicitly critical of the milling industry and the Ministry of Food. The Lancet, dismayed at what was happening, attacked the large milling concerns, which they 87 Harriette Chick and Margaret Hume, \"The Work of the Accessory Food Factors Committee,\" British Medical Bulletin 12, no. 1 (1956): 7. 88 \"National Flour,\" Lancet 237, no. 6144 (31 May 1941): 703-04. 89 R. A. Murray Scott, \"National Wheatmeal Flour?\" British Medical Journal 1, no. 4229 (24 January 24 1942): 125. 90 \"What's in the National Wheatmeal Loaf?\" Lancet 238, no. 6168 (15 November 1941): 605. 301 claimed were behind the effort to produce National wheatmeal not up to the original specifications laid out by the MRC. \"In this conflict between the national and individual interests,\" the editor lamented, \"the millers have won the day, and a golden opportunity for furthering the health of the people has been lost.\" They hoped, however, that \"a good loaf, made to a strict specification and supported by wise propaganda, might still achieve victory.\"91 In a letter to the British Medical Journal, S. W. Swindells suspected that the milling industry might have been behind the Ministry of Food's puzzling failure to adopt the MRC's clear specifications for National wheatmeal .92 In the Lancet, Sir Ernest Graham-Little likewise suggested that the medical profession should \"lead the revolt\" against the milling industry, \"with its formidable occupation of key positions in the Ministry of Food.\"93 In early 1942, Graham-Little and a large portion of the medical members of Parliamen t sponsored a resolution calling for a select committee to find out whether commercial interests were behind the obstruction of the exclusive provision of a National wheatmeal loaf.94 Mean while, the scientific debate over the value of white versus brown bread continued unabated. One of the major points of contention concerned the possible reduction in the supply of milling offals for feeding livestock that would result from a 10-15 percent increas e in the flour extraction rate. By sharply reducing milk supplies, the argument went, there would be a loss in the British diet of important nutrients such as high-quality protein, riboflavin, and calcium that would outweigh the gains in thiamin, iron, and niacin obtained from 85 percent extraction flour. This thesis was first presented in detail in 1941 by Norman 91 \"What's in the National Wheatmeal Loaf?\" 605. 92 S. W. Swindells, \"National Wheatmeal Flour?\" Medical Journal 2, 1941): 822-23. 93 E. Graham-Little, \"What's in Loaf?\" no. 6171 (6 December 1941): 714. 94 \"On the Floor of the House,\" Lancet 239, no. 6180 (7 February 1942): 182. 302 Wright, Director of the Hannah Dairy Research Institute, and corroborated with modifications by Albert L. Bach arach of Glaxo Laboratories, a large pharmaceutical company. D. W. Kent-Jones and A. J. Amos similarly calculated the gains and losses that would result from a change in the extraction rate from 70 to 85 percent and concluded that \"the case for national wheatmeal is finally and definitely exploded on sound scientific grounds.\"95 But ultimately, the \"animal feed\" argument against higher-extraction wheatmeal did not carry much weight. Gottfried Fraenkel of the Imperial College, London questioned Wright's calculations on the grounds that he had confused shipping space with nutritional values. The premise of the case against National wheatmeal was that if 6 million tons of wheat were milled to 75 percen t extraction, 4.5 million tons of white flour would be obtained, plus 1.5 million tons of wheat feed (\"wheat offals\" or \"wheat middlings\") for livestock. By milling to 85 percent, 5.1 million tons of wheatmeal flour would be obtained and only 900,000 tons of wheat feed. Fraenkel pointed out that for proper consideration, only 4.5 million tons of 85 percen t wheatmeal flour should be milled from 6 million tons of wheat, leaving 1.5 million tons of wheat feed \"offals\" and unmilled whole wheat combined to be fed to livestock. In this way, Fraenkel maintained, the same amount of milk would be obtained regardless of the flour produced and no additional shipping space would be needed.96 Wright's study was also criticized for ignoring the increasing quantities of homegrown oats, \"dredge corn,\" and other cereals not intended for use in bread that had 95 N. C. Wright, \"Wheatmeal Bread or Milk?\" Chemistry and Industry 60 (1941): 623; D. W. Kent-Jones and A. J. Amos, \"The Milling Aspects of Fortified Flour,\" Lancet 237, no. 6145 (7 June 1941): 731 96 G. Fraenkel, \"White Bread and Brown Bread,\" Chemistry and Industry 61 (1942): 84. 303 been grown since the war began as part of the massive \"ploughing-up camp aign.\"97 Moreover, as H. D. Kay of the University of Reading's National Institute for Research in Dairying pointed out, \"that the dairy cow is largely dependent in either peace or war on wheat middlings is incorrect. To regard them as an ideal concentrate for milk production is a flight of fancy.\" He pointed out that since the bacteria in the cow's rumen were capable of synthesizing B vitamins, it was only the monogastric chicken and the pig that substantially benefited from the B vitamins contained in milling offals. Kay advised that the extra shipping space made available by an increase in the extraction rate to 85 percent could be used to import more nutritionally valuable concentrated animal feeds. 98 A related argument against National wheatmeal , which originated well before World War II, was that its protein and total calories were less digestible than those in 70-75 percent extraction flour, due to its higher fiber content. In their memoranda, the members of the MRC's Accessory Food Factors Committee considered this argument, alongside concerns about popular acceptance, in their recommendation for 80-85 percent extraction \"wheatmeal\" flour rather than 90-100 percent extraction \"wholemeal\" flour. But there were critics, such as Norman Wright, who felt that the extraction rate in the MRC recommendation was still too high. Wright argued that the losses from decreased digestibility would balance the additional flour gained from an 85 percen t extraction, and that nothing would be gained by the change to 85 percent wheatmeal from straight-run flour.99 Owing mostly to difficulties in developing reliable methods for determining \"digestibility,\" the issue remained unsettled in the early years of the war. But several 97 \"Wheatmeal Bread or Milk?\" Lancet 238, no. 6161 (27 September 27 1941): 373. 98 H. D. Kay, \"The Value of Wheat Offals for Milk Production,\" Proceedings of the Nutrition Society 4 (1946): 36. 99 N. C. Wright, \"Wheatmeal Bread or Milk?\" Chemistry and Industry 60 (1941): 623. 304 digestibility studies published in 1942 and 1943 shifted the balance of evidence in favor of high-extraction flour. Hans Krebs and Kenneth Mellanby conducted dietary experiments on six adult male volunteers and found that the difference in the digestibility of white and wheatmeal bread was much smaller than Wright had assumed.100 Wright, citing the handful of other digestibility experiments that had been conducted earlier, contested the results of the Krebs-Mellanby study.101 Wright's paper, in turn, drew an angry reply from Fraenkel, who pointed out that \"even if Dr. Wright's claims on digestibility of National Wheatmeal were right, which seems doubtful,\" the loss of protein would be almost negligible, and there would be no loss of calories.102 Harriette Chick made an important contribution to the digestibility debate in April 1942, when she published her investigation of the biological value of the proteins of 100 percen t extraction wholemeal flour, 85 percent extraction National wheatmeal flour, and 73-75 percen t extraction white flour. Using the growth method with rats, she observed that the proteins of 85 percent extraction flour showed an advantage in growth of 13 to 16 percent over the proteins of white flour, while wholemeal flour showed an advantage of 17 to 26 percen t. The utilization (digestibility) of wholemeal was less than white flour by six percent and national wheatmeal by three percent. Chick therefore concluded that the superior biological value of the proteins of the germ and outer layers of the wheat grain more than offset the greater digestibility of the proteins of white flour.103 Chick's earlier experiment comparing the nutritional value of National wheatmeal and thiamin-enriched white flour was also attacked and confirmed. Margaret Wright 100 H. A. Krebs and Kenneth Mellanby, \"Digestibility of National Wheatmeal,\" Lancet 239, No. 6185 (14 March 1942): 319-321. 101 N. C. Wright, \"Digestibility of National Wheatmeal,\" Lancet 240, No. 6206 (8 August 1942): 165. 102 G. Fraenkel, \"Digestibility of Wheatmeal,\" Lancet 240, 6208 (22 August 1942): 230-231. 103 Harriette Chick, \"Biological Value of the Proteins Contained in Wheat Flours,\" Lancet 239, no. 6188 (4 April 1942): 405-07. 305 critiqued Chick's work because the food intakes of the rats receiving the reinforced white flour and National wheatmeal were not equalized. Wright argued that the superior growth and wellbeing of the group of rats eating wheatmeal was due to the fact that the wheatmeal was consumed in greater quantity than the white flour. In her experiment, Wright fed the same amount of flour to both groups of rats, and under these conditions she found little differen ce in growth between the two groups. She repeated the experiment with the two flours along with other foods, such as potatoes, meat, milk, vegetables, margarine, and jam, so that the total would represent the ordinary mixed British diet under wartime rationing. On the mixed diet containing National wheatmeal the rats limited to the same food intake voluntarily consumed by the rats on the white flour fortified with thiamin was only slightly superior, to the point of statistical insignificance.104 But Chick's argument was helped by the rat-feeding experiment carried out at the University of Illinois, discussed above, which used diets consisting of whole-wheat bread, enriched white bread, enriched white bread with nonfat milk solids, and whole-wheat with nonfat milk solids.105 In any case, the extraction rate controversy was ended on March 11, 1942, when Lord Woolton issued an order mandating that all bread on sale be National wheatmeal or authorized specialty \"brown breads.\" The Ministry of Food decided that the tightening of shipping space due to the extension of the British war effort to the Far East was sufficient to tip the scales in favor of the exclusive provision of National wheatmeal. Under the government order, the sale of white bread was not permissible except under license. In addition, all biscuits, cakes, and flour confectionary had to be made from flour of not less 104 Margaret D. Wright, \"The Nutritive Value of Bread: Fortified White Flour and National Wheatmeal Compared,\" British Medical Journal 2, no. 4219 (15 November 1941): 689-92. 105 Mitchell, Hamilton, and Shields, \"The Contribution of Non-Fat Milk Solids to the Nutritive Value of Wheat Breads,\" 585-603. But both this and Chick's rat studies did not have much of a long-term impact on the enrichment debate, since they used various kinds of bread as the sole constituent of the diet rather than as part of a mixed diet approximating what the public actually ate. 306 than 85 percent extraction. The addition of calcium carbonate (as creta preparata, or powdered chalk) was also mandated for National wheatmeal , to increase calcium intake and counteract any immobilizing influence on calcium of phytic acid, which studies carried out before and during the war had suggested was present in significant amounts in higher-extraction flours. Because National wheatmeal contained only about one-fifth less thiamin than \"wholemeal\" of 90-100 percent extraction, enrichment with the vitamin was deemed unnecessary. 106 Owing to problems in obtaining adequate supplies of thiamine, fortification of white flour with the vitamin had not even been completely accomplished by the spring of 1942, so its beneficial effect on the population was still only hypothetical when the flour and bread policy was changed. Nutritionists and medical workers greeted Lord Woolton's announcement with relief and approval, even if some among them grumbled that the 85 percent extraction rate of National wheatmeal was unnecessarily low.107 A. L. Bacharach happily reported that the adoption of National wheatmeal flour was \"the best of both worlds,\" since the loss of vitamins was substantially less than in white flour, while palatability and popular acceptance remained high. \"There is now a convergence of opinion, a British compromise, if you will,\" Bach arach stated, \"by the acceptance of flour with extraction somewhere in the range of 80 to 90 per cent.\"108 Harriette Chick and her team at the Lister Institute similarly concluded that the Ministry of Food's decision to mandate National wheatmeal \"offered a compromise\" that was generally acceptable to all the parties responsible for the nation's bread.109 Oxford 106 \"National Loaf,\" Lancet 239, no. 6186 (21 March 1942): 357; and \"On the Floor of the House,\" Lancet 239, no. 6186 (21 March 1942): 367. 107 See, e.g., R. A. Murray Scott, \"National Wheatmeal Flour?\" British Medical Journal 1, no. 4229 (24 January 1942): 125. 108 A. L. Bacharach, \"Bread as Human Food,\" Proceedings of the Nutrition Society 4 (1946): 26. 109 Harriette Chick and A. M. Copping, \"Nutritive Value of Wheat Flours of Different Extraction-Rate,\" Lancet 247, no. 6389 (9 February 1946): 197. 307 biochemist Sir Rudolph Peters approvingly referred to 85 percen t extraction flour as \"a very beautiful and wonderfully chosen foodstuff.\"110 National wheatmeal became a major component of a wartime diet that famously improved the health of Britain's population despite food shortages and the physical and psychological stresses of war. World War II proved to be a great opportunity for British nutrition experts to act on long-held ambitions to become involved in the formulation and implemen tation of a scientific food policy. Partly as a result of the advice of nutritionists, the Ministry of Food reorganized imports to prioritize foods, such as cheese, skimmed dried milk, dehydrated eggs, and canned meat and fish, that provided the most nutritional value for the least amount of shipping space. Under rationing, the consumption of meat, eggs, sugar, and imported fruits declined. But potatoes, carrots, oatmeal , and wheatmeal bread remained plentiful in all but the darkest moments of the war. At the urging of nutritionists, the supply of milk and milk products was increased, and margarine was fortified with imported concentrates of vitamins A and D. Moreover, the Ministry of Food established a rationing policy that assured a more even distribution of the protective foods to the poorest and most vulnerable groups in society. Particularly gratifying to nutritionists was the fact that expectant or nursing mothers received extra rations of milk and eggs; babies and small children were given cod liver oil and orange juice, or blackcurrant and rosehip syrup as an alternative source of vitamin C. Under these wartime rationing meas ures, the incidence of ailments such as dental decay, diabetes, and iron-deficiency anemia fell. In addition, the disparities in health and nutritional status between social classes identified by nutrition experts during the 1930s were reduced. 110 Sir Rudolph Peters, \"Bread as Human Food,\" Proceedings of the Nutrition Society 4 (1946): 1. 308 Jack Drummo nd shared with many of his fellow nutrition scientists the hope that the wartime experience would build in the mind of the public a preference for breads made from higher-extraction flours. Speculation and anecdotal evidence was both supportive and dismissive of this hope. Some commentators, particularly those connected to the milling and baking industries, were confident that the British public would overwhelmingly turn back to white bread as soon as they were permitted.111 The widespread popular preference for soft, smooth-textured white bread over dense, branny whole-wheat bread was also one of the guiding assumptions behind the development of the National wheatmeal by the MRC and the Ministry of Food. After the fiasco in 1941 surrounding the Ministry of Food's milling specifications for National wheatmeal , standards were tightened in early 1942 to more closely follow the MRC's recommendations. In June 1942, the MRC further advised that any bran particles in the flour should not have a cross-section exceeding 0.2 millimeters. Acting on this advice, the Ministry of Food encouraged the millers to reduce the bran content of National wheatmeal. The result was that the bran content dropped from about six to four percent, which consequently lightened the color and improved the baking quality of the flour.112 Some reports suggested that British habits in bread could change. One study carried out in several Royal Air Force cafeterias showed that there was no overwhelming prejudice against National bread when it was served alongside ordinary white bread. The airmen under observation seemed to respond more to such factors as the manner of presentation and the quantity of margarine spread on the bread, rather than the type of bread itself.113 Other 111 A. M. Maiden, \"'White' Bread,\" Nature 151, no. 3829 (20 March 1943): 336. 112 T. Moran and Sir Jack Drummond, \"Scientific Basis of 80% Extraction Flour,\" Lancet 245, no. 6353 (2 June 1945): 698. 113 T. F. Yudkin, 85% National Wheatmeal Bread,\" Lancet 247, no. 6389 (9 February 1946): 214. 309 commentators felt that the popular unwillingness to eat darker breads was a supposition repeated without statistical evidence. Oliver Stewart, for example, argued, \"Before we go any farther in this matter, that alleged preference for paleness, it seems to me, should be scientifically demonstrated.\"114 But the public preference for white bread was, for the most part, accepted as a given. This assumption guided the government's decision to lower the extraction rate toward the end of the war when the wheat supply improved. In October 1944, the Ministry of Food to reduce the extraction rate without public notice to 82\u00bd percent, and then to 80 percent in December 1944. The government defended its decision on the grounds that new milling techniques worked out at the Cereals Research Station at St. Albans had made it possible to reduce the percentage of extraction without substantially lowering the vitamin content of the resulting flour.115 Despite these assurances, politicians publicly attacked the change. A debate in the House of Lords in February 1945 showed very little support for the Ministry of Food's decision to reduce the extraction rate. One of the most outspoken critics of the ministry in the debate was Lord Horder, a physician who had defended the consumption of white bread in the 1920s before reversing his opinion on the grounds that scientific knowledge of nutrition was still too uncertain to warrant tinkering with natural foods.116 Horder claimed that he and his colleagues had witnessed a decrease in the incidence of stomach ulcers, anemia, and constipation after the adoption of 85 percent extraction flour in 1942. Nutritionists were also concerned by the government's decision. A. L. Bach arach observed that the reduction of the extraction rate from 85 to 80 percen t led to a 22 percent reduction in 114 Oliver Stewart, \"Flour of Good Colour,\" Lancet 245, no. 6354 (9 June 1945): 740. 115 McCance and Widdowson, Breads White and Brown, 112. 116 Lord Horder, \"Cautious Food Reform,\" Lancet 244, no. 6306 (8 July 1944): 53; McCance and Widdowson, Breads White and Brown, 98. 310 the available iron\u2014a much-discussed nutrient, given the high prevalence of anemia among young women at the time. Similarly, Alice Copping of the Lister Institute reported in 1945 that the riboflavin level in National bread had decreased to about two-thirds of its 1942 value when the extraction rate was changed from 85 to 82\u00bd percen t at the end of 1944. The British Medical Journal sided against the government decision as well. \"Unfortunately it looks as if the Ministry of Food, the Ministry of Health, and the millers have made an uneasy compromise,\" read an editorial, \"and have lamentably failed to seize this opportunity of restoring to the people of this country its former healthy taste for whole-meal flour.\"117 Notwithstanding these criticisms, the mandated minimum extraction rate remai ned at 80 percen t in the final months of the war. As the war drew to a close, the conflict between medical and nutritional authorities and the milling and baking industries came to a head. In January 1945, the Ministry of Food convened a conference to advise on postwar bread and flour policy, and in particular on any regulations that might have to be made when wartime controls ended. The Conference on the Post-War Loaf, as it was called, was attended by representatives from the Ministry of Health, the Ministry of Agriculture, the Department of Health for Scotland, the Medical Research Council, the milling and baking industries, and flour importers. The conference attendees were unanimous in recommending that flour should not be allowed to fall below specified levels of three \"token\" nutrients: 0.24 mg thiamin, 1.6 mg niacin, and 1.65 mg iron per 100g of flour. But that is where agreement ended. The representatives of the milling and baking industries were convinced that the public preferred white bread to any alternative. They granted that the specified levels of token nutrients ought to be attained, but they favored 117 \"The Political Loaf,\" British Medical Journal 1, no. 4393 (17 March 1945): 373-74. 311 enrichment of white flour with vitamins and minerals as a mean s to that end. Moreover, they were not hopeful that the flour-exporting countries, particularly Canada and Australia, would agree to the enforcement of a high extraction rate for imported flour, in view of the disturbance that would result in their milling industry and export trade. British manufacturers of pre-made cakes and biscuits claimed that they would not be able to compete in overseas mark ets after the war if they were not allowed to use low-extraction white flour to make their products. The official scientific and medical representatives, on the other hand, considered the retention of the natural constituents of the wheat grain to be \"so incomparably preferable to reinforcement,\" as they phrased it, that they opposed any policy of enriching low-extraction flour. A basic point of contention between nutrition experts and the representatives of industry was over the implications of the term \"token nutrient.\" The industrial representatives maintained that there was satisfactory evidence of widespread deficiencies of only the three \"token\" nutrients. The scientific and medical representatives, on the other hand, took a position similar to that of the whole-wheat proponents in the American enrichment controversy. They argued that an adequate intake of the three token nutrients was merely an indicator that the typical British diet was probably sufficient in \"non-token\" and other possibly unknown nutrients as well: The three token nutrients for which a prescribed minimum is proposed have been accep ted because they are those with which it is practicable to deal by of way of legal sanctions. These vitamins, however, are only elements in an organic complex which includes other substances some of which are known to be physiologically active, though knowledge of them is still imperfect.... There is a strong prima facie case for thinking that the vitamins in the wheat berry exist in the form of a balance which has been proved to be conducive to health, and therefo re much caution is 312 called for before starting upon a new line of action which might upset the balance with deleterious effects. 118 The cautious approach that nutrition experts took at the conference was reinforced by evidence that was just then emerging of the existence of other vitamins in the B complex. In a talk given shortly after the conference, Rudolph Peters repeated the \"argument from ignorance\" against enrichment: \"You have to bear in mind that if you once start on a very thorough programme of enrichment it must be thorough and it is impossible to stop. We have three or four vitamins, minerals and so forth, now recognized as necessary but we already know of many more, at least three, which also will have to be considered soon.\" Enrichment of refined food, Peters warned, \"is indeed a slippery slope upon which to step.\"119 However, the extraction rate debate did not require resolution immed iately after the war. Food remained in very short supply and continued to be rationed. Much of the agriculture and infrastructure of the liberated countries lay in ruins after years of conflict and occupation, and millions were on the brink of starvation. The shortage in Britain was accen tuated on account of the fact that in 1945 the government lent 200,000 tons of the cereals being imported into the country to the \"semi-starving\" countries of Europe, and also supplied 400,000 tons to the British Zone of Occupation in Germany. Wheat was so scarce, in fact, that the extraction rate was raised by government order to 90 percent for four months in 1946, before being lowered to 85 percent until August 1950, when it was again lowered to 80 percen t.120 118 Ministry of Food, Report of the Conference on the Post-War Loaf (London: HMSO, 1945), 116-17. 119 R. A. Peters, \"Bread As Human Food,\" Proceedings of the Nutrition Society 4 (1946): 51; McCance and Widdowson, Breads White and Brown, 117. 120 R. A. Morton, \"The Report of the Panel on Flour,\" Proceedings of the Nutrition Society 17 (1958): 21. 313 But as economic recovery took place and trade in agricultural goods renormalized, the government's rationale for controlling the flour extraction rate began to weaken. Even though the scientific arguments in favor of higher-extraction flours had not been undermined in any substantial way, the restriction of consumer choice by government fiat in a presumably democratic country could not be so easily defended.121 Moreover, the American example of selective restoration of vitamins and minerals\u2014the path initially explored, albeit abortively, in Britain as well\u2014was available to follow. By the end of the 1940s, the technical barriers to the mass-production and incorporation of the \"token\" nutrients into flour and bread had been well overcome. Unsurprisingly, therefore, the extraction rate was decontrolled with the end of rationing in 1953. But the controversy on reinforcement persisted, and the government settled on a new compromise. They decided to permit the production of flour of an extraction rate below 80 percent, but with the requirement that the three \"token\" nutrients be added so that the minimum levels specified at the Conference on the Post-War Loaf would be met. But in deference to the views of their own nutritional advisers, the government subsidized only bread made of National flours of not less than 80 percent extraction. This new policy, as expressed in the Flour Order (1953) and the Bread Order (1953) gave the consumer the choice between National bread at 7\u00bd d. a 1\u00be lb. loaf and bread made from whiter flour enriched with token nutrients costing from 10\u00bd d. to 1 s. for a loaf of the same weight. Despite this relatively small price difference, the majority of British consumers 121 Harriette Chick, A. M. Copping, and E. B. Slack, \"Nutritive Values of Wheat Flours of Different Extraction-Rate,\" Lancet 247, no. 6389 (9 February 1946): 196-99. 314 continued buying the National loaf. The popular preference for white bread, it seemed, was not completely immutable.122 Up to the mid-1950s, therefore, the controversy over how to improve the nutritional quality of bread was settled differently in the United States and Britain. But in both cases, a fundamen tal source of disagreement between disputants was over how the rapidly growing chemical knowledge of vitamins was to be applied to the food supply. With the advent of cheap mass-produced synthetic vitamins in the late 1930s, it became possible to accommo date consumer tastes and preexisting food industry practices while addressing widespread deficiencies. Yet following this course of action would violate one of the principal lessons that many nutritionists had learned from the discovery of vitamins: to consume foods in a fresh and natural state, with as little processing and refining taking place between producer and consumer as possible. Before the emerg ence of the newer knowledge of nutrition, scientists enthralled by chemical analysis had presented the public an oversimplified view of what comprised an adequate diet, and many nutrition experts feared repeating the same mistake. But what united most of the proponents and detractors of enrichment was their shared belief that the public would benefit, in the long run, from consuming more of the protective foods in their natural form. However, as the next chapter will explain, consensus on even this point began falling apart in the two postwar decades. 122 McCance and Widdowson, Breads White and Brown, 129. 315 CHAPTER FIVE FOOD FADDISM, FAT FEARS, AND THE DECLINE OF CONSERVATIVE NUTRITION AFTER WORLD WAR II In 1956, a paper by Surgeon Captain Thomas Latimore \"Peter\" Cleave entitled \"The Neglect of Natural Principles in Current Medical Practice\" appeared in the Journal of the Royal Naval Medical Service. A fourth-generation Navy man, Cleave had spent his career practicing in Royal Naval Hospitals in Britain as well as Malta and Hong Kong, before becoming the Director of Medical Research at the Royal Navy Medical School in 1952 until his retirement a decade later. It was while serving on the battleship King George V during World War II that he earned his Naval nickname, \"the bran man,\" after he had sacks of bran brought aboard to combat the constipation that was prevalent in the sailors.1 The hypothesis that Cleave presented in \"The Neglect of Natural Principles in Current Medical Pract ice\" had been in gestation since the early years of his medical career in the 1920s and 1930s, when he began thinking of human illness in terms of Darwinian evolution.2 He classified diseases into two basic categories: the \"natural\" and \"unnatural.\" The natural group of diseases, according to Cleave, consisted of those of an infectious nature that are spontaneously communicable, such as malaria and typhus. He noted that modern science had been very successful in greatly diminishing the prevalence of these diseases in the industrialized West. The unnatural group of diseases consisted of all the other known 1 Biographical details from T. L. Cleave Papers, Wellcome Library Archives and Manuscripts, London, UK; Kenneth Heaton, \"Surgeon Captain T L Cleave FRCP,\" in \"Founders of Modern Nutrition,\" available from http://www.mccarrisonsociety.org.uk/founders-of-nutrition-othermenu-149/cleave-othermenu-144, accessed 25 August 2012. 2 One of the earliest articulations of his hypothesis was a self-published paper, \"A Plea for a Dietetic Test in Diseases of Unknown Primary Causation, London, October 1936,\" PP/TLC/F1/1, Wellcome Library Archives and Manuscripts, London, UK. 316 maladies of the human race that were virtually absent in nature, but occurred in humanity, and also in animals living in a civilized or partially civilized environment. Cleave argued that all the degenerative chronic diseases of Western societies\u2014heart disease, obesity, diabetes, peptic ulcer, and a variety of digestive ailments\u2014should be considered as belonging to the unnatural group. Darwin's theory of evolution led Cleave to believe that these chronic diseases must be caused by a relatively rapid change in the modern environment to which we had not yet adapted, since they appeared to be exceedingly rare in wild animals and primitive human societies. The most likely culprit in the rising rates of unnatural diseases, he concluded, was the consumption of refined carbohydrates, particularly sugar, and to a lesser degree white flour and other highly milled grains. \"Whereas cooking has been going in the human race for probably 200,000 years,\" Cleave wrote, \"there is no question yet of our being adapted to the concentration of carbohydrates by machinery. Such processes have been in existence little more than a century for the ordinary man and from an evolutionary point of view this counts as nothing at all.\"3 Cleave saw his hypothesis as an extension of the earlier critiques of refined foods formulated by nutrition researchers such as Weston Price and Robert McCarri son.4 But whereas these individuals framed their comparisons of \"primitive\" and \"modern\" diets mainly in terms of vitamins and minerals, Cleave focused on the different forms of carbohydrates. He contended that the concentration of carbohydrates through refining processes\u2014the production of white sugar from sugarcane and beets in particular\u2014led to \"a definitely excessive consumption,\" which resulted in obesity and the degenerative diseases often associated with it, such as heart disease and diabetes. Cleave regarded the argument 3 T. L. Cleave, \"The Neglect of Natural Principles in Current Medical Practice,\" Journal of the Royal Naval Medical Service 42, no. 2 (Spring 1956): 58. 4 Ibid., 60, 64. 317 that obesity was simply due to an excess of appetite as a \"tragic error,\" since animals in the wild never become overweight. \"We have only to notice,\" he wrote, \"that no rabbit ever ate too much grass, no rook ever pulled up too many worms, no herring ever caught too much plankton.\" The excessive consumption of refined carbohydrates, on the other hand, \"comes about because their taste is too highly geared , as it were, for the tongue to be able to stop\"\u2014that is, the concentrated carbohydrates deceive our appetite, which over the long process of evolution adapted only to less dense, fibrous, unrefined carbohydrates. An ordinary bar of chocolate, Cleave pointed out, contained almost as much sucrose as a dozen average apples. \"The tongue would know when to stop eating the apples, but not how far along the bar of chocolate,\" as he put it.5 Cleave also pointed to the historical association between the growth of refined sugar consumption in Britain and the rising incidence of what were known as the \"diseases of civilization,\" such as diabetes and heart disease. Cleave therefore represents a transitional figure in the history of nutrition science, between the golden era of vitamin research, which ran roughly from 1915 to 1945, and the three decades following World War II. The latter period was characterized by increasing concern not over vitamin and mineral deficiencies, but over the relationship between diet and the chronic degenerative conditions such as heart disease that were taking an increas ingly heavy toll in the West. Yet Cleave's dietary recommendations for avoiding these diseases were remarkably similar to those that nutritionists had developed in the golden era of vitamin research. He advised people to consume liberal quantities of meat, fish, eggs, milk and milk products, fruits and vegetables, and wholegrain breads and cereals; and minimize consumption of foods containing white flour and sugar. He stressed that eating foods mostly in an unrefined, natural state should entail very little sacrifice. \"It is very 5 Ibid., 58, 63. 318 unfortunate,\" Cleave ruefully noted, \"that any form of natural diet is usually taken to mean living on nuts, turnip juice, etc., whereas really it should mean as big a range of beef-steaks, turkey, salmon, etc. as can be afforded.\"6 Cleave was certainly not alone among nutritionists and physicians in the postwar period in stressing the unique healthfulness of a diet consisting of unprocessed, natural foods. He was, however, in an increasingly embattled minority. The two decades following World War II were characterized by a growing sense of complacency on the part of health authorities regarding the adequacy of the modern diet. After reaching a peak in the late 1930s and early 1940s, expert claims for the existence of widespread \"borderline\" or \"subclinical\" deficiency states due to inadequate vitamin and mineral intake diminished with remarkable rapidity. The unprecedented economic prosperity that Western countries experienced in the postwar period encouraged the rise of this less bleak view of the nutritional status of the population. The food system, like other areas of the economy, underwent a drastic industrial transformation during that time. This development resulted\u2014much to the satisfaction of most health authorities\u2014in cheaper and more abundant food supplies, including some of the so-called \"protective foods.\" The nutritionists and physicians who questioned this emerging optimistic view and continued to press for drastic dietary reform, Cleave among them, struggled to maintain their professional respectability. Moreover, when this sense of complacency started to crumble in the 1960s, it was not due to a revival of the older claims about the healthfulness of a diet rich in vitamins and minerals, but mainly to the fact that the \"fat-cholesterol hypothesis\" was gaining increasing 6 Ibid., 69. 319 favor in the scientific and medical communities.7 According to this hypothesis, it was the excessive consumption of saturated fat, and in some versions dietary cholesterol, that was at fault for the rising rates of heart disease and related ailments in the affluent Western countries. The remedy, the proponents of the fat-cholesterol hypothesis argued, was for populations of these countries to reduce their consumption of some of the very same foods previously deemed \"protective\"\u2014whole milk, cheese, butter, meat, and eggs\u2014and replace them with starchy carbohydrates and vegetable oils rich in supposedly healthier polyunsaturated fats. The shift in mainstream opinion in favor of the fat-cholesterol hypothesis was sudden and, given the numero us flaws that researchers identified in it while it was still being debated, quite unsettling. Cleave was one of the many skeptics of this hypothesis\u2014and, in terms of his standing in the field of nutrition, a marg inal one at that. Nevertheless, between the 1960s and 1980s, many leading health organizations and government institutions began putting their weight behind the fat-cholesterol hypothesis, and the contrary opinions of those like Cleave were relegated to the unorthodox fringe. These changes in the food system and in scientific opinion\u2014for the most part unforeseeable and unanticipated in the interwar period\u2014worked together to marginalize the conservative nutrition perspective, just at the moment when it appeared to be coalescing into a cohesive and cross-disciplinary body of thought. Even though evidence continued to 7 The term \"fat-cholesterol hypothesis\" comes originally from Taubes, Good Calories, Bad Calories. This hypothesis has also somewhat confusingly been called the \"lipid hypothesis\" and the \"diet-heart hypothesis.\" The lipid hypothesis proposes that blood lipid accumulation in the arterial wall causes atherosclerosis. According to this hypothesis, a connection exists between cholesterol levels in the blood the development of coronary heart disease. The diet-heart hypothesis proposes a connection between certain components of the diet and the development of coronary heart disease. Although the two hypotheses are not mutually exclusive, they are not identical. I use the term \"fat-cholesterol hypothesis\" to refer to a particular combination of these two hypotheses that emerged in the 1950s. The proponents of the fat-cholesterol hypothesis claimed that dietary fat contributed to heart disease by raising blood cholesterol levels. Slightly later, they shifted the blame to saturated fat, which is found mostly in animal products and tropical vegetable oils derived mainly from palm and coconut. 320 accumulate in favor of this perspective in the latter half of the twentieth century, it nevertheless failed to garner strong institutional backing and popular appeal, and declined into relative obscurity. The Rise of Complacency: Nutrition Science and the Industrial Diet in the Postwar Period World War II marked a high point in concern among American and British nutrition experts about vitamin and mineral deficiency. The discovery of widespread malnutrition in the late 1930s and early 1940s, described in earlier chapters, occurred just as both countries were being drawn into the conflict. From the outset, war planners recognized the need to mobilize the entire adult population: young men had to be recruited and turned into soldiers; workers had to be hired and trained for new jobs in the growing defense industries; farm families had to maintain and even increase their production of crops with fewer able-bodied laborers; and women, many of whom were to find work outside the home, had to learn how to help their families survive the stresses and deprivations of the war. The disturbing news that malnutrition was widespread in the United States and Britain\u2014particularly among the lower income groups, on whom would fall the brunt of the war effort\u2014thus caused a great deal of concern. Government authorities in both countries were particularly distressed by the high percentage of men rejected from the armed services because of physical under-development and other defects usually considered as due to malnourishment. World War II proved to be a great opportunity for nutrition enthusiasts in the scientific and medical communities to realize long-held ambitions to become involved in the formulation of food policy. In Britain, senior nutrition scientists such as John Boyd Orr and Edward Mellanby played an important advisory role in agricultural and food planning. 321 Appointed to head the food-advice division of the Ministry of Food at the beginning of the war, biochemist J. C. Drummond led a massive and much-lauded effort to educate the public in how to cook and eat for health.8 Nutrition experts played a less active role in determining agricultural output and rationing policy in the United States, but even before the country became an official belligerent in December 1941, the federal government had begun organizing a nationwide educational campaign under the scientific guidance of Committee on Food and Nutrition (which was later renamed the Food and Nutrition Board) of the National Research Council. Physicians who were pioneers in using nutritional therapy, such as James McLester and Russell Wilder, sat on the Food and Nutrition Board (FNB) during the war and helped to publicize the idea that mild forms of vitamin and mineral deficiency were a major public health threat. \"All the evidence from numerous surveys over the past ten years to the present among persons of all ages in many localities is without exception in complete agreement that inadequate diets are widespread,\" the FNB members concluded in a 1943 report.9 During the war years, nutrition experts also continued to accumulate evidence in support of the notion that dietary deficiencies of a mild to moderate degree were widespread in North America and Britain. In 1942, Harry D. Kruse, a medical researcher at Johns Hopkins University who had pioneered the use of the biomicroscope to detect early and minor forms of several vitamin deficiencies, published a much-cited paper in which he drew 8 Levenstein, Paradox of Plenty, ch. 5 passim; Ackerman, \"Interpreting the 'Newer Knowledge of Nutrition',\" 377-94; Griggs, The Food Factor, ch. 15 passim; David F. Smith, \"Nutrition Science and the Two World Wars,\" in Nutrition in Britain: Science, Scientists and Politics in the Twentieth Century (Routledge: London and New York, 1997), 154-61. 9 Norman Jolliffe, James S. McLester, and H. C. Sherman, \"The Prevalence of Malnutrition,\" Journal of the American Medical Association 118, no. 12 (21 March 1942): 944-50; NRC Food and Nutrition Board, Committee on Diagnosis and Pathology of Nutritional Deficiencies, Inadequate Diets and Nutritional Deficiencies in the United States: Their Prevalence and Significance, NRC Bull. No. 109 (Washington: NRC, 1943): 46. See also Levenstein, Paradox of Plenty, 64-79. 322 a distinction between chronic and acute deficiency states. In the paper, Kruse pointed out that the pathologic processes produced by nutritional deficiencies progressed at varying rates according to the velocity of onset and the degree of deficiency. He divided these processes into four types: acute severe, chronic severe, acute mild, and chronic mild. Acute deficiencies, Kruse explained, were brought on by a sudden change from a good diet to a very bad one, and they produced immediate symptoms. Chronic deficiency diseases, on the other hand, developed slowly following the long-term consumption of a moderately inadequate diet. According to Kruse, the physical effects caused by these chronic diseases often took years to manifest themselves, and therefore mainly affected older people. He used the terms \"severe\" or \"mild\" to describe the extent to which nutrient intake deviated from the actual requirement. The severe forms of nutrient deficiency produced the gross anatomical lesions that physicians had no problem recognizing, whereas mild forms caused subtle tissue changes that could only be observed with a biomicroscope. Kruse also found that the effectiveness of nutritional therapy depended on the state of the disease. Acute forms, such as were produced in clinical trials to determine minimum vitamin requirements, responded rapidly to treatment, whereas chronic forms responded slowly. The gross lesions produced by severe deficiency states disappeared quickly, but they left behind microscopic tissue changes that required a long period of treatment before they healed. Kruse concluded from his research that mild chronic forms of deficiency were much more widespread than physicians appreciated. \"Very few persons,\" he contended, \"have consistently followed throughout life a diet satisfactory in all its essentials, escaped the many other causes of malnutrition, or had complete recovery from any impairmen t of their nutrition.10 10 H. D. Kruse, \"A Concept of the Deficiency States,\" Milbank Memorial Fund Quarterly 20, no. 7 (July 1942): 245-55; H. D. Kruse, \"A Concept of the Etiological Complex of Deficiency States with 323 Around the same time that Kruse's paper on deficiency states appeared, several other researchers reported findings that appeared to confirm the concept of \"optimal nutrition.\" In the early 1940s, Canadian scientists J. H. Ebbs, F. F. Tisdall, and W. A. Scott conducted a blinded dietary intervention trial in a Toronto prenatal clinic on 400 low-income pregnant women. In the study, one group found to be on poor diet was left as a control. A second group on a poor diet was given supplemen tary rations of egg, milk, cheese, tomato, orange, wheat germ, and vitamin D during the last trimester of pregnancy. A third group deemed to have a relatively good diet was given education in the best use of food. Ebbs, Tisdall, and Scott examined the women in the poor diet groups and determined that none of them showed signs of any deficiency diseases, which indicated that their minimum nutritional needs were being met. At the end of the trial, the researchers found that the women whose diets were considered to be good\u2014either because of their customary choices of foods or because of the supplementary foods of high nutritive value\u2014appeared to have better health and fewer complications of pregnancy than did women who customarily had poor diets. In addition, the well-fed groups had healthier babies and appeared better able to nurse them. \"While it is recognized that there are other important factors in the successful outcome of pregnancy,\" Ebbs, Tisdall, and Scott concluded, \"this study suggests that the nutrition of the mothers during the prenatal period influences to a considerable degree the whole course of pregnancy, and, in addition, directly affects the health of the child during the first six months of life.\"11 Soon after the Toronto study was published, several other Especial Consideration of Conditions,\" Milbank Memorial Fund Quarterly 27, no. 1 (January 1949): 5-97. 11 J. H. Ebbs, F. F. Tisdall, and W. A. Scott, \"The Influence of Prenatal Diet on Mother and Child,\" Milbank Memorial Fund Quarterly 20, no. 1 (January 1942): 35-46. 324 research groups working with expectant and nursing mothers in the United States and Britain reported similar positive results from improving marginally adequate diets.12 But in the years following World War II, fears about widespread malnutrition and suboptimal health waned in the United States and Britain. The growing prevalence during the postwar period of conditions often associated with dietary excess, such as heart disease, hypertension, obesity, and diabetes, diminished concern about vitamin and mineral deficiencies, since most nutrition experts at the time could not accep t the notion that diseases associated with overeating could coexist with nutritional deficiencies (of which more below).13 The decline in concern about malnutrition was also partly due to growing scientific skepticism toward the nutritional assessment methods that had been used in the 1930s and 1940s to determine the extent of malnutrition. Even before the end of the war, several research groups began reporting that they could not obtain consistent positive correlations between the results from dietary surveys, clinical examinations for signs of deficiency disease, and biochemical tests of blood and urine. The idea that mild chronic nutrient deficiencies produced pathological signs different from severe acute defiencies, promoted by Kruse and other researchers, proved to be particularly controversial.14 And since little progress was made in developing improved methods for diagnosing mild or 12 See, e.g., People's League of Health, \"Nutrition of Expectant and Nursing Mothers,\" Lancet 240, no. 6201 (4 July 1942): 10-12; M. I. Balfour, \"Supplementary Feeding in Pregnancy,\" Lancet 243, no. 6285 (12 February 1944): 208-11; Bertha S. Burke, \"Nutrition and Its Relationship to the Complications of Pregnancy and Survival in the Infant,\" American Journal of Public Health 35, no. 4 (April 1945): 334-39. 13 See, e.g., USDA, Proceedings of the National Food and Nutrition Institute, USDA Agricultural Handbook No. 56 (Washington: GPO, 1953). 14 See, e.g., Hugh M. Sinclair, \"Wartime Nutrition in England as a Public Health Problem,\" American Journal of Public Health 34, no. 8 (August 1944): 828-32; W. J. Dann and William J. Darby, \"The Appraisal of Nutritional Status (Nutriture) in Humans with Especial Reference to Vitamin Deficiency Diseases,\" Physiological Reviews 25 (1945): 326-46; \"Standards for Interpretation of Dietary Surveys,\" Nutrition Reviews 2, No. 9 (September 1944): 264-66; Agnes Fay Morgan and Lura M. Odland, \"The Nutriture of People,\" in Food: The of Agriculture, 1959 (Washington: GPO, 1959), 186-224. 325 subclinical deficiencies in the two postwar decades, doubts about the nature of the relationship between diet and health persisted. As A. M. Thomson and D. L. Duncan admitted in a 1954 article in Nutrition Abstracts and Reviews, \"Despite the growth of an enormous literature... we are still without diagnostic criteria of specific deficiency states short of rank deficiency disease.\"15 While nutrition experts in the postwar period did not openly reject the idea that mild or subclinical vitamin and mineral deficiencies posed a serious public health problem, they were forced to acknowledge that better evaluation methods were needed before any definite conclusions could be drawn. Some researchers suggested that physicians could best identify these forms of malnutrition by correlating the results of the physical exam with a dietary history and the results of biochemical tests, and seeing if nutritional therapy relieved the symptoms presented by the patient. But this multi-pronged approach was far from perfect, since not all malnutrition was caused by poor diet and not all symptoms responded quickly to therapy.16 In a 1950 article, clinical nutritionist L. B. Pett summed up the methodological difficulties confronting him and his colleagues, writing: \"Malnutrition can not be properly assessed from one sign alone, or from diet alone, or from laboratory tests alone. Even with all these approaches much information is still needed before a satisfactory system of assessment will be evolved.\"17 At the same time that claims for the existence of widespread but mild nutrient deficiencies were coming under question, health authorities also lost interest in the concept 15 A. M. Thompson and D. L. Duncan, \"The Diagnosis of Malnutrition in Man,\" Nutrition Abstracts and Reviews 24, no. 1 (January 1954): 1-18. Cf. Irvin C. Plough and Edwin B. Bridgforth, \"Relations of Clinical and Dietary Findings in Nutrition Surveys,\" Public Health Reports 75 (August 1960): 699-706. 16 NRC Food and Nutrition Board, Committee on Nutrition Surveys, Nutrition Surveys: Their Techniques and Value, NRC Bull. No. 17 (Washington: NAS-NRC, 1949), 47-99. 17 L. B. Pett, \"Signs of Malnutrition in Canada,\" Canadian Medical Journal 63, no. 1 (July 1950): 10. 326 of \"optimal nutrition,\" long championed by nutritionists such as Henry Sherman and John Boyd Orr. As these nutritionists retired or died in the 1950s and 1960s, they were replaced by a new generation of clinical researchers who put much less stock in the animal experiments that held out the promise of optimal nutrition. In addition, studies to test one of the principle claims made by the proponents of optimal nutrition\u2014that consuming vitamins and minerals well above the minimally required quantities increas ed resistance to infection and disease\u2014did not produce consistently positive results. These findings, along with the emergence of highly effective antibiotics, dampened enthusiasm among postwar-era clinical researchers for the use of better nutrition in building resistance to infection. Finally, a number of laboratory experiments and clinical studies carried out in the postwar period showed that whenever animals and people were deprived of food or particular nutrients, their bodies adjusted themselves by reducing their requirement for the deficient nutrients or by making more efficient use of them. Reflecting on this emerging research, Harvard nutritionist Mark Hegsted commented in a 1957 article in Nutrition Reviews: \"The philosophy that all 'nutritional stress,' is bad, or that the body should be kept loaded in case of deficient periods come along, deserves a much closer look. It may be completely wrong.\"18 One series of experiments conducted by two highly respected Cambridge nutritionists, Elsie Widdowson and Robert McCance, illustrates the growing skepticism among nutrition experts in the postwar period toward the notion that refined foods were responsible for widespread borderline deficiencies and suboptimal health. The idea for the Widdowson-McCance study, as it was commonly called, originated in a dispute that arose at 18 H. M. Hegsted, \"Calcium Requirements,\" Nutrition Reviews 15, no. 9 (September 1957): 258. Cf. \"Adequate Versus Optimal Diet,\" Nutrition Reviews 15 (March 1957): 84-85. 327 the Conference on the Post-War Loaf in 1945, mentioned in chapter four, over whether the British government should continue in peacetime to mandate the production of National wheatmeal bread for the sake of public health. The spokesmen of the milling and baking industries at the conference disagreed with the contention made by the scientific and medical representatives that the \"non-token\" nutrients\u2014that is, those not measured in nutritional surveys\u2014were important enough to affect policy on bread. Although the industry spokesmen did not succeed in convincing the government to deregulate the flour extraction rate at that time, it was agreed by both sides that the nutritional value of different breads should be tested on humans. Widdowson and McCan ce consequently conducted a series of feeding experiments under the auspices of the Medical Research Council on undernourished orphanage children in Germany over a twelve-month period in 1947 and 1948.19 The first experiment was carried out at an orphanage in the German town of Duisburg. Widdowson and McCan ce sought to confine the test to the proteins, the B-complex vitamins, and iron, since these were the constituents of wheat likely to be removed by milling. All the children therefore received supplements of calcium and vitamins A, C, and D at the outset of the experiment. The children were divided into five groups, each group being allocated bread made with differen t types of flour: whole wheat, 85 percent extraction wheatmeal , 70 percent extraction, and 70 percent extraction flour enriched with thiamin, niacin, riboflavin, and iron to the concentrations found in whole wheat and 85 percen t extraction flours. In all five groups, the children were allowed to eat as much bread as they desired. Before the trial, the children were examined, weighed, measured, and X- 19 Information for the following paragraphs comes from E. M. Widdowson and R. A. McCance, Studies on the Nutritive Value of Bread and on the Effect of Variations in the Extraction Rate of Flour on the Growth of Undernourished Children, Medical Research Council Special Report, Series 287 (London: HMSO, 1954). An earlier report of Widdowson and McCance's orphanage experiments was published as Studies of Undernutrition, Wuppertal 1946-49, Medical Research Council Special Report, Series 275 (London: HMSO, 1951). 328 rayed, and had their blood examined in various ways. On the basis of all these tests, they were divided into five groups as comparable as possible. The examinations were repeated occasionally during the experiment and used to assess the nutritive value of the breads. The trial lasted about one year, and during this time the children consumed about three-quarters of their calories as bread. The remaining quarter of the diet consisted of the typical German ration of vegetables, soups, potatoes, and very small amounts of milk and animal protein. While the experiment at the Duisburg orphanage was being carried out, Widdowson and McCance realized from chemical analyses of the diets that the children in all five groups were getting sufficient quantities of the B-complex vitamins, owing to the large amounts of vegetables that they were eating. In order to bring the intake of these substances from unenriched white flour as near the margin of inadequacy as possible, Widdowson and McCance undertook another feeding experiment at an orphanage in the German city of Wuppertal. In this experiment, there were only three groups of children: one had their bread made from 100 percent extraction flour, another from 70 percent extraction flour, and the last from 70 percent extraction flour enriched with thiamin, riboflavin, niacin, and iron. But the children in the Wuppertal orphanage were allowed only half as much flour as the children in Duisburg. No limit was set on the children's calorie intake, which was made up of refined fat and sugar in the form of cakes and bread spread thickly with margarine and jam. The Wuppertal experiment also lasted a year. Widdowson and McCance made two general observations from their experiments. First, the children in both trials grew and improved equally well on all the breads. Not only were the enriched 70 percent extraction flours as good as the high-extraction flours they were meant to imitate, but the unenriched 70 percent extraction flour was also equally satisfactory. Second, the growth rate of the orphanage children, who began the experiment 329 under the height and weight considered normal from their age, was one and a half times as great as the average growth rate for well-nourished British and American children. Surprised by these unexpected findings, Widdowson and McCance repeated the much-cited experiments on rats made by Harriette Chick during World War II using diets consisting exclusively of white enriched flour and plain wholemeal flour. Like Chick, they found that wholemeal flour produced better growth and development in the animals. But Widdowson and McCance obtained these results only with weanling rats. When they started with slightly older rats, which have a less rapid growth rate, they found that white and wholemeal flours gave equally good growth. In another experiment, Widdowson and McCance determined that imitations of the orphanage children's diets gave equally good growth in pigs, regardless of the type of flour used in the diet. Unlike the children, however, none of the pigs grew as well on these diets as they did on their ordinary stock diets. Widdowson and McCance reasoned that all of the orphanage diets did not contain a high enough protein-to-calorie ratio for growing rats and pigs, which have substantially faster growth rates than children. Widdowson and McCance drew several conclusions from their work. First, their experiments showed clearly, as they put it, \"how important it is not to generalise too widely from any experiment carried out within limiting conditions.\" The results obtained from experimental animals such as young rats, they maintained, did not necessarily apply to the growing child. The fact that children subsisting mostly on white bread and vegetables did not develop any outright nutritional deficiencies, and indeed seemed to thrive, suggested to Widdowson and McCance that although \"differences in the nutritional values of flours of 330 differen t extraction certainly exist, it is unlikely that they have any practical importance in Britain today.\"20 They also surmised from their experiments that nutrition experts had erred in laying so much emphasis on the value of unrefined flour for all persons under all conditions. \"The mistake was made,\" McCance and Widdowson confessed, \"partly because the scientists, ourselves among them, had allowed themselves to be drawn into a controversy with commerci al interests which was fatal to scientific detachment.\" McCance and Widdowson suspected that the preference for wholegrain bread was part of a nostalgic, almost mystical belief of modern city-dwellers in the lost virtues of their ancestors' rural life. These sorts of assumptions, they stated, \"should have had no place in scientific judgment.\" The orphanage experiments also led them to question the notion\u2014which by the mid-twentieth century had become virtually axiomatic among nutritionists\u2014that the diet had to contain abundant protective foods to ensure sound physical development and health.21 The Widdowson-McCan ce bread study had an impact soon after it was reported in 1954. Aware that they were challenging the established scientific view in Britain, McCance and Widdowson cautioned in the official report\u2014somewhat at odds with what they told audiences under franker circumstances\u2014that the first conclusion to be drawn from their experiments was \"unquestionably that the greatest caution must be exercised in coming to any conclusion at all.\"22 Despite this disclaimer, the Widdowson-McCance report emboldened the milling and baking industries against the critics of white flour. These industries becam e unwilling to accept the basic contention that, in the typical British diet of 20 R. A. McCance and E. M. Widdowson, Breads White and Brown: Their Place in Thought and Social History (London: Pitman Medical Publishing, 1956), xi. 21 R. A. McCance and E. M. Widdowson, \"Old Thoughts and New Work on Breads White and Brown,\" Lancet 266, no. 6883 (30 July 1955): 210. 22 Widdowson and McCance, Studies on the Nutritive Value of Bread, 68. 331 the 1950s, a flour of 80 percent extraction or higher was superior to a whiter flour of lower extraction enriched with the \"token\" nutrients. The subsidy on National bread, they claimed, had little basis in scientific evidence. When the Widdowson-McCance report was publicized in the press, commentators seized on the opportunity to take populist swipes at the nutrition experts responsible for preventing the British public from consuming white bread and baked goods. In a Daily Express article by Chapman Pincher titled \"The White Bread Man Wins Through,\" McCance was Widdowson unfortunately, but unsurprisingly for that time, recei ving only a passing mention\u2014for forcing the government to admit that \"white bread is just as nourishing as the less palatable grey bread we have been bulldozed into eating 'for our own good.'\" Pincher also claimed that the results of the experiments were shelved by the MRC for several years to prevent government advisers and \"the old brigade of dieticians\" from losing face.23 Yet even after the release of the Widdowson-McCance report, some leading nutritionists in Britain continued to advocate National wheatmeal. This was partly due to the fact that, several years before the publication of the report, then-secretary of the MRC Edward Mellanby had dealt a serious blow to the reputation of the milling and baking industries. In 1946, he published a study documenting the harmful effects of \"agenized\" flour on experimental dogs. The \"agene method,\" which involved the mixing of nitrogen trichloride gas with freshly milled flour, improved the baking qualities of white flour by chemically altering the wheat protein. This method economized on flour, since much of the wheat that was grown, if freshly milled, produced flour that was unsuitable for use in baking. But if this wheat was allowed to become \"aged,\" or oxidized by mean s of long periods of 23 Chapman Pincher, \"The White Bread Man Wins Through,\" Daily Express, 22 May 1956. 332 storage, and was then milled, the resulting flour had greatly improved baking properties. Beginning in the late nineteenth century, millers had developed various oxidizing agents, or \"improvers,\" to artificially speed up this flour aging process. Nitrogen trichloride, patented in 1921, was one such improver, and the agene method subsequently becam e commonplace in the milling industry. By 1946, when wheat of any quality was particularly scarce, it was estimated that over 90 percent of the flour milled in Britain was being treated with nitrogen trichloride.24 In his feeding experiment, Mellanby found that dogs fed agenized flour as a large proportion of their diet developed neurological disorders resembling epileptic fits, which he termed \"canine hysteria,\" while dogs fed untreated flour did not. Other investigators repeated Mellanby's results in some, though not all, species of laboratory animals. Mellanby and his coworkers subsequently isolated a toxic compound from agenized flour, methionine sulfoximine.25 Although the neurological disturbances could not be reproduced in young human subjects fed agenize flour, concerns about its potential toxic effects on human health led to the discontinuation of its use in Britain and the United States in the early 1950s.26 Therefore, when the Widdowson-McCan ce study was published in 1954, proponents of National wheatmeal were all the more reluctant to change their position. \"When it is a question of altering what has been a staple food for a very long time, the onus of proving advantage lies on those who wish to make the alteration, not on those who oppose it,\" an editorialist in The Lancet maintained. \"Neither for the use of chemical improvers of flour nor for a low extraction-rate do we regard this advantage as proved.\" In the first place, the 24 Edward Mellanby, \"Diet and Canine Hysteria: Experimental Production by Treated Flour,\" British Medical Journal 2, no. 4484 (14 December 1946): 885-87. 25 P. N. Campbell, T. S. Work, and E. Mellanby, \"Isolation of Crystalline Toxic Factor from Agenized Wheat Flour,\" Nature 165 (4 March 1950): 345-46. 26 C. A. Shaw and J. S. Bains, \"Did Consumption of Flour Bleached by the Agene Process Contribute to the Incidence of Neurological Disease?\" Medical Hypotheses 51 (1998): 479. 333 editorialist pointed out, the Widdowson-McCance orphanage experiments lasted only about a year. Moreover, analysis of the experimental diets showed that the children were consuming adequate amounts of all nutrients, with the possible exception of riboflavin. Finally, the 70 percent extraction flour used in the experiment reportedly had a higher content of thiamin and niacin than the flour of the same extraction rate used in Britain. All of these factors, The Lancet concluded, called into question the relevance of Widdowson and McCance's findings to the debate over the subsidy on the National loaf. Following the issuing of the Widdowson-McCan ce report, rising pressure from the milling and baking industries forced the government to appoint a panel to reexamine the available evidence. In the years following World War II, a number of other nutrition researchers besides Widdowson and McCan ce had also begun to question whether it was necessary for the British public to consume National wheatmeal.27 The 1955 Panel on Composition and Nutritive Value of Flour, headed by Sir Henry Cohen, was tasked with determining the differen ces in the composition and nutritive value between National flour and low-extraction flour with or without the three token nutrients. The Cohen Panel, as it came to be called, was also intended to advise the government on whether any of the nutritive differen ces between these flours were significant from the perspective of the population's health. The situation confronting the Cohen Panel was a complicated one. Aside from the three \"token\" nutrients\u2014thiamin, riboflavin, and iron\u2014very little was known about the human requirements for the many other vitamins and minerals contained in the wheat grain. It was known that the sodium, magnesium, and manganese content of flours all varied with 27 See, e.g., John Yudkin, \"Fighting Food Faddism,\" International Journal of Food Sciences and Nutrition 7, no. 4 (1953): 186-87; D. W. Kent-Jones, \"The Case for Fortified Flour,\" Proceedings of the Nutrition Society 17 (1958): 38-43. 334 the extraction rate, but the Panel resolved that \"the differences in the contribution made by flours of different extraction rates to the total intake and dietary requirements of these elemen ts is so small as to be negligible.\" 28 With regard to vitamins, the panelists were more circumspect. They acknowledged that human requirements for pyridoxine, pantothenic acid, biotin and folic acid were not known, and that information as to their distribution in foods and flours of various grades was \"far from complete.\" The panelists also did not consider vitamin E\u2014a fat-soluble vitamin concentrated in the wheat germ that was almost completely remo ved in the production of white flour\u2014since scientific knowledge of human requirements for it was still sparse. Despite these gaps in the research, the Cohen Panel came to conclusions more or less identical to those of Widdowson and McCance. \"In spite of weighty opinion to the contrary,\" the Panel declared, \"a lowering of the extraction rate from 80 to 70 per cent is very unlikely to lead to any nutritional disturbances from a lack of these vitamins.\"29 The \"weighty opinion to the contrary\" included the MRC, which had convened a conferen ce in September 1955 to consider the latest evidence and furnish its opinion to the government. After the release of the Cohen Panel's decision, the MRC representatives issued a memorandum in which they took a muted yet still critical view of white flour. \"The consumption of a 70% extraction flour, even if this is partially fortified,\" they argued, \"would lead to a reduced intake of some nutrients. Although such a reduction would not necessarily lead to an increase in recognizable illness, it would, in the present state of 28 Report of the Panel on the Composition and Nutritive Value of Flour, Command Paper 9757 (London: HMSO, 1956). Cohen himself was apparently unhappy with the circumstances under which the report was produced and publicized. See letter (27 June 1956) from Cohen to Harriette Chick, PP/CHI/C.15, Dame Harriette Chick Collection, Wellcome Library Archives and Manuscripts, London, UK. 29 Cited in D. W. Kent-Jones, \"The Case for Fortified Flour,\" Proceedings of the Nutrition Society 17 (1958): 42. 335 knowledge, constitute a risk which can be avoided.\" The MRC representatives concluded that great caution was needed before making changes in the nation's staple food, since \"the absence of recognizable illness in an individual is not a sufficient criterion that his diet is nutritionally sound.\"30 In other words, the MRC deemed the threat of subclinical deficiencies too great to support the abandonment of a proactive government policy toward less-refined forms of flour. Despite the misgivings of the MRC, the government accepted the counsel of the Cohen Panel and changed its flour and bread policy. At the end of September 1956, the government terminated the subsidy on bread made from 80 percent extraction flour and mandated enrichment of lower-extraction flour with the three token nutrients as well as calcium. The National wheatmeal era was over, and nutrition experts never again attempted to mount a campaign to mandate or subsidize wholegrain (or nearly wholegrain) flour and bread.31 \"Power to Produce\": Agriculture and the Food System in the Postwar Era It was not just the emergence of new research such as the Widdowson-McCan ce study that contributed to the growing sense of complacency among postwar-era health authorities toward the modern diet. Wide-ranging changes in the food system, and in the socioeconomic context more generally, also played a large role. On the demand side, rising incomes and employment levels meant that more people could afford to purchase nutrient- 30 \"Nutritive Value of Flour: M.R.C. Evidence,\" British Medical Journal 1, no. 4979 (9 June 1956): 1354-56. 31 A handful of British nutrition researchers continued to criticize enriched white bread in the years following the change in government policy. See, e.g., Harriette Chick, \"Wheat and Bread: A Historical Introduction,\" Proceedings of the Nutrition Society 17 (1958): 1-7; H. M. Sinclair, \"Nutritional Aspects of High-Extraction Flour,\" Proceedings of the Nutrition Society 17 (1958): 28-37. 336 dense foods. These economic changes were reflected dietary surveys conducted in the two postwar decades, which showed that vitamin and mineral intakes were, on average, markedly higher than they had been in the economically depressed 1930s. On the supply side, there were drastic increases in agricultural output and productivity, which translated into lower food prices and greater abundance for the consumer. This postwar \"agricultural revolution\" arose from a potent combination of state interventionism and scientific and technological developments. The war years were a turning point in British agriculture. The outbreak of hostilities ushered in fundamental changes in the control and regulation of the agricultural sector, as the government scrambled to reduce the country's dependence on overseas imports by inducing farmers to maximize domestic food production. But with the end of the war approaching, the farming community feared that the government might repeat its actions following World War I, when, despite assurances to the contrary, it had rapidly dismantled the wartime system of production control and guaranteed prices and returned to unregulated foreign competition. By the latter stages of World War II, however, the two main political parties in Britain acknowledged that farmers should be rewarded with a high degree of security for their contribution to victory. Following the end of the war, moreover, the ruling Labour government faced a number of serious problems that underscored the desirability of promoting the expansion of domestic food production. First, with the exception of North America, food shortages had reach ed crisis proportions in 1945 and 1946, threatening mass starvation across large areas of war-torn Europe. In Britain, bread and potatoes, which were available in unrestricted 337 quantities during the war, were rationed for the first time between 1946 and 1948.32 Famine loomed in countries such as India as well, forcing the diversion of available food supplies to these areas. Seco nd, war-induced dislocations in international trade precluded the rapid resumption of food imports on the scale achieved by 1939. Finally, the sudden termination of American Lend-Lease aid in August 1945 created a dollar shortage in Britain, which made the purchase of high-priced foodstuffs in the dollar-dominated world mark ets financially undesirable and logistically impractical because of shortages in shipping space.33 The cornerstone of the Labour government's legislation was the 1947 Agricultural Act. This act had two interrelated aims: the promotion of a \"stable\" agricultural sector to ensure fair returns for farmers, farm workers, and landlords; and the development of an \"efficient\" system to increase food production and relieve the tightly rationed British consumer. The legislation guaranteed prices and producer subsidies to aid greater output, and formally laid out principles of good husbandry with which farmers had to comply to avoid dispossession. In the summer of 1947, following a severe balance-of-payments crisis, the British government initiated a five-year agricultural expansion program, with the aim of reducing the importation of products that had the highest monetary value, such as pork, milk, and eggs. Agricultural investment, as well as the cost of government support for farming, increas ed significantly in the following years, giving rise to the widely held belief that farmers were being \"feather-bedded\" at the taxpayers' expense. But the expansion program achieved its overall goal of increasing food production by 20 percent in five years. When the Conservative Party returned to power in 1951, food shortages were in fact being replaced 32 Dorothy Hollingsworth, \"Rationing and Economic Constraints on Food Consumption in Britain since the Second World War,\" in Diet and Health in Modern Britain, Derek Oddy and David S. Miller, eds. (London: Croom Helm, 1985). 33 The Development of Modern Agriculture: British Farming since 1931 (New York: St. Martin's Press, 2000), 67-77. 338 by surpluses of certain commodities. By the mid-1950s, international food surpluses began appearing as well. These emerg ing surpluses coincided with a general retrenchment of the British economy and a curtailment of government expenditure in response to a series of postwar financial crises. The objective of agricultural policy therefo re shifted from rapid expansion to the promotion of more efficient methods of production, leading to some alterations of the terms of the 1947 Agricultural Act. But the act nevertheless continued to set the general pattern for the state's role in farming until Britain joined the European Economic Community in 1973.34 British agriculture, then, was not to return to the laissez-faire model that had characterized it in peacetime from the mid-nineteenth century up to the early years of the Great Depression. In the United States as well, the state's role in farming was cemented during World War II and in the years afterwards. The country's entry into the war promptly solved the problems of low prices and overproduction that had dogged agriculture through the 1930s despite the unprecedented state support that farmers received as part of the New Deal. To ensure high agricultural output during the war, the government prohibited farm prices from falling below 110 percent of \"parity\"\u2014the price in real dollars that farmers received for their agricultural commo dities in the years just before World War I, when their purchasing power relative to industrial commodities had reached a peak. This ruling drew strong criticism of the farm bloc's apparently overweening political clout in Congress. Nevertheless, the government also guaranteed American farmers\u2014who greatly feared a repeat of the price collapse that occurred after World War I\u2014that the basic agricultural commodities would remain at 90 percent of parity for two years after the war ended. As it turned out, however, world demand for American agricultural commodities kept prices high after the war drew to 34 Ibid., 69-93. 339 a close in 1945. But by the late 1940s, it was becoming obvious that farmers had the capacity to produce agricultural commo dities far in excess of demand, and prices began to slump. Fortunately for the farmer, the Korean War, which began in June 1950, temporarily reversed the trend. To assure expanded production during this conflict, the government continued to protect high wartime prices by extending price supports on the basic farm commodities.35 But chronic overproduction remained a problem through the rest of the 1950s and 1960s, and both political parties were unwilling to anger the farm bloc by lowering price supports below profitable levels. One compromise solution, approved by Congress in 1954, was the Agricultural Trade Development and Assistance Act, which authorized the federal government to dispose of agricultural surpluses abroad, particularly to help friendly and \"underdeveloped\" nations. This legislation, known as Public Law 480, or the \"Food for Peace\" program, essentially turned a domestic overproduction problem into an instrument of Cold War diplomacy . Aside from subsidizing exports in this way, American agricultural policy during the 1950s and 1960s remained fundamentally tied to the ideas developed during the 1930s\u2014to control production, shrink surpluses, and increase prices and farmer income.36 Agriculture in the postwar period was transformed not only by sustained state intervention and support, but also by a scientific and technological revolution. One of the most profound changes in farming was its near-total mechanization. The number of farm mach ines powered by internal combustion engines rose rapidly during World War II and continued to climb in the postwar years. In the United States, as in other European settler 35 R. Douglas Hurt, Problems of Plenty: The American Farmer in the Twentieth Century (Chicago: Ivan R. Dee, 2002), ch. 4 passim. 36 Hurt, Problem of Plenty, ch. 5 passim. 340 societies, the shift from the use of hand labor and draft animals to petroleum-powered mach ines was already well advanced by the close of the interwar period, particularly with regard to the adoption of the multipurpose tractor. But the mechanization of farm operations became much more widespread and thoroughgoing during World War II and in the following years. Farmers responded to the war-induced shortage of labor and seemingly insatiable deman d for agricultural products by buying tractors and tractor-powered implemen ts, combine harvesters, binders, and elevators, despite their scarcity and high cost. The farm mechanization trend accelerated in the two postwar decades, particularly in European countries such as Britain, which had previously depended more on hand labor and animal power than the settler societies. In addition, the electrification of rural areas, which had been only partial and uneven in the interwar years, became nearly universal. By providing power for lights, heating and cooling systems, milking mach ines, grinders, and myriad other implements, electricity was crucial in bringing farms into the \"Machine Age.\"37 Accompanying mechanization in the postwar drive for increas ed agricultural productivity and output was the \"chemicalization\" of crop production. Prior to World War II, soil fertility was typically maintained or improved by growing restorative crops such as legumes, applying farmyard manure, and the using soil amen dments such as lime and phosphate. The postwar expansion of agricultural output was aided to a large extent by an escalation in the use of concentrated chemical fertilizers. Fertilizers supplying nitrogen were particularly crucial in increasing crop yields, but phosphorus and potassium fertilizers were important as well. Application of chemical fertilizers in the interwar period had remained low in most branches of agriculture, due in large part to their inconsistent quality and 37 See USDA, Power To Produce: The Yearbook of Agriculture, 1960 (Washington: GPO, 1960); Hurt, Problems of Plenty, 115-16; Martin, Development of Modern Agriculture, 98-101; Paul K. Conkin, A Revolution Down on the Farm: The Transformation of American Agriculture Since 1929 (Lexington, KY: University Press of Kentucky, 2008), 99-107. 341 difficulty in storage and application. In addition, farmers lacked reliable information on the effect of fertilizer applications on yields of particular crops in particular soils. But these problems were largely overcome during and immediately after World War II. As a result, farmers began making heavy applications of chemical fertilizers to boost yields and, they hoped, profitability. The broad orientation of postwar agricultural policy toward maximizing labor productivity and subsidizing commo dity crop production also spurred the rapid increas e in the quantity of chemical fertilizers applied by farmers . The increased yields usually attributed to chemical fertilizers were, in part, made possible by new breeds of crop plants. Commercial and academic plant breeders developed new crop varieties, such as short-stemmed wheats and hybrid maize, that could divert more of their growth potential into producing grain rather than stem, leaf, and root mass. The new varieties were better adapted to utilizing regular applications of concentrated chemical fertilizers, which provide nutrients that are more immediately available to plants than those found in decaying organic matter.38 Scientific and technological changes also revolutionized livestock farming in the postwar period. Feed companies and agricultural scientists devised novel mixtures and methods of feeding animals to maximize output of meat, milk, and eggs at minimum cost. Animal breeders concurrently developed genetically uniform strains of livestock capable of producing high yields on these vitamin- and mineral-enriched commercial formulas. An important factor in this process, particularly with regard to dairy cattle, was artificial insemination. Following the development of several key techniques and technologies that 38 Conkin, Revolution Down on the Farm, 108-12; Martin, Development of Modern Agriculture, 102-05. 342 made artificial insemination commercially feasible, the procedure experienced phenomenal growth in the 1940s and 1950s.39 The use of antibiotics also became widespread in livestock farming in the 1950s, shortly after the unintentional discovery that they improved the feed conversion ratio\u2014that is, the number of pounds of feed required to produce one pound of weight gain. The delivery of antibiotics through animal feed and drinking water also became necessary to ward off the contagious diseases and depressed growth caused by the stress of crowded confinemen t feeding systems, which were becoming commo nplace in livestock farming in the postwar decades. Concentrated feedlots and indoor facilities were economically attractive to livestock producers because they allowed for the precise feeding of cattle, pigs, and chickens on commercial feeds under carefully monitored conditions. Confinement feeding also reduced or eliminated the close tie between the seasons and the lifecycle of livestock that had previously characterized animal husbandry, resulting in more even supplies of meat throughout the year.40 Agriculture was further chemicalized by the widespread adoption of synthetic herbicides and pesticides. The control of weeds in grain crops was completely transformed following the introduction of the synthetic plant growth regulators, MCPA (2-methyl-4-chlorophenoxyacetic acid) and 2,4-D (2,4-Dichlorophnoxyacetic acid) in the immediate postwar years. These compounds eradicated commo n broadleaf weed species in a way that was more cost-effective than the traditional methods of crop rotation, bare fallow, and mech anical cultivation. In addition, by eliminating competition for nutrients from weeds, yields from herbicide-treated crops were generally higher. Following on these early 39 R. H. Foote, \"The History of Artificial Insemination: Selected Notes and Notables,\" Journal of Animal Science 80 (2002): 1-10. 40 Martin, Development of Modern Agriculture, 107-29. 343 successes, the agrochemical industry developed a large array of herbicides in the 1950s and 1960s to control weeds in cereal, root, forage, vegetable, and fruit crops.41 Pest control was also thoroughly chemicalized in the postwar years. DDT was the best-known\u2014and ultimately the most publicly controversial\u2014pesticide used in the 1940s and 1950s. Employed extensively by the Allies during World War II to control the insect vectors of typhus and malaria, DDT was made available to farmers as a commercial insecticide in 1945. In the years that followed, farmers escalated their use of DDT and other more toxic organochloride pesticides, such as Aldrin and Dieldrin, to control pests in their crops and livestock. These pesticides produced seemingly miraculous increases in yields and labor productivity for relatively little cost. As was the case with herbicides, the rapid postwar growth in the consumption of pesticides by farmers spurred the agrochemical industry to develop an ever-greater number of insecticides and fungicides made from complex synthetic compounds.42 The postwar technological and scientific revolution transformed many other aspects of the food system beyond the farm gate. Food processing facilities of all kinds, from bakeries to packing sheds to slaughterhouses, were increasingly mech anized to improve economies of production and reduce labor costs. Food chemists invented hundreds of new additives to aid in processing and preserving foods. Another significant change that enabled the lengthening of the time and distance separating the production and consumption of food was the near-universal adoption of freezing and refrigeration technology by food processors and retailers, as well as by consumers at the household level. Equally remark able and far-reach ing in the modernization of the food supply was the postwar \"packaging revolution.\" 41 Conkin, Revlution Down on the Farm, 112-16; Martin, Development of Modern Agriculture, 101-02. 42 Martin, Development of Modern Agriculture, 101-02. 344 In the interwar period, food packaging was largely confined to paper and cardboard wrapping, wooden boxes and crates, glass bottles and jars, and metal cans. The only plastic substance in commercial use\u2014particularly in the high-end bread market\u2014was cellophane, a clear and waterproof wrapping derived from wood cellulose. But in the 1940s and 1950s, a plethora of plastic and composite packaging materials came into widespread use. Virtually all segments of the food supply were affected by these new forms of packaging: fresh and frozen produce, dairy products, meat and seafood, bread and other baked goods, cereals, prepared meals, beverages, and snack foods.43 This cluster of scientific and technological innovations, driven by the awesome outpouring of novel substances from the synthetic chemical industry and powered by cheap and abundant petroleum and electricity, had a profound cumulative effect on the food system. Almost all steps in the food production chain were altered, as was the manner in which consumers purchased and interact ed with what they ate. Health authorities, on the whole, viewed this transformation of the food system in a positive light\u2014declining food costs, greater variety of food, and rising incomes gave them yet another reason to believe that vitamin and mineral undernutrition was no longer a threat to public health. The transformation of the food system and the decline in concern about malnutrition in the two postwar decades also contributed to the waning of enthusiasm among nutritionists, agricultural scientists, and government officials toward the soil-nutrition issue, described in chapter three. The increasing abundance and declining cost of food during that time belied the fears voiced during the interwar period that widespread soil erosion and nutrient depletion were a threat to public health. \"We see no need for alarm concerning the soil 43 Mildred M. Boggs and Clyde L. Rasmussen, \"Modern Food Processing,\" in Food: Yearbook of Agriculture, 1959 (Washington: GPO, 1959), 418-33. 345 factor,\" declared Kenneth Beeson of Cornell University in 1951. Dismissing the gloomy prognostications put forward in popular books on soil erosion, Beeson maintained that soil and plant scientists \"are ever impressed with the potential fertility the our soils, even in those situations where soils have been used for a very long time.\" 44 In addition, researchers in the postwar period lost interest in the question of whether soil fertility influenced the nutritional quality of crops. While agricultural experts continued to stress the importance of using fertilizers and maintaining or improving the humus content of the soil in order to increase crop yields as well as the types of crops that could be grown, they came to the conclusion that fertilization practices had a relatively insignificant affect on the vitamin, mineral, or protein content of these crops. Other factors, such as the breed of the plant and the stage at which it was harvested, were found to be much more important.45 \"The Killer in Your Sugar Bowl\": Critiques of the Indus trial Food Supply in the Postwar Era Not everyone, however, agreed with the orthodox view. After World War II, a popular dietary reform movement emerged in the United States and Britain dedicated to the notion that a significant amount of ill health suffered by the populations in these countries was the result of eating too many processed foods deficient in vitamins, minerals, and high-quality proteins. Supporters of this movement also believed that the consumption of a diet of natural, nutrient-dense foods was the best way to prevent illness, enhance vitality, and 44 Kenneth Beeson, \"Soils and the Nutritive Quality of Plants,\" Presentation before Section O, American Association for the Advancement of Science, Philadelphia, PA (27 December 1951), in unsorted file, \"Nutrition and Health,\" Malabar Farm State Park, Lucas, OH. 45 HMSO, Plant and Animal Nutrition in Relation to Soil and Climatic Factors (London: HMSO, 1951); Conrado F. Asenjo, \"Variations in the Nutritive Values of Foods,\" American Journal of Clinical Nutrition 11 (November 1962): 368-74; USDA, Agricultural Research Service, The Effects of Soils and Fertilizers on the Nutritional Quality of Plants, Agriculture Info. Bull. No. 299 (Washington: GPO, 1965). 346 increas e longevity. (Because of the centrality of natural, minimally processed foods in this movement's teachings, I will hereafter refer to it as the \"natural food movement.\" And, as I will explain below, this was the term that some supporters of the movement had begun using by the early 1960s.)46 This was not the first movement to maintain that \"natural\" foods were the key to health. Enthusiasm for natural foods had been one of the distinguishing charact eristics of a health reform tradition that had originated over a century earlier in the teachings of American naturopath and preacher Sylvester Graham. Although his name has survived only in a cracker and in the coarsely ground whole-wheat flour he so ardently advocated, Graham's ideas inspired a popular dietary reform movement that ran from the mid-nineteenth century into the interwar period. Grahamites and postwar natural food advocates\u2014who I will also refer to as \"natural foodists\"\u2014shared several other beliefs in common. Both groups criticized conventionally trained physicians for placing too much emphasis on curative medicine while failing to appreciate the importance of prevention. And most significantly, both groups expressed faith in the benevolence of nature, and maintained that unnatural eating habits and food production methods were responsible for much of the ill health that people in modern societies suffered .47 Yet, as Michael Ackerman has pointed out, postwar-era natural foodists did not derive their ideas on food and health from Graham or his ideological heirs. Rather, their 46 Since \"health food\" stores played a major role in promoting this movement's ideas, scholars and critics have also referred to it as the \"health food movement.\" See, e.g, Barbara Griggs, The Food Factor: Why We Are What We Eat (Harmondsworth and New York: Viking, 1986), ch. Nutrition',\" 8 passim. 47 James C. Whorton, Crusaders for Fitness: A History of American Health Reformers (Princeton, NJ: Princeton University Press, 1982); Griggs, The Food Factor, ch. 6 pasim; \"Patient, Health Thyself: Popular Health Reform Movements as Unorthodox Medicine,\" in Other Healers: Unorthodox Medicine in America, ed. Norman Gevitz (Baltimore: Johns Hopkins University Press, 1988), 52-81; P. S. Brown, \"Nineteenth-Century American Health Reformers and the Early Nature Cure Movement in Britain,\" Medical History 32 (1988): 174-94. 347 views were inspired by the work and pronouncements of nutrition researchers, most of whom belonged in the scientific mainstream. The principal ideas that led to the emergence of the natural food movement were formulated by nutritionists during the first half of the twentieth century: the concept of optimal nutrition; the discovery that \"natural\" foods were nutritionally superior to \"refined\" foods; the determination that mild or subclinical vitamin and mineral deficiencies were widespread in the population; and the concern that nutrient-poor soils produced nutrient-poor crops. And unlike the Grahamites, postwar-era natural foodists believed that scientific research was the sole authoritative source of nutritional and medical knowledge. In fact, most of the founders of the natural food movement were physicians and dentists themselves. After World War II, many of these individuals becam e involved in the American Academy of Applied Nutrition (AAAN), an organization that was established when the vitamin and mineral discoveries were beginning to influence the outlook of the medical and dental professions in the 1930s. The AAAN had its origins in the Academy of Dental Nutrition, a seminar for dentists established by Harold Hawkins in 1931. Hawkins, a staff member of the University of Southern California College of Dentistry in the 1920s and 1930s, was a minor player in the debate that developed at that time over the role of nutrition in dental health.48 While teaching bacteriology at the College of Dentistry in the early 1920s, he became convinced that there must be some way in which the alarming prevalence of dental caries in the United States could be reduced. Finding little of use in the professional literature, Hawkins conducted nutritional experiments with white rats and pigs from 1922 to 1926, just as other dental research groups were independently embarking on similar work. From his animal studies, Hawkins determined that immu nity or susceptibility 48 For a discussion of this debate, see chapter two. 348 to caries and gum disease corresponded very closely to the \"buffer capacity\" of the saliva\u2014that is, the concentration in the saliva of calcium and other alkaline salts. He then tested his findings on a group of one hundred children at the Los Angeles Orphan Home from 1927 to 1930 and achieved remarkable success in arresting caries through dietary changes. Hawkins concluded that the only way to positively alter the chemistry of the saliva was to consume a diet high in the protective foods and low in refined sugar and white flour. He also placed particular emphasis on ensuring an adequate intake of vitamin D by exposure to ultraviolet radiation or by consumption of cod liver oil and other foods containing the vitamin. Hawkins found that he could halt existing tooth decay and pyorrhea and prevent their reoccurrence through dietary means, regardless of the level of oral hygiene practiced, in over 95 percent of his subjects. He was confident that refinements in technique would bring the proportion even higher.49 Many members of the dental profession in southern California were impressed by the success of Hawkins's orphanage experiment, so in January 1931 he began giving courses through the College of Dentistry to teach other area dentists about the relationship between nutrition and dental health. The interest and enthusiasm of those who had completed the courses was such that Hawkins felt compelled to continue giving seminars on nutrition, which the participants designated \"The Hawkins Study Club.\" Hawkins thought that a more formal designation for the group would be better, so in December 1931 its official title became the Academy of Dental Nutrition. But over time, the members felt that they should 49 Harold F. Hawkins, \"A Rational Technique for the Control of Caries and Systemic Pyorrhea,\" Journal of Dental Research 11, no. 2 (April 1931): 201-34; Harold F. Hawkins, Applied Nutrition (La Habra, CA: International College of Applied Nutrition, 1947). Early on in his dietary experiments, Hawkins agreed with Martha Jones, a dental researcher who was best known for propounding the hypothesis that the diet had to have a net alkaline reaction to confer immunity to caries. But he appears to have back off from this notion at some point in the 1930s; critics rightly pointed out that the diets of many primitive peoples who were almost completely immune to caries were quite acid in reaction. 349 broaden the Academy's base and make it possible for physicians and those with doctorates who had specialized in nutrition to participate. In 1936, the organization was renamed the American Academy of Applied Nutrition and chartered as a nonprofit corporation. The AAAN's aim, according to its founding constitution, was \"to promote and advance the science and art of nutrition especially as it pertains to the prevention and treatment of disease; to establish applied nutrition as a specialty of dentistry and medicine.\"50 Membership in the organization was initially restricted to physicians, dentists, nutritionists, agronomists, chemists, and veterinarians with doctoral degrees. But in 1946, the membership was widened, though without voting rights, to include graduates with training in nutrition-related fields such as nursing, dietetics, dental hygiene, public health, and food technology. The general public was also encouraged to join the academy as nonprofessional members. In 1947, the AAAN began publishing its own research journal, the Journal of the American Academy of Applied Nutrition (shortened to the Journal of Applied Nutrition in 1953). Initial growth of the organization was robust. Membership doubled from 175 in 1946 to 350 a year later. But dissatisfaction arose at the annual AAAN conventions, due to the fact that scientific papers were written with a mixed audience in mind. The omission of technical details and documentation failed to satisfy trained scientists, while the inclusion of statistics and the presentation of disagreements in the scientific literature bored or confused those without formal scientific training. So, in 1951, the organization was divided into the Academy of Nutrition and the American Nutrition Society (ANS), the former for those with a doctorate and the latter, under the sponsorship of the academy, for all others. Adopting a two-tiered publication model similar to that of the American Medical Association, the 50 Harold F. Hawkins, \"Early History of the American Academy of Applied Nutrition,\" Journal of the American Academy of Applied Nutrition 1, no. 1 (Spring 1947): 39-42. 350 AAAN began Modern Nutrition in 1951, a lay-oriented monthly magazine. By 1955, the AAAN had 1500 members, with 450 in the \"professional\" category, in thirteen local chapters.51 The physicians and dentists of the natural food movement, whether associated with the AAAN or not, shared a number of convictions on matters related to health and nutrition. First and foremost, they believed that nutritional deficiencies were rampant in modern society. Some of them subscribed to this view years before health authorities reached this conclusion in the late 1930s and early 1940s. Daniel Thomas Quigley, a faculty member at the University of Nebraska College of Medicine and an AAAN affiliate, was in the vanguard of clinicians who pioneered the use of vitamin- and mineral-rich diets in the cure and prevention of disease during the interwar period. Befo re becoming interested in nutrition, Quigley was a founding member of the American College of Surgeons and one of the first American physicians to use radium in the treatment of cancer. He established the thirty-bed Radium Hospital in Omaha, Nebraska in 1917, and wrote The Conquest of Cancer by Radium and Other Methods (1929), an illustrated book on the effects of radiation.52 From 1922 to 1932, Quigley conducted his own long-term study to determine the impact of vitamins and minerals on the health of patients suffering from various kinds of tumor and cancer. He examined and treated approximately three thousand patients and found that \"they all were on more or less deficient diets and that none had tumor or cancer as a single disease.\" Obesity, diabetes, constipation, and gallbladder disease were especially common. Quigley observed that his poor and indigent patients, whose diets consisted almost entirely 51 Granville F. Knight, \"The President's Page,\" Journal of Applied Nutrition 8 (Spring 1955): 341-32; \"Harold F. Hawkins, D.D.S., 1884-1966,\" Journal of Applied Nutrition 19, nos. 1 and 2 (1966-1967): 6-7. 52 Howard B. Hunt, \"Daniel Thomas Quigley, 1876-1968,\" American Journal of Roentgenology 106, no. 2 (June 1969): 447-48. 351 of foods made from refined flour and sugar, suffered the highest rates of reoccurrence after cancer treatment. All his patients experienced significant health improvements, including better resistance to reoccurrence of cancer, after being placed on diets abundant in protective foods.53 In Notes on Vitamins and Diet (1933), Quigley reported the results of his clinical work, as well as the findings of other nutrition researchers. He concluded that it was the increas ing consumption of sweets, candies, white bread, cake, cookies, biscuits, and other processed foods that was responsible for the high incidence of cancer and other degenerative diseases in the West.54 N. Philip Norman, a New York physician and an AAAN chapter president, was also an early advocate of nutritional therapy. After graduating from Vanderbilt University School of Medicine in 1912, Norman gravitated toward neuropsychiatry and psychoanalysis. But he became disillusioned with the field after a few years and began studying the role of diet on physical and mental health. In 1921, Norman happened to attend lectures on nutrition at the University of Pittsburgh given by Robert McCarrison, who was on a speaking tour of the United States at that time. In these lectures, McCarrison advanced his hypothesis that most disease and physical degeneration was rooted in faulty diet. It was a conversion moment for Norman. As a consultant in the Neuropsychiatric Division of the Army Medical Corps during World War I, Norman had the opportunity to critically appraise the health of volunteers and draftees. He was amazed at the high rejection rate and began to wonder why so many men were unfit for military duty. McCarrison's researches provided Norman the answer to this question, and he thereafter became a staunch critic of refined foods and the food processing industries that were, in his words, \"well entrenched in Western food culture 53 Daniel Thomas Quigley, \"Diet Deficiency in Everyday Life,\" Modern Nutrition 6, no. 8 (September 1953): 6-9. 54 Daniel Thomas Quigley, Notes on Vitamins and Diet (Chicago: Consolidated Book Pub., 1933). 352 and economy.\"55 In a speech to the American Therapeutic Society in 1926, for example, Norman argued that \"Decayed teeth, poor skeletal development, increase of chronic diseases, nervous instability, mental deficiency, gastro-intestinal disease and malfunctions have increased in direct proportion to the number of 'processed' foods manufactured.\"56 Another early critic of modern eating habits who became a leading figure in the natural food movement was American dentist Fred Miller. Around the same time that the newer knowledge of nutrition was emerg ing in the 1910s and 1920s, Miller became convinced that proper diet was key in curing and preventing tooth decay and gum disease. Even though Miller did no original scientific work himself, he developed practical techniques over his long career as a private practitioner for working with patients who became interested in saving their and their family's teeth through diet. His dietary program, though a radical departure from what Americans typically ate, was in fact quite similar to what conventional nutritionists advocated in that era. Miller recommended a diet high in the \"protective foods\": dairy products, fresh fruits and vegetables, eggs, organ meat s, and wholegrain cereals. He further recommended cod liver oil for children whose teeth were developing and for adults during the winter months. Like many of his nutritionally minded colleagues in the dental profession, he frequently assailed the excessive consumption of foods containing sugar and white flour. Echoing what some other dentists and nutritionists were arguing at that time, Miller also believed that tooth decay, gum disease, and orthodontic problems were only the most visible signs of a disturbed bodily metabolism. 55 Biographical details in N. Philip Norman, The \"Triad Disease\": Mankind's No. 1 Killer (Milwaukee, WI: The Lee Foundation for Nutritional Research, 1958), iii-xx; and Alfred J. Asgis, \"Applied Nutrition for Practicing Dentists,\" Dental Items of Interest (January 1948): 6. 56 N. Philip Norman, \"A Summary of the Present Knowledge of Vitamins and Their Therapeutic Importance,\" International Clinics 3 (September 1926): 49. 353 Because the natural food movement emerg ed just before and during World War II\u2014the very moment when health authorities were most concerned about the consumption of foods deficient in vitamins and minerals\u2014there was little, if any, antagonism between the two groups. This congruity of opinion can be seen, for instance, in the sympathetic response that the AAAN received from Berkeley nutritionist Agnes Fay Morgan in 1946, shortly after she gave a presentation at the organization's annual meeting on the pathological effects of vitamin B6 (pyridoxine) deficiency. \"I congratulate you upon the organization of a practical and progressive group of professional people,\" she wrote to AAAN vice-president Francis Pottenger, Jr., \"and I hope that this academy will expand and thrive.\"57 In a similar vein, Fred Miller attained such a high standing in the dental profession that he was invited as a delegate to the National Nutrition Conference for Defense in May 1941, where health authorities discussed how to respond to the malnutrition \"crisis.\"58 The following year, one of Miller's articles, \"Eating for Sound Teeth,\" was published in the AMA's lay-oriented magazine, Hygeia (renamed Today's Health in 1949).59 But after World War II, the views of the physicians and dentists in the natural food movement began to diverge from those of health authorities. While many nutrition experts became less concerned about refined foods and the problem of nutritional deficiencies in the two postwar decades, supporters of the natural food movemen t continued to insist that millions of people in the prosperous Western countries were being harmed by inadequate diets. Adelle Davis, for example, warned in her popular book Let's Eat Right to Keep Fit (1954): \"Everything we eat is tinkered with in one way or another. With every tinkering 57 Letter (18 July 1946) from Agnes Fay Morgan to Francis M. Pottenger, Jr., Box 2, BANC MSS 75/63c, Agnes Fay Morgan Collection, Bancroft Library, University of California, Berkeley. 58 Fred D. Miller, Open Door to Health (New York: Devin-Adair, 1959), 119. 59 Fred D. Miller, \"Eating for Sound Teeth,\" Hygeia, July 1942. 354 comes losses, some small and unavoidable, some large and avoidable; the cumulative amount of these losses is staggering and crippling.\"60 In support of their position that malnutrition was widespread, natural foodists frequently pointed to the dramatically increasing sums of money that were being spent on medical and dental services, as well as on over-the-counter medications. They also claimed that nutrient deficiencies were responsible for the apparent rise in the prevalence of mental illness. To counter these disturbing trends, natural food advocates asserted that much more attention had to be given to preventing disease and physical degeneration through improved diet. \"Health can be purchased in a well-organized food culture\u2014it cannot be purchased as a commo dity in hospitals, clinics, or custodial institutions,\" N. Philip Norman argued in a 1947 address to doctors and dentists. He continued: \"In the last analysis, physicians, hospitals, and other medical service institutions are in the business of treating sickness. Until we overcome our distorted therapeutic habituations and cease to think in terms of remedial measures for sick people, our health programs shall be farcical.\"61 Perhaps the greatest area of disagreement between supporters of the natural food movement and health authorities was over the etiology of the degenerative diseases that were becoming the leading killers and disablers in the West during the postwar period. Natural foodists rejected the assertion made by most health authorities that cancer, heart disease, arthritis, and other chronic illnesses associated with old age were on the increase simply because people were living longer. \"There are many approaches to the prevention and treatment of these degenerative diseases,\" physician W. Coda Marin asserted, \"but their 60 Adelle Davis, Let's Eat Right to Keep Fit (New York: Harcourt, Brace & World, 1954), 235. 61 N. Philip Norman, \"Fundamentals of Nutrition for Physicians and Dentists,\" American Journal of Orthodontics and Oral Surgery 33, no. 11 (November 1947): 781. 355 basic causes appear to have a common denominator in nutrition.\"62 Natural food advocates correctly pointed out that almost all the increase in life expectancy that had occurred during the twentieth century had resulted from the prevention and treatment of infectious disease, and that the medical profession had accomplished very little with regard to the chronic degenerative diseases. \"Despite the antibiotics that save so many older people from their one-time dread killer pneumonia,\" Fred Miller wrote in 1958, \"the man in middle life today has scarcely any more chance of reaching 75 than had his father who had to take far greater chances with pneumonia.\"63 Moreover, natural food advocates maintained that chronic disease was afflicting not just the elderly but millions of middle-aged people as well. As Mart in lamented, \"the incidence of degenerative disease is creeping ever deeper into our life span, affecting progressively large numbers of young people and even children.\"64 While the medical community as a whole refused to accept the claim that vitamin and mineral deficiencies caused chronic degenerative diseases without conclusive evidence from controlled, long-term clinical studies, natural foodists had little trouble finding studies in the medical and scientific literature that supported their views.65 Most of these studies fell short of providing conclusive evidence, but their presence in reputable journals gave supporters of the natural food movement reason to believe that their views on chronic disease had scientific legitimacy . Moreover, a handful of mainstream nutrition researchers continued to suggest in the postwar period that much of the illness suffered in the affluent 62 W. Coda Martin, A Matter of Life: Blueprint for a Healthy Family (New York: Devin-Adair, 1964), 85. 63 Miller, Open Door to Health, 177-78. 64 Martin, A Matter of Life, 7. 65 See, e.g., Lovell Langstroth, \"Relation of American Dietary to Degenerative Disease,\" Journal of the American Medical Association 93, no. 21 (23 November 1929): 1607-1613; Agnes Fay Morgan, \"Vitamins and Senescence,\" The Scientific Monthly 52, no. 5 (May 1941): 416-421. 356 West was caused by nutrient deficiencies.66 As physician and leading British natural foodist Kenneth Vickery noted in 1953, \"never a week goes by without the appearance of some published paper on the effects of doses of one or more of the so-called vitamins or other nutrient on some well- or lesser-known disease with varying degrees of success. Most certainly, in recent years, an ever-increasing number of established diseases have been wholly or partially ascribed to nutritional deficiencies by experts in orthodox curative medicine.\"67 To further substantiate their expansive view of the relationship between nutrition and health, natural food advocates also relied on the widely accepted proposition that a well-nourished body could better withstand the effects of aging, illness, and stress. Between the late 1910s and 1930s, as we saw in chapter one, nutrition researchers such as Elmer McCollum and Henry Sherman had determined that a diet judged to be adequate because it enabled laboratory rats to grow and reproduce could be made more nutritious by up to a fourfold increase in its content of vitamins, minerals, and high-quality protein. They found that rats fed such a diet grew more quickly, acted more vigorously, aged more slowly, and lived longer than rats eating a so-called adequate diet. McCollum and Sherman publicly stated that these experiments proved that humans too could enjoy optimal health if they consumed a vitamin- and mineral-rich diet. While health authorities embraced the teachings of McCollum and Sherman in principle, they were reluctant to apply the results of animal- 66 See, e.g., Franklin Bicknell and Frederick Prescott, The Vitamins in Medicine, 2nd ed. (London: Heinemann, 1946); Tom Spies, Rehabilitation through Better Nutrition (Philadelphia, London: W. B. Saunders & Co., 1947); H. D. Kruse, \"A Concept of the Etiological Complex of Deficiency States with Especial Consideration of Conditions,\" Milbank Memorial Fund Quarterly 27, no. 1 (January 1949): 5-97; Roger Williams, Biochemical Individuality: The Basis for the Genotrophic Concept (New York: John Wiley & Sons, 1956); Michael G. Wohl and Robert S. Goodhart, eds., Modern Nutrition in Health and Disease: Dietotherapy (Philadelphia: Lea & Febiger, 1955); Robert S. Goodhart, Nutrition for You (New York: Dutton, 1958). 67 K. O. A. Vickery, \"Positive Health and the Relationship of Man to His Living Environment,\" Journal of the Royal Society for the Promotion of Health 73 (1953): 232. 357 feeding experiments in formulating their recommended nutrient intakes, relying instead on the lower values derived from research on humans. Natural foodists, meanwhile, were completely convinced of the benefits of \"optimal nutrition,\" and they accordingly suggested vitamin and mineral intakes much higher than their mainstream counterparts. Natural food advocates also frequently pointed to the work of Robert McCarrison and Weston Price, whose field studies of isolated primitive groups appeared to support the notion that the consumption of a diet consisting of vitamin- and mineral-rich natural foods produced resistance to infection, excellent physical development, and freedom from degenerative diseases. As a corollary to their unconventionally high standards for nutritional adequacy, natural foodists believed that the destruction of vitamins and minerals that took place during the processing and storage of foods made it very difficult for almost everyone in modern industrialized societies to secure a nutritious diet. The physicians and dentists who founded the natural food movement had expressed this view in the 1920s and early 1930s, even though they had little supporting evidence beyond animal-feeding experiments, scattered observations of \"primitive\" groups, and their own clinical experience. Then, as described in previous chapter, in the late 1930s and early 1940s many leaders of the scientific and medical communities became convinced that a substantial proportion of the populations in the United States and Britain did not consume sufficient vitamins and minerals despite consuming more than enough calories, because they ate too much white flour, white sugar, and margarine. This emerg ing consensus gave the founders of the natural food movement reason to believe that their views were scientifically credible. The physicians and dentists in the natural food movement, however, viewed the consumption of processed foods as a much greater problem than did most nutrition 358 authorities, and they therefore recommended that the consumption of processed foods be avoided almost entirely. Some natural foodists even believed that people should drink certified raw milk instead of pasteurized milk on the grounds that pasteurization destroyed some of the vitamins found in milk. Raw milk advocates in the natural food movement also hypothesized, based on several animal-feeding experiments, that nutritionally important enzymes and possibly other as-yet-unidentified factors were destroyed by pasteurization.68 Natural foodists were also highly critical of foods enriched with synthetic vitamins and minerals, claiming that they were nutritionally inadequate and potentially harmful. As Lord Douglas of Barloch's opined in a 1953 presentation: There should be a legal prohibition of the substitution of artificial substances for natural foodstuffs. Some may consider that this requirement is too drastic, but the fact that we have discovered during recent years that the essential qualities of foods are far more numero us and complicated than had ever been suspected, should make us extremely chary of authorizing the use of any synthetically-produced chemical instead of a naturally-grown article of food.69 68 The experiments of Francis M. Pottenger, Jr., using cats were particularly influential in the postwar-era raw milk movement. See, e.g., F. M. Pottenger, Jr. and D. G. Simonsen, \"Heat Labile Factors Necessary for the Proper Growth and Development of Cats,\" Journal of Laboratory and Clinical Medicine 25, no. 6 (December 1939): 238-40; Francis M. Pottenger, Jr., \"The Effect of Heat-Processed Foods and Metabolized Vitamin D Milk on the Dentofacial Structures of Experimental Animals,\" American Journal of Orthodontics and Oral Surgery 32, no. 8 (August 1946): 467. Proponents of raw milk in the health food movement, including Pottenger, also frequently drew on a series of experiments conducted in the 1930s and 1940s by Rosalind Wulzen and Alice M. Bahrs of Oregon State College. Using the guinea pig as a test animal, Wulzen and Bahrs identified what they called an \"anti-stiffness factor\" found in unpasteurized cream, uncooked leafy green vegetables, and raw cane sugar juice that was destroyed by heating. They observed that guinea pigs deprived of very small quantities of this \"factor\" developed wrist stiffness, tissue calcification, and collagen necrosis. Other research groups, however, could not confirm their findings. For a review of the work of Wulzen and her colleagues and the debate that ensued, see Hugo Krueger, \"The Wulzen Dystrophy Syndrome in Guinea-Pigs,\" American Journal of Medicine 34, no. 1 (February 1955): 185-209. In the 1950s, research groups at Wisconsin and Cornell observed that soft tissue calcification in guinea pigs and cotton rats of the type described by Wulzen was likely due to magnesium deficiency. These findings suggested that the \"Wulzen dystrophy syndrome\" was, in fact, two distinct conditions produced by deficiencies of magnesium\u2014a common problem at that time in highly herbivorous animals fed laboratory chow\u2014and the \"grass juice factor\" identified in the 1930s by G. O. Kohler, C. A. Elvehjem, and E. B. Hart at Wisconsin. See idem, \"Growth Stimulating Properties of Grass Juice,\" Science 83, no. 2158 (8 May 1936): 445. 69 Douglas of Barloch, \"Soil, Food and Health,\" Journal of the Royal Society for the Promotion of Health 73 (1953): 228. 359 Health authorities, by contrast, maintained that individuals could obtain adequate nutrition if they chose their foods wisely, and that the chief obstacles to proper food selection were nutritional ignorance and a limited food budget. They believed that the poor were much more likely to suffer deficiencies than those in the middle and upper classes, who could afford a healthy diet if they knew which foods to purchase and desired to buy them. But health authorities never opposed the consumption of all processed foods, such as pasteurized milk or canned and frozen foods, which retained a large percentage of their natural nutritional content. And, as noted earlier, very few nutrition scientists opposed enrichment after health authorities in the United States and Britain determined that there was insufficient evidence to support legislation mandating the production of \"brown\" or unrefined flour and cereals. Leading nutritionists such as Elmer McCollum and John Boyd Orr even tolerated the consumption of some refined foods, so long as the rest of the diet consisted of the right quantities and kinds of \"protective foods\" to assure a satisfactory intake of vitamins and minerals. Despite their conviction that nutritional deficiencies were the cause of most illnesses, the physicians and dentists in the natural food movement did not generally endorse the use of synthetic vitamin preparations to compensate for the shortcomings of the modern food supply. They were concerned that people who depended on dietary supplemen ts would deprive themselves of the wide array of nutrients, known and unknown, present in natural foods. Natural foodists also suspected that manmade dietary supplements might contain unhealthful amounts and combinations of nutrients, leading to possible nutritional imbalances or toxic reactions. Nevertheless, some popularizers of the natural food movement such as Adelle Davis, Catharyn Elwood, and Carlton Fredericks condoned the use 360 of vitamins, and also recommended the daily consumption of natural dietary supplements\u2014including powdered milk, wheat germ, brewers' yeast, desiccated liver powder, and cod liver oil\u2014that contained unusually large amounts of vitamins or minerals in relation to their caloric value. But the physicians and dentists who founded the natural food movement had a less enthusiastic view of these dietary supplements, for they always emphasized that eating liberal quantities of minimally processed dairy products, meat , grains, and fruits and vegetables was the key to optimal health. The medical professionals in the movement who did condone the use of supplements maintained, like their orthodox counterparts, that they should be used only as a temporary restorative measure under professional supervision. Daniel Thomas Quigley, for example, devised concentrates for his patients that contained a mixture of synthetic and naturally derived vitamins and minerals, which he believed were necessary\u2014alongside improved diet\u2014to bring what he called \"deficiency diseased persons\" back to health.70 In addition to their exhortations to avoid processed foods, many natural foodists also urged consumers to seek out foods that were grown on highly fertile soils. As we saw in chapter three, soil-nutrition relationships became a topic of interest in the 1930s to conservationists, who were concerned that soil erosion and declining fertility in agricultural soils might be contributing to widespread malnutrition in North America and Europe. By the 1950s, however, most agricultural scientists had come to the conclusion that soil fertility was crucial in determining the types of food and forage crops could be grown, but not, to any great extent, the nutritional quality of these crops. Nevertheless, studies carried out in the 70 D. T. Quigley, \"The Prevention of Recurrence of Peptic Ulcer,\" Nebraska State Medical Journal 30, no. 4 (April 1945): 118. Here my analysis differs somewhat from Michael Ackerman, who argues that the physicians in the health food movement did not endorse the therapeutic use of vitamins. See idem, \"Science and the Shadow of Ideology in the American Health Foods Movement,\" in The Politics of Healing: Histories of Alternative Healing in Twentieth-Century North America, ed. Robert D. Johnston (Routledge: New York & London, 2004), 59. 361 1930s and 1940s indicated that variations in soil fertility and fertilization levels could influence the protein and mineral content of crops. While scientists ultimately determined that these variations were insignificant from a public health perspective, natural foodists insisted that such complacency was unwarranted. Moreover, many natural foodists argued that the increasing dependence of farmers on chemical fertilizers and pesticides in the postwar period was not a sign of agricultural progress, but of declining vigor and health in crops that could not meet their nutritional needs from the soil. Animals and people fed on these crops, they further claimed, would also be prevented from enjoying optimal health.71 In addition, some natural foodists promoted an unorthodox approach to agriculture called \"organic farming.\" This form of farming probably first got its name from Lord Northbourne in his 1940 book, Look to the Land (1940), in which he outlined the idea of the farm as an \"organic\" whole in a metaphorical sense\u2014to refer to a complex interrelationship of parts similar to a living organism. But organic farming methods had developed independently in the 1920s and 1930s from the ideas of Albert Howard, a controversial British agricultural scientist who spent most of his career in India, and Rudolf Steiner, an Austrian esoteric philosopher who founded a spiritual movement known as \"anthroposophy.\" These two men and their followers contended that crops grown with the aid of natural, organic fertilizers were nutritionally superior to those grown with 71 See, e.g., Jonathan Forman and O. E. Fink, eds., Soil, Food and Health (Columbus, OH: Friends of the Land, 1949); James Asa Shield, \"The Relationship of Soil Fertility and Psychic Reactions,\" Virginia Medical Monthly 72 (March 1945): 114-20; Firman E. Bear, \"Fertilizers and Human Health,\" Better Crops with Plant Food Magazine 1, no. 2 (1947); Lee Van Derlinden, \"Fertile Soil Will Help Fight Nation's Ills: Malnutrition,\" Better Fruit Magazine (November 1946); L. W. Blau, \"The Effects of Soil Erosion, Loss of Soil Fertility, Storage, Transportation, and Processing on the Nutritional Value of Food,\" The Texas Academy of Science Conservation Council, Monograph no. 3 (January 1949); N. Philip Norman, \"Soil, Food and Health,\" American Journal of Digestive Disease 16, no. 2 (February 1949); Calvin W. Woodruff, \"Nutrition in Relation to the Soil,\" Southern Medical Journal 44, no. 2 (February 1951): 161-65; Joe Nichols, \"A Concept of Totality,\" in Natural Food and Farming Digest, 6-8. 362 commerci ally produced chemical fertilizers. Some proponents of this claim argued that humus contained nutritionally valuable substances that were not present in chemical fertilizers, which were mainly composed of highly purified inorganic compounds.72 \"Even when junk foods are avoided,\" Adelle Davis claimed, \"it is difficult to meet all body requirements unless one can obtain produce grown organically on fertile soils.\"73 Proponents of organic agriculture also believed that many chemical fertilizers harmed soil biota and produced nutritionally unbalanced crops. They thus urged farmers to stop using these types of fertilizers and to work instead on increasing the humus content of their soil by applying composts derived from vegetable and animal wastes. Some natural foodists, however, thought that organic farming advocates had gone too far in rejecting the use of inorganic fertilizers to improve soil fertility. As Karl Mickey argued in Health from the Ground Up (1946), \"intelligent soil treatment consists of the maintenance of a balance between the organic and inorganic portions of the soil.\"74 Finally, natural foodists were far more troubled than health authorities by the growing presence of synthetic chemicals in foods in the postwar period. Prior to World War II and in the years following the war, the chemical industry had begun manufacturing hundreds of novel colorants, preservatives, extenders, texture improvers, and other food 72 A. M. Scofield, \"Organic Farming\u2014The Origin of the Name,\" Biological Agriculture and Horticulture 4 (1986): 1-5; Sir Albert Howard, An Agricultural Testament (London: Oxford University Press, 1940); Lady Eve Balfour, The Living Soil (London: Faber and Faber, 1943); Lionel James Picton, Feeding (New York: Devin-Adair, 1949); Philip Conford, Origins of the Organic Movement (Edinburgh: Floris Books, 2001); Rudolf Steiner, Spiritual Foundations for the Renewal of Agriculture: A Course of Lectures Held at Koberwitz, Silesia, June 7 to June 16, 1924, trans. Catherine E. Creeger and Malcolm Garnder (Kimberton, Farming 1993); Ehrenfried Pfeiffer, Bio-dynamic Farming and Gardening: Soil Fertility, Renewal and Preservation (New York: Anthroposophic Press; London: Rudolf Steiner Publishing Co., 1938). 73 Davis, Let's Eat Right to Keep Fit, 223. 74 Mickey, Health from the Ground Up, 56. Similar \"moderate\" views can be found in, e.g., \"Organics Only? A Symposium of Opinions Aroused by J. I. Rodale's Recent Book, Pay Dirt, and by Further Recent Detractors of 'Devil's Dust',\" The Land 5, no. 1 (Winter, 1945-6): 45-61; Louis Bromfield, Malabar Farm (New York: Harper and Brothers, 1948), 274-94. 363 additives. Many other food additives, such as chemical residues from packaging material, found their way into the food supply unintentionally. Another type of \"incidental\" or \"acci dental\" food additive was the residues of pesticides and herbicides, many of which had just recen tly been developed. By the late 1940s and early 1950s, a number of scientists and public health officials had begun raising concerns about the potential adverse health effects from all these new chemicals, and called for legislation requiring that all new food additives be proven safe prior to their use.75 But official fears about the presence of food additives abated following the passage of laws in the 1950s and 1960s mandating testing to ensure that these substances were safe for people to ingest.76 Natural foodists, however, remained unconvinced that the kinds of tests used to assess the safety of food additives were adequate. As Francis Pottenger, Jr. of the AAAN argued: Befo re we accept the food technologists' product, let us make sure that it has been proven just as good. Let us make sure that he does not use increas e of weight as his criterion of excellence of growth, but that he uses homogeneity of offspring, and excellence of physical and biological performance as the criteria. These studies cannot be run in 30 days or even one year. It will require years before a synthetic product can be proved. The technologist says this will block progress. No, it will not block progress. It might save a large segment of the human race from much illness and unhappiness.77 Moreover, natural foodists maintained that most of the chemicals used in the production of food were completely unnecessary. Chemical additives, in their view, only increased the palatability and shelf appeal of nutritionally impoverished processed foods. Many natural 75 See, e.g, Kenneth P. Dubois, \"Food Contamination and the New Insecticides,\" Journal of the American Dietetic Association 26, no. 5 (May 1950): 325-28; Edward Mellanby, \"The Chemical Manipulation of Food,\" British Medical Journal 2, no. 4736 (13 October 1951): 863-69; E. C. Dodds, \"Chemicals and no. 6773 (20 June 1953): 1211-14; \"Viewpoints on the Problems of Chemicals in Foods,\" American Journal of Public Health 44, no. 8 (August 1954): 977-93; Leonard A. Maynard, \"How Should the Nutritionist Regard Additives?\" Journal of the American Dietetic Association 31, no. 4 (April 1955): 329-32. 76 George P. Larrick, \"The Pure Food Law,\" in Food: Yearbook of Agriculture, 1959, 444-51; Ackerman, \"Interpreting the 'Newer Knowledge of Nutrition',\" 651. 77 Francis M. Pottenger, Jr., \"A Fresh Look at Milk,\" Modern Nutrition 15, no. 11 (November 1962): 21. 364 foodists also believed that crops grown in highly fertile, humus-rich soils were vigorous and healthful enough to resist attacks by pests, and therefore did not require the chemical pesticides that eventually found their way in minute quantities into conventionally grown foods. Notwithstanding their belief that modern food processing and industrial agriculture were harmful to human health, natural foodists did not reject all new technologies. In general, they supported the use of any technology that made natural foods cheaper, more readily available to consumers, or easier to produce, provided that it did not impair the nutritional quality of the product. They looked favorably, for example, on refrigeration and freezing, the fortification of foods with nutrient-rich natural foods such as dried milk and soy flour, and the mech anization of farming and compost production. In their book Tomorrow's Food (1947), James Rorty and physician N. Philip Norman expressed the technological optimism that was common among postwar-era natural foodists: \"Agronomic and nutritional science plus food technology can banish both hunger and malnutrition from the experience of our people almost as completely as bacteriologic science and public health administration banished yellow fever and cholera\u2014and this within a comparatively few years.\"78 Natural foodists were opposed, rather, to how the food industry used technology to produce nutritionally impoverished processed foods. \"Unfortunately,\" asserted Adelle Davis, \"there is an ever-growing list of so-called foods marketed solely to make money, their purveyors having not the slightest concern with the resulting illnesses they produce even in infants and children.\"79 78 James Rorty and N. Philip Norman, Tomorrow's Food (New York: Prentice-Hall, 1947), 6. 79 Davis, Let's Eat Right to Keep Fit, 222. 365 The stance of natural foodists to the food industry, as on many other issues, was not very different from those of many leading nutritionists in the interwar period. \"Advice concerning the most satisfactory type of diet,\" Elmer McCollum and Ernestine Becker asserted in one of their popular diet books written during that time, \"must ignore financially vested interests in the production and mark eting of any particular products.\"80 But following the implementation of cereal and margarine enrichment and the general improvement in diets in the postwar period, nutrition authorities became markedly less antagonistic toward the food industry. In addition, the number of inaccurate or exaggerated nutritional claims made for food products declined during that time, indicating to scientists that the food industry was gradually becoming more research-minded.81 It did not take long after the emergence of the natural food movement in the late 1930s and early 1940s for health authorities to begin attacking it as a form of \"food faddism\" and \"nutritional quackery.\" In the United States, where the natural food movement was particularly well organized, nutrition scientists, physicians, government officials, and representatives of the food industry came together in the 1950s to launch a concerted legal and educational campaign to counter its spread. They were not concerned by the fact that natural foodists promoted \"health foods\" such as organic fruits and vegetables and wholegrain bread, since these were nutritionally good foods, but rather by the educational and economic hazards that natural foodists' teachings imposed on the public. By raising unwarranted fears about the purity and quality of the conventional food supply, health authorities argued, natural foodists created a market for the pills, special-purpose foods, and spurious literature of charlatans and nutrition quacks. These products were often sold at 80 McCollum and Becker, Food, Nutrition and Health, 5th ed., 105. 81 Rita S. Rosenberg, \"Nutritional Claims in Food Advertising,\" Journal of the American Dietetic Association 32, no. 7 (July 1956): 631-35. 366 exorbitant prices to misguided, desperate people in search of health whose money would have been better spent on a well-chosen, varied diet of normal foods from the grocery store and on adequate medical care from properly trained physicians.82 At the same time that the campaign against the natural food movement was ramping up in the 1950s, nutrition scientists became more circumspect about how they transmitted their findings to the wider public. Elmer Nelson of the US Food and Drug Administration (FDA), for example, contended in a 1954 presentation that food faddism continued to thrive in part because nutritionists had not communicated their research with sufficient caution. He advised: If a scientist has made a discovery which may be of practical importance to the layman, he should not feel that he has completed his job when he has reported his findings to a scientific body. He should carefully check his manuscript to see that he has not made statements that may be readily lifted from the text and used in improperly interpreting his work; he should make clear the practical limitations of his contributions; and he should not indulge in speculations with respect to the practical application of his findings.83 Nutrition scientists also reminded their colleagues that they had to assure the internal integrity of their professions. \"When an individual who has at one time been considered a reputable professional person becomes a puppet for the pseudo-scientists, nutritionists and dieticians find it exceedingly difficult or impossible to successfully combat the information he spreads,\" noted Adelia Beeuwkes of the University of Michigan. \"It would be well,\" she 82 See, e.g, \"Symposium: What Can We Do About Food Faddism?\" Federation Proceedings 13, no. 3 (September 1954): 780-96; Joseph N. Bell, \"Let 'em Eat Hay,\" Today's Health, September 1958; \"Food Facts Versus Food Quacks,\" Agricultural and Food Chemistry 7, no. 2 (February 1959): 144; AMA, Proceedings: National Congress on Medical Quackery, October 6-7, 1961 (Chicago: AMA, 1961). 83 Elmer M. Nelson, \"Control of Nutrition Claims Under the Food, Drug, and Cosmetic Act,\" Federation Proceedings 13, no. 3 (September 1954): 792. 367 recommended, \"for all professional groups to be continuously alert, to be certain that their professional house is in order.\"84 Some nutritionists also sought to distance themselves from the enthusiasm that had characterized their profession in the 1930s and early 1940s. Berk eley nutrition researcher Ruth Huenemann, for instance, felt that it was not difficult for trained scientists to express viewpoints that could be considered \"faddist.\" As she noted at a meeting of the American Dietetic Association in 1958, \"Perhaps we have all, on one occasion or another, been guilty of 'exaggerated zeal' in regard to a particular nutrient or a particular food.\" Huenemann admitted that she had probably overemphasized the importance of vitamin C in her teaching when she was doing her research on the substance, and she wondered if during World War II \"most of us didn't become rather unduly zealous about enriched bread.\"85 H. William Sebrell, one of the leaders of the enrichment movement in the early 1940s, made a similar observation: \"In the pursuit of knowledge, in the acceptance of hypotheses and sometimes conclusions, the scientist himself is not immu ne to faddism. He should be carefu l not to impart his unfounded enthusiasms to the public.\"86 The response of the physicians and dentists in the natural food movement to the official campaign against their views was varied. To some natural foodists, the intensifying persecution that they experienced in the 1950s only confirmed their long-held belief that the field of nutrition science had become economically subservient to the processed-food industries. As Royal Lee, a non-practicing dentist and vitamin manufacturer, declared in a 1955 article: \"There is a frightful conspiracy to keep the public in the dark about the 84 Beeuwkes, 786. 85 Ruth L. Huenemann, \"Combating Food Misinformation and Quackery,\" Journal of the American Dietetic Association 32, no. 7 (July 1956): 623. 86 W. H. Sebrell, \"Food Faddism and Public Health,\" Federation Proceedings 13, no. 3 (September 1954): 784. 368 devastating death-dealing effects of modern food counterfeits, the synthetic glucose, the synthetic hydrogenated fats, the refined cereals, the refined breakfast foods, the coal tar dyes and coal tar flavors that insure acceptance of otherwise tasteless and colorless food frauds which destroy human life to the tune of over a million victims a year.\"87 But most of the physicians and dentists in the natural food movement were caught off-guard by the charges of food faddism and nutritional quackery leveled against them, since they believed that their nutritional claims had been substantiated by scientific research . This sense of confusion is illustrated in a 1962 article by physician Jonathan Forman entitled \"The Natural Food Movement.\" (This was probably the first time that this term was used by one of the movement's supporters.) Forman expressed his appreciation to the AMA and FDA for going after \"those who sell useless things for the prevention and cure of disease,\" but he was frustrated that these organizations were \"lumping all of us with the quacks and crackpots who believe in the health-giving quality of foods, foods rich in balanced supply of all the essential nutrients, having been grown on soil of optimal fertility; foods unpoisoned by pesticides and herbicides; foods unspoiled in the harvesting, storing or processing from the time they leave the field until they arrive on our tables in properly balanced menus.\" He urged fellow members of what he termed the \"natural food movement\" to calmly support their claims with solid scientific research, and he cautioned against displaying the emotionalism that he felt was all too common when ideologies clashed over nutrition and health. This tendency to become argumentative, Forman felt, only helped to reinforce the stereotype that the natural food movement was pervaded by \"anti-intellectualism.\"88 87 Royal Lee, \"Who Does the Law Protect?\" Natural Food and Farming 2, no. 5 (August 1955): 16. 88 Jonathan Forman, \"The Natural Food Movement,\" Natural Food and Farming 9, no. 1 (January 1962): 21-29. 369 As Forman's commentary suggests, the natural foodists who strove for professional respectability in the 1950s and 1960s found themselves attempting a difficult balancing act. They were sympathetic, on the one hand, with official efforts to crack down on the distribution of fraudulent products and nutrition misinformation by those untrained in medicine or science. But they felt, on the other hand, that health authorities had gone too far in lumping together all the critics of the conventional food supply and eating habits as quacks and faddists. \"We have been confronted with an extraordinary campaign of attempts at book banning, sweeping allegations of quackery and fraud,\" remarked physician Michael Walsh in 1966, \"in the course of which the American Academy of Applied Nutrition and the American Nutrition Society have become tarred with the same brush and identified with so-called quacks.\" The sudden dampening of enthusiasm for nutrition among health authorities in the postwar years was painfully palpable to Walsh. Born in Ireland, he completed his academic and scientific education at the University College, Dublin and the Royal Institute of Chemistry of London. After moving to the United States, Walsh served the medical and dental professions as an itinerant nutrition consultant for the University of California Extension Division from the 1930s to the 1950s. He also oversaw the rapid expansion of the AAAN in the immediate postwar years, before it ran afoul of mainstream scientific opinion. Reflecting on the field of nutrition education at the end of his career in the mid-1960s, Walsh observed that, after a promising period of advance between 1935 and 1950, there was a \"regression,\" as he called it, between 1950 and 1965. The major reason for this shift, he believed, was the influx of relatively young medical doctors who entered the field after World War II. Because these doctors were mainly concerned with building up their practices quickly and making a \"fast buck,\" they changed the culture of the medical profession for the worse. Walsh also pointed out that the advent of the so-called wonder 370 drugs, particularly antibiotics and tranquilizers, \"probably did more to divert the medical profession away from consideration of nutrition than any other factor.\" In dentistry, the official acceptance of fluoridation around 1950 rapidly turned the attention of the profession away from nutrition as well. Finally, Walsh placed blame on \"drugstore nutrition in the form of 900-calorie reducing gimmicks\" and the \"vast flood of spurious inaccurate popular books on nutrition\" for turning doctors and dentists against nutrition education after 1950.89 Other physicians in the natural food movement besides Walsh took pains to distance themselves from the untrained enthusiasts and vitamin pitchmen with whom they were commonly associated. N. Philip Norman identified himself as a \"non-conforming medical practitioner,\" but he also stressed that he was a member of the New York County and State Medical Societies, the AMA, and a number of other orthodox medical organizations. \"While I am liberal minded and congenitally optimistic,\" he asserted, \"I cannot subscribe to chiropractors, naturopaths, herbalists, amateur 'doctors' and others who want professional fees from the public without formal medical training.\" Norman also derided the unconventional dietary regimens propounded by \"health food\" gurus such as Jerome Rodale and Gayelord Hauser, who had no professional training in medicine or science: \"My patients are not advised to run around with their pockets filled with sunflower and sesame seeds, kelp or yeast tablets, raw cashew nuts, etc. Nor do I have a special formula for Panther or Dragon's milk, etc.\"90 Norman's sentiments were echoed by another physician associated with the natural food movement, H. Curtis Wood, who lamen ted the fact that most of the writing on vitamins and minerals \"has been done by food faddists, who frequently suggest 89 Michael J. Walsh, \"The Changing Image,\" Journal of Applied Nutrition 19, nos. 1 and 2 (1966-67): 12. 90 Norman, The \"Triad Disease,\" xix-xx. It is interesting to note that Norman continued to believe in the harmfulness of traditional \"mixed meals\" consisting of foods rich in protein and carbohydrates until the end of his career, well after the notion was dismissed as a form of food faddism. 371 many rather bizarre diets and who lack sufficient medical background to properly evaluate and discuss such a complicated subject.\"91 Even nutritionists and physicians who did not identify closely with the natural food movement, but who found a receptive audience in natural foodist circles, found themselves struggling to maintain their scientific reputability. Peter Cleave, for example, allowed the republication of \"The Neglect of Natural Principles in Current Medical Pract ice\" in two magazines associated with the postwar-era natural food movement, Prevention and the Journal of Applied Nutrition. Prevention, still in print today, was then the health lifestyle magazine of the publishing empire of Jerome Irving Rodale, one of the most important figures in popularizing the organic food and farming movemen t when it was in its infancy in the 1940s and 1950s. The Journal of Applied Nutrition, already mentioned earlier in this chapter, was started by the American Academy of Applied Nutrition shortly after World War II. The appearance of Cleave's paper in these publications garnered an enthusiastic response and a flood of requests for reprints from readers, but alarmed his publisher in Britain, John Wright & Sons of Bristol. So when the question arose over how Cleave should promote Fat Consumption and Coronary Disease (1957), one of his earliest books to claim that it was refined carbohydrates and not animal fat that was at the root of heart disease, he was told to take a more cautious approach. \"We feel that the best plan would be to have an article in The Lancet and leave it at that,\" he was advised. \"We do not feel happy about any association, however loosely, with natural health people,\" his publisher continued, \"not 91 H. Curtis Wood, Jr., Overfed But Undernourished: Nutrition Aspects of Health and Disease (New York: Exposition Press, 1959), 5. 372 because we necessarily think they are eccentric but because of the sharp dividing line that exists in this country between the qualified and the unqualified.\"92 But as we have seen, the defenders and critics of the dietary status quo in the two postwar decades were not neatly divided between \"the qualified and the unqualified.\" It would be more accurate to describe what happened during that time as a polarization process, wherein nutrition experts distanced themselves from the enthusiastic view of vitamin- and mineral-rich natural foods that had characterized their field between the 1910s and early 1940s, while a small dissident group of physicians, dentists, and researchers embraced an extreme interpretation of this view. Yet despite the disagreements that developed between these two groups on a number of key points\u2014the use of pesticides, the pasteurization of milk, the importance of consuming wholegrain foods, to name a few\u2014their dietary advice was remark ably similar. Mainstream nutritionists and natural foodists both recommended a diet consisting of liberal quantities of dairy products, eggs, meats, fresh and cooked vegetables, raw and dried fruits, and organs occasionally\u2014that is, a diet not altogether different from what nutrition experts had been promoting since the 1910s and 1920s. But events in the field of cardiovascular disease assured that much of this dietary advice would not survive in the succeeding decades. The Great Aberration: The Rise of the Fat-Cholesterol Hypothesis Around the same time that the conflict between health authorities and the natural food movement was reaching its high point in the 1950s, an influential group of researchers began convincing physicians and the public that many of the foods deemed \"protective\" in 92 Letter (9 February 1959) to Cleave from John Wright & Sons Limited, PP/TLC/C1/6, T. L. Cleave Papers, Wellcome Library Archives and Manuscripts, London, UK. 373 the interwar period were artery-clogging agents of death. In the years following World War II, health authorities and the press began talking up an emerging heart disease \"epidemic,\" which by that time had become one of the leading causes of death in the United States and Britain. In atherosclerotic heart disease, the arteries that supply blood and oxygen to the heart\u2014known as coronary arteries because they descend on the heart like a crown\u2014are restricted or blocked by a buildup of cholesterol and fatty deposits (or plaques). If the blood flow is arrested entirely, the result is a heart attack, or, in technical terms, a myocardial infarction. Partial blockages will starve the heart of oxygen, leading to a condition known as ischemia. A heart attack is most commonly caused by a blood clot, or thrombosis, typically where the arteries are already narrowed by atherosclerosis.93 Following the discovery that atherosclerotic plaques consist primarily of cholesterol, researchers reasoned that dietary cholesterol might contribute to heart disease. Cholesterol is found only in foods of animal origin\u2014eggs yolks and organ meat s contain particularly large quantities, although it is also present in muscle meats and dairy products. The evidence initially cited in support of the argument that dietary cholesterol contributed to atherosclerosis came from research on animals. In 1913, Russian scientists Nikolai Anitschkov and Semen Chalatov reported that they could induce lesions in rabbits closely resembling atherosclerosis in humans by feeding them pure cholesterol dissolved in sunflower oil. In the years following Anitschkov and Chalatov's rabbit experiments, researchers produced atherosclerosis in a variety of other animals by raising their cholesterol levels. But the findings from these animal-feeding experiments were somewhat inconsistent. In atherosclerotic rabbits, for example, their plaques never ruptured and they never got heart attacks. Moreover, in the case of highly carnivorous animals such as dogs, the feeding of 93 Taubes, Good Calories, Bad Calories, 5-6. 374 cholesterol alone did not produce atherosclerosis, because they can covert the cholesterol into bile acids. Research ers were able to induce atherosclerosis in cholesterol-feeding experiments with dogs only after discovering that inhibiting thyroid hormone would stop dogs from making this conversion. Indeed, as Anitschkov himself argued, it was not the mere feeding of cholesterol to rabbits that produced atherosclerosis, but the overwhelming of their capacity to use and dispose of that cholesterol. Human atherosclerosis, Antischkov and others concluded, was probably a disease of cholesterol metabolism rather than the result of excessive dietary cholesterol intake. The question of whether cholesterol-feeding studies in animals were relevant to humans thus remained open.94 One of the first researchers to discredit the notion that dietary cholesterol caused atherosclerosis in humans was Ancel Keys, director of the Laboratory of Physiological Hygiene at the University of Minnesota. Keys became famo us early in World War II by formulating the K-ration for combat troops. During the war, he also conducted an influential series of experiments on the effects of vitamin supplementation, which was an issue that was being hotly debated at that time. Keys and his associates at the University of Minnesota sought to determine whether the administration of several of the B-complex vitamins had any effect on physical performance, appetite, morale, and general wellbeing in a group of soldiers. Contrary to other contemporaneous studies that were being conducted, the Minnesota group found no evidence that vitamin supplemen tation had any benefit. In the later years of the war, Keys also directed the seminal study of human starvation, using conscientious objectors as his subjects. It was hoped that the findings from the experiment could be applied in liberated areas where the population had been forced to live on semi- 94 Nikolai Anitschkow, \"Experimental Arteriosclerosis in Animals,\" in Arteriosclerosis: A Survey of the Problem, E. V. Cowdry ed. (New York: Macmillan, 1933), 271-322. 375 starvation diets for extended periods of time. In 1950, Keys and his colleagues published the results of their experiment in a two-volume, fourteen-hundred-page text entitled The Biology of Human Starvation, which became the authoritative source on the subject.95 Shortly after World War II, Keys began focusing his attention on coronary heart disease. In one experiment published in 1950, Keys fed men for months at a time on diets either high or low in cholesterol and determined that it made no difference to the cholesterol levels in their blood. He concluded from his findings that dietary cholesterol had little relevance to heart disease. But based on other studies with human subjects that he had initiated, Keys began to suspect that dietary fat played a significant role in the development of heart disease. He determined from these studies that blood cholesterol concentrations correlated positively with the proportion of total calories consumed as fat. Since blood cholesterol levels tended to be high in individuals with coronary heart disease, Keys inferred that dietary fat was the primary cause of heart disease. Although he had no direct evidence that could explain how high blood cholesterol levels produced atherosclerosis, he nevertheless began recommending that Americans reduce their fat consumption to between 25 and 30 percent of total calories.96 Nutrition authorities remained wary to Keys's ideas. Leonard Maynard, for example, wrote, \"On the basis of inadequate evidence, people are being advised by some physicians, and particularly by popular writers, to avoid or cut down on the use of eggs, butter, milk, and organ meat s. Arteriosclerosis is a serious degenerative 95 Ancel Keys, Austin Henschel, Henry Longstreet Taylor, Olaf Mickelsen, and Josef Brozek, \"Absence of Rapid Physical Deterioration in Men Doing Hard Physical Work on a Restricted Intake of Vitamins of the B Complex,\" Journal of Nutrition 27, no. 4 (June 1944): 485-95; Taubes, Good Calories, Bad Calories, 16. 96 Ancel Keys, \"Human Atherosclerosis and the Diet,\" Circulation 5 (January 1952): 115-18. 376 disease. On the other hand, the animal products in question are of vital importance for an optimal diet.\"97 But in 1953, Keys published an epidemiological study that reinforced his fat-cholesterol hypothesis and had a profound effect on thinking about heart disease for years afterwards. He analyzed data from six countries that showed a direct, almost straight-line positive correlation between mortality from coronary heart disease and percentage of calories from dietary fat. These data showed Japan at the lowest point, with less than 10 percen t of calories as fat, and the United States as the highest point, with 40 percent of calories as fat. \"No other variable in the mode of life besides the fat calories in the diet is known which shows anything like such a consistent relationship to the mortality rate from coronary degenerative heart disease,\" Keys stated in his study. He further pointed out that these cross-cultural comparisons were consistent with historical trends in the United States. \"The present high level of fat in the American diet did not always prevail,\" Keys asserted, \"and this fact may not be unrelated to the indication that coronary disease is increasing in this country.\"98 Keys insisted through the mid-1950s that all fats, of both vegetable and animal origin, elevated cholesterol and were therefore equally culpable in causing coronary heart disease. But a growing number of studies challenged this view. In 1952, a team of Dutch researchers demonstrated in a prolonged feeding experiment with humans that cholesterol levels were independent of the total amount of fat consumed. They showed that cholesterol levels were lowest in the subjects on a vegetarian diet with a high fat content and highest on an animal-fat diet that had less total fat. The same year, Laurance Kinsell, director of the 97 L. A. Maynard, \"An Action Program for Better National Nutrition,\" Nutrition Reviews 9, no. 12 (December 1951): 355. 98 Ancel Keys, \"Atherosclerosis: A Problem in the Newer Public Health,\" Journal of Mount Sinai Hospital, New York 20, no. 2 (July-August 1953): 118-39. 377 Institute for Metabolic Research at the Highland-Alameda County Hospital in Oakland, California, reported that the ingestion of liquid formula diets containing relatively large amounts of vegetable oil consistently resulted in a significant decrease in blood cholesterol levels, and animal fats raised them. Keys eventually changed his stance in response to these new findings, after he managed to replicate them in a 1957 study using patients in a Minnesota mental hospital.99 When Kinsell and his associates first reported their results with diets high in vegetable oil, it was not clear whether the cholesterol-lowering effects were due merely to lack of dietary cholesterol, or caused by a positive effect of some unidentified substance in the vegetable oil. But later in the 1950s, Kinsell and Edward Ahrens of Rockefeller University demonstrated that the crucial factor in controlling cholesterol levels was not whether the fat was animal or vegetable in origin, but its degree of \"saturation,\" as well as what is known as the chain length of the fats.100 An unsaturated fat is a fat or fatty acid in which some of the carbon atoms in the hydrocarbon chain are joined by double bonds. A fat molecule is monounsaturated if it contains only one double bond, and polyunsaturated if it contains more than one double bond. Monounsaturated fatty acids are found in large amounts in olive oil, palm oil, and lard; polyunsaturated fatty acids are found in large 99 J. B. Groen, B. K. Tijong, C. E. Kamminga, and A. F. Willebrands, \"The Influence of Nutrition, Individuality, and Some Other Factors, Including Various Forms of Stress, on the Serum Cholesterol: An Experiment of Nine Months Duration in 60 Normal Human Volunteers,\" Voeding 13 (1952): 556-87; L. W. Kinsell, S. Margen, and G. Michaels, \"Dietary Modification of Serum Cholesterol and Phospholipid Levels,\" Journal of Clinical Endocrinology & Metabolism 12, no. 7 (July 1952): 909-13; Joseph T. Anderson, Ancel Keys, and Francisco Grande, \"The Effect of Different Food Fats on Serum Cholesterol Concentration in Man,\" Journal of Nutrition 62, no. 3 (July 1957): 421-44; Ancel Keys, Joseph T. Anderson, and Francisco Grand, \"Prediction of Serum-Cholesterol Responses of Man to Changes in Fats in the Diet,\" Lancet 273, no. 7003 (16 November 1957): 959-66. 100 E. H. Ahrens, Jr., J. Hirsch, W. Insull, Jr., T. T. Tsaltas, R. Blomstrand, and M. L. Peterson, \"Dietary Control of Serum Lipids in Relation to Atherosclerosis,\" Journal of the American Medical Association 164, no. 17 (24 August 1957): 1905-11; L. W. Kinsell, R. W. Friskey, G. D. Michaels, and S. Splitter, \"Essential Fatty Acids, Lipid Metabolism, and Atherosclerosis,\" Lancet 271, no. 7016 (15 February 1958): 334-39. 378 amounts in highly liquid vegetable oils derived from corn, soybeans, safflower seeds, and sunflower seeds. A saturated fat is a fat or fatty acid in which all the carbon atoms in the hydrocarbon chain are joined by single bonds. A saturated fat has the maximum number of hydrogen atoms bonded to the carbon atoms, and is therefore \"saturated\" with hydrogen. Saturated fatty acids are found in large amounts in fats and oils that are solid at room temperature, such as butter, tallow, and coconut oil. The finding in 1955 that unsaturated fatty acids lowered cholesterol and saturated fats raised it gave birth to the erroneous idea, still widely accepted today, that all animal fats are \"bad\" saturated fats, and all \"good\" unsaturated fats are found in vegetable oils and maybe fish. (In fact, most of the fatty acids typically found in meat are unsaturated, or are converted by the body into unsaturated fatty acids.)101 One of the studies that proved pivotal in vilifying saturated fat was the \"Seven Countries Study,\" a massive international epidemiological project launched by Ancel Keys in 1956. Keys and his collaborators selected about thirteen thousand middle-aged men in sixteen mostly rural populations in the Netherlands, Yugoslavia, Finland, Japan, Greece, Italy, and the United States. The men were given physical examinations at the outset of the study, and the state of their health was assessed periodically thereafter. Investigators analyzed diet and nutrient intakes; meas ured blood pressure, weight, and blood cholesterol levels; and asked participants how much they smoked and exercised. Results were first published in 1970, and showed widely varying mortality rates from heart disease. At the lowest extreme were the inhabitants of the Greek island of Crete, while at the highest extreme were the lumberjacks and farmers in North Karelia, Finland. The other surveyed 101 Mary Enig, Know Your Fats (Bethesda, MD: Bethesda Press, 2000), 9-24. 379 populations fell somewhere in between, with rural Japanese at the lower end and Americans railroad workers at the higher end. Keys drew several interesting conclusions from the Seven Countries Study. He continued to insist, as he had in his earlier six-country analysis, that total cholesterol levels predicted heart disease risk. The lowest cholesterol levels were found among the Japanese, who experienced a far lower incidence of deaths from coronary heart disease than subjects in the United States and Finland, the two countries with the highest average blood cholesterol levels. But the lowest rates of heart disease in the study were found among the residents of Crete, whose average cholesterol levels were between these two extremes. In fact, Cretans were the healthiest participants in the entire study, experiencing the lowest mortality rates from not just heart disease, but from all causes, despite the fact that their diets were roughly 40 percen t fat. To explain this finding, Keys maintained that saturated fat contributed to heart disease, while monounsaturated fat protected against heart disease. According to this updated version of Keys's fat-cholesterol hypothesis, it was the excessive consumption of red meat , eggs, and butterfat in the American and Finnish diets that was to blame for the high rates of heart disease in these countries, while the liberal intake of olive oil kept Cretan villagers relatively free from the disease despite their high-fat diets and only moderately low cholesterol levels. Aside from these seemingly improvisational adjustments to his hypothesis, Keys failed to present several major incongruities in the data from the Seven Countries Study. For example, in the Italian district of Crevalcore, the number of deaths from heart disease was 2.5 times greater than in another district in the same country, Montegiorgio, despite the fact that average blood cholesterol levels and saturated fat intakes were nearly identical in both. People living in the region of North Karelia, in the east of Finland, died five times more 380 often of heart attacks than people living in the area of Turku, in the southwest of Finland, although average blood cholesterol levels and saturated fat intakes differed only slightly between the two populations. Most tellingly, people died five times more often from heart attacks on the Greek island of Corfu than on nearby Crete, even though average blood cholesterol levels were slightly lower on Corfu. In other words, within countries the number of heart attacks showed no demonstrable correlation with diet or cholesterol levels.102 But it was not just Keys's Seven Countries Study that suffered from serious defects. From the very beginning, critics in a wide array of fields pointed out that virtually all the evidence used to support the fat-cholesterol hypothesis was flawed in some way. In 1957, Jacob Yerushalmy, who headed the Biostatistics Department at the University of California, Berk eley, and Herman Hilleboe, the New York State Commissioner of Health, coauthored a critique of Keys's six-country analysis. They noted that Keys had selected only six countries in his comparison of death rates from coronary heart disease and the percentage of total calories consumed as fat, even though the relevant data were available for twenty-two countries. Yerushalemy and Hilleboe found that when the data from all twenty-two countries were included in the analysis, the positive correlation between dietary fat and heart disease mortality became weaker. \"Clearly this tenuous association cannot serve as much support for the hypothesis which implicates fat as an etiologic factor in arteriosclerotic and degenerative heart disease,\" they stated. Yerushalmy and Hilleboe also looked at the relationship between dietary fat and mortality from all other causes besides heart disease. They found that there was a strong negative association between dietary fat\u2014both as the total number and as the percentage of calories consumed\u2014and non-heart-disease mortality. 102 These omissions are discussed in Uffe Ravnskov, The Cholesterol Myths (Washington: New Trends Publishing, 2000), 25-27, 66-72. 381 Furthermore, Yerushalmy and Hilleboe could not identify any statistically significant relationship between dietary fat or animal fat and death from all causes, which was the only criterion that was ultimately relevant in advocating any public health measure. Finally, they reproached Keys for failing to acknowledge the limits of epidemiological observations, which could only identify associations between mortality from heart disease and dietary fat, and not prove the cause of the disease.103 In the same year that Yerushalemy and Hilleboe's paper appeared, another critique of Keys's hypothesis by nutritionist John Yudkin was published in The Lancet. Yudkin analyzed statistical data on coronary heart disease mortality and food consumption from fifteen industrialized countries around the world, and found that the increasing consumption of sugar could more logically explain the association reported by Keys. When Yudkin turned his attention to the data on the consumption of differen t types of dietary fat in these countries, he found that the relationship between animal fat intake and mortality from coronary heart disease was especially weak, and weaker still when only the consumption of butterfat\u2014the type of animal fat highest in saturated fatty acids\u2014was examined. Yudkin also examined the association between coronary heart disease and various mark ers for prosperity. He found that that the strongest predictor of coronary mortality was the level of radio ownership, followed closely by car ownership. Although possession of these consumer goods obviously did not mean that they were the cause of heart disease, it did 103 Jacob Yerushalmy and Herman Hilleboe, \"Fat in the Diet and Mortality from Heart Disease: A Methodologic Note,\" New York State Journal of Medicine 57, no. 14 (15 July 1957): 2343-54. It should be noted that Yerushalmy and Hilleboe identified a statistically significant positive correlation between the consumption of animal fat\u2014both as the percentage and as total number of calories consumed\u2014and heart disease mortality. For this reason, proponents of the fat-cholesterol hypothesis in later years cited Yerushalmy and Hilleboe's paper as supporting evidence for their view, though without noting the less convenient data regarding the negative correlation between animal fat and non-heart-disease-related and all-cause morality. See, e.g., Jeremiah Stamler, \"Lifestyle, Major Risk Factors, Proof and Public Policy,\" Circulation 58, no. 1 (July 1978): 3. 382 suggest that there was an association between the adoption of a more sedentary lifestyle pattern and heart disease. \"A consideration of some of the more readily available data on the incidence of coronary deaths and on food consumption,\" Yudkin concluded, \"makes it difficult to support any theory which supposes a single or major dietary cause of coronary thrombosis.\"104 In the years after the publication of his critique of Keys's six-country analysis, Yudkin became increasingly convinced that it was the rising consumption of sugar that was driving the heart disease epidemic in the West. In 1964, Yudkin published a widely cited analysis of patterns and trends in carbohydrate consumption and disease that clearly implicated the consumption of sugar in the rising incidence of not only heart disease, but also obesity, diabetes, and dental caries.105 Through the 1960s, he also published the results of a series of experiments on a variety of laboratory animals as well as volunteer college students and staff that incriminated high-sugar diets in heart disease and myocardial infarction.106 Yudkin became so convinced of the link between sugar and heart disease, as well as other \"diseases of civilization,\" that he published a popularly oriented book on the subject in 1972 entitled Pure, White, and Deadly in Britain and Sweet and Dangerous in the United States. But Keys and other supporters of the fat-cholesterol hypothesis remained 104 John Yudkin, \"Diet and Coronary Thrombosis: Hypothesis and Fact,\" Lancet 273, 6987 (27 July 1957): 155-62. 105 John Yudkin, \"Patterns and Trends in Carbohydrate Consumption and Their Relation to Disease,\" Proceedings of the Nutrition Society 23 (1964): 149-62. 106 Stephen Szanto and John Yudkin, \"The Sucrose on Blood Lipids, Serum Platelet Adhesiveness and Body Weight in Human Volunteers,\" Postgraduate Medical Journal 45, no. 527 (September 1969): 602-07; John Yudkin, Stephen Szanto, and Intake, Serum Insulin and Platelet Adhesiveness in Men with and without Peripheral Vascular Disease,\" Postgraduate Medical Journal 45, no. 527 (September 1969): 608-11. 383 unconvinced, and were quick to point out the weaknesses in the epidemiological data supporting Yudkin's sugar hypothesis.107 One of the main reasons that the controversy over the role of sugar and saturated fat in causing coronary heart disease persisted was that the intake of these two dietary components within population groups tended to be closely and positively correlated with one another\u2014that is, people as a rule consume more animal products as well as sugar when they become more prosperous. Even Keys's Seven Countries Study, which was one of the few epidemiological projects that actually meas ured sugar consumption levels, showed that sugar predicted heart-disease rates as well as saturated fat did. Increasing prosperity also tended to be associated with higher prevalence of smoking, less physical exercise, and better medical care, all of which could influence heart disease incidence and mortality rates. As Yerushalmy and Hilleboe pointed out in their 1957 critique of Keys's work, \"it may be that the amount of fat and protein available for consumption is an index of a country's development, industrially, nutritionally, medically, and no doubt in other respects as well.\"108 Notwithstanding this problem of isolating variables in epidemiological research, studies of non-industrialized populations eating diets high in animal fat and low in modern processed foods also failed to confirm the fat-cholesterol hypothesis. One of the most comprehensive of these studies was carried out by physician Aharon Cohen of Hadassah University Hospital in Jerusalem. In the latter half of the 1950s, Cohen and his collaborators examined two distinct populations of Yemenite Jews that had immigrated to Israel. One group consisted of \"new immigrant\" Yemenite families, who were questioned on their food 107 Ancel Keys, \"Sucrose in the Diet and Coronary Heart Disease,\" Atherosclerosis 14, 2 (September-October 1971): 193-202. 108 Yerushalmy and Hilleboe, \"Fat and Mortality from Heart Disease,\" 2353. 384 habits while in Yemen. Cohen identified two striking features of the diets mentioned by all of those interviewed. First, the main sources of fat in Yemen had been mutton fat, beef fat, and samne (a type of dehydrated butter). Second, the quantity of sugar used in Yemen had been negligible; the carbohydrates consumed consisted solely or mainly of unrefined starches. The second group examined by Cohen and his associates consisted of \"settled\" Yemenite families who had resided in Israel for over twenty-five years. Although the quantity of animal fat consumed by the settled Yemenite families was about the same as the Yemenite Jews living in Yemen, other significant dietary changes had occurred. In addition to animal fat, the settled Yemenite families also consumed margarine and liquid vegetable oils derived from olive, soy, and sesame. Although the quantity of carbohydrates consumed by both groups was roughly equal, in Israel white sugar accounted for 25 to 30 percent of the total. Yet despite the fact that the total quantity of animal fat and carbohydrate consumption remained relatively unchanged, heart disease and diabetes was much more prevalent among the Yemenites who had been in Israel for a quarter-century or more. Since the levels of physical activity between the two populations appeared to be similar, Cohen concluded that sugar consumption was the one significant dietary difference that might explain this change.109 Cohen's findings were confirmed in other studies of non-Western populations. In 1967, physician S. L. Malhotra published the results of his examination of the incidence of coronary heart disease in over one million male Indian railway employees in the British Heart Journal. He analyzed data on deaths due to ischemic heart disease in different railway zones in the country and compared it with a variety of factors, such as diet, socioeconomic 109 A. M. Cohen, Sarah Bavly, and Rachel Poznanski, \"Change of Diet of Yemenite Jews in Relation to Diabetes and Ishaemic Heart-Disease,\" Lancet 2, no. 7217 (23 December 1961): 1399-1401; A. M. Cohen, \"Fats and Carbohydrates as Factors in Atherosclerosis and Diabetes in Yemenite Jews,\" American Heart Journal 65, no. 3 (March 1963): 291-93. 385 status, physical activity, smoking, and stress. Malhotra found that morality from coronary heart disease was seven times more commo n in Madras in southern India than in Punjab in northern India, despite the fact that the consumption of fat was eight to nineteen times higher in the north than in the south. In addition, most of the fat consumed in the north came from animal sources, particularly dairy products rich in saturated fat, while in the south most of the fat consumed was in the form of vegetable oils derived from seeds.110 Research teams working in East Africa among the Masai, Rendille, and Samburu\u2014nomadic pastoralists who lived primarily on milk and meat\u2014observed that these groups did not suffer from heart attacks or any other symptoms of coronary heart disease, despite the fact that their diets were exceptionally high in saturated fat. Even more incongruously from the point of view of the fat-cholesterol hypothesis was the fact that blood cholesterol levels in these groups varied greatly. Investigators working among Polynesian islanders, whose traditional diet consisted of large quantities of coconut\u2014a food exceptionally rich in saturated fat\u2014also found very little evidence of heart disease until these populations began consuming modern refined foods. It was only after they began consuming white flour, sugar, jam, lard, and other \"trade foods\" in large quantities that these groups became afflicted by heart disease, as well as diabetes, dental decay, and other chronic diseases.111 110 S. L. Malhotra, \"Epidemiology of Ischaemic Heart Disease in India with Special Reference to Causation,\" British Heart Journal 29 (1967): 895-905; S. L. Malhotra, \"Serum Lipids, Dietary Factors, and Ischemic Heart Disease,\" American Journal of Clinical Nutrition 20, no. 5 (May 1967): 462-74. 111 A. G. Shaper, \"Cardiovascular Studies in the Samburu Tribe of Northern Kenya,\" American Heart Journal 63, no. 4 (April 1962): 437-42; A. G. Shaper, K. W. Jones, M. Jones, and J. Kyobe, \"Serum Lipids in Three Nomadic Tribes of Northern Kenya,\" American Journal of Clinical Nutrition 13, no. 3 (September 1963): 135-46; G. V. Mann, R. D. Shaffer, R. S. Anderson, and H. H. Sandstead, \"Cardiovascular Disease in the Masai,\" Journal of Atherosclerosis Research 4, no. 4 (July 1964): 289-312; I. A. Prior, F. Davidson, C. E. Salmond, and Z. Czochanska, \"Cholesterol, Coconuts, and Diet on Polynesian Atolls: A Natural Experiment: The Pakapuka and Tokelau Island Studies,\" American Journal of Clinical Nutrition 34, no. 8 (August 1981): 1552-61; J. M. Stanhope, V. M. Sampson, I. M. 386 But it was not just in the area of epidemiology that the fat-cholesterol hypothesis was challenged by conflicting evidence. Analysis of historical dietary trends in the United States and Britain belied the assumption made by Keys and other advocates of the hypothesis that the rising rates of heart disease in these countries coincided with increasing consumption of animal fat. Studies carried out in the 1960s on trends in nutrient intakes in the United States revealed that the proportion of the average American diet made up by fat had indeed grown over the course of the twentieth century, but supposedly \"heart-healthy\" vegetable fats accounted for virtually the entire increase. Analysts also found that the consumption of lard and butterfat had declined in the two postwar decades, during the height of the supposed heart disease \"epidemic.\" Although the proportion of the American diet made up by carbohydrates had declined very slightly, it was starchy carbohydrates\u2014mainly cereals, potatoes, and sweet potatoes\u2014that wholly accounted for the shift. Sugar intake, on the other hand, had increased until it reached a peak in the 1930s and stayed roughly at that level, with the exception of the World War II years when rationing was in place, up through the postwar period. By the mid-1960s, sugar made up nearly half the total calories consumed as carbohydrates in the average American diet. Analysts identified a similar trend in British eating habits. John Yudkin, for example, pointed out that coronary heart disease mortality had continuously risen in Britain since the late 1920s\u2014interrupted only by slight falls in 1940-42 and 1953\u2014despite the fact that animal fat consumption had markedly declined at the beginning of World War II as a result of rationing and had not yet returned to its prewar peak by the mid-1950s.112 Prior, \"The Tokelau Island Migrant Study: Serum Lipid Concentrations in Two Environments,\" Journal of Chronic Disease 34, nos. 2-3 (1981): 45-55; Ravnskov, Cholesterol Myths, 99-103. 112 Berta Friend, \"Nutrient Supply in the United States Food Supply: A Review of Trends, 1909-13 to 1965,\" American Journal of Clinical Nutrition 20, no. 8 (August 1967): 907-14; Yudkin, Diet and Coronary Thrombosis,\" 160. 387 Some researchers also questioned the value of total blood cholesterol for predicting coronary heart disease risk. One of the earliest studies designed to demonstrate a possible correlation between blood cholesterol and degree of atherosclerosis was published in 1936 by Warren Sperry, a biochemist in the Department of Forensic Medicine at New York University and co-inventor of the measurement technique for blood cholesterol, and Kurt Land\u00e9, a pathologist with the New York City Medical Examiner. Sperry and Land\u00e9 autopsied over a hundred very recently deceased New Yorkers who had died violently. They reasoned that the cholesterol levels in these individuals would not have been affected by their cause of death, as might have been the case had they died of a chronic condition such as coronary heart disease. Sperry and Land\u00e9 found no correlation between the amount of cholesterol in the blood and the degree of atherosclerosis determined at autopsy.113 Their findings were repeatedly confirmed in similar postmortem studies in the following decades. In the 1968 International Atherosclerosis Project, for example, over 22,000 corpses in 14 countries were autopsied and examined for arterial plaques. Investigators found the same degree of atherosclerosis in all parts of the world\u2014in populations that consumed large amounts of fatty animal products and those that were largely vegetarian, and in populations that suffered from high rates of heart disease and in those that suffered very little or none at all.114 Perhaps most damning for the fat-cholesterol hypothesis was the failure of controlled dietary intervention trials to demonstrate a lowered death rate by the reduction of animal fat in the diet. One of the earliest and most widely publicized of these studies was the Coronary Heart Disease Study Project, also known as the \"Anti-Coronary Club Trial.\" 113 K. E. Land\u00e9 and W. M. Sperry, \"Human Atherosclerosis in Relation to the Cholesterol Content of Blood Serum,\" Archives of Pathology 22 (1936): 301-12. 114 H. C. McGill, J. Arias-Stella, L. M. Carbonell, et al., \"General Findings of the International Atherosclerosis Project,\" Laboratory Investigations 18, no. 5 (1968): 498-502. 388 This project was initiated in 1957 by Norman Jolliffe, Director of the Nutrition Bureau of the New York City Health Department who had played a large role in the enrichment movement. 814 middle-aged businessmen in Jolliffe's Anti-Coronary Club were prescribed the \"Prudent Diet,\" which he had devised in 1956. The participants on this diet were encouraged to eat adequate amounts of animal protein in the form of lean poultry, fish, egg whites, skim-milk cottage cheese, and fat-free milk, but were limited to four meal s a week containing beef, lamb, mutton, or pork. They were not allowed more than four whole eggs per week, nor were they permitted to consume any butter, cream, whole milk, ice cream, lard, or partially hydrogenated margarine. The diet also included at least one-and-a-half ounces of liquid vegetable oil rich in polyunsaturated fatty acids every day. In total, the Prudent Diet was only about 30 percent fat calories, and the proportion of polyunsaturated fat to saturated fat was four times greater than that of typical American diets. Obese Anti-Coronary Club members were placed on a sixteen-hundred-calorie \"Prudent Reducing Diet\" that consisted of less than 20 percent fat. Jolliffe and his colleagues then recruited 463 men of the same age who continued eating their normal diet to serve as a control group. Investigators anticipated that the Prudent Diet would protect against heart disease and save lives.115 When results of the Anti-Coronary Club Trial were published in 1966, the study authors reported that there was a significantly lower incidence of heart disease symptoms in the Prudent Diet group. Moreover, the men on the Prudent Diet had an average blood cholesterol level of approximately 220, compared to 250 in the control group. But the investigators were also obliged to note that there were eight deaths from heart disease in the 115 Norman Jolliffe, Ethel Maslansky, Florence Rudensey, Martha Simon, and Alice Faulkner, \"Dietary Control of Serum Cholesterol in Clinical Practice,\" Circulation 24 (1961): 1415-21. 389 Prudent Diet group, and none in the control group. Twenty-six of the men on the Prudent Diet had died during the trial, compared to only six of the men in the control group. This mean t that total mortality was about two times higher in the Prudent Diet group than in the control group.116 Defenders of the fat-cholesterol hypothesis argued that the number of participants in the Anti-Coronary Club Trial were too small to draw any definitive conclusions. But other larger trials carried out in the 1960s and 1970s using cholesterol-lowering diets high in polyunsaturated fats did not produce any consistent findings. In addition, animal-feeding experiments and clinical trials suggested that diets rich in polyunsaturated fats could cause cancer, which cast serious doubts on the therapeutic value of vegetable oils for heart disease.117 Advances in the scientific understanding of lipid metabolism during the postwar period also challenged the basic premises the fat-cholesterol hypothesis. At the same time that epidemiologists were examining the role of blood cholesterol in the genesis of heart disease, a very different line of inquiry was being pursued by John Gofman, a medical physicist working out of the Donner Laboratory at the University of California, Berk eley. In 1948, Gofman acquired one of the first ultracentrifuges in the United States, a device that spins liquid samples at high speeds in order to separate them by density. He set out to use the ultracen trifuge to study how cholesterol and fat are transported through the blood and how this process might be affected by diet and perhaps cause atherosclerosis and heart disease. Gofman first reported his research in the journal Science in 1950. He pointed out that cholesterol is only one of several fatlike chemical constituents in the bloodstream that 116 George Christakis, Seymour H. Rinzler, Morton Archer, and Ethel Maslansky, \"Summary of the Research Activities of the Anti-Coronary Club,\" Public Health Reports 81, no. 1 (January 1966): 65-70. 117 Taubes, Good Calories, Bad Calories, chs. 2 and 3 passim. 390 are collectively known as lipids or blood lipids. Some of the other major chemicals are free fatty acids, phospholipids, and triglycerides. Gofman noted that none of these substances exist in isolation in the bloodstream, but are instead building blocks of very large lipid-protein complexes, or lipoproteins. Cholesterol, phospholipids, and triglycerides are found in all lipoproteins, but in varying abundance in the different lipoprotein classes. Gofman suggested that in heart disease the problem might be caused not by cholesterol, but by a defect in one of these lipoproteins or an abnormal concentration of certain lipoproteins classes. Using the ultracentrifuge, he fractionated lipoproteins into differen t classes depending on their density. Gofman determined that lipoproteins having a Svedberg flotation rate of 10-20, which would later identified as the intermediate-density lipoproteins (IDL), were a specific cause of coronary heart disease.118 After Science published his article, Gofman persuaded the National Institutes of Health (NIH) to fund a test of his hypothesis that lipoprotein density was the key to understanding the pathogenesis of heart disease, and that cholesterol itself was not. This test involved four research groups\u2014led by Gofman at Berk eley, Irving Page at the Cleveland Clinic, Fred Stare and Paul Dudley White at Harvard University, and Max Lauffer at the University of Pittsburgh\u2014that collectively identified five thousand men free of heart disease. When heart disease eventually appeared, they would determine whether total cholesterol or Gofman's lipoprotein measurement was the more accurate predictor. But while the study was being planned, and even after it was underway, Gofman expanded his hypothesis to include all the lipoproteins in a Svedberg flotation range of 0-400, and not just IDL. He determined that the single best predictor of heart disease risk was an \"atherogenic 118 J. W. Gofman and F. P. Lindgren, \"The Role of Lipids and Lipoproteins in Atherosclerosis,\" no. 2877 (17 February 1950): 166-86. 391 index,\" which took into account the low-density lipoproteins (LDL) and the very low-density lipoproteins (VLDL), weighted them differently, and added them together. The greater the atherogenic index, Gofman concluded, the greater the risk of atherosclerosis and heart disease. This and other post hoc alterations to Gofman's basic hypothesis, however, greatly annoyed the other investigators, so they refused to accept any further modifications. As a result, the final report of the four groups, published in the journal Circulation in 1956, included two different conclusions. The Berk eley group, taking the minority opinion, claimed that their results confirmed the importance of lipoproteins, particularly those in the low- and very-low density range, in the genesis of heart disease. The three other groups, taking the majority opinion, concluded that the highly complicated lipoprotein measurements added little predictive power beyond that given by a single measurement of total blood cholesterol, which was simpler and less expensive.119 Although Gofman would later be vindicated, the failure of the cooperative study to confirm his hypothesis mean t that the conventional thinking on heart disease in the 1950s and 1960s remained fixated on total cholesterol levels. It also meant that the dietary implications of Gofman's research were largely overlooked. He found that, while the consumption of saturated fats could indeed elevate LDL levels, it was carbohydrates that elevated VLDL, which was the lipoprotein class that contained some cholesterol and most of the triglycerides in the blood. Only by restricting carbohydrates, Gofman determined, could VLDL be lowered. This finding led him to oppose the notion that all patients with high cholesterol should be prescribed a low-fat diet. Gofman argued that the increased 119 Cooperative Study of Lipoproteins and Atherosclerosis, \"Evaluation of Serum Lipoprotein and Cholesterol Measurements as Predictors of Clinical Complications of Atherosclerosis: Report of a Cooperative Study of Lipoproteins and Atherosclerosis,\" Circulation 14, No. 4 (October 1956): 689-742. 392 consumption of carbohydrate on these low-fat diets might raise VLDL so much that the diet would actually do more harm than good. He discovered, for example, that the average blood level of all classes of lipoproteins tended to increase with increas ing degree of overweight, but that that this effect was most mark ed for VLDL.120 \"If preventive and therapeutic measures are considered only in the light of the blood cholesterol level,\" Gofman cautioned in 1958, \"serious errors of management will eventuate and patients will be denied effective therapy.\"121 Around the same time that Gofman began warning about the dangers of indiscriminately prescribing low-fat, high-carbohydrate diets, other researchers working in the field of lipid metabolism were coming to the same conclusion. In 1958, Marg aret Albrink, a physician in the Department of Medicine at Yale University, and Evelyn Man, a physician at the Grace-New Haven Community Hospital, published a landmark article demonstrating that elevated triglyceride levels were far more commo n in coronary heart disease patients than high cholesterol.122 Albrink reconfirmed these findings in later studies, and further determined that almost all individuals with abnormally high serum triglycerides also showed depressed levels of high-density lipoproteins (HDL) and increased levels of VLDL.123 Concurrently, Pete Ahrens and his colleagues at Rockefeller University showed 120 John W. Gofman, Alex V. Nichols, and E. Virginia Dobbin, Dietary Prevention and Treatment of Heart Disease (New York: G. P. Putnam's Sons, 1958), 55. In this book, Gofman and his associates advocated a low-carbohydrate, low-saturated fat diet, which tended to lower all the lipoprotein classes, as a preventive measure against heart disease for the general population. 121 John Gofman, \"Diet in the Treatment of Myocardial Infarction,\" American Journal of Cardiology 1, no. 2 (February 1958): 272. 122 Margaret J. Albrink and Evelyn B. Man, \"Serum Triglycerides in Coronary Artery Disease,\" Archives of Internal Medicine 103, no. 1 (January 1959): 4-8. 123 Margaret J. Albrink, \"Lipoprotein Pattern as a Function of Total Triglyceride Concentration of Serum,\" Journal of Clinical Investigation 40, no. 3 (March 1961): 536-44; Margaret J. Albrink, \"Triglycerides, Lipoproteins, and Coronary Artery Disease,\" Archives of Internal Medicine 109, no. 3 (March 1962): 345-59; \"The Significance of Serum Triglycerides,\" Journal of the Dietetic Association 42 (January 1963): 29-31. 393 that abnormally elevated triglycerides in patients with \"essential lipemia\" could be divided into two forms on the basis of their response to formula diets: carbohydrate-induced and fat-induced lipemia. They also discovered that the vast majority of individuals in the population with elevated triglycerides had the carbohydrate-induced form of lipemia.124 Mean while, the first large-scale test of the \"carbohydrate-triglyceride hypothesis\" was begun under the auspices of the National Heart, Lung, and Blood Institute (NHLBI), a division of the NIH. The study originated from a series of papers coauthored by lipid metabolism specialists Donald Fredrickson, Robert Levy, and Robert Lees that appeared in The New England Journal of Medicine in 1967. They divided the lipoproteins in the bloodstream into four categories: VLDL, which carried most of the triglycerides; LDL, which carried most of the cholesterol; HDL, which contained the highest proportion of protein to cholesterol; and chylomicrons, which carried dietary fats from the intestine to other parts of the body. Fredrickson, Levy, and Lees then proposed a generalized classification scheme for blood lipid abnormalities, each represented by a roman numeral. Four of the five lipoprotein disorders described in this series were characterized by abnormally elevated levels of triglycerides and VLDL. For this reason, Fredrickson, Levy, and Lees warned against the dangers of prescribing low-fat diets to all patients with elevated cholesterol levels, because these diets typically increased carbohydrate consumption and so would likely elevate triglycerides and VLDL even further. By far the most common type of the five lipoprotein disorders was the one designated Type IV, characterized by elevated triglycerides and VLDL, which had to be treated with a carbohydrate-restricted diet. 124 E. H. Ahrens, Jr., J. Hirsch, K. Oette, J. W. Farquhar, and Y. Stein, \"Carbohydrate-Induced and Fat-Induced Lipemia,\" Transactions of the American Association of Physicians 74 (1961): 134-46. 394 Accompanying their classification scheme for blood lipid disorders, Fredrickson, Levy, and Lees also described a new technique for measuring triglycerides and cholesterol that was simpler, less expensive, and more reliable than preexisting ones. Consequently, the NHLBI funded several epidemiological studies to meas ure HDL, LDL, and VLDL in five populations\u2014male civil service employees in Albany, New York; a general population of white and black men in Evans County, Georgia; a general population of men and women in Framingham, Massachusetts; and the general populations of men of Japanese ancestry living in Honolulu, Hawaii and San Francisco, California. The focus of these epidemiological investigations was on LDL and VLDL, since these lipoprotein fractions had been the basis of the five-class typology devised by Fredrickson, Levy, and Lees; HDL was measured only because their new measurement technique required that the amount of cholesterol in HDL be known so that the amount in LDL could be calculated. But when the data were reviewed almost a decade after the studies had been begun, a surprising fact emerg ed: cases with heart disease had lower average concentrations of HDL cholesterol than healthy controls in every age group, every race, and both sexes. The LDL cholesterol was, as expected, generally higher in the cases than in the healthy controls, but the relation was neither as strong nor as consistent as that for HDL cholesterol. Investigators also identified a strong negative association between HDL and VLDL levels. Since it was known that VLDL carried most of the triglycerides in the blood, this finding confirmed the observation, which had been made by researchers since the 1950s, that individuals with high concentrations of serum triglycerides were more likely to develop heart disease than individuals with lower concentrations. The team of investigators who analyzed the data from the five study populations concluded in a 1977 report that \"there is now no question from a scientific point of view that partitioning total cholesterol is prefer able to a single meas ure of total cholesterol 395 in assessing CHD risk; and a lipid profile based on HDL and LDL cholesterol and triglyceride is a logically preferable method of meas uring the CHD risk associated with lipid characteristics.\"125 Subsequently, the research group working in Framingham was able to confirm this finding in a prospective study\u2014that is, by meas uring triglyceride, lipoprotein, and cholesterol levels in 2,815 men and women aged 49 to 82 years, and then waiting four years to see how well these levels predicted the appearance of heart disease.126 But by the time this test of the carbohydrate-triglyceride hypothesis was completed in 1977, the notion that saturated fat was responsible for the rising rates of coronary heart disease had already begun its transformation from controversial speculation to nutritional dogma. The American Heart Association (AHA) played a pivotal role in this shift. Up through the 1950s, the AHA had remained undecided on the question of dietary fat and coronary heart disease. In 1957, a five-member ad hoc committee chaired by Irving Page released a report in the AHA journal Circulation expressing skepticism about the fat-cholesterol hypothesis. While the committee members granted that fat and saturated fat might play an important role in the development of heart disease, they emphasized that more research was needed. As they concluded in the 1957 report, \"There is not enough evidence available to permit a rigid stand on what the relationship is between nutrition, particularly the fat content of the diet, and atherosclerosis and coronary heart disease.\"127 The conservative stance of the AHA at that time was supported by the National Academy of 125 W. P. Castelli, J. T. Doyle, T. Gordon, C. G. Hames, M. C. Hjortland, S. B. Hulley, A. Kagan, and W. J. Zukel, \"HDL Cholesterol and Other Lipids in Coronary Heart Disease: The Cooperative Lipoprotein Phenotyping Study,\" Circulation 55, no. 5 (May 1977): 767-72. 126 T. Gordon, W. P. Castelli, M. C. Hjortland, W. B. Kannel, and T. R. Dawber, \"High Density Lipoprotein as a Protective Factor Against Coronary Heart Disease: The Framingham Study,\" American Journal of Medicine 62, no. 5 (May 1977): 707-14. 127 I. H. Page, F. J. Stare, A. C. Corcoran, H. Pollak, and C. F. Wilkinson, Jr., \"Atherosclerosis and the Fat Content of the Diet,\" Circulation 16 (1957): 164-78 See also I. H. Page, F. J. Stare, A. C. Corcoran, H. Pollak, and C. F. Wilkinson, Jr., \"Atherosclerosis and the Fat Content of the Diet,\" Journal of the American Medical Association 164, no. 18 (31 August 1957): 2048-51. 396 Sciences, the Nutrition Foundation, and the Food and Drug Administration. Enthusiasm for cholesterol-lowering diets was characterized by many in the health establishment as a new form of \"food faddism.\" But in 1960, the AHA changed direction. The state of the evidence remained the same as it had been four years earlier, but the composition of the committee that formulated dietary policy for the AHA had changed. Two of the new members were Ancel Keys and another vocal supporter of the fat-cholesterol hypothesis, Northwestern University cardiologist Jeremi ah Stamler. In its updated report, the AHA cautiously endorsed dietary change as a possible means of preventing coronary heart disease in high-risk patients. These were defined as people of both sexes who had already suffered one or more atherosclerotic heart attacks or strokes, as well as men with a strong family history of atherosclerotic or blood vessel disease, high cholesterol levels, elevated blood pressure, and \"who lead sedentary lives of relentless frustration.\" The updated report recommended that patients at high risk for coronary heart disease should reduce their intake of total fat, saturated fat, and cholesterol, and increase intake of polyunsaturated fat. But the AHA committee also acknowledged that \"there is as yet no final proof that heart disease or strokes will be prevented by such meas ures.\"128 In the years following the publication of the updated report, the AHA became increasingly unconditional in its promotion of the fat-cholesterol hypothesis. By the mid-1960s, the organization had extended the scope of its dietary recommendations to include people who were not at high immediate risk of coronary heart disease. Around the same time, the AMA also changed course and endorsed similar dietary modifications as a primary 128 Irvine H. Page, Edgar V. Allen, Francis L. Chamberlain, Ancel Keys, Jeremiah Stamler, and Frederick J. Stare, \"Dietary Fat and Its Relation to Heart Attacks and Strokes,\" Circulation 23 (January 1961): 133-36. 397 preventive measure for healthy people who wanted to avoid coronary heart disease in the future. These shifts in policy were not accompanied by any new proof from laboratory experiments or dietary trials, but rather by an ever-increasing mass of epidemiological data that showed a positive correlation\u2014at least in middle-aged men\u2014between high blood cholesterol levels and heart disease risk.129 Mean while, the proponents of the fat-cholesterol hypothesis conveyed their message to the public in popularly oriented books and magazine articles. Ancel Keys and his wife Marg aret were especially influential in this regard. In their best-selling \"healthy-heart\" cookbook, Eat Well and Stay Well (1959), Ancel and Marg aret recommended that people reduce their consumption of saturated fat, keep total fats under 30 percent of total caloric intake, avoid heavy use of salt and refined sugar, and favor fresh vegetables, fruits, and non-fat milk products. They also advised maintaining a normal body weight, getting plenty of exercise and outdoor recreation, and avoiding excessive stress. Following upon the publication of the Seven Countries Study in 1970, Ancel and Marg aret wrote an updated version of Eat Well and Stay Well, entitled How to Eat Well and Stay Well the Mediterranean Way (1975). In the book, they extolled diets such as those consumed by Cretan villagers that were low in saturated fat and rich in plant foods. The Mediterranean diet, according to the Ancel and Marg aret, was characterized by an abundance of bread, cereals, legumes, nuts, and vegetables; fresh fruit as the typical dessert; olive oil as the main source of fat; low to moderate amounts of dairy products, poultry, and fish; and very small quantities of eggs and red meat. 129 Karin Garrety, \"Science, Policy, and Controversy in the Cholesterol Arena,\" Symbolic Interaction 21, no. 4 (1998): 404-09. 398 Certain segments of the food industry also played a major role in pushing the public toward a belief in the evils of fat, and fat-rich animal foods in particular. A 1957 magazine ad for the breakfast cereal Wheaties, for example, touted the product's low fat content after warning readers that \"important new discoveries have caused many nutrition experts to express concern over the high percentage of fat in our meal s.\" A low-fat breakfast including Wheaties, the ad claimed, \"may actually help you live longer.\"130 In advertising campaigns aimed at physicians and the public, the vegetable oil and margarine industries were also quick to promote the idea that their products were a healthy alternative to animal fats such as butter, lard, and tallow. (In the late 1950s, margarines high in polyunsaturated fatty acids began appearing on the market after food scientists developed methods for solidifying liquid vegetable oils at room temperature. Margarines had previously contained a large proportion of saturated fatty acids, including the now-vilified trans fatty acids produced by the hydrogenation process.)131 The scientists and medical researchers who were sympathetic to the fat-cholesterol hypothesis aided these food industries by endorsing their products. For example, Corn Products Company, the makers of Mazo la Corn Oil and Mazola Margarine, sponsored the publication of a revised edition of Your Heart Has Nine Lives: Nine Steps to Heart Health (1966), a short self-help book authored by Alton Blakeslee and Jeremiah Stamler. Even though they recognized that proof for the health benefits of reducing saturated fat intake had not yet been produced, Blakeslee and Stamler nevertheless advocated substituting skim milk and low-fat cheeses for cream, butter, and whole-milk cheeses, reducing egg consumption, 130 Time, 15 April 1957. 131 Karin Garrety, \"Social Worlds, Actor-Networks and Controversy: The Case of Cholesterol, Dietary Fat and Heart Disease,\" Social Studies of Science 27, no. 5 (October 1997): 739. 399 and cutting the fat off red meats. They also called for replacing butter and other supposedly artery-clogging animal fats with vegetable oils and special margarines.132 Mean while, concerns about the consumption of animal products were arising independent of the scientific debate over their effects on cardiovascular health. Throughout the 1960s and early 1970s, famine in the Third World was a recurring subject in the news. Observers came to the conclusion that the main cause of this problem was an ever-increasing world population, but blame also fell on eating habits in the affluent Western countries, particularly the United States, where animal products made up a large proportion of the diet. As Francis Moore Lapp\u00e9 pointed out in her best-selling book, Diet for a Small Planet (1971), the American livestock industry required approximately twenty million tons of protein from grains to produce just two million tons of animal protein. This meant that fully one-half of the harvested agricultural land in the United States was planted with crops that could be directly eaten by humans, but went to feeding livestock instead. By feeding these sources of vegetable protein to animals rather than humans, Lapp\u00e9 argued, Americans were wasting scarce protein resources and driving up food prices at a time when much of the world was suffering from hunger and malnutrition. This inefficient process also required vast amounts of land, water, fertilizers, pesticides, and herbicides, according to Lapp\u00e9.133 Over the course of the 1970s, the social and environmental arguments against diets rich in animal foods made by Lapp\u00e9 and others became intertwined with the medical issues of fat and cholesterol in the diet.134 By that point, as we saw earlier, the \"nutritional conservation\" perspective had long faded from public consciousness and the discourse on 132 Alton Blakeslee and Jeremiah Stamler, Your Heart Has Nine Lives: Nine Steps to Heart Health (New York: Pocket Books, 1966). 133 Francis Moore Lapp\u00e9, Diet for a Small Planet (New York: Friends of the Earth/Ballantine Books, 1971), 3-30. 134 Taubes, Good Calories, Bad Calories, 43. 400 agricultural policy. With it went the notion that an abundance of animal-derived foods could be produced in an ecologically benign manner when incorporated into pasture-based and mixed-crop farming regimes. In any case, in the decades after World War II growing numbers of livestock spent increasingly longer parts of their lifespan in confinement feeding operations. By the 1970s, livestock came to be seen by environmentalists as an ecologically harmfu l extravagance rather than as an integral component in efforts to check erosion and build soil fertility. Perhaps the single most important event in establishing the fat-cholesterol hypothesis as the conventional nutritional wisdom was the publication of Dietary Goals for the United States in early 1977. This report was the product of the Senate Select Committee on Nutrition and Human Needs, which was sometimes refer red to as the McGovern committee after its chairperson, Senator George McGovern. Initially set up in 1968 with the mandate to wipe out hunger and undernutrition in the United States, the committee later turned its attention to diet and chronic degenerative diseases such as obesity, diabetes, and heart disease. In 1976, the McGovern committee initiated a series of hearings with the evocative title, \"Diet Related to Killer Diseases.\" After listening to two days of testimony on the topic, the committee staff turned the task of researching and writing Dietary Goals over to Nick Mottern, a former labor reporter with no experience writing about science, nutrition, or health. As science writer Gary Taubes has pointed out in his investigation of the origins of Dietary Goals, Mottern saw dietary fat as the nutritional equivalent of cigarettes, and the food industry as similar to the tobacco industry in suppressing the truth in the interest of profits. Moreover, in drawing up recommendations for fat and cholesterol intake, Mottern relied almost exclusively for his expertise on Harvard School of Public 401 Health nutritionist Mark Hegsted, who by that point believed unconditionally in the benefits of a low-fat diet.135 The underlying foundation of the recommendations in Dietary Goals was the notion that heart disease, cancer, obesity, stroke, and other diseases of affluence were directly linked to changes that had taken place in the American diet over the course of the twentieth century. \"Our diets have changed radically within the past fifty years,\" McGovern remark ed in the introduction to the report, \"with great and often harmful effects on our health.\" To reverse this trend, according Dietary Goals, Americans had to make a number of changes to their diet: increas e carbohydrate consumption to account for approximately 55-60 percent of energy intake; decrease total fat consumption from approximately 40 percen t to 30 percent of all calories; reduce saturated fat consumption to account for about 10 percent of total energy; and reduce cholesterol consumption to about 300 milligrams per day. To achieve these low-fat, low-cholesterol dietary goals, Americans would have to eat considerably less meat , eggs, and full-fat dairy products. Dietary Goals also called for a reduction in sugar consumption by about 40 percent to account for about 15 percen t of total energy intake, as well as a decrease in salt consumption by 50 to 85 percent to about three grams per day. The report also urged Americans to consume more fruits, vegetables, and whole grains, and raised concerns about the large quantities of chemical preservatives and additives in the food supply.136 Although Dietary Goals acknowledged that no evidence existed to suggest that reducing the fat content of the diet would actually prevent heart attacks, it justified this 135 Garrety, \"Science, Policy, and Controversy in the Cholesterol Arena,\" 409; Nestle, Food Politics, 40-42; Gary Taubes, \"The Soft Science of Dietary Fat,\" Science 291, no. 5513 (30 March 2001): 2536-45; Taubes, Good Calories, Bad Calories, 46. 136 Select Committee on Nutrition and Human Needs of the United States Senate, Dietary Goals for the United States (Washington, DC: GPO, 1977). 402 recommendation on the scientifically unproven assumption that people would be less likely to gain weight if they lowered the percentage of dense fat calories in their diet. (Fat provides nine calories per gram, whereas carbohydrate and protein provide only four calories per gram.) Defenders of Dietary Goals also claimed that the public had nothing to lose by following the recommendations laid down in the report. As Mark Hegsted stated shortly after the release of the report, \"What are the risks associated with eating less meat , less fat, less saturated fat, less cholesterol.... There are none that can be identified and important benefits can be expected.\"137 By the mid-1970s, in any case, Americans had already begun responding to the low-fat message that was being spread by nongovernmental health organizations and certain segments of the food industry\u2014compared to a decade earlier, they were eating less whole milk, butter, eggs, and sugar, and more fish, margarine, potatoes, and vegetables.138 Dietary Goals unsurprisingly provoked a storm of protest from food producers, particularly the meat , egg, and dairy industries. Some scientists and physicians\u2014including spokesmen for the AMA\u2014were also unhappy with the report, highlighting the lack of adequate proof and raising concerns about potential unintended negative consequences. After holding several more hearings, the McGovern committee made some compromises and released a revised edition of the report just before being disbanded at the end of 1977. The committee succumbed to pressure from the livestock industry and replaced the statement \"reduce consumption of meat\" with the less offensive \"reduce consumption of animal fat, and choose meats, poultry, and fish which will reduce saturated fat intake.\" It also added the statemen t that \"some consideration should be given to easing the cholesterol goal for pre- 137 Quoted in Senate Select Committee for Nutrition and Human Needs, Dietary Goals for the United States, 3. 138 Levenstein, Paradox of Plenty, 211. 403 menopausal women, young children and the elderly in order to obtain the nutritional benefits of eggs in the diet.\" However, the alterations to the original report were minor, and the central claims about the desirability of fat and cholesterol restriction were not undermined. In addition, the appearance at the hearings of prominent researchers alongside representatives from the dairy, egg, and cattle industries served to cast doubt on the legitimacy of the scientific criticisms.139 In the years following the publication of Dietary Goals, a growing number of public health organizations and governmental institutions lined up behind the fat-cholesterol hypothesis. In 1979, the surgeon general of the Department of Health, Education, and Welfare issued a report, Healthy People, calling for a national health strategy that included eating fewer calories and less red meat , saturated fat, cholesterol, salt, and sugar. It also advised Americans to eat more fish, poultry, complex carbohydrates, and legumes\u2014more or less the same dietary regime as that proposed in Dietary Goals. Soon after the surgeon general's report appeared, the USDA released similar recommendations, with a great deal of media fanfare, in Nutrition and Your Health: Dietary Guidelines for Americans (1980).140 The same year that the USDA's report was published, however, the Food and Nutrition Board (FNB) of the National Academy of Sciences released its own set of guidelines, Toward Healthful Diets, which did not specifically call for reductions in fat or cholesterol intake. The only reliable dietary advice that could be given to healthy Americans, the FNB report concluded, was to maintain a normal bodyweight and that everything else, including dietary fat, would take care of itself. But by then, the scientific and medical communities had for the most part endorsed the fat-cholesterol hypothesis. As a 139 Nestle, Food Politics, 40-42; Taubes, Good Calories, Bad Calories, 46-48. 140 Garrety, \"Science, Policy, and Controversy in the Cholesterol Arena,\" 412-13; Nestle, Food Politics, 42-47; Taubes, Good Calories, Bad Calories, 46-48. 404 result, Toward Healthful Diets was \"attacked with unbelievable vehemence by the professional and lay press,\" in the words of David Kritchevsky, who was one of the FNB members responsible for writing up the report.141 Critics charged that the scientists who authored Toward Healthful Diets were not only irresponsible, but also had demonstrable ties to the meat, egg, and dairy industries. Embarrassed by the controversy, the National Academy of Sciences reorganized the FNB. A short time later, the new members\u2014with fewer ties to the affected industries\u2014issued a report that largely supported the low-fat claims in Dietary Goals and Dietary Guidelines. By 1981, the National Cancer Institute, the Society for Clinical Nutrition, and the AMA, among other organizations, had also come around to recommending low-fat diets. The dietary-fat controversy was then all but extinguished in 1984, when the National Heart, Lung, and Blood Institute held a \"Consensus Conference on Lowering Blood Cholesterol to Prevent Heart Disease.\" Despite dissent from some of the participants, the conference \"consensus panel\" produced a statement that strongly endorsed dietary saturated fat and cholesterol restriction as a means of reducing coronary heart disease risk.142 In Britain, meanwhile, official opinion was moving in a similar direction. In 1974, the UK Committee on Medical Aspects of Food Policy (COMA) issued a report recommending that individuals should lower their saturated fat intake. But John Yudkin, who was one of the members of the COMA panel tasked with producing this report, included a note reservation stating that the other panel members had overemphasized the role of fat in heart disease and underestimated the role of sugar. Because Yudkin had been a vocal critic of sugar in the popular press in the years leading up to the issuing of the report, 141 David Kritchevsky, \"History of Recommendations to the Public about Dietary Fat,\" Journal of Nutrition 128, no. 2 (February 1998): 450S. 142 Garrety, \"Science, Policy, and Controversy in the Cholesterol Arena,\" 414-15; Nestle, Food Politics, 47-49. 405 his dissenting opinion helped to give the impression that the COMA panel was undecided on whether lowering saturated fat intake would do any good. As a result, the British government took little action to promote dietary change. But in the years following the release of the COMA report, this all changed. In 1979, the National Advisory Committee on Nutrition Education (NACNE) was established by the Health Education Council and the British Nutrition Foundation, a food industry-funded organization. In 1983, NACNE issued a report entitled A Discussion Paper on Proposals for Nutritional Guidelines for Health Education in Britain, which outlined the links between diet and a range of conditions associated with affluence, including constipation, bowel disease, dental caries, and coronary heart disease. The NACNE report recommended that individuals in the general population consume less total fat, saturated fat, sugar, alcohol and salt, and more dietary fiber. However, Nutritional Guidelines became embroiled in controversy after the British Nutrition Foundation and the Department of Health and Social Secu rity greeted it with hostility and sought to alter it. But a short time later, COMA issued a revised report, Diet and Cardiovascular Disease (1984), which outlined recommendations for the prevention of cardiovascular disease that were largely in agreement with those of Nutritional Guidelines. Although the authors of the COMA report acknowledged that \"there has been no controlled clinical trial of the effect of decreasing dietary intake of saturated fatty acids on the incidence of coronary heart disease nor is it likely that such a trial will be undertaken,\" they nevertheless advised the British public to consume less fat and saturated fat.143 143 Mark W. Bufton, \"British Expert Advice on Diet and Heart Disease c.1945-2000,\" in Making Health Policy: Networks in Research and Policy After 1945, ed. Virginia Berridge (New York: Rodopi, 2005), 125-48. 406 In the space of just a few decades, then, official dietary advice had shifted drastically. Between the 1920s and the 1970s, health authorities had gone from calling for an increase in the consumption of the so-called \"protective foods\" to warning about the deleterious effects of some of these very same foods on cardiovascular health. Bread, cereals, pasta, and other carbohydrate-rich fare, which nutritionists in the interwar period had deemed as mere \"energy foods\" eaten in excess by the poor and those ignorant of modern dietetic principles, became the basis of the \"healthy-heart\" diet. And after health authorities had quietly abandoned the concept of \"optimal nutrition\" in the two postwar decades, calls for a radical reduction in the consumption of sugar in the affluent West were mainly limited to the supporters of the natural food movement and the few cardiologists and epidemiologists who saw it as the most likely culprit in the rising incidence of heart disease. But they were not the only ones to make this recommendation. By the mid-twentieth century, a number of researchers had discovered that the excessive consumption of refined carbohydrate foods was also driving the growing prevalence of type 2 diabetes and obesity in the West. Had their findings been incorporated into mainstream thinking on the relationship between diet and health, I will argue in the epilogue following this chapter, nutrition science would likely not be so widely perceived, as it is today, as a field afflicted by perpetual reversals in what it views as \"healthy eating.\" 407 EPILOGUE AND CONCLUSION Beyond Heart Disease: Conservative Nutrition and the Debate over Diabetes and Obes ity Seen from the perspective of entire twentieth century, it is remark able that the proponents of the fat-cholesterol hypothesis were so successful in changing government policy and public opinion between the 1960s and 1980s. Whereas the nutritionists who called for an increase in the consumption of the \"protective foods\" nearly a half-century earlier could plausibly point to all \"primitive\" populations to substantiate their dietary advice, the same could not be said for the proponents of the fat-cholesterol hypothesis. Furthermore, advancements in diagnostic technique between the 1920s and 1940s tended to confirm the argument that vitamin and mineral deficiencies were a significant public health problem, while the opposite occurred in the case of the fat-cholesterol hypothesis in later decades. Between the 1950s and the 1970s, it became increas ingly clear to specialists in lipid metabolism that the total blood cholesterol level was a crude and often highly misleading predictor for heart disease risk, and that better methods existed that took into consideration triglycerides and the various lipoprotein fractions. These improved methods cast doubt on the notion that dietary fat, and saturated fat more specifically, was the culprit in heart disease, yet they had little impact on how the controversy over fat and cholesterol played out in the public health arena. What is more, decades of research on \"diseases of civilization\" besides atherosclerosis also belied the notion that excessive consumption of saturated fat was the core problem with the Western diet. One such disease, diabetes, is a metabolic disorder in which the body is unable to use for fuel the carbohydrates in the circulation\u2014known as 408 serum glucose or blood glucose. In diabetics, glucose accumulates in the bloodstream and then passes through the kidneys into the urine, causing a condition known as glycosuria. Symptoms of diabetes include constant hunger (especially for sugar and other easily digestible carbohydrates), increased thirst, and frequent urination. Long-term complications arising from diabetes include heart disease, stroke, kidney failure, blindness, and poor circulation in limbs, leading in extreme cases to amputations. By the late 1910s, it had been established that insulin, a hormone produced by the pancreas , was essential for the utilization of carbohydrates for energy. Following upon these advances, Frederick Banting and Charles Best in Toronto discovered a process for isolating insulin in 1921. Soon after this breakthrough, diabetologists began using injections of insulin to manage the symptoms of diabetes. Before insulin treatment became available in the 1920s, the only means that diabetic patients had to mitigate the symptoms of the disease were by controlled starvation, a draconian therapy that was only moderately successful, or by severely restricting the starches and sugars in their diet. This finding led some authorities on diabetes treatment to the conclusion that sugar and other carbohydrates played an obvious role in the disease.1 Sugar and white flour were also suspected in the etiology of diabetes because the dramat ic increase in consumption of these foods beginning in the latter decades of the nineteenth century in North America and Europe coincided with dramatic increases in diabetes incidence and mortality. One of the earliest epidemiological studies of diabetes, conducted by two physicians in the Department of Public Health Administration at Columbia University, Haven Emerson and Louise Larimore, pointed to the consumption of sugar as the single most likely factor in the rising incidence of the disease. When this study 1 Taubes, Good Calories, Bad Calories, 100-01. 409 was published in 1924, the incidence of diabetes in the United States had increased more rapidly than that of any other disease in the previous fifty years, and diabetes had risen to become the tenth leading cause of death in the country. Emerson and Larimore compiled food consumption and mortality statistics from the United States, Britain, and France, and found a close positive correlation between rates of sugar consumption and the incidence of diabetes. They identified a particularly striking decrease in death rates from diabetes in these countries following the sudden reduction in per capita sugar intake during World War I. Emerson and Larimore cautioned that epidemiological data could not prove a direct causal relationship between sugar consumption and diabetes incidence, but they nevertheless felt that their findings were strongly suggestive. \"The changes in food habits in the United States have probably contributed to the increase of diabetes, the higher carbohydrate element and greater abundance or superalimentation being believed to be a cause of overfatiguing the function of sugar tolerance,\" they noted in their conclusion.2 Even one of the critics of Emers on and Larimore's study, C. A. Mills, conceded, \"of the thirteen countries highest in consumption of sugar, eleven are found among the thirteen highest in death from diabetes.\"3 During the interwar period, the notion that a connection existed between diabetes and the excessive consumption of sugar and refined carbohydrates, though not universally accep ted, was hardly unorthodox. \"Sugar and candies do not cause diabetes, but contribute to the burden on the pancreas and so should be used sparingly,\" contended diabetologist Garfield Duncan in his book Diabetes Mellitus and Obesity (1935). He recommended that diabetics take their carbohydrates in starchy forms such as fruits, vegetables, and cereals, 2 Haven Emerson and Louise D. Larimore, \"Diabetes Mellitus: A Contribution to Its Epidemiology Based Chiefly on Mortality Statistics,\" Archives of Internal Medicine 34, no. 5 (November 1924): 585-630. 3 C. A. Mills, \"Diabetes Mellitus: Sugar Consumption in Its Etiology,\" Archives of Internal Medicine 46, no. 4 (October 1930): 582-84. 410 since \"the absorption is slower and the functional strain minimal.\"4 Nutrition authorities at the time also voiced concerns about the role of sugar in predisposing people to diabetes. In Food, Nutrition and Health (1940), for example, Elmer McCollum and Ernestine Becker warned, \"There are reasons for believing that excessive consumption of carbohydrates by sedentary persons may in time overtax the insulin-producing capacity of the pancreas and cause a diabetic tendency.\"5 But the hypothesis that sugar and other refined carbohydrates were responsible for diabetes faded to the scientific margins after World War II. This was mainly due to the fact that the leading American diabetologist in the postwar period, Elliott Joslin, believed that it was the consumption of fat rather than carbohydrates that predisposed individuals to diabetes. Joslin refused to accept that sugar or refined starches could have a unique property that other carbohydrates did not, since they all broke down to glucose after digestion, or glucose and fructose in the case of sucrose (of which white table sugar is entirely composed). Joslin pointed out that the insulin-releasing cells of the pancreas, which are dysfunctional in diabetes, respond only to glucose. As a result, he rejected the positive correlation that epidemiologists such as Emers on and Larimore had identified between increas ing refined sugar consumption and the rising death rate from diabetes in the United States. In Joslin's view, the rising consumption of fat was the obvious dietary culprit, since the consumption of carbohydrates had declined or remained the same. Like the proponents of the fat-cholesterol hypothesis, he also pointed to the Japanese, who at the time consumed a high-carbohydrate, low-fat diet but experienced almost no diabetes.6 4 Garfield G. Duncan, Diabetes Mellitus and Obesity (Philadelphia: Lea & Febiger), 59. 5 E. V. McCollum and J. Ernestine Becker, Food, Nutrition and Health, 5th ed. (Baltimore, MD: Lord Baltimore Press, 1940), 91. 6 Joslin's views are summarized in Taubes, Good Calories, Bad Calories, 104-107. 411 Joslin based his views on the relationship between diet and diabetes primarily on the research of Harold Himsworth of University College Hospital, London. (Himsworth was one of the first researchers to differentiate between insulin-dependent or juvenile diabetes, now known as type 1 diabetes, and non-insulin-dependent or adult-onset diabetes, now known as type 2 diabetes.) Like Joslin, Himsworth did not differen tiate between natural and refined sources of carbohydrates, so when looking at dietary trends he concluded that it was the increas ing consumption of fat rather than sugar that was the primary culprit in the rising death rate from diabetes. \"The progressive rise in diabetic mortality in Western countries during the last fifty years,\" Himsworth argued in 1949, \"coincides with a gradual change towards higher fat and lower carbohydrate diets.\"7 Yet in the decades following World War II, investigators continued to publish reports showing that recently Westernized population groups subsisting on diets low in fat but high in sugar and refined cereals suffered from remarkably high rates of diabetes.8 At the same time, researchers began uncovering the biological mechanisms that could explain how excessive consumption of refined carbohydrates contributes to diabetes. In 1960, Rosalyn Yalow and Solomon Berson announced the discovery of the radioimmu noassay technique, which was capable of measuring the concentration of insulin in human blood much more reliably than previous methods. Using this technique, Yalow and Berson showed that those who had developed diabetes as adults\u2014that is, type 2 diabetes\u2014had 7 H. P. Himsworth, \"Diet in the Aetiology of Human Diabetes,\" Proceedings of the Royal Society of Medicine 42, no. 5 (1949): 323-26. 8 See, e.g., G. D. Campbell, \"Diabetes in Asians and Africans in and Around Durban,\" South African Medical Journal 37 (30 November 1963): 1195-208; I. A. Prior, B. S. Rose, and F. Davidson, \"Metabolic Maladies in New Zealand Maoris,\" British Medical Journal 1, no. 5390 (25 April 1964): 1065-69. 412 levels of circulating insulin significantly higher than those of healthy individuals.9 This discovery definitively discredited the longstanding belief that all cases of diabetes could be explained by a deficiency of insulin. Yalow and Berson later suggested that adult-onset diabetics appeared to be lacking insulin while simultaneously having excessive insulin in their circulation because they were insulin-resistant, which they defined as \"a state (of a cell, tissue, system or body) in which greater-than-normal amounts of insulin are required to elicit a quantitatively normal response.\" Type 2 diabetics, they argued, had to secrete more of the hormone to maintain their blood sugar within healthy levels, and this would become increas ingly difficult to achieve the longer they remained insulin-resistant.10 Since it was known that insulin was required to metabolize glucose, Yalow and Berson's findings suggested that carbohydrates played a key role in the etiology of type 2 diabetes. But it was not until the 1970s and 1980s that other research groups confirmed this hypothesis, by which point public health authorities were recommending low-fat, high-carbohydrate diets for the prevention of atherosclerosis. Strangely, this is a disease that is much more prevalent among type 2 diabetics and is one of the primary reasons that they have a significantly shorter life span compared to non-diabetics.11 In the decades before dietary fat reduction came to preoccupy the thinking of health authorities, a wealth of clinical evidence had accumulated showing that the excessive consumption of sugar and other refined carbohydrates was driving the increase in another \"disease of civilization\" besides diabetes: obesity. Obesity is a medical condition in which excess fat has accumulated in the body's adipose tissue to the extent that it may have an 9 R. S. Yalow and S. A. Berson, \"Immunoassay of Endogenous Plasma Insulin in Man,\" Journal of Clinical Investigation 39, no. 7 (July 1960): 1157-1175. 10 R. S. Yalow, Seymour M. Glick, Jesse Roth, and Solomon A. Berson, \"Plasma Insulin and Growth Hormone Levels in Obesity and Diabetes,\" Annals of the New York Academy of Sciences 131, no. 1 (October 1965): 357-73. 11 Taubes, Good Calories, Bad Calories, 186. 413 adverse effect on health. As early as the mid-nineteenth century, physicians had achieved positive results prescribing diets high in fat and protein and low in carbohydrates to obese patients. One such patient, a British coffin-maker named William Banting, published a small booklet recounting his success losing body fat with carbohydrate restriction, A Letter on Corpulence, Addressed to the Public (1863). Banting had started to become overweight in his thirties, and as time went on he continued to accumulate excess fat despite repeated efforts to restrict his caloric intake and increase his energy expenditure through vigorous exercise. By his mid-sixties, the five-foot-five Banting weighed over 200 pounds, and had begun suffering from a number of other health problems related to his excess weight. His eyesight and hearing had also started to deteriorate. But in 1862, Banting had a life-changing consultation with an ear surgeon named William Harvey. Harvey had been researching the effects of obesity on disease, and he believed that Banting's physical ailments, including his increasing deafness, were caused by his corpulence. Harvey hypothesized that excessive obesity and diabetes had similar causes, so he prescribed Banting a diet that was known to check the secretion of glucose in the urine of diabetics. Harvey advised Banting to eat three meals a day consisting of five or six ounces of meat or fish, a serving of green vegetables, and a very small amount of toast or cooked fruit. He was also allowed to drink dry wine and spirits, as well as tea without milk or sugar. The aim of this dietary regimen was to avoid foods containing what Banting called \"starch and saccharine matter,\" in particular bread, milk, beer, sugar and sweets, and potatoes. By early 1864, he had dropped fifty pounds and his other physical ailments had disappeared. Banting was so impressed with his results on the diet that he published and distributed A Letter on Corpulence at his own expense. And despite a hostile response to the booklet from the medical community, the public was impressed with Banting's results as well. In a short 414 time, A Letter on Corpulence went into many editions and was widely translated; the term \"Ban ting\" came to be synonymous with dieting.12 For a century after Banting published his booklet, physicians routinely prescribed versions of his low-carbohydrate diet to treat obese patients. These diets typically allowed for the consumption of liberal quantities of meat , seafood, fowl, eggs, and cheese, as well as green vegetables and low-carbohydrate fruits, and restricted sugar, sweets, bread, cereals, and potatoes. Many of these low-carbohydrate diets, like Banting's, were relatively high in fat and did not call for restriction of total caloric intake, since it had been repeatedly shown in animal-feeding experiments and clinical studies of obese patients that semi-starvation (calorie-restricted) diets merely reduced energy expenditure and basal metabolic rate, and failed to produce long-term reductions in body fat. Moreover, physicians and dieticians commonly observed that obese patients put on carbohydrate-restricted diets voluntarily restricted their caloric intake and lost weight without complaining of hunger when they were allowed to eat as much food containing protein and fat as they desired.13 Reflecting on these low-carbohydrate, calorie-unrestricted diets, Alfred Pennington noted in 1954, \"Here was a treatment that, in its encouragement to eat plentifully, seemed to oppose not only the 12 William Banting, Letter on Corpulence, Addressed to the Public, 4th ed. (London: Harrison, 1864); A. W. Pennington, \"Treatment of Obesity: Developments of the Past 150 Years,\" American Journal of Digestive Diseases 21, no. 3 (March 1954): 65-69; Taubes, Good Calories, Bad Calories, ix-xii. 13 See, e.g. H. Gardiner-Hill, \"The Treatment of Obesity,\" Lancet 206, no. 5333 (14 November 1925): 1034-35; W. C. Cutting, \"The Treatment of Obesity,\" Journal of Clinical Endocrinology 3, No. 2 (February 1943): 85-88; \"Treatment of Obesity, International Clinics; Vilhj\u00e1lmur Stefansson, Bread Alone (New York: Macmillan, 1946); A. Kekwick and G. L. S. Pawan, \"Metabolic Study in Human Obesity with Isocaloric High in Fat, Protein, or Carbohydrate,\" Metabolism: Clinical and Experimental 6, no. 5 (September 1957): 447-60; Richard Mackarness, Eat Fat and Grow Slim (London: Harvill Press, 1958); Edgar S. Gordon, Marshall Goldberg, and Grace J. Chosy, \"A New Concept in the Treatment of Obesity,\" Journal of the American Medical Association 186, no. 1 (5 October 1963): 50-60. 415 prevailing theory of obesity, but, in addition, principles basic to the biological sciences and other sciences as well.\"14 But by the 1960s, in fact, researchers in the field of fat metabolism had discovered the \"principles basic to the biological sciences\" that could explain how a low-carbohydrate, calorie-unrestricted diet could produce weight loss without any ill effect. In 1937, University of Sheffield biochemist Hans Krebs announced the discovery of the citric acid cycle. This cycle, which also came to be known as the Krebs cycle, is a series of chemical react ions that all aerobic organisms, including humans, use to generate energy. Krebs had begun his research assuming, as was commo n at the time, that carbohydrate was the prefer red energy source of muscle tissue. But he found that the citric acid cycle generates energy whether the initial fuel is fat, carbohydrate, or protein. (Researchers in later years found that the body does not, in fact, require any dietary carbohydrates to function normally.)15 Shortly before Krebs announced his discovery of the citric acid cycle in 1937, Columbia University biochemists Rudolf Schoenheimer and David Rittenberg published a series of experiments that overturned the longstanding belief that fat was deposited in the adipose tissue only when calories were taken excess of the requirement. Using isotopic tracer substances, they found that the bulk of dietary fat, as well as a considerable portion of the carbohydrates ingested, is converted to fat\u2014or technically, triglycerides\u2014in the adipose tissue before being used for fuel by the cells. These triglycerides, Schoenheimer and Rittenberg observed, are then continuously broken down into their component fatty acids, released into the circulation, move to and from the organs and tissues, re-synthesized, and 14 Pennington, \"Treatment of Obesity: Developments of the Past 150 Years,\" 67. 15 Taubes, Good Calories, Bad Calories, 319, 383-84. 416 merg ed with fatty acids from the diet to reform a mixture of triglycerides in the adipose tissue.16 Following the publication of Schoenheimer and Rittenberg's seminal experiments, other investigators found that the controlling factors in the movement of fat to and from the adipose tissue had little bearing on the amount of fat present in the blood, and therefore little bearing on the quantity of calories consumed at that time. This indicated that the accumulation and mobilization of fat were governed not by the nutritional state of the whole body, but by \"a factor acting directly on the cell,\" as Israeli biochemist Ernst Wertheimer put it in a 1948 article.17 And in fact, by the mid-1960s researchers had established beyond doubt that the critical factors determining the balance of storage and mobilization of fatty acids, and therefore the accumulation and reduction in body fat, were glucose and insulin. Carb ohydrates, they determined, were responsible for prompting insulin secret ion. Insulin, in turn, was the hormone responsible for inducing the accumulation of fat in the adipose tissue. Conversely, the one fundamental requirement to increas e the flow of fat out of adipose tissue and into the circulation, where it could be used as fuel, was to lower the concentration of insulin in the bloodstream. This mean t, above all, restricting the intake of carbohydrates, especially those in a highly refined form that prompted the strongest insulin response. Furthermore, by the early 1980s, researchers had determined that sugar, owing to its chemical composition, was a particularly powerful promoter of fat accumulation when consumed over long periods of time. The combination of fructose and glucose in the sucrose 16 Rudolf Schoenheimer and David Rittenberg, \"Deuterium as an Indicator in the Study of Intermediary Metabolism,\" Science 82, no. 2120 (16 August 1935): 156-57; Taubes, Good Calories, Bad Calories, 383. 17 E. Wertheimer and D. Shapiro, \"The Physiology of Adipose Tissue,\" Physiology Reviews 28, no. 4 (October 1948): 454. 417 molecule, they found, simultaneously elevates insulin levels while overloading the liver with carbohydrates.18 These findings linking insulin to fat accumulation accorded well with the long-known fact that obesity was much more common in type 2 diabetics than in the general population.19 As Edgar Gordon of the University of Wisconsin School of Medicine wrote in 1963: It may be stated categorically that the storage of fat, and therefo re the production and maintenance of obesity, cannot take place unless glucose is being metabolized. Since glucose cannot be used by most tissues without the presence of insulin, it also may be stated categorically that obesity is impossible in the absence of adequate tissue concentrations of insulin. This statement is true for obese diabetics who are known to have too much, rather than too little, insulin. Thus an abundant supply of carbohydrate food exerts a powerful influence in directing the stream of glucose metabolism into lipogenesis, whereas a relatively low carbohydrate intake tends to minimize the storage of fat.20 Researchers such as Gordon were arguing, in other words, that fat accumulation and obesity were caused by a dysfunction in the hormonal regulation of adipose tissue and fat metabolism, and not, as was widely believed, by simply consuming more calories than were expended. Someone suffering from obesity, according to this alternative explanation, could experience hunger despite the outward appearance of being overnourished, since chronically elevated insulin levels prevented the release of fat from their adipose tissue into the circulation to be used as energy for metabolism and physical activity. Under such conditions, caloric restriction and increased physical activity would do nothing to address the fundamen tal problem unless the carbohydrates were restricted at the same time. But the mounting research that implicated the excessive consumption of carbohydrates\u2014particularly sugar and refined starches\u2014in the etiology of type 2 diabetes 18 Taubes, Good Calories, Bad Calories, 197-201. 19 See, e.g., H. P. Himsworth and R. B. Kerr, \"Insulin-Sensitive and Insulin-Insensitive Types of Diabetes Mellitus,\" Clinical Science 4, no. 119 (1939): 119-52. 20 Gordon et al., \"A New Concept in the Treatment of Obesity,\" 53-54. 418 and obesity did not prevent nutritionists and public health officials from endorsing low-fat, high-carbohydrate diets in the last third of the twentieth century. The lack of resistance from the scientific commu nity to this campaign was partly due to the fact that a dietary regimen low in fat could still be composed of a wide array of foods, thereby conforming to the notion that a \"varied\" diet was essential for avoiding deficiency diseases. Moreover, nutrition scientists developed their view of what a \"balanced\" diet was during the golden era of vitamin research between the 1910s and 1930s, when the scientific understanding of fat and carbohydrate metabolism was still in its infancy. As a result, it became conventional wisdom that the body required dietary carbohydrates to function normally, though research would later prove this notion to be false. Many nutritionists were also led to oppose carbohydrate-restricted, high-fat diets because they confused ketosis with ketoacidosis, two related but distinct metabolic states. Ketosis is a normal condition in which the liver increas es its synthesis of molecules called ketone bodies to supply the necessary fuel to the brain and nervous system in the absence of dietary carbohydrates, whereas ketoacidosis is a pathological condition, seen most commonly in untreated diabetes, marked by extreme and uncontrolled ketosis. Nutritionists Robert and Violet Plimme r, for example, confused these two conditions, which led them to warn in their popular book Food, Health, Vitamins (1935) that \"the Banting diet is an unbalanced one and should not be continued for any length of time.\"21 But the discovery of vitamins and minerals did not necessarily conflict with the idea that high-fat, carbohydrate-restricted diets could be healthful. The vast majority of nutrition experts in the interwar period claimed that the only way to healthfully lose weight was to reduce calorie consumption below the daily requirements while at the same time making 21 Plimmer and Plimmer, Food, Health, Vitamins, 7th ed., 154, 22. 419 sure that the vitamin, mineral, and protein were still being taken in optimal amounts. This mean t omitting not only sugar and refined starches from the diet\u2014\"solid fattening material,\" as the Plimmers described these foods\u2014but also fat-rich foods that contained few vitamins, such as bacon, lard, and vegetable fats such as shortening and olive oil. However, relatively fat-rich foods containing a large quotient of vitamins and minerals relative to their calorie content, such as eggs, dairy products, and organ meats, were viewed positively. (Nutrition experts of that era also tended to advise the consumption of watery and fibrous foods such as fresh fruits and vegetables, on the assumption that the extra bulk produced a feeling of satiety sooner than if dry or non-fibrous foods were eaten.)22 Furthermore, as I have described in previous chapters, nutritionists were becoming increas ingly antagonistic toward sugar and refined cereals between the 1910s and 1940s. Virtually all the vitamin and mineral deficiency diseases discovered during that era\u2014including beriberi, xerophthlamia, rickets, and pellagra\u2014were induced by diets high in sugar and refined cereal s and low in meat, seafood, eggs, and dairy products. In addition, before the concept of \"borderline\" or \"subclinical\" deficiency states fell into scientific obscurity in the two postwar decades, researchers detected the highest prevalence of these states in groups and individuals consuming large quantities of highly processed carbohydrate-rich foods. And many nutritionists\u2014notwithstanding their wariness toward \"unbalanced\" diets lacking in variety\u2014granted that non-Westernized populations living on diets consisting almost exclusively of meat, or meat and dairy products, could be remarkably healthy. Elmer McCollum, for example, stressed that groups such as the Eskimo and Plains Indians consumed not just the muscle meats of animals, but also the vitamin- and mineral-rich 22 For dietary advice to the overweight and obese, see, e.g., McCollum and Becker, Food, Nutrition and Health, 5th ed., 90-93, 113-16; Plimmer and Plimmer, Food, Health, Vitamins, 7th ed., 153-55; Morris Fishbein, Your Diet and Your Health (New York & London: Whittlesey House, 1937), 160-72. 420 glandular organs, as well as eggs and soft bones. \"Like the carnivorous animals which subsist on a similar diet,\" McCollum contended, \"the carnivorous peoples are healthy and capable of considerable endurance.\"23 Finally, as I explained in chapter two, most nutrition and dental researchers had come to the conclusion by the late 1930s that a \"diabetic diet\" high in fat-rich \"protective foods\" and low in refined carbohydrates was uniquely effective in preventing and even halting dental caries and gum disease. Yet the prospects for the acceptance of carbohydrate-restricted diets faded in the latter half of the twentieth century. In the two postwar decades, at least, many health authorities kept an open mind about what was causing the rising prevalence of overweight and obesity in the West. W. Henry Sebrell, for example, remarked in a 1957 presentation: \"All obesity results from overeating\u2014that is, eating more calories than are needed for energy. But the underlying causes of excessive appetite are very complex and incompletely understood.\" Moreover, as noted above, physicians well into the 1960s commonly prescribed carbohydrate-restricted diets to obese patients and reported their successes in the medical and scientific literature. Nevertheless, many health authorities refused to accept the notion that carbohydrates were somehow uniquely fattening, or that low-carbohydrate diets could reduce weight even when caloric intake was unrestricted. As Ronald Deutsch argued in his authoritative \"diet-debunking\" book, The Nuts Among the Berries (1961), \"The truth is that eating too much makes one heavier, and eating too little causes the body to burn up some of the reserve fat it has stored, make one thinner. There are no short cuts, no magic 23 McCollum, Orent-Keiles, and Day, Newer Knowledge of Nutrition, 5th ed., 582. Cf. McCollum, Newer Knowledge of Nutrition, 2nd ed., 388-90. Here my analysis differs to some extent from that of Gary Taubes, who claims that the nutritionists of the 1920s and 1930s did not know that a carnivorous diet could be healthy. See idem, Good Calories, Bad Calories, 321-22. 421 ways to reduce, only the nasty business of eating less than you would like to eat.\"24 But it was the rise of the fat-cholesterol hypothesis to conventional nutritional wisdom between the 1960s and 1980s, more than anything, that drove carbohydrate-restricted diets to the scientific margins. By the last third of the twentieth century, health authorities were telling obese individuals to follow calorie-restricted versions of the low-fat, high-carbohydrate diet that also supposedly reduced risk for heart disease. The research showing that this \"heart-healthy\" diet could actually worsen the metabolic dysfunction driving fat accumulation barely had an impact. Conclusion: Conservative Nutrition as a Scientific Tradition The failure of specialists in diabetes, obesity treatment, and the science of fat metabolism to influence the outcome of the debate over the healthfulness of low-fat diets between the 1960s and 1980s illustrates a larger pattern running throughout this study: the failure of nutrition experts, at key points in the twentieth century, to recognize the need for their views on the relationship between diet and health to agree across multiple disciplines and methodologies. Had they recognized this requirement for intrascientific agreement, I believe, authoritative dietary advice would not have vacillated so drastically over the course of the twentieth century. In other words, the researchers in the relevant scientific fields did not put the pieces together in time, and we are still\u2014as evidenced by the persistence and even worsening rates of heart disease, diabetes, obesity, and dental decay in the West\u2014living with the consequences. 24 Ronald M. Deutsch, The Nuts Among the Berries: An Expose of America's Food Habits (New York: Ballantine Books, 1961), 184. My claim that this book was \"authoritative\" is based on the strong endorsements it received from American nutrition and medical authorities. 422 I am not claiming, however, that lack of intrascientific agreement was the only factor driving the constant changing of official dietary advice in the twentieth century. As I have described in the previous chapters, nutrition researchers have often been unable to conduct the kinds of experiments that would provide definitive answers as to the nature of the ideal diet. As a result, there has been seemingly unending controversy on key issues in the field, such as the extent and nature of malnutrition, the health benefits of nutrient intakes above the minimal requirements, the value of synthetic vitamins, and the safety of various food additives. And because the science has been indefinite, the differing professional values, economic interests, and political inclinations of nutritionists and other health professionals have affected the way it has been interpreted and conveyed to the public. Furthermore, developments in the wider historical context like the Great Depression, World War II, and the postwar economic boom have also played a major role in how nutritional controversies have played out. My analysis therefore partially concurs with the recent scholarship that has placed blame for the frequent shifts in nutritional thinking over the past century on the scientific uncertainty inherent to the field, working in combination with extrascientific factors. But the historians and sociologists of science who have contributed to this scholarship, like many health authorities both past and present, have not appreciated the full extend of the research that has been conducted on the relationship between diet and health. The \"conservative nutrition\" perspective that I have traced in this study suggests that, while scientific uncertainty and extrascientific factors have indeed been crucial in driving the apparently endless changes in what the experts consider healthy eating over the past century, knowledge of the nature of the ideal diet has nevertheless increas ed in depth and coherence at the same time. Namely, there has been a steady accumulation of scientific evidence 423 against the consumption of refined starches and sugars (and to a lesser extent refined fats), and in favor of what Elmer McCollum originally termed the \"protective foods\"\u2014dairy products, eggs, organ and muscle meats, seafood, vegetables, fruits, and wholegrain foods. A smaller but nonetheless compelling body of research indicates that most individuals suffering from \"diseases of civilization\" such as atherosclerosis, obesity, and diabetes would benefit from restricting their intake of carbohydrate-rich foods even in their unrefined, natural state, at least until the metabolic dysfunctions underpinning the conditions are normalized. My interpretation of the evolution of nutrition science over the course of the twentieth century also diverges from that presented in the recent literature on the ideology of \"nutritionism.\" According to contributors to this literature such as Gyorgy Scrinis and Michael Pollan, the nutrient-centered view of food and diet long championed by nutrition scientists has abetted the further industrialization of the food supply by allowing food manufacturers to promote highly processed\u2014and ultimately unhealthful\u2014foods solely on the basis of their nutrient content. Critics of nutritionism also argue that the nutrient-centered approach to food has helped to produce popular confusion and anxiety about diet, as well as unwarranted faith in scientific expertise. A healthier approach to food and eating, they claim, should include traditional, ecological, and sensual dimensions. Yet I have found that the legacy of nutrition science has been considerably more ambivalent than the largely negative assessment put forward by the critics of nutritionism. As I described in chapter one, nutrition scientists commo nly criticized the \"industrial\" or \"Western\" diet and praised certain \"traditional\" or \"primitive\" diets between the 1910s and 1940s. Furthermore, the role that the nutrient-centered approach to food has played in advancing the industrialization of the food supply has been, at best, ambiguous. The 424 researchers who were at the forefront of nutrition science in the first half of the twentieth century believed that their improving knowledge of individual vitamins and minerals would help to elucidate the nature of the ideal diet, but they did not then claim that they had found all the beneficial nutrients present in natural foods. This conservative attitude can be seen in the response of the nutrition science commu nity to the enrichment movement, described in chapter three. Many nutritionists opposed enrichment of flour and bread when it was first proposed in the late 1930s and early 1940s on the grounds that too little was known about the makeup of natural foods to permit acceptance of refined foods fortified with synthetic vitamins. Enrichment proponents, for their part, generally supported only the \"restoration\" of vitamins to milled cereals up to the levels found in their natural, unrefined counterparts. In addition, the researchers who supported enrichment felt that it was merely a temporary expedient while the slower process of educating the public about the value of wholegrain foods took place. It was only in the two postwar decades that nutrition authorities becam e complacent about the consumption of refined bread and cereals, and even this stance was short-lived: research in recent decades has indicated that the consumption of wholegrain foods reduces risk for developing certain chronic degenerative diseases.25 Nor, contrary to the critics of nutritionism, have nutrition scientists overlooked the ecological dimensions of eating. As I described in chapter three, as early as the interwar period researchers explored the influence of environmental factors such as soil fertility and climate on the nutritional content of food and forage crops, and, in turn, on animal and human health. Moreover, this emerg ing \"nutritional ecology\" perspective arose in tandem 25 See, e.g., Joanne L. Slavin et al., \"The Role of Whole Grains in Disease Prevention,\" Journal of the American Dietetic Association 101 (July 2001): 780-85; C. J. Seal, \"Whole Grains and CVD Risk,\" Proceedings of the Nutrition Society 65 (2006): 24-34; J. Mann, \"Dietary Carbohydrate: Relationship to Cardiovascular Disease and Disorders of Carbohydrate Metabolism,\" European Journal of Clinical Nutrition 61 (Suppl 1) (2007): S100-11. 425 with increasing concern about the environmental and social consequences of what were then conventional farming methods, and even informed the discourse among scientists and government officials over how to respond to the erosion crisis of the 1930s. This \"nutritional conservation\" viewpoint favored decreasing the number of acres devoted to arable grain crops and increasing that devoted to pasture and green forage for livestock, as well as fruits and vegetables. Only in the two postwar decades did this viewpoint fall into obscurity and get taken up\u2014and reinterpreted\u2014by small dissident groups such as the organic and natural food movements. In other words, nutrition science has not always been, as is commo nly assumed in the discourse on nutritionism, indifferen t toward the wider ecological dimensions of eating. At the beginning of this dissertation, I quoted a note of caution that nutritionist Clive McCay gave to his colleagues in 1947, just when the last vitamin (B12) was being discovered and confidence in nutrition science was at its twentieth-century apogee. \"The history of nutrition,\" McCay wrote, \"tends to inculcate a spirit of modesty in regard to our own time and to make us realize that we have made but a beginning in solving the intricate and difficult problems of feeding men.\" But now, assessments of nutrition science have gone beyond the merely cautionary, with observers either claiming that the field is inherently too uncertain to permit provable hypotheses to be formulated or dismissing its conceptual foundations altogether. The \"conservative nutrition\" perspective that I have described in this study challenges these interpretations. While many questions about the nature of the ideal diet remain unanswered, our best response is not to simply shrug our shoulders or hold up \"tradition,\" \"culture,\" or \"ecology\" as superior alternatives to nutrition science. For decades, researchers in fields as diverse as biochemistry, physiology, anthropology, medicine, dentistry, and agricultural science have had something to say about the relationship between 426 nutrition and health. Although these voices have at times sounded like a cacophony, they have, over the long term, become more harmonious. This is just as well, since whatever we in the industrialized world had been taught about healthy eating before the emerg ence of nutrition science did not prevent us from adopting a dietary pattern that was far from perfect. 427 BIBLIOGRAPHY ARCHIVAL AND MANSUCRIPT COLLECTIONS Cornell University, Carl A. Kroch Library (Cornell, NY) Leonard A. Maynard Papers Clive M. McCay Papers Malabar Farm State Park Library (Lucas, OH) Louis Bromfield Papers Jonathan Forman Papers Price-Pottenger Nutrition Foundation (La Mesa, CA) Granville F. Knight Papers Royal Lee Papers Francis M. Pottenger, Jr. Papers Weston A. Price Papers William A. Albrecht Papers Rowett Research Institute, Archives and Manuscripts (Aberdeen, UK) Sir John Boyd Orr Papers Rowett Research Institute Archives University of Wisconsin-Madison, Steenbock Memorial Library (Madison, WI) Conrad Elvehjem Papers Edward B. Hart Papers Paul H. Phillips Papers Harry Steenbock Papers Emil Truog Papers University of California, Berk eley, Bancroft Library (Berkeley, CA) Agnes Fay Morgan Papers Vanderbilt University, Eskind Biomedical Library, History of Nutrition Collection (Nashville, TN) American Institute of Nutrition Archives Franklin C. Bing Papers John R. Murlin Papers 428 E. Neige Todhunter Papers John B. Youmans Papers Wellcome Library, Archives and Manuscripts (London, UK) Dame Harriette Chick Papers \"Peter\" Thomas Latimore Cleave Papers Sir Edward Mellanby Papers May Mellanby Papers SELECTED PERIODICALS American Journal of Clinical Nutrition American Journal of Public Health American Journal of Diseases of Children Archives of Internal Medicine Biochemical Journal British Dental Journal British Heart Journal British Medical Journal Circulation Dental Cosmos Ecology Federation Proceedings Journal of the American Dental Association Journal of the American Dietetic Association Journal of the American Medical Association Journal of Applied Nutrition Journal of Biological Chemistry Journal of Dairy Science Journal of Dental Research Journal of Home Economics Journal of Nutrition The Lancet The Land Modern Nutrition Natural Food and Farming Nature Nutrition Abstracts and Reviews Nutrition Reviews Physiological Reviews Plant Physiology Proceedings of the Nutrition Society Proceedings of the Royal Society of Medicine Proceedings of the Society for Experimental Biology and Medicine 429 Science Soil Science Soil Science Society of America Proceedings SELECTED PRIMARY SOURCES Albrecht, William A. Soil Fertility and Animal Health. Webster City, IA: Hahne Publishing, 1958. ________. The Albrecht Papers: Volume I. Edited by Charles Walters, Jr. Kansas City, MO: Acres U.S.A., 1975. ________. The Albrecht Papers: Volume II. Edited by Charles Walters, Jr. Kansas City, MO: Acres U.S.A., 1975. ________. The Albrecht Papers: Hidden Lessons in Unopened Books (Volume III). Edited by Charles Walters, Jr. Kansas City, MO: Acres U.S.A., 1989. AMA Council on Pharmacy and Chemistry. New and Non-Official Remedies. Chicago: AMA, 1946. AMA Council on Foods and Nutrition. Handbook of Nutrition. New York: Blakiston, 1951. Astor, Viscount, and Seebohm B. Rowntree. British Agriculture: The Principles of Future Policy. London/New York: Penguin, 1938. ________. Mixed Farming and Muddled Thinking: An Analysis of Current Agricultural Policy. London: Macdonald & Co., 1946. Bach arach, Alfred L. Science and Nutrition. London: Watts, 1938. Balfour, Lady Eve. The Living Soil. London: Faber and Faber, 1943. Bear, Firman E. Soils and Fertilizers. New York: John Wiley & Son; London: Chapman and Hall, 1942. ________. Earth: The Stuff of Life. Norman, OK: Oklahoma University Press, 1961. Bees on, Kenneth C. The Mineral Composition of Crops with Particular Reference to the Soils in which They Were Grown: A Review and Compilation. USDA Misc. Pub. No. 369. Washington: USDA, 1941. Bennett, Hugh H. Soil Conservation. New York, London: McGraw-Hill, 1939. 430 Bennett, Hugh H., and W. R. Chapline. Soil Erosion: A National Menace. Washington: USDA, 1928. Bicknell, Franklin, and Frederick Prescott. 2nd ed. The Vitamins in Medicine. London: Heinemann, 1946. Blakeslee, Alton, and Jeremiah Stamler. Your Heart Has Nine Lives: Nine Steps to Heart Health. New York: Pocket Books, 1966. Borsook, Henry. Vitamins: What They Are and How They Can Benefit You. New York: Viking Press, 1941. Borsook, Henry, and William Huse. Vitamins for Health. New York: Public Affairs Committee, 1942. Bromfield, Louis. Pleasant Valley. New York, London: Harper & Brothers, 1945. ________. Malabar Farm. New York: Harper & Brothers, 1948. ________. Out of the Earth. New York: Harper & Brothers, 1950. ________. From My Experience. New York: Harper & Brothers, 1955. Bunting, Russell W. The Story of Dental Caries. Ann Arbor, MI: Overbeck, 1953. BMA. Nutrition and the Pubic Health: Medicine, Agriculture, Industry, Education. Proceedings of a National Conference on the Wider Aspects of Nutrition, April 27-29, 1939. London: BMA, 1939. Carter, Vernon Gill, and Tom Dale. Topsoil and Civilization. Norman, OK: Oklahoma University Press, 1955. Chaney, Marg aret Stella. Nutrition, 5th ed. Cambridge, MA: Houghton Mifflin, 1954. Chase, Stuart. Rich Land, Poor Land: A Study of Waste in the Natural Resources of America. New York: Whittelsey House, 1936. Cleave, T. L. \"The Neglect of Natural Principles in Current Medical Practice.\" Journal of the Royal Naval Medical Service 42, no. 2 (Spring 1956): 55-83. ________. Fat Consumption and Coronary Disease: An Evolutionary Answer to This Problem. Bristol: John Wright & Sons, 1957. ________. The Saccharine Disease: The Master Disease of Our Time. New Canaan, CT: Keats Publishing, 1975. 431 Cleave, T. L., and G. D. Campbell. Diabetes, Coronary Thrombosis, and the Saccharine Disease. Bristol: John Wright & Sons, 1966. Clendening, Logan. The Care and Feeding of Adults. New York: A. A. Knopf, 1931. ________. The Balanced Diet. New York: D. Appleton-Century, 1936. Committee on Medical Aspects of Food. Diet and Cardiovascular Disease: Report of the Panel on Diet in Relation to Cardiovascular Disease. London: HMSO, 1984. Cowdry, E. V. ed. Arteriosclerosis: A Survey of the Problem. (New York: Macmillan, 1933). Cowgill, George Raymond. The Vitamin B Requirement of Man. New Haven, CT: Yale University Press, 1934. Crawford, Sir William, and Herbert Broadley. The People's Food. London: Heinemann, 1938. Daniel, Esther Peterson, and Hazel E. Munsell. Vitamin Content in Foods: A Summary of the Chemistry of Vitamins, Units of Measurement, Quantitative Aspects in Human Nutrition and Occurrence in Foods. USDA Misc. Pub No. 275. Washington, DC: USDA, 1937. Davis, Adelle. Let's Eat Right to Keep Fit. New York: Harcourt, Brace, 1954. Deutsch, Ronald M. The Nuts Among the Berries: An Expos\u00e9 of America's Food Fads. New York: Ballantine Books, 1961. Donnison, C. P. Civilization and Disease. Baltimore: William Wood, 1938. Drummond, J. C. Lane Medical Lectures: Biochemical Studies of Nutritional Problems. London: Humphrey Milford, 1934. Dunlap, F. L. White Versus Brown Flour. Belleville, NJ: Wallace & Tiernan Co., 1945. Economic Refo rm Club. Health, Agriculture and the Standard of Living. London, 1939. Elwood, Catharyn. Feel Like a Million. New York: Devin-Adair, 1956. Faulkner, Edward. Plowman's Folly. Norman, OK: Oklahoma University Press, 1943. Federal Security Agency, Office of the Director of Defense Health and Welfare Services. Proceedings of the National Nutrition Conference for Defense. Washington: GPO, 1942. Fishbein, Morris. Your Diet and Your Health. New York, London: Whittlesey House, 1937. 432 ________. The National Nutrition. Indianapolis: Bobbs-Merrill, 1942. Forman, Jonathan, and O. E. Fink, eds. Soil, Food and Health: \"You Are What You Eat.\" Columbus, OH: Friends of the Land, 1948. Fredericks, Carlton. Living Should Be Fun. New York: Institute of Nutritional Research, 1943. Fredericks, Carlton, and Herbert Bailey. Food Facts and Fallacies: The Intelligent Person's Guide to Nutrition and Health. New York: Arc Books, 1965. Funk, Casimir. The Vitamines. 2nd ed. Translated by Harry E. Dubin. Baltimore: Wilkins and Wilkins, 1922. Gilbert, Frank A. Mineral Nutrition of Plants and Animals. Norman, OK: University of Oklahoma Press, 1948. ________. Mineral Nutrition and the Balance of Life. Norman, OK: University of Oklahoma Press, 1957. Gofman, John W., Alex V. Nichols, and E. Virginia Dobbin. Dietary Prevention and Treatment of Heart Disease. New York: G. P. Putnam's Sons, 1958. Goodhart, Robert S. Nutrition for You. New York: Dutton, 1958. Graham, Michael. Soil and Sense. London: Faber and Faber, 1944. Hauser, Gayelord. Look Younger, Live Longer. New York: Farrar, Straus and Co., 1950. HMSO. Plant and Animal Nutrition in Relation to Soil and Climatic Factors. London: HMSO, 1951. Hilgard, Eugene W. Soils: Their Formation, Properties, Composition, and Relation to Climate and Plant Growth in the Humid and Arid Regions. New York: Macmillan, 1906. Holt, Luther Emmett. Food, Health and Growth: A Discussion of the Nutrition of Children. New York: Macmillan, 1922. Hopkins, Cyril G. Soil Fertility and Permanent Agriculture. Boston, New York: Ginn, 1910. Hopkins, Donald P. Chemicals, Humus, and the Soil. London: Faber and Faber, 1944. Howard, Albert. An Agricultural Testament. London: Oxford University Press, 1940. 433 ________. The War in the Soil. Emmaus, PA: Organic Gardening, 1946. _______. The Soil and Health: A Study of Organic Agriculture. New York: Devin-Adair, 1947. Howard, Louise E. The Earth's Green Carpet. Emmaus, PA: Rodale Press, 1947. Hyams, Edward. Soil and Civilization. London, New York: Thames and Hudson, 1952. Imperial Bureau of Animal Nutrition. Wheat: Pre-Eminence as a Cereal Food; Nutritive Value; Relation to Health and Disease. Technical Commu nication No. 7. Aberdeen: Rowett Institute, 1936. Jacks, G. V., and R. O. Whyte. The Rape of the Earth: A World Survey of Soil Erosion. London: Faber & Faber, 1939. Jenks, Jorian. The Stuff Man's Made Of. London: Faber and Faber, 1959. Jenny, Hans. Factors of Soil Formation: A System of Quantitative Pedology. New York, London: Mc-Graw Hill, 1941. Jolliffe, Norman, ed. Clinical Nutrition. New York: Hoeber, 1950. Kellogg, Charles. The Soils That Support Us. New York: Macmillan, 1941. King, Franklin H. Farmers of Forty Centuries. Madison, WI: Mrs. F. H. King, 1911. Kordel, Lelord. Health through Nutrition. New York: The World Publishing Co., 1950. Lapp\u00e9, Francis Moore. Diet for a Small Planet. New York: Friends of the Earth/Ballantine Books, 1971. League of Nations. Final Report of the Mixed Committee on the Relation of Nutrition to Health, Agriculture and Economic Policy. Geneva: League of Nations, 1937. League of Nations Health Committee. Rural Dietaries in Europe. Series of League of Nations Publications No. 26. Geneva: League of Nations, 1939. Lee, Royal, and Jerome S. Stolzoff. The Special Nutritional Qualities of Natural Foods. Report No. 4. Milwaukee: Lee Foundation for Nutritional Research , 1942. Leighton, Gerald, and Peter L. McKinlay, Milk Consumption and the Growth of School Children: Report on an Investigation in Lanarkshire Schools. Edinburgh: HMSO, 1930. Leslie Harris, Vitamins in Theory and Practice, 3rd ed. New York: Macmillan; Cambridge: Cambridge University Press, 1938. 434 Lord, Russell. To Hold This Soil. Washington: USDA, 1938. ________. Behold Our Land. Boston: Houghton-Mifflin Co., 1938. ________. The Care of the Earth: A History of Husbandry. New York: Nelson, 1962. Lowdermilk, W. C. Conquest of the Land Through 7,000 Years. Washington: GPO, 1950. Macy , Icie Gertrude, and Harold Henderson Williams. Hidden Hunger. Lancaster, PA: Jaques Cattell Press, 1945. Mann, H. C. Corry. Diets for Boys During the School Age, Medical Research Council Special Report, Series No. 105. London: HMSO, 1926. Mart in, W. Coda. A Matter of Life: Blueprint for a Healthy Family. New York: Devin-Adair, 1964. Massingham, H. J., ed. England and the Farmer: A Symposium. London: B. T. Batsford, 1941. Maynard, Leonard A. Animal Nutrition. New York: McGraw-Hill, 1937. McCance, R. A., and E. M. Widdowson. Breads, White and Brown: Their Place in Thought and Social History. London: Pitman Medical Pub., 1956. McCarrison, Robert. Studies in Deficiency Disease. London: Frowde and Hodder and Stoughton, 1921. McCarrison, Sir Robert. The Works of Sir Robert McCarrison, ed. H. M. Sinclair. London: Faber and Faber, 1952. ________. Nutrition and National Health, Being the Cantor Lectures Delivered Before the Royal Society of Arts, 1936. London: Faber and Faber, 1953. McCollum, Elmer Verner. The Newer Knowledge of Nutrition: The Use of Food for the Preservation of Vitality and Health. New York: Macmillan, 1918. ________. The Newer Knowledge of Nutrition: The Use of Food for the Preservation of Vitality and Health. 2nd ed. New York: Macmillan, 1922. McCollum, E. V., and Nina Simmonds. Food, Nutrition and Health. Baltimore: Lord Baltimore Press, 1925. McCollum, E. V., Elsa Orent-Keiles, and Harry G. Day. The Newer Knowledge of Nutrition. 5th ed. New York: Macmillan, 1939. 435 McCollum E. V., and J. Ernestine Becker. Food, Nutrition and Health. Rev. ed. Baltimore: Lord Baltimore Press, 1940. McLester, James S. Nutrition and Diet in Health and Disease. Philadelphia: W. B. Saunders, 1929. Mellanby, Edward. Experimental Rickets. MRC Special Report Series No. 61. London: HMSO, 1921. ________. Nutrition and Disease: The Interaction of Clinical and Experimental Work. Edinburgh and London: Oliver and Boyd, 1934. ________. A Story of Nutritional Research: The Effect of Some Dietary Factors on Bones and the Nervous System. Abraham Flexner Lectures, Series No. 9. Baltimore, MD: Williams & Watkins Co., 1950. Mellanby, May. Diet and the Teeth: An Experimental Study. MRC Special Reports, Series No. 140. London: HMSO, 1930. ________. Diet and the Teeth: An Experimental Study, Part II. MRC Special Reports, Series No. 153. London: MRC, 1930. ________. Diet and the Teeth: An Experimental Study, Part III. MRC Special Reports, Series No. 191. London: MRC, 1934. Michigan State University College of Agriculture. Centennial Symposium: Nutrition of Plants, Animals, Man. Michigan State University: 1955. Mickey, Karl B. Health from the Ground Up. Chicago: International Harvester Co., 1946. Milbank Memorial Fund. Nutrition: The Newer Diagnostic Methods. New York: Milbank Memorial Fund, 1938. Milk Nutrition Committee. Milk and Nutrition: New Experiments Reported to Milk Nutrition Committee from the National Institute for Research in Dairying, Vols. 1-4. Read ing: Poynder, 1937-39. Miller, Fred D. Open Door to Health: A Dentist Looks at Life and Nutrition. New York: Devin-Adair, 1958. Miller, W. D. Micro-Organisms of the Human Mouth. Philadelphia: S. S. White Publishing Co., 1890. Ministry of Food. Report of the Conference on the Post-War Loaf. London: HMSO, 1945. NRC Committee on Cereals. Cereal Enrichment in Perspective. Washington, DC: NAS-NRC Council Food and Nutrition Board, 1958. 436 NRC Committee on Dental Health, A Survey of the Literature of Dental Caries. Washington: NAS, 1952. NRC Food and Nutrition Board, Committee on Diagnosis and Pathology of Nutritional Deficiencies, Inadequate Diets and Nutritional Deficiencies in the United States: Their Prevalence and Significance. NRC Bull. No. 109. Washington: NRC, 1943. NRC Food and Nutrition Board, Committee on Nutrition Surveys. Nutrition Surveys: Their Tech niques and Value. NRC Bull. No. 17. Washington: NAS-NRC, 1949. NRC Food and Nutrition Board. Toward Healthful Diets. Washington: NAS, 1980. Norman, N. Philip. The \"Triad Disease\": Mankind's No. 1 Killer. Milwaukee, WI: The Lee Foundation for Nutritional Research, 1958. Orr, John Boyd. The National Food Supply and its Influence on Public Health. Westminster: P. S. King & Son, 1934. ________. Food, Health and Income: A Survey of Adequacy of Diet in Relation to Income, 2nd ed. London: Macmillan, 1937. Orr, J. B., and J. L. Gilks. Studies of Nutrition: The Physique and Health of Two African Tribes. Medical Research Council, Special Reports Series No. 155. London: HMSO, 1931. Orr, John Boyd, and Helen Scherbatoff. Minerals in Pastures and Their Relation to Animal Nutrition. London: H. K. Lewis & Co Ltd., 1929. Panel on Composition and Nutritive Value of Flour. Report of the Panel on the Composition and Nutritive Value of Flour, Command Paper 9757. London: HMSO, 1956. Pears e, Innes, and Lucy Crocker. The Peckham Experiment: A Study in the Living Structure of Society. London: Allen and Unwin, 1944. Pfeiffer, Ehrenfried. Bio-dynamic Farming and Gardening: Soil Fertility, Renewal and Preservation. New York: Anthroposophic Press; London: Rudolf Steiner Publishing Co., 1938. ________. The Earth's Face and Human Destiny. Emmaus, PA: Rodale Press, 1948. Pickerill, H. P. The Prevention of Dental Caries and Oral Sepsis. London: Balliere, Tindall, and Cox, 1912. Picton, Lionel James. Nutrition and the Soil: Thoughts on Feeding. New York: Devin-Adair, 1949. 437 Plimmer, R. H. A., and Violet G. Plimmer. Vitamins and the Choice of Food. London: Longmans, Green and Company, 1922. ________. Food, Health, Vitamins, 7th ed. London: Longmans, Green and Co., 1935. Price, Weston A. Dental Infections. 2 vols. Cleveland, OH: Penton Publishing Co., 1923. ________. Nutrition and Physical Degeneration: A Comparison of Primitive and Modern Diets and Their Defects. New York: Hoeber, 1939. ________. Nutrition and Physical Degeneration. 2nd ed. The Author, 1945). Pyke, Magnus. Townsman's Food. London: Turnstile Press, 1952. Quigley, Daniel Thomas. Notes on Vitamins and Diet. Chicago: Consolidated Book Pub., 1933. ________. The National Malnutrition. 2nd ed. Milwaukee: Lee Foundation for Nutritional Research, 1950. Ratcliffe, Francis. Flying Fox and Drifting Sand: The Adventures of a Biologist in Australia. Sydney: Angus & Robertson, 1951. Rodale, J. I. Pay Dirt: Farming and Gardening with Composts. New York: Devin-Adair, 1945. The Healthy Hunzas. Emmaus, 1948. ________. The Organic Front. Press, 1948. Roholm, Kaj. Fluorine Intoxication: A Clinical-Hygienic Study, with a Review of the Literature and Some Experimental Investigations. Copenhagen: Nyt nordisk forlag; London: H.K. Lewis & Co., 1937. Rorty, James, and N. Philip Norman. Tomorrow's Food: The Coming Revolution in Nutrition. New York: Prentice-Heall, 1947. ________. Tomorrow's Food. Rev. ed. New York: Devin-Adair, 1956. Schopfer, William H. Plants and Vitamins. Waltham, MA: Chronica Britannica, 1949. Sears , Paul. Deserts on the March. Norman, OK: Oklahoma University Press, 1935. Select Committee on Nutrition and Human Needs of the United States Senate. Dietary Goals for the United States. Washington, DC: GPO, 1977. 438 Sherman, Henry C. Chemistry of Food and Nutrition. New York: Macmillan, 1918. ________. Food and Health. New York: Macmillan, 1934. ________. Calcium and Phosphorus in Foods and Nutrition. New York: Columbia University Press, 1947. ________. The Nutritional Improvement of Life. New York: Columbia University Press, 1950. Sherman, Henry C., and Constance S. Pearson. Modern Bread from the Viewpoint of Nutrition. New York: Macmillan, 1942. Smith, J. Russell. Tree Crops: A Permanent Agriculture. Rev. ed. New York: The Devin-Adair Company, 1950. Spies, Tom. Rehabilitation through Better Nutrition. Philadelphia, London: W. B. Saunders Co., 1947. Sprague, Howard B., and Stanley Arthur Barb er. Hunger Signs in Crops: A Symposium. New York: McKay, 1964. Storck, John, and Walter Dorwin Teague. Flour for Man's Bread. Minneapolis, MN: University of Minnesota Press, 1952. Sykes, Friend. Humus and the Farmer. London: Faber and Faber, 1946. ________. Food, Farming, and the Future. London: Faber and Faber, 1951. USDA. Soils and Men: Yearbook of Agriculture, 1938. Washington: GPO, 1938. ________. Food and Life: Yearbook of Agriculture, 1939. Washington: GPO, 1939. ________. Climate and Man: Yearbook of Agriculture, 1941. Washington: GPO, 1941. ________. Diets of Families of Employed Wage Earners and Clerical Workers in Cities, by Hazel K. Stiebeling and Esther F. Phipard. USDA Circ. No. 507. Washington: GPO, 1939. ________. Are We Well Fed? A Report on the Diets of Families in the United States, by Hazel K. Stiebeling. USDA Misc. Pub. No. 430. Washington, DC: USDA, 1941. ________. Factors Affecting the Nutritive Value of Foods: Studies at the U. S. Plant, Soil, and Nutrition Laboratory. Misc. Pub. No. 664. Washington: GPO, 1948. 439 ________. Proceedings of the National Food and Nutrition Institute. USDA Agricultural Handbook No. 56. Washington: GPO, 1953. ________. Science in Farming: Yearbook of Agriculture, 1943-1947. Washington: GPO, 1947. ________. Grass: Yearbook of Agriculture, 1948. Washington: GPO, 1948. ________. Soil: Yearbook of Agriculture, 1957. Washington: GPO, 1957. ________. Food: Yearbook of Agriculture, 1959. Washington: GPO, 1959. ________. Power to Produce: Yearbook of Agriculture, 1960. Washington: GPO, 1960. ________. Nutrition and Your Health: Dietary Guidelines for Americans. Washington: USDA, 1980. Wallace, Henry. New Frontiers. New York: Reynal & Hitchcock, 1934. Waksman, Selman A. Soil Microbiology. New York: John Wiley & Sons, 1954. Wickenden, Leonard. Our Daily Poison: The Effects of DDT, Fluorides, Hormones, and Other Chemicals on Modern Man. New York: Devin-Adair, 1955. Widdowson, E. M., and R. A. McCance. Studies on the Nutritive Value of Bread and on the Effect of Variations in the Extraction Rate of Flour on the Growth of Undernourished Children. Medical Research Council Special Report, Series 287. London: HMSO, 1954. Wilder, Russell M, and Robert R. Williams. Enrichment of Flour and Bread: A History of the Movement. NRC Bull. No. 110. Washington, DC: NRC, 1944. Williams, Robert R., and Tom D. Spies. Vitamin B1 (Thiamin) and Its Use in Medicine. New York: Macmillan, 1938. Williams, Roger. Biochemical Individuality: The Basis for the Genotrophic Concept. New York: John Wiley & Sons, 1956. Williamson, G. S. Physician Heal Thyself: A Study of Need s and Means. London: Faber and Faber, 1944. Wohl, Michael G. Dietotherapy: Clinical Application of Modern Nutrition. Philadelphia and London: W. B. Saunders Co., 1945. Wohl, Michael G., and Robert S. Goodhart, eds. Modern Nutrition in Health and Disease: Dietotherapy. Philadelphia: Lea & Febiger, 1955. 440 Wood, H. Curtis, Jr. Overfed But Undernourished: Nutrition Aspects of Health and Disease. New York: Exposition Press, 1959. Wrench, Guy Theodore. The Wheel of Health. London: C. W. Daniel, 1938. ________. Reconstruction by Way of the Soil. London: Faber and Faber, 1946. Yudkin, John. Pure, White, and Deadly: The Problem of Sugar. London: Davis-Poynter, 1972. SECONDARY SOURCES Ackerman, Michael. \"Interpreting the 'Newer Knowledge of Nutrition': Science, Interests, and Values in the Making of Dietary Advice in the United States, 1915-1965.\" Ph.D. diss, University of Virginia, 2005. ________. \"Science and the Shadow of Ideology in the American Health Foods Movement.\" In The Politics of Healing: Histories of Alternative Medicine in Twen tieth-Century North America, ed. Robert D. Johnston. New York: Routledge, 2004. Anderson, David. \"Depression, Dust Bowl, Demography and Drought: The Colonial State and Soil Conservation in East Africa during the 1930s.\" African Affairs 83, no. 322 (July 1984): 321-43. Apple, Rima D. Vitamania: Vitamins in American Culture. New Brunswick, NJ: Rutgers University Press, 1996. ________. \"Vitamins Win the War: Nutrition, Commerce, and Patriotism in the United States during the Second World War.\" In Food, Science, Policy, and Regulation in the Twentieth Century: International and Comparative Perspectives, eds. David F. Smith and Jim Phillips, 135-49. London: Routledge, 2000. Aronson, Naomi. \"The Discovery of and Scientific Careers.\" Isis 77 (1986): 630-46. Back strand, Jeffrey R. \"The History and Future of Food Fortification in the United States: A Public Health Perspective,\" Nutrition Reviews 60, no. 1 (January 2002): 15-26. Baro na, Joseph. \"Nutrition and Health: The International Context During the Inter-war Crisis,\" Social History of Medicine 21, no. 1 (2008): 87-105. Beeman, Randal. \"Friends of the Land and the Rise of Environmentalism, 1940-1954.\" Journal of Agricultural and Environmental Ethics 8, no. 1 (1995): 1-16. 441 Beeman, Randal, and James Pritchard. A Green and Permanent Land: Ecology and Agriculture in the Twentieth Century. Lawrence, KS: University of Kansas Press, 2001. Bees on, Kenneth C., and Gennard Matrone. The Soil Factor in Nutrition: Animal and Human. New York: Marcel Dekker, 1976. Beinart, William. \"Soil Erosion, Conservationism and Ideas about Development: A Southern African Exploration, 1900-1960.\" Journal of Southern African Studies 11, no. 1 (October 1984): 52-83. ________. The Rise of Conservation in South Africa: Settlers, Livestock, and the Environment, 1770-1950. Oxford: Oxford University Press, 2008. Belasco, Warren J. Appetite for Change: How the Counterculture Took on the Food Industry. Rev. ed. Ithaca, NY: Cornell University Press, 1993. ________. Meals to Come: A History of the Future of Food. Berk eley, CA: University of California Press, 2006. Bing, Franklin C., and Harry J. Prebluda. \"E. V. McCollum: Pathfinder in Nutrition Investigations and World Agriculture.\" Agricultural History 54 (January 1980): 157-66. Bishai, David, and Rita Nalubola. \"The History of Food Fortification in the United States: Its Relevance for Current Fortification Efforts in Developing Countries.\" Economic Development and Cultural Change 51 (October 2002). Blum, Barton. \"Composting and the Roots of Sustainable Agriculture.\" Agricultural History 66 (Spring 1992): 171-88. Bran dt, Karl. The Reconstruction of World Agriculture. New York: W. W. Norton & Company, 1945. Bran tley, Cynthia. \"Kikuyu-Maasai Nutrition and Colonial Science: The Orr and Gilks Study in Late 1920s Kenya Revisited.\" International Journal of African Historical Studies 30, No. 1 (1997): 49-86. Brown, P. S. \"Nineteenth-Century American Health Reformers and the Early Nature Cure Movement in Britain.\" Medical History 32 (1988): 174-94. Bryson, Christopher. The Fluoride Deception. New York: Seven Stories Press, 2004. Bufton, Mark W. \"British Expert Advice on Diet and Heart Disease c.1945-2000.\" In Making Health Policy: Networks in Research and Policy After 1945, ed. Virginia Berri dge, 125-48. New York: Rodopi, 2005. 442 Bufton, Mark W., David F. Smith, and Virginia Berridge. \"Professional Ambitions, Political Inclinations, and Protein Problems: Conflicts and Compromise in the BMA Nutrition Committee 1947-1950.\" Medical History 47, no. 4 (2003): 473-92. Burchardt, Jeremy. Paradise Lost: Rural Idyll and Social Change in England Since 1800. London: I. B. Taurus, 2002. Burnett, John. Plenty and Want: A Social History of Diet in England from 1815 to the Present Day. London: Nelson, 1966. Cannadine, David. \"The Past and Present in the Industrial Revolution 1880-1980.\" Past and Present 103 (1984): 131-72. Carp enter, Kenneth J. \"Acute versus Marginal Deficiencies of Nutrients,\" Nutrition Reviews 60 (September 2002): 277-80. ________. \"A Short History of Nutritional Science: Journal of Nutrition 133 (2003): 977-83. ________. \"A Short History of Nutritional Science: Journal of Nutrition 133 (2003): 3023-32. ________. \"A Short History of Nutritional Science: Part 4 (1945-1985).\" Journal of Nutrition 133 (2003): 3331-42. Carp enter, Kenneth J., Alfred E. Harper, and Robert E. Olson. \"Experiments that Changed Nutritional Thinking.\" Journal of Nutrition 127 (1997): 1017S-53S. Carter, K. Codell. \"The Germ Theory, Beriberi, and the Deficiency Theory of Disease.\" Medical History 21 (1977): 119-36. Chick, Harriette. \"Study of Rickets in Vienna, 1919-1922.\" Medical History 20, no. 1 (January 1976): 41-51. Conford, Philip. The Origins of the Organic Movement. Edinburgh: Floris Books, 2001. Conkin, Paul K. A Revolution Down on the Farm: The Transformation of American Agriculture Since 1929. Lexington, KY: University Press of Kentucky, 2008. Cummings, Richard Osborn. The American and His Food: A History of Food Habits in the United States. Chicago: University of Chicago Press, 1941. Cunfer, Geoff. On the Great Plains: Agriculture and Environment. College Station, TX: Texas A&M University Press, 2005. Dale, Henry H. \"Edward Mellanby, 1884-1955.\" Biographical Memoirs of the Fellows of the Royal Society 1 (November 1955): 193-292. 443 Day, Harry G. \"E. V. McCollum: Doyen of Nutrition Science.\" Nutrition Reviews 37, no. 3 (March 1979): 66-67. Dixon, Jane. \"From the Imperial to the Empty Calorie: How Nutrition Relations Underpin Food Regime Transitions.\" Agriculture and Human Values 26 (2009): 321-33. Dixon, Jane, and Cathy Bramwell. \"Re-embedding Trust: Unraveling the Construction of Modern Diets.\" Critical Public Health. 14, no. 2 (June 2004): 117-31. Drummond, J. C., and Anne Wilbraham. The Englishman's Food: A History of Five Centuries of English Diet. London: J. Cape, 1939. Duncan, Colin. The Centrality of Agriculture: Between Humankind and the Rest of Nature. Montreal, Kingston: McGill-Queen's University Press, 1996. DuPuis, E. Melanie. Nature's Perfect Food: How Milk Became America's Drink. New York: New York University Press, 2002. Eliot, Christopher. \"Method and Metaphysics in Clements's and Gleason's Ecological Explanations.\" Studies in History and Philosophy of Biological and Biological Sciences 38 (2007): 85-109. Enig, Mary. Know Your Fats. Bethesda, MD: Bethesda Press, 2000. Forbes, Eric G. \"The Professionalization of Dentistry in the United Kingdom.\" Medical History 29 (1985): 169-81. Freeze, R. Allan, and Jay H. Lehr. The Fluoride Wars: How a Modest Public Health Measure Became America's Longest Running Political Melodrama. Hoboken, NJ: John Wiley 2009. Garrety, Karin. \"Social Worlds, Actor-Networks and Controversy: The Case of Cholesterol, Dietary Fat and Heart Disease,.\" Social Studies of Science 27 (1997): 727-73. ________. \"Science, Policy, and Controversy in the Symbolic Interaction 21 (1998): 401-24. Gelbier, Stanley. \"125 Years of Development in Dentistry, 1880-2005.\" British Dental Journal 199, nos. 6-12 (2005). Gratzer, Walter. Terrors of the Table: The Curious History of Nutrition. Oxford: Oxford University Press, 2005. Griggs, Barbara. The Food Factor: Why We Are What We Eat. Harmondsworth and New York: Viking, 1986. 444 Guggenheim, Karl Y. Basic Issues in the History of Nutrition. Jerusalem: Akademia University Press, 1990. Guggenheim, Karl Y, and Ira Wolinsky. Nutrition and Nutritional Diseases: The Evolution of Concepts. Lexington, MA: Collamore Press. Gutmann, James L. \"The Evolution of America's Scientific Advancements in Dentistry in the Past 150 Years.\" Journal of the American Dental Association 140 (September 2009): 8S-15S. Guttenberg, Albert Z. \"The Land Utilization Movement of the 1920s.\" Agricultural History 50, no. 3 (July 1976): 477-90. Hall, Ross Hume. Food for Nought: The Decline in Nutrition. New York: Vintage Books, 1976. Hansen, Z. K., and G. D. Libecap. \"Small Farms, Externalities, and the Dust Bowl of the 1930s.\" Journal of Political Economy 112, no. 3 (2004): 665-94. Heckman, Joseph. \"A History of Organic Farming: Transitions from Sir Albert Howard's War in the Soil to USDA National Organic Program,\" Renewable Agriculture and Food Systems 21, no. 3 (2005): 143-50. Helms, Douglas. \"Soil and Southern History.\" Agricultural History 74, no. 4 (Autumn 2000): 723-58. Helms, Douglas, Anne B. W. Effland, and Patricia J. Durana, eds. Profiles in the History of the U.S. Soil Survey. Ames, IA: Iowa State University Press, 2002. Hite, Adele H., Valerie Goldstein Berk owitz, and Keith Berkowitz,. \"Low-Carbohydrate Diet Review: Shifting the Paradigm.\" Nutrition in Clinical Practice 26, no. 3 (June 2011): 300-08. Hodge, Joseph Morgan. Triumph of the Expert: Agrarian Doctrines of Development and the Legacies of British Colonialism. Athens, OH: Ohio University Press, 2007. Horrocks, Sally M. \"The Business of Vitamins: Nutrition Science and the Food Industry in Inter-War Britain.\" In The Science and Culture of Nutrition, 1840-1940, eds. Harmke Kamminga and Andrew Cunningham, 235-58. Amsterdam: Rodopi, 1995. ________. \"Nutrition Science and the Food and Pharmaceutical Industries in Inter-War Britain.\" In Nutrition in Britain: Science, Scientists, and Politics in the Twentieth Century, ed. David F. Smith, 53-74. London: Routledge, 1996. Hu, Frank B., JoAnn E. Manson, and Walter C. Willett. \"Types of Dietary Fat and Risk of Coronary Heart Disease: A Critical Review.\" Journal of the American College of Nutrition 20, no. 1 (2001): 5-19. 445 Hudson, Norman. \"A World View of the Development of Soil Conservation.\" Agricultural History 59, no. 2 (April 1985): 326-59. Hurt, R. Douglas. Problems of Plenty: The American Farmer in the Twen tieth Century. Chicago: Ivan R. Dee, 2002. Ihde, Aaron J. \"The Roles of Cod Liver and Light.\" Pharmacy in History 17 (1975): 13-17. Ihde, Aaron J., and Stanley L. Beck er, \"Conflict of Concepts in Early Vitamin Studies.\" Journal of the History of Biology 4 (Spring 1971): 1-33. Jackson, Carlton. J. I. Rodale, Apostle of Nonconformity. Pyramid Books, 1974. Kamminga, Harmke. \"'Axes to Grind': Popularising the Science of Vitamins, 1920s and 1930s.\" In Food, Science, Policy and Regulation in the Twentieth Century: International and Comparative Perspectives, eds. David F. Smith and Jim Phillips, 83-100. London and New York: Routledge, 2000. Kamminga, Harmke, and Andrew Cunningham, eds. The Science and Culture of Nutrition, 1840-1940. Amsterdam: Rodopi, 1995. Kamminga, Harmke, I: Frederick Gowland Hopkins' Construction of Dynamic Biochemistry.\" Medical History 40 (1996): 269-92. Kandel, R. F., and G. H. Pelto. \"The Health Food Movemen t: Social Revitalization or Alternative Health Maintenance System.\" In Nutritional Anthropology: Contemporary Approaches to Diet and Culture, eds. Norge W. Jerome, Randy F. Kanel, and Gretel H. Pelto, 327-63. Pleasantville, NY: Redgrave Cartography: 1924-1974.\" Geoderma 12 (1974): 347-62. Keys, Ancel, Margaret Keys. How to Eat Well and Stay Well. Garden City, NY: Doubleday, 1959. ________. How to Eat Well and Stay Well the Mediterranean Way. Garden City, NY: Doubleday, 1975. King, Charles Glen. A Good Idea: The History of the Nutrition Foundation. New York: The Nutrition Foundation, 1976. Korcak, Ronald F. \"Early Roots of the Organic Movement: A Nutrition 2 (April/June 1992): 263-67. 446 Kritchevsky, David. \"The History of Recommendations to the Public about Dietary Fat.\" Journal of Nutrition 128 (1998): 449S-52S. Kruse, Harry D. Nutrition: Its Scope, Meaning, and Significance. Springfield, IL: Thomas, 1969. Lawrence, Christopher, and George Weisz, eds. Greater than the Parts: Holism in Biomedicine, 1920-1950. New York, Oxford: Oxford University Press, 1998. Levenstein, Harvey. Revolution at the Table: The Transformation of the American Diet. New York: Oxford University Press, 1988. ________. Paradox of Plenty: A Social History of Eating in America. New York: Oxford University Press, 1993. ________. \"The Politics of Nutrition in North America.\" Neuroscience and Biobehavioral Reviews 20, no. 1 (1995): 75-78. ________. Paradox of Plenty: A Social History of Eating in America. Rev. ed. Berkeley: University of California Press, 2003. Manlay, Rapha\u00ebl J., Christian Feller, and M. J. Swift. \"Historical Evolution of Soil Organic Matter Concepts and Their Relationships with the Fertility and Sustainability of Cropping Systems.\" Agriculture, Ecosystems and Environment 119 (2007): 217-33. Marald, Erland. \"Everything Circulates: Agricultural Chemistry and Recycling Theories in the Second Half of the Nineteenth Century.\" Environment and History 8 (2002): 65-84. Mart in, John. The Development of Modern Agriculture: British Farming since 1931. London: Macmillan, 2000. Matless, David. \"Bodies Made of Grass Made of Earth Made of Bodies: Organicism, Diet and National Health in Mid-Twentieth Century England.\" Journal of Historical Geography 27 (2001): 355-76. Maurer, Donna, and Jeffrey Sobal, eds. Eating Agendas: Food and Nutrition as Social Problems. New York: Aldine de Gruyter, 1995. Mayhew, Madeline. \"The 1930s Nutrition Controversy.\" Journal of Contemporary History 23, no. 3 (July 1988): 445-65. McCay, Clive M. Notes on the History of Nutrition Research. Edited by F. Verz\u00e1r. Bern : Hans Huber Pub., 1973. McCay, Jeanette B. Clive McCay: Nutrition Pioneer. Charlotte Harbor, FL: Tabby House, 1994. 447 McCollum, Elmer Verner. A History of Nutrition: The Sequence of Ideas in Nutrition Investigations. Boston: Houghton Mifflin Company, 1957. ________. From Kansas Farm Boy to Scientist: The Autobiography of Elmer Verner McCollum. Lawrence, KS: University of Kansas Press, 1964. Murray, C. A., and W. P. Saunders. \"Root Canal Treatment and General Health: A Review of the Literature.\" International Endodontic Journal 33 (2000): 1-18. NRC. Fluoride in Drinking Water: A Scientific Review of EPA's Standards. Washington: National Academies Press, 2006. Needham, Joseph, ed. Hopkins and Biochemistry, 1861-1947. Cambridge: W. Heffer and Sons, Ltd., 1949. Nestle, Mari on. Food Politics: How the Food Industry Influences Nutrition and Health. Rev. ed. Berkeley: University of California Pres, 2007. Oddy, Derek J. From Plain Fare to Fusion Food: British Diet from the 1890s to the 1990s. Woodbridge, UK: Boydell Press, 2002. Oddy, Derek J. and Derek S. Miller. The Making of the Modern British Diet. London: Croom Helm, 1976. Pallasch, Thomas J., and Michael J. Wahl. \"Focal Infection Theory: New Age or Ancient History?\" Endodontic (2003): 32-45. Park, Youngmee K. \"History of Cereal-Grain Product Fortification the United States.\" Nutrition Today, May/June 2001, 124-37. Paton, T. R., and G. S. Humphreys. \"A Critical Evaluation of the Zonalistic Foundations of Soil Science in the United States. Part I: The Beginning of Soil Classification.\" Geoderma 139 (2007): 257-67. Petty, Celia. \"Food, Poverty and Growth: The Application of Nutrition Science, 1918-1939.\" Society for the Social History of Medicine Bulletin 40 (1987): 37-40. Phillips, Sarah T. This Land, This Nation: Conservation, Rural America, and the New Deal. Cambridge: Cambridge University Press, 2007. Platt, B. S. \"Sir Edward Mellanby, G.B.E., K.C.B., M.D., F.R.C.P., F.R.S. (1884-1955): Statesman.\" Annual Reviews of Biochemistry 25 (1956): 1-28. Picard, Alyssa. Making the American Mouth: Dentists and Public Health in the Twentieth Century. New Brunswick, NJ: Rutgers University Press, 2009. 448 Pollan, Michael. In Defense of Food: An Eater's Manifesto. New York: The Penguin Press, 2008. Ravnskov, Uffe. The Cholesterol Myths: Exposing the Fallacy that Saturated Fat and Cholesterol Cause Heart Disease Washington, DC: New Trends Publishing, 2000. Robin, Libby, and Tom Griffiths. \"Environmental History in Australasia.\" Environment and History 10 (2004): 446-47. Scofield, A. M. \"Organic Farming\u2014The Origin of the a, Richard D. \"Vitamin A as 'Anti-Infective' Therapy, 1920-1940.\" Journal of Nutrition 129, no. 4 (April 1999): 183-71. Shaw, C. A., and J. S. Bains. \"Did Consumption of Flour Bleached by the Agene Process Contribute to the Incidence of Neurological Disease?\" Medical Hypotheses 51 (1998): 477-81. Showers, Kate. Imperial Gullies: Soil Erosion and Conservation in Lesotho. Athens, OH: Ohio University Press, 2007. Sims, Laura S. The Politics of Fat: Food and Nutrition Policy in America. Armonk, NY: M. E. Sharpe, 1998. Smith, David F., ed. Nutrition in Britain: Science, Scientists, and Politics in the Twentieth Century. London/New York: Routledge, 1997. ________. \"The Use of 'Team Work' the Practical Management of Research in the Inter-War Period: John Boyd Orr at the Rowett Research Institute.\" Minerva 37 (1999): of Paton, Findlay and Cathcart: Conservative Thought in Chemical Physiology, Nutrition and Public Health.\" Social Studies of Science 19, no. 2 (May 1989): 195-238. Smith, David F., and Jim Phillips, eds. Food, Science, Policy, and Regulation in the Twen tieth Century: International and Comparative Perspectives. London/New York: Routledge, 2000. Smith, David F., and Case of Parturiunt Montes, Nascetur Ridiculus the Political Disengagement 449 of Nutrition Science.\" Journal of the History of Medicine and Allied Sciences 59, no. 2 (2004): 240-72. Stare, Frederick J. Adventures in Nutrition. Hanover, MA: Christopher Pub. House, 1991. Starr, Paul. The Social Transformation of American Medicine. New York: Basic Books, 1982. Suddick, Richard P., and Norman O. Harris, \"Historical Perspectives of Oral Biology: A Series,\" Critical Reviews in Oral Biology and Medicine 1, no. 2 (1990): 135-51. Taubes, Gary. \"The Soft Science of Dietary Fat.\" Science 291, no. 30 (March 2001): 2536-45. ________. Good Calories, Bad Calories: Challenging the Conventional Wisdom on Diet, Weight Control, and Disease. New York: Knopf, 2007. Tilley, Helen. \"African Environments and Environmental Sciences: The African Research Survey, Ecological Paradigms and British Colonial Development, 1920-1940.\" In Social History and African Environments, eds. William Beinart and JoAnn McGregor, 109-30. Oxford: James Currey, David A. Sirois, and Connie C. Mobley, eds. Nutrition and Oral Medicine. Totowa, NJ: Humana Press, 2005. Trentmann, Frank, and Flemming Just. Food and Conflict in Europe in the Age of the Two World Wars. New York: Palgrave Macmillan, 2006. van der Ploeg, R. R., W. B\u00f6hm, and M. B. Kirkham. \"On the Origin of the Theory of Mineral Nutrition of Plants and the Law of the Minimum.\" Soil Science Society of America Journal 63 (1999): 1055-1062. van Stuijvenberg, Johannes Hermanus. Margarine: A Social and Economic History, 1869-1969. Liverpool: Liverpool University Press, 1969. Vernon, James. Hunger: A Modern History. London: Belknap, 2007. Webster, Charles. \"Hungry or Healthy Thirties?\" History Workshop Journal 13 (1982): 110-29. Weinberg, Alvin M. \"Science and Trans-Science.\" \"The Diet-Heart Hypothesis: A Critique.\" Journal of the American College of Cardiology. 43, no. 5 (3 March 2004): 731-33. Whorton, James C. Crusaders for Fitness: A History of American Health Reformers. Princeton, NJ: Princeton University Press, 1982. 450 ________. \"Patient, Health Thyself: Popular Health Reform Movements as Unorthodox Medicine.\" In Other Healers: Unorthodox Medicine in America, ed. Norman Gevitz, 52-81. Baltimore: Johns Hopkins University Press, 1988. ________. Inner Hygiene: Constipation and the Pursuit of Health in Modern Society. Oxford: Oxford University Press, 2000. ________. Nature Cures: The History of Alternative Medicine in America. New York: Oxford University Press, 2002. Widdowson, E. M. \"Adventures in Nutrition Over Half a Century,\" Proceedings of the Nutrition Society 39 (1980): 293-306. Williams, H. H. \"History of the American Institute of Nutrition: The First Fifty Years.\" Journal of Nutrition 108 (1979): 25-197. Worboys, Michael. \"The Discovery of Colonial Malnutrition between the Wars.\" In Imperial Medicine and Indigenous Society, ed. David Arnold, 208-25. Manchester: Manchester University Press, 1988. Young, James Harvey. The Medical Messiahs: A Social History of Health Quackery in Twen tieth-Century America. Rev. ed. Princeton, NJ: Princeton University Press, 1992. Zero, D. T. \"Sugars\u2014The Arch Criminal?\" Caries "}