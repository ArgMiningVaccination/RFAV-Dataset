{"title": "PDF", "author": "PDF", "url": "https://www.stat.colostate.edu/graybillconference2015/PDF%20Files/abstractsBook%200601.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "Published by : International Chinese Statistical Association and Department of Statistics, Colorado State University Photographer for the front cover: Ya Zhang24th Applied Statistics Symposium and 13th Graybill Conference 2015 CONFERENCE INFORMATION , PROGRAM AND ABSTRACTS June 14 - 17, 2015 Colorado State University: Lory Student Center Fort Collins, Colorado, USA Organized by International Chinese Statistical Association and Department of Statistics, Colorado State University c 2015 International Chinese Statistical Association and Department of Statistics, Colorado State UniversityProfessor Franklin A. Graybill Department of Statistics Colorado State University The following graduates of the Department of Statistics at Colorado State University completed their degrees under the guidance of Professor Franklin A. Graybill Mohamed H. Albohali (MS '79) Robert A. Ahlbrandt (MS '87) Carmen E. Arteaga (MS '80) James H. Baylis (MS '77) David C. Bowden (MS '65, PhD '68) Brent D. Burch (MS '93, PhD '96) James A. Calvin (PhD '85) Terrence L. Connell (MS '63, PhD '66) Ruth Ann Daniel (MS '80) Ali Mashat Deeb (MS '81) Richard M. Engeman (MS '75) Rana Fayyad (PhD Mark J. (MS Rongde Gui (PhD '92) '77) William C. Heiny (MS '81) Sakthivel Jeyaratnam (PhD '78) Dallas E. Johnson (PhD '71) Thomas A. Jones (MS '67) Yongsang Ju (MS '92) Adam Kahn (MS '78) M. Kazem Kazempour (PhD '88) Albert Kingman (PhD '69) Stephen L. Kozarich (PhD '71) Ricardo A. Leiva (MS '82) Tai-Fang Chen Lu (MS '79, PhD '85) Sandra Mader (MS '77) Farooq Maqsood (MS '84) Louise R. Meima n (MS '67) Ronald R. Miller (MS '76) George A. Milliken (MS '68, PhD '69) Michael E. Mosier (PhD '92) William B. Owen (PhD '65) Antonio Reverter -Gomez (MS '94) Robert C. Rounding (PhD '65) Bhabesh Sen (PhD '88) Jeanne Simpson (MS '78) Syamala Srinivasan (MS '84, PhD '86) R. Kirk Steinhorst (MS '69, PhD '71) Naitee Ting (PhD '87) N. Scott Urquhart (MS '63) Antonia Wang (MS '82) Chih -Ming (Jack) Wang (PhD 1: Best Practices for Delivery of Adaptive Clinical Trials Illustrated with Case Chemistry, Manufacturing, and Controls (CMC) in Pharmaceuticals: Current Statistical Challenges I . 44 Session 3: Chemistry, Manufacturing, and Controls (CMC) in Pharmaceuticals: Current Statistical Challenges II 45 Session 4: New Techniques for Functional and Longitudinal Data Recent Developments in the Theory and Applications of The Application of Latent Variable and Mixture Models to the Biological to Subgroup Analysis in Conrmatory Clinical Trials: Challenges and Opportunities . 59 Session 24: Recent Developments Go Decision Criteria and Probability of Success in Pharmaceutical Drug Development . . . . 63 Session 29: High-Dimensional Data Analysis: Theory and Applications . Sinica Special Invited Session on Spatial and and Unblinded Evaluation of Aggregate Safety Data during Clinical Development . . . . . . 82 Effective Identication of Biomarkers and Subgroups for New Advances in Adaptive Design and Analysis of in Empirical Likelihood Methodologies: Diagnostic Studies, Goodness-of-Fit Advances in Statistical Methods of Identifying Subgroup in Genetics & Pharmacogenomics (GpGx) . . . . 95 Conference , For t Collins, Colorado, June 14 -17 | 1 Welcome 2015 I CSA/Graybill Joint Conference June 14 -17, Fort Collins, Colorado, USA Welcome to the 2015 Joint 24th International Chinese Statistical Association (ICSA) Applied Statistics Symposium and 13th Graybill Conference! The Executive Committee of this Joint Conference has been working hard to put together a very strong program including 7 short courses, two Plenary Presentations, one Plenary Leadership Forum, 94 scien tific sessions and social events . The keynote speech will be delivered by Professor Susan Murphy (University of Michigan), the Graybill Plenary speech will be presented by Professor Richar d Davis (Columbia University). Three panelists (Professor Xiao -Li Me ng representing academia, Dr. Greg Campbell representing government, and Dr. Janet Wittes representing industry/business) will participate in the Plenary Leadership Forum moderated by Dr. Wei Shen, who is the 2015 President of ICSA . The scientific program mainly covers two broad statistical application areas - those topics relating to bio -pharmac eutical applications, and topics which are non -bio-pharmaceutical applications . We hope this Joint Conference will provide abundant opportunities for you to engage, learn and network . We also hope you will be able to obtain inspirations to advance old research ideas and to develop new ones. We sincerely believe this will be a memorable and worthwhile learning experience for you. Social events in this 2015 Joint Con ference include mixer (Sunday, June 14 evening), banquet (Tuesday, June 16 - banquet speaker will be Dr. Howard Wainer) and three excursion programs . If you come to Colorado, you may not want to miss the opportunities of enjoying the various activities bel ow. In June, Fort Collins has daily high temperatures that range from 74\u00b0F to 85\u00b0F with evening temperatures in the low 50s . Nestled next to the foothills, it is home to Colorado State University and is only an hour away from Rocky Mountain National Park . Often called the \"Napa Valley of Craft Beer,\" Fort Collins is home to a number of microbreweries that together produce one of the largest city volumes of craft brewed beer in the country . Convenient access, clear water, challenging rapids and beautiful s cenery make the Cache la Poudre a rafter's paradise from May through September. Thanks for coming to the 2015 ICSA/Graybill Join t Conference at Fort Collins, Colorado . Naitee Ting, Chair, Executive Committee, 2015 ICSA/Graybill Joint Conference 2 | 2015 ICSA/Graybill Joint Conference , For t Collins, Colorado, June 14 -17 Committee Naitee Ting (Chair) Boehringer -Ingelheim Pharmaceuticals, Inc. Scott Evans (Program Committee Chair ) Harvard School of Public Health Jim zumBrunnen (Local Committee Co -Chair ) Colorado State University Haonan Wang ( Local Committee Co -Chair ) Colorado State University Yingqi Zhao (Treasurer) University of Wisconsin -Madison Xiaoming Li (Fund Raising Chair) Gilead Pharmaceuticals Local Committee Jim zumBrunnen (Co-Chair ) Colorado State University Haonan Wang ( Co-Chair ) Colorado State University Program Committee Scott Evans (Chair) Harvard School of Public Health Invited Session Committee Greg Wei ( Co-Chair ) SynteractHCR Jay Br eidt (Co -Chair) Colorado State University Chunming Zhang University of Wisconsin -Madison Jun Zhu University of Wisconsin -Madison Contributed Session Committee Peng -Liang Zhao (Chair ) Sanofi -aventis U.S. LLC. Poster Session Committee Jun Yan ( Chair ) University of Connecticut Yu Cheng University of Pittsburgh Haoda Fu Eli Lilly and Company Rui Song North Carolina State University Chengguang Wang Johns Hopkins University Student Paper Award Committee Shuangge Ma (Chair) Yale University Gang Li University of California, Los Angeles Tiejun Tong HongKong Baptist University Ray Liu, Takeda Lili Yu GoergiaSouthern University Richard McNally Covance Inc. Short Courses Committee Brian Wiens (Chair) Portola Pharmaceuticals Ivan Chan Merck & Co. 2015 ICSA/Graybill Joint Conference , For t Collins, Colorado, June 14 -17 | 3 Committee Program Book Committee Fang Yu (Chair ) University of Nebraska Medical Center Fang Qiu University of Nebraska Medical Center Casey Blaser University of Nebraska Medical Center Fei Jiang University of Nebraska Medical Center Zhezhen Jin Columbia University Tian Zheng Columbia University Mengling Liu New York University Proceeding Book Committee Jianchang Lin ( Chair ) Takeda Bushi Wang Boehringer -Ingelheim Pharmaceuticals, Inc. Xiaowen Hu Colorado State University Kun Chen University of Connecticut Lan Huang U.S. Food and Drug Administration Fund Raising Committee Xiaoming Li (Chair) Gilead Pharmaceuticals Guojun Yuan EMD Serono, Merck KGaA Ranye Sun Bank of America Jingyang Zhang Fred Hutchinson Cancer Research Center Treasurer s Yingqi Zhao (Treasure) University of Wisconsin -Madison Wen Zhou (Assistant Treasure r) Colorado State University Webmasters Jim zumBrunnen (Conference Web ) Colorado State University Simon Gao (ICSA Web ) BioPier Inc. The 2015 ICSA/Graybill joint conference program committees gratefully acknowledge the invaluable and generous support of our sponsors and Ex- hibitors. Sponsors Exhibitors CRC Press \u2014 Taylor & Francis Group The Lotus Group LLC Springer Science & Business Media 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14 -17 | 5 Lory Student Center, Colorado State University Floor Plans (2nd Floor) 6 | 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14 -17 Lory Student Center, Colorado State University Floor Plans (3rd Floor) 2015 ICSA/Graybill Joint Conference , For t Collins, Colorado, June 14 -17 | 7 Short Program Sunday June 14, 2015 Time Room Session 7:30 AM - 5:00 PM 3rd floor Foyer Registration 8:00 AM - 5:00 PM 304 Short Course : Measurement error 8:00 AM - 5:00 PM 306 Short Course : Prevention and treatment of missing data: turning guidance into practice 8:00 AM - 5:00 PM 308 Short Course : Practical Bayesian computation 8:00 AM - 12:00PM 310 Short Course : Graphical approaches to multiple test problems 8:00 AM - 12:00PM 300 Short Course : Network based analysis of big data 9:45 AM - 10:15 AM Grand Ballroom C/D Break 12:00 PM - 1:00 PM LSC Food Court Lunch for Full-Day Course Attendees 1:00 PM - 5:00 PM 300 Short Course : Classification and regression trees and forests 1:00 PM - 5:00 PM 310 Short Course : Patient -Reported Outcomes: measurement, implementation and interpretation 2:45 PM - 3:15 PM Grand Ballroom C/D Break 6:00 PM - 8:30 PM Long s Peak ICSA Board Meeting (Dinner - Invited only) 7:00 PM - 9:00 PM Grand Ballroom C/D Opening Mixer Monday June 15, 2015 8:00 AM - 5:00 PM 3rd floor Foyer Registration 8:20 AM - 8:40 AM Grand Ballroom A/B Welcome Naitee Ting, Conference Chair Wei Shen, President ICSA Dean Jan Nerger, College of Natural Science s, CSU 8:40 AM - 9:40 AM Grand Ballroom A/B Keynote : Susan Murphy, University of Michigan 9:40 AM - 10:00 AM Grand Ballroom C/D Break 10:00 AM - 11:40 AM See Program Parallel sessions 11:40 AM - 1:00 PM Lunch on own 1:00 PM - 2:40 PM See Program Parallel sessions 2:40 PM - 3:00 PM Grand Ballroom C/D Break 3:00 PM - 4:40 PM See Program Parallel sessions 4:40 PM - 6:00 PM Grand Ballroom C/D Poster presenters Tuesday June 16, 2015 8:00 AM - 5:00 PM 3rd floor Foyer Registration 8:40 AM - 9:40 AM Grand Ballroom A/B Graybill Plenary : Richard Davis, Columbia University 9:40 AM - 10:00 AM Grand Ballroom C/D Break 10:00 AM - 11:40 AM See Program Parallel sessions 11:40 AM - 1:00 PM Lunch on own 1:00 PM - 2:40 PM See Program Parallel sessions 2:40 PM - 3:00 PM Grand Ballroom C/D Break 3:00 PM - 4:00 PM Grand Ballroom A/B Leadership Forum 6:00 PM Grand Ballroom C/D Cash Bar 6:30 PM - 9:00 PM Grand Ballroom C/D Banquet (fee event) Wednesday June 17, 2015 8:00 AM - 12:30 PM 3rd floor Foyer Registration 8:40 AM - 10:20 AM See Program Parallel sessions 10:20 AM - 10:40 AM Grand Ballroom C/D Break 10:40 AM - 12:20 PM See Program Parallel sessions 1:30 PM Excursion (fee event ) 8 | 2015 ICSA/Graybill Joint Conference , Fort Collins, Colorado, June 14 -17 Keynote Speaker Susan Murphy H.E. Robbins Professor of Statistics & Professor of Psychiatry University of Michigan Susan Murphy's research focuses on improving sequential, individualized, decision making in health, in particular on clinical trial design and data analysis to inform the development of adaptive interventions (e.g. treatment algorithms). She is a leading developer of the Sequential Multiple Assignment Randomized Trial (SMART) design which has been and is being used by clinical researchers to develop adaptive interventions in depression, alcoholism, treatment of ADHD, substance abuse, HIV treatment, obesity , diabetes, and autism. She collaborates with clinical scientists, computer scientists and engineers and mentors young clinical scientists on developing adaptive interventions. Susan is currently working as part of several interdisciplinary teams to develop clinical trial designs and learning algorithms for settings in which patient information is collected in real time (e.g. via smart phones or other wearable devices) and thus sequences of interventions can be individualized online. She is a Fellow of IMS, ASA, the College on Problems in Drug Dependence, a former editor of the Annals of Statistics, a member of the Institute of Medicine and a 2013 MacArthur Fellow. Title: Experimental Design, Data Analysis Methods for Mobile Interventions Location an d Time: Grand Ballroom A/B, Monday Jun e 15, 8:40 -9:40 AM Abstract: Micro -randomized trials are trials in which individuals are randomized 100's or 1000's of times over the course of the study. The goal of these trials is to assess the impact of momentary inte rventions, e.g. interventions that are intended to impact behavior over small time intervals. A fast growing area of mHealth concerns the use of mobile devices for both collecting real -time data, for processing this data and for providing momentary interv entions. We discuss the design and analysis of these types of trials. 2015 ICSA/Graybill Joint Conference , For t Collins, Colorado, June 14 -17 | 9 Graybill Plenary Speaker Richard Davis Chair, Howard Levene Professor of Statistics Columbia University Richard Davis is Chair and Howard Levene Professor of Statistics at Columbia University. He is currently president -elect of the Institute of Mathematical Statistics. He received his Ph.D. degree in Mathematics from the University of California at San Diego in 1979 and has held academic position s at MIT, Colorado State University, and visiting appointments at numerous other universities. Recently he was Hans Fischer Senior Fellow at the Technical University of Munich and Villum Kan Rasmussen Visiting Professor at the University of Copenhagen. Davis is a fellow of the Institute of Mathematical Statistics and the American Statistical Association, and is an elected member of the International Statistical Institute. He is co -author (with Peter Brockwell) of the bestselling books, \"Time Series: y and Methods\", \"Introduction to Time Series and Forecasting\", and the time series analysis computer software package, \"ITSM2000\". Together with Torben Andersen, Jens -Peter Kreiss, and Thomas Mikosch, he co -edited the \"Handbook in Financial Time Series.\" In 1998, he won (with collaborator W.T.M Dunsmuir) the Koopmans Prize for Econometric Theory. He has served on the editorial boards of major journals in probability and statistics and most recently was Editor - in-Chief of the Bernoulli Journal, 2010 -2012. He has advised/co -advised 31 PhD students and has presented short courses on time series and heavy -tailed modeling. His research interests include time series, applied probability, extreme value theory, and spatial -temporal modeling. Title: Sparse Vector Autoregressive Modeling Location and Ballroom Tuesday Jun16, 8:40 -9:40 AM Abstract: The vector autoregressive (VAR) model has been widely used for modeling temporal dependence in a multivariate time series. For large (and even moderat e) dimensions, the number of VAR parameters can be prohibitively large resulting in noisy estimates and difficult -to-interpret temporal dependence. As a remedy, we propose a methodology for fitting sparse VAR models (sVAR) in which most of the autoregress ive coefficients are set equal to zero. The first step in selecting the nonzero coefficients is based on an estimate of the partial squared coherency (PSC) together with the use of BIC. The PSC is useful for quantifying conditional relationships between marginal series in a multivariate time series. A refinement step is then applied to further reduce the number of parameters. The performance of this 2 -step procedure is illustrated with both simulated data and several real examples. The inclusion of a reduced rank covariance estimator of the noise will also be discussed. (This is joint work with Pengfei Zang and Tian Zheng) . 10 | 2015 ICSA/Graybill Joint Conference , Fort Collins, Colorado, June 14 -17 Leadership Forum Panelists Gregory Campbell Director, Division of Biostatistics, Office of Surveillance and Biometrics, Center for Devices and Radiological Health, FDA Gregory Campbell is the Director of the Division of Biostatistics in the Office of Surveillance and Biometrics (OSB) of Center for Devices and Radiological Health (CDRH) of the Food and Drug Administration (FDA) since he came to FDA in 1995 from a tenured scientist positions at NIH. Dr. Campbell leads a group of about 60 statisticians at the FDA that provides statistical support to CDRH as a whole and, in particular, the statistical reviews of FDA's pre -market device submissions. With the help of statisti cians in his Division, he pioneered the implementation in a regulatory environment of Bayesian statistics (and more recently propensity scores and adaptive designs). He is an Associate Editor for the journal Statistics in Pharmaceutical Research. He has been the recipient of the FDA's Commendable Service Award, Award of Merit and Outstanding Service Award as well as the CDRH Outstanding Scientific Award for Excellence in Analytical Science and the CDRH Diversity Award. He has been a member for ove r ten y ears of the Senior Biomedical Research Service in the Department of Health and Human Services and been a Fellow of the Americ an Statistical Association since 1998. He has served in leadership positions for the Eastern North American Region of the Inter national Biometric Society and on the Board of Directors of the Society for Clinical Trials and has been instrumental in the recent es tablishment of the Medical Device and Diagnostics Section of the American Statistical Association. He gave a keynote address at ICSA Applied Statistics Symposium in Indianapolis in 2010 . Xiao -Li Meng Dean, Harvard University Graduate School of Arts and Sciences Xiao -Li Meng, Dean of the Harvard University Graduate School of Arts and Sciences (GSAS), Whipple V. N. Jones Profe ssor and former chair of Statistics at Harvard, is well known for his depth and breadth in research, his innovation and passion in pedagogy, and his vision and effectiveness in administration, as well as for his engaging and entertaining style as a speaker and writer. Meng has received numerous awards and honors for the more than 120 publications he has authored in at least a dozen theoretical and methodological areas, as well as in areas of pedagogy and professional development; he has delivered more than 400 research presentations and public speeches on these topics, and he is the author of \"The XL -Files,\" a regularly appearing column in the IMS (Institute of Mathematical Statistics) Bulletin. His interests range from the theoretical foundations of statist ical inferences (e.g., the interplay among and -resolution inferences) to statistical methods and computation (e.g., posterior predictive p -value; EM algorithm; Markov chain Monte Carlo; bridge and path sampling) to applications in natural, social, and medical sciences and engineering (e.g., complex statistical modeling in astronomy and astrophysics, assessing disparity in mental health services, and quantifying statistical information in genetic studies). Meng received his BS in mathematics from Fudan University in 1982 and his PhD in statistics from Harvard in 1990. He was on the faculty of the University of Chicago from 1991 to 2001 before retu rning to Harvard as Professor of Statistics, where he was appointed department chair in 2004 and the Whipple V. N. Jones Professor in 2007. He was appointed G SAS Dean on August 15, 2012. Janet Wittes President, Statistics Collaborative, Inc. Janet Wittes, PhD is President of Statistics Collaborative, Inc. which she founded in 1990. One of the main activities of Statistics Collaborative is to serve as the statistical reporting group for independent data monitoring committees. Previously, she was Chief, Biostatistics Research Branch, National Heart, Lung, & Blood Institute (1983 -89). H er 2006 monograph, \"Statistical Monitoring of Clinical Trials - A Unified Approach\" by Proschan, Lan, and Wittes, deals with sequential trials. Her research has focused on design of randomized clinical trials, capture -recapture methods in epidemiology, and sample size recalculation. She has served on a variety of advisory committees and data monitoring committees for government (NHLBI, the VA, and NCI) and industry. For the FDA, she has been a regular member of the Circulatory Devices Advisory Panel and has served as an ad hoc member of several other panels. Currently, she is a regular member of the Gene Therapy Advisory Committee. She was formerly Editor in Chief of Controlled Clinical Trials (1994 -98). She received her Ph.D. in Statistics from Harvard Univ ersity. 2015 ICSA/Graybill Joint Conference , For t Collins, Colorado, June 14 -17 | 11 Student Award Winner s Jiann -Ping Hsu Pharmaceutical and Regulatory Sciences Student Paper Award Yang Ni, Rice University Title: Bayesian Nonlinear Model Selection for Gene Regulatory Networks Session 82: The Jiann -Ping Hsu Invited Session on Biostatistical and Regulatory Sciences (Virginia Dale, level 3 ) Time: Tuesday , June 16th 10:00 AM - 11:40 AM ASA Bio -pharmaceutical Awards Qingning Zhou, University of Missouri Title: A Sieve Semiparametric Maximum Likelihood Approach for Regression Analysis of Bivariate Interval -censored Failure Time Data Session 64 : Recent Development in Personalized Medicine and Survival Analysis (ASCSU Senate Chamber, Level 2 ) Time: Monday, June 15th 1:00 PM - 2:40 PM Wei Ding , University of Michigan Title: Composite Likelihood Approach in Gaussian Copula Regression Models with Missing Data Session 24: Recent Developments in Missing Data Analysis (386, level 3) Time: Wednesday, June 1 7th 10:40 AM- 12:20PM ICSA Student Paper Awards Yinfei Kong, Univer sity of Southern California Title: Innovated Interaction Screening for High -Dimensional Nonlinear Classification Session 7 : Scalable Multivariate Statistical Learning with Massive Data (386, level 3 ) Time: Monday , June 1 5th 1:00 PM - 2:40 PM Yuan Huang, Pennsylvania State University Title: Projection Test for High - Dimensional Mean Vectors with Optimal Direction Session 91 : Recent Developments of High -Dimensional Data Inference and Its Applications (306, level 3 ) Time: Wednesday , June 17th 10:40 AM - 12:20 PM Weichen Wang, Princeton University Title: Projected Principal Component Analysis in Factor Models Session 91 : Recent Developments of High -Dimensional Data Inference and Its Applications (306, level3 ) Time: , June 17th 10:40 AM PM Chongliang Luo, University of Connecticut Title: Canonical Variate Regression Session 27: Bayesian Applications in Biomedical Studies (310, level 3 ) Time: Wednesday, June 1 7th 10:40 AM- 12:20PM Yanping Liu , Temple University Title: A New Approach to Multiple Testing of Grouped Hypotheses Congratulation s! 12 | 2015 ICSA/Graybill Joint Conference , Fort Collins, Colorado, June 14 -17 Social Events Opening Mixer Sunday June 1 4, 7-9 PM Grand Ballroom C/D Cash Bar Tuesday June 16, 6 PM Grand Ballroom C/D Banquet Tuesday June 16 6:30-9:00 PM Grand Ballroom C/D Excursions Departure location and t ime: CSU/ Hilton , Wednesday June 17 1:30 PM Option 1 (5 hour tour): Rocky Mountain National Park The excursion by bus includes admittance into Rocky Mountain National Park. Spectacular views are available along the highest continuous paved road in North America, which crests at the Continental Divide. The highest point on Trail Ridge road is 12,183 f eet. Price: $50 (transportation and box lunch included) Option 2 (3 hour tour): Brewery Tour in Fort Collins The tour and tasting is at New Belguim Brewery , which is Fort Collins' largest microbrewery. Fort Collins has been called the Napa Valley of Craft Beer, and is home to the most brewers and microbreweries per capita in all of Colorado. Price: $25 (Transportation included) Option 3 (5 hour tour): Rafting the Poudre River This is whitewater rafting on Colorado's only Wild & Scenic river, Cache La Poudre. Our outfitter/guide will be Wanderlust Adventure of Fort Collins. Many fun and continuous rapids like Pinball, Roller Coaster, the Squeeze, Slideways and Headless will be traversed. Price: $95 (Transportation and box lunch; paddle jackets, fleece pullovers life jacket & helmet included) + $10-12 (Optional ; for Wetsuits) 2015 ICSA/Graybill Joint Conference , For t Collins, Colorado, June 14 -17 | 13 Banquet Speaker Howard Wainer Distinguished Research Scientist, National Board of Medical Examiners Howard Wainer was born in Brooklyn New York, on October 26, 1943. He received a BS in mathematics from Rensselaer Polytechnic Institute in 1965 and an AM and PhD from Princeton in psychometrics in 1967 and 1968 respectively. He taught at The University of Chicago befor e moving to the Bureau of Social Science Research during the Carter administration. He was a Principal Research Scientist at ETS for 21 years before assuming his current position as Distinguished Research Scientist at the National Board of Medical Examiner s. From 2003 until 2013 he was (adjunct) Professor of Statistics at the Wharton School of the University of Pennsylvania. He has published more than 400 articles and chapters in scholarly journals and books; his 20th book, Medical Illuminations: Using Evid ence, Visualization & Statistical thinking to Improve Healthcare was published by Oxford University Press in 2014 and was a finalist for the Royal Society Winton Book Prize. His next book Truth or Truthiness: Distinguishing Fact from Fiction by Learning t o Think like a Data Scientist will be published by Cambridge University Press next year. He is a Fellow of the American Statistical Association and the American Educational Research Association and has been the recipient of numerous awards including: ACT/ AERA E. F. Lindquist Award for Outstanding Research in Testing & Measurement, 2015. American Educational Research Association Significant Contribution to Educational Measurement & Research Methodology Award, 2014. Psychometric Society Lifetime Achievement Award, 2013. The Samuel J. Messick Award for Distinguished Scientific Contributions from Division 5 of the American Psychological Association, 2009. Career Achievement Award for Contributions to Educational Measurement . National Council on Measurement in Education, April 2007. Award for Scientific Contribution to a Field of Educational Measurement for the development of Testlet Response Theory, National Council on Measurement in Education, April 2006. Senior Scientist Award , Educational Testing Service, 199 0-1992 Title: Pictures at an exhibition: Sixteen visual conversations about one thing Time and Location: Grand Ballroom C/D, Tuesday June 16 6:30 PM Abstract: In 1951 the famous graphic designer Will Burtin presented a graphic showing the efficacy of three antibiotics in treating 16 bacteria. In this talk we will examine Burtin's solution as well as 15 others. We will find that that there are many paths to salvation, but that had the data been displayed dif ferently, important discoveries could have been accelerated by decades. 14 | 2015 ICSA/Graybill Joint Conference , Fort Collins, Colorado, June 14 -17 Short Courses 1. Measurement Error Presenters: John Buanoccorsi, Professor Emeritus, Dept. of Mathematics and Statistics, University of Massachusetts Amherst. Email: johnpb@math.umass.edu Course length: One day Outline/Description Measurement error is ubiquitous and it is well known that the inability to exactly measure predictors in regression problems often leads to biased estimators and invalid inferences. This problem has a long history in linear problems but saw an explosion of interest over the last twenty years as methods were expanded to both deal with more complex models and address a number of practical problems that arise in practice. The methodology has been successfully, and widely used across a wide range of disciplines, most notably (but certainly not limited to) Epidemiology. This course will present an introductory, and relatively applied, look at measurement error in regression settings including linear and nonlinear models, the latter including generalized linear models and more explicitly logistic regression. The goal of the course is to introduce attendees to models used for measurement error, the impacts of measurement error on so - called naive analyses, which ignore it, and provide an extensive overview of the myriad techniques available to correct for it, along with associated inferences. We deal both with the case of additive error, in which case the measurement error parameters are usually es timated through replication, as well as non - additive error, where validation data (either internal or external) is exploited to correct for measurement error. Detailed examples will be provided from a variety of disciplines and, although the course does no t have a computer component associated with it, an overview of available software and its use will be presented. Time permitting, we will briefly discuss measurement error in mixed/longitudinal models and time series. Students should have some prior expos ure to basic mathematical statistics and have familiarity with regression models, including seeing models and methods expressed in matrix -vector form. References Buonaccorsi (2010), \"Measurement Error: Models, Methods and Applications''; Chapman & Hall. About the presenter John Buonaccorsi is Professor Emeritus of Mathematics and Statistics at the University of Massachusetts -Amherst. He received his M.S. and Ph.D. degrees from Colorado State University and has been at the University of Massachusetts since 1982. He was a long -time member of the University's Statistical Consulting Center and coordinator of the graduate options in Statistics for many years. He is the author of over 70 articles and book chapters and is author of the 2010 book \"Measurement Erro r: Models, Methods and Applications\", part of the Chapman -Hall series on interdisciplinary statistics. His original research interests were in optimal experimental design, estimation of ratios and calibration, followed by a focus on measurement error, an a rea he has worked in for over 25 years. He has also published extensively in various applied areas including quantitative ecology, with a recent emphasis on population dynamics. He has a long -standing collaboration with colleagues at the University of Oslo Medical School addressing measurement error methods in epidemiologic contexts. 2. Prevention and Treatment of Missing Data: Turning Practice Presenters: Craig Mallincrokdt Course Length: One day Outline/Description: Recent research has fostered new guidance on preventing and treating missing data in clinical trials. This short course is based on work from the Drug Information Association's Scientific Working Group (DIASWG) on Missing Data. The first half - day of the course begins with an overview of the research and other background that fostered the new guidance, including a brief history of the work by the National Research Council Expert Panel on missing data that provided detailed advice to FDA on the prevention a nd treatment of missing data. The first half day will also distill common elements from recent guidance into 3 pillars: 1) setting clear objectives; 2) minimizing missing data; and, 3) pre -specifying a sensible primary analyses and appropriate sensitivity analyses. Specific means for putting the guidance into action are proposed, including detailed coverage of developing an overall analytic road map. In the second half - day, specific software tools developed by the DIASWG to implement the analytic road ma p will be demonstrated on an example data set. Attendees will be provided with these programs at no cost and encouraged to run the programs concurrent with the demonstration. Several DIASWG members will be available to assist attendees in running the programs. Learning objectives 1) Understand the three pillars of preventing and treating missing data, with emphasis on developing a complete analytic road map that includes a sensible primary analysis and appropriate sensitivity analyses. 2) Be able to apply the three pillars principles to their own research 2015 ICSA/Graybill Joint Conference , For t Collins, Colorado, June 14 -17 | 15 Short Courses 3) Understand the theory behind key sensitivity analyses and be able to run the macros developed by the DIASWG that will be given free of charge to attendees. About the presenters Dr. Mallinckrodt received his PhD in 1993 from Colorado State University, where he subsequently held a joint appointment in the departments of statistics and clinical sciences. Craig joined Lilly in 1998 and has extensive drug development experience cover ing all four clinical phases in multiple therapeutic areas. Dr. Mallinckrodt has published extensively on missing data. He led the PhRMA expert team and currently leads the Drug Information Association Scientific Working Group on missing data. Dr. Malli nckrodt is a Fellow of the American Statistical Association and recently won the Royal Statistical Society's award for excellence in the pharmaceutical industry for his book titled A Practical Guide to the Prevention and Treatment of Missing Data. Dr. Geert Molenberghs is Professor of Biostatistics at the Universiteit Hasselt Katholieke Universiteit Leuven in Belgium. He received the B.S. degree in mathematics (1988) and a Ph.D. in biostatistics (1993) from the Universiteit Antwerpen. Dr Molenberghs published methodological work on surrogate markers in clinical trials, categorical data, longitudinal data analysis, and on the analysis of non -response in clinical and epidemiological studies. He served as Joint Editor for Applied Statistics (2001 -2004), Co-editor for Biometrics (2007 --2009) and as President of the International Biometric Society (2004 -2005). He currently is Co -editor for Biostatistics (2010 --). He was elected Fellow of the American Statistical Association and received the Guy Medal in Br onze from the Royal Statistical Society. He has held visiting positions at the Harvard School of Public Health (Boston, MA). He is founding director of the Center for Statistics at Hasselt University and currently the director of the Interuniversity Institute for Biostatistics and statistical Bioinformatics, I - BioStat, a joint initiative of the Hasselt and Leuven universities. Geert Molenberghs and Geert Verbeke are editor and author of several books on longitudinal data analysis, possibly subject to missi ngness (Springer Lecture Notes 1997, Springer Series in Statistics 2000, Springer Series in Statistics 2005, Chapman Hall/CRC 2007), and they have taught well over a hundred short and longer courses on the topic in universities as well as industry, in Euro pe, North America, Latin America, and Australia. Geert Verbeke and Geert Molenberghs received several Excellence in Continuing Education awards for courses offered at the Joint Statistical Meetings. Dr. Bohdana Ratitch is Senior Statistical Scientist at Quintiles. Bohdana received a Ph.D. degree in computer science/statistical learning from McGill University in Montreal, Canada in 2005. Bohdana has been working in biostatistics and clinical trials for over 10 years. Missing data in clinical trials is one of her areas of special interest and she is working actively to advance and promote knowledge in this filed in the clinical research community. She is a member of the DIA Special Working Group on Missing Data and a co -author of the book \"Clinical Trials wi th Missing Data: A Guide for Practitioners\" (Wiley, 2014). Dr. Lei Xu is Principal Biostatistician at Biogen. Lei received his Ph.D degree in Statistics from University of Wisconsin - Madison and have 8 years of experience on late phase drug development. He is a member of both ICSA and ASA. He previously served as missing data Hub leader at Eli Lilly and has been actively served in the Drug Information Association Scientific Working Group on missing data since 2012. 3. Practical Bayesian Computation Prese nter: Fang Chen, Bob Lucas, SAS . Email: FangK.Chen@sas.com Course length: One Day Outline/Description This one -day course reviews the basic concepts of Bayesian inference and focuses on the practical use of Bayesian computational methods. The objectives are to familiarize statistical programmers and practitioners with the essentials of Bayesian computing, and to equip them with computational tools through a series of worked -out examples that demonstrate sound practices for a variety of sta tistical models and Bayesian concepts. The first part of the course will review differences between classical and Bayesian approaches to inference, fundamentals of prior distributions, and concepts in estimation. The course will also cover MCMC methods a nd related simulation techniques, emphasizing the interpretation of convergence diagnostics in practice. The rest of the course will take a topic - driven approach that introduces Bayesian simulation, analysis, and illustrates the Bayesian treatment of a wid e range of statistical models using software with code explained in detail. The course will present major applications areas and case studies, including multi -level hierarchical models, multivariate analysis, non -linear models, meta -analysis, latent variab le models, and survival models. Special topics that are discussed include Monte Carlo simulation, sensitivity analysis, missing data, model assessment and selection, variable subset selection, and prediction. The examples will be done using SAS (PROC MCMC) , with a strong focus on technical details. Attendees should have a background equivalent to an M.S. in applied statistics. Previous exposure to Bayesian methods is useful but not required. Familiarity with material at the level of this text book is appr opriate: Probability and Statistics (Addison Wesley), DeGroot and Schervish. About the presenter Fang Chen (PhD in Statistics from Carnegie Mellon University in 2004) is Senior Manager of Bayesian Statistical Modeling in 16 | 2015 ICSA/Graybill Joint Conference , Fort Collins, Colorado, June 14 -17 Short Courses Advanced Analytics Division at SAS Institute Inc. Among his responsibilities are the development of Bayesian analysis software and the MCMC procedure. He has written about Bayesian modeling using the MCMC pr ocedure and taught courses and tutorials on practical Bayesian computation. 4. Graphical Approaches to Multiple Test Problems. Xi, Statistical Methodologist, Novartis. Email: Day Outline/Description Methods for addressing multiplicity are becoming increasingly more important in clinical trials and other applications. Examples of such study objectives include investigation of multiple doses or regimens of a new treatment, multiple endpoints, subgroup analyses or any combination of these. This short course will provide a practical guidance on how to construct multiple testing procedures (MTPs) for such hypotheses with an emphasis on graphi cal approaches. Course outline 1. Introductio n to multiple testing procedures In the first part of this course, we will introduce the concept of multiplicity and its impact on scientific research. To deal with multiplicity issues, we will discuss basic concepts of MTPs including the error rate, adju sted p -values and single -step such as Bonferroni, Holm, Hochberg and Dunnett will be introduced and compared. We will describe the closure principle and closed testing procedures as an important way to construct MTPs. 2. Graphical approaches to multiple testing In the second part of the course, we will focus on graphical approaches that can be applied to common multiple test problems. Using graphical approaches, one can easily construct and explore different test strategies and thus tailor the test procedure to t he given study objectives. The resulting multiple test procedures are represented by directed, weighted graphs, where each node corresponds to an elementary hypothesis, together with a simple algorithm to generate such graphs while sequentially testing the individual hypotheses. We also present one case study to illustrate how the approach can be used in clinical practice. The presented methods will be illustrated using the graphical user interface from the gMCP package in R, which is freely available on CR AN. Textbook/References Bretz, F., Hothorn, T., and Westfall, P. (2010) Multiple Comparisons with R . Chapman and Hall, Boca Raton. Bretz, F., Maurer, W., Maca, J (2014) Graphical approaches to multiple testing. Young, W. and Chen, D. (eds.), Clinica l Trial Biostatistics and Biopharmaceutical Applications , Taylor & Francis. Dmitrienko, A., Tamhane, A. C. and Bretz, F. (Eds.) (2009) Multiple Testing Problems in Pharmaceutical Statistics . Chapman & Hall/CRC Biostatistics Series, Boca Raton. About th e presenter Dong Xi is a statistical methodologist in the Statistical Methodology and Consulting Center at Novartis Pharmaceuticals Corporation. He received his Ph.D. in Statistics from Northwestern University before joining Novartis. He has been supportin g the design and analysis of clinical trials across different therapeutic areas. His research interest includes multiplicity issues, dose finding and missing data. 5. Network Based Analysis of Big Data Presenter: Shuangge Steven Ma, Yale University. Email: shuangge.ma@yale.edu Course Length: Half day Outline/Description With the fast development in data collection and storage techniques, big data are now routinely encountered in biomedicine, engineering, social science, and many other scientific fields. In many of the existing analyses, the interconnections among function al units have not been sufficiently accounted for, leading to a loss of efficiency or even failures of many statistical models. Recently, network -based analysis has emerged as an effective analysis tool for modeling big data. In this short course, we will survey the newly developed network -based analysis methods for big data, with an emphasis on methodological development and applications. Topics to be covered will include: Background of network analysis, including motivating examples from biomedicine and social science. A brief survey of network construction methods. Experiment -based, statistical, and hybrid methods will be introduced. We will introduce network construction algorithms, their rationale, and software implementation. Incorporating network information in statistical modeling. With big data, the two main analysis paradigms are marginal analysis and joint analysis. For each paradigm, we will introduce multiple recently -developed statistical methods, their rationale, and software implementation . Demonstrating examples from biomedicine will be provided, showing the practical impact of network analysis. 2015 ICSA/Graybill Joint Conference , For t Collins, Colorado, June 14 -17 | 17 Short Courses Network analysis of samples. A representative example is social netwo rk. Another example is the recently proposed concept of \"human disease network\". We will introduce concepts and analysis methods and show data analysis examples. After taking the course, audiences are expected to have a good understanding of (a) the \"big picture\" of analyzing big data using network -based methods, (b) a set of recently proposed methods, and (c) their software implementation. The demonstrating examples will be from multiple scientific fields and expected to be tightly related to daily pract ice of the audience. Intended audience will include researchers from academia, pharmaceutical companies, consulting firms, and government agencies as well as advanced graduate students. Prerequisites: master -level training in statistics or a related fiel d; generic knowledge of big data; knowledge of statistical software especially R will be a plus but not required. About the presenter Shuangge (Steven) Ma, PhD in Statistics from University of Wisconsin in 2004, is an associate professor in biostatistics at Yale University. His research interests include analysis of high-dimensional data, genetic epidemiology, cancer studies, and health economics. He has published two books and over 100 journal articles. He has served as associate editor of multiple journa ls. He is an elected member of ISI and fellow of ASA. 6. Classification and Regression Trees and Forests Presenters : Wei-Yin Loh, University of Wisconsin Email: loh@stat.wisc.edu Course length: Half day Outline/Description It is more than 50 and 30 years since AID (Morgan and Sonquist 1963) and CART (Breiman et al 1984) appeared. Rapidly increasing use of trees among practitioners has led to great advances in algorithmic research over the last two decades. Modern tree mode ls have higher prediction accuracy and do not have selection bias. They can fit linear models in the nodes using GLM, quantile, and other loss functions; response variables may be multivariate, longitudinal, or censored; and classification trees can emplo y linear splits and fit kernel and nearest -neighbor node models. The course begins with examples to compare tree and traditional models. Then it reviews the major algorithms, including AID, CART, C4.5, CHAID, CRUISE, CTREE, GUIDE, M5, MOB, and QUEST. Rea l data are used to illustrate the features of each, and results on prediction accuracy and model complexity versus forests and some machine learning methods are presented. Examples are drawn from business, science, and industry, and include applications t o subgroup identification for personalized medicine, missing value imputation in surveys, and differential item functioning in educational testing. Relevant software is mentioned where appropriate. Attendees should be familiar with multivariate analysis a t the level of Johnson and Wichern's \"Applied Multivariate Statistical Analysis.\" Course Outline: The target audience is statistical researchers and practitioners from academia, business, government, and industry. The course is particularly useful for pe ople who routinely analyze large and complex datasets and who want to know the latest advances in algorithms and software for classification and regression tree methods. About the presenter Wei-Yin Loh is Professor of Statistics at the University of Wisco nsin, Madison. He has been developing algorithms for classification and regression trees for thirty years. He is the co - author (with his students) of the FACT, QUEST, CRUISE, and LOTUS algorithms and the author of GUIDE (www.stat.wisc.edu/~loh/guide.html) . Versions of his QUEST algorithm are implemented in IBM SPSS and Dell Statistica. Dr. Loh is a fellow of the American Statistical Association and the Institute of Mathematical Statistics and a consultant to government and industry. He is a recipient of th e Benjamin Reynolds Award for teaching, the U.S. Army Wilks Award for statistics research and application, and an Outstanding Science Alumni Award from the National University of Singapore. He has supervised the thesis research of twenty nine PhDs to date . 7. Patient -Reported Outcomes: Measurement, Implementation and Interpretation Presenter: Joseph C. Cappelleri, Pfizer, Inc. Email: joseph.c.cappelleri@pfizer.com Course length: Half day Outline/Description: This half -day short course provides an exposition on health measurement scales - specifically, on patient -reported outcomes based on the instructor's co -authored book. Some key elements in the development of a patient -reported outcome (PRO) instrument are noted. Highlighted here is the importance of the conceptual framework used to depict the relationship between items in a PRO instrument and the concepts measured by it. The core topics of validity and reliability are discussed. Validity, which is assessed in several ways, provides the evidence and extent that the PRO taps into the concept that it is purported to measure in a particular setting. Reliability of a PRO instrument inv olves its consistency or reproducibility as 18 | 2015 ICSA/Graybill Joint Conference , Fort Collins, Colorado, June 14 -17 Short Courses assessed by internal consistency and test -retest reliability. Exploratory factor analysis and confirmatory factor analysis are described as techniques to understand the underlying s tructure of a PRO measure with multiple items. While most of the presentation centers on psychometrics from a classical test theory perspective, attention is also given to item response theory as an approach to scale development and evaluation. Cross -sectional analysis and longitudinal analysis of PRO scores are covered. Also covered is the topic of mediation modeling as a way to identify and explain the mechanism that underlies an observed relationship between an independent variable and a dependent varia ble via the inclusion of a third explanatory variable, known as a mediator variable. Variations of missing data for PRO measures are highlighted, as is the topic of multiple testing. Finally, approaches to interpret PRO results are elucidated in order to m ake these results useful and meaningful. Illustrations are provided mainly through real -life examples and also through simulated examples using SAS. Textbook/References: Cappelleri JC, Zou KH, Outcomes: Measurement, Implementation and Interpretation . Boca Raton, Florida: Chapman & Hall/CRC Press. 2013. Cappelleri JC, Bushmakin AG. Interpretation of patient - reported outcomes. Statistical Methods in Medical Research . 2014; Bell SS, Duttagupta S. Development and validation of the Self -Esteem And Relationship (SEAR) questionnaire in erectile dysfunction. International Journal of Impotence Research Kim ST, Chen I, Michaelson MD, Motzer RJ. Quality of life in patients with metastatic renal cell carcinoma treated with sunitinib versus interferon -alfa: Results from a phase III randomized Journal of Clinical Oncology . 2008; 26:3763 -3769. Fairclough DL. Design and Analysis of Quality of Life Studies in Clinical Trials . 2nd edition. Boca Raton, FL: Chapman & Hall/CRC Press. 2010. Food and Drug Administration (FDA). 2009. Guidance for industry on patient -reported outcome measures: Use in medical product development to support labeling claims. Federal Register 74(235):65132 -65133. http://www.fda.gov/downloads/ Drugs/GuidanceCompliance Kline Leidy N, Martin ML, Molsen E, Ring L. Content validity \u2014Establishing and reporting the evidence in newly developed patient reported outcomes (PRO) instruments for medical product evaluation: ISPOR PRO good research practices task force report: Part 1\u2014Eliciting concepts for a new PRO instrument. Value in Health. 2011; 14:967 -977. Patrick DL, Burke LB, Gwaltney CH, Kline Leidy N. Martin ML, Molsen E, Ring L. Content Validity \u2014Establishing and reporting the evidence in newly developed patient reported outcomes (PRO) instruments for medi - cal product evaluation: ISPOR PRO good research practices task force report: Part 2 \u2014Assessing respondent understanding. Value in Health. AG, Whalen E, Barrett JA, Sadosky A. The effects of pregabalin on sleep disturbance symptoms among individuals with fibromyalgia syndrome. Sleep Medicine . 2009; 10:604 - 610. About the presenter: Joseph C. Cappelleri earned his M.S. in statistics from the City University of New York (Baruch College), Ph.D. in psychometrics from Cornell University, and M.P.H. in epidemiology from Harvard University. In Ju ne 1996, Joe joined Pfizer Inc as a statistical scientist collaborating with Outcomes Research and is a senior director of biostatistics at Pfizer. He is also an adjunct professor of biostatistics at Brown University, adjunct professor of statistics at the University of Connecticut, and adjunct professor of medicine at Tufts Medical Center. A Fellow of the American Statistical Association, and chair of its Health Policy Statistics Section, Joe has delivered numerous conference presentations and has publishe d extensively on clinical and methodological topics, including regression -discontinuity designs, meta -analysis, and health measurement scales. He is the lead author of the book \"Patient -Reported Outcomes: Measurement, Implementation and Interpretation.\" Bin Yu University of California Berkeley David Madigan Columbia University Keynote Speakers For inquiries, please email: Dr . Yichuan Zhao yichuan@gsu.edu 25th ICSA Applied Statistics Symposium June 12 -15, 2016 Hyatt Regency Hotel 265 Peachtree Street NE Atlanta, GA 30303 20 | 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14 -17 10th ICSA International Conference The 10th ICSA International Conference Shanghai, China, December 19 -22, 2016 The 10th ICSA International Conference will be held at Xuhui campus of Shanghai Jiao Tong University (SJTU), Shanghai, China, during December 19 -22, 2016. The theme of this conference is to promote global growth of modern statistics in the 21st century. The purpose of this conference is to bring statisticians from all over the world to Shanghai, China, which is the financial, trade, information and shipping center of China, to share cutting -edge research, discuss emerging issues in the field of modern probability and statistics with novel applications, and network with colleagues from all parts of the world. James O. Berger of Duke University, Tony Cai of University of Pennsylvania, Kai-Tai Fang of Beijing Normal University - Hong Kong Baptist University United International College (UIC), Zhi-Ming Ma of the Academy of Math and Systems Science, CAS , Marc A. Suchard of the UCLA Fielding School of Public Health and David Geffen School of Medicine at UCLA, Lee-Jen Wei of Harvard University, and C. F. Jeff Wu of Georgia Institute of Technology will deliver keynote presentations. There will be a special session in honor of the receipt(s) of the second Pao -Lu Hsu award. In addition, there will be ample of invited and contributed sessions. All participants including invited speakers are responsible for paying registr ation fees and booking hotel rooms directly from the hotels listed on the conference website. The scientific program committee of the 2016 ICSA International Conference, co -chaired by Ming -Hui Chen of University of Connecticut, Zhi Geng of Peking Univer sity, and Gang Li of University of California at Los Angeles, welcomes the submission of invited session proposals. The deadline for submitting invited session proposals is May 1, 2016 . All of the invited session proposals are sent via email to Ming -Hui Chen at ming -hui.chen@uconn.edu . For conference logistics, please directly contact Dong Han and Weidong Liu, the co -chairs of the local organizing committee. All inquiries should be sent to Ms. Limin Qin at qinlimin@sjtu.edu.cn . Please visit the conference website http://www.math.sjtu.edu.cn/conference/2016icsa/ for more detailed information. All of you are welcome to participant in this important ICSA conference and to visit Shanghai, one of beautiful and historica l cities in the world during December 19 -22, 2016. 2015 ICSA/Graybill Joint Conference , For t Collins, Colorado, June 14 -17 | 21 ICSA Banquet at JSM 2015 Announcement: ICSA Annual Members Banquet at JSM 2015 The ICSA will be holding a banquet on August 12, 2015, at 7:30pm, at the China Harbor Restaurant, 2040 Westlake Avenue North, Seattle, WA 98109 (206 -286-1688, http://www.chinaharborseattle.com/). China Harbor is a family owned restaurant that offers the best in authentic Chinese cuisine located on the shores of Lake Union. Charter busses will be provided to transport ICSA members from the Convention Center to the restaurant. The banquet menu will include: Appetizer Plate ( )/ Seafood Hot and Sour Soup ( )/ Honey Walnut Prawns ( )/ General Tso's Chicken ( ) / Sweet & Sour Pork ( ) / Broccol i Beef()/ Mixed Vegetables ( )/ Fu Chow Seafood Fried Rice ( )/ Steamed Fish Filet in Wine Sauce ( ). Complementary soda & tea will be served as well as Season Fresh Fruit. A full service bar will be available and the restaurant has live music and dance. Ying Q. Chen, ( yqchen@fhcrc.org ) CONTEMPORARY CLINICAL TRIALS COMMUNICATIONS www.elsevier.com/locate/issn/24518654 EDITORS : Dr. Zhezhen Jin Columbia University, New York, NY USA Dr. Zheng Su Deerfield Institute, New York, NY USA A NEW OPEN ACCESS SISTER JOURNAL OF CONTEMPORARY CLINICAL TRIALS ACCEPTING RESEARCH ON BOTH RANDOMIZED AND NON -RANDOMIZED TRIALS A NEW OPEN ACCESS JOURNAL ES 3017 Contemporary Clinical Trials Comms AND SCOPE www.elsevier.com/locate/issn/24518654 Contemporary Clinical Trials Communications is an international peer reviewed open access journal that publishes articles pertaining to all aspects of clinical trials, including, but not limited to, design, conduct, analysis, regulation and ethics. Manuscripts submitted should appeal to a readership drawn from a wide range of disciplines including medicine, life science, pharmaceutical science, biostatistics, epidemiology, computer science, management science, behavioral science, and bioethics. Contemporary Clinical Trials Communications is unique in that it is outside the confines of disease specifications, and it strives to increase the transparency of medical research and reduce publication bias by publishing scientifically valid original research findings irrespective of their perceived importance, significance or impact. Both randomized and non-randomized trials are within the scope of the Journal. Some common topics include trial design rationale and methods, operational methodologies and challenges, and positive and negative trial results. In addition to original research, the Journal also welcomes other types of communications including, but are not limited to, methodology reviews, perspectives and discussions. Through timely dissemination of advances in clinical trials, the goal of Contemporary Clinical Trials Communications is to serve as a platform to enhance the communication and collaboration within the global clinical trials community that ultimately advances this field of research for the benefit of patients. TOSUBMIT A MANUSCRIPT AND FOR MORE INFORMATION , VISIT EDITORS -IN -C HIEF Zhezhen Jin Columbia University, New York, NY, USA Zheng Su Deerfield Institute, New York, NY, USA ASSOCIATE EDITORS Julian Abrams Columbia University, New York, NY, USA Vance Berger National Institute of Health, Rockville, MD, USA Cindy Cooper University of Sheffield, Sheffield, UK Brian Everitt King's College, London, London, UK Luis Garcia-Ortiz University of Salamanca, Salamanca, Spain Pei He Genentech Inc, South San Francisco, CA, USA Li-Shan Huang National Tsinghua University, Taiwan, Hsinchu City, Taiwan Yunzhi Lin AbbVie, North Chicago, IL, USA Xiaolong Luo Celgene Corporation, Summit, NJ, USA Prakash Satwani Columbia University, New York, NY, USA Consolato Sergi University of Alberta, Edmonton, AB, Canada Yu Shen The University of Texas, Houston, TX, USA Say Beng Tan National University of Singapore, Singapore Corrine Voils Duke University, Durham, NC, USA Xiaonan Xue Yeshiva University, Bronx, NY, USA Anny-Yue Yin Roche (China) Holding Ltd., Shanghai, China Ming Zhu AbbVie, North Chicago, IL, USA Christos Zouboulis Dessau Medical Center, Dessau, Germany ES Program (\u0007Presenting Author ) Scientic Program (June 15th - June 17th) Monday, June 15. 8:20 AM - 9:40 AM Keynote Session (Keynote ) Room: Grand Ballroom A/B, Level 2 Organizers: Executive Committee of the 2015 ICSA/Graybill Joint Conference. Chair: Naitee Ting, Boehringer-Ingelheim Pharmaceuticals Inc.. 8:20 AM Welcome Naitee Ting , Conference Chair Wei Shen , President ICSA Dean Jan Nerger , College of Natural Sciences, Colorado State University 8:40 AM Keynote Lecture Susan Murphy . University of Michigan 9:40 AM Floor Discussion. Monday, June 15. 10:00 AM-11:40 AM Session 4: New Techniques for Functional and Longitudinal Data Analysis (Invited ) Room: 300, level 3 Organizer: Guanqun Cao, Auburn University. Chair: Guanqun Cao, Auburn University. 10:00 AM Variable Selection Methods for Functional Regression Mod- els Nedret Billor . Auburn University 10:25 AM Structured Functional Principal Component Analysis in Mul- tilevel Functional Mixed Models for Physical 10:50 Diurnal Patterns Maize Leaf with RNA- sequencing Data Wen State Uni- versity3Monsanto company4Donald Danforth Science Center 11:15 AM Partial and Tensor Quantile Regressions in Functional Data Analysis \u0007Dengdeng Yu, Linglong Kong and Ivan Mizera . University of Alberta 11:40 AM Floor Discussion. Session 5: Recent Advancements in Statistical Machine Learning (Invited ) Room: 304, level 3 Organizers: Jinyuan Chang, University of Melbourne; Wen Zhou, Colorado State University. Chair: Jinyuan Chang, University of Melbourne.10:00 AM Sparse CCA: Minimax Adaptive Estimation \u0007Chao Gao1, Zongming Ma2and Harrison Zhou1.1Yale University2University of Pennsylvania 10:25 AM Asymptotic Normality in Estimation Large Ising 10:50 AM Optimal Tests of Independence with Applications to Testing More Structures \u0007Fang Han1and Han Liu2.1Johns Hopkins University 2Princeton University 11:15 AM Bootstrap Tests on High Dimensional Covariance Matrices with Applications 11:40 AM Floor Discussion. Session 15: Innovative Statistical Approaches in Nonclinical Research (Invited ) Room: 308, level 3 Organizer: Alan Chiang, Eli Lilly and Company. Chair: Grace Li, Eli Lilly and Company. 10:00 AM Identifying Predictive Biomarkers in A Dose-Response Study \u0007Yuefeng Lu, Xiwen Ma and Wei Zheng . Sano-aventis U.S. LLC. 10:25 AM Bayesian Integration of In Vitro Biomarker Data to In Vivo Safety Assessment \u0007Ming-Dauh Wang and Alan Chiang . Eli Lilly and Com- pany 10:50 AM Functional Structural Equation Model for DTI Derived Re- sponses in Twin Study Rui Song1.1North Car- olina State University2The University of North Carolina at Chapel Hill 11:15 AM Estimating Contamination Rates from Matched Tumor- normal Exome Sequencing 11:40 AM Floor Discussion. Session 37: Statistical Methods for Large Computer Experi- ments (Invited ) Room: Grey Rock, level 2 Organizer: Thomas Lee, University of Carlifornia, Davis. Chair: Chun-Yip Yau, The Chinese University of Hong Kong. 24j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Scientic Program (\u0007Presenting Author ) Monday, June 15. 10:00 AM-11:40 AM 10:00 AM Uncertainty Propagation using Dynamic Discrepancy for a Multi-scale Carbon Capture System \u0007K. Sham Bhat1, Corporation 10:25 AM Bayesian Calibration of Computer Models with Informative Failures \u0007Peter Marcy Curtis Storlie . Los Alamos National Lab- AM A Approach to Computer Model Calibration \u0007Raymond K. W. Wong1, Curtis B. Storlie2and Thomas C. M. Lee3.1Iowa State University2Los Alamos National Laboratory3University of California, Davis 11:15 AM Floor Discussion. Session 40: Use of Biomarker and Genetic Data in Drug De- velopment (Invited ) 310, level 3 Organizer: Xiaoxi Li, Biogen Idec. Chair: Yuan Wang, M. D. Anderson Cancer Center. 10:00 AM An Overview of Statistical Methods in Biomarker Evaluation \u0007Dawei Liu, John Zhong, Kimberly Crimin, lakshmi Stacy Lindborg and Donald Johns . Biogen Idec 10:25 AM A Case Study of Integrating Scientic Knowledge with Sta- tistical Biomarker Analysis Sheng Feng . Biogen Idec Genetic Heterogeneity Analysis and and Venkatraman Seshan . Memorial Sloan- Kettering Cancer Center 11:15 AM Floor Discussion. Session 42: New Methodology in Spatial and Spatio- Temporal Data Analysis (Invited ) Room: 386, level 3 Organizer: Yehua Li, Iowa State University. Chair: Zhengyuan Zhu, Iowa State University. 10:00 AM Estimation of Spatial Variation in Disease Risk from Uncer- tain Locations Using SIMEX Dale Zimmerman . University of Iowa 10:25 AM Bayesian Estimates of CMB Gravitational Lensing Ethan Anderes . University of California, Davis 10:50 AM Bayesian Functional Data Models for Coupling High- dimensional LiDAR and Forest Variables over Large Geo- graphic Domains Sudipto Banerjee2, State University2University of California, Los Angeles3National Aeronautics and Space Administration 11:15 AM Spatial Bayesian Hierarchical Model for Small Area Estima- tion of Categorical Data Xin Wang1, and Gabriel of Missouri-Columbia11:40 AM Floor Discussion. Session 48: Trends and Innovation in Missing Data Sensitiv- ity Analyses (Invited ) Room: 312, level 3 Organizer: Craig Mallinckrodt, Eli Lilly and Company. Chair: Lei Xu, Biogen Idec. 10:00 AM Missing Data Sensitivity Analyses for Continuous Endpoints Using Controlled Imputations Craig Mallinckrodt . Eli Lilly and Company 10:25 AM Sensitivity Analysis for 10:50 AM Analysis and Sensitivity Incomplete Categorical Data Geert Molenberghs1;2.1Universiteit Hasselt2Katholieke Universiteit Leuven 11:15 AM Food and Drug Admin- istration 11:40 AM Floor Discussion. Session 60: Toward More Effective Identication of Biomarkers and Subgroups for Development of Tailored Therapies (Invited ) Room: 324, level 3 Organizer: Lei Shen, Eli Lilly and Company. Chair: Yu Kong, Eli Lilly and Company. 10:00 AM Condence Intervals for Assessing SNP Effects on Treat- University2University of Pittsburgh3Eli Lilly and Company 10:25 AM Identication of Biomarker Signatures Using Adaptive Elas- tic Net \u0007Xuemin Lei Yaoyao Xu3.1Bristol-Myers Lilly Bias in Biomarker Identication Shengchun Kong . Purdue University 11:15 AM Analysis Optimization for Biomarker and Subgroup Identi- cation Lei Shen . Eli Lilly and Company 11:40 AM Floor Discussion. Session 69: Recent Developments in Empirical Likelihood Methodologies: Diagnostic Studies, Goodness-of-Fit Test- ing, and Missing Values (Invited ) Room: 372//374, level 3 Organizers: Dongliang Wang, SUNY Lili Tian, University at Buffalo . Chair: Dongliang Wang, SUNY Upstate Medical University. 10:00 AM Jackknife Empirical Likelihood Condence Regions for the Evaluation of Continuous-scale Diagnostic Tests cation Bias Univer- sity2Georgia State ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j25Monday, June 15. 1:00 PM - 2:40 PM Scientic Program (\u0007Presenting Author ) 10:25 AM Jackknife Empirical Likelihood Goodness-Of-Fit Tests Com- pany 10:50 AM Jackknife Empirical Likelihood Interval Estimators for AM Jackknife Empirical Likelihood 11:40 AM Floor Discussion. Session 79: Recent Developments on Combining Inferences and Hierarchical Models (Invited ) Room: 306, level 3 Organizer: Min-Ge Xie, Rutgers University. Chair: Cen Wu, Yale University. 10:00 AM Statistical Issues in Health Related Quality of Life research Mounir Mesbah . University Pierre et Marie Curie 10:25 AM Analysis with Individual Sciences AM Combining Liu2and Minge Xie2.1University of Cincinnati2Rutgers Models for Document Networks \u0007Linda Tan1, Aik Hui Chan2and Tian Zheng1.1Columbia University2Naitional University of Singapore 11:40 AM Floor Discussion. Session 80: Recent Advances in Development and Evaluation of Predictive Biomarkers (Invited ) Room: 376//378, level 3 Organizers: Haiwen Shi, U.S. Food and Drug Administration; Jingjing Ye, U.S. Food and Drug Administration. Chair: Xiaojing Wang, University of Connecticut. 10:00 AM Identifying Optimal Biomarker Combinations for Treatment Selection through Randomized Controlled Trials Ying Huang . Fred Hutchinson Cancer Research Center 10:25 AM The Challenge in Making Inference about a Biomarker's Pre- dictive Capacity Holly Janes . Fred Hutchinson Cancer Research Center 10:50 AM A Potential Outcomes Framework for Evaluating of Health11:15 AM Discussant: Greg Campbell, U.S. Food and Drug Adminis- tration 11:40 AM Floor Discussion. Session 81: What Are the Expected Professional Behaviors After Statistics Degrees (Invited Panel ) Room: 382, level 3 Organizers: Bin Yu, University of California, Berkeley; Haoda Fu, Eli Lilly and Company. Chair: Haoda Fu, Eli Lilly and Company. Panelists: Richard Davis, Columbia University Susan Murphy, University of Michigan Jean Opsomer, Colorado State University 11:40 AM Floor Discussion. Session 90: Adaptive Designs and Personalized Medicine (Invited ) Room: 322, level 3 Organizer: Yingqi Zhao, University of Wisconsin-Madison. Chair: Yingqi Zhao, University of Wisconsin-Madison. 10:00 AM Interpretable and Parsimonious Treatment Regimes Using and Marie Da- vidian . North Carolina State University 10:25 AM Regression Analysis for Cumulative Incidence Function un- Stage, Adaptive Enrichment Designs for Ran- domized Trials, using Sparse Linear Programming \u0007Michael Rosenblum1, Xingyuan (Ethan) Fang2and Han Liu2.1Johns Hopkins University2Princeton University 11:15 AM Floor Discussion. Session C02: Design and Analysis of Clinical Trials (Contributed ) Room: Virginia Dale, level 3 Organizer: Peng-Liang Zhao, Sano-aventis U.S. LLC.. Chair: Bo Huang, Pzer Inc.. 10:00 AM Sample Size Re-Estimate of BE Studies with Adaptive De- sign Peng Roger Qu . Pzer China R&D Center 10:15 AM Sequential Phase II Clinical Trial Design for Molecularly Targeted Agents \u0007Yong Zang1and Ying Yuan2.1Florida Atlantic University 2M. D. Anderson Cancer Center 10:30 AM On Sensitivity Analysis for Missing Data using Control- based Imputation Frank Liu . Merck & Co. 10:45 AM Choosing Covariates for Adjustment in Non-Inferiority Tri- als Based on Influence Ramakrishnan and Va- of South Carolina 26j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Scientic Program (\u0007Presenting Author ) Monday, June 15. 1:00 PM - 2:40 PM 11:00 AM Statistical Assessment for Establishing Biosimilarity Medical Uni- versity4Foxconn International Company 11:15 AM Floor Discussion. Monday, June 15. 1:00 PM - 2:40 PM Session 1: Best Practices for Delivery of Adaptive Clinical Trials Illustrated with Case Studies (Invited ) Room: 382, level 3 Organizer: Zoran Antonijevic, Cytel Inc.. Chair: Zoran Antonijevic, Cytel Inc.. 1:00 PM Adaptive Design Scientic Working Group Best Prac- tices Team: Objectives and Case Studies Eva Miller . inVentiv Health Clinical 1:25 PM An Adaptive Phase 3 Trial Resulting in FDA Approval of Crofelemer Zoran Antonijevic . Cytel Inc. 2:15 PM Floor Discussion. Session 2: Chemistry, Manufacturing, and Controls (CMC) in Pharmaceuticals: Current Statistical Challenges I (Invited ) Room: 310, level Organizers: Richard Quiroz, Abb- Vie Inc.. Chair: Richard Burdick, Amgen Inc.. 1:00 PM Statistical Methods for Analytical Comparability Leslie Sidor . Amgen Inc. 1:25 PM How Type I Error Impacts Quality System Effectiveness Jeff Gardner . DataPharm Statistical & Data Management Services 1:50 PM Alternative Procedures for Shelf Life Estimation Utilizing Mixed Models \u0007Michelle Quinlan1, Walt Stroup2and Dave Christopher3. Co. 2:15 PM Discussant: Laura Pack, Amgen Inc. 2:40 PM Floor Discussion. Session 7: Scalable Multivariate Statistical Learning with Massive Data (Invited ) Room: 386, level 3 Organizer: Kun Chen, University of Connecticut. Chair: Kun Chen, University of Connecticut.1:00 PM False Discovery Control under Unknown Dependence Jianqing Fan1and\u0007Xu Han2.1Princeton University 2Temple University 1:25 PM Gao1,\u0007Zongming Ma2and Harrison Zhou1.1Yale University2University of Pennsylvania 1:50 PM A Class of Accelerated MM Algorithms for Scalable Opti- mization Yiyuan She . Florida State University 2:15 for High-Dimensional Non- linear Classication Yingying Fan,\u0007Yinfei Kong, Daoji Li and Zemin Zheng . University of Southern California 2:40 PM Floor Discussion. Session 9: SII Special Invited Session on Modern Bayesian Statistics I (Invited ) Room: 300, level 3 Organizers: Ming-Hui Chen, University of Connecticut; Heping Zhang, Yale University. Chair: Ming-Hui Chen, University of Connecticut. 1:00 PM Binary State Space Mixed Models with Flexible Link Func- tions: a Case Study on Deep Brain Stimulation on Attention Reaction Time Carlos Abanto-Valle1, Dipak Dey2and\u0007Xun Jiang3. 1Universidade Federal do Inc. 1:25 PM Bayesian Semi-parametric Joint Modeling of Biomarker Data with a Latent Changepoint: Assessing the Tempo- ral Performance of 2:15 PM Quantile Regression for Censored Mixed-Effects Models with Applications to HIV studies \u0007Victor Hugo of Connecticut3Universidade Federal do Rio de Janeiro 2:40 PM Floor Discussion. Session 16: Statistical Advances for Genetic Data Analysis (Invited ) Room: 312, level 3 Organizer: Yuehua Cui, Michigan State University. Chair: Ping-Shou Zhong, Michigan State University. 1:00 PM Incorporating External Information to Improve Case-control Genetic Association Analyses Hong Zhang1, Nilanjan Chatterjee2and\u0007Jinbo Institutes of Health3University of ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j27Monday, June 15. 1:00 PM - 2:40 PM Scientic Program (\u0007Presenting Author ) 1:25 PM Generalized Partial Linear Varying Index Coefcient Model for Gene-Environment Interactions \u0007Xu Liu, Bin Gao and Cui State Univer- sity 1:50 PM in Sequencing Studies Guolian Kang . St. Jude Children's Research Hospital 2:15 PM A Penalized Robust Semiparametric Approach State University 2:40 PM Floor Discussion. Session 19: Recent Developments in the Theory and Applica- tions of Spatial Statistics (Invited ) Room: 304, level 3 Organizer: Juan Du, Kansas State University. Chair: Zhengyuan Zhu, Iowa State University. 1:00 PM Estimating a Low Rank Covariance Matrix for Spatial Data \u0007Siddhartha Nandy, Chae-Young Lim Maiti . Dong University 1:50 PM Statistical Method for Change-set Analysis Jun Zhu . University of Wisconsin-Madison 2:15 PM Floor Discussion. Session 22: Clinical Trials with Multiple Objectives: Maxi- mizing the Likelihood of Success (Invited ) Room: 308, level 3 Organizers: Toshimitsu Hamasaki, National Cerebral and Cardio- vascular Health Chin-Fu Hsiao, National Health Research Insititutes, Tai- wan. 1:00 PM Statistical Challenges in Testing Multiple Endpoints in Com- plex Trial Designs \u0007H.M. James Hung and Sue-Jane Wang . U.S. Food and Drug Administration 1:25 PM Group-Sequential Clinical Trials When Considering Multi- University 1:50 PM Sample Size Determination for a Specic Region in Multi- regional Clinical Trials Huang1and School2:40 PM Floor Discussion. Session 26: Challenges in Analyzing Complex Data Using Re- gression Modeling Approaches (Invited ) Room: 324, level 3 Organizers: Fang-Chi Hsu, Wake Forest University School of Medicine; Wei-Ting Hwang, University of Pennsylvania. Chair: Jun Yan, University of Connecticut. 1:00 PM Goodness-of-Fit Tests of Finite Mixture Regression Models University3Duke University 1:25 PM Comparing Methods Hospital of Philadelphia 1:50 PM Alzheimer's Disease Early Prediction and Imaging Genetics Analyses Based on Large Scale Regularization \u0007Fang-Chi Hsu, Mark Espeland and Ramon Casanova . Wake Forest University School of Medicine 2:15 PM Discussant:Wei-Ting Hwang, University of Pennsylvania 2:40 PM Floor Discussion. Session 30: Tensor-Structured Statistical Modelling level 3 Organizer: Su-Yun Huang, Institute of Statistical Science, Academia Sinica. Chair: Yanyuan Ma, University of South Carolina. 1:00 PM Dimenstion Reduction for Tensor Structure Data I-Ping Tu . Institute of Statistical Science, Academia Sinica 1:25 PM Detection of Gene-Gene Chung Hsing University4Yahoo Inc.5North Carolina State University 1:50 PM Rank Selection for Multilinear PCA Dai-Ni Hsieh,\u0007Su-Yun Huang and I-Ping Tu . Institute of Statistical Science, Academia Sinica 2:15 PM Discussion: Tensor-Structured Statistical Modelling and In- Huang . National Sun Yat-sen University 2:40 PM Floor Discussion. Session 41: New Frontier of Functional Data Analysis (Invited ) Room: 306, level 3 Organizer: Yehua Li, Iowa Chair: Yehua Li, Iowa State University. 28j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Scientic Program (\u0007Presenting Author ) Monday, June 15. 1:00 PM - 2:40 PM 1:00 PM Making Patient-specic Treatment Decisions Based on Functional and Imaging Data Adam Connectivity in Resting State with Model \u0007Simeng Jane-Ling Wang2and Xiao Wang1.1Purdue University2University of California, Davis PM Robust and Gaussian Adaptive Mixed Models for Correlated Functional Data, with Application to Event-Related Potential Data \u0007Hongxiao Zhu1and Jeffrey Morris2.1Virginia Tech2M. D. Anderson Cancer Center 2:40 PM Floor Discussion. Session 54: Recent Development in Epigenetic Research (Invited ) Room: Grey Rock, level 2 Organizer: Yongseok Park, University of Pittsburgh. Chair: Yongseok Park, University of Pittsburgh . 1:00 PM A Hidden Markov Random Field Based Bayesian Method for the Detection of Long-range Chromosomal 1The of North Carolina at Chapel Western Reserve University3New York University 1:25 PM Base-resolution Methylation Patterns Accurately Predict Transcription Factor Bindings In Vivo \u0007Tianlei Xu, Ben Li, Meng Zhao, Keith E. Szulwach, R. Craig Street, Li Lin, Bing Yao, Feiran Zhang, Peng Jin, Hao Wu and Zhaohui Qin . Emory University 1:50 PM Statistical Analysis HumanMethylation450 BeadArrays \u0007Jie Liu and Kimberly Siegmund . University Southern California 2:15 PM Differential Methylation Analysis for BS-seq Data under General Experimental Design \u0007Yongseok Park1and Hao Wu2.1University of Pittsburgh 2Emory University 2:40 PM Floor Discussion. Session 64: Recent Development in Personalized Medicine and Survival Analysis (Invited ) Room: ASCSU Senate Chambers, level 2 Organizer: Rui Song, North Carolina State University. Chair: Peng Roger Qu, Pzer China R & D Center. 1:00 PM Estimating the Optimal Dynamic Treatment Regime from a Classication PM Parsimonious and Robust Treatment Strategies for Target Populations Using Clinical Trial Data \u0007Yingqi Zhao1and Donglin Zeng2.1University of Wisconsin-Madison2The University of North Carolina at Chapel Hill 1:50 PM A Sieve Semiparametric Maximum Likelihood Approach for Regression Analysis of Bivariate Interval-censored Failure Time Data \u0007Qingning Zhou1, Tao Hu2and Jianguo Sun1.1University of Missouri-Columbia2Capital Normal University 2:15 PM Floor Discussion. Session 67: New Advances in Adaptive Design and Analysis of Clinical Trials (Invited ) Room: 322, level 3 Organizers: Ming Tan, Georgetown University; Peter Zhang, Ot- suka Pharmaceutical Development & Commercialization Inc.. Chair: Peter Zhang, Otsuka Pharmaceutical Development & Com- mercialization Inc.. 1:00 PM Sensitivity Analyses for Missing Not at Random (MNAR) in Clinical Trials Peter Zhang . Otsuka Pharmaceutical Development & Com- mercialization Inc. 1:25 PM Moment-based Covariate Adjustment Method for Treatment \u0007Xiaofei Wang1, Junling of Finance and Economics 1:50 PM On Design and Analysis of a Stratied Biomarker Time-to- Event Clinical Trial in the Presence of Measurement Error Aiyi Liu . National Institutes of Health 2:15 PM Floor Discussion. Session 75: Model Selection in Complex Data Settings (Invited ) Room: 372//374, level 3 Organizer: Hai Liu, Indiana University. Chair: Hai Liu, Indiana University. 1:00 PM Meta-analysis Based Variable Selection for Gene Expression Data Quefeng Li1,\u0007Sijian Wang2, Menggang Yu2and Jun Shao2. at Chapel Hill2University of Wisconsin-Madison 1:25 PM Structural Discovery for Joint Models of Longitudinal and Survival Outcomes \u0007Zangdong He, Wanzhu Tu and Zhangsheng Yu . Indiana University 1:50 PM An Empirical Bayes Approach to Integrate Multiple GWAS with Gene Expressions from Multiple Tissues \u0007Jin Liu1and Can Yang2.1Duke-NUS2Hong Kong Baptist University 2:15 PM 2:40 PM Floor Discussion. 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j29Monday, June 15. 3:00 PM - 4:40 PM Scientic Program (\u0007Presenting Author ) Session 86: Cutting-Edge New Tools for Statistical Analysis and Modeling (Invited ) Room: Virginia Dale, level 3 Organizer: Yanwei Zhang, Chair: Yanwei Zhang, Pzer Inc.. 1:00 PM Web-based Analytics for Business Decision Making Sam Weerahandi . Pzer Inc. 1:25 PM A GUI Software for Synchronizing Study Design, Statistical Analyses, and Reporting into Simple Clicks Yanwei Zhang . Pzer Inc. 1:50 PM Bayesian Mechanism to Enhance Financial Value of Clinical Development Portfolio Shu Han . Pzer Inc. 2:15 PM An R Package Suite for Meta-analysis in Differentially Ex- pressed Gene Analysis Wang2.1Henry Ford Health System2University of Pittsburgh 2:40 PM Floor Discussion. Monday, June 15. 3:00 PM - 4:40 PM Session 8: New Statistical Advance in Genomics and Health Science Applications (Invited ) Room: 312, level 3 Organizer: Jin Liu, University of Illinois at Chicago. Chair: Kun Chen, University of Connecticut. 3:00 PM Linking Lung Airway Structure to Pulmonary Function via Hierarchical Feature Selection 3:25 PM Imputing Transcriptome of Inaccessible Tissues In and Be- yond Jiebiao with the Haseman Elston Approximate Regression Xiang Zhou . University of Michigan 4:15 PM Improved Ancestry Estimation for both Genotyping and Se- quencing Data using Southwestern Medical Center 3Harvard University4University of Michigan 4:40 PM Floor Discussion. Session 23: Issues Related to Subgroup Analysis in Con- rmatory Clinical Trials: Challenges and Opportunities (Invited ) Room: 310, level 3Organizers: Chin-Fu Hsiao, National Health Research Insititutes, Taiwan; Toshimitsu Hamasaki, National Cerebral and Cardiovascu- lar Center, Japan. Chair: Toshimitsu Hamasaki, National Cerebral and Cardiovascular Center, Japan. 3:00 PM A Statistical Decision Framework Applicable to Multipopu- lation Tailoring Trials Brian Millen . Eli Lilly and Company 3:25 PM A Multiple Comparison Procedure for Subgroup Analyses with Binary Endpoints . Pharmaceutical Corporation 3:50 PM Interaction Trees for Exploring Stratied and Individualized Treatment Effects Xiaogang Su . The University of Texas at El Paso 4:15 PM Considering Regional Difference in Design and Evaluation of MRCTs for Binary Endpoints \u0007Chi-Tian Chen and Chin-Fu Hsiao . National Health Re- search Institutes 4:40 PM Floor Discussion. Session 25: Spatial and Spatio Temporal Modeling in Envi- ronmental and Ecological Studies (Invited ) Room: 300, level 3 Organizers: Ephraim Hanks, Pennsylvania State University; Jun Zhu, University of Wisconsin-Madison. Chair: Mevin Hooten, Colorado State University. 3:00 on Spheres \u0007Juan Du1and Chunsheng Ma2.1Kansas 2Wichita State Coefcient Models University2U.S. Environmental Protection 3:50 PM Modeling Animal Abundance with A Semi-Parametric Space-Time Model Devin Johnson . The National Oceanic and Atmospheric Administration 4:15 PM An Efcient Non-parametric Estimate for Spatially Corre- lated Functional Data Kim-Anh Do, Jianhua Hu and Brian Hobbs . M. D. Anderson Cancer Center 4:40 PM Discussant: Yuan Wang, M. D. Anderson Cancer Center. Session 35: Novel Designs and Applications of Adaptive Ran- domization in Medical Research (Invited ) Room: 322, level 3 Organizer: Jack Lee, M. D. Anderson Cancer Center. Chair: Brian Hobbs, M. D. Anderson Cancer Center. 3:00 PM Statistical Inference for Covariate Adaptive Randomized Clinical Trials Wang1, Jing Ning2and\u0007Hongjian Zhu1.1The Uni- versity of Texas of Public Health2M. D. Anderson Cancer Center 30j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Scientic Program (\u0007Presenting Author ) Monday, June 15. 3:00 PM - 4:40 PM 3:25 PM Biomarker-Stratied Adaptive Basket Designs for Multiple Cancers Lorenzo Trippa . Dana-Farber Cancer Institute 3:50 PM Outcome Adaptive Comparative Effec- tiveness Clinical Trials Mei-Chiung Shih . V A Cooperative Studies Program 4:15 PM Worth Adapting? When and How to Apply Adaptive Ran- domization to Make More Bang for the Buck \u0007J. Jack Lee and Yining Du . M. D. Anderson Cancer Center 4:40 PM Floor Discussion. Session 47: New Development in Nonparametric Methods and Big Data Analytics (Invited ) Room: 386, level 3 Organizer: Ping Ma, University of Georgia. Chair: Ping Ma, University of Georgia. 3:00 PM A Nonparametric Spectral-Temporal Model for High-energy University2Harvard-Smithsonian Center Astrophysics3University of California, Davis 4Imperial College 3:25 PM Multistage Adaptive Testing of Sparse Signals Wenguang Sun . University of Southern California 3:50 PM Variable Selection for Sufcient Dimension Reduction using Weighted Leverage Score Wenxuan Zhong . University of Georgia 4:15 PM Efcient Computation Smoothing via Huang2and Nan Zhang2.1University of Georgia2Texas A&M University 4:40 PM Floor Discussion. Session 50: Biostatistics and Health Sciences (Invited ) Room: 324, level Curie. Chair: Mounir Mesbah, University Pierre et MarieCurie. 3:00 PM When to Initiate Combined Antiretroviral Therapy in HIV- infected Individuals to Reduce the Risk of AIDS or Severe Non-AIDS Morbidity Using Marginal Structural et Marie Curie 3:25 PM Trace Elements Uptake and Effect of Two Steppic Medici- nal Species of a Mining Area on Their Soil Trace Element Contents versus Bulk 3:50 PM Study of the Effect of Trace Metals from Old Antimony Mine on Biodiversity by Stepwise PM Floor Session 52: Advances in Survey Statistics (Invited ) Room: 304, level 3 Organizer: Jean Opsomer, Colorado State University. Chair: Jay Breidt, Colorado State University. 3:00 PM Quantile Regression Imputation for a Survey Sample Emily Berg and\u0007Cindy Yu . Iowa State University 3:25 PM Triply Robust Inference in the Presence of Missing Survey Opsomer, Jiwen Wu and Mary Meyer . Colorado State University 4:15 PM Floor Discussion. Session 63: Adaptive Design and Sample Size Re-Estimation (Invited ) Room: Xiaohua U.S.. 3:00 PM Methods for Sample-Size Design in Clinical Trials \u0007Gang Li1, Weichung Shih2and Yining Wang1.1Johnson & Johnson2Rutgers University 3:25 PM Blinded Sample Size Re-estimation in Trials with Survival Outcomes and Incomplete Information Thomas Cook . University of Wisconsin-Madison 3:50 Drug Admin- istration 4:40 PM Floor Discussion. Session 68: Design in Drug Combination Stud- ies(Invited ) Room: 372//374, Hopkins University. Chair: Zhiwei Zhang, U.S. Food and Drug Administration. 3:00 PM Design and Statistical Analysis of Multidrug Combinations in Preclinical Studies and Clinical Trials Ming Tan . Georgetown University 3:25 PM Bayesian Hierarchical Monotone Regression I-splines for Dose-Response Assessment PM A Bayesian Nonparametric Approach for Synergy Assess- ment in Drug Combination Studies Chenguang Wang . Johns Hopkins University 4:15 PM Discussant: Ying Yuan, M. D. Anderson Cancer Center 4:40 PM Floor Discussion. 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j31Monday, June 15. 4:40 PM - 6:00 PM Scientic Program (\u0007Presenting Author ) Session 74: Room: Organizer: Mei-Cheng Wang, Johns Hopkins University. Chair: Mei-Cheng Wang, Johns Hopkins University. 3:00 PM ANOV A for Longitudinal Data with Missing Values \u0007Songxi Chen1and Ping-Shou Zhong2.1Iowa State Uni- versity2Michigan State University 3:25 PM Calibration in Missing Data Analysis Through Empirical Likelihood Peisong Han . University of Waterloo 3:50 PM Asymptotic Behavior of the Sample Average of Partial Like- lihood for the Cox Model Jian-Jian Ren . University of Maryland 4:15 PM Efcient Estimation of the Cox Model with Auxiliary Sub- group Survival Information \u0007Chiung-Yu Huang1, Jing 3Georgetown University 4:40 PM Floor Discussion. Session 77: Recent Innovative Methodologies and Applica- tions in Genetics & Pharmacogenomics (GpGx) (Invited ) Room: Virginia Dale, level 3 Organizer: Ziwen Wei, Merck & Co.. Chair: Lynn Kuo, University of Connecticut. 3:00 PM Tree-based Rare Variants Analyses Chi Song and\u0007Heping Zhang . Yale University 3:25 PM Composite Kernel Machine Regression Based on Likelihood Ratio Test and its Application on Genomic Studies \u0007Ni Zhao and Michael Wu . Fred Hutchinson Cancer Re- search Center 3:50 PM Improving the Robustness of Variable Selection and Predic- tive Performance of Lasso and Elastic-net Regularized Gen- eralized Linear Models and Cox Hong and Viswanath Devanarayan . AbbVie Inc. 4:15 Discussion. Nonparametric and Statistics (Invited ) University of Wisconsin-Madison. Chair: Jiayang Sun, Case Western Reserve University. 3:00 PM Quantile Regression for Extraordinarily Large Data Stanislav Volgushev1and\u0007Guang Cheng2.1Cornell Uni- versity2Purdue University 3:25 PM A Validated Information Criterion to Determine the Struc- tural Dimension in Dimension Reduction Models \u0007Yanyuan Ma1and Xinyu Zhang2.1University of South Carolina2Chinese academy of sciences PM Systematic Clustering and Network Structures: a New Non- parametric Approach that Reveals Unprecedented Structures and Patterns, with Applications to Large CMS Data. Junheng Ma,\u0007Jiayang Sun and Gq Zhang . Case Western Reserve University4:15 PM Semiparametric Model Building for Regression Models with Time-Varying Parameters Ting Zhang . Boston University 4:40 PM Floor Discussion. Session 89: Recent Advances in Biostatistics (Invited ) Room: Grey Rock, level 2 Organizer: Yuping Zhang, University of Connecticut. Chair: Aiyi Liu, National Institutes of Health. 3:00 PM Promoting Similarity of Sparsity Structures in Integrative Analysis Shuangge Ma . Yale University 3:25 PM Graphical Models and Genomics Analyzing Spatially Glaucoma Mwanza2, Angelo Tanna3 and of North Carolina at Chapel Hill3Northwestern University 4:40 PM Floor Discussion. Session 93: Negotiation Skills Critical for Statistical Career Development (Invited Panel ) Room: 382, level 3 Organizer: Kelly Zou, Pzer Inc.. Chair: Kelly Zou, Pzer Inc.. Panelists: Ivan S. F. Chan, Merck & Co. Mary W. Gray, American University Susan Murphy, University of Michigan Wei Shen, Eli Lilly and Company 4:40 PM Floor Discussion. Monday, June 15. 4:40 PM - 6:00 PM Session P01: Poster Session (Poster ) Room: Grand Ballroom C/D, level 3 Organizer: Jun Yan, University of Connecticut. Chair: Jun Yan, University of Connecticut. 1: Correction for Confounding Effect in Random Forests Anal- ysis \u0007Yang Zhao and Donghua Lou . Nanjing Medical University 2: Strategies of Genetic Risk Prediction with Lung Cancer GWAS Data \u0007Donghua Lou, Weiwei Duan, Zhibin Hu and Feng Chen . Nanjing Medical University 3: Hierarchical Model for Genome-wide Association Study \u0007Honggang Yi, Hongmei Wo, Yang Zhao, Ruyang Zhang, Junchen Dai, Guangfu Jin Hongxia Ma . Nanjing Med- ical University 32j2015 ICSA/Graybill Joint Fort Collins, Colorado, June 14-17Scientic Program (\u0007Presenting Author ) Tuesday, June 16. 10:00 AM - 11:40 AM 4: A Review of Nonparametric Methods for Testing Isotropy in Spatial Data \u0007Zachary Weller and Jennifer Hoeting . Colorado State Nebraska-Lincoln3University of Washington 6: A Dynamical Model for Networks of Neuron Spike Trains \u0007Hongyu Tan, Phillip Chapman and Haonan Wang . Col- orado State University Sup \u00b4erieure de Biotechnologie Hypothesis Testing for an Extended Cox Model with Time- varying Coefcients \u0007Takumi Saegusa, Chongzhi Di and Ying Chen . Fred Hutchinson Cancer Research Center 9: The Stragety for Selecting Target Population Using Adaptive Phase II/III Seamless Design Based on Time-to-event Data \u0007Hao Yu, Dandan Miao and Feng Chen . Nanjing Medical University Tuesday, June 16. 8:40 AM - 9:40 AM Graybill Plenary Session (Plenary ) Room: Grand Ballroom A/B, Level 2 Organizers: Executive Committee of the 2015 ICSA/Graybill Joint Conference. Chair: Duane Boes, Colorado State University. 8:40 AM Graybill Plenary Lecture Richard Davis . Columbia University 9:40 AM Floor Discussion. Tuesday, June 16. 10:00 AM - 11:40 AM Session 3: Chemistry, Manufacturing, and Controls (CMC) in Pharmaceuticals: Current Statistical Challenges II (Invited ) Room: 308, level Organizers: Richard 10:00 AM Statistical Methods for Analytical Validation of Accuracy and Precision Richard Burdick . Amgen Inc. 10:25 AM Statistical Applications for Biosimilar Product Development Richard Montes . Hospira, Inc. 10:50 AM How to Set Up in Biosimilar Session 10: SII Special Invited Session on Modern Bayesian Statistics II (Invited ) Room: Grey Rock, level 2 Organizers: Ming-Hui Chen, University of Connecticut; Heping Zhang, Yale University. Chair: Heping Zhang, Yale University. 10:00 AM A Bayes Testing Approach to Metagenomic Proling of Connecticut 10:50 A Bayesian Approach to Identify Genes and Gene-level SNP Aggregates in a Genetic Analysis of Cancer Data Francesco Stingo1,\u0007Michael Swartz2and Marina Vannucci3.1M. D. Anderson Cancer Center2The Uni- versity of Texas School of Public Health3Rice University 11:15 AM Adjusting Nonresponse Bias in Small Area Estimation with- out Covariates via 11:40 AM Floor Discussion. Session 13: Recent Advance in Longitudinal Data Analyses (Invited ) Room: 310, level 3 Organizer: Yu Cheng, University of Pittsburgh. Chair: Ruosha Li, The University of Texas School of Public Health. 10:00 AM Integrative and Adaptive Weighted Group Lasso and Gener- alized Local Quadratic Approximation Qing Pan1and\u0007Yunpeng Zhao2.1The George Washington University2George Mason University 10:25 AM A Dynamic Risk Prediction Model for Data with Competing Risks \u0007Chung-Chou Chang1and Qing Liu2.1University of Pitts- burgh2Novartis Pharmaceutical Corporation 10:50 AM Simultaneous Inference of a Misclassied Outcome and Competing Risks Failure Time Data \u0007Sheng Luo1, Xiao Su1, Min Yi2and Kelly Hunt2.1The University of Texas at Houston2M. D. Anderson Cancer Center 11:15 AM Copula-based Quantile Regression for Longitudinal Data \u0007Huixia Wang1and Xingdong Eco- nomics 11:40 AM Floor Discussion. 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j33Tuesday, June 16. 10:00 AM - 11:40 AM Scientic Program (\u0007Presenting Author ) Session 20: Risk Prediction Modeling in Clinical Trials (Invited ) Room: 312, level 3 Organizer: Fenghai Duan, Brown University. Chair: Ying Huang, Fred Hutchinson Cancer Research Center. 10:00 AM Evaluating Calibration of Risk Prediction Models Ruth Pfeiffer . National Institutes of Health 10:25 AM Statistical Considerations for Evaluating Prognostic Imaging Biomarkers Zheng Zhang . Brown University 10:50 AM Risk Assessment for Patients with Hepatitis C: A Scoring System Approach \u0007Weining Shen1, Jing Ning1, Ziding Feng1and Anna Lok2.1M. D. Anderson Cancer Center2University of Michigan 11:15 AM Risk Prediction Modeling in the National Lung Screening Trial Fenghai Duan . Brown University 11:40 AM Floor Discussion. Session 28: Go/No Go Decision Criteria and Probability of Success in Pharmaceutical Drug Development (Invited ) Room: 322, level 3 Organizer: Bo Huang, Pzer Inc.. Chair: Bo Huang, Pzer Inc.. 10:00 AM Sample Size Allocation in a Dose-Ranging Trial Combined with PoC Qiqi Deng and\u0007Naitee Ting . Boehringer-Ingelheim Phar- maceuticals Inc. 10:25 AM Selecting Development Strategy with Biomarkers \u0007Feng Gao, Yi Liu and Mingxiu Hu . Takeda 10:50 AM Backward Bayesian Go/No-Go in the Early Phases Yin Yin . Parexel International 11:15 AM Evaluation of Program Success for Programs with Multiple Trials in Binary Outcomes \u0007Meihua Wang, Guanghan Liu and Jerald Schindler . Merck & Co. 11:40 AM Floor Discussion. Session 44: Funding Opportunities and Grant Applications (Invited Panel ) Room: 382, level 3 Organizer: Aiyi Liu, National Institutes of Health. Chair: Aiyi Liu, National Institutes of Health. Panelists: Debashis Ghosh, University of Colorado at Denver Hulin Wu, University of Rochester Heping Zhang, Yale University Li Zhu, National Institutes of Health 11:40 AM Floor Discussion.Session 55: New Method Development for Survival Analysis (Invited ) Room: 324, level 3 Organizer: Limin Peng, Emory University. Chair: Jong Jeong, University of Pittsburgh. 10:00 AM Analysis of the Proportional Hazard Model for with Sparse Longitudinal Covariates \u0007Hongyuan Cao1, Matthew M. of Missouri-Columbia 2University of Chicago3The University of North Carolina at Chapel Hill 10:25 AM Hypoglycemic Events Analysis via Recurrent Time-to-Event (HEART) Models Haoda Fu . Eli Lilly and Company 10:50 AM Accelerated Intensity Frailty Model for Recurrent Events Data Bo Liu1, Wenbin Lu1and\u0007Jiajia Zhang2.1North Carolina State University2University of South Carolina 11:15 AM A New Flexible Association Measure for Semi-Competing Risks Data \u0007Jing Yang and Limin Peng . Emory University 10:40 AM Floor Discussion. Session 56: Recent Developments in Statistical Learning Methods (Invited ) Room: 300, level 3 Organizer: Xingye Qiao, Binghamton University. Chair: Ganggang Xu, Binghamton University. AM Multiclass Sparse Mai1, Yi Yang2and Hui Zou2.1Florida State Univer- sity2University of Minnesota 10:25 AM Composite Large Margin Classiers with Latent Subclasses for Heterogeneous Liu2and Michael Kosorok2. 1Vanderbilt University2The University at Chapel Hill 10:50 AM Positive Denite University of Hong Kong3University of Minnesota 11:15 AM Feature Selection Utilizing the Whole Solution Path Yang Liu1and\u0007Peng Wang2.1Bowling Green State Uni- versity2University of Cincinnati 11:40 AM Floor Discussion. Session 57: Recent Developments on High-Dimensional In- ference in Biostatistics (Invited ) Room: of Chair: Yumou Qiu, University of Nebraska-Lincoln. 10:00 AM Proling and Accounting for Heterogeneity in the Analysis of Cancer Sequencing Data Mengjie Chen . The University of North Carolina at Chapel Hill 34j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Scientic Program (\u0007Presenting Author ) Tuesday, June 16. 10:00 AM - 11:40 AM 10:25 AM Optimal Detection of Weak Positive Dependence between Two Mixture Distributions \u0007Sihai Zhao1, Tony Cai2and Hongzhe Li2.1University of Illinois at Urbana-Champaign2University of Pennsylvania 10:50 AM Multiple Testing for Conditional Dependence by Quantile- Table \u0007Jichun Xie1and Ruosha Li2.1Duke University2The Uni- versity of Texas School of Public Health 11:15 AM Spurious Discoveries for High-dimensional Data Jianqing Fan1, Qi-Man Kong 11:40 AM Floor Discussion. Session 61: Design and Analysis Issues in Clinical Trials (Invited ) Room: 376//378, level 3 Organizer: Ye Shen, University of Georgia. Chair: Yichen Qin, University of Cincinnati. 10:00 Proce- dures Zhongqiang Liu1and\u0007Feifang Hu2.1Renmin University of China2The George Washington University 10:25 AM Ecological Momentary Assessment for Measuring Outcome in Clinical Trials Stephen Rathbun . University of Georgia 10:50 AM Robust Yang Li3.1University of Cincinnati2University of Georgia3Renmin University of China 11:15 AM Joint Modeling Tumor Burden and Time to Event Data in Oncology Research India Labs4Renmin University of China 11:40 AM Floor Discussion. Session 66: Recent Advances in Empirical Likelihood Method (Invited ) Room: 306, level 3 Organizers: Fei Tan, Hanxi- ang Peng, Indiana University-Purdue University. Chair: Fei Tan, Indiana University-Purdue University. 10:00 AM Jackknife Empirical Likelihood for U-Statistics with Esti- mated Constraints \u0007Fei Tan and Hanxiang Peng . Indiana University-Purdue University 10:25 AM Jackknife Empirical Likelihood for Order-restricted Statisti- cal Inference with Missing Data \u0007Heng Wang and Ping-Shou Zhong . Michigan State Uni- versity10:50 AM Improving Estimation in Structural Equation Models: An Easy Empirical Likelihood Approach \u0007Shan Wang and Hanxiang Peng . Indiana University- Purdue University 11:15 AM Composite Empirical Likelihood \u0007Nicole Lazar and Adam Jaeger . University of Georgia 11:40 AM Floor Discussion. Session 71: Next Generation Functional Data (Invited ) Room: 386, level 3 Organizer: Jane-Ling Wang, University of Carlifornia, Davis. Chair: Jane-Ling Wang, University of Carlifornia, Davis. 10:00 AM Analysis of Clustered Longitudinal/Functional Data Naisyin Wang . University of Michigan 10:25 AM Functional Data Analysis for Quantifying Brain Connectiv- ity \u0007Hans-Georg Mueller1, Alexander Petersen1and Owen Carmichael2.1University of California, Davis2Louisiana State University 10:50 AM Functional Principal Component Analysis of Spatial- Temporal Point Processes with Applications in Disease Surveillance \u0007Yehua Li1and Yongtao Guan2.1Iowa State University 2University of Miami 11:15 AM Localized Functional Principal Component Analysis \u0007Kehui Chen1and Jing Lei2.1University of Pittsburgh 2Carnegie Mellon University 11:40 AM Floor Discussion. Session 82: The Jiann-Ping Hsu Invited Session on Biostatis- tical and Regulatory Sciences (Invited ) Room: Virginia Dale, level 3 Organizer: Lili Yu, Georgia Southern University. Chair: Lili Yu, Georgia Southern University. 10:00 AM A Generalized Birth and Death Process for Modeling the Fates of Gene Duplication Jing Zhao1, Southern University 10:25 AM A Nonparametric Approach for Partial Areas under the Re- ceiver Operating Characteristic Curve and Ordinal Domi- nance Curve Kun Chicago3Georgia State University 10:50 AM Analysis of Longitudinal Multivariate Outcome Data from Couples Cohort Studies: Application to HPV Transmission Dynamics Xiangrong Kong . Johns Hopkins University 11:15 AM Bayesian Nonlinear Model Selection 11:40 AM Floor Discussion. 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j35Tuesday, June 16. 1:00 PM - 2:40 PM Scientic Program (\u0007Presenting Author ) Session C01: Disease Models, Observational Studies, and High Dimensional Regression (Invited ) Room: ASCSU Senate Chamer, level 2 Organizer: Peng-Liang Zhao, Sano-aventis U.S. LLC.. Chair: Xiaoyu Jia, Boehringer-Ingelheim Pharmaceuticals Inc.. 10:00 AM Evaluate the Most Accurate Animal Model With Application to Pediatric Medulloblastoma \u0007Lan Gao1, Behrouz Shamsaei2and Stan Pounds3. University of Tennessee at Chattanooga3St. Jude Children's Research Hospital 10:15 AM A Generalized Mover-Stayer Model for Disease Progres- sions with Death in Consideration of Age at Entry \u0007Yi-Ran Lin1, Wei-Hsiung Chao2and Chen-Hsin Sinica2National Dong Hwa University3National Taiwan University 10:30 AM Improving Cancer Mortality Rate Estimation Using Population-specic Structure in Direct Age-standardization \u0007Beverly Fu1and Wenjiang Fu2.1Okemos High 2University of AM An Augmented ADMM Algorithm for Linearly Regularized Statistical Estimation Problems Yunzhang Zhu . The Ohio State University 11:00 AM Public Health Impacts Following the World Trade Center At- tacks of September 11th 2001; Statistical analyses of data from residents of lower Manhattan, New York \u0007L. for Toxic Substances and Disease Registry 10:40 AM Estimation of Discrete Survival Function through the Model- ing of Diagnostic Accuracy for Mismeasured Ingelheim Pharmaceuticals Inc. 11:15 AM Floor Discussion. Tuesday, June 16. 1:00 PM - 2:40 PM Session 12: Taiwan National Health Database (Invited ) Room: 310, level 3 Organizer: Kuang-Fu cheng, Taipei Medical University. Chair: Yen-Kuang Lin, Taipei Medical University. 1:00 PM Effective Analysis of Primary Preventive Anti-HBV Medicine to Prevent Hepatitis Reactivation in Cancer Pa- tients Undergoing Chemotherapy Using National Health Insurance Data Base and Cancer Registry Data Ruey-Kuen Hsieh and\u0007Wen-Kuei Chien . Taipei Medical University 1:25 PM A Nationwide Cohort Study of Influenza Vaccine on Stroke Prevention in the Chronic Kidney Disease Population \u0007Chang-I Chen, Wen-Kuei Chien, Yen-Kuang Lin, Ruey- and Hao-Weng Deng . Taipei Medical University1:50 PM Economic Movement and Health: 2:15 PM Floor Discussion. Session 21: The Application of Latent Variable and Mixture Models to the Biological Sciences (Invited ) Room: 312, level 3 Organizers: Youyi Fong, Fred Hutchinson Cancer Research Center; Nathan Vandergrift, Duke University. Chair: Nathan Vandergrift, Duke University. 1:00 PM The Role of Item Response Theory in Assessment and Eval- uation Studies \u0007Li Cai and Lauren Harrell . University of California, Los Angeles 1:25 PM The application of Structural Equation Modeling to Biomarker data \u0007Nathan Vandergrift, Sallie Permar and Barton Haynes . Latent and Observed Variables in Kernel-Penalized Regres- sion Models Timothy Randolph . Fred Hutchinson Cancer Research Cen- ter 2:15 PM A Mixture Model Approach to Estimating a Nonlinear Errors-in-Variables Model for Serial Dilution Assay Youyi Fong . Fred Hutchinson Cancer Research Center 2:40 PM Floor Discussion. Session 29: Machine Learning for Big Data Problems (Invited ) Room: ASCSU Senate Chambers, level 2 Organizer: Peng Huang, Johns Hopkins University. Chair: Hee-Koung Joeng, University of Connecticut. 1:00 PM A Scalable Integrative Model for Heterogeneous Genomic Data Types under Multiple Conditions Mai Shi and\u0007Yingying Wei . The Chinese University of Hong Kong 1:25 PM Greedy Tree Learning of Optimal Personalized Imaging Texture Analysis \u0007Peng Huang, Siva Raman, Linda Chu, Jamie Schroeder, Malcolm Brock, Franco Verde and Elliot Fishman . Johns Hopkins University 2:40 PM Floor Discussion. 36j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Scientic Program (\u0007Presenting Author ) Tuesday, June 16. 1:00 PM - 2:40 PM Session 32: Recent Development on Next Generation Se- quencing Based Data Analysis (Invited ) Room: 322, level 3 Organizers: Hongmei Jiang, Northwestern University; Lingling An, University of Arizona. Chair: Hongmei Jiang, Northwestern University. 1:00 PM Bayesian nonparametric of Texas at Austin4University of California, Santa Cruz 1:25 PM Leveraging in Big Data Analytics Ping Ma . University of Georgia 1:50 PM Investigating Microbial Co-occurrence and Filtration for Accurate Pathogen Iden- tication in Clinical Samples PM Session 34: Recent Advances in Genomics (Invited ) Room: 324, level 3 Organizer: Lynn Kuo, University of Connecticut. Chair: Ziwen Wei, Merck & Co.. 1:00 PM Accounting For Gene Length in RNA-Seq Data \u0007Patrick Harrington and Lynn Kuo . University of Connecti- cut 1:25 PM Integrating Diverse Genomics Data to Infer \u0007Yuping Hongyu Zhao2.1University of Con- necticut2Yale University 1:50 PM Evolution with Drift \u0007Mandev Gill and Marc Suchard . University of California, Los Angeles 2:15 PM Floor Discussion. Session 36: Lifetime Data Analysis (Invited ) Room: 372//374, level 3 Mei-Ling Lee, University of Maryland. Chair: Yichuan Zhao, Gerogia State University. 1:00 PM Statistical Inference on Quantile Residual Life Jong Jeong . University of Pittsburgh 1:25 PM Onset Time of Chronic Pseudomonas Aeruginosa Infection in Cystic Fibrosis PM A Model for Time to Fracture with a Shock Stream Superim- posed on Progressive Degradation: the Study of Osteoporotic Fractures Baltimore County 2:15 PM Explained Variation in Correlated Survival Data Gordon Honerkamp-Smith and\u0007Ronghui Xu . University of California, San Diego 2:40 PM Floor Discussion. Session 38: New Approaches for Analyzing Time Series Data (Invited ) Room: 304, level 3 Organizer: Thomas Lee, University of Carlifornia, Davis. Chair: Raymond Wong, Iowa State University. 1:00 PM Spectral Analysis of Linear Time Series in Moderately High Dimensions Lili Wang1, Alexander Aue2and\u0007Debashis Paul2. 1Zhejiang University2University of Corrected Estimator of Time-average Variance Constant \u0007Chun yip Yau and Kin Wai Chan . The Chinese University of Hong Kong 1:50 PM Floor Discussion. Session 46: Recent Advances in Integrative Analysis of Omics Data (Invited ) Room: 300, level 3 Organizer: Qi Long, Emory University. Chair: Qi Long, Emory University. 1:00 PM A Bayesian Model for the Identication of Differentially Ex- pressed ter 1:25 PM A Bayesian Approach to Biomarker Selection Networks Thierry Kim-Ahn University North Carolina at Chapel Hill2National Institutes of Health 3North Carolina State University 2:15 PM Floor Discussion. Session 51: Recent Developments in Analyzing Censored Sur- vival Data (Invited ) Room: 376//378, level 3 Organizer: Bin Nan, University of Michigan. Chair: Shou-En Lu, Rutgers University. 1:00 PM Stacking Survival Models Debashis Ghosh . University of Colorado at Denver 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j37Tuesday, June 16. 1:00 PM - 2:40 PM Scientic Program (\u0007Presenting Author ) 1:25 PM Estimation of Concordance Probability with Censored Re- gression Models \u0007Zhezhen Jin and Xinhua Liu . Columbia University 1:50 PM Improving Efciency in Biomarker Research Center2Harvard University 2:15 PM Nonparametric Tests of Treatment Effect for a Recurrent Event Process that Terminates Nabihah Tayob1and\u0007Susan Murray2.1M. D. Anderson Cancer Center2University of Michigan 2:40 PM Floor Discussion. Session 53: Innovative Statistical Methods in Genomics and Genetics (Invited ) Room: Virginia Dale, level 3 Organizer: Zhengqing Ouyang, The Jackson Laboratory. Chair: Ming Hu, New York University. 1:00 PM Statistical Analysis of Differential Alternative Splicing using RNA-Seq Data Mingyao Li . University of Pennsylvania 1:25 PM Integrating Auxillary Information in Complex Traits Studies Marc Coram, Sophie Candille and\u0007Hua Tang . Stanford University 1:50 PM Detecting Nonlinear Associations in High-throughput Data with Applications Clustering and Variable Selection \u0007Tianwei Yu and Hesen Peng . Emory University 2:15 PM Hypothesis Test of Mediation Effect in Causal Mediation Model with High-dimensional Mediators \u0007Yen-Tsung Huang and Wen-Chi Pan . Brown University 2:40 PM Floor Discussion. Session 70: Use of Simulation in Drug Development and De- cision Making (Invited ) Room: 382, level 3 Organizer: Fei Wang, Amgen Inc.. Chair: Jack Lee, M. D. Anderson Cancer Center. 1:00 PM Simulations: The Future of Clinical Trial Design Ben Saville . Berry Consultants 1:25 PM Bayesian Application in Optimizing Probability of Study Success with Multiple Endpoints Setting \u0007Grace Li, Honghua Jiang, Shen Lei, Karen Price, Haoda Fu and David Manner . Eli Lilly and Company 1:50 PM Evaluation of Strategies for Designing Phase 2 Dose Finding Studies Cristiana Mayer . Johnson & Johnson 2:15 PM Discussant: Amy Xia, Amgen Inc. 2:40 PM Floor Discussion. Session 73: Non-Parametrics and Semi-Parametrics: New Advances and Applications (Invited ) 306, level 3 Organizer: Lily Wang, Iowa StateUniversity. Chair: Gang Li, Johnson & Johnson.1:00 PM Single-index Function-on-Function Regression \u0007Guanqun State Linear Models Ella Revzin1and\u0007Jing Wang2.1Coyote Logistics 2University of Illinois at Chicago 1:50 PM White Noise Testing and Model Diagnostic Checking for Functional Time Series Xianyang Zhang . University of Missouri-Columbia 2:15 PM Collective Estimation of Multiple Bivariate Density Func- tions with Application to lah University of Science and Technology 2:40 PM Floor Discussion. Session 76: Advances in Statistical Methods of Identifying Subgroup in Clinical Studies (Invited ) Room: Grey Rock, level 2 Organizer: Xiaojing Wang, University of Connecticut. Chair: Yuefeng Lu, Sano-aventis U.S. LLC.. 1:00 PM The Bias Correction in Comparing the Treatment Effect in Different Subgroups of Patients from a Randomized Clinical Trial \u0007Lu Tian1, Fei Jiang2and LJ Wei2.1Stanford University 2Harvard University 1:25 PM A Regression Tree Approach to Identifying Subgroups with Differential Treatment Effects Wei-Yin Loh . University of Wisconsin-Madison 1:50 PM Identifying Subgroups of Enhanced Predictive Accuracy from Longitudinal Biomarker Data Using Tree-based Ap- proaches: Applications to Monitoring Fetal Growth \u0007Jared Foster, Danping Liu, Paul Albert and Aiyi Liu . Na- tional Institutes of Health 2:15 PM A Bayesian Approach For Subgroup Analysis James O. Berger1,\u0007Xiaojing Wang2and Lei Shen3.1Duke University2University of Connecticut3Eli Lilly and Com- pany 2:40 PM Floor Discussion. Session 78: Analysis and Classication of High Dimensional Data (Invited ) Room: 308, level 3 Organizer: Yichao Wu, North Carolina State University. Chair: Fangfang Wang, University of Illinois at Chicago. 1:00 PM Neyman-Pearson Classication under High-dimensional Settings Anqi Zhao, Yang Feng, Lie Wang and\u0007Xin Tong . University of Southern California 1:25 PM Index Models for Functional Data \u0007Peter Radchenko, Xinghao Qiao and Southern California 38j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Scientic Program (\u0007Presenting Author ) Wednesday, June 17. 8:40 AM - 10:20 AM 1:50 PM Stabilized Nearest Neighbor Classier and Its Theoretical Properties Wei Sun1,\u0007Xingye Qiao2and Guang Cheng1.1Purdue University2Binghamton University 2:15 PM Floor Discussion. Session 92: Issues in Probabilistic Models for Random Graphs (Invited ) Room: 386, level 3 Organizers: Mark Kaiser, Iowa State University; Zhengyuan Zhu, Iowa State University. Chair: Zhengyuan Zhu, Iowa State University. 1:00 PM Exponential-family Random Hypergraph Models for Group Relations PM Local De- pendence Michael Schweinberger . Rice University 1:50 PM Discussant: Mark Kaiser, Iowa State University 2:40 PM Floor Discussion. Tuesday, June 16. 3:00 PM - 4:00 PM Leadership Forum (Plenary Panel ) Room: Grand Ballroom A/B, Level 2 Organizers: Executive Committee of the 2015 Joint ICSA/Graybill Conference. Chair: Wei Shen, Eli Lilly and Company. Panelists: Gregory Campbell, U.S. Food and Drug Administration Xiao-Li Meng, Harvard University Janet Wittes, Statistics Collaborative Inc. Wednesday, June 17. 8:40 AM - 10:20 AM Session 33: Challenges of Quantile Regression in High- Dimensional Data Analysis: Theory and Applications (Invited ) Room: 300, level 3 Organizer: Linglong Kong, University of Chair: Shengchun Kong, Purdue University. 8:40 AM Regularized of Wisconsin-Madison4Memorial Sloan-Kettering Cancer Center9:05 AM Michigan 9:30 AM Focused Information Criterion and Model Averaging Based on Weighted Composite Quantile Regression \u0007Ganggang Regression via Dirichlet Process Mixture of Logistic Distributions \u0007Chao Chang and Nan Lin . Washington University in St. Louis 10:20 AM Floor Discussion. Session 39: Statistica Sinica Special Invited Session on Spa- tial and Temporal Data Analysis (Invited ) Room: 382, level 3 Organizer: Bo Li, University of Illinois at Urbana-Champaign. Chair: Scott Holan, University of Missouri-Columbia. 8:40 AM Likelihood Approximations for Big Nonstationary Spatial Temporal Lattice Data \u0007Joseph Guinness and Montserrat Fuentes . North Carolina State University 9:05 AM A Multivariate Gaussian Process Factor Model for Hand Shape During Reach-to-Grasp Lucia Castellanos1,\u0007Vincent Vu2, Sagi Perel1, Mellon University State University3University of Pittsburgh 9:30 AM A Covariance Parameter Estimation Method for Polar- Orbiting Satellite Data \u0007Michael Horrell and Michael Stein . University of Chicago 9:55 AM Bayesian Analysis AM Floor Analy- Xiaohua Sheng, U.S.. 8:40 AM Design and Analysis of Multiregional Clinical Trials in Eval- uation of Medical Devices: A Two-component Bayesian Ap- proach for Targeted Regulatory Decision Making \u0007Yunling Xu and Nelson Lu . U.S. Food and Drug Adminis- tration 9:05 AM Assessing Benet and Consistency of Treatment Effect under a Discrete Random Effects Model in Multiregional Clinical Trials \u0007Hsiao-Hui Tsing Hua University 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j39Wednesday, June 17. 8:40 AM - 10:20 AM Scientic Program (\u0007Presenting Author ) 9:30 AM Multi-Regional Clinical Trials - Where We Have Been and Where We Are Going Bruce Binkowitz . Merck & Co. 9:55 AM Discussant: Chin-Fu Hsiao, National Health Research Insti- tutes, Taiwan 10:20 AM Floor Discussion. Session 58: Blinded and Unblinded Evaluation of Aggregate Safety Data during Clinical Development (Invited ) Room: 310, level 3 Organizers: Greg Ball, AbbVie Inc.; Richard Entsuah, Merck & Co.. Chair: Richard Entsuah, Merck & Co.. 8:40 AM Continuous Safety Signal Monitoring with Blinded Data \u0007Greg Ball1and William Wang2.1AbbVie Inc.2Merck & Co. 9:05 AM How Should the Final Rule Affect DMCs? Janet Wittes . Statistics Collaborative 9:30 AM Implementation of the Investigational New Drug Safety Re- porting Requirements \"Final Rule\" Brenda Crowe . Eli Lilly and Company 9:55 AM Floor Discussion. Session 65: New Strategies to Identify Disease Associated Ge- nomic Biomarkers (Invited ) Room: 304, level 3 Organizer: Wei Sun, The University of North Carolina at Chapel Hill. Chair: Wei Sun, The University of North Carolina at Chapel Hill. 8:40 AM Discovering Disease Associated Molecular Interactions Us- ing Discordant Correlation Charlotte Siska and\u0007Katerina Kechris . University of Col- orado at Denver 9:05 AM Joint Analysis of Genomic Data from Different Sources us- ing Kernel Machine Regression with Multiple Kernels \u0007Michael Wu and Ni Zhao . Fred Hutchinson Cancer Re- search Center 9:30 AM Transformed Low-rank ANOV A Models for High Dimen- AM Proper Use of Allele-Specic Expression Improves Statisti- cal Power for cis-eQTL Car- olina at Chapel Hill3North Carolina State University 10:20 AM Floor Discussion. Session 84: Design More Efcient Adaptive Clinical Trials Using Biomarkers (Invited ) Room: 322, level 3 Organizer: Ying Yuan, M. D. Anderson Cancer Center. Chair: Yong Zang, Florida Atlantic University.8:40 AM Sequential Designs for Individualized Dosing in Phase I Cancer Clinical Trials \u0007Xuezhou Mao1and Ying Cheung2.1Sano-aventis U.S. Free Biomarker Designs for Randomized Trials with Adaptive Enrichment Noah Simon . University of Washington 9:30 AM Bayesian Predictive Modeling for Personalized Treatment Selection in Oncology Junsheng Ma, Francesco Stingo and\u0007Brian Hobbs Cancer Center 9:55 AM Optimal Marker-Adaptive Designs for Targeted Therapy Yong Zang1, Suyu Liu2and\u0007Ying Yuan2.1Florida Atlantic University2M. Anderson Cancer Center 10:20 AM Floor Discussion. Session 87: Advanced Methods for Graphical Models (Invited ) Room: 386, level 3 Organizer: Yuping Zhang, University of Connecticut. Chair: Jun Li, University of Notre Dame. 8:40 AM Learning Causal Networks Genome-wide Infer- ence of RNA Structure Zhengqing Ouyang . The Jackson Laboratory 9:30 AM Distance Shrinkage and Euclidean Embedding via Regular- ized Kernel Estimation Ming Yuan . University of Wisconsin-Madison 9:55 AM Detecting Overlapping Communities in Networks with Spec- tral Methods Yuan Zhang, Elizaveta Levina and\u0007Ji Zhu . University of Michigan 10:20 AM Floor Discussion. Session C03: Functional Data, Semi-parametric and Non- parametric Methods Chair: Xin Wang, AbbVie Inc.. 8:40 AM An Unbiased Measure of Integrated V olatility in the Fre- quency Domain Fangfang Wang . University of Illinois at Chicago 8:55 AM Functional data analysis for density functions by transforma- tion to a Hilbert space \u0007Alexander Petersen and Hans-Georg \u00a8uller . University of California, Davis 9:10 AM Cross-covariance Functions for Divergence-free and Curl- free Fan1and Tomoko Matsuo2.1University of Califor- nia, AM Empirical Likelihood-based Inference for Linear Compo- nents in Su . Montclair State University 40j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Scientic Program (\u0007Presenting Author ) Wednesday, June 17. 10:40 AM - 12:20 PM 9:40 AM Consistency of Bayesian Semiparametric Models through Joint Density Estimation Yuefeng Wu . University of Missouri-St. Louis 9: 55 AM Analysis of Water Quality in New Jersey \u0007Kaitlyn Scrudato and Haiyan Su . Montclair State Univer- sity 10:10 AM Floor Discussion. Wednesday, June 17. 10:40 AM - 12:20 PM Session 6: Recent Advances in Analyzing Genomic Data (Invited ) Room: 300, level 3 Organizer: Hao Chen, University of California, Davis. Chair: Hua Tang, Stanford University. 10:40 AM Davis2Stanford University3University of Pennsylvania 11:05 AM A Statistical Approach to Prioritizing GWAS Results by In- tegrating of Carolina 2Hong Kong Baptist University3Yale University 11:30 AM GLAD: A Mixed-membership Model for Berkeley3University of Massachusetts 11:55 AM Learning Genetic Regulatory Networks Using RNA-seq data \u0007Jie Peng1and Ru Wang2.1University of California, Davis 2Guidewire 12:20 PM Floor Discussion. Session 11: Emerging Issues in Time-to-Event Data (Invited ) Room: 304, level 3 Organizer: Qingxia Chen, Vanderbilt University. Chair: Yu Cheng, University of Pittsburgh. 10:40 AM Bayesian Path Specic Frailty Models for de AM Quantile Cheng2, Qingxia Chen3and Jason Fine4.1The University Health 2University of Pittsburgh3Vanderbilt University4The Uni- versity of North Carolina at Chapel Hill 11:30 AM Robust Estimation for Clustered Failure Time Data: AM Floor Discussion. Session 18: Statistical Methods for Sequencing Data Analysis (Invited ) Room: 308, level 3 Organizer: Yanming Di, Oregon State University. Chair: Yanming Di, Oregon State University. 10:40 AM Unit-free and Robust Detection of Differential Expression from RNA-Seq Data Hui Jiang . University of Michigan 11:05 AM Robust Estimation of Isoform Expression with RNA-Seq Data \u0007Jun Li1and Hui Jiang2.1University of Notre Dame 2University of Michigan 11:30 AM Genetic Association Testing for Binary Traits in the Presence of Population Structure \u0007Duo Jiang1, Sheng Zhong2and Mary sara Mcpeek2. 1Oregon State University2University of Chicago 11:55 AM Identication of Stably Expressed Genes from Arabidopsis RNA-Seq Data Bin Zhuo,\u0007Yanming Di, Sarah Emerson and Jeff Chang . Oregon State University 12:20 PM Floor Discussion. Session 24: Recent Developments in Missing Data Analysis (Invited ) Room: 386, level 3 Organizer: Peisong Han, University of Waterloo. Chair: Peisong Han, University of Waterloo. 10:40 AM Variable Selection in the Presence of Missing Data: pling and Using Link-Preserving Imputation for Logistic Einstein College of Medicine 11:30 AM Composite Likelihood Approach in Gaussian Copula Re- gression Models with Missing Data \u0007Wei Ding and Peter Song . University of Michigan 11:55 AM Test the Reliability of Doubly Robust Estimation with Miss- ing Response Data \u0007Baojiang Chen1and Jing Qin2.1University of Nebraska Medical Center2National Institutes of Health 12:20 PM Floor Discussion. Session 27: Bayesian Applications in Biomedical Studies (Invited ) Room: 310, level 3 Organizer: Xiaowen Hu, Colorado State University. Chair: Xiaowen Hu, Colorado State University. 10:40 AM Bayesian Analysis \u0007Jing Cao1and Song Zhang2.1Southern Methodist Univer- sity2The University of Texas Southwestern Medical Center 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j41Wednesday, June 17. 10:40 AM - 12:20 PM (\u0007Presenting Author ) 11:05 AM \u0007Song Yin Xi2.1The University of Texas South- western Medical Center2Southern Methodist University 11:30 AM Adjusting for Heterogeneity in Infectivity in HIV Prevention Clinical Trials \u0007Jingyang Zhang1and Elizabeth Brown1;2.1Fred Hutchin- son Cancer 31: for Early-Phase Oncology Clin- ical Trials AM A Curve-free Bayesian Decision-theoretic Design for Two- agent Phase I Trials Bee Leng Lee1,\u0007Shenghua Fan2and Ying Lu3.1San Jose State University2California State University, East Bay 3Stanford University 11:30 AM Bayesian Dose-nding Designs for Combination of Molecu- larly Targeted Agents Assuming Partial Stochastic Ordering Beibei Guo1and\u0007Yisheng Li2.1Louisiana State University 2M. D. Anderson Cancer Center 11:55 AM Floor Discussion. Session 45: Advances and Case Studies for Multiplicity Issues in Clinical Trials (Invited ) Room: 322, level 3 Organizer: Guanghan Frank Liu, Merck & Co.. Chair: Meihua Wang, Merck & Co.. 10:40 AM Condence Intervals for Multiple Comparisons Procedures Brian Wiens . Portola Pharmaceuticals 11:05 AM Li1and Jin Xu2.1Pzer Inc.2Merck & Co. 11:30 AM Multiplicity Adjustment in Vaccine Efcacy Trial with Adaptive Population-Enrichment Design Shu-Chih Su . Merck & Co. 11:55 AM Discussant: Lei Shen, Eli Lilly and Company 12:20 PM Floor Discussion.Session 59: Design and Analysis of Non-Inferiority Clinical Trials (Invited ) Room: 324, level 3 Organizers: Yongzhao Shao, New York University; Ming Zhou, Bristol-Myers Squibb Compnay. Chair: Zhezhen Jin, Columbia University. 10:40 AM Some Comments on the Three-Arm Non-inferiority Trial Design Zhou Non-inferiority Prognostic Models \u0007Ning Xu and Yongzhao Shao . New York University 11:30 AM Discussant: Ming Hu, New York University 11:55 AM Floor Discussion. Session 62: Statistical Challenges in Economic Research In- volving Medical Costs (Invited ) Room: 372//374, level 3 Organizers: Yu Shen, M. D. Anderson Cancer Center; Ya-Chen Tina Shih, M. D. Anderson Cancer Center. Chair: Jing Ning, M. D. Anderson Cancer Center. 10:40 AM Projecting Survival and Lifetime Costs from Short-Term Smoking Cessation Trials Daniel Heitjan1;2.1Southern Methodist University2The University of Texas Southwestern Medical Center 11:05 AM A Flexible Model for Correlated Medical Costs, with Appli- cation to Medical Expenditure Data Tina Shih3, Chicago 2Northwestern University3M. D. Anderson Cancer Center 4North Carolina State University 11:30 AM A Bivariate Copula Random-Effects Model for Length of Stay Zhehui Luo2and\u0007Joseph Nonparametric Inference for the Joint Distribution of Recur- rent Marked Variables and Recurrent Survival Time Laura Yee and\u0007Gary Chan . University of Washington 12:20 PM Floor Discussion. Session 83: Dose Response/Finding Studies in Drug Develop- ment (Invited ) Room: Grey Rock, level 2 Organizer: Guojun Yuan, Cubist Pharmaceuticals Inc.. Chair: Naitee Ting, Boehringer-Ingelheim Pharmaceuticals Inc.. 10:40 AM Calibration of Continual Reassessment Method \u0007Xiaoyu Lee2and Ken Cheung2.1Boehringer- Ingelheim Pharmaceuticals Inc.2Columbia University 11:05 AM A Practical Application with Interim Analysis in a Dose Ranging Design Xin Wang . AbbVie Inc. 11:30 AM Design Considerations in Dose Finding Studies Xin Zhao . Johnson & Johnson 11:55 AM Dose Response Relationship in a Phase 1b Dose Ranging Study in Subjects with Chronic Hepatitis C Virus Infection Di An . Gilead Sciences, Inc. 42j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Scientic Program (\u0007Presenting Author ) Session name 12:20 PM Floor Discussion. Session 88: Advanced Development in Big Data Analytics Tools (Invited ) Room: 382, level 3 Organizer: Yuping Zhang, University of Connecticut. Chair: Fenghai Duan, Brown University. 10:40 AM Clique-based Method for Social Network Clustering \u0007Dipak Dey and Guang Ouyang . University of Connecticut 11:05 AM Sparse Partially Linear Propagating Probabilities Between Data Points \u0007Guojun Gan, Yuping Zhang and Dipak Dey . University of PM Floor Discussion. Session 91: Recent Developments of High-Dimensional Data Inference and Its Applications (Invited ) Room: 306, level 3 Organizer: Wen Zhou, Colorado State University. Chair: Wen Zhou, Colorado State University. 10:40 AM Segmenting Multiple Time Series by Contemporaneous Lin- ear Transformation: PCA for Minnesota11:30 AM Thresholding Tests for Signal Detection on High- Dimensional Count Distributions \u0007Yumou Qiu1, Songxi Chen2and Dan Nettleton2. 1University of Nebraska-Lincoln2Iowa State University 11:55 AM Projected Principal Component Analysis in Factor Models Jianqing Fan1, Yuan Liao2and\u0007Weichen Wang1. 1Princeton University2University of Maryland 12:20 Floor Discussion. Session C04: Multiple Comparisons, Meta-analysis, and Mis- measured Outcome Data (Contributed ) Room: Virginia Dale, level 3 Organizer: Peng-Liang Zhao, Sano-aventis U.S. LLC.. Chair: Ye Shen, University of Georgia. 10:55 AM Generalized Holm's Procedure for Multiple Testing Problem Huajiang Yi Ma2and\u0007Hong INC. 2Quintiles, Inc.3Arkansas State University 11:10 AM Generalized Condence Interval Approach for Combining Multiple Comparisons \u0007Atiar Rahman and Ram Tiwari . U.S. Food and Drug Ad- ministration 11:25 AM Considerations for Two Correlated Cochran-Armitage Trend Tests \u0007Yihan Li, Su Chen, Ying Zhang and Yijie Zhou . 11:55 AM Pitfalls in Assessing Relative Efcacy Across Trials Xiao Sun . Merck & Co. 12:10 PM Floor Discussion. 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j43Abstracts Abstracts Session 1: Best Practices for Delivery of Adaptive Clinical Trials Illustrated with Case Studies DIA Adaptive Design Scientic Working Group Best Practices Team: Objectives and Case Studies Eva Miller inVentiv Health Clinical eva.miller@inventivhealth.com Members of the DIA ADSWG BP team are senior statisticians from the FDA and industry who endeavor to identify gaps in implemen- tation of adaptive trials, and promote the broader use of adaptive design by applying best practices. As we learned from the 2012 AD Survey by Morgan, et.al and surveys undertaken by CBER and CDHR, Adaptive Designs have not grown to be as large a proportion of total clinical trials undertaken as advocates would have hoped. Barriers to adoption include concerns about regulatory acceptance, the time involved in planning, simulations, communications and teamwork, extra considerations for DMCs, and availability of ap- propriate software for planning, simulations, and implementation. In this talk, we summarize the committee's objectives and current practices and evolving issues related to the use of adaptive trials, and consider what went well and what we learned from a number of case studies: the LOTS trial, Raptor's PRODYSBI, Merck Phase II/III vaccine trial, the INHANCE trial, a population enrichment de- sign trial, a phase II/III seamless adaptive conrmatory trial with interim treatment selection and the A-HeFT Trial. ADVENT and V ALOR, while part of this committee's review effort will be dis- cussed in detail by other speakers. The DIA ADSWG BP team has as its objectives to: 1. Understand AD trials that are categorized as \"Less well- understood\", with regard to specic features or aspects of those tri- als that may have led them to be categorized in that way. 2. Review these features in regard to design and/or the end-to- end trial execution processes, and identify areas where suggestions and/or improvements may help in their acceptance. 3. Acknowledging the importance of planning and simulations, as best practices for ADs, the Subteam also reviews available software for AD simulations and implementation. Features of software con- sidered in our review include: levels of user friendliness, ease of use, required user level of statistical and software usage experience, documentation, and validation. An Adaptive Phase 3 Trial Resulting in FDA Approval Secretory diarrhea in HIV positive patients remains a serious un- met clinical need, even and especially in the age of highly active anti-retroviral therapy (HAART). In 2013 Crofelemer was approved by the FDA as a rst-in-class anti-diarrheal agent indicated for the symptomatic relief of non-infectious diarrhea in adult HIV patients on anti-retroviral therapy (ART). The safety and efcacy of crofele- mer were established through ADVENT, an innovative two-stage, seamless adaptive clinical trial with dose selection at the end ofstage 1. In this talk we will highlight the clinical, statistical, reg- ulatory and operational challenges of this adequate and well con- trolled trial. Its successful implementation reflects the high degree of precision with which the trial was planned and executed. To our knowledge this is the rst example of an adaptive conrmatory trial with dose selection that has proceeded all the way to NDA submis- sion and approval. Promising Zone Design; Methodology, Strategy, tation Zoran Antonijevic Cytel Inc. been catego- rized as a \"less-well understood\" type of adaptive design in the CDER/CBER draft guidance on adaptive design. There has been, however, an increase in application of this type of design since the publication of this guidance. This design features a so-called promising zone at interim analysis. If data fall into this zone that means that study is trending towards a positive efcacy, but the cur- rent sample size is not large enough to assure success with sufcient probability. In this case the study design allows an option for a one- time increase in sample size based on pre-specied criteria. In this session we will present a case study of a trial using the uSSR with a promising zone. The presentation will address statistical method- ology, clinical trial design, regulatory interactions and operational challenges. Session 2: Chemistry, Manufacturing, and Controls (CMC) in Pharmaceuticals: Current Statistical Chal- lenges I Statistical Methods for Analytical Comparability Leslie Sidor Amgen Inc. leslie.sidor@gmail.com In all manufacturing settings, there is an inherent drive to improve product through the reduction in process variation, implement- ing new technology, increasing efciency, optimizing resources, and improving customer experience through innovation. In the pharmaceutical industry, these improvements come with added re- sponsibility to the patient such that product made under the post- improvement or post-change condition maintains the safety and ef- cacy of the pre-change product. Regulatory agencies also recognize the importance in providing manufacturers the flexibility to improve their manufacturing processes and it is acknowledged that the extent and rigor of the evaluation should be appropriately adjusted based on the magnitude of the change. To assess analytical comparability, there are a number of approaches that may be used and are the focus of this presentation. How Type I Error Impacts Quality System Effectiveness Jeff Gardner DataPharm Statistical & Data Management Services jgardner@data-pharm.com Univariate Shewhart control charts enjoy widespread use among pharmaceutical and biotechnology manufacturers, particularly in 44j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts monitoring product quality at the end of the production work stream. A seldom-discussed aspect of using univariate Shewhart charts in this way is the propagation of Type I error when multiple charts are being used to monitor multiple product quality attributes. De- spite the increased risk of false signals, manufacturers integrate the use of univariate Shewhart charts into their quality systems in such a way as to initiate the quality investigation process whenever an out-of-control signal is observed. By doing so, companies inher- ently link the effectiveness of their quality systems to the overall Type I error rate for their monitoring programs and suffer signicant consequences as a result. This presentation discusses the nature of these consequences and their organizational impact. The presenta- tion also presents strategies for reducing overall Type I error and makes a case for these strategies as the best means of achieving maximum quality investigation and CAPA effectiveness. Alternative Procedures for Shelf Life Estimation Utilizing Mixed Models \u0007Michelle Quinlan1, Walt Stroup2and Dave Christopher3 Nebraska-Lincoln 3Merck & Co. mquinlan22@yahoo.com Shelf life is the length of time a product is expected to remain within the established acceptance criteria. The traditional metric for as- sessing shelf life is the regression line. The criterion for determining shelf life involves insuring that all future batches meet or exceed the established shelf life of the product with an acceptably high proba- bility. Given this criterion which involves prediction, batch effects are most appropriately modeled as random. Two new procedures have been developed which employ a model where batch effects are considered random: (1) utilizing batch specic Best Linear Unbi- ased Prediction (BLUP) and focusing on the batch with the shortest shelf life; and (2) utilizing BLUP combined with quantile regression techniques. Both procedures seek to estimate a suitably small quan- tile of the distribution of batch shelf lives while accounting for the inherent variability among batch shelf lives. The methodology and implementation of these procedures for shelf life estimation are dis- cussed and the results are compared to previously developed shelf life estimation procedures. Session 3: Chemistry, Manufacturing, and Controls (CMC) in Pharmaceuticals: Current Statistical Chal- lenges II Statistical Methods for Analytical Validation of Accuracy and Precision Richard Burdick Amgen Inc. rburdick@amgen.com A recently proposed USP general information chapter numbered h1210iwas proposed as a companion chapter to h1225iValidation of Compendial Procedures. The purpose of h1210iis to provide statistical methods that can be used in the validation of analyti- cal methods. In this talk, we review some of these best practices for demonstration of accuracy and precision for analytical methods. Examples are provided for experimental designs that are typically employed in method validation experiments. The importance of pre-validation work is highlighted as well as the need to perform a formal statistical test of hypotheses in order to demonstrate that an analytical procedure is t for use. Approaches are presented forindividual validation of accuracy and precision, as well as a simul- taneous validation of these two properties. Statistical Applications for Biosimilar Product Development Richard Montes Hospira, Inc. richard.montes@hospira.com The approval of the rst biosimilar by the FDA in March 2015 is an historic event for the U.S. healthcare system. The inevitable ad- vent of biosimilars in the U.S. brings challenges and opportunities in statistical applications. Demonstration of analytical biosimilar- ity between the biosimilar product and the reference product is the primary new challenge especially amidst the evolving regulatory landscape. The 2012 FDA draft biosimilars guidance documents present a stepwise approach starting with structural and functional characterization of the biosimilar and reference products. Recent FDA feedback includes recommendations for a tiered statistical ap- proach - equivalence testing for Tier 1, quality range for Tier 2, and graphical comparison for Tier 3. The demonstration of analyt- ical biosimilarity through comparative analytical testing and robust statistical analysis is an important component of the assessment of totality of evidence to support biosimilar product approval. In addi- tion to the analytical biosimilarity assessment, the standard CMC re- quirements for new biologic product licensing applications are also needed for biosimilar product applications. Requirements such as stability analysis for shelf-life estimation and specication setting rely heavily on statistical methodologies. As a consequence of the early characterization work requisite for the analytical biosimilarity assessment, the available analytical data is considerably larger than the minimum of 3 stability lots required for a new biologic. The larger body of data in biosimilar product offers the statistician more options to utilize statistical methodologies that results in improved statistical inference. This presentation will cover two main areas. First, the application of the FDA tiered statistical approach for evaluation of analytical biosimilarity data will be illustrated. Second, a statistical method for setting release and shelf-life specication limits taking into ac- count the random lot effects will be explored. Specically, a hier- archical (multilevel) regression modeling will be used to estimate xed and random effects parameters which are then used in simula- tion to quantify the limits. The simulation will be compared against other ad hoc industry methods to calculate specication limits. How to Set Up Biosimilarity Bounds in Biosimilar Product De- velopment \u0007Lanju Zhang AbbVie Inc. lanju.zhang@abbvie.com Abstract is Pending. Session 4: New Techniques for Functional and Longitu- dinal Data Analysis Variable Selection Methods for Functional Regression Models Nedret Billor Auburn University billone@auburn.edu In the last fteen years, a substantial amount of attention has been drawn to the eld of functional data analysis. While the study of the probabilistic tools for innite dimensional variables started in the beginning of the 20th century, the development of statistical mod- els and methods for functional data has only really been developed 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j45Abstracts in the last two decades since many scientic elds involving ap- plied statistics have started measuring and recording massive con- tinuous data due to rapid technological advancements. Functional linear regression is widely used technique to explore the relation- ship between a scalar response and functional predictors, where a large number of functional predictors is collected on few subjects are becoming increasingly common. In order to select important functional predictors among many that are actually useful in pre- dicting the response, variable selection methods are utilized. In this talk we review variable selection methods based on L1 regulariza- tion where selection of the important functional predictors and es- timation of the corresponding regression coefcient functions will be done simultaneously. Further, the effect of outliers in variable selection procedures for functional regression models are discussed and some new developments in variable selection methods which are resistant to outliers are proposed. Structured Functional Principal Component Analysis in Multi- level Functional Mixed Models for Physical Activity 3Ludwig-Maximilians-Universit at Munchen hshou@mail.med.upenn.edu Novel measures for biomedical research in the past decade are col- lected in many modern observational studies to understand normal and abnormal mental status. Physical activity data using wearable computing sensors, for example, link the direct phenotype of hu- man behaviors with disease symptoms and diagnosis. These data, objectively assessed and are often recorded with higher sampling frequency that capture real-time events, have also brought many challenges for statistical modeling and analysis. More specically, the challenges include: 1) the fundamental observational unit is of- ten a function of time or space that can be of high dimension; 2) there are underlying correlation structures induced by the sampling design; and 3) the measures could be noisy depending on the tech- niques, while individual levels of variability are of interest. To ac- count for the complexity, we propose a principal component analy- sis based approach that accounts for the natural inheritance of cor- relation structures from the sampling design. This is achieved by introducing latent processes that capture explicit levels of variabil- ity using the same concept from standard mixed effects models, but by replacing random effects with random processes. Our method decomposes multilevel variations in functional models and achieves feature extraction via level-specic spectral decomposition of latent process covariance operators. A computationally fast and scalable estimation procedure through rank preserved projection is devel- oped for high-dimensional data. We illustrate the approach with accelerometry data from National Institute of Mental Health family studies of affective spectrum disorders. Similar modeling approach can also be used to correct measurement errors and improve relia- bility in the functional objects. Exploration of Diurnal Patterns in Maize Leaf with RNA- sequencing Data Wen Liu2, Lin University 2Iowa University 3Monsanto company 4Donald Danforth Plant Science Center PLIU@iastate.eduTo study the diurnal patterns of gene expression proles along maize leaf development, a set of RNA-sequencing experiments were done over 24 hours with samples taken every two hours from four differ- ent sections of maize leaves. We explore the diurnal patterns using a semi-parametric method through smoothing splines and a flexible mathematical model. Interesting features of the diurnal patterns are described through geometric properties of the mathematical model. In this talk, we will provide some preliminary results of our analy- sis. Partial and Tensor Quantile Regressions in Functional Data Analysis \u0007Dengdeng Yu, Linglong Kong and Ivan Mizera University of Alberta dengdeng@ualberta.ca In functional linear quantile regression model, we are interested in how to effectively and efciently extract the bases for estimating functional coefcients. Therefore, we propose a prediction proce- dure using partial quantile covariance techniques to extract the func- tional bases effectively by sequentially maximizing the partial quan- tile covariance between the response and projections of functional covariates. Moreover, we develop an efcient algorithm for the pro- cedure. Under the homoscedasticity assumption, we further extend our method to functional composite quantile regression by using the composite quantile covariance, and obtain the corresponding algo- rithm. In functional linear quantile regression model, the functional coefcients may have multidimensional structure. To make efcient predictions without losing the structure information, we also pro- pose a prediction procedure using tensor linear quantile regression. In addition, simulations and real data are studied to show the supe- riority of our proposed methods. This is a joint work with Linglong Kong and Ivan Mizera at University of Alberta. Session 5: Recent Advancements in Statistical Machine Learning Sparse CCA: Minimax Rates and Adaptive Estimation \u0007Chao Gao1, 1Yale University 2University of Pennsylvania chao.gao@yale.edu Canonical correlation analysis (CCA) is a classical and important multivariate technique for exploring the relationship between two sets of variables. It has applications in many elds including ge- nomics and imaging, to extract meaningful features as well as to use the features for subsequent analysis. This talk presents the minimax rates and adaptive estimation of leading sparse canonical directions when the ambient dimensions are high. We show that the mini- max rates do not depend on the marginal covariance matrices. The optimal rates can be achieved by a two-stage convex programming procedure. Asymptotic Normality in Estimation of Models Cun-Hui 2Rutgers University 3Yale University zren@pitt.edu The high dimensional graphical model, a powerful tool for study- ing conditional dependency relationship of random variables, has 46j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts attracted great attention in recent years. This paper investigates sta- tistical inference of each edge for some families of large graphical models which include Ising graphical model and Gaussian graphical model as two special cases. Unlike the Gaussian graphical model, in general there is no explicit correspondence between the structure of the graph and precision matrix of the underlying data. Hence this in- ference problem is different from inference of precision matrix and is very challenging. In this paper, we propose a novel estimator of each edge and show that, under certain sparsity assumption, our esti- mator is asymptotically normal and has parametric square-root rate in a large graphical model. Our proof applies a linearization idea and a novel projection procedure which is motivated by statistical inference in high dimensional regression. A careful analysis of this new methodology relaxes the commonly imposed sparsity assump- tion, uniform signal strength condition, bounded maximum neigh- borhood weight and incoherence condition in the literature of large Ising graphical model. This is a joint work with Cun-Hui Zhang and Harrison Zhou. Optimal Tests of Independence with Applications to Testing More Structures \u0007Fang Han1and Han Liu2 1Johns Hopkins University 2Princeton University fhan@jhu.edu We consider the problem of testing mutual independence of all en- tries in a d-dimensional random vector based on n independent ob- servations with d possibly larger than n. For this, we consider two families of distribution-free test statistics that converge weakly to an extreme value type I distribution. We further study the powers of the corresponding tests against the sparse alternative and justify certain optimality. As important examples, we show that the tests based on Kendall's tau and Spearman's rho are rate optimal tests of independence. For further generalization, we consider accelerating the rate of convergence via approximating the exact distributions of the test statistics. We also study the tests of two more structural hypotheses: m-dependence and data homogeneity. For these, we propose two rank-based tests and show their optimality. Bootstrap Tests on High Dimensional Covariance Matrices Melbourne riczw@stat.colostate.edu Recent advancements in genomic study and clinical research have drew growing attention to understanding how relationships among genes, such as dependencies or co-regulations, vary between differ- ent biological states. Complex and unknown dependency among genes, along with the large number of measurements, imposes methodological challenge in studying genes relationships between different states. Starting from an interrelated problem, we propose a bootstrap procedure for testing the equality of two unspecied covariance matrices in high dimensions, which turns out to be an important tool in understanding the change of gene relationships between states. The two-sample bootstrap test takes maximum ad- vantage of the dependence structures given the data, and gives rise to powerful tests with desirable size in nite samples. The theo- retical and numerical studies show that the bootstrap test is pow- erful against sparse alternatives and more importantly, it is robust against highly correlated and nonparametric sampling distributions. Encouraged by the wide applicability of the proposed bootstrap test,we design a gene clustering algorithm to understand gene clustering structures. We apply the bootstrap test and gene clustering algo- rithm to the analysis of a human asthma dataset, for which some interesting biological implications are discussed. Session 6: Recent Advances in Analyzing Genomic Data 3University of Pennsylvania hxchen@ucdavis.edu The progression and clonal development of tumors often involve amplications and deletions of genomic DNA. Estimation of allele- specic copy number, which quanties the number of copies of each allele at each variant loci rather than the total number of chromo- some copies, is an important step in giving a more complete por- trait of tumor genomes and the inference of their clonal history. We propose a novel method, falcon, for nding somatic allele-specic copy number changes by next generation sequencing of tumors with matched normal. Falcon is based on a change-point model on a bi- variate mixed Binomial process, which explicitly models the copy numbers of the two chromosome haplotypes and corrects for local allele-specic coverage biases. By using the Binomial distribution rather than a normal approximation, falcon more effectively pools evidence from sites with low coverage. We applied this method in the analysis of a pre-malignant colon tumor sample and late-stage colorectal adenocarcinoma from the same individual. The allele- specic copy number estimates obtained by falcon allow us to draw detailed conclusions regarding the clonal history of the individual's colon cancer. A Statistical Approach to Prioritizing GWAS Results Inte- Kong Baptist University 3Yale University chungd@musc.edu Results from Genome-Wide Association Studies (GWAS) have shown that complex diseases are often affected by many genetic variants with small or moderate effects. Identication of these risk variants remains a very challenging problem. Hence, there is a need to develop more powerful statistical methods to leverage available information to improve upon traditional approaches that focus on a single GWAS dataset without incorporating additional data. In this presentation, I will discuss our novel statistical approach, GPA (Genetic analysis incorporating Pleiotropy and Annotation), to in- crease statistical power to identify risk variants through joint anal- ysis of multiple GWAS data sets and annotation information. Our approach is motivated by the observations that (1) accumulating ev- idence suggests that different complex diseases share common risk bases, i.e., pleiotropy; and (2) functionally annotated variants have been consistently demonstrated to be enriched among GWAS hits. GPA can integrate multiple GWAS datasets and functional annota- tions to identify association signals, and it can also perform hypoth- esis testing to test the presence of pleiotropy and the enrichment of 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j47Abstracts functional annotation. I will discuss the power of GPA with its ap- plication to real GWAS datasets with various functional annotations and the simulation studies. GLAD: A Mixed-membership Model Berkeley 3University of Massachusetts pflaherty@umass.edu Genomic analyses of many solid cancers have demonstrated exten- sive genetic heterogeneity between as well as within individual tu- mors. However, statistical methods for classifying tumors by sub- type based on genomic biomarkers generally entail an all-or-none decision, which may be misleading for clinical samples contain- ing a mixture of subtypes and/or normal cell contamination. We have developed a mixed-membership classication model, called GLAD, that simultaneously learns a sparse biomarker signature for each subtype as well as a distribution over subtypes for each sam- ple. We demonstrate the accuracy of this model on simulated data, in-vitro mixture experiments, and clinical samples from the Cancer Genome Atlas (TCGA) project. We show that many TCGA samples are likely a mixture of multiple subtypes. Learning Genetic Regulatory Networks Using RNA-seq data \u0007Jie Peng1and Ru Wang2 1University of California, Davis 2Guidewire jiepeng@ucdavis.edu In this talk we will discuss constructing genetic regulatory net- works by graphical models using RNA-seq data. Since RNA-seq are counts data, many commonly used methods for graphical mod- els are not directly applicable as they assume multivariate normal distributions. We will tackle this problem by modeling the joint dis- tribution of RNA-seq counts through a hierarchical model. We will discuss algorithms for tting this model as well as an application to a data set consisting of RNA-seq libraries for 76 introgression lines of tomato. Session 7: Scalable Multivariate Statistical Learning with Massive Data False Discovery Control under Unknown Dependence Jianqing Fan1and\u0007Xu Han2 1Princeton University 2Temple University hanxu3@temple.edu Multiple hypothesis testing is a fundamental problem in high di- mensional inference, with wide applications in many scientic elds. In genome-wide association studies, tens of thousands of hypotheses are tested simultaneously to nd if any genes are asso- ciated with some traits. In practice, these tests are correlated. False discovery control under general covariance dependence is a very challenging and important open problem in the modern research. In this talk, we extend our principal factor approximation approach (PFA) that was designed for the known covariance matrix case to a more general situation where the covariance dependence is un- known. In practice, this unknown dependence has to be estimated rst, and the estimation accuracy can greatly affect the convergence of FDP or even violate its consistency. We will give conditions on the dependence structures and estimation procedures such that theestimate of FDP is consistent. Such dependence structures include sparse covariance matrices and strong dependence matrices, which encompass most practical situations. The nite sample performance of our procedure is critically evaluated by various simulation stud- ies. Our approach is further illustrated by some real data in breast cancer research. Sparse CCA: Adaptive Estimation and Computational Barriers Chao Gao1,\u0007Zongming 1Yale University 2University of Pennsylvania zongming@wharton.upenn.edu Canonical correlation analysis (CCA) is a classical and important multivariate technique for exploring the relationship between two sets of variables. It has applications in many elds including ge- nomics and imaging, to extract meaningful features as well as to use the features for subsequent analysis. This paper considers adaptive and computationally tractable estimation of leading sparse canon- ical directions when the ambient dimensions are high. Three in- trinsically related problems are studied to fully address the topic. First, we establish the minimax rates of the problem under predic- tion loss. Separate minimax rates are obtained for canonical direc- tions of each set of random variables under mild conditions. There is no structural assumption needed on the marginal covariance ma- trices as long as they are well conditioned. Second, we propose a computationally feasible two-stage estimation procedure, which consists of a convex programming based initialization stage and a group-Lasso based renement stage, to attain the minimax rates un- der an additional sample size condition. Finally, we provide evi- dence that the additional sample size condition is essentially nec- essary for any randomized polynomial-time estimator to be consis- tent, assuming hardness of the Planted Clique detection problem. The computational lower bound is faithful to the Gaussian models used in the paper, which is achieved by a novel construction of the reduction scheme and an asymptotic equivalence theory for Gaus- sian discretization that is necessary for computational complexity to be well-dened. As a byproduct, we also obtain computational lower bound for the sparse PCA problem under the Gaussian spiked covariance model. This bridges a gap in the sparse PCA literature. A Class of Accelerated MM Algorithms for Scalable Optimiza- tion Yiyuan She Florida State University yshe@stat.fsu.edu Due to the explosion of large-scale datasets in statistical applica- tions, people often favor rst-order optimization methods to obtain an estimator in complex learning tasks. We study a class of esti- mators following an MM algorithm design by use of Bregman di- vergences and shrinkage functions. Perhaps interestingly, in this high dimensional nonconvex setting, a series of acceleration tech- niques are still effective and result in fast convergence rates. Non- asymptotic results on the statistical accuracy and computational ac- curacy are presented. Innovated Interaction Screening for High-Dimensional Nonlin- ear Classication Yingying Fan,\u0007Yinfei Kong, Daoji Li and Zemin Zheng University of Southern California yinfeiko@usc.edu This paper is concerned with the problems of interaction screening and nonlinear classication in high-dimensional setting. We pro- pose a two-step procedure, IIS-SQDA, where in the rst step an 48j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts innovated interaction screening (IIS) approach based on transform- ing the original p-dimensional feature vector is proposed, and in the second step a sparse quadratic discriminant analysis (SQDA) is pro- posed for further selecting important interactions and main effects and simultaneously conducting classication. Our IIS approach screens important interactions by examining only pfeatures instead of all two-way interactions of order O(p2). Our theory shows that the proposed method enjoys sure screening property in interaction selection in the high-dimensional setting of pgrowing exponentially with the sample size. In the selection and classication step, we es- tablish a sparse inequality on the estimated coefcient vector for QDA and prove that the classication error of our procedure can be upper-bounded by the oracle classication error plus some smaller order term. Extensive simulation studies and real data analysis show that our proposal compares favorably with existing methods in in- teraction selection and high-dimensional classication. Session 8: New Statistical Advance in Genomics and Health Science Applications Linking Lung Airway Structure to Pulmonary Function via Hi- erarchical Feature 3Kansas State University kuniowa@gmail.com The human lung airway is a complex inverted tree-like structure resulting from repeated segmental bifurcations/trifurcations for up to 28 segmental generations starting from the trachea. The airway measurements extracted from CT lung images for each lung com- prise a large number of features for each airway segment, e.g., seg- mental wall thickness, airway diameter, lumen area, parent-child branch angles, etc. The wealth of lung airway data provides a unique opportunity for advancing our understanding of the funda- mental structure-function relationships within the lung. An impor- tant problem is to construct and identify signicant lung airway features in normal subjects and connect these to standardized pul- monary function test data such as the percent predicted forced ex- piratory volume in one second (FEV1%). Because of many unique features of the high-dimensional lung airway measurements, cus- tomized dimension reduction approaches for constructing and se- lecting interpretable airway features are required. In particular, the problem is complicated by the fact that a particular airway feature may be an important predictor only when it pertains to segments of certain generations. Thus, the key is an efcient, consistent method for simultaneously conducting group selection (lung airway feature types) and within-group variable selection (airway generations), i.e., bi-level selection. Here we streamline a comprehensive procedure to processing the lung airway data via imputation, normalization, transformation and groupwise principal component analysis, and then adopt a new composite penalized regression approach for con- ducting bi-level feature selection. As a prototype of the composite penalization approaches, the proposed composite bridge regression method is shown to admit an efcient algorithm, enjoy bi-level ora- cle properties, and outperform several existing methods when group misspecication exists. We analyze the CT lung image data from a cohort of 132 subjects with normal lung function. Our results show that, lung function in terms of FEV1% is promoted by having a lessdense and more homogeneous lung comprising an airway whose segments enjoy more heterogeneity in wall thicknesses, larger mean diameters, lumen areas and branch angles. These data hold the po- tential of dening more accurately the normal subject population with borderline atypical lung functions that are clearly influenced by many genetic and environmental factors. Imputing Transcriptome of Inaccessible Tissues In and Beyond the GTEx Jiebiao Wang1, Eric Gamazon2, of Chicago 2Vanderbilt University lchen@health.bsd.uchicago.edu Gene expression and its regulation largely depend on cell context. However, due to tissue accessibility, large-scale gene expression studies often measure expression proles in the peripheral whole blood or its derivatives. In order to synthesize new knowledge about the organization of gene expression across human tissues, the Genotype-Tissue Expression (GTEx) project collected the tran- scriptome data in a wide variety of human tissues from a large numbers of post-mortem donors. By analyzing data from nine se- lected model tissues in the GTEx pilot project, we show that in- accessible/uncollected transcriptome data can be imputed by har- nessing rich information in the GTEx, and that it is feasible to use GTEx data as a reference and impute inaccessible/uncollected tis- sues in future studies. We EQTLs' (RIMEE), evaluate its performance and compare it to existing imputing methods, and suggest cost-effective strategies for future multi-tissue expression studies. Efcient Variance Component Estimation with the Haseman El- ston Approximate Regression Xiang Zhou University of Michigan xzhousph@umich.edu Linear mixed models (LMMs) have attracted considerable recent in- terest, and have been widely applied in geneticstudies to dissect the genetic architecture of many common diseases and complex traits. However, maximal likelihood estimation of the variance compo- nents in LMMs is computationally expensive and requires a step that scales cubically with the sample size, hence is not applicable to large samples. Here, we present a method of moment approach to address this computational bottleneck. Our method not only is computationally fast and scales quadratically with the sample size, but also requires only summary statistics, allowing it to be applied to large consortium studies where raw genotype and phenotype data are often unavailable. With realistic simulations, we show that our method produces unbiased estimates with only small lose of ef- ciency compared with maximal likelihood estimates. With two real data applications, we show that our method is useful in partition- ing phenotypic variance into different chromosomes or functional genomic annotations. Our method is implemented in the GEMMA software package, freely available at www.xzlab.org/software.html. Improved Ancestry Estimation for both Genotyping and Se- quencing Data using Projection University of Texas Southwestern Medical Center 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j49Abstracts 3Harvard University 4University of Michigan wangcl@gis.a-star.edu.sg Accurate estimation of individual ancestry is important in genetic association studies, especially when a large number of samples are collected from multiple sources. However, existing approaches de- veloped for genome-wide SNP data do not work well with mod- est amounts of genetic data, such as in targeted sequencing or ex- omechip genotyping experiments. We propose a statistical frame- work to estimate individual ancestry in a principal component an- cestry map generated by a reference set of individuals. This frame- work extends and improves upon our previous method for estimat- ing ancestry using low-coverage sequence reads (LASER 1.0) to analyze either genotyping or sequencing data. In particular, we in- troduce a projection Procrustes analysis approach that uses high- dimensional principal components to estimate ancestry in a low- dimensional reference space. Using extensive simulations and em- pirical data examples, we show that our new method (LASER 2.0), combined with genotype imputation on the reference individuals, can substantially outperform LASER 1.0 in estimating ne-scale genetic ancestry. Specically, LASER 2.0 can accurately estimate ne-scale ancestry within Europe using either exomechip geno- types or targeted sequencing data with off-target coverage as low as 0.05X. Under the framework of LASER 2.0, we can estimate in- dividual ancestry in a shared reference space for samples assayed at different loci or by different techniques. Therefore, our ancestry es- timation procedure will accelerate discovery in disease association studies not only by helping model ancestry within individual stud- ies but also by facilitating combined analysis of genetic data from multiple sources. Session 9: SII Special Invited Session on Modern Bayesian Statistics I Binary State Space Mixed Models with Flexible Link Functions: a Case Study on Deep Brain Stimulation on Attention Reaction Time Carlos Abanto-Valle1, Dipak Dey2and\u0007Xun Jiang3 1Universidade Federal do Rio de Janeiro of Connecticut 3Amgen Inc. tonyjiangxun@gmail.com State space models (SSM) for binary time series data usinga flexible skewed link functions are introduced in this paper. Commonly used logit, cloglog and loglog links are prone to link misspecication be- cause of their xed skewness. Here we introduce two flexible links as alternatives, they are the generalized extreme value (GEV) and the symmetric power logit (SPLOGIT) links. Markov chain Monte Carlo (MCMC) methods for Bayesian analysis of SSM with these links are implemented using the JAGS package, a freely available software. Model comparison relies on the deviance information cri- terion (DIC). The flexibility of the proposed model is illustrated to measure effects of deep brain stimulation (DBS) on attention of a macaque monkey performing a reaction-time task. Empirical re- sults showed that the flexible links t better over the usual logit and cloglog links. Bayesian Semi-parametric Joint Modeling of Biomarker Data with a Latent Assessing the Temporal of California, Irvine 3University of Prince Edward Island norris@csus.edu We develop a joint model for longitudinal biomarker data with one binary and one continuous variable for the purpose of quantifying the diagnostic capabilities of the data. We treat the no gold standard case where the actual timing of infection is unknown and must be estimated from the data. We incorporate random effects to allow for subject-specic, post-infection trajectories and model the random effects distribution using a nonparametric Dirichlet Process mixture to allow additional flexibility. The model is applied to the prob- lem of diagnosing Johne's Disease in cattle. Applying the model to these data showed that there are two clusters of cows in the data, one having a slower serology reaction to infection and the other having a more rapid reaction. Inference Functions in High-Dimensional Bayesian State University snm@stat.osu.edu Nonparametric Bayesian models, such as those based on the Dirich- let process or its many variants, provide a flexible class of models that allow us to t widely varying patterns in data. Typical uses of the models include relatively low dimensional driving terms to cap- ture global features of the data along with a nonparametric structure to capture local features. The models are particularly good at han- dling outliers, a common form of local behavior, and examination of the posterior often shows that a portion of the model is chasing the outliers. This suggests the need for robust inference to discount the impact of the outliers on the overall analysis. We advocate the use of inference functions to dene relevant parameters that are ro- bust to the deciencies in the model and illustrate their use in two examples. Quantile Regression for Censored Mixed-Effects Models with Applications to HIV studies \u0007Victor 3Universidade Federal do Rio de Janeiro hlachos@gmail.com HIV RNA viral load measures are often subjected to some up- per and lower detection limits depending on the quantication as- says. Hence, the responses are either left or right censored. Lin- ear/nonlinear mixed-effects models, with slight modications to ac- commodate censoring, are routinely used to analyze this type of data. Usually, the inference procedures are based on normality (or elliptical distribution) assumptions for the random terms. However, those analyses might not provide robust inference when the distri- bution assumptions are questionable. In this paper, we discuss a fully Bayesian quantile regression inference using Markov Chain Monte Carlo (MCMC) methods for longitudinal data models with random effects and censored responses. Compared to the conven- tional mean regression approach, quantile regression can character- ize the entire conditional distribution of the outcome variable, and is more robust to outliers and misspecication of the error distribu- tion. Under the ssumption that the error term follows an asymmetric Laplace distribution, we develop a hierarchical Bayesian model and 50j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts obtain the posterior distribution of unknown parameters at the pth level, with the median regression (p=0.5) as a special case. The pro- posed procedures are illustrated with two HIV AIDS studies on vi- ral loads that were initially analyzed using the typical normal (cen- sored) mean regression mixed-effects models, as well as a simula- tion study. Session 10: SII Special Invited Session on Modern Bayesian Statistics II A Bayes Testing Approach to Metagenomic Proling Bacteria of Washington jclarke3@unl.edu Using next generation sequencing (NGS) data, we use a multino- mial with a Dirichlet prior to detect the presence ofbacteria in a metagenomic sample via marginal Bayes testing for each bacterial strain. The NGS reads per strain are counted fractionally with each read contributing an equal amount to each strain it might represent. The threshold for detection is strain-dependent and we apply a cor- rection for the dependence amongst the (NGS) reads by nding the knee in a curve representing a tradeoff between detecting too many strains and not enough strains. As a check, we evaluate the joint posterior probabilities for the presence of two strains of bacteria and nd relatively little dependence. We apply our techniques to two data sets and compare our results with the results found by the Human Microbiome Project. We conclude with a discussion of the issues surrounding multiple corrections in a Bayes context. Nonparametric Bayesian lynn.kuo@uconn.edu Time-course microarray experiments track gene expression levels across several time points. They provide valuable insights into genome-wide dynamic aspects of generegulations. We focus on gene clustering analysis in this paper. We explore a nonparamet- ric Bayesian method for constructing clusters in a functional space from the characteristics of gene proles. In particular, we model each gene prole using a B-spline basis. So each gene is character- ized by the basis coefcients of the spline tting. Then we place a Dirichlet process prior on the basis coefcients to determine clus- ters of the genes. We essentially construct a hierarchical Dirichlet processes mixing model that assigns genes into the same cluster if they share the same latent basis coefcients. A simulation study is conducted to compare the proposed method to the K-means clus- tering method, a model-based clustering method (MCLUST), and a two-stage version of them in terms of the adjusted Rand index. We show our new method has better adjusted Rand index number among all these methods. We apply this nonparametric Bayesian clustering method to a real data set with 6 time points to gain further insights into how genes with similar proles are clustered together and we nd their functional annotation in Gene-Ontology groups using GOstats.A to Identify Genes and Gene-level SNP Ag- gregates in a Genetic Francesco Stingo1,\u0007Michael Swartz2and Marina Vannucci3 1M. D. Anderson Cancer Center 2The University of Texas School of Public Health 3Rice University michael.d.swartz@uth.tmc.edu Complex diseases, such as cancer, birth defects, and cardiovascular disease, arise from complex interplay of multiple genetic factors. Univariate analyses to identify genes cannot model this complex in- terplay, and small contributors to risk could be missed. With com- plex diseases multiple genetic markers, known as single nucleotide polymorphisms (SNPs), work in concert rather than isolation to af- fect risk. As a result, researchers investigating complex diseases turn to multivariate methods that can analyze groups of SNPs. When considering multiple SNPs simultaneously, we can also capitalize on additional biological information, such as biological grouping and genetic correlation. In this talk we will discuss a novel Bayesian modeling approach to identify SNPs associated with disease. We combine pathway based approaches to analyze multiple SNPs in a region of interest. Our model uses a gene level score based on SNP allele frequencies and use the linear modeling framework to model association between the SNP level scores and disease risk. Our gene scores use weights based on genotype frequencies so rarer genotypes have more weight in the score. We also employ Markov random eld priors that accounts for genetic correlation structures. This method was motivated by a lung cancer data set, and a basic introduction to relevant genetic concepts will be included in the talk. Adjusting Nonresponse Bias in Small Area Estimation without Covariates via a Spatial Model \u0007Xiaoming of Missouri-Columbia sherry.gao@mdc.mo.gov Sometimes a survey sample is drawn from a large area even if the estimate of interest is at a smaller subdomain level. This strategy, however necessary, may cause small sample problems. The esti- mation problem is further complicated by survey nonresponse. We build a Bayesian hierarchical spatial model that takes into account both small sample size and nonresponse. This Bayesian model gives the estimates of marginal satisfaction rates at subdomains even when there is no covariate available via modeling the phase- specic response rates and conditional satisfaction rates given re- sponse status at subdomains. This method is illustrated using data from the 2001 Missouri Deer Hunter Attitude Survey. Satisfaction, in this survey, refers to whether respondents were satised with the Missouri Department of Conservation's deer management program. The estimated satisfaction rates are lower after adjusting for non- response bias compared to the satisfaction rates based only on re- sponses. Session 11: Emerging Issues in Time-to-Event Data Bayesian Path Specic Frailty Models for Multi-state Mario castro1,\u0007Ming-Hui Chen2and Yuanye Zhang3 1Universidade de Sao 3Novartis Pharmaceutical Corporation ming-hui.chen@uconn.edu 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j51Abstracts Multi-state models can be viewed as generalizations of both the standard and competing risks models for survival data. Models for multi-state data have been the theme of many recent published works. Motivated by bone marrow transplant data, we propose a Bayesian model using the gap times between two successive events in a path of events experienced by a subject. Path specic frailties are introduced to capture the dependence structure of the gap times in the paths with two or more states. Under improper prior dis- tributions for the parameters, we establish propriety of the posterior distribution. An efcient Gibbs sampling algorithm is developed for drawing samples from the posterior distribution. An extensive sim- ulation study is carried out to examine the empirical performance of the proposed approach. A bone marrow transplant data set is ana- lyzed in detail to further demonstrate the proposed methodology. Quantile Association for Bivariate Survival Chen3and Jason 1The University Texas School of Public Health 2University of Pittsburgh 3Vanderbilt University 4The University of North Carolina at Chapel Hill ruosha.li@uth.tmc.edu Bivariate survival data arise frequently in familial association stud- ies of chronic disease onset, as well as in clinical trials and obser- vational studies with multiple time to event endpoints. The associ- ation between two event times is often scientically important. In this paper, we examine the association via a novel quantile associa- tion measure, which describes the dynamic association as a function of the quantile levels. The quantile association measure is free of marginal distributions, allowing direct evaluation of the underlying association pattern at different locations of the event times. We pro- pose a nonparametric estimator for quantile association, as well as a semiparametric estimator that is superior in smoothness and ef- ciency. The proposed methods possess desirable asymptotic prop- erties including uniform consistency androot-n convergence. They demonstrate satisfactory numerical performances under a range of dependence structures. An application of our methods suggests in- teresting association patterns between time to myocardial infarction and time to stroke in an atherosclerosis study. Robust Estimation for Clustered Failure Time South Carolina 3Columbia University tpgarcia@sph.tamhsc.edu An important goal in clinical and statistical research is estimatingthe distribution for clustered failure times, which have a natural intra- class dependency and are subject to censoring. We propose to han- dle these inherent challenges with a novel approach that does not impose restrictive modeling or distributional assumptions. Rather, using a logit transformation, we relate the distribution for clustered failure times to covariates and a random, subject-specic effect such that the covariates are modeled with unknown functional forms, and the random effect is distribution-free and potentially correlated with the covariates. Over a range of time points, the model is shown to be reminiscent of an additive logistic mixed effect model. Such a structure allows us to handle censoring via pseudo-value regres- sion and to develop semiparametric techniques that completely fac- tor out the unknown random effect. We show both theoretically and empirically that the resulting estimator is consistent for any choiceof random effect distribution and for any dependency structure be- tween the random effect and covariates. Lastly, we illustrate the method's utility in an application to the Cooperative Huntington's Observational Research Trial data, where our method provides new insights into differences between motor and cognitive impairment event times in subjects at risk for Huntington's disease. Session 12: Taiwan National Health Database Effective Analysis of Primary Preventive Anti-HBV Medicine to Prevent Hepatitis Reactivation in Cancer Patients Undergo- ing Chemotherapy Using National Health Insurance Data Base and Cancer Registry Data Ruey-Kuen Hsieh and\u0007Wen-Kuei Chien Taipei Medical University motorona74@gmail.com Introduction: Use of anti-HBV medications to prevent hepati- tis reactivation for cancer patients with HBV carrier state during chemotherapy was reimbursed in Taiwan since 2009 for all cancer types. This universal coverage policy was not embraced by the Can- cer Society but suggested by Hepatology Society. This analysis us- ing national insurance data base and cancer registry data base will try to delineate the effectiveness of this policy in patients with com- mon solid tumors (Breast, lung and colorectal). Methods: Data from the National health insurance data base (2006- 2012), Cancer registry data (2006-2012) and Cause of Death Data (2006-2012) were used. We followed the patients, aged 10 and above, who have common solid tumors (Breast, lung and colorec- tal) undergoing chemotherapy to see whether the primary preventive anti-HBV medicine can reduce hepatitis activation and hepatitis- related death. Analysis: Chi-Square tests were applied to compare the hepati- tis activation rate, hepatitis-related death rate between people start chemotherapy before and after policy change. We further assess the association between use of anti-HBV medication and hepatitis activation/hepatitis-related death to see whether use of anti-HBV medication as prophylaxis is effective. Results and conclusion: Before the reimbursement, there are n=1,147 cases of hepatitis reactivation among n=49,368 cases of patients with lung, breast and colon cancer and had chemotherapy. After the universal reimbursement, there are n=707 Cases of hep- atitis reactivation among n=10,503 patients with there three can- cers and had chemotherapy. Before the reimbursement, there are n=13,719 cases cancer related deaths, n=37 cases hepatitis related deaths due to reactivated hepatitis among n=14,499 cases of patients with lung, breast and colon cancer and had chemotherapy. After the universal reimbursement, there are n=5,406 cases of cancer related deaths, n=11 cases hepatitis related deaths due to hepatitis reacti- vation among n= 5,702 patients with there three cancers and had chemotherapy. There were not improvement hepatitis related death after reimbursement. 1. Use of anti-HBV medication as prophylaxis is effective in reducing hepatitis reactivation, reducing hepatitis re- activation rate from 2.32 to 1.74. 2. There is no signicant improve- ment in reducing hepatitis related death in this group of patients, number of death decreasing from 37(0.26) to 11(0.19). 3. There is no effect in cancer related survival, increasing from 94.62% to 94.81% 4. Cost effectiveness should be considered before imple- mentation of a universal coverage. A Nationwide Cohort Study of Influenza Vaccine on Stroke Pre- 52j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts vention in the Chronic Kidney Disease Population \u0007Chang-I Chen, Wen-Kuei Chien, Yen-Kuang Lin, Ruey-Kuen Hsieh, Chao-Feng Lin, Liu and Hao-Weng Deng Taipei Medical University dcchen@tmu.edu.tw Purpose: Patients with chronic renal disease (CKD) are known to be at risk of vascular events. Whether vaccinating CKD patients with influenza vaccination to reduce the risk of stroke has not been fully revealed. Method: We conducted a retrospective population- based study within a nationwide longitudinal database in Taiwan from 1999 to 2008. CKD patients were selected according to diag- nosis claims and were divided into 2 subgroups: with and without vaccination. Furthermore, analysis was also conducted for CKD pa- tients with 1, 2, 3 cumulative numbers of vaccination. The primary outcome was hospitalization for stroke. Cox proportional hazards regression was used to measure the hazard ratios (HR) of stroke. Results: 2247 CKD patients with at least one vaccination and 2159 patients with no vaccination during study period were followed. The adjusted HR of influenza vaccination was 0.37 (95 Conclusion: Influenza vaccination was suggested to protect CKD patients more than 55 years-old from the risk of stroke. A cumula- tive effect of vaccination was also found in the present study. Economic Movement and Mental Health: A Population-based Study. 2Mingdao University robbinlin@tmu.edu.tw study examines the potential relationship between various eco- nomic indices and the incidences of mental disorder using National Health Insurance Research Database (NHIRD) during 2000-2008. As of 2007, approximately 98.4% of Taiwanese were enrolled in the NHIRD. Daily observations of 3285 from the Taiwan Stock Ex- change Capitalization Weighted Stock Index, NHIRD, and Execu- tive Yuan were retrieved. We examined the association among the stock index, housing index and the mental disorder incidence. In general, we found that the higher incidence of mental health dis- order were associated with the increasing housing index and lower stock index. We also stratied the study sample based on their sex, age and urbanization levels. Both gender follow the similar pat- tern. During 2000 to 2008, although the rising economic indices may bring fortune, but the sacrice of the mental health could be the cost we have to pay. Session 13: Recent Advance in Longitudinal Data Analy- ses Integrative and Adaptive Weighted Group Lasso Local Approximation Qing University 2George Mason University yzhao15@gmu.edu Longitudinal clinical outcomes are often collected in genomic stud- ies, where selection methods accounting for dynamic genetic effects are desirable. We model the biomarker effects by smoothing splines, and select the coefcients by group lasso with novel weight func- tions based on the extremum of the biomarker effects over time. In addition to the common practice treating weights as adaptive functions whose values depend on some rst-stage estimates, wefurther propose an integrative group lasso method to treat the loss, penalty and weightfunctions as an integrative whole, where param- eters in all three are jointly estimated in one step. While the adap- tive group lasso can be solved by standard local quadratic approxi- mation, guidelines for more general local quadratic approximations are developed to optimize the integrative group lasso. Consistency and sparsistency are proved for both adaptive and integrative meth- ods, while the integrative version is more general in requiring fewer assumptions. Both procedures show superior specicity and bias over unweighted group lasso in simulation studies. The methods are illustrated with the GWAS from the Epidemiology and Inter- vention of Diabetes Complication trial. To accommo- date more candidate markers, 23 chromosomes are analyzed separately, and the estimates are pooled in selecting common tuning parameters. A Dynamic Risk Prediction Model for Data with Competing Risks \u0007Chung-Chou Chang1and Qing Liu2 1University of Pittsburgh 2Novartis Pharmaceutical Corporation changj@pitt.edu Risk prediction modeling has been widely used for assessing the effects of change in risk factors on the absolute risk of disease in- cidence or disease progression, weighing the risk and benets of an intervention, and designing future prevention trials, yet most of such models to date are applicable only to information observed at or before study baseline. Recent dynamic risk prediction models in- corporate information after study baseline into modeling, which are more capable of handling the effects of disease progression. To fur- ther advance such modeling approach, we proposed a risk prediction model which not only incorporates longitudinally updated informa- tion but also can account for the effect of competing risks. With predictors of current patient characteristics, history of the disease prole, and future longitudinally collected information our model can be applied to the planning of personalized medicine. Our pro- posed model has several advantages over currently used risk predic- tion models in addressing competing risks. First, it is robust against violations of the proportional subdistributional hazards assumption. Second, the model enables users to make predictions with a set of landmark points (i.e., prediction baseline time points) in one step. Third, the proposed model can incorporate various types of time- varying information. Finally, our model is not computationally in- tensive and can be easily implemented with existing statistical soft- ware. The performance of our model was assessed via simulations. We also demonstrated the use of our model with a data set from a multicenter clinical trial for breast cancer patients. Simultaneous Inference of a Misclassied Outcome and Com- peting Risks Failure Time Data \u0007Sheng Luo1, Xiao Su1, Min Yi2and Kelly Hunt2 1The University of Texas at Houston 2M. D. Anderson Cancer Center sheng.t.luo@uth.tmc.edu Ipsilateral breast tumor relapse (IBTR) often occurs in breast can- cer patients after their breast conservationtherapy. The IBTR status classication (true local recurrence versus new ipsilateral primary tumor) is subject to error and there is no widely accepted gold stan- dard. Time to IBTR is likely informative for IBTR classication be- cause new primary tumor tends to have a longer mean time to IBTR and is associated with improved survival as compared with the true local recurrence tumor. Moreover, some patients may die from breast cancer or other causes in a competing risk scenario during 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j53Abstracts the follow-up period. Because the time to death can be correlated to the unobserved true IBTR status and time to IBTR (if relapse occurs), this terminal mechanism is non-ignorable. In this paper, we propose a unied framework that addresses these issues simul- taneously by modeling the misclassied binary outcome without a gold standard and the correlated time to IBTR, subject to dependent competing terminal events. We evaluate the proposed framework by a simulation study and apply it to a real data set consisting of 4477 breast cancer patients. The adaptive Gaussian quadrature tools in SAS procedure NLMIXED can be conveniently used to t the pro- posed model. We expect to see broad applications of our model in other studies with a similar data structure. Copula-based Quantile Regression for Longitudinal Data \u0007Huixia Wang1and Xingdong Feng2 1The George Washington University 2Shanghai University of Finance and Economics judywang@gwu.edu Inference and prediction in quantile regression for longitudinal data are challenging without parametric distributional assumptions. We propose a new semiparametric approach that uses copula to account for intra-subject dependence and approximates the marginal distri- butions of longitudinal measurements, conditionally on covariates, through regression of quantiles. The proposed method is flexible, and it can provide not only efcient estimation of quantile regres- sion coefcients but also prediction intervals for a new subject given the prior measurements and covariates. The properties of the pro- posed estimator and prediction are established theoretically, and as- sessed numerically through simulation study and the analysis of a Pennsylvania nursing home data. Session 15: Innovative Statistical Approaches in Nonclin- ical Research Identifying Predictive Biomarkers in A Dose-Response Study \u0007Yuefeng Lu, Xiwen Ma and Wei Zheng Sano-aventis U.S. LLC. yuefeng.lu@sanofi.com The problem of retrospectively identifying subgroups using biomarkers has recently been of great interests. So far most research has been focused on two-group study design. For studies with mul- tiple doses, either partial data is used or multiple dose groups are pooled, leading to sub-optimal results. We will present a frame- work to deal with multi-dose studies, and compare the new method with ad-hoc methods through simulations. Bayesian Integration of In Vitro Biomarker Data to In Vivo Safety Assessment \u0007Ming-Dauh Wang and Alan Chiang Eli Lilly and Company mingdauhwang@yahoo.com Adverse effects of pharmaceuticals on electrocardiograms ECGs) are one of the most critical drug safety concerns facing drug devel- opment companies today. It is recognized that a better understand- ing of the interrelationship between preclinical measures of cardio- vascular safety biomarkers as well as a more integrated approach to risk assessment could dramatically speed the development of safe and effective medicines for patients in need. Using the Health and Environmental Sciences Institute of the International Life Sciences Institute (ILSI/HESI) dataset, we constructed a Bayesian repeated analysis of covariance model to directly assess the cardiovascular risk in QT interval.With prior distributions derived from in vitrohERG ionic current concentration, posteriors for treatment effects were obtained and the likelihood of risk was calculated. Sensitivity of the proposed Bayesian analysis to prior selection and comparison with the classical method were characterized. The results demon- strate that Bayesian integration of in vitro and in vivo biomarkers provides an effective preclinical risk assessment for QT interval and can reduce unnecessary animal exposure in toxicology studies. Functional Structural Equation Model for DTI Derived Re- sponses in Twin Study \u0007Shikai Rui Song1 1North Carolina State University 2The University of North Carolina at Chapel Hill sluo@ncsu.edu Motivated by analyzing massive functional data from large biomed- ical studies, we propose a class of functional structural equation model (FSEM) for modeling xed effect process for characterizing the association between functional responses and covariates of inter- est and for modeling random effect processes for capturing spatial correlations of functional responses from twin studies. We develop an efcient estimation procedure to estimate the varying coefcient functions and the spatial covariance operators. We also systemat- ically carry out the theoretical analysis of FSEM. First, we estab- lish the weak convergence of the maximum likelihood estimate of xed effect functions. Second, we propose a pointwise testing pro- cedure to the existence of genetic effect at each xed tract point via weighted likelihood ratio test and a global testing procedure to the existence of global genetic effect along the entire tract. We also propose a resampling procedure for approximating the p-values for both tests. Third, we establish the asymptotic normality for the es- timated spatial covariance kernels. We conduct extensive Monte Carlo simulations to examine the nite-sample performance of the estimation and inference procedures. Finally, two real data sets are analyzed to illustrate the application of our theoretical results. Estimating Contamination Rates from Matched Tumor-normal The accuracy of next-generation sequencing (NGS) technology is greatly compromised, when tumor-normal samples from individuals are contaminated by each others. This contamination from sample mixture is quite common in cancer study, since differentiating pure tumor cells from normal cells is difcult in a sample preparation step. Hence, the sample contamination needs to be quantied from a NGS tumor-normal dataset, before researchers study somatic mu- tation signals. In order to estimate contamination rates from tumor- normal pair exome sequencing data, we propose to use a mixture model. Our approach requires only a pair of tumor-normal samples, and it can easily applied to any deep-sequenced recapture dataset as an initial quality check tool. Session 16: Statistical Advances for Genetic Data Analy- sis Incorporating External Information to Improve Case-control Genetic Association Analyses Hong Zhang1, Nilanjan 2National Institutes of Joint Conference, Fort Collins, Colorado, June 14-17Abstracts 3University of Pennsylvania jinboche@mail.med.upenn.edu In the standard logistic regression analyses of case-control genetic association studies, it has been shown in empirical studies that ad- justing for informative covariates could lead to decreased power. This result conflicts with that for clinical trials, where adjusting for baseline covariates always leads to increased power. We offer theo- retical explanations for this dilemma, and propose a unied solution to this problem. For rare phenotypes, we conclude that the most practical strategy is the standard analyses without adjusting for co- variates. For common phenotypes, our proposed solution can lead to meaningful power improvement compared with the analysis ig- noring covariates. Generalized Partial Linear Varying Index Coefcient Model for Gene-Environment Interactions \u0007Xu Liu, Bin Gao and Yueyua Cui Michigan State University liuxuxm@gmail.com Epidemiological studies have suggested the joint effect of simulta- neous exposures to multiple environments on disease risk. How- ever, how environmental mixtures as a whole jointly modify genetic effect on disease risk is still largely unknown. Given the impor- tance of gene-environment (G imes E) interactions on many com- plex diseases, rigorously assessing the interaction effect between genes and environmental mixtures as a whole could shed novel in- sights into the etiology of complex diseases. For this purpose, we propose a generalized partial linear varying-index coefcient model (GPLVICM) to capture the genetic effect on disease risk modulated by multiple environments as a whole. GPLVICM is semiparametric in nature which allows different index loading parameters in differ- ent index functions. We estimate the parametric parameters by a prole procedure, and the nonparametric index functions by a B- spline backtted kernel method. Under some regularity conditions, the proposed parametric and nonparametric estimators are shown to be consistent and asymptotically normal. We propose a generalized likelihood ratio (GLR) test to rigorously assess the linearity of the joint environmental effect, while apply a parametric likelihood test to detect linear G imes E interaction effect. The nite sample per- formance of the proposed method is examined through simulation studies and is further illustrated through a real data analysis. Set-valued System Identication Approach to Identifying Ge- netic Variants in Sequencing Studies Guolian Kang St. Jude Children's Research Hospital guolian.kang@stjude.org For genetic association studies that involve a binary or an ordered categorical phenotype, the standard logistic regression (LG) or or- dered LG (oLG) model is commonly used to identify genetic as- sociations. However, these approaches can lose power or cannot control type I error rate if the phenotype is derived from dichotomiz- ing/categorizing a continuous phenotype following a normal distri- bution or from complicated unobservable or unobserved continuous variables or if the genetic mutations are rare. We propose a set- valued (SV) system model, which is a generalized form of LG, oLG, Probit (Probit) regression, or ordered Probit (oPRB) regression, to be considered as a method for discovering genetic variants, espe- cially rare genetic variants in next generation sequencing studies. We propose a new set-valued system identication (SVSI) method to estimate all the underlying key system parameters for the SV model and compare it with LG in the setting of genetic association studies for a binary phenotype and with LG (a regrouped pheno-type), oLG and oPRB for an ordered categorical phenotype. For a binary phenotype, simulations showed that the SV method main- tained Type I error control and had similar or greater power than the LG method which is robust to different distributions of noise: logistic, normal or t distributions. Additionally, the SV association parameter estimate was 2.7-46.8 fold less variable than the LG log- odds ratio association parameter estimate. Less variability in the association parameter estimate translates to greater power and ro- bustness across the spectrum of minor allele frequencies (MAFs), and these advantages are the most pronounced for rare variants. For instance, in a simulation that generated data from an additive logis- tic model with odds ratio of 7.4 for a rare single nucleotide poly- morphism with a MAF of 0.005 and a sample size of 2300, the SV method had 60% power whereas the LG method had 25% power at the ?=10-6 level. Consistent with these simulation results, the set of variants identied by the LG method was a subset of those identied by the SV method in two example analyses. For an or- dered categorical phenotype, simulations and two examples showed that SV and LG accurately controlled the Type I error rate even at a signicance level of 10-6 but not oLG and oPRB in some cases. LG had signicantly smaller power than the other three methods due to disregarding of the ordinal nature of the phenotype, and SV had similar or greater power than oLG and oPRB. Thus, we recom- mend that the SV model with SVSI be used in SNP-based genetic association studies, especially for detecting rare variants or given a small sample size such as for some rare pediatric cancer genomics projects. A Penalized Robust Semiparametric Approach for Finance and Economics 3Michigan State University cen.wu@yale.edu In genetic and genomic studies, gene-environment (G imes E) inter- actions have important implications. Some of the existing G imes E interaction methods are limited by analyzing a small number of G factors at a time, by assuming linear effects of E factors, by as- suming no data contamination, and by adopting ineffective selection techniques. In this study, we propose a new approach for identify- ing important G imes E interactions. It jointly models the effects of all E and G factors and their interactions. A partially linear varying coefcient model (PLVCM) is adopted to accommodate possible nonlinear effects of E factors. A rank-based loss function is used to accommodate possible data contamination. Penalization, which has been extensively used with high-dimensional data, is adopted for selection. The proposed penalized estimating approach can auto- matically determine if a G factor has an interaction with an E factor, main effect but not interaction, or no effect at all. The proposed approach can be effectively realized using a coordinate descent al- gorithm. Simulation shows that it has satisfactory performance and outperforms the direct competitor that is not robust. The proposed approach is used to analyze a lung cancer study with gene expres- sion measurements and clinical variables. It identies genes with important implications. 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j55Abstracts Session 18: Statistical Methods for Sequencing Data Analysis Unit-free and Robust Detection of Differential Expression from RNA-Seq Data Hui Jiang University of Michigan jianghui@umich.edu Ultra high-throughput sequencing of transcriptomes (RNA-Seq) has recently become one of the most widely used methods for quanti- fying gene expression levels due to its decreasing cost, high accu- racy and wide dynamic range for detection. However, the nature of RNA-Seq makes it nearly impossible to provide absolute measure- ments of transcript concentrations. Several units or data summa- rization methods for transcript quantication have been proposed to account for differences in transcript lengths and sequencing depths across genes and samples. However, none of these methods can re- liably detect differential expression directly without further proper normalization. We propose a statistical model for joint detection of differential expression and data normalization. Our method is inde- pendent of the unit in which gene expression levels are summarized. We also introduce an efcient algorithm for model tting. Due to the L0-penalized likelihood used by our model, it is able to reliably normalize the data and detect differential expression in some cases when more than half of the genes are differentially expressed in an asymmetric manner. The robustness of our proposed approach is demonstrated with simulations. Robust Estimation of Isoform Expression with RNA-Seq Data \u0007Jun Li1and Hui Jiang2 1University of Notre Dame 2University of Michigan jun.li@nd.edu Qualifying gene and isoform expression is one of the primary tasks for RNA-Seq experiments. Given a sequence of counts representing numbers of reads mapped to different positions (exons and junc- tions) of isoforms, methods based on Poisson generalized linear models (GLM) with the identity link function have been proposed to estimate isoform expression levels from these counts. These Pois- son based models have very limited ability in handling the overdis- persion in the counts brought by various sources, and some of them are not robust to outliers. We propose a negative binomial based GLM with identity link, and use a set of robustied quasi-likelihood equations to make it resistant to outliers. An efcient and reliable numeric algorithm has been identied to solve these equations. In simulations, we nd that our approach seems to outperform exist- ing approaches. We also nd evidence supporting this conclusion in real RNA-Seq data. Genetic Association Testing for Binary Traits in the Presence of Population Structure \u0007Duo Jiang1, Sheng Zhong2and Mary sara Mcpeek2 1Oregon State University 2University of Chicago jiangd@stat.oregonstate.edu In genetic association mapping, failure to properly adjust for pop- ulation structure can lead to severely inflated type I error and loss of power. Meanwhile, adjustment for relevant covariates is often desirable and sometimes necessary to protect against spurious as- sociation and to improve power. Many recent methods to account for population structure and covariates are based on linear mixed models (LMM), primarily designed for quantitative traits. For bi- nary traits, however, LMM is often a misspecied model and canlead to power loss. We develop a new method for binary trait asso- ciation testing using a quasi-likelihood framework, which exploits the dichotomous nature of the trait by modelling covariate effects on a logit scale and achieves computationally efciency through es- timating equations. We show through simulation studies that our method provides power improvement over the linear mixed method approach in a variety of population structure settings and trait mod- els. Applied to an association analysis for Crohn's disease in the WTCCC data, our methodidenties 18 signicantly associations, one of which has not been previously reported. Identication of Stably Expressed Genes from Arabidopsis RNA-Seq Data Bin Zhuo,\u0007Yanming Di, Sarah Emerson and Jeff Chang Oregon State University diy@stat.oregonstate.edu We examine RNA-Seq data from many different experiments car- ried out by different labs and identify genes that are stably expressed across biological samples, experiment conditions, and labs. We t a random-effect model to the read counts for each gene and decom- pose the total variance to into between-sample, between-condition and between lab variance components. Identifying stably expressed genes is useful for count normalization. The variance component analysis is a rst step towards understanding the sources and nature of the RNA-Seq count variation. Session 19: Recent Developments in the Theory and Ap- plications of Spatial Statistics Estimating a Low Rank Covariance Matrix for Spatial Data \u0007Siddhartha Nandy, Chae-Young Lim and Tapabrata Maiti Michigan State University nandysid@stt.msu.edu We are interested in estimating a low rank covariance matrix for spatial data. We consider the spatial covariance matrix of the pro- cess is decomposed into two components: a diagonal matrix com- ing from a measurement error process and a low rank covariance matrix which has a non-stationary structure. We propose a two-step approach using group LASSO type shrinkage estimation technique for estimating the rank of the covariance matrix and the matrix it- self. A block coordinate descent method for a block multi-convex function under regularizing constraints is utilized to implement the proposed approach. Computational Instability of Spatial Covariance Matrices State University wuweiying1011@mail.ndhu.edu.tw The computing a covariance matrix is seen very often in statistics. For example, Gaussian likelihood function involves the covariance matrix. Spatial prediction called Kriging involves the computation of the inverse of a spatial covariance matrix. For the computation of a spatial covariance matrix, numerically unstable results are found when the observation locations are getting dense. In this talk, we investigate why and when computational instability in calculating Mat'ern covariance matrix makes maximum likelihood estimator (MLE) or Kriging unreasonable in the ill-conditioned sense. Also, some possible approaches to relax such computational instability are also discussed. 56j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts Statistical Method for Change-set Analysis Jun Zhu University of Wisconsin-Madison jzhu@stat.wisc.edu Here we consider a change-set method for grouping spatial units on a lattice to have similar characteristics within a group and dis- tinction among groups. This may be viewed as an extension of the existing change-point analysis for one-dimensional data in space or time. In particular, we propose an entropy measure and establish a quasi-likelihood procedure for parameter estimation and statistical inference. The large sample properties of our method are estab- lished and the nite sample properties are investigated in a simu- lation study. For illustration, our method is applied to analyze a county-based socio-economic data set. Session 20: Risk Prediction Modeling in Clinical Trials Evaluating Calibration of Risk Prediction Models Ruth Pfeiffer National Institutes of Health pfeiffer@mail.nih.gov Statistical models that predict disease incidence, disease recurrence or mortality following disease onset have broad public health and clinical applications. Before a model can be recommended for practical use, its performance characteristics need to be under- stood. General criteria to evaluate prediction models for dichoto- mous outcomes include predictive accuracy, proportion of variation explained, calibration and discrimination. Most recent validation studies have emphasized calibration and discrimination. A model is called \"well calibrated\" (or unbiased) when the predicted prob- abilities agree with observed risk in subsets of the population and overall. I propose and study novel criteria to assess the calibration of models that predict risk of disease incidence and compare their performance to standard methods to assess model calibration. I il- lustrate the methods with models that predict incidence of breast cancer and response to Hepatitis C treatment. Statistical Considerations for Evaluating Prognostic Imaging Biomarkers Zheng Zhang Brown University zzhang@stat.brown.edu Biomarkers derived from quantitative imaging modalities such as Ktrans from DCE, rCBV from DSC and ADC from DWI are being evaluated in various cancer clinical trials to determine their abilities in predicting treatment effect. For binary outcome such as patho- logical response, area under the ROC curve (AUC) has been a pre- ferred method to determine the marker's predictive performance. However, odds ratio (OR) from logistic regression has also been routinely used in this situation, although the relationship between OR and AUC is not very well documented. In addition, since most of the imaging markers are continuous, choice has to be made on what form of the data (continuous and binary) should be used in the logistic regression model. If we choose to model the biomarker as a binary data, a pre-specied threshold has to be chosen.In this talk, we will present simulation study results on the relationship between AUC and OR, as well as impact of threshold on the odds ratio esti- mation. We demonstrate that OR is not suitable for evaluating the prognostic marker, whereas AUC is a better choice. Risk Assessment for Patients with Hepatitis C: A Scoring Sys-tem Approach Center 2University of Michigan wshen@mdanderson.org Modern genomic technologies have generated a large number of biomarkers for early-phase detection and prognosis of diseases. A major challenge is how to identify informative risk factors to con- struct a score system for predicting the likelihood of developing diseases. In this talk, I will introduce a time-dependent receiver operating characteristic based method to construct a score system that combines informative biomarkers and other baseline informa- tion. The proposed methods bypass the need to model the outcomes, and can be extended to accommodate data from complex clinical trial designs (e.g., nested case-control design). Theoretical prop- erties (e.g., selection consistency and asymptotic normality) of the proposed estimators are established. We apply the method to data from the Hepatitis C Antiviral Long-term Treatment against Cirrho- sis (HALT-C) Trial. Risk Prediction Modeling in the National Lung Screening Trial Fenghai Duan Brown University fduan@stat.brown.edu Introduction: In the National Lung Screening Trial (NLST) a 20% relative reduction in lung cancer mortality was observed using re- duced dose helical computed tomography (CT) relative to chest-x- ray screening in older smokers. This sub-study aims to determine how the observed nodules and the associated features can influence lung cancer diagnosis. Methods: In 26,455 participants who under- went at least one CT screen, sensitivity, specicity, positive predic- tive value, and negative predictor value for lung cancer were de- termined separately for different types of nodules. Relative risk of lung cancer was determined as the ratio of lung cancer in the nodule-detected group compared to the non-nodule group. The ap- proach of a two-stage modeling was then applied to determine how the observed nodules and the associated features can influence lung cancer diagnosis. In Stage 1, a Cox proportional hazards model was tted at the participant level to assess if the presence of a nodule at baseline increases the hazards of developing lung cancer. The time- varying effect of the nodule type and other clinical variables were also evaluated in this stage. In Stage 2, a generalized linear mixed model was tted on the observed nodules to determine how the as- sociated nodule features can affect the probability of lung cancer diagnosis in the same lobe. Conclusions: Clinical and nodule fea- tures can be used to better stratify risk of lung cancer diagnosis and improve CT screening performance. Session 21: The Application of Latent Variable and Mix- ture Models to the Biological Sciences The Role of Item Response Theory in Assessment and Evalua- tion Studies \u0007Li Cai and Lauren Harrell University of California, Los Angeles lcai@ucla.edu Item response theory (IRT) modeling, as a latent variable model- ing approach, grew out of the factor analysis tradition within psy- chometrics. Its usefulness in educational, psychological, mental and behavioral health assessment studies has been broadly estab- lished. \"Proceduralized\" IRT as a routine data analytic method has 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j57Abstracts also seen inclusion in new releases of mainstream statistical soft- ware packages such as SAS and Stata. On the other hand, in both biomedical and social sciences, treatment evaluation studies have relied heavily on models that grew out of the regression modeling framework. In this paper we present new IRT models that integrate measurement and regression modeling. The application of Structural Equation Modeling to Biomarker data \u0007Nathan Vandergrift, Sallie Permar and Barton Haynes Duke University nathan.vandergrift@duke.edu Equation Modeling (SEM) is a covariance structure anal- ysis method that allows for simultaneous dimensionality reduction and regression parameter testing. With the increase in use of multi- plex assays the need for different modeling approaches to the anal- ysis of biomarker data is acute. SEM has not been used with any frequency in the biological sciences. We will lay out the reasons that bio-medical researchers may wish to adopt SEM. We will also present a SEM analysis of biomarker data from a Mother-to-Infant Transmission (MTCT) study to show its practical utility. In the anal- ysis we nd evidence of an underlying mechanism for autologous neutralization which led to the isolation via flow cytometry of those antibodies from the sera of a non-transmitting mother (Permar, et al. In Press). Latent and Observed Variables in Kernel-Penalized Regression Models Timothy Randolph Fred Hutchinson Cancer Research Center trandolp@fhcrc.org Kernel-based tests for the (global) association between a high- dimensional vector of measurements, x, and a phenotype, y, may be formulated in the context of a reproducing kernel Hilbert space, H. A relevant H is one spanned by the eigenvectors of an appropriately- chosen kernel, K; i.e., a matrix of similarities between pairs of the observed vectors. In this sense H represents a vector space of latent structure, and a kernel-based test statistic is dened in terms of how well y is predicted with respect to this structure. However, a global test of this type does not reveal insight about individual elements of x. In this talk, we consider a framework for choosing K (equiva- lently, H) that can provide both a powerful score test and an estimate of individual associations between the elements of x and y. Moti- vation is provided by examples from microbiome and metabolomic data analyses. A Mixture Model Approach to Estimating a Nonlinear Errors- in-Variables Model for Serial Dilution Assay Youyi Hutchinson Cancer Research Center youyifong@gmail.com Serial dilution assays with continuous experimental outcomes are often used to quantify substance in a biomedical sample. The de- sign and analysis of serial dilution assay data has so far primar- ily been based on estimating dilution-response curves, the nonlin- ear relationship between sample dilutions and experimental out- comes. Motivated by the analysis of binding antibody multiplex assays (BAMA) that has played an important role in the study of immune responses to HIV-1 infection and vaccination, we study the nonlinear functional relationship between experimental outcomes measured at two different dilutions, which we call paired response curve (prc, Fong et al. 2015). Paired response curve model is a non- linear errors-in-variables model. In this talk, we take a structuralapproach and treat the incidental parameters as random effects from a nite-dimensional mixing distribution. We describe an algorithm for nding the MLE, and study its performance through simulation studies and real data illustration. Session 22: Clinical Trials with Multiple Objectives: Maximizing the Likelihood of Success Statistical Challenges in Testing Multiple Endpoints in Complex Trial Designs \u0007H.M. James Hung and Sue-Jane Wang U.S. Food and Drug Administration HsienMing.Hung@fda.hhs.gov Methodology for testing multiple endpoints is increasingly chal- lenging as a clinical development program is more advanced. In group sequential designs, interim analyses are mostly based on the primary endpoint with a pre-specied alpha-spending strategy. When the trial stops to declare a win on the primary endpoint, how to test a secondary endpoint is challenging as noted in Hung et al (2007) and subsequently stipulated in Glimm et al (2010) and Tamhane et al (2010). In another context, stopping the trial should arguably be based on a harder endpoint, such as mortality, which is often at best a secondary endpoint. Statistical testing for the pri- mary endpoint and the secondary endpoint is also quite challenging in this scenario. When the secondary endpoint requires a greater amount of statistical information, more than one trial may need to be integrated in a pre-specied plan to improve statistical power for testing. This presents additional challenges to statistical testing. In this presentation, I shall discuss the methodological challenges and stipulate some viable approaches to the problems. Group-Sequential Clinical Trials When Considering Multiple Outcomes Hamasaki1, 2Harvard University toshi.hamasaki@ncvc.go.jp We discuss the decision-making frameworks for clinical trials with multiple outcomes as co-primary endpoints in a group-sequential setting. The decision-making frameworks can account for flexibil- ities, such as a varying number of analyses, equally or unequally spaced increments of information, and xed or adaptive Type I er- ror allocation among endpoints. The frameworks can provide ef- ciency, that is, potentially fewer trial participants, than the xed sample size designs. We investigate the operating characteristics of the decision-making frameworks and provide guidance on con- structing efcient group-sequential strategies in clinical trials with multiple co-primary endpoints. Sample Size Determination for a Specic Region in Multi- regional Clinical Trials 2National Cerebral and Cardiovascular Center chinfu@nhri.org.tw To accelerate the drug development process and shorten approval time, the design of multi-regional clinical trials (MRCTs) incorpo- rates subjects from many countries/regions around the world under the same protocol. After showing the overall efcacy of a drug in all global regions, one can also simultaneously evaluate the possibility of applying the overall trial results to all regions and subsequently 58j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts support drug registration in each of them. Most of the recent ap- proaches for design and evaluation of MRCTs are only concerned with one primary endpoint. However, in some therapeutic areas (e.g. Alzheimer's disease), the clinical efcacy of a new treatment may be characterized by a set of possibly correlated endpoints be- cause patients' responses to the treatment may comprise several dif- ferent aspects. We focus on a specic region and establish statistical criteria for consistency between the region of interest and overall re- sults in MRCTs with multiple co-primary endpoints. More specif- ically, four criteria are considered for each endpoint. Two criteria are to assess whether the treatment effect in the region of interest is as large as that of the other regions or of the regions overall, while the other two criteria are to assess the consistency of the treatment effect of the specic region with other regions or the regions overall. Sample size required for the region of interest can also be evaluated based on these four criteria. Fallback Medical School robin.ristl@meduniwien.ac.at When efcacy of a treatment is measured by co-primary endpoints, efcacy is claimed only if for each endpoint an individual statistical test is signicant at level alpha . While such a strategy controls the family-wise type I error rate (FWER), it is often strictly conserva- tive and allows for no inference if not all null hypotheses can be rejected. This situation can be unsatisfying, especially in settings such as rare diseases where optimal use of the available informa- tion is of high importance.We therefore investigate fallback tests, which are dened as uniform improvements of the classical test for co-primary endpoints. They reject whenever the classical test re- jects but allow for inference also in settings where only a subset of endpoints show a signicant effect. Similar to the fallback tests for hierarchical testing procedures (Wiens et al. 2005) these fallback tests for co-primary endpoints allow one to continue testing even if the primary objective of the trial was not met. Examples of fall- back tests for two and three co-primary endpoints are investigated that control the FWER in the strong sense under the assumption of multivariate normal test statistics with arbitrary correlation ma- trix. The power of the considered fallback tests is investigated in a simulation study. The discussion on the fallback procedures for co- primary endpoints is illustrated with a clinical trial in a rare disease and an example for a diagnostic trial. This work has been funded by the FP7-HEALTH-2013-INNOV ATION-1 project Advances in Small Trials Design for Regulatory Innovation and Excellence (AS- TERIX) Grant Agreement No. 603160. Session 23: Issues Related to Subgroup Analysis in Con- rmatory Clinical Trials: Challenges and Opportunities A Statistical Decision Framework Applicable to Multipopula- tion Tailoring Trials Brian Millen Eli Lilly and Company bmillen@lilly.com The promise of tailored therapies has resulted in increased attention on the evaluation of treatment effects in focused subpopulations in clinical trials. Such assessments present new opportunities for drug development and, ultimately, patients and prescribers. Statistical considerations enabling such treatment assessments in conrmatoryclinical trials will be presented in this talk. Particular attention will be paid to contrasting the properties and operating characteristics of various approaches to support appropriate inference. A Multiple Comparison Procedure for Subgroup Analyses with Binary Bretz Novartis Pharmaceutical Corporation dong.xi@novartis.com It is a common practice in clinical trials to investigate the efcacy and safety of a test treatment in subgroups of patients in addition to the overall study population. However, the ndings of subgroups analyses do not provide conrmatory evidence unless they are spec- ied a priori. We consider the conrmatory subgroup analysis for Phase II trials in oncology, where the hypotheses of interest are tested in the overall study population and the pre-specied sub- population. Since these trials are usually conducted as single-arm designs with binary endpoints, e.g., overall response rate (ORR), a multiple comparison procedure is proposed to ensure the strong control of the familywise Type I error rate as well as to take into account the discreteness of data and the correlation between popu- lations. Interaction Trees for Exploring Stratied and Individualized Treatment Effects Xiaogang Su The University of Texas at El Paso xsu@utep.edu Assessing heterogeneous treatment effects has become agrowing in- terest in many application elds including personalized medicine. Concerning experimental data collected from randomized trials, we expand Interaction Trees (IT; Su et al., 2009), to explore stratied and individual treatment effects in a variety of ways. As an alterna- tive to greedy search, a smooth sigmoid surrogate (SSS) method is rst used to speed up IT. On the basis of IT, causal inference at dif- ferent levels are then made. More specically, an aggregated group- ing procedure straties data into rened groups where the treatment effect remains homogeneous. Ensembles of IT models can provide prediction for individual treatment effects and it compares favorably to the traditional 'separate regression' methods. Besides, a recent innitesimal jackknife method of Wager Hastie, and Efron (2014) is adopted to obtain the standard errors for the individualized treat- ment effects. In order to extract meaningful interpretations, we also made available several other features such as variable importance and partial dependence plot. An empirical illustration of the pro- posed techniques is made via an analysis of quality of life (QoL) data among breast cancer survivors. Considering Regional Difference in Design and Evaluation of MRCTs for Binary Endpoints \u0007Chi-Tian Chen and Chin-Fu Hsiao National Health Research Institutes citchen@nhri.org.tw Multiregional clinical trial (MRCT) is a clinical trial of global col- laboration for pharmaceutical product development. The planning and implementation of MRCT is conducted at the same time across countries/regions based on a common protocol. The key issue lies in how to address the possible geographic variations of efcacy and safety for the global drug development since it is known that the regional difference among regions may have impact upon a medicine's effect. Therefore, the heterogeneity of treatment effect due to the regional difference should be considered in design and 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j59Abstracts analysis of MRCT. In this presentation, we focus on binary end- points and address heterogeneous treatment effect (response rate) across regions by a power-function distribution. Then, the design and evaluation of MRCT is based on a Bernoulli-power-function model. After showing the efcacy of a drug in various local re- gions, the consistency between the specic region and entire group is conrmed according to the concept of the method 1 proposed by Japanese MHLW. Finally, the proportion of subjects in the specic region is determined by ensuring that the assurance probability of the consistent criterion reaches a desired level, say 80% or 90%. Session 24: Recent Developments in Missing Data Analy- sis Variable Selection in the Presence of Missing Data: and Imputation of Rochester qlong@emory.edu In the presence of missing data, variable selection procedures need to be tailored to missing data mechanisms and statistical approaches used for handling missing data. We focus on the mechanism of missing at random and variable selection procedures that can be combined with imputation methods. We investigate a general re- sampling approach that combines bootstrap imputation and stability selection, the latter of which was developed for fully observed data. The proposed approach is general and can be applied to a wide range of settings including general missing data patterns. Our simulation studies show that the proposed approach achieves the best or close to the best performance compared to several existing methods for both low-dimensional and high-dimensional problems. In particu- lar, it is not very sensitive to tuning parameter values. The proposed approach is further illustrated using two real data examples, one for a low-dimensional problem and the other for a high-dimensional problem. Using Link-Preserving Imputation for Logistic Partially Einstein College of Medicine qc2138@columbia.edu To handle missing data one needs to specify auxiliary models such as the probability of observation or imputation model. Doubly ro- bust (DR) method uses both auxiliary models and produces con- sistent estimation when either of the model is correctly specied. While the DR method in estimating equation approaches could be easy to implement in the case of missing outcomes, it is compu- tationally cumbersome in the case of missing covariates especially in the context of semiparametric regression models. In this paper, we propose a new kernel-assisted estimating equation method for logistic partially linear models with missing covariates with appli- cations to two-phase studies. We replace the conditional expectation in the DR estimating function with an unbiased estimating function constructed using the conditional mean of the outcome given the ob- served data, and impute the missing covariates using the so called link-preserving imputation models to simplify the estimation. The proposed method is valid when the nonresponse model is correctlyspecied and is more efcient than the kernel-assisted inverse prob- ability weighting estimator. It is doubly robust under missing com- pletely at random or when the regression coefcients of the missing parametrically modeled covariates are equal to zero. The proposed estimator is consistent and asymptotically normal. We evaluate the nite sample performance in terms of efciency and robustness, and illustrate the application of the proposed method to the health insur- ance data using the 2011-2012 National Health and Nutrition Ex- amination Survey, in which data were collected in two phases. Composite Likelihood Approach in Gaussian Copula Regres- sion Models with Missing Data \u0007Wei Ding and Peter Song University of Michigan dingwei@umich.edu Misaligned missing data occur in many large-scale studies due to some impediments in data collection such as policy restriction, equipment limitation and budgetary constraint. By misaligned miss- ingness we mean a missing data pattern in which two sets of vari- ables are measured from disjoint subgroups of subjects with no overlapped observations. An analytic challenge arising from the analysis of such data is that some of correlation parameters related to those misaligned variables are not point identiable but possi- bly partially identiable. This parameter identication issue hinders us from utilizing classical multivariate models in the data analysis. To overcome this difculty, we propose a composite likelihood ap- proach based on marginal distributions of variables with full ob- servations, so that the resulting pseudo likelihood is free of any unidentiable parameters. After obtaining estimates of the point identiable parameters, we further estimate the parameter range for partially identiable parameters. For implementation, we develop an effective peeling optimization procedure to obtain estimates of point identiable parameters. We investigate the performance of the proposed composite likelihood method through simulation studies, with comparisons to the classical maximum likelihood estimation obtained from both EM algorithm and multiple imputation strategy. The proposed method is illustrated by one data example from our collaborative project. Test the Reliability of Doubly Robust Estimation with Missing Response Data \u0007Baojiang Chen1and Jing Qin2 1University of Nebraska Medical Center 2National Institutes of Health baojiang.chen@unmc.edu In statistical inference one has to make sure that the underlying regression model is correctly speciedotherwise the resulting es- timation may be biased. Model checking is an important method to detect any departure of the regression model from the true one. Missing data is a ubiquitous problem in social and medical stud- ies. If the underlying regression model is correctly specied, re- cent researches show great popularity of the doubly robust estimates method for handling missing data because of its robustness to the misspecication of either the missing data model or the conditional mean model, i.e. the model for the conditional expectation of true regression model conditioning on the observed quantities. However, little work has been devoted to the goodness of t test for doubly ro- bust estimates method. In this paper, we propose a testing method to assess the reliability of the estimator derived from the doubly robust estimating equation with possibly missing response and al- ways observed auxiliary variables. Numerical studies demonstrate that the proposed test can control type I errors well. Furthermore the 60j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts proposed method can detect departures from model assumptions in the marginal mean model of interest powerfully. A real dementia data set is used to illustrate the method for the diagnosis of model misspecication in the problem of missing response with an always observed auxiliary variable for cross-sectional data. Session 25: Spatial and Spatio-Temporal Modeling in En- vironmental and Ecological Studies Multivariate Spatial Modeling on Spheres \u0007Juan University 2Wichita State University dujuan@ksu.edu Multivariate spatial processes on spheres are often used to model large scale spatial data sets with multiple attributes observed on a globe, such as those in geophysical and atmospheric studies. To directly construct valid and applicable covariance or variogram ma- trix functions on spheres, we rst nd characterizations of those structures. For intrinsically stationary vector random processes on spheres, the variogram matrix function on a sphere can be repre- sented in terms of an innite sum of the products of positive def- inite matrices and ultraspherical polynomials. The non-stationary but is also explored. Some parametric vari- ogram or covariance matrix models are derived on spheres via dif- ferent constructional approaches. A simulation study is conducted to illustrate the implementation of the proposed model in estima- tion and cokriging, whose performance is compared with that using linear model of coregionalization. Autoregressive Spatially-varying Coefcient Models 2U.S. Environmental Protection Agency erin.schliep@duke.edu There is high demand for accurate air quality information in human health analyses. The sparsity of ground monitoring stations across the United States motivates the need for advanced statistical models to predict air quality metrics, such as PM 2:5, at unobserved loca- tions. Remote sensing technologies have potential to expand our knowledge of PM 2:5spatial patterns beyond what we can predict from current PM 2:5monitoring networks. Data from satellites have an additional advantage in not requiring extensive emission inven- tories necessary for most atmospheric models that have been used in earlier data fusion models of air pollution. Statistical models combining monitoring station data with satellite-obtained aerosol optical depth (AOD) have been proposed in the literature with vary- ing levels of accuracy in predicting PM 2:5. The benet of using AOD is that satellites provide complete gridded spatial coverage. The challenges of these models, however, is that (1) the correlation between the two data sources varies both in time and in space, (2) the data are temporally and spatially misaligned, and (3) there are extensive missing data in both data sources. We propose a hierar- chical autoregressive spatially-varying coefcients model to jointly model the two data sources addressing the foregoing modeling chal- lenges. Additionally, we apply formal model comparison on com- peting models in terms of model t and out of sample prediction of PM 2:5. The models are applied to daily observations of PM 2:5 and AOD in the summer of months of 2013 across the conterminous United States. Most notably, we nd slight in-sample improvementincorporating AOD into our autoregressive model but little out-of- sample predictive improvement during this time period. Modeling Animal Abundance with A Semi-Parametric Space- Time Model Devin Johnson The National Oceanic and Atmospheric Administration devin.johnson@noaa.gov We consider a model-based clustering approach to examining abun- dance trends in a metapopulation. Our proposed trend analysis in- corporates a clustering method that is an extension of the classic Dirichlet process prior, which allows for inclusion of distance co- variates between sites. This approach has two main benets: (1) nonparametric spatial association of trends and (2) reduced dimen- sion of the spatio-temporal trend process. We Gibbs sampler for making Bayesian inference that is efcient in the sense that all of the full conditionals can be directly sam- pled from save one. To demonstrate the proposed method we exam- ine long term trends in northern fur seal pup production at nineteen rookeries in the Pribilof Islands, Alaska. There was strong evidence that clustering of similar year-to-year deviation from linear trends was associated with whether rookeries were located on the same is- land. Clustering of local linear trends did not seem to be strongly associated with any of the distance covariates. In the fur seal trends analysis an overwhelming proportion of the MCMC iterations pro- duced a 73-79% reduction in the dimension of the spatio-temporal trend process, depending on the number of cluster groups. An Efcient Non-parametric Estimate for Spatially Correlated Functional Data \u0007Yuan Wang, Kim-Anh Do, Jianhua Hu and Brian Hobbs M. D. Anderson Cancer Center ywang46@mdanderson.org We consider the functional data being observed at multiple units on a subject and data observed on neighboring units are correlated. A weighted kernel smoothing estimate is designed to leverage the space and time correlation. Asymptotic results are derived for the proposed estimate, and there is a unique most efcient estimate that achieves minimum asymptotic variance. A simultaneous prediction for individual curves using discrete samples are discussed. Simula- tion studies has illustrated the improved performance of the reduced mean square error for the weighted estimate. We apply the methods to the perfusion computed tomography data where subjects were measured sequentially over time and several regions of interest were acquired for each subject. We show the proposed method remains good performance when reducing the number of scans. The general method offers potential to improve estimation and prediction per- formance in the case of sparse observation. The method is attractive for many biomedical application that utilizes biomarkers to identify features intrinsic to a particular disease at multiple interdependent sites within an organ. Session 26: Challenges in Analyzing Complex Data Using Regression Modeling Approaches Goodness-of-Fit Tests of Finite Mixture Regression 3Duke University shouen.lu@rutgers.edu 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j61Abstracts Evaluating the goodness-of-t (GOF) for a mixture regression model can be challenging due to the complexity in the model struc- ture. In this paper, we propose a GOF test to evaluate the t of nite mixture regression models, according to the principle of cumulative residuals. Specically, we dene the cumulative pseudo-residuals based on the score functions of a normal mixture regression model for independent observations, and develop GOF tests to evaluate the t of the regression models for the component specic means and the mixing proportions, with respect to the link function and the functional form of a covariate. The proposed tests are extended to the two-component normal mixture regression models for depen- dent observations, where the dependence of observations is mod- elled by random effects. Extensive simulation studies showed that the proposed GOF tests maintained the type I error rate and had a reasonable power to detect model deviations. The data from the Beijing Health Effects of Air Pollution Reduction Trial (HEART) were used to illustrate the proposed methodologies. Comparing Methods of Modeling Individual Infancy Growth Curves Pennsylvania 2Children's Hospital of Philadelphia rxiao@mail.med.upenn.edu Comparing methods of modeling individual infancy growth curves- Rui Xiao, Sani M. Roy, Alessandra Chesi, Frank F.A. Grant, Babette S. Zemel, Shana E. McCormack Recent studies have reported that rapid growth in infancy is associated with a greater risk of subsequent obesity as well as other health outcomes in later life. Therefore it is important to adequately characterize in- dividual infancy growth patterns, which may promote effective early interventions of obesity. We used a longitudinal cohort with healthy, non-preterm infants (n=2114) in the Genetic Causes for Complex Pediatric Disorders study (The Children's Hospital of Philadelphia) with at least six body mass index (BMI) measurements from birth to 30.25 months of age. In this study, we present several com- monly used methods of modeling individual BMI growth curves in this cohort, including 1) Individual polynomial regression, 2) Penalized cubic spline mixed-effect model, 3) Parametric mixed- effect model, and 4) SuperImposition by Translation and Rotation (SITAR) model, a shape invariant model (SIM) with a single tted curve. We compared their performance through AIC and residual standard deviation (RSD). Alzheimer's Disease Early Prediction and Imaging Genetics Analyses Based on Large Scale Regularization \u0007Fang-Chi Hsu, Mark Espeland and Ramon Casanova Wake Forest University School of Medicine fhsu@wakehealth.edu Study of Alzheimer's disease (AD) has entered into a new era when biological measures from multiple platforms, such as neuroimag- ing and genetics, are being collected to help deepen understanding of the disease and improve prevention and diagnosis. Because it is believed that pathological processes that lead to AD start many years before the clinical symptoms are observed, the problem of early detection and risk assessment is a major focus of AD research. High-dimensional data pose a serious challenge to developing mod- els for AD risk prediction. We discuss here models we used for AD risk assessment based on structural magnetic resonance imaging (sMRI- voxel) and genome wide single-nucleotide polymorphism(SNP) data Initiative (ADNI) using large scale regularization. Sparsity learning based on elastic net regular- ization was used to develop anatomical scores of AD risk that we called AD Pattern Similarity (AD-PD) scores. Using ADNI data, we validated the scores by relating them to different cognitive states (e.g. cognitively normal, mild cognitive impairment (MCI) and AD) and to times of MCI to AD conversion. We also show results of GWAS analyses when the AD-PS scores were used as quantitative traits. In addition, we show that very large classications problems could be solved successfully in SNP space using the same approach. Thus, large scale regularization approaches could be useful in sum- marizing the MRI data and identifying genetic markers that predict AD risk. Session 27: Bayesian Applications in Biomedical Studies Bayesian Functional Enrichment Analysis \u0007Jing Cao1and Song Zhang2 1Southern Methodist University 2The University of Texas Southwestern Medical Center jcao@smu.edu Functional enrichment analysis is used in high-throughput data analysis to provide functional interpretation for a list of genes or proteins that share a common property, for example, genes that are differentially expressed (DE). The hypergeometric P-value is commonly used in the enrichment analysis to investigate whether genes from pre-dened functional groups, e.g., as represented by Gene Ontology (GO) annotations, are enriched in the DE gene list. The hypergeometric P-value has three limitations: 1) it is com- puted independently for each GO term and thus neglects the inter- relationship among neighboring terms; 2) the P-value has a size constraint, i.e., a lower limit determined by the size of GO term, which makes it biased towards selecting larger (less-specic) GO terms; and 3) overlapping genes in GO terms are repeatedly used in the calculation of the P-value. In this paper, we propose a Bayesian model based on the non-central hypergeometric distribution to over- come the above limitations. The dependence structure among GO terms is incorporated through a prior on the non-centrality parame- ter. The resulting measure for enrichment is a posterior probability which does not have the size constraint. Also, the overlapping infor- mation is removed from the likelihood function. We show that this method, with the above improvements, can detect moderate but con- sistent enrichment signals and identify sets of closely-related and biologically-meaningful GO terms rather than individual isolated GO terms. Bayesian Spatial Clustering Method and Its Applicatoin in Ra- diology \u0007Song Zhang1and Yin Xi2 1The University of Texas Southwestern Medical Center 2Southern Methodist University song.zhang@utsouthwestern.edu Kidney cancer is among the ten most common cancers in human. The dynamic contrast-enhanced MRI (DCE-MRI) takes advantage of the interaction between a contrast agent and adjacent water pro- tons which generates brighter signals in the scan image. In this study, we propose a novel Bayesian spatial clustering method based on a mixture of multivariate normal distribution. A latent condi- tional regression (CAR) process is employed to account for the spa- tial correlation among clustering indexes. The proposed method is demonstrated to provide smoother and more accurate clustering of 62j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts pixels. A simulation study and a real application example are pre- sented. Adjusting for Heterogeneity in Infectivity in HIV Prevention Clinical Trials \u0007Jingyang Zhang1and Elizabeth Brown1;2 1Fred Hutchinson Cancer Research Center 2University of Washington jzhang2@fredhutch.org The focus of this work is to estimate the effectiveness of a candi- date intervention at an individual exposure to HIV . Unlike the over- all effectiveness for the population estimated by the Cox propor- tional hazard model, the individual-level effectiveness accounts for the variations in the exposure process among study participants. We propose a Bayesian hierarchical model to estimate the individual- level effectiveness by adjusting for the heterogeneity in risk with multi-state processes. This model allows a time-varying magnitude of exposure process for each participant. A simulation is conducted to assess the model performance and a data set from an HIV pre- vention trial is applied for illustration. Canonical elds, multi-view datasets, measuring multiple distinct but interrelated sets of characteristics on the same set of subjects, to- gether with data on certain outcomes or phenotypes, are routinely collected. For example, in cancer study, microscopic organ tissue measurements, genetic proles, and organ function test results may all be available. The objective in such a problem is often twofold: both to explore the association structures of multiple sets of mea- surements and to develop a parsimonious model for predicting the future outcomes. We study a unied canonical variate regression framework to tackle the two problems simultaneously, allowing them to flexibly borrow strength from each other and hence rein- force each other. The proposed criterion integrates multiple canon- ical correlation analysis with predictive modeling, balancing be- tween the association strength of the canonical variants and their joint predictive power on the outcomes. Moreover, the proposed criterion seeks multiple sets of canonical variates simultaneously to enable the examination of their joint effects on the outcomes, and is able to handle multivariate and non-Gaussian outcomes through a general loss function formulation. The approach thus success- fully bridges the gap between an unsupervised canonical correla- tion analysis and a generalized reduced-rank regression method at two extremes. An efcient algorithm based on the ideas of variable splitting and Lagrangian multipliers is developed. Simulation stud- ies show superior performance of the proposed approach compared to existing alternative methods. We showcase the effectiveness of the proposed approach in an alcohol dependence study. Session 28: Go/No Go Decision Criteria and Probability of Success in Pharmaceutical Drug Development Sample Size Allocation in a Dose-Ranging Trial Combined with PoC Qiqi Deng and\u0007Naitee Ting Boehringer-Ingelheim Pharmaceuticals Inc. naitee.ting@boehringer-ingelheim.comIn recent years, pharmaceutical industry has experienced many chal- lenges in discovering and developing new drugs, including long clinical development timelines with signicant investment risks. In response, many sponsors are working to speed up the clinical devel- opment process. One strategy is to combine the Proof of Concept (PoC) and the dose-ranging clinical studies into a single trial at the early Phase II development. One important question in designing this trial is how to calculate the sample size for such a study. In most of the early Phase II development programs, the budget con- cerns and ethical concerns may limit the total sample size for the trial. This manuscript discusses various ways of allocating the sam- ple size to each treatment group, under a given total sample size. Selecting Development Strategy with Biomarkers \u0007Feng Gao, Yi Liu and Mingxiu Hu Takeda feng.gao@takeda.com Using biomarker to dene patient subpopulation with enhanced treatment effect can play important roles in drug development. When a biomarker is fully established in a disease setting, the decision is to choose between the whole population and some biomarker-dened subpopulations or to incorporate both in the trial. This presentation will discuss how statisticians may influence the decision making process and the decision itself in development strategy. We will describe a simulation tool for assisting the de- velopment team and the company management to make better deci- sions. Backward Bayesian Go/No-Go in the Early Phases Yin Yin Parexel International yin.yin@parexel.com This work assists sponsors to decide early phase Go/No-Go crite- ria based on the ultimate efcacy or safety target which is usu- ally clearer for Phase 3. Based on the denition of success for Phase 3, prior information, cost of later phases, this work graphi- cally presents the quantitative relationships between the following factors: true effect, early study result, study designs (e.g., sample size, duration, or dose), target probability of success (PoS), ex- pected nancial loss, expected probability of terminating a poten- tially successful asset. An example demonstrates how to accomplish these objectives for an exponential model describing the trajectory of weight loss. An Excel Workbook calculates PoS, probability of wrong Go, probability of wrong No-Go, and expected quantitative consequences along with a variety of relationships and trends when- ever an exponential model is appropriate, e.g., for a dose response study. The sponsor can optimize the Go/No-Go criteria based on the upper limit of the expected loss. It can also be generalized for other nonlinear models. Evaluation of Program Success for Programs with Multiple Tri- als in Binary Outcomes \u0007Meihua Wang, Guanghan Liu and Jerald Schindler Merck & Co. meihuawang99@gmail.com A late stage clinical development program typically contains mul- tiple trials. Conventionally, the program's success or failure may not be known until the completion of all trials. Nowadays, interim analyses are often used to allow evaluation for early success and/or futility for each individual study by calculating conditional power, predictive power and other indexes. It presents a good opportunity for us to estimate the probability of program success (POPS) for the entire clinical development earlier. The sponsor may abandon 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j63Abstracts the program early if the estimated POPS is very low, and therefore permit the resource savings and reallocation to other products. We provide a method to calculate probability of success (POS) at indi- vidual study level and also POPS for clinical programs with multi- ple trials in binary outcomes. Methods for calculating variation and condence measures of POS and POPS, and timing for interim anal- ysis will be discussed and evaluated through simulations. We also illustrate our approaches on historical data retrospectively from a completed clinical program for depression. The features and limi- tations of the proposed approaches will be discussed, as well as the operational challenges in practical implementations. Session 29: Machine Learning for Big Data Problems A Scalable Integrative Model for Heterogeneous Genomic Data Types under Multiple Conditions Mai Shi and\u0007Yingying Wei The Chinese University of Hong Kong yweicuhk@gmail.com A key problem in biology is how the same copy of a genome within a person can give rise to hundreds of cell types. Plentiful convincing evidence indicates multiple elements, such as transcription factor binding, histone modication, and DNA methylation, all contribute to the regulation of gene expression levels in different cell types. Therefore, it is crucial to understand how these heterogeneous regu- latory elements collaborate together, how the cooperation at a given genomic region changes across diverse cell lines, as well as how such dynamic cooperation patterns across cell lines vary along the whole genome. Here, we propose a scalable hierarchical proba- bilistic generative model to cluster genomic regions according to the dynamic changes of their open chromatin and DNA methylation status across cell types. The model will overcome the exponential growth of parameter space as the number of cell types integrated in- creases. The tted results of the model will provide a genome-wide region-specic, cell-line-specic open chromatin and DNA methy- lation landscape map. Greedy Tree Learning of Optimal propose a subgroup identication approach for detecting opti- mal personalized treatment rules. The problem is transformed into a weighted classication problem, and the subgroups are identied through a subject-weighted classication tree. A greedy tree con- struction process adopts a newly proposed method, reinforcement learning trees, to pursuit signals in high-dimensional settings. The method is also extended to right censored survival data by utilizing the accelerated failure time model and introducing double weighting to the classication trees. The performance of the proposed method is demonstrated via simulation studies and analyses of the Cancer Cell Line Encyclopedia (CCLE) data. ROC Analysis for frequently observed or collected for de- tecting or understanding a disease. In this talk, we extend tools of ROC analysis from univariate marker setting to multivariate marker setting for evaluating predictive accuracy of biomarkers using a tree-based classication rule. Using an arbitrarily combined and-or classier, an ROC function together with a weighted ROC function (WROC) and their conjugate counterparts are introduced for exam- ining the performance of multivariate markers. Specic features of the ROC and WROC functions and other related statistics are dis- cussed in comparison with those familiar properties for univariate marker. Nonparametric methods are developed for estimating the ROC and WROC functions, and area under curve (AUC) and con- cordance probability. With emphasis on population average perfor- mance of markers, the proposed procedures and inferential results are useful for evaluating marker predictability based on multivari- ate marker measurements with different choices of markers, and for evaluating different and-or combinations in classiers. Tissue Classication Through Imaging Texture Analysis \u0007Peng Huang, Siva Raman, Linda Chu, Jamie Schroeder, Malcolm Brock, Franco Verde and Elliot Fishman Johns Hopkins University phuang12@jhmi.edu Cancer development and progression is associated with intratumoral heterogeneity. Tumor diagnosis and staging are typically based on lesion's anatomic appearance and extent of tumor spread. A limita- tion of current tumor diagnosis methods for all imaging modalities is that image interpretation is based on visual process, yet many im- age features cannot be visualized by naked eyes. We used image texture analysis to evaluate image intensity and the position of the pixels within an image to derive texture features to quantify intra- tumoral heterogeneity. Random forest was used to analyze these imaging features along with clinical and demographic characteris- tics. We illustrate our methodology using data from lung, kidney, liver, and pancreatic cancer studies. Session 30: Tensor-Structured Statistical Modelling and Inferences Dimenstion Reduction for Tensor Structure Data I-Ping Tu Institute of Statistical Science, Academia Sinica iping@stat.sinica.edu.tw Dimension reduction is one key step in statistical analysis for high dimensional data. When eachobservation is a matrix or a higher or- der tensor, the traditional approach is to vectorize the data before executing reduction algorithms. This approach often leads to an extremely high dimensional problem which comes along with in- tensive computations and inefcient estimations. High order SVD and Multilinear principal component analysis (MPCA) are thus pro- posed for tensor structure data. They reduce each mode space of the tensor separately and thus reduce the computations signicantly. One criticism to the new approach is that, unlike PCA, the projected data in the reduced space are still correlated. To this end, we pro- pose a two stage dimension reduction method, called structure PCA (SPCA). SPCA employs MPCA on the tensor data rst, and then applies PCA on the vectorized projected core scores from MPCA. A successful application of SPCA on a cryo-electron microscopy image data will also be presented. 64j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts Detection of Gene-Gene Interactions Academia Sinica 3National Chung Hsing University 4Yahoo Inc. 5North Carolina State University jytzeng@stat.ncsu.edu Finding an efcient and computationally feasible approach to deal with the curse of high-dimensionality is a daunting challenge faced by modern biological science. The problem becomes even more severe when the interactions are the research focus. To improve the performance of statistical analyses, we propose a sparse and low-rank (SLR) screening based on the tensor interaction regres- sion and lasso. SLR models the interaction effects using a low- rank matrix to achieve parsimonious parametrization. The low-rank model increases the efciency of statistical inference and, hence, SLR screening is able to more accurately detect gene-gene interac- tions than conventional methods. We illustrate the utility of the pro- posed procedure using real data application and simulations. The results suggest that the proposed procedure can identify main and interaction effects that would have been omitted by conventional screening methods. Rank Selection for Multilinear PCA Dai-Ni Hsieh,\u0007Su-Yun Huang and I-Ping Tu Institute of Statistical Science, Academia Sinica syhuang@stat.sinica.edu.tw We study the intrinsic model complexity of multilinear principal component analysis for model rank selection. This model com- plexity, called effective degrees of freedom, is dened as the rst derivative of the tted tensor as a function of tensor observations. An unbiased estimate of the degrees of freedom is derived using Stein's identity. The degrees of freedom depend on the eigenvalues dispersion of each tensor mode. We illustrate how the degrees of freedom can be used for multilinear PCA rank selection. Discussion: Tensor-Structured Statistical Modelling and Mong-Na Lo Huang National Sun Yat-sen University lomn@math.nsysu.edu.tw We will discuss issues related to tensor-structured statistical models in reducing the dimension of regression models for making infer- ences with high dimensional data, especially those related to image and genomic studies. Session 31: Adaptive Designs for Early-Phase Oncology Clinical of at Austin 4NorthShore University HealthSystem 5University of Chicago yxu.stat@gmail.com Targeted therapies based on biomarker proling are becoming a mainstream di- rection of cancer research and treatment. Dependingon the expression of specic prognostic biomarkers, targeted ther- apies assign different cancer drugs to subgroups of patients even if they are diagnosed with the same type of cancer by traditional means, such as tumor location. For example, Herceptin is only in- dicated for the subgroup of patients with HER2+ breast cancer, but not other types of breast cancer. How- ever, subgroups like HER2+ breast cancer with effective targeted therapies are rare and most can- cer drugs are still being applied to large patient populations that in- clude many patients who might not respond or benet. Also, the response to targeted agents in humans is usually unpredictable. To address these issues, we propose SUBA, subgroup-based adaptive designs that simultaneously search for prognostic subgroups and al- locate patients adaptively to the best subgroup-specic treatments throughout the course of the trial. The main features of SUBA in- clude the continuous reclas- sication of patient subgroups based on a random partition model and the adaptive allocation of patients to the best treatment arm based on posterior predictive proba- bili- ties. We compare the SUBA design with three alternative designs in- cluding equal randomization, outcome-adaptive randomization and a design based on a probit re- gression. In simulation studies we nd that SUBA compares favorably against the alternatives. A Curve-free Bayesian Decision-theoretic Design for Two-agent Phase I Trials Bee Lee1,\u0007Shenghua Fan2and Ying Lu3 1San Jose State University 2California State University, East Bay 3Stanford University kelly.fan@csueastbay.edu Although Bayesian statistical methods are gaining attention in the medical community, as they provide a natural framework for incor- porating prior information, the complexity of these methods limited their adoptions in clinical trials. This article proposes a Bayesian design for two-agent phase I trials that is relatively easy for clin- icians to understand and implement, yet performs comparably to more complex designs, so that it is more likely to be adopted in ac- tual trials. In order to reduce model complexity and computational burden, we choose a working model with conjugate priors so that the posterior distributions have analytical expressions. Furthermore, we provide a simple strategy to facilitate the specication of priors based on the toxicity information accrued from single-agent phase I trials. The proposed method should be useful in terms of the ease of implementation and the savings in sample size without sacricing performance. Moreover, the conservativeness of the dose-nding algorithm renders it a relatively safe method. Bayesian Dose-nding Designs for Combination of Molecularly Targeted Agents Assuming Partial Stochastic Ordering Beibei Guo1and\u0007Yisheng Li2 1Louisiana State University 2M. D. Anderson Cancer Center ysli@mdanderson.org Molecularly targeted agent (MTA) combination therapy is in the early stages of development. When using a xed dose of one agent in combinations of MTAs, toxicity and efcacy do not necessarily increase with an increasing dose of the other agent. Thus, in dose- nding trials for combinations of MTAs, interest may lie in identify- ing the optimal biological dose combinations (OBDCs), dened as the lowest dose combinations (in a certain sense) that are safe and have the highest efcacy level meeting a prespecied target. The limited existing designs for these trials use parametric dose-efcacy and dose-toxicity models. Motivated by a phase I/II clinical trial of a 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j65Abstracts combination of two MTAs in patients with pancreatic, endometrial, or colorectal cancer, we propose Bayesian dose-nding designs to identify the OBDCs without parametric model assumptions. The proposed approach is based only on partial stochastic ordering as- sumptions for the effects of the combined MTAs and uses isotonic regression to estimate partially stochastically ordered marginal pos- terior distributions of the efcacy and toxicity probabilities. We demonstrate that our proposed method appropriately accounts for the partial ordering constraints, including potential plateaus on the dose-response surfaces, and is computationally efcient. We de- velop a dose-combination-nding algorithm to identify the OBDCs. We use simulations to compare the proposed designs with an alter- native design based on Bayesian isotonic regression transformation and a design based on parametric change-point dose-toxicity and dose-efcacy models and demonstrate desirable operating charac- teristics of the proposed designs. Session 32: Recent Development on Next Generation Se- quencing Based Data Analysis Bayesian nonparametric models 3The University of Texas at Austin 4University of California, Santa Cruz koaeraser@gmail.com I will discuss Bayesian nonparametric models for the inference of tumor heterogeneity using next-generation sequencing data. The task is to reconstruct cell subpopulations in a tumor sample or mul- tiple samples possessing unique genetic variants. Statistical infer- ence is based on feature allocation models such as the Indian buffet process (IBP). I will discuss extensions of the classical IBP and their applications to tumor heterogeneity. Various examples will be pre- sented aiming to reveal intra-tumor heterogeneity using subclonal single nucleotide variants and copy number variants. Leveraging in Big Data Analytics Ping Ma University of Georgia pingma@uga.edu Rapid advance in science and technology in the past decade brings an extraordinary amount of data, offering researchers an unprece- dented opportunity to tackle complex research challenges. The op- portunity, however, has not yet been fully utilized, because effective and efcient statistical tools for analyzing super-large dataset are still lacking. One major challenge is that the advance of computing resources still lags far behind the exponential growth of database. In this talk, I will introduce a family of statistical leveraging meth- ods to facilitate scientic discoveries using current computing re- sources. Leveraging methods are designed under a subsampling framework, in which one samples a small proportion of the data (subsample) from the full sample, and then performs intended com- putationsfor the full sample using the small subsample as a sur- rogate. The key of the success of the leveraging methods is to construct nonuniform sampling probabilities so that influential data points are sampled with high probabilities. These methods stand as the very unique development of their type in big data analytics andallow pervasive access to massive amounts of information without resorting to high performance computing and cloud computing. Investigating Microbial Co-occurrence Patterns Based on Metagenomics has provided us a powerful tool to study the micro- bial organisms living in various environments. Characterizing the interactions among the microbes can give us insights into how they work and live together as a community. Analyzing microbial re- lationships on metagenomic compositional data using conventional correlation methods has been shown prone to bias that leads to ar- tifactual correlations. We propose a novel method, REBACCA, to identify co-occurrence patterns using log ratios of count data and solve its equivalent system using the l1-norm shrinkage method. Our simulation studies show that REBACCA 1) achieves higher accuracy in general 2) is more robust to various structures of net- works; 3) is computationally efcient as compared to other existing methods. Rapid Alignment and Filtration for Accurate Pathogen Identi- cation in Clinical University wej@bu.edu The use of sequencing technologies to investigate the microbiome of a sample can positively impact patient healthcare by providing ther- apeutic targets for personalized disease treatment. However, these samples contain genomic sequences from various sources that com- plicate the identication of pathogens. Here we present a pipeline to rapidly and accurately remove host contamination, isolate microbial reads, and identify potential disease-causing pathogens. We devel- oped an optimized framework for pathogen identication using a computational subtraction methodology in concordance with read trimming and ambiguous read reassignment. We have also demon- strated the ability of our approach to identify multiple pathogens in a single clinical sample, accurately identify pathogens at the sub- species level, and determine the nearest phylogenetic neighbor of novel or highly mutated pathogens using real clinical sequencing data. Finally, we have shown our approach outperforms previously published pathogen identication methods with regard to computa- tional speed, sensitivity, and specicity. Session 33: Challenges of Quantile Regression in High- Dimensional Data Analysis: Theory and Applications Regularized Quantile Regression 3University of Wisconsin-Madison 4Memorial Sloan-Kettering Cancer Center qhe@fhcrc.org Genetic association studies often involve quantitative traits, such as Body Mass Index, blood pressure, and lipids level. Quantile re- 66j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts gression method considers the conditional quantiles of the response variable, and is able to describe the underlying structure in a more comprehensive manner. Here, we introduce a regularized quantile regression method that is able to characterize the underlying genetic structure under high dimensions, and at the same time is able to ac- count for the potential genetic heterogeneity. We investigate the theoretical property of our method, and examine its performance through a series of simulation studies. A real dataset is analyzed to demonstrate the usefulness of the proposed method. Globally Adaptive Quantile Regression with High Dimensional become a valuable tool to analyze hetero- geneous covaraite-response associations that are often encountered in practice. The development of quantile regression methodology for high dimensional covariates primarily focuses on examination of model sparsity at a single or multiple quantile levels, which are typically prespecied ad hoc by the users. The resulting models may be sensitive to the specic choices of the quantile levels, lead- ing to conceptual difculties in identifying relevant variables of in- terest. We propose a new penalization framework for quantile re- gression in the high dimensional setting. We employ adaptive L1 penalties, and more importantly, propose a uniform selector of the tuning parameter for a set of quantile levels to avoid potential prob- lems with model selection at individual quantile levels. Our pro- posed approach achieves consistent shrinkage of regression quantile estimates across a continuous range of quantiles levels, enhancing the flexibility and robustness of the existing penalized quantile re- gression methods. Our theoretical results include the oracle rate of uniform convergence and weak convergence of the parameter esti- mators. We also use numerical studies to conrm our theoretical ndings and illustrate the practical utility of our proposal. Focused Information Criterion and Model Averaging Based on Weighted Composite Quantile Regression \u0007Ganggang Xu1, A&M gang@math.binghamton.edu We study the focused information criterion and frequentist model averaging and their application to post-model-selection inference for weighted composite quantile regression (WCQR) in the context of the additive partial linear models. With the non-parametric func- tionsapproximated by polynomial splines, we show that, under cer- tain conditions, the asymptotic distribution of the frequentist model averaging WCQR-estimator of a focused parameter is a non-linear mixture of normal distributions. This asymptotic distribution is used to construct condence intervals that achieve the nominal coverage probability. With properly chosen weights, the focused informa- tion criterion basedWCQR estimators are not only robust to outliers and non-normal residuals but also can achieve efciency close to the maximum likelihood estimator, without assuming the true error distribution. Simulation studies and a real data analysis are used to illustrate the effectiveness of the proposed procedure. Bayesian Quantile Regression via Dirichlet Process Mixture of Logistic Distributions \u0007Chao Chang and Nan Lin Washington University in St. Louiscccc1987@gmail.com We propose a new nonparametric Bayesian approach to solve quan- tile regression for a single quantile. One innovation of this paper is that the error distribution is modelled by using the Dirichlet pro- cess mixture of relatively unexplored densities - logistic distribu- tions, which have the desired feature of being smooth and having a close-formed quantile function. Also unlike other methods based on Dirichlet process mixture, which require the kernel densities to satisfy the quantile constraint, our kernel function is just the sim- ple logistic densities. The quantile constraint is satised by a post- process of the Dirichlet process mixture by a suitable location shift. Although we have a simpler kernel, our mixture model can still pro- vide great flexibility by mixing over both the location parameter and the scale parameter. The posterior consistency of our proposed model is studied carefully. And Monte Carlo Markov chain algo- rithm is provided to do posterior inference. The performance of our approach is evaluated using simulated data and real data. Session 34: Recent Advances in Genomics Accounting For Gene Length in RNA-Seq Data \u0007Patrick Harrington and Lynn Kuo University of Connecticut PaddyBHarrington@gmail.com Next-generation sequencing is being used to advance genetics and biological research at a rapid pace. One of the goals is to identify differentially expressed transcripts between two or more different conditions based on RNA-seq data. In addition to the complexity of the data, it has been shown empirically that differentially expressed (DE) transcripts of a longer length are more likely to be identied than their shorter counterparts. While methods have been suggested to correct for this bias, we explore hierarchical Bayesian models with a negative binomial distributional assumption to address this length bias and identify differentially expressed transcripts. We dis- cuss how gene length is calculated and considered, and introduce a condition specic gene length. We also use gene length to create a zero inflated model to account for abundance of zero counts in RNA-Seq data. We use real data as well as a simulation study to show the benet of our approach, as well as address questions of over and under tting and the effect on classication of DE genes. Integrating Diverse Genomics Data to Infer Regulations \u0007Yuping Zhang1and Hongyu Zhao2 advances in high-throughput biotechnologies have generated unprecedented types and amounts of data for biomedical research. It is likely that integrating results from diverse experiments may lead to a more unied and global view of complex diseases such as cancer. In this talk, we will address statistical issues in data inte- gration and present a new statistical learning method for integrating diverse genomics data. Our method provides an integrated picture of commonalities and differences across tumor types. The perfor- mance of our method will be demonstrated through simulations and applications to real cancer data. Phylogenetic Trait Evolution with Drift \u0007Mandev Gill and Marc Suchard University of California, Los Angeles mandevgill@ucla.edu 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j67Abstracts An important issue in statistical phylogenetics is understanding the processes giving rise to quantitative characters associated with molecular sequence data. Examples of such characters include ge- ographic coordinates and phenotypic traits. A popular approach is to model trait evolution as a Brownian diffusion process acting on a phylogenetic tree. We extend this approach in a flexible Bayesian model that incorporates a nontrivial drift in the Brownian diffusion. We apply our model to viral data sets and demonstrate an improved ability to understand the dynamics of phenotypic drift and the spa- tiotemporal spread of epidemics. Session 35: Novel Designs and Applications of Adaptive Randomization in Medical Research Statistical Inference for Covariate Adaptive Randomized Clini- cal Lu Wang1, Jing Ning2and\u0007Hongjian Zhu1 1The University of Texas School of Public Health 2M. D. Anderson Cancer Center hongjian.zhu@uth.tmc.edu Covariate adaptive randomization (CAR) procedures such as the stratied permuted block randomization become standard in clin- ical trials. It is well known that the type I error rates for clinical trials using CAR are problematic if not all the design covariates are included in the data analysis. However, there is few research on the theoretical investigation for these trials with time to event outcomes. In this talk, we study clinical trials using CAR for randomization and the accelerated failure time model for analysis. We derive the asymptotic distribution of the test statistics and explicitly show the reason for the conservation of the type I error rate. We also pro- pose two approaches to protect the type I error rate and improve the efciency. Numerical studies support our theoretical ndings and demonstrate the advantages of the proposed method. Biomarker-Stratied Adaptive Basket Designs for Multiple Cancers Lorenzo Trippa Dana-Farber Cancer Institute ltrippa@jimmy.harvard.edu Advances in cancer research have shown that tumors have hetero- geneous genetic events, many of which are targetable by anticancer agents. Tests of treatment-biomarker interactions tend to be under- powered when done as secondary objectives in trials of drug ef- cacy. Clinical trials that make use of adaptive randomization and Bayesian prediction have been proposed as more efcient for inves- tigating multiple agents and predictive biomarkers. We proposed that clinical studies enrolling patients with multiple cancer modal- ities (CSMCM) would greatly contribute to statistical learning and accelerate the pace at which new drugs are studied. In silico sim- ulations to evaluate the benet of CSMCMs in a research portfolio require accurate parameters of (1) accession of patients by disease modality, and (2) joint prevalence of target gene mutations. At the Dana-Farber Cancer Institute (DFCI), a research study was initiated in 2011 to parallelize molecular proling with routine histopathol- ogy at diagnosis or disease progression, and to date has assayed >5000 patients across 11 disease centers. I present the design and characteristics of in silico studies parameterized from this cohort Outcome Adaptive Randomization for Comparative Effective- ness Clinical Trials Mei-Chiung Shih V A Cooperative Studies Programmei-chiung.shih@va.gov The goal of comparative effectiveness research (CER) is to support evidence-based choices of treatments. Currently comparative effec- tiveness trials are a small fraction of the totality of CER studies. The point-of-care (POC) comparative effectiveness trials, which aim to embed clinical research in routine care, have been proposed to ad- dress some of the challenges of randomized CER trials including cost, complexity, and large sample sizes due to small to moderate effect sizes. In this talk, we describe the use of outcome adaptive randomization to bring the benets of knowledge generated from the POC trial to improve health care without having to mount a sep- arate implementation strategy. This is in particular useful when the goal of the POC trial is to select treatments whose results are close to the best (yet unknown) available treatment. Worth Adapting? When and How to Apply Adaptive Random- ization to Make More Bang for the Buck \u0007J. Jack Lee and Yining Du M. D. Anderson Cancer Center jjlee@mdanderson.org Outcome adaptive randomization (AR) allocates more patients to the better treatments as the information accumulates in the trial. Is it worth it to apply AR in clinical trials? There are still controver- sies in the medical and statistical communities. Compare to equal randomization (ER), AR produces a higher overall response rate at the cost of larger sample size and with higher variability in the trial operating characteristics. However, improvements can be made to address these weaknesses. For example, adding a burn-in period of ER, applying a power transformation on the randomization proba- bility, or bounding the randomization probability by a clip method. The tradeoff of the numbers of patients on the trial and beyond the trials in the light of the total patient horizon is also examined. By carefully choosing the method and the tuning parameters, AR meth- ods can be tailored to strike a balance between achieving the desired statistical power, limit the increase sample size and its variability, and enhancing the overall response rate. Session 36: Lifetime Data Analysis Statistical Inference on Quantile Residual Life Jong Jeong University of Pittsburgh jjeong@pitt.edu The residual lifetime has a straightforward interpretation for the analysis results from time-to-event data. The quantile residual life function is desirable for summarizing a skewed time-to-event distri- bution, which is often encountered in reliability and survival data. In this talk, recent developments in statistical inference on the quan- tile residual life function with and without competing risks will be reviewed. Specic numerical examples will be presented to demon- strate how the methods work and the methods will be also illustrated with real examples based on clinical trial datasets. Onset Time of Chronic Pseudomonas Aeruginosa Infection in Cystic pseudomonas aeruginosa (PA) infection indicates lung function deterioration in children cystic brosis (CF) patients. Mod- eling the onset time of chronic PA infection is important for clini- 68j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts cians to devise better treatment plan and patient management. Due to the scheduled visits for patients in the cystic brosis foundation patient registry and the denition of chronic PA infection, the onset time is only known up to a certain interval. The analysis if fur- ther challenged by the need to allow some risk factors to have time- varying effects on the onset time. This problem ts into the frame- work of Bayesian dynamic Cox model for interval censored data recently developed by Wang, Chen, and Yan (2013, Lifetime Data Analysis 19:297-316). Application of the methodology to the onset time of chronic PA infection of children CF patients revealed in- teresting ndings. Compared with patients diagnosed via new born screening, patients diagnosed via meconium ileus or other symp- toms had moderately higher risks of acquiring PA infections within certain range of young age. Two cohorts of 5 years apart were com- pared, and patients in the more recent cohort were found to have lower risks of chronic PA infection before age 3. A Model for Time to Fracture with a Shock Stream Superim- posed on Progressive Degradation: the Study of Osteoporotic Fractures of Maryland Baltimore County xinhe@umd.edu Osteoporotic hip fractures in the elderly are associated with a high mortality in the rst year following fracture and a high incidence of disability among survivors. We study rst and second fractures of elderly women using data from the Study of Osteoporotic Frac- tures. We present a new conceptual framework, stochastic model, and statistical methodology for time to fracture. Our approach gives additional insights into the patterns for rst and second fractures and the concomitant risk factors. Our modeling perspective involves a novel time-to-event methodology called threshold regression, which is based on the plausible idea that many events occur when an un- derlying process describing the health or condition of a person or system encounters a critical boundary or threshold for the rst time. In the parlance of stochastic processes, this time to event is a rst hitting time of the threshold. The underlying process in our model is a composite of a chronic degradation process for skeletal health combined with a random stream of shocks from external traumas, which taken together trigger fracture events. Explained Variation in Correlated Survival Data Gordon Honerkamp-Smith and\u0007Ronghui Xu University of California, San Diego rxu@ucsd.edu Explained variation in survival data has attracted much attention in recent years. In this talk we consider explained variation in cor- related survival data, specically under the proportional hazards mixed-effects modeling (PHMM) of such data, which provides a natural setting for the decomposition of different sources of varia- tion. A motivation of the concept originated from genetic epidemi- ology. More generally, explained variation can be formulated in different ways, and we discuss the formulations that are often en- countered in the literature and that are interpretable in practice. We study the proposed measures both in theory and through simulation, and discuss some common pitfalls in using and understanding such measures in practice. To conclude, we show some interesting appli- cations to both multi-center clinical trials and recurrent events.Session 37: Statistical Methods for Large Computer Ex- periments Uncertainty Propagation using Dynamic Discrepancy for a Multi-scale Carbon Capture System \u0007K. Sham Bhat1, Curt 2West Virginia University 3URS Corporation bhat9999@lanl.gov Uncertainties from model parameters and model discrepancy from small-scale models impact the accuracy and reliability of predic- tions of large-scale systems. Inadequate representation of these un- certainties may result in inaccurate and overcondent predictions during scale-up to larger systems. Hence multiscale modeling ef- forts must accurately quantify the effect of the propagation of uncer- tainties during upscaling. Using a Bayesian approach, small-scale solid sorbent model to (TGA) data on a functional prole using chemistry-based priors. Crucial to this effort is the representation of model discrepancy, which uses a Bayesian Smoothing Splines (BSS-ANOV A) framework. Our un- certainty quantication (UQ) approach could be considered intru- sive as it includes the discrepancy function within the chemical rate expressions; resulting in a set of stochastic differential equations. Such an approach allows for easily propagating uncertainty by prop- agating the joint model parameter and discrepancy posterior into the larger-scale system of rate expressions. The broad UQ framework presented here could be applicable to virtually all areas of science where multiscale modeling is used. Bayesian Calibration of Computer Models with Informative Failures \u0007Peter Marcy and Curtis Storlie Los Alamos National Laboratory peter.marcy@gmail.com process emulators are widely used to calibrate determin- istic computer codes (simulators) to experimental/eld data. The goal of such an analysis is to determine which values of the physi- cal parameters used in the computer model are most consistent with the experimental observations. However, there may be incomplete simulated data as some simulators can fail to produce output for particular combinations of the inputs. Under the assumption that the failed model runs correspond to regions of the parameter space which are not physically feasible, the missing data can be incorpo- rated into a Bayesian calibration routine. In this talk I detail the procedure and then illustrate it using a computational fluid dynam- ics model for carbon capture. A Frequentist Approach to Computer Model Calibration \u0007Raymond K. W. Wong1, Curtis B. Storlie2and Thomas C. M. Lee3 1Iowa State University 2Los Alamos National Laboratory 3University of California, Davis raywong@iastate.edu We consider the computer model calibration problem and provide a general frequentist approach with uncertainty quantication. Under the proposed framework, the data model is semi-parametric with a nonparametric discrepancy function which accounts for any dis- crepancy between the physical reality and the simulator. In an at- tempt to solve the fundamentally important (but often ignored) iden- tiability issue between the computer model parameters and the dis- crepancy function, we propose a new and identiable parametriza- 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j69Abstracts tion of the calibration problem. We also develop a two-step pro- cedure for estimating all the relevant quantities under the new pa- rameterization. This estimation procedure is shown to enjoy excel- lent rates of convergence and can be straightforwardly implemented with existing software. For uncertainty quantication, bootstrap- ping is adopted to construct condence regions for the quantities of interest. The practical performance of the proposed methodology is illustrated through simulation examples and an application to a computational fluid dynamics model. Session 38: New Approaches for Analyzing Time Series Data Spectral Analysis of Linear Time Series in Moderately High Di- mensions Lili Wang1, Alexander Aue2and\u0007Debashis Paul2 1Zhejiang University 2University of Davis debpaul@ucdavis.edu We study the spectral behavior of p-dimensional linear processes in the moderately high-dimensional setting when both dimensionality pand sample size ntend to innity so that p=no 0. Under appropri- ate regularity conditions, it is shown that the empirical spectral dis- tributions of the renormalized symmetrized sample autocovariance matrices converge almost surely to a nonrandom limit distributions. The key structural assumption is that the linear process is driven by a sequence of p-dimensional real or complex random vectors with iid entries possessing zero mean, unit variance and nite fourth mo- ments, and that the coefcient matrices in the linear process rep- resentation are Hermitian and simultaneously diagonalizable. The results facilitate inference on model parameters and model diagnos- tics. High Order Corrected Estimator of Time-average Variance Constant \u0007Chun yip Yau and Kin Wai Chan The Chinese University of Hong Kong cyyau@sta.cuhk.edu.hk Estimation of time-average variance constant (TA VC), which is the asymptotic variance of the sample mean of a time series, is of fun- damental importance in statistical inference. In this paper, by con- sidering high order corrections to the asymptotic biases, we develop a new class of TA VC estimator that enjoys optimal convergence rate under different strength of dependence of the time series. Com- parisons to existing TA VC estimators are comprehensively investi- gated. In particular, the high order corrected estimator has the best performance in terms of mean squared error. Session 39: Statistica Sinica Special Invited Session on Spatial and Temporal Data Analysis Likelihood Approximations for Big Nonstationary Spatial Tem- poral Lattice Data \u0007Joseph Guinness and Montserrat Fuentes North Carolina State University jsguinne@ncsu.edu We propose a nonstationary Gaussian likelihood approximation for the class of evolutionary spectral models for data on a regular lattice. Lattice data include many important environmental data sources such as weather model output or gridded data products derived from satellite observations. The likelihood approximation is an extensionof the Whittle likelihood and is computationally efcient to evalu- ate when the evolutionary transfer function can be expressed in a flexible low-dimensional form. The low-dimensional form for the evolutionary transfer function is an attractive modeling framework since it allows the practitioner to build nonstationary models in a sequential manner and choose the appropriate dimension based on changes in approximate loglikelihood. While the transfer functions are low-dimensional, the resulting covariance matrices are gener- ally full rank, and thus no rank reduction is required for the compu- tational efciency of the methods. We study the covariance matrix implied by the likelihood approximation and give its asymptotic rate of approximation to the exact covariance matrix. We evaluate the likelihood approximation in a simulation study and show that it can produce asymptotically efcient parameter estimates when an op- eration similar to tapering is applied. We introduce an algorithm based on the Ising model to partition the domain into stationary subregions and show in a simulation that the methodscan reliably recover an unknown partition. We apply our modeling and estima- tion framework to analyze spatial-temporal output from a regional weather model comprised of 151,200 wind speed values, and we demonstrate that the tted covariances are consistent with local em- pirical variograms. A Multivariate Gaussian Process Factor Model Reach-to-Grasp Movements Lucia Castellanos1,\u0007Vincent Vu2, Sagi 2The Ohio State University 3University of Pittsburgh vqv@stat.osu.edu We propose a Multivariate Gaussian Process Factor Model to esti- mate low dimensional spatio-temporal patterns of nger motion in repeated reach-to-grasp movements. Our model decomposes and re- duces the dimensionality of variation of the multivariate functional data. We rst account for time variability through multivariate func- tional registration, then decompose nger motion into a term that is shared among replications and a term that encodes the variation per replica- tion. We discuss variants of our model, estimation al- gorithms, and we evaluate its performance in simulations and in data collected from a non-human primate executing a reach-to-grasp task. We show that by taking advantage of the repeated trial struc- ture of the experiments, our model yields an intuitive way to inter- pret the time and replication variation in our kinematic dataset. A Covariance Parameter Estimation Method for Polar-Orbiting Satellite Data \u0007Michael Horrell and Michael Stein University of Chicago horrellm87@gmail.com We consider the problem of estimating an unknown covariance function of a Gaussian random eld for data collected by a polar- orbiting satellite. The complex and asynoptic nature of such data re- quires a parameter estimation method that scales well with the num- ber of observations, can accommodate many covariance functions, and uses information throughout the full range of spatio-temporal lags present in the data. Our solution to this problem is to develop new estimating equations using composite likelihood methods as a base. We modify composite likelihood methods through the in- clusion of an approximate likelihood of interpolated points in the estimating equation. The new estimating equation is denoted the I- likelihood. We apply the I-likelihood method to 30 days of ozone 70j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts data occurring in a single degree latitude band collected by a polar orbiting satellite, and we compare I-likelihood methods to compet- ing composite likelihood methods. The I-likelihood is shown ca- pable of producing covariance parameter estimates that are equally or more statistically efcient than competing composite likelihood methods and to be more computationally scalable. Bayesian Analysis of high-dimensional functional responses utilizing multi- dimensional functional covariates is complicated by spatial and/or temporal dependence in the observations in addition to high- dimensional predictors. To utilize such rich sources of informa- tion we develop multi-dimensional spatial functional models that employ low-rank basis function expansions to facilitate model im- plementation. These models are developed within a hierarchical Bayesian framework that accounts for several sources of uncer- tainty, including the error that arises from truncating the innite- dimensional basis function expansions, error in the observations, and uncertainty in the parameters. We illustrate the predictive abil- ity of such a model through a simulation study and an application that considers spatial models of soil electrical conductivity depth proles using spatially dependent near-infrared spectral images of electrical conductivity covariates. Session 40: Use of Biomarker and Genetic Data in Drug Development An Overview of Statistical Methods in Biomarker Evaluation \u0007Dawei Liu, John Zhong, Kimberly Crimin, lakshmi Amaravadi, Lindborg and Donald Johns Biogen Idec dawei.liu@biogen.com Biomarkers have been playing an important role in drug discovery and development, biomedical research and clinical practice. Based on their utilities and applications, biomarkers can be classied into different categories, for example, pharmacodynamic markers, screening markers, diagnostic markers, prognostic markers, predic- tive markers, safety markers, and surrogate endpoints. The statis- tical methods used for biomarker evaluation are highly diverse in the sense that different types of markers require different sets of an- alytic techniques. Moreover, most of these methods are also very specialized as they are not part of the standard statistical toolbox. In this presentation, a broad overview of various statistical methods for biomarker evaluation will be given, with a focus on challenges and recent methodological developments in the assessment of prognos- tic, predictive and safety biomarkers, as well as surrogate endpoints. In the eld of neurodegenerative diseases, neuroimaging biomarkers have received increasing attention in drug development and clinical studies. In this talk statistical issues in the analysis of neuroimaging biomarkers wull also be reviewed. Toward the end of the presen- tation, the application of machine learning methods in biomarker evaluation will be discussed.A Case Study of Integrating Scientic Knowledge with Statisti- cal Biomarker Analysis Sheng Feng Biogen Idec sheng.feng@biogen.com Former ASA president Marie Davidian emphasized the importance of scientic training in statistical education, especially in the era of BIG DATA. In this presentation, I will introduce a case study, where science plays a key role in determining statistical analysis strategies. In designing a future clinical trial, scientists and clinicians asked if ApoE, a well-known gene associated with Alzheimer's disease (AD), should be used as a population stratication factor. Scientic knowledge of ApoE and how it was discovered being associated with AD helped the statistician to better understand the research question, better communicate with collaborators, better plan the sta- tistical analysis, and better organize and present results. Intratumor Genetic Heterogeneity Analysis and Its Implica- tions in Personalized Medicine \u0007Ronglai Sloan-Kettering Cancer Center shenr@mskcc.org heterogeneity the presence of geneti- cally and phenotypically distinct subclones of tumor cells within an individual tumor. Such genetic diversity within a tumor is increas- ingly recognized as a driver of rapid disease progression, resistance to targeted therapies, and poor survival outcome. It also has impor- tant implications in dening \"actionable\" driver genes for making treatment decisions. Inferring subclonal genetic alterations from whole-exome or whole-genome sequencing studies is often con- founded by tumor sample purity (normal cell contamination) and local copy number states. We present a statistical framework for inferring intratumor heterogeneity using whole-exome and whole- genome sequencing data. We demonstrate its performance in lung and breast cancer sequencing datasets. Session 41: New Frontier of Functional Data Analysis Making Patient-specic Treatment Decisions Based on Func- tional and Imaging York University 3Wright State University to166@columbia.edu A major goal of precision medicine is to use information gathered at the time that a patient presents for treatment to help clinicians determine, separately for each patient the particular treatment that provides the best expected outcome. In psychiatry it is thought that various brain imaging techniques may allow for the discovery of in- formation vital to predicting response to treatment. We will present the general problem of using both scalar and functional data to guide patient-specic treatment decisions and describe some approaches that can be used to perform model tting and variable selection. Quantifying Connectivity in Resting State fMRI with Func- of Delaware janelwang@ucdavis.edu 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j71Abstracts Resting state blood oxygen level dependent (BOLD) functional magnetic resonance imaging (fMRI) has begun to provide new sci- entic insights into the role of functional connectivity in brain net- work. Temporal Pearson correlation is frequently used to measure time series similarity, but it ignores statistical dependencies between time points in a time series, and in some situations leads to a biased group-level correlation measure. We consider BOLD fMRI time series at each voxel of a subject as time-discretized observations of a random function, and employ a functional data approach to evaluate functional connectivity. Two corrections for Pearson cor- relation will be discussed in this talk and compared with the Pear- son correlation in simulations and in a resting state fMRI study that compares functional connectivity between normal participants and Alzheimer's disease (AD) patients. Optimal Estimation for the Functional Cox \u0007Simeng Qu1, Jane-Ling Xiao covariates are common in many medical, biodemo- graphic,and neuroimaging studies. The aim of this paper is to study functional Cox models with right-censored data in the presence of both functional and scalar covariates. We study the asymptotic properties of the maximum partial likelihood estimator and estab- lish theasymptotic normality and efciency of the estimator of the nitedimensional estimator. Under the framework of reproducing kernel Hilbert space, the estimator of the coefcient function for a functional covariate achieves the minimax optimal rate of conver- gence under a weighted L2-risk. This optimal rate is determined jointly by the censoring scheme, the reproducing kernel and the co- variance kernel of the functional covariates. Implementation of the estimation approach and the selection of the smoothing parameter are discussed in detail. The nite sample performance is illustrated by simulated examples and a real application. Robust and Gaussian Adaptive Mixed Models for Correlated Functional Data, with Application to Event-Related Potential Data \u0007Hongxiao Zhu1and Jeffrey Morris2 1Virginia Tech 2M. D. Anderson Cancer Center hongxiao@vt.edu Event-related potential (ERP) data is a type of functional data with complex hierarchical structure and spatial correlation at the lowest level of hierarchy. Existing analytical methods focus on known or extracted features from prespecied time-windows and multivariate analysis, therefore could miss potentially interesting information. Motivated by ERP data in a cigarette-addiction study, we propose a general data analysis strategy that compares the effects of different stimuli on the ERP curves. This new strategy relies on the Bayesian functional mixed models (FMMs) which flexibly capture the com- plex data structure yet yield intuitive and natural inferential sum- maries that adjust for multiple testings. In particular, we generalize the Gaussian and the robust FMMs to incorporate channel-specic xed effects as well as spatial correlations at the lowest level of a hierarchy design. A correlated normal-exponential-gamma (CNEG) prior is assumed for the channel-specic xed effects, and a Matern structure with both separable and non-separable conguration is assumed for the spatial correlation. The proposed models are t- ted in the dual space of wavelet coefcients using discrete wavelet transform (DWT), and inference is performed in the data domainby applying the inverse DWT to the posterior samples. The pre- diction performance of the proposed models are compared through computing posterior predictive likelihoods on a validation dataset. In the posterior inference, based on the channel-specic xed ef- fects, we are able to flag out signicant regions on the contrast ef- fects using either Simultaneous Band Scores (SimBaS) or Bayesian false discovery rate (BFDR) based analysis, both adjusting for the family-wise error rate in the inherent multiple testing problem. The application to the ERP data shows different degrees of similarity between the cigarette and the emotional stimuli during the time pe- riod of 248-700ms. Within the period, the cigarette stimulus shows more similarity with the pleasant than with the unpleasant stimu- lus during 248-512ms, and shows similarity with both pleasant and unpleasant stimuli during 516-700ms. Prior or post the period of 248-700ms, cigarette and the emotional stimuli show different ef- fects on the ERP in contrast with the neural stimulus. As compared with the initial results of Versace et al. (2011), our proposed ap- proach provides more rened information about when and where the contrast effects are signicant. Session 42: New Methodology in Spatial and Spatio- Temporal Data Analysis Estimation of Spatial Variation in Disease Risk from Uncertain Locations Using SIMEX Dale Zimmerman University of Iowa dale-zimmerman@uiowa.edu The assignment of spatial locations to addresses of subjects in a spatial epidemiologic study, through a process known as geocod- ing, typically results in positional errors. Ignoring these errors in a statistical analysis may lead to biased estimators and incorrect con- clusions. This talk explores the utility of Simulation-Extrapolation (SIMEX) methods for accounting for positional errors in the estima- tion of spatial variation in risk from geocoded locations of disease cases and controls. The performance of SIMEX, relative to naively ignoring the positional errors, is investigated by simulation. The methodology is also applied to childhood asthma data from an Iowa county. Bayesian Estimates of CMB Gravitational Lensing Ethan Anderes University of California, Davis anderes@ucdavis.edu This talk will present a new Bayesian methodology for CMB lens- ing estimates. The quadratic estimator, developed by Hu and Oko- moto (2001, 2002), is the current state-of-the-art estimator of CMB lensing. Possibly the most promising alternative to the quadratic estimator is through the use of Bayesian methodology. Indeed, Bayesian techniques applied to the lensed CMB have the poten- tial to drastically changing the way lensing is estimated and used for inference. Current frequentest estimators of the lensing poten- tial treat the unlensed cosmic microwave background as a source of shape noise which is marginalized out. Conversely, a Bayesian lensing posterior treats the lensing potential and the unlensed cos- mic microwave background as joint unknowns, whereby obtaining scientic constrains jointly rather than marginally. Moreover, the posterior distribution is easier to interpret and sequentially update with additional data. From a statistical perspective, the lensing of the CMB is a perfect scenario for Bayesian methods in that both the observations and the unknown lensing potential are very nearly 72j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts Gaussian random elds. Physicists have known, for some time, that Bayesian methods could potentially provide next-generation lens- ing estimates. However, the main obstacle for naive Gibbs im- plementations is that iterations do not converge nearly fast enough to produce even approximate simulations. Our new results have shown that, indeed, there does exist a practical way to obtain Gibbs iterations which converge quickly. The solution is through a re- parameterization of cosmic microwave background lensing prob- lem. Instead of treating the lensing potential as unknown we work with inverse-lensing or an approximation we call anti-lensing. Sur- prisingly, the slowness of naive Gibbs translates to fast convergence of the re-parameterized Gibbs chain. Bayesian Functional Data Models for Coupling High- dimensional LiDAR and Forest Variables over Large Geo- graphic 2University of California, Los Angeles 3National Aeronautics and Space Administration finleya@msu.edu Recent advances in remote sensing, specically Light Detection and Ranging (LiDAR) sensors, provide the data needed to quantify for- est variables at a ne spatial resolution over large geographic do- mains. In this talk I will dene several Bayesian functional spatial data models for coupling high-dimensional and spatially indexed LiDAR signals with forest variables. The proposed modeling frame- works explicitly: 1) reduce the dimensionality of signals in an opti- mal way; 2) propagate uncertainty in parameters through to predic- tion, and; 3) acknowledge and leverage spatial dependence among the derived regressors and model residuals to meet statistical as- sumptions and improve prediction. Gaussian Processes (GPs) are applied in several model components where spatial dependence can be used to improve inference. Fitting such models requires ma- trix operations whose complexity increase in cubic order with the number of spatial locations resulting in a computational bottleneck. In each case, the dimensionality of the problem is tackled by re- placing the GP with its low-rank model counterpart. Two different approaches are explored for modeling LiDAR signals. The rst con- siders a non-separable spatial covariance function to capture within and among signal dependence, whereas the second accommodates dependence structures via a spatial factor model. The proposed frameworks are illustrated using LiDAR and spatially coinciding forest inventory data collected on the Penobscot Experimental For- est, Maine. Spatial Bayesian Hierarchical Model for Small Area Estimation of Categorical Data Xin Wang1, Emily Berg1,\u0007Zhengyuan Zhu1, Missouri-Columbia zhuz@iastate.edu The National Resource Inventory (NRI) survey is a large longitudi- nal survey to assess the status and change in soil, water, and other related natural resources in US. State and local stakeholders are in- terested in estimation of land cover at the county level to address local resource concerns. Though the NRI survey provide reliable es- timates at the state level, the direct survey estimators at the county level are unreliable due to small sample sizes. In this paper, we develop a spatial hierarchical Bayesian model to construct smallarea predictors of proportions for several mutually exclusive and exhaustive land cover classes. At the rst level, the design based estimators of the proportions are assumed to follow the Generalized Dirichlet distribution (GD). After proper transformation, the design based estimators is then modeled by beta regression. We consider a logit mixed model for the expectation of the beta distribution, which incorporates covariates through xed effects and spatial structure through a conditionally autoregressive (CAR) process. The covari- ates are derived from the Cropland Data Layer (CDL), a land cover map based on satellite data. The method is applied to NRI data, and the Bayesian small area estimators are shown to have smaller relative root mean squared error than design based estimators. Session 44: Funding Opportunities and Grant Applica- tions Funding Applications \u0007Debashis 1University Institutes of Health debashis.ghosh@ucdenver.edu; hulin wu@urmc.rochester.edu; heping.zhang@yale.edu; li.zhu@nih.gov This is a special session aims at providing junior statisticians with funding opportunities and strategies for successful grant applica- tions. Dr. Li Zhu, Mathematical Statistician and Prgram Director, in the Statistical Methodology and Applications Branch (SMAB), the National Cancer Institute, will talk about founding opportuni- ties and application and review procedures for biostatisticians. Pro- fessors Debashis Ghosh of Department of Biostatistics and Infor- matics, Colorado School of Public Health, University of Colorado, Hulin Wu of Department of Biostatistics and Computational Biol- ogy, University of Rochester, Heping Zhang of Department of Bio- statistics, Yale School of Public Health, will exchange their expe- riences in successfully getting research funding, such as survival strategies that include exploring different funding sources and fund- ing channels to support statistical methodology research and collab- oration practice. Session 45: Advances and Case Studies for Multiplicity Issues in Clinical Trials Condence Intervals for Multiple Comparisons Procedures Brian Wiens Portola Pharmaceuticals BMWiens@aol.com We consider condence intervals that correspond to common mul- tiple comparisons procedures. Test-based condence intervals (or inverted hypothesis tests) are not a new idea. In spite of some ap- pealing properties, they may be overshadowed by more recently proposed condence regions. Because testing for some endpoints is dependent on results of previously tested endpoints, the testing and therefore condence intervals are adaptive, and regions may be concave. We discuss advantages and disadvantages, and present an example. Composite Endpoints - Some Common Misconceptions \u0007David Li1and Jin Xu2 1Pzer Inc. 2015 Collins, Colorado, June 14-17 j73Abstracts 2Merck & Co. david.li1@pfizer.com A composite endpoint may be used in a clinical trial to combine the information from multiple components in a single outcome. The performance of the composite endpoint relative to the individual components of the endpoint may be counterintuitive. Some exam- ples will be discussed. Multiplicity Adjustment in Vaccine Efcacy Trial with Adaptive Population-Enrichment Design Shu-Chih Su Merck & Co. Shu-Chih Su@merck.com Adaptive design has the flexibility allowing pre-specied modi- cations to an ongoing trial to mitigate the potential risk associated with the assumptions made at the design stage. It allows studies to include broader target patient population and to evaluate the perfor- mance of vaccine/drug across subpopulations simultaneously. One of the most important statistical consideration, in adaptively de- signed clinical trials is the control of the overall study type I er- ror rate under adaptation. Our work is motivated by a Phase III event-driven vaccine efcacy trial. Two target patient populations are being enrolled with the assumption that vaccine efcacy can be demonstrated based on the two patient subpopulations combined. It is recognized due to the heterogeneity of the patient characteristics, the two subpopulations might respond to the vaccine differently. i.e., the vaccine efcacy in one population could be lower than that in the other. To maximize the probability of demonstrating vaccine efcacy in at least one patient population while taking advantage of combining two populations in one single trial, an adaptive de- sign strategy with potential population enrichment is developed. As there is no analytic form to present the overall study type I error rate in this setting, simulations were conducted to better accommodate the feature of population enrichment for adaptive design. Session 46: Recent Advances in Integrative Analysis of Omics Data A Bayesian Model for the Identication of Differentially Ex- pressed D. Anderson Cancer Center marina@rice.edu This talk will introduce a Bayesian hierarchical model for the identi- cation of differentially expressed genes in Daphnia Magna organ- isms exposed to chemical compounds. The proposed model con- stitutes one of the rst attempts at a rigorous modeling of the bio- logical effects of water purication. The model incorporates a vari- able selection mechanism for the identication of the differential expressions, with a prior distributionon the probability of a change that accounts for the available information on the concentration of chemical compounds present in the water. The model successfully identies a number of pathways that show differential expression between consecutive purication stages. We also nd that changes in the transcriptional response are more strongly associated to the presence of certain compounds, with the remaining contributing to a lesser extent. A Bayesian Approach to Biomarker Selection through miRNARegulatory James The availability of cross-platform, large-scale genomic data has enabled the investigation of complex biological relationships for many cancers. Identication of reliable cancer-related biomarkers requires the characterization of multiple interactions across com- plex genetic networks. MicroRNAs are small non-coding RNAs that regulate gene expression; however, the direct relationship be- tween a microRNA and its target gene is difcult to measure. We propose a novel Bayesian model to identify microRNAs and their target genes that are associated with survival time by incorporating the microRNA regulatory network through prior distributions. We assume that biomarkers involved in regulatory networks are likely associated with survival time. We employ non-local prior distribu- tions and a stochastic search method for the selection of biomarkers associated with the survival outcome. Using simulation studies, we assess the performance of our method, and apply it to experimen- tal data of kidney renal cell carcinoma (KIRC) obtained from The Cancer Genome Atlas. Our novel method validates previously iden- tied cancer biomarkers and identies biomarkers specic to KIRC progression that of North Carolina at Chapel Hill 2National Institutes of Health 3North Carolina State University weisun@email.unc.edu We have developed a statistical method named IsoDOT to assess differential isoform expression (DIE) and differential isoform usage (DIU) using RNA-seq data. Here isoform usage refers to relative isoform expression given the total expression of the corresponding gene. IsoDOT performs two tasks that cannot be accomplished by existing methods: to test DIE/DIU with respect to a continuous co- variate, and to test DIE/DIU for one case versus one control. The latter task is not an uncommon situation in practice, e.g., compar- ing the paternal and maternal alleles of one individual or comparing tumor and normal samples of one cancer patient. Simulation stud- ies demonstrate the high sensitivity and specicity of IsoDOT. We apply IsoDOT to study the effects of haloperidol treatment on the mouse transcriptome and identify a group of genes whose isoform usages respond to haloperidol treatment. Session 47: New Development in Nonparametric Meth- ods and Big Data Analytics A Nonparametric High-energy State 3University of California, Davis 4Imperial College tcmlee@ucdavis.edu 74j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts Variable-intensity astronomical sources are the result of complex and often extreme physical processes. Abrupt changes in source in- tensity are typically accompanied by equally sudden spectral shifts; i.e., in the wavelength distribution of the emission. In this work we develop a nonparametric spectral-temporal model for such high- energy astronomical data. It includes the automatic detection of emission lines and change points in the temporal direction. The L1 penalty is applied to regularize the model tting. The \"dimension\" of the best-tting model is chosen by a new form of the minimum description length principle that is designed for the \"large p small n\" scenario. This is joint work with Vinay Kashyap, David van Dyk and Raymond K. W. Wong. Multistage Adaptive Testing of Sparse Signals Wenguang Sun University of Southern California wenguans@marshall.usc.edu A common feature in large-scale scientic studies is that signals are sparse and it is desirable to narrow down the focus and iden- tify the signals in a sequential manner. In this talk, I discuss how to nd a subset that virtually contains all and only signals via mul- tistage adaptive testing (MAT). At each stage, the MAT procedure aims to simultaneously eliminate noise, localize signals and elect units for the next stage analysis. We develop a sequential compound decision-theoretic framework for multistage simultaneous inference and propose an MAT procedure based on the sequential probability ratio test (SPRT). It is shown that the MAT procedure based on the SPRT controls both the false positive rate and missed discovery rate at the nominal level and minimizes the total sampling efforts. Variable Selection for Sufcient Dimension Reduction using Weighted Leverage Score Wenxuan Zhong University of Georgia wenxuanzhong@yahoo.com Sufcient dimension reduction is a very important data exploratory tool in big data analysis. As the rapid development of information technology, there is a high demand for novel sufcient dimension reduction methods which can help us extract information from data with complicated structure, particularly, sparse structure. In this talk, we will discuss a simple variable selection strategy for esti- mating the sparse sufcient dimension reduction model based on the weighted leverage score. The weighted leverage score is a variant of the leverage score that has been widely used for the diagnostic of linear regression. As demonstrated by our early bio-threat detection example, our method can quickly pin down some genomic markers that are the reliable indicators of certain infectious disease. Efcient Computation of Smoothing Splines via Adaptive Basis of nonparametric high computational cost of smoothing splines for large data sets has hindered their wide application. In this arti- cle, we develop a new method, named adaptive basis sampling, for efcient computation of smoothing splines in super-large samples. Except for univariate case the Reinsch algorithm is ap- plicable, a smoothing spline for a regression problem with sample sizencan be expressed as a linear combination of n basis functions and its computational complexity is generally O(n3). We achievea more scalable computation in the multivariate case by evaluating the smoothing spline using a smaller set of basis functions, obtained by an adaptive sampling scheme that uses values of the response variable. Our asymptotic analysis shows that smoothing splines computed via adaptive basis sampling converge to the true function at the same rate asfull basis smoothing splines. Using simulation studies and a large-scale deep earth core-mantle boundary imaging study, we show that the proposed method outperforms a sampling method that does not use the values of response variable. Session 48: Trends and Innovation in Missing Data Sen- sitivity Analyses Missing Data Sensitivity Analyses for Continuous Endpoints Using Controlled Imputations Craig Mallinckrodt Eli Lilly and Company cmallinc@lilly.com Recent research has fostered new guidance on conducting sensi- tivity analyses in clinical trials. The controlled imputation fam- ily of sensitivity analyses, that includes reference-based and delta- adjustment methods are rapidly gaining acceptance for continuous endpoints, and work is ongoing to extend these principles and meth- ods to binary and time to event time endpoints. This session will begin by explaining and illustrating controlled-imputation sensitiv- ity analyses for continuous endpoints, and how these methods can be used to support inferences from the primary analysis. Sensitivity Analysis for Time-to-event Endpoints Analyses of time-to-event data can be challenged with respect to their robustness to censored data when subjects leave the study prior to experiencing an event of interest and prematurely withdraw from treatment and/or follow-up. Several approaches for analysis of continuous and categorical endpoints with non-ignorable miss- ingness (such as control-based imputation, delta-adjustment, and tipping point analysis) have been gaining acceptance in the clinical trial community because of their clinically meaningful and clearly interpretable ways to stress-test the assumption of ignorable miss- ingness. We will discuss how these strategies can be adapted for stress-testing an assumption of ignorable censoring typically used in the time-to-event analysis of clinical trials. We will present sev- eral methods for conducting such analyses based on multiple im- putation with time-to-event data using parametric, semi-parametric, and non-parametric imputation models for survival. We will illus- trate the results using a real-world dataset and share some insights about performance of these methods based on a simulation study. Analysis and Sensitivity Analysis of Incomplete Categorical Data Geert Molenberghs1;2 1Universiteit Hasselt 2Katholieke Universiteit Leuven geert.molenberghs@uhasselt.be Incomplete data are prominent in many areas of empirical research. While a lot of work has been done, many of it is in the context of continuous data. It is helpful to review methods for incomplete categorical data. Not only analysis methods are presented, also sen- sitivity analysis methodology is reviewed. Emphasis is placed on: 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j75Abstracts directed likelihood, inverse probability weighting, multiple imputa- tion, and pattern-mixture-based sensitivity analysis. Session 49: Multi-Regional Clinical Trial Design and Analysis Design and Analysis of Multiregional Clinical Trials in Evalua- tion of Medical Devices: A Two-component Bayesian Approach for Targeted Regulatory Decision Making \u0007Yunling Xu and Nelson Lu U.S. Food and Drug Administration yun-ling.xu@fda.hhs.gov Current statistical design and analysis of multiregional clinical trials generally follows a paradigm where the treatment effect of interest is assumed consistent among US and OUS regions. In this presen- tation, we discuss the situations where the treatment effect might vary among US and OUS regions, and illustrate a two-component Bayesian approach for regulatory decision making. In this ap- proach, anticipated treatment difference among US and OUS re- gions is formally taken into account, hopefully leading to increased transparency and predictability of local regulatory decision-making. Assessing Benet and Consistency of Treatment Effect under a Discrete Random Effects Model in Multiregional Clinical Trials \u0007Hsiao-Hui 3National Tsing Hua University tsouhh@nhri.org.tw In recent years, developing pharmaceutical products via a multire- gional clinical trial (MRCT) has become standard. Traditionally, an MRCT would assume a xed effects model. However, hetero- geneity among regions may have impact upon the evaluation of a medicine's effect. In this study, we consider a random effects model using discrete priors (DREM) to account for heterogeneous treat- ment effects across regions for the design and evaluation of MRCTs. We derive power for a treatment is benecial under DREM and il- lustrate determination of the overall sample size in an MRCT. We use the concept of consistency based on Method 2 of the Japanese Ministry of Health, Labour and Welfare guidance to evaluate the probability for treatment benet and consistency under DREM. We further derive an optimal sample size allocation over regions to max- imize the power for consistency. In practice, regional treatment ef- fects are unknown. Thus, we provide some guidelines on the design of MRCTs with consistency when the regional treatment effect are assumed to fall into a specied interval. Numerical examples are given to illustrate the applications of the proposed approach. Multi-Regional Clinical Trials - Where We Have Been and Where We Are Going Bruce Binkowitz Merck & Co. binkowitz@merck.com The topic of multi-regional clinical trials has now matured into an active area of research among statisticians and other disciplines. Regulators have long recognized the importance of this issue, but we are now seeing some of the rst movement toward formal guid- ances. This session will review how the topic of MRCT has grown, some of the current activities, and propose areas where MRCT re- searchers can continue to investigate.Session 50: Biostatistics and Health Sciences When to Initiate Combined Antiretroviral Therapy in HIV- infected Individuals to Reduce the Risk of AIDS or Severe Non- AIDS Morbidity Using Marginal Structural Model Background:The optimal CD4 cell count at which the combined an- tiretroviral therapy (cART) should be initiated is still a matter of debate. Clinical guidelines from the European AIDS Clinical Soci- ety recommend initiating cART when CD4 cell count has decreased to less than 350 cells/ mul, the threshold recommended by the World Health Organization is 500 cells/ mul, whereas U.S. guidelines rec- ommend initiating cART regardless of their CD4 cell count. These variations in clinical recommendations reflect the uncertainty of available evidence. The primary objective of the study was to com- pare two strategies of cART initiation: \"start cART when CD4 cell count rst drop below 500 cells/ muL\" versus \"start cART when CD4 cell count rst drop below 350 cells/ muL\" to reduce the risk of AIDS-dening event or severe non-AIDS dening event or death. Methods: From the FHDH-ANRS CO4 cohort, we selected therapy- naive HIV1-infected individuals included between 1997 and 2012, at 15 years of age or older, with no history of AIDS-dening event and baseline CD4 cell counts at or above 500 cells/ mul. A further requirement was an inclusion at least one year before the closing date. We used marginal structural models to estimate the relative causal effect of the two strategies in the presence of time-dependent confounders. The primary endpoint was an AIDS- dening severe non-AIDS dening event leading to hos- pitalization or death and the secondary endpoint was an AIDS- dening event or death. The hazard ratios were estimated in an ex- panded dataset using inverse probability weighted Cox proportional hazards model. Stabilized weights were truncated at a maximum value of 10 for statistical efciency. The method assumes that all relevant confounders were measured. The models included the fol- lowing (15-34,35-49, months from baseline to rst CD4 cell count below 500 cells/ mul, and time since the beginning of follow- up (restricted cubic splines with 4 knots month). Results: A total of 9389 patients met the eligibility criteria for the study. Among these patients, 4623 follows the strategy \"CD4 cell count below 500 cells/ mul\" experiencing 668 primary end- points (579 non-AIDS, 78 AIDS, 11 deaths) and 4607 follows the strategy \"CD4 cell count below 350 cells/ mul\" experiencing 851 primary endpoints (676 non-AIDS, 163 AIDS, 12 deaths). Com- pared to initiating cART at the CD4 cell count threshold of 500 cells/mul, the primary endpoint hazard ratio was 1.04 (0.92-1.18) for the 350 cells/ mul threshold. The corresponding hazard ratio was 1.27 (0.93-1.74) for the secondary endpoint. Conclusion: We did not observe a signicant difference between initiating cART when the CD4 cell count decreases below 500 cells/mul or delaying cART initiation until the CD4 cell count de- creased below 350 cells/ mul for AIDS- dening event or severe non-AIDS dening event or death. How- ever our ndings tend to support cART initiation once the CD4 cell count threshold decreased below 500 cells/ mul in order to increase 76j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts AIDS-free survival. Trace Elements Uptake and Effect of Two Steppic Medicinal Species of a Mining Area on Their Soil Trace Element Contents versus Bulk Soils alba Asso Thymus algeriensis Boiss. & Reut) appeared, in pre- vious studies, able to grow on the most polluted soils by several trace metals. This study aims to determine if the two species had dif- ferent effects on the intrinsic characteristics of their soils which can result in metal accumulation differences in these ones and in plant tissues. For this purpose seven stations were randomly selected in the mining area. In each one the aerial parts of two specimens of T. algeriensis and of A. herba alba were taken. The two species rhizosphere soils and adjacent bulk soils were also sampled. Cad- mium (Cd), copper (Cu), lead (Pb) and zinc (Zn) were measured in soil and plant extracts by flame atomic absorption spectrome- ter (FAAS). Soils were also the subject of organic matter (OM), pH, electrical conductivity (EC), active limestone (AC), total cal- cium (TC) and available phosphorus (AP ) analysis. The analysis of variance (ANOV A) revealed signicant differences between the two species soils and between those and bulk soils, from the point of view of the pH, OM and AC contents. They result in different effects on their Cd, Pb and Zn soil contents and on the two species metal uptake. Study of the Effect of Trace Metals from Old Antimony Mine on Biodiversity by Stepwise Regression. \u0007Alima de Biotechnologie alima.bentellis@yahoo.fr A previous study on the effect of pollution by trace metals of an old mine antimony on contamination of the mining area wadi bank soils showed that they contained high concentrations of arsenic and anti- mony and were contaminated with other potentially toxic elements, including zinc and lead.This study aims, using the stepwise regres- sion, whether antimony, had a major effect, compared to other trace elements and/or physicochemical soil factors, on the Biodiversity of the mining area. Results of the linear stepwise regression analysis (forward) of the diversity index and the floristic richness based on all trace metals studied, the distance to the mine and the distance to the road, adjusted on two soil factors of rotation, show that the ef- fect of these two factors is not signicant for the diversity index and that only the distance to the mine, and the cobalt and chromium soil contents are involved in the diversity index prediction. On the other side, only the soil intrinsic characteristics set has a signicant effect on the floristic richness and under this effect, there are only arsenic, distance to the road, copper, zinc and cadmium which signicantly enter in its prediction. Analysis of the forward selection (forward) synthesis of the distance to the mine and soil antimony concentra- tion as a function of species presence adjusted on two edaphic fac- tors, the distance to the road and/or the distance to the mine shows that for the two syntheses the effect of the two rotation factors and the distance to the road is very signicant. Under this effect, several groups of plant species emerge by their absence/presence accord- ing to their associations with the distance to the road and/or the soilantimony concentrations. Session 51: Recent Developments in Analyzing Censored Survival Data Stacking Survival Models Debashis Ghosh University of Colorado at Denver debashis.ghosh@ucdenver.edu In many clinical prediction settings and other machine learning con- texts, there is interest in combining multiple models. Stacking is an approach coined by Wolpert but which has roots in statistics dat- ing back to the early 1970s. It involves constructing combinations of predictions from various modelling and algorithmic approaches. This approach also belies the current \"super learning\" approach of Van der Laan and collaborators. In this talk, we revisit the idea of stacking and discuss the roles of two components: residuals and principal components. Using a toy scenario, we are able to reveal insights in stacking that provide some guidance as to the types of models that should be combined. We then discuss the approach with censored data and illustrate the methodology with some simulated and real data examples. Estimation of Concordance Probability with Censored Regres- sion Models \u0007Zhezhen Jin and Xinhua Liu Columbia University zj7@columbia.edu In this talk, evaluation and comparison of various methods often arise in medical research. The concordance probability can be used to assess the discriminatory power of censored regression models. In this talk, we present the estimation of concordance probability with various censored regression models in the analysis of right cen- sored data. Improving Efciency in Biomarker yzheng@fhcrc.org Cost-effective yet efcient designs are critical to the success of biomarker evaluation research. Two-phase sampling designs, un- der which expensive markers are only measured on a subsample of cases and non-cases, are useful in novel biomarker studies for pre- serving study samples and minimizing cost of biomarkerassaying. Statistical methods for quantifying the predictiveness of biomark- ers under two-phase studies have been recently proposed (Cai and Zheng, 2012; Liu and others, 2012), based on a class of inverse probability weighted (IPW) estimators where weights are 'true' sampling weights that simply reflect the sampling strategy of the study. While simple to implement, one major limitation of these existing IPW estimators is lack of efciency. We investigate a vari- ety of two-phase design options and provide statistical approaches aimed at improving the efciency of simple IPW estimators by in- corporating auxiliary information available for the entire cohort. we consider accuracy summary estimators that accommodate auxiliary information in the context of evaluating the incremental values of novel biomarkers over existing prediction tools and provide formal theoretical justications in terms of consistency and weak conver- gence. In addition, we evaluate the relative efciency of a variety of sampling and estimation options under both types of two-phase 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j77Abstracts studies, to shed light on issues pertaining to both the design and analysis of biomarker validation studies. Nonparametric Tests of Treatment Effect for a Recurrent Event Process that Terminates Nabihah Tayob1and\u0007Susan Murray2 1M. D. Anderson Cancer Center 2University of Michigan skmurray@umich.edu Recurrent and terminal events are common outcomes for studying treatment effects in clinical studies. Existing approaches follow ei- ther a time-to-rst event analysis approach or a recurrent event mod- eling approach. Recurrent event analyses are often restricted by in- dependence assumptions on gap-times between events. Although time-to-rst event analyses are not subject to this restriction, this analysis discards information that occurs beyond the initial event and is much less powerful for detecting treatment differences. We develop two new approaches for this data structure, motivated by less restrictive assumptions of time-to-rst event analyses, that com- bine information from multiple follow-up intervals in determining treatment effects. Each approach follows behavior of short term outcomes during pre-specied intervals over time. The rst test- ing procedure pools (correlated) short term au-restricted outcomes from pre-specied intervals starting at times tk; k= 1; :::; b , and compares estimated au-restricted mean survival across treatment groups from this combined dataset. The second procedure calcu- lates conditional au-restricted means from those at risk at times tk; k= 1; :::; b , and compares the area under a function of these by treatment. Variances calculations, taking into account correlation of short-term outcomes within individuals, linearize random com- ponents of the test statistics following Woodruff (1971) and more recently Williams (1995). Simulations compare the nite sample performance of our tests to the robust proportional rates model pro- posed by Lin et al. (2000) and the Ghosh and Lin (2000) test for re- current events subject to death. In treatment effect patterns follow- ing proportional hazards, delayed treatment effect, short duration treatment effect and moderate duration effect the proposed methods perform favorably when compared to existing methods. These new analysis approaches also produce correct type I error rates when gap-times between events are correlated. The analysis approach is illustrated in data from a randomized trial of azithromycin in pa- tients with chronic obstructive pulmonary disease (COPD). Session 52: Advances in Survey Statistics Quantile Regression Imputation for a Survey Sample Emily Berg and\u0007Cindy Yu Iowa State University cindyyu@iastate.edu We study an imputation method based on quantile regression for a complex survey design. The imputed data are simulated from an estimate of the conditional quantile function, evaluated at observed values of covariates. The quantile regression imputation procedure relies on fewer assumptions than procedures that involve specica- tion of full distributions. In a simulation, linearization and boot- strap variance estimators are evaluated, and the quantile regression imputation method is compared to parametric fractional imputation. Informative and non-informative sample designs are considered. Triply Robust Inference in the Presence of Missing Item nonresponse typically treated by some form of single impu- tation in statistical agencies. For example, deterministic regression imputation that includes ratio and mean imputation within classes as special cases, is widely used in surveys. Recently, there has been an interest in doubly robust imputation procedures. An imputation procedure is said to be doubly robust wen the resulting imputed es- timator is consistent if either the imputation model (also called the outcome regression model) or the nonresponse model is correctly specied. However, in the presence of influential units, the result- ing imputed estimator may be potentially very unstable. We pro- pose a robust version of the doubly robust imputed estimator based on the concept of conditional bias of a unit. Implementation of the proposed method via a calibrated imputation procedure will be dis- cussed. Finally, the results from an empirical study will be shown. Adaptive Post-stratication Using Monotonicity Constraints \u0007Jean Opsomer, Jiwen Wu and Mary Meyer Colorado State University jopsomer@stat.colostate.edu In large-scale government surveys, it is common that estimates are desired for numerous small domains. Unfortunately, even for sur- veys of very large overall size, in many domains the sample size is often too small to ensure that the direct survey estimates are suf- ciently reliable to be released. A solution to this problem is to adap- tively collapse some neighboring domains to ensure sufcient sam- ple size in all estimation domains, but this is both time-consuming and heuristic. We propose a method to adaptively pool neighboring domains, which is suitable for situations in which it is reasonable to assume that the estimates in the domains are monotone. This situa- tion is not uncommon practice, and we give some examples during the talk. The method is based on ideas from isotonic regression, and we describe the case with a single covariate dening the domains, for which we propose a weighted version of the pooled adjacent vi- olator algorithm (PA V A). The resulting estimator is equivalent to an adaptively post-stratied estimator, with the post-strata chosen so that the domain estimates satisfy the monotonicity constraint while providing weights that can be applied to any survey variable and do- main of interest. We describe the asymptotic design properties, in- cluding design consistency and asymptotic distribution, of the pro- posed domain estimator and we illustrate its nite sample behavior in simulations. Finally, we discuss variance estimation using repli- cation methods. Session 53: Innovative Statistical Methods in Genomics and Genetics Statistical Analysis of Differential Alternative Splicing using RNA-Seq Data Mingyao Li University of Pennsylvania mingyao@mail.med.upenn.edu RNA sequencing (RNA-seq) allows an unbiased survey of the en- tire transcriptome in a high-throughput manner. It has rapidly re- placed microarrays as the major platform for transcriptomics stud- ies. A major application of RNA-seq is to detect differential al- ternative splicing (DAS), or transcript usage, across experimental conditions. Differential analysis at the transcript level is of great biological interest due to its direct relevance to protein function and disease pathogenesis. However, DAS analysis using RNA-seq data 78j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts is challenging because of the difculty of quantifying alternative splicing and various biases present in RNA-seq data. In this talk, I will present several statistical issues related to the analysis of DAS. I will discuss methods for detecting DAS for both paired and un- paired data, and compare the performance of exon-based and gene- based tests of DAS. I will show simulation results as well as some examples from real transcriptomics studies. Integrating Auxillary Information in Complex Traits Studies Marc Coram, Sophie Candille and\u0007Hua Tang Stanford University huatang@stanford.edu Genome-wide association studies (GWAS) have become a standard approach for identifying loci influencing complex traits. However, GWAS in non-European populations are hampered by limited sam- ple sizes and are thus underpowered. We describe an empirical Bayes approach, which improves the power for mapping complex trait loci in a minority population by adaptively integrating infor- mation from another ethnic population. Extending this approach, we discuss methods for model selection and genetic risk prediction. Detecting Nonlinear Associations in High-throughput Data with Applications Clustering and Variable Selection \u0007Tianwei Yu and Hesen Peng Emory University tianwei.yu@emory.edu High-throughput expression technologies measure thousands of fea- tures, i.e. genes or metabolites etc., on a continuous scale. In such data, both linear and nonlinear relations exist between fea- tures. Nonlinear relations can reflect critical regulation patterns in the biological system. Howeverthey are not identied and utilized by traditional methods based on linear associations. We developed a sensitive nonparametric measure of general dependency between (groups of) random variables in high-dimensions. Based on this de- pendency measure, we developed new clustering methods to unravel structures of the data, as well as a stage-forward variable selection scheme to select features that are nonlinearly associated with the continuous outcome variable. We evaluated the methods using sim- ulations and real data analysis. Hypothesis Test of Mediation Effect in Causal Mediation Model with High-dimensional Mediators \u0007Yen-Tsung Huang and Wen-Chi Pan Brown University Yen-Tsung Huang@brown.edu Causal mediation modeling has become a popular approach for studying the effect of an exposure on an outcome through a media- tor. However, current methods can not be applicable to the setting with a large number of mediators. We propose a testing procedure for mediation effects of high-dimensional mediators. We character- ize the marginal mediation effect, the multivariate component-wise mediation effects and the L2 norm of the component-wise effects, and develop a Monte-Carlo procedure for evaluating their statistical signicance. To accommodate the setting with a large number of mediators and a small sample size, we furhter propose a transfor- mation model using the spectral decomposition. Under the trans- formation model, mediation effects can be estimated using a series of regression models with a univariate transformed mediator, and examined by our proposed testing procedure. Extensive simulation studies are conducted to assess the performance of our methods for continuous and dichotomous outcomes. We apply our methods to analyze genomic data investigating the effect of microRNA miR- 223 on the dichotomous survival status of patients with glioblas-toma multiforme (GBM). We identify nine gene ontology sets with expression values that signicantly mediate the effect of miR-223 on GBM survival. Session 54: Recent Development in Epigenetic Research A Hidden Markov Random Field Based Bayesian Method for the Detection of Long-range Chromosomal 1The University of North Carolina at Chapel Hill 2Case Western Reserve University 3New York University ming.hu@nyumc.org Motivation: Advances in chromosome conformation capture and next-generation sequencing technologies are enabling genome-wide investigation of dynamic chromatin interactions. For example, Hi- C experiments generate genome-wide contact frequencies between pairs of loci by sequencing DNA seg-ments ligated from loci in close spatial proximity. One essen-tial task in such studies is peak calling, that is, detecting non-random interactions between loci from the two-dimensional contact frequency matrix. Successful fulllment of this task has many important implications including identifying long-range interactions that assist interpreting a sizable fraction of the results from genome-wide association studies. The task dis-tinguishing biologically meaningful chromatin interactions from massive numbers of random interactions poses great chal- lenges both statistically and computationally. Model-based methods to address this challenge are still lacking. In particu-lar, no statisti- cal model exists that takes the underlying de-pendency structure into consideration. Results: In this paper we propose a hidden Markov random eld (HMRF) based Bayesian method to rigorously model in-teraction probabilities in the two-dimensional space based on the contact frequency matrix. By borrowing information from neigh- boring loci pairs, our method demonstrates superior reproducibility and statistical power in both simulation studies and real data analy- sis. Base-resolution Methylation Patterns Accurately Predict Tran- scription Factor Bindings In Vivo \u0007Tianlei Xu, Ben Li, Meng Zhao, Keith E. Szulwach, R. Craig Street, Li Lin, Bing Yao, Feiran Zhang, Peng Jin, Hao Wu and Zhaohui Qin Emory University tianlei.xu@emory.edu Detecting in vivo transcription factor (TF) binding is important for understanding gene regulatory circuitries. ChIP-seq is a powerful technique to empirically dene TF binding in vivo. However, the multitude of distinct TFs makes genome-wide proling for them all labor-intensive and costly. Algorithms for in silico prediction of TF binding have been developed, based mostly on histone modi- cation or DNase I hypersensitivity data in conjunction with DNA motif and other genomic features. However, technical limitations of these methods prevent them from being applied broadly, espe- cially in clinical settings. We conducted a comprehensive survey involving multiple cell lines, TFs, and methylation types and found that there are intimate relationships between TF binding and methy- lation level changes around the binding sites. Exploiting the con- nection between DNA methylation and TF binding, we proposed a novel supervised learning approach to predict TF-DNA interac- tion using data from base-resolution whole-genome We devised beta-binomial models to charac- 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j79Abstracts terize methylation data around TF binding sites and the background. Along with other static genomic features, we adopted a random for- est framework to predict TF-DNA interaction. After conducting comprehensive tests, we saw that the proposed method accurately predicts TF binding and performs favorably versus competing meth- ods. Statistical Analysis of Illumina HumanMethylation450 BeadAr- rays \u0007Jie Liu and Kimberly Siegmund University of Southern California kims@usc.edu DNA methylation is a commonly studied epigenetic mark, its im- portance day, HumanMethylation450 arrays provide the most cost-effective means of high-throughput DNA methylation analysis. As with other types of microarray platforms, technical artifacts are a concern. I will introduce the Illumina HumanMethylation450 array, discuss approaches to assess data quality, and methods for signal processing. I will show a combination of within-array normaliza- tion steps that can improve between-array reproducibility as well as, or better than, other published approaches. The methods will be illustrated using replicate controls and biological samples. I will conclude with a discussion of the variety of statistical approaches applied in DNA methylation association studies. Differential Methylation Analysis for BS-seq Data eral sequencing (BS-seq) has emerged as the technology of choice to prole DNA methylation because of its accuracy, genome coverage, and resolution. Methods to identify differentially methy- lated (DML) or regions (DMRs) from BS-seq data are avail- able, but mostly designed to work for two-group comparison. We develop a novel statistical model, based on a beta-binomial regres- sion model with arcsine link function, to detect DML from BS-seq data under general experimental design. Simulation and real data analyses demonstrate that our method is accurate, powerful, robust and computationally efcient. The method is implemented in Bio- conductor package DSS. Session 55: New Method Development for Survival Anal- ysis Analysis of the Proportional Hazard Model for with Sparse Longitudinal Covariates \u0007Hongyuan Cao1, of Chicago 3The University of North Carolina at Chapel Hill caohong@missouri.edu Regression analysis of censored failure observations via the pro- portional hazards model permits time-varying covariates which are observed at death times. In practice, such longitudinal covariates are typically sparse and only measured at infrequent and irregularly spaced follow-up times. Full likelihood analyses of joint models for longitudinal and survival data impose stringent modelling as- sumptions which are difcult to verify in practice and which arecomplicated both inferentially and computationally. In this article, a simple kernel weighted score function is proposed with minimal assumptions. Two scenarios are considered: half kernel estimation in which observation ceases at the time of the event and full ker- nel estimation for data where observation may continue after the event, as with recurrent events data. It is established that these es- timators are consistent and asymptotically normal. However, they converge at rates which are slower than the parametric rates which may be achieved with fully observed covariates, with the full kernel method achieving an optimal convergence rate which is superior to that of the half kernel method. Simulation results demonstrate that the large sample approximations are adequate for practical use and may yield improved performance relative to last value carried for- ward approach and joint modelling method. The analysis of the data from a cardiac arrest study demonstrates the utility of the proposed methods. Hypoglycemic Events Analysis via Recurrent Time-to-Event (HEART) Models Haoda Fu Eli Lilly and Company fuhaoda@gmail.com Diabetes affects an estimated 25.8 million people in the United States and is one of the leading causes of death.A major safety concern in treating diabetes is the occurrence of hypoglycemic events. Despite this concern, the current methods of analyzing hy- poglycemic events, including the Wilcoxon rank sum test and nega- tive binomial regression, are not satisfactory. The aim of this paper is to propose a new model to analyze hypoglycemic events with the goal of making this model a standard method in industry. Our method is based on a gamma frailty recurrent event model. To make this method broadly accessible to practitioners, this paper provides many details of how this method works and discusses practical is- sues with supporting theoretical proofs. In particular, we make ef- forts to translate conditions and theorems from abstract counting process and martingale theories to intuitive and clinical meaningful explanations. For example, we provide a simple proof and illustra- tion of the coarsening at random condition so that the practitioner can easily verify this condition. Connections and differences with traditional methods are discussed, and we demonstrate that under certain scenarios the widely used Wilcoxon rank sum test and nega- tive binomial regression cannot control type 1 error rates while our proposed method is robust in all these situations. The usefulness of our method is demonstrated through a diabetes dataset which pro- vides new clinical insights on the hypoglycemic data. Accelerated Intensity Frailty Model for Recurrent Events Data Bo Liu1, Wenbin Lu1and\u0007Jiajia Zhang2 1North Carolina State University 2University of South Carolina jzhang@mailbox.sc.edu In this article we propose an accelerated intensity frailty (AIF) model for recurrent events data and derive a test for the variance of frailty. In addition, we develop a kernel-smoothingbased EM algorithm for estimating regression coefcients and the baseline in- tensity function. The variance of the resulting estimator for regres- sion parameters is obtained by a numerical differentiationmethod. Simulation studies are conducted to evaluate the nite sample per- formance of the proposed estimator under practical settings and demonstrate the efciency gain over the Gehan rank estimator based on the AFT model for counting process. Our method is further il- lustrated with an application to a bladder tumor recurrence data. 80j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts A New Flexible Association Measure for Semi-Competing Risks Data \u0007Jing Yang and Limin Peng Emory University jyang89@emory.edu In the semi-competing risks setting, it is often of interest to assess the impact of the non-terminalevent on the terminal event, which may help understand the role of some important disease landmark to reveal the disease progression. In this work, we propose a new \"impact\" measure which is well tailored to the data structure of semi-competing risks and can accommodate the exploration of the potential changing pattern of such impact in the identiable region. We develop a nonparametric estimation procedure for the proposed measure by adopting a working quantile residual lifetime model. The estimation method can readily be extended to adjust for con- founders. We establish the asymptotic properties of the proposed es- timator and develop inferences accordingly. The proposed methods can be implemented based on standard statistical software without involving smoothing or resampling. Our proposals are illustrated via simulation studies and an application to real data. Session 56: Recent Developments in Statistical Learning Methods Multiclass Sparse Discriminant Analysis \u0007Qing Mai1, Yi State University 2University of Minnesota mai@stat.fsu.edu In recent years many sparse linear discriminant analysis methods have been proposed for high-dimensional classication and variable selection. However, most of these proposals focus on binary classi- cation and they are not directly applicable to multiclass classica- tion problems. There are two sparse discriminant analysis methods that can handle multiclass classication problems, but their theo- retical justications remain unknown. In this paper, we propose a new multiclass sparse discriminant analysis method that estimates all discriminant directions simultaneously. We show that when ap- plied to the binary case our proposal yields a classication direc- tion that is equivalent to those by two successful binary sparse LDA methods in the literature. An efcient algorithm is developed for computing our method with high-dimensional data. Variable selec- tion consistency and rates of convergence are established under the ultrahigh dimensionality setting. We further demonstrate the su- perior performance of our proposal over the existing methods on simulated and real data. Composite Large Margin Classiers with Latent Subclasses for Heterogeneous Biomedical Liu2and Michael Kosorok2 1Vanderbilt University 2The University North Carolina at Chapel Hill g.chen@vanderbilt.edu High dimensional classication problems are prevalent in a wide range of modern scientic applications. Despite alarge number of candidate classication techniques available to use, practitioners of- ten face a dilemma of the choice between linear and general nonlin- ear classiers. Specically, simple linear classiers have good inter- pretability, but may have limitations in handling data with complex structures. In contrast, general nonlinear kernel classiers are more flexible but may lose interpretability and have higher tendency forovertting. In this paper, we consider data with potential latent sub- groups in the classes of interest. We propose a new method, namely the Composite Large Margin Classier (CLM) to address the issue of classication with latent subclasses. The CLM aims to nd three linear functions simultaneously: one linear function to split the data into two parts, with each part being classied by a different lin- ear classier. Our method has comparable prediction accuracy to a general nonlinear kernel classier and it maintains the interpretabil- ity of traditional linear classiers. We demonstrate the competitive performance of the CLM through comparisons with several existing linear and nonlinear classiers by Monte Carlo experiments. An- alyzing Alzheimer's disease classication problem using CLM not only provides lower classication error in discriminating cases and controls, but also identies subclasses in controls which are more likely to develop into disease in the future. Positive Denite Regularized Estimation Kong 3University of Minnesota lxx6@psu.edu The regularized covariance estimator proper- ties matrices, but it often has nega- tive eigenvalues when used in real data analysis. To simultaneously achieve desired low-dimensional structure and positive deniteness, we develop a unied positive regularized efcient alternat- ing direction method of multipliers is derived to solve the challeng- ing optimization problem and its convergence properties are estab- lished. Under weak regularity conditions, non-asymptotic statistical theory is also established for the proposed estimator. The compet- itive nite-sample performance of our proposal is demonstrated by both simulation and real applications. Feature Selection Utilizing the Whole Solution Path Yang Liu1and\u0007Peng Wang2 1Bowling Green State University 2University of Cincinnati jwpeng@gmail.com The performances of penalized likelihood approaches profoundly depend on the selection of the tuning parameter, however there has not been a common agreement on the criterion for choosing the tun- ing parameter. Moreover, penalized likelihood estimation based on a single value of the tuning parameter would suffer from several drawbacks. In this project, we introduces a novel approach for fea- ture selection based on the whole solution path rather than choosing one value for the tuning parameter, which signicantly improves the selection accuracy. Moreover, it allows for feature selection using ridge or other strictly convex penalties. The key idea is to classify the variables as relevant or irrelevant for each tuning parameter and then select all the variables which have been classied as relevant at least once. We establish the theoretical properties of the method, and illustrate the advantages of the proposed approach with simula- tion studies and a data example. 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j81Abstracts Session 57: Recent Developments on High-Dimensional Inference in Biostatistics Proling and Accounting for Heterogeneity in the Analysis of Cancer Sequencing Data Mengjie Chen The University of North Carolina at Chapel Hill mengjie@email.unc.edu The cancer genome is characterized by genetic heterogeneity that is seen across tumor types, among samples of a particular type and within an individual tumor. This heterogeneity has posed great chal- lenges in both cancer treatment and basic cancer biology research. Recent advances of next-generation sequencing technology provide the opportunity to scrutinize the cancer genome at single base-pair resolution. Harnessing the statistical properties of the sequencing data enables us to prole the genetic heterogeneity of each tumor, which potentially generates unprecedented insights into the devel- opment of cancers. I'll introduce how to infer purity and clon- ality from different cancer sequencing data, including whole ex- ome/genome sequencing and targeted sequencing. Optimal Detection of Weak Positive Dependence between Two Mixture Distributions \u0007Sihai Zhao1, Tony Cai2and Hongzhe 2University of Pennsylvania sdzhao@illinois.edu This paper studies the problem of detecting dependence between two mixture distributions, motivated by questions arising from sta- tistical genomics. The fundamental limits of detecting weak posi- tive dependence are derived and an oracle test statistic is proposed. It is shown that for mixture distributions whose components are stochastically ordered, the oracle test statistic is asymptotically op- timal. Connections are drawn between dependency detection and signal detection, where the goal of the latter is to detect the presence of non-null components in a single mixture distribution. It is shown that the oracle test for dependency can also be used as a signal de- tection procedure in the two-sample setting, and there can achieve detection even when detection using each sample separately is prov- ably impossible. A nonparametric data-adaptive test statistic is then proposed, and its closed-form asymptotic distribution under the null hypothesis of independence is established. Simulations show that the adaptive procedure performs as well as the oracle test statistic, and that both can be more powerful than existing methods. In an application to the analysis of the shared genetic basis of psychiatric disorders, the adaptive test is able to detect genetic relationships not detected by other procedures. Multiple Testing for Conditional Dependence by Quantile- Based Contingency Table \u0007Jichun Xie1and Ruosha Li2 1Duke University 2The University of Texas School of Public Health jichun.xie@duke.edu Pearson chi-square test is a popular tool for testing dependence between two categorical or ordinal variables in the framework of contingency table, where the categorical boundaries are pre- determined. In this paper, we discuss another type of contingency tables, called quantile-based contingency tables, where the categor- ical boundaries are estimated quantiles depending on other covari- ates. This type of tables can be used to test general dependence between two continous variables conditioning on other covariates. After organizing data using the quantile-based contingency table,we prove that the null distribution of the Pearson statistic is still the chi-square distribution, but its degree of freedom only depends on the dimension of the table. Furthermore, when the number of vari- ables is much greater than the sample size, we propose a multiple testing method to test whether each pair of variables is dependent conditioning on covariates. The multiple testing method asymp- totically controls the false discovery rate at the desired level. In addition to the theoretical analysis, we perform numerical studies to compare the performance of the proposed test and other com- petitive tests. The proposed test is both robust and powerful under various settings. We demonstrate the effectiveness of the test by a large-scale genomic data analysis. Spurious Shao2and\u0007Wen-Xin University of Hong Kong zhouwenxin1986@gmail.com Over the last two decades, many exciting variable selection meth- ods have been developed for nding a small group of covariates that are associated with the response from a large pool. Can the discoveries by such data mining approaches be spurious? Can our fundamental assumptions on exogeneity of covariates needed for such variable selection be validated with the data? To answer these questions, we need to derive the distributions of the maximum spu- rious correlations given certain number of predictors. When the covariance matrix of covariates possesses the restricted eigenvalue property, we derive such distributions, using Gaussian approxima- tion and empirical process techniques. However, such a distribution depends on the unknown covariance matrix of the covariate. Hence, we propose a multiplier bootstrap method to approximate the un- known distributions and establish the consistency of such a simple bootstrap approach. The results are further extended to the situation where residuals are from regularized ts. Our approach is then ap- plied to construct the upper condence limit for the maximum spu- rious correlation and testing exogeneity of covariates. The former provides a baseline for guiding false discoveries due to data mining and the latter tests whether our fundamental assumptions for high- dimensional model selection are statistically valid. Our techniques and results are illustrated by both numerical examples. Session 58: Blinded and Unblinded Evaluation of Aggre- gate Safety Data during Clinical Development Continuous Safety Signal Monitoring with Blinded Data \u0007Greg Ball1and William Wang2 1AbbVie Inc. 2Merck & Co. gregball6@att.net Concerns over product safety have resulted in late stage program failures and market withdrawals. This has focused more interest and attention paid to aggregate safety analyses during clinical develop- ment. The Code of Federal Regulations (CFR) requires a safety re- port whenever aggregate analysis indicates that \"events occur more frequently in the drug treatment group than in a concurrent control group\". The FDA guidance on Safety Reporting Requirements for INDs and BA/BE Studies amplies the CFR in asserting that a \"sys- tematic approach for safety surveillance... should include a process for reviewing, evaluating and managing accumulating safety data from the entire clinical trial database at appropriate intervals\". An independent group, whether an external DMC or an internal safety 82j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts team, should use all of the available data, including data from out- side of the clinical trials in assessing the safety of the developing product. Any suspected adverse reactions should be evaluated rel- ative to other events in ongoing studies as well as previous stud- ies. Emerging trends in clinical trial data must be frequently and carefully evaluated to protect the safety and well-being of patients. Data Monitoring Committees (DMCs) evaluate accumulating un- blinded data and make recommendations about the continuing safe conduct of trials. Trial sponsors must decide how best to implement these recommendations and whether or not to stop a trial. How- ever, while DMC recommendations are based on differences ob- served between treatment arms, sponsors are generally blinded to these differences, adding considerable uncertainty to the decision- making process. Our blinded safety signals can provide a full mea- sure of blinded data that trial leadership can use, in combination with open information from the DMC, to evaluate the strength of evidence contained in the data in order to make properly informed decisions that protect patients from unnecessary harm while allow- ing trials to lead to conclusive results. Safety monitoring is an essentially dynamic process which requires the flexibility of a likelihood based method to respond to unforeseen developments. We use a Bayesian approach to provide a unied framework for continuous safety signal monitoring with all of the available blinded information in order to produce probability state- ments that are easy to interpret. A moderately informative prior is used to regulate influence for a signal. Designing useful safety signals requires an active collaboration between Statistics, Pharma- covigilance and Clinical. We have evaluated this method in a re- cently completed Phase 2b study and are currently working together to implement it in a clinical trial program. Our goal is to formalize and improve conversations about safety signal monitoring among all stake-holders in the conduct of randomized clinical trials. This presentation was sponsored by AbbVie. AbbVie contributed to the design, research, and interpretation of data, writing, review- ing, and approving of the presentation. Greg Ball is an employee of AbbVie, Inc. How Should the Final Rule Affect DMCs? Janet Wittes Statistics Collaborative janet@statcollab.com The FDA's Final Rule on Safety Reporting changes the way spon- sors of drugs report serious adverse events to the FDA. Many phar- maceutical companies have already changed their internal processes to comply with the new rule. DMCs have traditionally been very re- luctant to report unblinded data to sponsors unless the data show clear enough evidence of specic harms to require a change in the Informed Consent Form. If sponsors are to comply with the Final Rule, DMCs will have to change their behavior and consider report- ing more frequently to sponsors when there sufcient evidence of causality arises during the course of a trial it is monitoring. This talk addresses those aspects of the Final Rule that affect DMCs and proposes approaches a DMC might use in deciding what serious adverse events it should report unblinded to the sponsor during the course of a trial. Implementation of the Investigational New Drug Safety Report- ing Requirements \"Final Rule\" Brenda Crowe Eli Lilly and Company bjcrowe@lilly.com In 2010, the Food and Drug Administration(FDA) issued legislation(the \"Final Rule\"*) addressing the safety reporting requirements for investigational new drug applications (INDs). It changed how com- panies were to assess serious adverse events with respect to which should be reported in an expedited fashion to FDA. I will discuss how processes that were already in place helped us implement the Final Rule. I will share some high-level case examples and discuss sound statistical methodologies, as well as the statistical leadership that it took to get them implemented. * Investigational New Drug Safety Reporting Requirements for Hu- man Drug and Biological Products and Safety Reporting Require- ments for Bioavailability and Bioequivalence Studies in Humans Session 59: Design and Analysis of Non-Inferiority Clini- cal Trials Some Comments on the Three-Arm De- sign \u0007Ming Squibb Company In non-inferiority clinical trials, when it is ethically justiable, a placebo arm is often included in addition to the experimental ther- apy and active comparator/reference therapy. This leads to a three- arm non-inferiority trial. Although there's an extensive literature on the design of three-arm non-inferiority trials, interesting questions and practical considerations can still arise when it comes to clinical trial planning. In this paper, we discuss the following two aspects of a gold standard three-arm non-inferiority trial design. Firstly, we examine the optimal sample size allocation based on overall power instead of single step power. We then discuss the procedure given in Rothman et al. (2011) for \"capturing all possibilities\", and show that it can be greatly simplied. Lastly, we compare the frequen- tist multiple-testing approaches and the Bayesian approach for the simplied procedure. Non-inferiority Tests for Prognostic Models \u0007Ning Xu and Yongzhao Shao New York University nx223@nyu.edu In this talk, we will discuss issues of testing for non-inferiority in comparing prognostic models and prognostic factors. Both logis- tic regression based and Cox proportional hazards regression based prognostic models are considered. A mixture model of survival out- come with latent case-control group information is also considered. Utilities of prognostic factors or prognostic models can be compared using common concordance measures. Session 60: Toward More Effective Identication of Biomarkers and Subgroups for Development of Tailored Therapies Condence Intervals for Assessing SNP Effects on Treatment Ying University 2University of Pittsburgh 3Eli Lilly and Company jch@stat.osu.edu An important decision in personalized medicine development is whether the new treatment should target a subgroup of the patients. 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j83Abstracts Testing Single Nucleotide Polymorphisms (SNPs) for use as poten- tial biomarkers in drug development is more complex than in tradi- tional case-control GWAS, because the drug development process typically involves comparing a new treatment with a control. We show, in fact, a SNP's effect on treatment efcacy cannot simply be characterized by the traditional denitions of dominant, recessive, and additive effects, and popular current techniques for SNP testing offer no protection against targeting wrong subgroups. Proposing a new formulation, we provide simultaneous condence intervals for the four possible SNP effects relevant to making sound decision on which patient subgroup to target. Our simultaneous condence level guarantee is equivalent to strong control of familywise error rate (FWER) in multiple testing within each SNP. To account for multiplicity of the SNPs, we use a thresholding technique that con- trols the per family error rate, the expected number of SNPs falsely inferred to be predictive of efcacy, while taking dependence across the SNPs into account. Our novel combination error rate control, familywise error rate control within each SNP and per family error rate control across the SNPs, is rigorous and has a clear practical in- terpretation. Across different SNP studies, the expected number of SNPs with incorrectly inferred target subgroup is controlled. Such control is appropriate in a drug development environment, as it al- lows flexibility in the exploration of multiple candidate SNPs, while being condent in the patient subgroup to target in any selected SNP. Identication of Biomarker Signatures Using Adaptive Elastic Net Lei Shen2and Yaoyao Xu3 1Bristol-Myers Squibb Company 2Eli Lilly and Company 3AbbVie Inc. xuemin.gu@bms.com A new method for Biomarker/Subgroup Identication has been de- veloped, utilizing penalized regression in a two-step procedure. Specically, adaptive elastic net is used for effective variable se- lection, and data splitting enables the calculation of valid p-values and analytical control of error rate. Two of the main advantages of this method are its ability to handle a large number of biomarkers, and identication of a signature based on multiple biomarkers that can have much stronger effect than its individual components. Correcting Ascertainment Bias in Biomarker Identication Shengchun Kong Purdue University kongsc@purdue.edu In biomarker studies, associations between clinical phenotype and biomarkers are of interest. After detecting signicant association, the estimated effect size is used to make important decisions, such as planning of the replication study. We consider multiplicity issue in estimation where upward bias exists in the estimated effect size for an inferred association. Efforts to replicate ndings often fail due to the overestimation of the effect. This ascertainment bias is also known as the winner's curse. We investigated different approaches to correct the ascertainment bias including resampling methods and empirical Bayes's approach. Simulations are conducted to compare these approaches. Analysis Optimization for Biomarker and Subgroup Identica- tion Lei Shen Eli Lilly and Company shen lei@lilly.comA number of statistical methods have been proposed to identify sub- groups of patients with enhanced treatment effect using data from clinical trials. While post hoc subgroup analyses have long been conducted for clinical trials, the more recent methods have been de- signed to proactively identify subgroups to enable the development of tailored therapies. With an increasing number of available op- tions, the a priori selection of subgroup identication method for a particular trial requires consideration of pertinent information, in- cluding prior knowledge and project needs, as well as a quantitative process to optimize the analytical method. The situation becomes even more complex when jointly considering two sequential trials, which often are realistically required for the identication and con- rmation of subgroup ndings. In this presentation, I present gen- eral considerations and a case study of analysis optimization to de- termine statistical methods for biomarker and subgroup identica- tion in this type of applications. Session 61: Design and Analysis Issues in Clinical Trials Nonparametric Response Adaptive Randomization Procedures Zhongqiang Liu1and\u0007Feifang Hu2 1Renmin University of China Washington University feifang@gwu.edu In literature, many response-adaptive randomization procedures have been proposed and extensively studiedin the past decades. However, Most of these procedures are based on parametric struc- ture and do not usually apply to nonparametric situations. In this talk, we propose a new family of response-adaptive randomization based the p-values of corresponding hypothesis tests. Thus, the pro- posed procedures apply to both parametric and non-parametric sit- uations. Under widely satised conditions, we derive the asymp- totic properties of the procedures and further obtain the power func- tion under non-parametric settings.The proposed procedures are: (i) more powerful; (ii) more robust; and (iii) more ethical than the clas- sical RAR procedures under some situations. The advantages are also illustrated in numerical studies. Ecological Momentary Assessment for Measuring Outcome in Clinical Trials Stephen Rathbun University of Georgia rathbun@uga.edu Ecological Momentary Assessment (EMA) is a research method in the behavioral sciences focused on the collection of subjects' cur- rent psychological states in their everyday environments. EMA is particularly well suited for patient-reported outcomes related to sub- jective symptoms, quality of life and emotional states. It has often been used to investigate recurrent addictive behavioral events such as the use of tobacco. In this paper we will rst review methods suitable for EMA data and for tting xed and mixed effects models predicting risk as a function of partially observed time-varying co- variates. These methods are based on covariates sampled over time according to a known probability-based sampling design, as well as a random sample of the recurrent events. The objective of the paper, however, is to construct optimal EMA sampling designs that balance minimization of the standard errors of parameter estimators against burden placed on study subjects. Time-varying covariates are sampled according to a self-correcting point process with pa- rameters that control the mean number of covariate samples per day as well as the distribution of inter-arrival times of covariate samples. Covariates are also sampled at a random subset of the events using 84j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts a thinning function that also controls the number of events sampled per day and the distribution of times between sampled events. Opti- mal design calls for increased sampling intensity when subjects are either at high or low risk of the event of interest. Our approach will be illustrated using data from an EMA of smoking and an EMA of dietary lapse. Robust Zero-Inflated Poisson/Negative Binomial Regression for Li3 1University Cincinnati 2University of Georgia 3Renmin University of China qinyn@ucmail.uc.edu In this article, we introduce a new robust estimation procedure for Zero-Inflated Poisson/Negative Binomial Regression. We show that, by robustifying the likelihood function, we can successfully al- leviate the influence of over-dispersion and model assumption vio- lation such as heavy tails. In addition, we introduce the zero-inflated component in the model to handle the excess zero-count in the data. An EM algorithm is also introduced for estimating such a model. Through simulation and real data, we demonstrate the effectiveness of our method and its advantage over the traditional approach. Joint Modeling Tumor Burden and Time to Event Data in On- cology 3Adobe Research India Labs 4Renmin University of China yeshen@uga.edu The tumor burden (TB) process is postulated to be the primary mechanism through which most anticancer treatments provide ben- et. In phase II oncology trials, the biologic effects of a therapeu- tic agent are often analyzed using conventional endpoints for best response, such as objective response rate and progression-free sur- vival, both of which causes loss of information. On the other hand, graphical methods including spider plot and waterfall plot lack any statistical inference when there is more than one treatment arm. Therefore, longitudinal analysis of TB data is well recognized as a better approach for treatment evaluation. However, longitudinal TB process suffers from informative missingness because of pro- gression or death. We propose to analyze the treatment effect on tu- mor growth kinetics using a joint modeling framework accounting for the informative missing mechanism. Our approach is illustrated by multisetting simulation studies and an application to a non-small cell lung cancer data set. The proposed analyses can be performed in early-phase clinical trials to better characterize treatment effect and thereby inform decision-making. Session 62: Statistical Challenges in Economic Research Involving Medical Costs Projecting Survival and Lifetime Costs from Short-Term Smok- ing Cessation Trials Daniel Heitjan1;2 1Southern Methodist University 2The University of Texas Southwestern Medical Center dheitjan@smu.edu Cigarette smoking is a leading potentially preventable cause of mor- bidity and mortality. Therefore there has been considerable interestin the development of treatments - both behavioral and pharmaco- logic - to assist smokers to conquer their nicotine addiction. As the number of smokers is large - roughly 20% of the US population, and more in many other countries - it is critical to evaluate not only the effectiveness of such treatments, but also their costs. The fact that many smokers will not experience the major health effects of their addiction for many decades substantially complicates this evalua- tion. Moreover, clinical trials of smoking cessation treatments com- monly take as their primary endpoint a short-term outcome such as six-month quit rate. Thus the evaluation of the cost-effectiveness of smoking cessation treatment, in its usual form of cost per life-year saved, involves a considerable extrapolation. In this talk I will de- scribe a micro-simulation model that projects early quit rates and treatment costs into survival and long-term cessation-related costs for a general smoking population. A Flexible Model for Correlated Medical Costs, with Applica- tion to Medical Expenditure Chicago 2Northwestern University 3M. D. Anderson Cancer Center 4North Carolina State University lei.liu@northwestern.edu We propose a flexible model for correlated medical cost data with several appealing features. First, the mean function is partially lin- ear. Second, we do not specify the distributional form for the re- sponse. Third, the covariance structure of medical costs has a semi- parametric form. We use the extended generalized estimating equa- tions to simultaneously estimate all parameters of interest. B-spline is used to estimate unknown functions, and a modication to Akaike Information Criterion is proposed for selecting the knots in spline bases. Simulation study is conducted to assess the performance of our method. Finally, we apply the model to correlated medical costs in the Medical Expenditure Panel Survey (MEPS) dataset. A Bivariate Copula Random-Effects Model for Length and Tang1, Zhehui Luo2and\u0007Joseph Gardiner2 1Allegheny Health Network 2Michigan State University JGARDINER@EPI.MSU.EDU Copula models and random effect models are becoming increas- ingly popular for modeling dependencies or correlations between random variables. Recent applications include in such elds as economics, nance and insurance, and survival analysis. We give a brief overview of the principles of construction of such cop- ula models from the Farlie-Gumbel-Morgenstern, Gaussian, and Archimedean families including Frank, Clayton, and Gumbel fami- lies. We develop a new flexible joint model for correlated measure- ment errors modeled by copulas and incorporate a cluster level ran- dom effect to account for individual and within-cluster correlations simultaneously. In an empirical application our proposed approach attempts to capture the various dependence structures of hospital length of stay and cost (symmetric or asymmetric) in the copula function. It takes advantage of the relative ease in specifying the marginal distributions and introduction of within-cluster correlation based on the cluster level random effects. Nonparametric Inference for the Joint Distribution of Recur- rent Marked Variables and Recurrent Survival Time Laura Yee and\u0007Gary Chan 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j85Abstracts University of Washington kcgchan@uw.edu Time between recurrent medical events may be correlated with the cost incurred at each event. We discuss a nonparametric estimator for the joint distribution of recurrent events and recurrent medical costs in right-censored data. We also derive the asymptotic variance of our estimator, a test for equality of recurrent marker distributions, and present simulation studies to demonstrate the performance of our point and variance estimators. Our estimator is shown to per- form well for a wide range of levels of correlation, demonstrating that our estimators can be employed in a variety of situations when the correlation structure may be unknown in advance. We apply our methods to hospitalization events and their corresponding costs in the second Multicenter Automatic Debrillator Implantation Trial (MADIT-II), which was a randomized clinical trial studying the preventing ventric- ular arrhythmia. Session 63: Adaptive Design and Sample Size Re- Estimation Methods for Flexible Sample-Size Design in Clinical Trials \u0007Gang Li1, 1Johnson & Johnson 2Rutgers University gli@its.jnj.com Sample size plays a crucial role in clinical trials. Flexible sample- size designs, as a part of the more general category of adaptive de- signs that utilizes interim data from the current trial, have been a popular topic in recent years. In this paper, we give a comparative review of four related methods for such a design. The likelihood method uses the likelihood ratio test with an adjusted critical region. The weighted method adjusts the test statistic with given weights rather than the critical region. The dual test method requires both the likelihood ratio statistic and the weighted statistic to be in the unadjusted critical region. The promising zone approach uses the likelihood ratio statistic with the unadjusted region with other con- straints. All four methods preserve the type-I error rate. We explore their properties and compare their relationships and merits. We de- lineate what is necessary to specify in the study protocol to ensure the validity of the statistical procedure and what can be kept implicit in the protocol so that more flexibility can be attained for conrma- tory phase III trials in meeting regulatory requirements. Blinded Sample Size Re-estimation in Trials with Survival Out- comes and Incomplete Information Thomas Cook University of Wisconsin-Madison cook@biostat.wisc.edu In many large multicenter clinical trials, especially in cardiology, the primary outcome is a composite of fatal and non-fatal events. Study power is determined by the total number of subjects with at least one primary event and the sample size and duration of follow- up are selected to achieve the target number of events given an as- sumed underlying survival distribution. Furthermore, potential pri- mary events typically require adjudication by an independent event classication committee (ECC). As the study progresses, informa- tion becomes available to assess these design assumptions, allowing blinded adjustments to be made to both sample size and study du- ration. This assessment is complicated by two factors, however. First, delays in the reporting of potential primary events resultsin incomplete ascertainment of potential events, and unless prop- erly accounted for, can lead to an underestimation of the primary event rate. Second, ECC review be incomplete for many reported events. In this talk I will describe an estimator that simultaneously accounts for both delayed ascertainment and incomplete adjudica- tion and show how this estimator can be used to make the desired design modications. an important tool for deploying state-of- the-art treatments from clinical trials into a treatment program, with the dual goals of learning about effectiveness of the treatments and improving the quality of care for patients enrolled into the program. In this talk, I will introduce a SMART-based methodology to opti- mize a treatment program of dynamic treatment regimens (DTRs) for patients with depression post acute coronary syndrome. The proposed method involves a novel application of adaptive random- ization aimed to address three main concerns of an implementation study: it allows incorporation of historical data or opinions, it in- cludes randomization for learning purposes, and it aims to improve care via adaptation throughout the program. By simulation, we il- lustrate that the inputs from historical data are important for the program performance measured by the expected outcomes of the enrollees, but also show that the adaptive randomization scheme is able to compensate poorly specied historical inputs by improving patient outcomes within a reasonable horizon. The simulation re- sults also conrm that the proposed design allows efcient learning of the treatments by alleviating the curse of dimensionality. Session 64: Recent Development in Personalized Medicine and Survival Analysis Estimating the Optimal Dynamic Treatment Regime from a Classication of Michigan mzhangst@umich.edu Personalized medicine, which is focused on making treatment de- cisions on individuals based on his/her own available information, has received much attention lately. Treatment of chronic disease of- ten involves a series of decisions on treatment at multiple points and personalizing medicine can be formalized using the concept of dynamic treatment regimes. A dynamic treatment regime is a set of sequential decision rules that determine how to treat a pa- tient over time using patient's information available at each decision point and the optimal dynamic treatment regime is the one that leads to the most favorable outcome on average if followed by the patient population. Currently, the two main approaches for identifying the optimal dynamic treatment regime are Q- and A-learning, where Q- learning involves modeling for the outcome and A-learning involves modeling for part of the outcome (eg, treatment contrast) and for treatment assignment. A key concern for Q-and A-learning is model misspecication. Recently, Zhang et al. (2013) proposed a method based on maximizing a doubly robust augmented inverse probabil- ity weighted estimator (AIPWE) for population mean outcome over a restricted class of regimes and this method has been shown to be 86j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts more robust to model misspecication. In practice, how to choose a restricted class of regimes under consideration may not be easy. In this study, we propose to recast the problem of identifying the op- timal treatment regime into a classication problem and identifying the optimal treatment regime is equivalent to minimizing a weighted classication error, for which many existing and powerful machine learning methods can be used. This method enjoys the robustness property of the Zhang et al. (2013) method by using a doubly robust AIPWE estimator for the treatment contrast function as well and, in addition, this framework is much more flexible. Parsimonious and Robust Treatment Strategies for Target Pop- ulations Using of Wisconsin-Madison 2The University of North Carolina at Chapel Hill yqzhao@biostat.wisc.edu Individualized treatment rules, which recommend treatments based on individual patient characteristics, have gained increasing inter- est in clinical practice. Properly planned and conducted random- ized clinical trials are ideal for constructing individualized treatment rules. However, it is often a concern that they are susceptible to lack of representativeness, which limits the applicability of the derived rules to a future large population. Furthermore, in order to inform clinical practice, it is crucial to provide rules that are easy to in- terpret and disseminate. To tackle these issues, using data from a single clinical trial study, we propose a two-stage procedure to de- rive the best parsimonious rule to maximize the proportion of future patients receiving their optimal treatments. The procedure is robust over a wide range of possible covariate distributions in the target population, with minimal requirements on the mean and covariance of the patients who benet from each treatment. The practical utility and the favorable performance of the methodology are demonstrated using extensive simulations and a real data application. A Sieve Semiparametric Maximum Likelihood Approach for Regression Analysis of Bivariate Interval-censored Failure Time Data qz4z3@mail.missouri.edu failure time data arise in a number of elds and many authors have discussed various issues related to their analysis. However, most of the existing methods are for univariate data and there exists only limited research on bivariate data, especially on re- gression analysis of bivariate interval-censored data. We present a class of semiparametric transformation models for the problem and for inference, a sieve maximum likelihood approach is developed. The model provides a great flexibility, in particular including the commonly used proportional hazards model as a special case, and in the approach, Bernstein polynomials are employed. The strong consistency and asymptotic normality of the resulting estimators of regression parameters are established andfurthermore, the estima- tors are shown to be asymptotically efcient. Extensive simulation studies are conducted and indicate that the proposed method works well for practical situations.Session 65: New Strategies to Identify Disease Associated Genomic Biomarkers Discovering Disease Associated Molecular Interactions Using Discordant Correlation Charlotte Siska and\u0007Katerina Kechris University of Colorado at Denver katerina.kechris@ucdenver.edu A common approach for identifying molecular features (such as transcripts or proteins) associated with a biological perturbation or disease is testing for differential expression or abundance in -omics data. However, this approach is limited for studying interactions be- tween molecular features, which would give a deeper knowledge of the relevant molecular systems and pathways. As an alternative, dif- ferentially correlated pairs of features can be identied that change correlation based on groups of samples (e.g., wildtype or mutant) or subjects (e.g. cases or controls). We have developed a method for this purpose called the Discordant method, which determines the posterior probability that a pair of features has discordant cor- relation between phenotypic groups using mixture models and the EM algorithm. We compare our method to existing approaches; one that uses Fisher's transformation and another that uses an Em- pirical Bayes joint probability model. In simulations we demon- strate that while all the methods have similar specicity, the Discor- dant method has better sensitivity and is more powerful at iden- tifying pairs that have a correlation coefcient close to 0 in one group and a largely positive or negative correlation coefcient in the other group. Using glioblastoma data from The Cancer Genome Atlas (TCGA), which has matched samples between miRNA and mRNA, we nd that the Discordant method nds relatively more glioblastoma-related miRNAs compared to other methods. The sim- ulations and TCGA data results indicate that the Discordant method is benecial for identifying interactions associated with phenotypic groups or disease severity. Joint Analysis of Genomic Data from Different Sources using Kernel Machine Regression with Multiple Kernels \u0007Michael Wu and Ni Zhao Fred Hutchinson Cancer Research Center mcwu@fhcrc.org Comprehensive understanding of complex trait etiology requires ex- amination of multiple sources of genomic variability. Integrative analysis of these data sources promises elucidation of the biologi- cal processes underlying particular outcomes. Consequently, many large GWAS consortia are expanding to simultaneously examine the joint role of DNA methylation. However, it is unclear how to lever- age both data types to determine if particular genetic regions are related to traits of interest. Therefore, we propose to use the power- ful kernel machine framework for rst testing the cumulative effect of both epigenetic and genetic variability on a trait, and for subse- quent mediation analysis to understand the mechanisms by which the genomic data types influence the trait. Specically, we use a multi-kernel approach to model the effects of methylation and geno- type on a continuous outcome while controling for potential con- founders. We demonstrate through simulations and real data appli- cations that our proposed testing approach often improves power to detect trait associated genes, while protecting type I error, and that our mediation analysis framework can often correctly elucidate the mechanisms by which genetic and epigenetic variability influences traits. Transformed Low-rank ANOV A Models for High Dimensional 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j87Abstracts Variable Selection \u0007Jianhua Hu1and Center 2University of Waikato jhu@mdanderson.org For high dimensional genetic data, an important problem is to search for associations between genetic variables and a phenotype\u2014 typically, a discrete variable (diseased versus normal). A conven- tional solution is to characterize such relationships through regres- sion models in which a phenotype is treated as the response variable and genetic variables are treated as the covariates. Not surprisingly, such a way incurs the challenging problem of the number of vari- ables much larger than the number of observations. We propose a statistical framework of expressing the transformed mean of the ge- netic variables in exponential distribution family via ANOV A type of models in which a low-rank interaction space captures associ- ation between phenotype and genetic variables. This alternative method transforms the variable selection problem to a well-posed problem with the number of observations larger than number of ge- netic variables. We also develop a new model selection criterion based on Bayesian information criterion for the new model frame- work with diverging number of parameters. In the talk, we focus on a specic application to genome-wide association studies. Proper Use of Allele-Specic Expression Improves Statistical Power for cis-eQTL Mapping with University of North Carolina at Chapel Hill 3North Carolina State University yijuan.hu@emory.edu Studies of expression quantitative trait loci (eQTLs) offer insight into the molecular mechanisms of loci that were found to be asso- ciated with complex diseases and the mechanisms can be classied into cis- and trans-acting regulation. At present, high-throughput RNA sequencing (RNA-seq) is rapidly replacing expression mi- croarrays to assess gene expression abundance. Unlike microar- rays that only measure the total expression of each gene, RNA- seq also provides information on allele-specic expression (ASE), which can be used to distinguish cis-eQTLs from trans-eQTLs and, more importantly, enhance cis-eQTL mapping. However, assessing the cis-effect of a candidate eQTL on a gene requires knowledge of the haplotypes connecting the candidate eQTL and the gene, which cannot be inferred with certainty. The existing two-stage approach that rst phases the candidate eQTL against the gene and then treats the inferred phase as observed in the association analysis tends to at- tenuate the estimated cis-effect and reduce the power for detecting a cis-eQTL. In this article, we provide a maximum-likelihood frame- work for cis-eQTL mapping with RNA-seq data. Our approach in- tegrates the inference of haplotypes and the association analysis into a single stage, and is thus unbiased and statistically powerful. We also develop a pipeline for performing a comprehensive scan of all local eQTLs for all genes in the genome by controlling for false discovery rate, and implement the methods in a computationally ef- cient software program. The advantages of the proposed methods over the existing ones are demonstrated through realistic simulation studies and an application to empirical breast cancer data from The Cancer Genome Atlas project.Session 66: Recent Advances in Empirical Likelihood Method Jackknife Empirical Likelihood for U-Statistics with Estimated Constraints \u0007Fei Tan and Hanxiang Peng Indiana University-Purdue University ftan@math.iupui.edu In this talk, the jackknife empirical likelihood for U-statistics (Jing, et al. (2009) and Peng et al. (2015)) is generalized to allow for a nite and growing number of estimated constraints. The latter is needed to handle naturally occurring nuisance parameters in semi- parametric models. The developed theory is applied to derive the jackknife empirical likelihood based tests and condence sets for the variance in a linear regression model, the variances in a bal- anced random effects model with estimated constraints; for Theil's test about the slope in a simple linear regression with growing num- ber of estimated constraints; for the Wilcoxon signed rank test on symmetry about a unknown center. Simulations are conducted to study their numerical behaviors. Jackknife Empirical Likelihood for Order-restricted Statistical Inference with Missing Data \u0007Heng Wang and Ping-Shou Zhong Michigan State University hengwang@msu.edu We consider testing means with an increasing order or a decreasing order for data with missing values. The missing values are imputed nonparametrically under the missing at random assumption. For data with imputation, the classical likelihood ratio test designed for testing the order restricted means is no longer applicable since the likelihood does not longer exist. This paper proposes a novel test based on jackknife empirical likelihood (JEL). It is shown that the JEL ratio statistic evaluated under the null hypothesis converges to a chi-bar distribution. Simulation study shows that our proposed test maintains the nominal level well under the null and has prominent power under the alternative. The test is also robust for normally and non-normally distributed data. Improving Estimation in Structural Equation Models: An Easy Empirical Likelihood Approach \u0007Shan Wang and Hanxiang Peng Indiana University-Purdue University wangshan@imail.iu.edu In a structural equation model (SEM), if the covarianceof two vari- ables is known, then it can be used to improve efciency. This is re- alized by replacing the covariance of the two variables in the struc- tured covariance matrix with the known covariance. This method will not work if side information is not given in covariances. In fact, SEM's only use the information up to second moments. For example, random errors are modeled as uncorrelated with covari- ates. It is common in statistics that random errors are modeled as independent of covariates, which can't be used the current SEM's. In this talk, we propose an easy empirical likelihood approach to incorporate side information in SEM's. We demonstrate efciency gain by modeling random errors (1) being independent of covariates (2) symmetric about zero. We exhibit that the implementation of the method is extremely convenient and can be done with the existing software. We report large simulation results to exhibit efciency gain. Composite Empirical Likelihood \u0007Nicole Lazar and Adam Jaeger 88j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts University of Georgia nlazar@stat.uga.edu The virtue of a non-parametric approach such as empirical likeli- hood over parametric counterparts to inference is that one does not have to specify a distributional family. This provides robustness and flexibility, but often comes with a computational cost. Composite likelihood, another alternative to classical parametric likelihoods, builds up a complete likelihood piecewise and hence is computa- tionally efcient. In this talk, I will propose and dene the \"com- posite empirical likelihood.\" The new construct builds a likelihood out of empirical likelihood pieces, thereby inheriting desirable prop- erties from both \"parents.\" Session 67: New Advances in Adaptive Design and Anal- ysis of Clinical Trials Sensitivity Analyses for Missing Not at Random (MNAR) in Clinical Trials Peter Zhang Otsuka Pharmaceutical Development & Commercialization Inc. peter.zhang@otsuka-us.com Missing data has become a focus area for regulatory authorities (FDA, EMA). The FDA reqires the Sponsor to pre-specify sen- sitivity analyses plan for missing not at random (MNAR) to sup- port the primary analysis under missing at random (MAR) assump- tion. Most of MNAR methods (Diggle P, Kenward MG, 1994) have treated all observations with dropout as if they fall within the same dropout type. In practice, we would nd that different dropout rea- sons may be related to the outcomes in different ways, for example, detailed dropout reasons for this study are: adverse events (AE), lack of efcacy (LOE), lost to follow-up, protocol deviation, spon- sor discontinued study, subject met (protocol specied) withdrawal criteria, subject was withdrawn from participation by the investi- gator, and subject withdrew consent to participate. Dropout due to an adverse event (AE) and lack of efcacy (LOE) may lead to MNAR dropout. Subject withdrew consent may also lead to MNAR dropout. However, it is debatable whether a dropout caused by sub- jects withdrew consent is MAR or MNAR. Except AE, LOE, and subject withdrew consent, all the other dropout reasons may be as- sumed as either MCAR or MAR dropout. Pattern Mixture Mod- els (PMM) based on Multiple Imputation (MI) with mixed missing data mechanisms will be used to investigate the response prole of dropout patients by last dropout reason under MNAR mechanism. Moment-based Covariate Adjustment Method for Treatment Effect and Economics xiaofei.wang@duke.edu In the analysis of randomized clinical trials, the covariates that cor- relate with the primary outcome are often adjusted in the estimation of treatment effect in order to improve efciency and to compen- sate for any lack of baseline covariate balance between treatment arms. There are many different techniques for adjusting for baseline covariates. One commonly used method and allowing estimation of conditional treatment effect is the multivariable regression mod- elling. We will review a class of new covariate adjustment methods that incorporate the covariates in treatment effect estimation and allow estimation of marginal treatment effect. We will discuss a new moment-based covariate adjustment method that constrain allhigher order moments of the covariate distribution. The proposed method follows the same spirit of the new class of covariate adjust- ment methods, but its efciency gain doesn't require correct speci- cation of the parametric form of any regression model. Asymptotic properties of the proposed method are established. Simulation stud- ies show that the proposed method has nice nite sample proper- ties and perform well compared to existing methods. The proposed method is illustrated with a data example. On Design and Analysis of a Stratied Biomarker Time-to- Event Clinical Trial in the Presence of Measurement Error Aiyi Liu National Institutes of Health liua@mail.nih.gov Clinical trials utilizing predictive biomarkers have become a topic of increasing research in the era of personalized medicine. We conne our attention to the stratied biomarker design where pa- tients with the same biomarker status are randomly assigned to ei- ther an experimental arm or the standard of treatment. The pri- mary interest of a stratied biomarker design is to investigate if patients respond differently to treatment based on their biomarker status. Despite the advancements in molecular assays, correctly identifying the biomarker status remains a challenging task. We an- alytically demonstrate the profound adverse effects of misclassied biomarker status on the estimates of treatment effect, biomarker ef- fect, treatment-biomarker interaction, the corresponding condence intervals, power of the tests, and required sample sizes. We further propose respective remedies that tackle the misclassied biomark- ers in the design and analysis phase of clinical trials. We illus- trate the serious consequences of ignoring the classication error and demonstrate the performance of the proposed solutions using simulations. Session 68: Design and Analysis in Drug Combination Studies Design and Statistical Analysis of Multidrug Combinations in Preclinical Studies and Clinical Trials Ming Tan Georgetown University mtt34@georgetown.edu Combination therapy is the hallmark of therapies for cancer, vi- ral or microbial infections, hypertension and other diseases involv- ing complex biological networks. Synergistic drug combinations, which are more effective than predicted from summing effects of in- dividual drugs, often achieve increased therapeutic index. Because drug-effect is dose-dependent, multiple doses of an individual drug need to be examined, yielding rapidly increasing number of com- binations and a challenging high dimensional statistical modeling problem. The lack of proper design and analysis methods for multi- drug combination studies have resulted in many missed therapeutic opportunities. Although system biology holds the promise to unveil complex interactions within biological systems, the knowledge on network remains predominantly at the level of topology. We pro- posed a novel two-stage procedure starting with an initial selection by utilizing an in silico model built upon experimental data of single drugs and current system biology information to obtain maximum likelihood estimate. In this talk, I will present an efcient experi- mental design on selected multi-drug combinations, statistical mod- eling of the resulting data and the proof of its statistical properties. 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j89Abstracts Then I will present an adaptive Bayesian trial design for multidrug combinations with the modeling concept. Bayesian Hierarchical Monotone Regression I-splines for Dose- Response Assessment Veerabhadran Baladandayuthapani3 1Johns University 2Amgen Inc. 3M. D. Anderson Cancer Center grosner1@jhmi.edu We provide a practical and flexible method for dose-response mod- eling and drug-drug interaction analysis. This semi-parametric Bayesian model allows a meta-analysis of indepen- dent repeated dose-response experiments. We use monotone re- gression I-splines to estimate the mean dose-response function for functional in vitro dose-response data and incorporate the spline- based model in a Bayesian hierarchical framework. Posterior infer- ence on quantities of interest is facilitated by Markov chain Monte Carlo (MCMC). Inference focuses on estimating inhibitory concen- trations and the Loewe Interaction Index for drug-drug interaction analysis. We compare our approach to analyses using a parametric Emax model. A Bayesian Nonparametric Approach for Synergy Assessment in Drug Combination Studies Chenguang Wang Johns Hopkins University cwang68@jhmi.edu For complex diseases, drug combinations are a common and hope- ful strategy for achieving synergistic treatment effect and reducing dosages and toxicity. Because of the intrinsic complexity of bio- logical systems, nevertheless, it is a thwarting challenge to specify the parametric probability model that can appropriately describe the mechanisms by which the individual drugs achieve their single and joint effects. In this paper, we propose a Bayesian non-parametric approach for in vitro drug combination studies which models both dose-response and dose-toxicity relationships in a flexible manner. A Bayesian synergism evaluation procedure is also developed using Loewe additivity as the reference. Session 69: Recent Developments in Empirical Likeli- hood Methodologies: Diagnostic Studies, Goodness-of-Fit Testing, and Missing Values Jackknife Empirical Likelihood Condence Regions for the Evaluation of Continuous-scale Tests with tion Bias York University 2Georgia University gqin@gsu.edu Recently, Wang and Qin proposed various bias-corrected empirical likelihood condence regions for anytwo of the three parameters, sensitivity, specicity, and cut-off value, with the remaining param- eter xed at a given value in the evaluation of a continuous-scale diagnostic test with verication bias. In order to apply those meth- ods, quantiles of the limiting weighted chi-squared distributions of the empirical loglikelihood ratio statistics should be estimated. In order to facilitate application and reduce computation burden, in this paper, jackknife empirical likelihood-based methods are pro- posed for any pairs of sensitivity, specicity and cut-off value, andasymptotic results can be derived accordingly. The proposed meth- ods can be easily implemented to construct condence regions for the evaluation of continuous-scale diagnostic tests with verication bias. Simulation studies are conducted to evaluate the nite sam- ple performance and robustness of the proposed jackknife empirical likelihood-based condence regions in terms of coverage probabili- ties. Finally, a real example is provided to illustrate the application of new methods. Jackknife Empirical Likelihood Goodness-Of-Fit Tests For Vec- 2Eli Lilly and Company hpeng@math.iupui.edu Motivated by applications to goodness of t U-statistic testing, the jackknife empirical likelihood (Jing, et al.) is justied with two al- ternative approaches and the Wilks theorems for vector U- statistics are proved. This generalizes Owen's empirical likelihood for vec- tors to vector U statistics and includes the JEL for U-statistics with side information as a special case. The results are extended to al- low for the constraints to use estimated criteria functions and for the number of constraints to grow with the sample size. The developed theory is applied to derive the JEL tests or condence sets for some useful vector U-statistics associated with for a linear ef- fects model; for a balanced random effects model; and and Theil's test; and for the simplicial depth function. A small simulation is conducted to evaluate the tests. Jackknife Empirical Likelihood Interval Estimators for the Gini Index \u0007Dongliang Yichuan Zhao2and University 2Georgia State University wangd@upstate.edu A variety of statistical methods have been developed to the interval estimation of a Gini index, one of the most widely used measures of economic inequality. However there is still plenty of room for improvement in terms of coverage accuracy and interval length. In this paper, we propose interval estimators for the index and the dif- ference of two Gini indexes via jackknife empirical likelihood. Via expressing the estimating equations in the form of U-statistics, our method can be simply applied as the standard empirical likelihood for a univariate mean and avoid maximizing theprole empirical likelihood for the difference of two Gini indexes. Simulation studies show that our method is comparable to existing empirical likelihood methods in terms of coverage accuracy, but yields shorter intervals. The proposed methods are illustrated via analyzing a real data set. Jackknife Empirical Likelihood Inference with Regression 2Westat pszhong@stt.msu.edu empirical likelihood (EL) methods for con- structing condence intervals of mean with regression imputation or nonignorable missingness. The condence 90j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts interval is constructed based on the adjusted jackknife pseudo- values (Rao and Shao, 1992). The proposed EL ratios evaluated at the true value converge to the standard chi-square distribution un- der both missing mechanisms for simple random sampling. Thus the EL can be applied to construct a Wilks type condence inter- val without any secondary estimation. We then extend the proposed method to accommodate Poisson sampling design in survey sam- pling. The proposed methods are compared with some existing methods in simulation studies. We also apply the proposed method to an Italy household income panel survey data set. Session 70: Use of Simulation in Drug Development and Decision Making Simulations: The Future of Clinical Trial Design Ben Saville Berry Consultants ben@berryconsultants.com Simulations are key to understanding complex clinical trial design. They allow investigators to prospectively \"see\" a trial in action and iteratively rene the design to align with the scientic goals of the trial. In addition, simulations are used to produce operating char- acteristics that assess performance and satisfy regulatory concerns. The utility of simulations is illustrated through a cancer study of a single experimental regimen applied to three different subtypes of sarcoma. A Bayesian hierarchical model is used to allow borrow- ing of information across the sarcoma cohorts. The design includes frequent interim analyses to allow stopping based on futility or suc- cess. Software and regulatory issues for simulation-based designs are discussed. Bayesian Application in Optimizing Probability of Study Suc- cess with Multiple Endpoints Setting \u0007Grace Li, Honghua Jiang, Shen Lei, Karen Price, Haoda Fu and David Manner Eli Lilly and Company liying grace@lilly.com Bayesian analysis is broadly applied in decision making throughout the drug development process, due to its intuitive framework and ability to provide direct answers to complex problems. Likewise, the graphical testing approach is also being applied more in clinical trials because of its flexibility in testing multiple endpoints while strongly controlling familywise error rate. In this talk, we present a case study for a Phase 3 clinical trial design. The Bayesian inte- grated two-component prediction model was t using virtual patient PK/PD-derived data. A set of Phase 3 studies were simulated from the posterior samples obtained via the Bayesian model. The simu- lations incorporated specic study characteristics, such as dropout rate and data collection scheme. A set of p-values for all endpoints within each simulated trial were calculated. Various graphical test- ing schemes were assessed to identify the optimal scheme with the highest probability of success. This case study is intended to demonstrate the value of optimizing the probability of study suc- cess (PrSS) by leveraging the strength of both Bayesian application and the graphical testing approach in a correlated multiple endpoint setting. This application has the potential to improve decision mak- ing and increase efciency in drug development. Evaluation of Strategies for Designing Phase 2 Dose Finding Studies Cristiana Mayer Johnson & Johnsoncmayer1@its.jnj.com Characterizing the dose response relationship for efcacy and safety remains a challenging component of drug development aiming to bring new therapeutic agents faster to the market. In Phase 2, ex- ploring the dose response relationship and the selecting the \"op- timal\" doses can result in a signicant acceleration of the overall drug development process, when done in a thorough and efcient way. The talk aims to illustrate a simulation example applied to a model-based technique to design a Phase 2 dose-nding study in the pulmonary disease therapeutic area. The MCP-Mod methodology is an efcient approach to explore the nature of the dose response relationship, estimate target doses of interest such as the minimally effective dose (MED), and adequately identify the safe and effective dose range to move into Phase 3. A specic example will be dis- cussed to highlight the critical role of modeling and simulation, and a comparison with the old-fashion conventional approach of pair- wise multiple comparisons will be made. Session 71: Next Generation Functional Data Analysis of Clustered Longitudinal/Functional Data Naisyin Wang University of Michigan nwangstat@gmail.com Modern medical diagnostic procedures now often involve records consisting of multiple longitudinal/functional data that can be con- sidered as clusters of assessment of certain underlying systems. We explore the use of model-based clustering approaches, with a focus of extending the scope of identication of different latent features embedded in the data, for the purpose of differentiating the observa- tions collected from normal versus abnormal samples. Various cri- teria, including out-of-sample prediction, were employed to gauge the use of different types of features and the bases on which they were evaluated. Effectiveness of the new methods is demonstrated using both synthetic data and data collected through medical stud- ies. Functional Data Analysis for Quantifying Brain Connectivity \u0007Hans-Georg Mueller1, Alexander Petersen1and Owen Carmichael2 1University of California, Davis 2Louisiana State University hgmueller@ucdavis.edu Functional Data Analysis for Quantifying Brain Connectivity Func- tional data analysis provides a toolbox for the analysis of data sam- ples that can be viewed as being generated by repeated realizations of an underlying (and often latent) stochastic process. The appli- cation of this methodology to paired processes (X,Y) will be illus- trated by quantifying resting state fMRI connectivity through mea- sures of functional correlation between X and Y . Resulting corre- lations between brain hubs and also of the voxels within hubs can be used for the construction of subject-specic intra-hub correlation density functions as well as inter-hub and intra-hub networks. We introduce connectivity threshold functions that quantify a selected characteristic of the network in dependency on the threshold, yield- ing one function per subject. Functional principal components can be extracted from these connectivity threshold functions and used to predict cognitive test scores. Functional Principal Component Analysis of Spatial-Temporal Point Processes with Applications in Disease Surveillance \u0007Yehua Li1and Yongtao Guan2 2015 ICSA/Graybill Joint Conference, 14-17 j91Abstracts 1Iowa State University 2University of Miami yehuali@iastate.edu In disease surveillance applications, the disease events are mod- eled by spatial-temporal point processes. We propose a new class of semiparametric generalized linear mixed Cox model for such data, where the event rate is related to some known risk factors and some unknown latent random effects. We model the latent spatial- temporal process as spatially correlated functional data, and propose composite likelihood methods based on spline approximation to es- timate the mean and covariance of the latent process. By performing functional principal component analysis to the latent process, we gain deeper understanding of the correlation structure in the point process, and we propose an empirical Bayes method to predict the latent spatial random effects, which can help highlighting the high risk spatial regions for the disease. Under an increasing domain and increasing knots asymptotic framework, we provide the asymp- totic distribution for the parametric components in the model and the asymptotic convergence rate for the functional principal compo- nent estimators. We illustrate the methodology through a simulation study and an application to the Connecticut Tumor Registry data. Localized Functional Principal Component Analysis \u0007Kehui Chen1and Jing Lei2 1University of Pittsburgh 2Carnegie Mellon University khchen@pitt.edu We propose localized functional principal component analysis (LF- PCA), looking for orthogonal basis functions with localized support regions that explain most of the variability of a random process. The LFPCA is formulated as a convex optimization problem through a novel Deflated Fantope Localization method and is implemented through an efcient algorithm to obtain the global optimum. The analyses of a country mortality data and a growth curve data reveal interesting features that cannot be found by standard FPCA meth- ods. Session 73: Non-Parametrics and Semi-Parametrics: New Regression Cao1and Lily Wang2 1Auburn University 2Iowa State University gzc0009@auburn.edu We propose a general framework for smooth regression of a func- tional response on multiple functional predictors, in which the mean of the response is related to the linear predictors via an unknown link function. Assuming that the functional predictors are observed at discrete points, we use B-spline basis functions to estimate the slope functions and the link function, and propose an iterative esti- mating procedure. Free-knot Splines Wang2 2University of A computational study of bootstrap condence bands based on a free-knot spline regression is explored for the generalized linear models in this paper. In free-knot spline regression, the knot loca- tions as additional parameters offers greater flexibility and the po-tential tobetter account for rapid shifts in slope and other important structures in the target function. However, the search for optimal solutions becomes very complicated because of \"freeing\" up the knots. In particular, the \"lethargy\" property in the objective func- tion results in many local optima with replicate knot solutions. To prevent solutions with identical knots, a penalized Quasi-likelihood estimating equation is proposed that relies on both a Jupp transfor- mation of knot locations and an added penalty on solutions with small minimal distances between knots. Focusing on logistic re- gression for binary outcome data, a parametric bootstrap is used to study the variability of the proposed estimator and to construct con- dence bands for the unknown form of the logistic regression link function. A real example is also studied. White Noise Testing and Model Diagnostic Checking for Func- tional Time Series Xianyang Zhang University of Missouri-Columbia zhangxiany@missouri.edu This paper is concerned with white noise testing and model diag- nostic checking for stationary functional time series. To test for the functional white noise null hypothesis, we propose a Cramer- von Mises type test based on the functional periodogram introduced by Panaretos and Tavakolithe (2013a). Using the Hilbert space ap- proach, we derive the asymptotic distribution of the test statistic under suitable assumptions. A new block bootstrap procedure is introduced to obtain the critical values from the non-pivotal limit- ing distribution. Compared to existing methods, our approach is robust to the dependence within white noise and it does not involve the choices of functional principal components and lag truncation number. We employ the proposed method to check the adequacy of functional linear models and functional autoregressive models of order one by testing the uncorrelatedness of the residuals. Monte Carlo simulations are provided to demonstrate the empirical advan- tages of the proposed method over existing alternatives. Our method is illustrated via anapplication to cumulative intradaily returns. Collective Estimation of Multiple Bivariate Density Func- tions with Application to University 3Shahid Beheshti University 4King Abdullah University of Science and Technology lzhou@stat.tamu.edu This paper develops a method for simultaneous estimation of den- sity functions for a collection of populations of protein backbone angle pairs using a shared set of bivariate spline basis functions that are determined by the observed data. The circular nature of angu- lar data is taken into account by imposing appropriate smoothness constraints across boundaries of the triangles. Maximum penal- ized likelihood is used to t the model and an alternating clockwise Newton-type algorithm is developed for computation. A simula- tion study shows that the collective estimation approach is statis- tically more efcient than estimating the densities separately. The proposed method was used to estimate neighbor-dependent distri- butions of protein backbone dihedral angles (i.e., Ramachandran distributions). The estimated distributions were applied to protein loop modeling, one of the most challenging open problems in pro- tein structure prediction, by feeding them into an angular-sampling- 92j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts based loop structure prediction framework. Our estimated distri- butions compared favorably to the Ramachandran distributions and the recently proposed distributions by tting a hierarchical Dirich- let process model; and in particular, our distributions showed sig- nicant improvements on the hard cases where existing methods do notwork well. Session 74: Empirical Likelihoods for Analyzing Imcom- plete Data ANOV A with \u0007Songxi Chen1and Ping-Shou Zhong2 1Iowa State University 2Michigan State University songchen@iastate.edu We carry out ANOV A comparisons of multiple treatments for longi- tudinal studies with missing values. The treatment effects are mod- eled semiparametrically via a partially linear regression which is flexible in quantifying the time effects of treatments. The empiri- cal likelihood is employed to formulate model-robust nonparamet- ric ANOV A tests for treatment effects with respect to covariates, the nonparametric time-effect functions and interactions between covariates and time. The proposed tests can be readily modied for a variety of data and model combinations, that encompasses parametric, semiparametric and nonparametric regression models; cross-sectional and longitudinal data, and with or without missing values. Calibration in Missing Data Analysis Through Empirical Like- lihood Peisong Han University of Waterloo peisonghan@uwaterloo.ca Calibration is a technique developed in sampling survey literature. Its application in missing data analysis has attracted considerable research interests recently. We will discuss how calibration, com- bined with the empirical likelihood method, can lead to many de- sirable properties when analyzing incomplete data. Especially, the robustness against model misspecication can be signicantly im- proved, resulting in the so-called multiply robust estimators. These estimators are consistent if any one of the postulated parallel para- metric models is correctly specied. Asymptotic Behavior of the Sample Average of Partial Likeli- hood for the Cox Model Jian-Jian Ren University of Maryland jjren@umd.edu The Cox model (Cox, 1972) has been the most popular model in the survival data analysis during the past several decades. In re- cent years, several authors have proposed adaptive LASSO for the Cox model to improve the computational efciency in the context of variable selection procedures. It is known that the issue of regular- ization parameter selection for penalized partial likelihood depends on the choice of the regularization parameter. For the study of the asymptotic behavior of the regularization parameter selector, in this article we surprisingly discover that Cox's partial likelihood does not behave like an ordinary likelihood in the sense that the 'sample average' of partial likelihood function diverges to innity, which is in contrast to the well-known fact that under mild regularity condi- tions, the sample average of the ordinary likelihood function con- verges to its expectation (a nite value) in probability as the samplesize n goes to innity. This is an interesting and surprising result because it has been shown that in most usual senses, Cox's partial likelihood behaves asymptotically like an ordinary likelihood. The comparison of our discovery here with the full likelihood (empiri- cal likelihood) based procedures for the Cox model (Ren and Zhou, 2011) is studied. Efcient Estimation of the Cox Model with Auxiliary Subgroup Survival Information \u0007Chiung-Yu Huang1, Jing Qin2and Huei-Ting Tsai3 of Health 3Georgetown University cyhuang@jhu.edu With the rapidly increasing availability of data in the public domain, combining information from different sources to infer about associ- ations or differences of interest has become an emerging challenge to researchers. We present a novel approach to improve efciency in estimating the survival time distribution by synthesizing informa- tion from the individual-level data with t-year survival probabilities from external sources such as disease registries. While disease reg- istries provide accurate and reliable overall survival statistics for the disease population, critical pieces of information that influence both choice of treatment and clinical outcomes usually are not available in the registry database. To combine with the published informa- tion, we propose to summarize the external survival information via a system of nonlinear population moments and estimate the survival time model using empirical likelihood methods. The proposed ap- proach is more flexible than the conventional meta-analysis in the sense that it can automatically combine survival information for dif- ferent subgroups and the information may be derived from different studies. Moreover, an extended estimator that allows for a differ- ent baseline risk in the aggregate data is also studied. Empirical likelihood ratio tests are proposed to examine whether the auxiliary survival information is consistent with the individual-level data. Session 75: Model Selection in Complex Data Settings Meta-analysis Based Variable Selection for Gene Expression Data Quefeng Li1,\u0007Sijian Wang2, Menggang Yu2and Jun Shao2 1The at Chapel Hill 2University of Wisconsin-Madison swang@biostat.wisc.edu Recent advance in biotechnology and its wide applicationshave led to the generation of many high-dimensional gene expression data sets that can be used to address similar biological questions. Meta- analysis plays an important role in summarizing and synthesizing scientic evidence from multiple studies. When the dimensions of datasets are high, it is desirable to incorporate variable selection into meta-analysis to improve model interpretation and prediction. In this talk, we propose two novel methods for variable selection with high-dimensional meta-data. Our methods not only borrow strength across multiple data sets to boost the power to identify important genes, but also keep the selection flexibility among data sets to take into account data heterogeneity. Our methods can incorporate prior biological knowledge, such as pathway information, into the mod- els. We show that our method possesses the gene selection con- sistency with NP-dimensionality. Simulation studies demonstrate the good performance of our method. We applied our meta-lasso method to a meta-analysis of cardiovascular studies. The analysis results are clinically meaningful. 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j93Abstracts Structural Discovery for Joint Models of Longitudinal and Sur- vival Outcomes \u0007Zangdong He, Wanzhu Tu and Zhangsheng Yu Indiana University zanghe@iu.edu Joint models of longitudinal and survival outcomes have been used with increasing frequency in clinical investigations. Correct spec- ication of functional forms of independent variables is essential for practical data analysis. Structural discovery in both longitudinal and survival components functions as a necessary safeguard against model misspecication. However, structural discovery in such mod- els has not been studied. No existing computational tools, to the best of our knowledge, have been made available to practitioners. In this paper, we describe a penalized likelihood method with adaptive least absolute shrinkage and selection operator (ALASSO) penalty functions for structural discovery in joint models by decomposing the independent variable effect into linear and nonlinear compo- nents. Then the selection of linear and nonlinear components mim- ics the selection of xed and random effects in the mixed-effects selection. In doing so, ALASSO and group ALASSO are used to select linear and nonlinear components, correspondingly. To re- duce the estimation bias resulted from penalization, we propose a two-stage selection procedure in which the magnitude of the bias is ameliorated in the second stage. The penalized likelihood is approx- imated by Gaussian quadrature and optimized by an EM algorithm. Simulation study showed excellent selection results in the rst stage and small estimation biases in the second stage. To illustrate, we an- alyzed a longitudinally observed clinical marker and patient survival in a cohort of cancer patients. An Empirical Bayes Approach to Integrate Multiple GWAS with Gene Expressions from Multiple Tissues \u0007Jin Liu1and Can Yang2 1Duke-NUS 2Hong Kong Baptist University jin.liu@duke-nus.edu.sg date, a large number of genome-wide association studies (GWAS) have been conducted. With advancement of array tech- niques, there are a large number of genomic data available from multiple sources: e.g., gene expression data from multiple tissues. The unbiased tissue studies can reveal new dimensions of biological effects [Dermitzakis, 2012]. Thus, it becomes essential to integrate gene expression from multiple tissues with GWAS that can increase the statistical power of the analysis of a single GWAS. We propose to use empirical-Bayes-based approach to model the status of each gene (null or non-null) enhanced by gene expression from tissues. We develop an expectation-maximization (EM) algorithm to opti- mize the corresponding complete log-likelihood function. These methods can jointly analyze two or more GWAS at the same time to test for the \"pleiotropic\" effects. We can also evaluate the sig- nicance of integrating a tissue. To integrate multiple tissues, we propose a three-stage strategy using penalized linear discriminant analysis (LDA) to transform expressions from multiple tissues to the new predictor with much lower dimension. Meanwhile, we es- timate the corresponding local false discovery rate (FDR) and for- mulate the hypothesis testing for \"pleiotropy\" and identication of associated tissues. Simulation studies are used to evaluate nite sample performance. We make comparison under different level of \"pleiotropy\" using generative model. Rheumatoid arthritis and type-1 diabetes from the Wellcome Trust Case Control Consortium (WTCCC) together with gene expression from multiple tissues are analyzed using the proposed approach.Model Selection in in multivariate semiparametric regres- sion for longitudinal data. We select xed and random effects us- ing a maximum penalized likelihood method with the adaptive least absolute shrinkage and selection operator (LASSO) penalty. The interdependence structure among multiple outcomes is determined through random effects selection. Additionally, interactions of in- dependent variables modeled by bivariate tensor product splines are selected using group LASSO. To implement the model selection method, we propose a two-stage expectation-maximization (EM) procedure. We assess the operating characteristics of the proposed method through a simulation study. The method is illustrated in a clinical study of blood pressure regulation in children. Session 76: Advances in Statistical Methods of Identify- ing Subgroup in Clinical Studies The Bias Correction in Comparing the Treatment Effect in Dif- ferent Subgroups of Patients from a 2Harvard University lutian@stanford.edu To test the interaction between treatment and a binary covariate, we often directly compare two na \u00a8ve estimators for the treatment effect in two subgroups with data from a randomized comparative clinical study. This method is criticized for potential bias due to the imbal- ance in important baseline characteristics for patients in the small subgroups. A novel and flexible augmentation procedure has been recently studied, for example, by Zhang and others (2008. Biomet- rics 64, 707-715) to improve the performance of the na \u00a8ve estimator for estimating the overall treatment effect utilizing the baseline co- variate information. In this talk, we will generalize this method to test the interactions and show that the resulting augmentation not only reduces the variance but also corrects the bias of the result- ing estimator for the treatment-covariate interaction. We will use numerical study as well as examples to illustrate the proposal. A Regression Tree Approach to Identifying Subgroups with Dif- ferential Treatment Effects Wei-Yin Loh University of Wisconsin-Madison loh@stat.wisc.edu For diseases such as cancer, it is often difcult to discover new treat- ments that benet all subjects. A more realistic goal is to identify subgroups of subjects for whom the treatment has a large effect. Re- gression trees are natural solutions because they partition the data space. For the subgroups to be reliable, however, it isimportant that there be no bias in the way splits are selected. We propose an approach that is unbiased and is applicable to data with censored or multivariate responses, missing predictor values, and treatments with two or more levels. Further, we introduce a bootstrap tech- nique for constructing condence intervals for selective inference in the nodes of the tree. Identifying Subgroups of Enhanced Predictive Accuracy from Longitudinal Biomarker Data Using Tree-based Approaches: 94j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts Applications to Monitoring Fetal Growth \u0007Jared Foster, Danping Liu, Paul Albert and Aiyi Liu National Institutes of Health jared.foster@nih.gov Longitudinal monitoring of biomarkers is helpful for predicting dis- ease or a poor clinical outcome. Typically these longitudinal pre- dictors are evaluated across an entire population; however, it is also possible that this prediction is only accurate in one or more sub- groups of the population. For example, recent work suggests that accurate prediction of large-for-gestational-age birth (LGA) from ultrasounds taken late in pregnancy is possible, but that this pre- diction is poor when only early ultrasound measurements are used. Thus, our goal is to identify subgroups of women for whom early prediction is more accurate, should they exist. We propose a tree- based approach, which extends the classication and regression tree (CART) methodology to a longitudinal classication setting, and si- multaneously controls the risk of false discovery of subgroups. To assess the performance of the proposed methods, extensive simula- tion studies are undertaken. The proposed methods are motivated by and applied to data from the Scandinavian Fetal Growth Study. A Bayesian Approach For Subgroup Analysis James O. Berger1,\u0007Xiaojing Wang2and Lei Shen3 1Duke University 2University of Connecticut 3Eli Lilly and Company xiaojing.wang@uconn.edu This talk discusses subgroup analysis, the goal of which is to deter- mine the heterogeneity of treatment effects across subpopulations. Searching for differences among subgroups is challenging because it is inherently a multiple testing problem with the complication that test statistics for subgroups are typically highly dependent, mak- ing simple multiplicity corrections such as the Bonferroni correc- tion too conservative. In this article, a Bayesian approach to iden- tify subgroup effects is proposed, with a scheme for assigning prior probabilities to possible subgroup effects that accounts for multi- plicity and yet allows for (preexperimental) preference tospecic subgroups. The analysis utilizes a new Bayesian model selection methodology and, as a by-product, produces individual probabili- ties of treatment effect that could be of use in personalized medicine. The analysis is illustrated on an example involving subgroup analy- sis of biomarker effects on treatments. Session 77: Recent Innovative Methodologies and Appli- cations in Genetics & Pharmacogenomics (GpGx) Tree-based Rare Variants Analyses Chi Song and\u0007Heping Zhang Yale University heping.zhang@yale.edu Since the development of next generation sequencing (NGS) tech- nology, researchers have been extending their efforts on genome- wide association studies (GWAS) from common variants to rare variants to nd the missing inheritance. Although various statistical methods have been proposed to analyze rare variants data, they gen- erally face difculties for complex disease models involving multi- ple genes. In this paper, we propose a tree-based method that adopts a non-parametric disease model and is capable of exploring gene- gene interactions. We found that our method outperforms the se- quence kernel association test (SKAT) in most of our simulation scenarios, and by notable margins in some cases. By applying thetree-based method to the Study of Addiction: Genetics and Environ- ment (SAGE) data, we successfully detected gene CTNNA2 and its 44 specic variants that increase the risk of alcoholism in women. This gene has not been detected in the SAGE data. Post hoc lit- erature search also supports the role of CTNNA2 as a likely risk gene for alcohol addiction. This nding suggests that our tree-based method can be effective in dissecting genetic variants for complex diseases using rare variants data. Composite Kernel Machine Regression Based on Likelihood Ratio Test and its Application on Genomic Studies \u0007Ni Zhao and Michael Wu Fred Hutchinson Cancer Research Center nzhao@fhcrc.org Semiparametric kernel machine regression has emerged as a power- ful and flexible tool in genomic studies in which genetic variants are grouped into biologically meaningful entities for association testing. Recent advances have expanded the method to test for the effect of multiple groups of genomic features via a composite kernel that is constructed as a weighted average of multiple kernels. Variance component testing is used to evaluate the signicance but requires xing the weighting parameters or perturbation. In this paper, we focus on the (restricted) likelihood ratio test for kernel machine re- gression with composite kernels where instead of xing the weight- ing parameter, we estimate the weighting parameter by maximizing the likelihood functions through the linear mixed model with mul- tiple variance components. We derive the spectral representation of (R)LRT in linear mixed models with multiple variance components to obtain their nite sample distribution. We conduct extensive sim- ulations to evaluate the power and type I error. Finally, we applied to proposed (R)LRT method to a real study to illustrate our method- ology. Improving the Robustness of Variable Selection and Predictive Performance of Lasso and Elastic-net Regularized Generalized Linear Models and Cox Proportional Hazard Models \u0007Feng Hong and Viswanath Devanarayan AbbVie Inc. feng.hong@abbvie.com In diagnostic and drug development applications, high-dimensional data from genomics, proteomics, imaging, etc., are generated for deriving signatures that predict patient phenotypes such as disease status/progression, drug efcacy and safety. Various statistical al- gorithms are utilized to identify an optimal subset of biomarkers, that when applied to an appropriate model, predicts the desired phe- notype. Both the composition and predictive performance of such biomarker signatures are critical. Recent algorithms proposed by Friedman et al (2010) and Simon et al (2011) for the regulariza- tion of generalized linear and cox regression models via cyclical coordinate descent are extremely useful as they are very fast and can handle different phenotypes (multinomial, counts, continuous, time-to-event). However the variable selection results tend to be un- stable and affect the composition of the biomarker signature. In this paper, we propose a Monte-Carlo approach with a cross-validation wrapper to improve the robustness and stability of the variable se- lection results and predictive performance evaluation. We illustrate the improvements via real datasets and simulations. 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j95Abstracts Session 78: Analysis and Yang Feng, Lie Wang and\u0007Xin Tong University of Southern California xint@marshall.usc.edu Most existing binary classication methods target on the optimiza- tion of the overall classication risk and may fail to serve some real- world applications such as cancer diagnosis, where users are more concerned with the risk of misclassifying one specic class than the other. Neyman-Pearson (NP) paradigm was introduced in this con- text as a novel statistical framework for asymmetric type I/II error priorities. It seeks classiers that minimize type II error while keep- ing type I error under a user specied level. This article is the rst attempt to construct classiers with theoretical performance guaran- tee under the NP paradigm in high-dimensional settings. Based on the fundamental Neyman-Pearson Lemma, we employ Naive Bayes models and use a plug-in approach to construct NP-type classiers. The proposed classiers satisfy NP oracle inequalities, which are natural NP paradigm counterparts of oracle inequalities in classical binary classication. Moreover, their numerical advantages in pri- oritized error control are demonstrated by both simulation and real data analysis. Index Models for Functional Data \u0007Peter Radchenko, Xinghao Qiao and Gareth James University of Southern California radchenk@usc.edu The regression problem involving functional predictors has many important applications and a number of functional regression meth- ods have been developed. However, a common complication in functional data analysis is one of sparsely observed curves, that is predictors that are observed, with error, on a small subset of the pos- sible time points. Such sparsely observed data induces an errors- in-variables model, where one must account for measurement er- ror in the functional predictors. Faced with sparsely observed data, most current functional regression methods simply estimate the un- observed predictors and treat them as fully observed; thus failing to account for the extra uncertainty from the measurement error. Since functional predictors are innite dimensional, performing a func- tional regression requires some form of dimension reduction. Many standard approaches use an unsupervised method, such as func- tional principal components analysis, to represent the predictors and then regress Y against the lower dimensional representation of X(t). We propose a new functional errors-in-variables approach, Sparse Index Model Functional Estimation (SIMFE), which uses a func- tional index model formulation to deal with sparsely observed pre- dictors. SIMFE has several advantages over more traditional meth- ods. First, the index model implements a non-linear regression and uses an accurate supervised method to estimate the lower dimen- sional space into which the predictors should be projected. Second, SIMFE can be applied to both scalar and functional responses and multiple predictors. Finally, SIMFE uses a mixed effects model to effectively deal with very sparsely observed functional predictors and to correctly model the measurement error. Stabilized Nearest Neighbor Classier and Its Theoretical Prop- 1Purdue University2Binghamton University qiao@math.binghamton.edu The stability of the statistical analysis is an important indicator for reproducibility, which is a critical property for the scientic re- search. It implies that similar statistical conclusions can be reached based on independent samples from the same population. In this article, we introduce a general measure of classication instability (CIS) to calibrate the sampling variability of the prediction made by a classication method. This allows us to analyze the behavior of the nearest neighbor classier. Motivated by an asymptotic expan- sion formula of the CIS of the weighted nearest neighbor classier, we propose the stabilized nearest neighbor (SNN) classier to ob- tain improvement. In theory, we prove that SNN attains the mini- max optimal convergence rate in the risk, and a sharp convergence rate in CIS, which is established in this article for general plug-in classiers under a low-noise condition. We compare the CIS and risk for SNN and some existing methods. Extensive simulation and data experiments demonstrate that SNN achieves a considerable im- provement in CIS over existing nearest neighbor classiers, with mostly equal, sometimes improved, classication accuracy. Session 79: Recent Developments on Combining Infer- ences and Hierarchical Models Statistical Issues in Health Related Quality of Life research Mounir Mesbah University Pierre et Marie Curie mounir.mesbah@upmc.fr HrQoL has become a major issue for longitudinal clinical or epi- demiological studies these last decades. It is particularly the case for chronic diseases such as HIV-infection, due to the lack of denitive cure. Long-term treatment of chronic diseases may involve some short- and long-term side-effects which can affect the HrQoL of pa- tients. So, the aim of such studies is an epidemiological surveillance of health, including HrQoL and survival. Such surveillance is prin- cipally based on comparison of longitudinal evolution of the HrQoL between different groups of patients.Statistical validation of quality of life instruments (or questionnaires) is mainly done through the validation of some specic measurement models relating the ob- served outcomes to the unobserved theoretical latent construct (the HrQoL variable that scientist aim to assess). Validation of such models, based on goodness of t (GOF) tests, is not straight for- ward, mainly because the set of variables involved in the models is partly unobserved. Goodness of t tests in the latent context still re- mains an issue. I will show in this talk, how and why the Backward Reliability Curve can be used to detect graphically non unidimen- sional instrument, and other departures from underlying theoretical measurement properties. The outcome provided by the question- naire is most often a categorical response, so the use of a generalized linear mixed model to analyze the evolution of the latent HrQoL is straightforward. Inside this framework, choice of a good measure- ment model and an a priori distribution for the longitudinal latent variable is the main issue. This issue is, in the HrQoL eld com- plicated by the possible occurrence for part of the population of a shifted response. In this talk, I will give an overview about the cur- rent research in Health Related Quality of Life (HrQoL) focusing on some important challenging issues for statistical science. ROC-based Meta Analysis with Individual Level Information Lu Countryman2, Julie Dicarlo2and Charles Peterfy2 Colorado, June 14-17Abstracts 1Stanford University 2Spire Sciences ylu1@stanford.edu Special statistical methods are needed to conduct meta analysis for investigating diagnostic value of a biomarker via reported sensitivity and specicity combinations from individual studies. Current sta- tistical method requires special modeling to restrict the shape of the underlying Receiver Operational Characteristic (ROC) curves since the central question is to construct the entire ROC curve by combin- ing often a moderate number of pairs of sensitivity and specicity. However, if individual level data are available for all or part of the studies used in the meta analysis, the information can be used to re- lax the stringent assumptions on the ROC curve and allow flexible combination of various summary measures related to ROC curve such as pairs of sensitivity and specicity and area under the ROC curve. This is in contrast to the meta analysis evaluating the treat- ment effect where the individual level information may not provide substantial additive value in additional to the study-level summary. In this talk, we will propose a novel semi-parametric methods for performing meta analysis on diagnostic values measured by ROC curve based on individual level information from part of the in- volved studies. Real data example and numerical studies will be used to illustrate and study the operational characteristics of pro- posed methods. Combining Nonparametric Inferences Using Data Depth For the purpose of combining inferences from several nonparamet- ric studies for a common hypothesis, we develop a new methodol- ogy using the concepts of data depth and condence distribution. A condence distribution (CD) is a sample-dependent distribution function that can be used to estimate parameters of interest. It is a purely frequentist concept yet can be viewed as a \"distribu- tion estimator\" of the parameter of interest. Examples Efron's bootstrap distribution and Fraser's signicance func- tion (also referred to as p-value function). In recent years, the con- cept of CD has attracted renewed interest and has shown high po- tential to be an effective tool in statistical inference. In this project, we use the concept of CD, coupled with data depth, to develop a new approach for combining the test results from several inde- pendent studies for a common multivariate nonparametric hypothe- sis. Specically, in each study, we apply data depth and bootstraps to obtain a p-value function for the common hypothesis. The p- value functions are then combined under the framework of com- bining condence distributions. This approach has several advan- tages. First, it allows us to resample directly from the empirical distribution, rather than from the estimated population distribution satisfying the null constraints. Second, it enables us to obtain test results directly without having to construct an explicit test statis- tic and then establish or approximate its sampling distribution. The proposed method provides a valid inference approach for a broad class of testing problems involving multiple studies where the pa- rameters of interest can be either nite or innite dimensional. The method will be illustrated using simulations and flight data from the Federal Aviation Administration (FAA). Latent Quality Models for Document Networks \u0007Linda Tan1, Aik Hui Chan2and Tian Zheng11Columbia University 2Naitional University of Singapore st2924@columbia.edu We present the latent quality model (LQM) for joint modeling of topics and citations in document networks. The LQM combines the strengths of the latent Dirichlet allocation (LDA) and the mixed membership stochastic blockmodel (MMB), and associates each document with a latent quality score. This score provides a topic- free measure of the impact of a document, which is different from the raw count of citations. We develop an efcient algorithm for tting the LQM using variational methods. To scale up to large networks, we develop an online variant using stochastic gradient methods and case-control likelihood approximation. We evaluate the performance of the LQM using the benchmark KDD Cup 2003 dataset with approximately 30,000 high energy physics papers and demonstrate that LQM can improve citation prediction signicantly. Session 80: Recent Advances in Development and Evalu- ation of Predictive Biomarkers Identifying Optimal Biomarker Combinations for Treatment Selection through Randomized Controlled Trials Ying Huang Fred Hutchinson Cancer Research Center yhuang@fhcrc.org Biomarkers associated with treatment-effect heterogeneity can be used to make treatment recommendations that optimize individ- ual clinical outcomes. To accomplish this, statistical methods are needed to generate marker-based treatment-selection rules that can most effectively reduce the population burden due to disease and treatment. Compared to the standard approach of risk modeling to derive treatment-selection rules, a more robust approach is to di- rectly minimize an unbiased estimate of total disease and treatment burden among a pre-specied class of rules. This problem is one of minimizing a weighted sum of 0-1 loss function, which is computa- tionally challenging to solve due to the non-smoothness of 0-1 loss. We develop a method that derives marker combinations to minimize the weighted sum of the Ramp loss function that approximates the 0-1 loss, based on data from randomized trials. The algorithm esti- mates treatment-selection rules by repetitively minimizing a smooth and differentiable objective function. Feature selection is further incorporated through the use of an L1 penalty. The advantage of the proposed estimator compared to existing approaches is demon- strated through extensive simulation studies. We illustrate the ap- plication of the method in host-genetics data from an HIV vaccine trial. The Challenge in Making Inference about a Biomarker's Pre- dictive Capacity Holly Janes Fred Hutchinson Cancer Research Center hjanes@fredhutch.ort Biomarkers that predict risk of an adverse outcome are highly sought after in many clinical contexts for guiding the use of inter- ventions to prevent the adverse outcome. A wide variety of statis- tical measures for characterizing the predictive capacity or perfor- mance of a risk model, and for contrasting the performance of dif- ferent models, have been proposed. Often, the same data are used both to t the risk model and to estimate its performance. In this setting, traditional approaches to doing inference about model per- formance, for example using normal theory or the bootstrap, do not 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j97Abstracts perform well. We show that this is because the model performance estimators are non-regular, and document the poor performance in simulations based on published studies. We also contrast measures of the performance of the tted risk model and of the performance of the true risk model, and show that inference about either is prob- lematic using traditional approaches. We discuss the practical im- plications of these results and provide recommendations for data analysis. A Potential Outcomes Framework for Evaluating Predictive Administration Institutes of Health zhiwei.zhang@fda.hhs.gov Predictive or treatment selection biomarkers are usually evaluated in a subgroup or regression analysis with focus on the treatment- by-marker interaction. However, the strength of the interaction is not directly related to the predictive value of the biomarker. The latter concept can be crystalized under a potential outcomes frame- work in which a predictive biomarker is considered a predictor for a desirable treatment benet dened by comparing potential out- comes for different treatments. Under this approach, a predictive biomarker can evaluated using familiar concepts in prediction and classication. A major challenge in this approach is that the desired treatment benet is unobservable because each patient can receive only one treatment in a typical study. One possible solution to this problem is to assume monotonicity of potential outcomes, with one treatment dominating the other in all patients. Motivated by an HIV example that appears to violate the monotonicity assumption, we propose a different approach based on covariates and random ef- fects for evaluating predictive biomarkers under the potential out- comes framework. Under the proposed approach, the parameters of interest can be identied by assuming conditional independence of potential outcomes given observed covariates, and a sensitivity analysis can be performed by incorporating an unobserved random effect that accounts for any residual dependence. Application of this approach to the motivating example shows that baseline viral load and CD4 cell count are both useful as predictive biomarkers for choosing antiretroviral drugs for treatment-naive patients. Session 81: What Are the Expected Professional Behav- iors After Statistics Degrees What Are the Expected Professional Behaviors After Statistics Degrees \u0007Richard Davis1,\u0007Susan Murphy2\u0007Jean Opsomer3 samurphy@umich.edu; jopsomer@stat.colostate.edu Many young ICSA members are fresh degree holders. It is vital for their career development and success in industry and academia for them to follow expected professional behaviors in our statistics community and workplace. In fact, knowing the expectations help ease anxiety and improve quality of life for these members as well because professional opportunities are results of both emotional in- telligence and intellectual intelligence. Our panelists are leaders in our profession and we encourage participants to come to the panel with prepared questions.Session 82: The Jiann-Ping Hsu Invited Session on Bio- statistical and Regulatory Sciences A Generalized Birth and Death Process for Modeling the Fates of Gene Duplication Jing Zhao1, Ashley 3Georgia Southern University lliu@uga.edu Several biological models have been proposed to depict the mech- anisms that lead to different evolutionary fates of a gene duplicate. In this paper, we develop a probabilistic model for understanding the duplication/loss process under 4 different mechanisms of gene retention (nonfunctionalization, neofunctionalization, subfunction- alization, and dosage balance), which can produce distinct patterns for the loss rate of a duplicate over time. The probabilistic model for duplication times is based on the reconstruction process with a time- dependent death rate that varies across 4 different mechanisms. We have derived the conditional density function of duplication times, given the rst duplication time and the number of gene copies at the present time. The conditional density function can be used to simulate duplication times under different mechanisms for a xed number of gene copies at the present time. The likelihood function for duplication times can be used to nd the maximum likelihood estimates of model parameters. Duplication times simulated from different mechanisms exhibit distinct patterns, indicating that the proposed probabilistic model can be used to reveal the underlying mechanism that drives the process of gene duplication and loss dur- ing the history of a gene family. A Nonparametric Approach for Partial Areas under the Re- ceiver Operating Characteristic Curve and Ordinal Dominance Curve Hanfang Yang1, Kun Lu2and\u0007Yichuan Zhao3 1Renmin University of China 2University of Chicago 3Georgia State University yichuan@gsu.edu The receiver operating characteristic (ROC) curve is a well-known technique used to measure the performance of a classication. From many reasons, such as economical efciency and ethical preference, people are concerned on a certain sensitivity range of area under the ROC curve, i.e., called pAUC (partial area under the ROC curve). After reversing axis, area under ordinal dominance curve is of great interest as well. Based on a novel estimator of pAUC proposed by Wang and Chang (2011), we develop nonparametric approaches to study partial AUC's for above two curves using normal approxi- mation method, jackknife method andjackknife empirical likelihood method. The simulation study demonstrates the drawback of the ex- isting method and shows the performance of three proposed meth- ods. We also compare the jackknife empirical likelihood and the normal approximation method, and verify the consistency of jack- knife variance estimator as well. The Pancreatic Cancer Serum Biomarker data set is used to illustrate the proposed methods which are useful in medical study. Analysis of Longitudinal Multivariate Outcome Data from Cou- ples Cohort Studies: Transmission Dynam- ics Xiangrong Kong 98j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts Johns Hopkins University xkong4@jhu.edu HPV is a common STI with 14 known oncogenic genotypes caus- ing anogenital carcinoma. While gender-specic infections have been well studied, one remaining uncertainty in HPV epidemiol- ogy is HPV transmission within couples. Understanding transmis- sion in couples however is complicated by the multiplicity of genital HPV genotypes and sexual partnership structures that lead to com- plex multi-faceted correlations in data generated from HPV couple cohorts, including inter-genotype, intra-couple, and temporal cor- relations. We develop a hybrid modeling approach using Markov transition model and composite pairwise likelihood for analysis of longitudinal HPV couple cohort data to identify risk factors as- sociated with HPV transmission, estimate difference in risk be- tween male-to-female and female-to-male HPV transmission, and compare genotype-specic transmission risks within couples. The method is applied on the motivating HPV couple cohort data col- lected in the male circumcision trial in Rakai, Uganda to iden- tify modiable risk factors (including male circumcision) associ- ated with HR-HPV transmission within couples. Knowledge from this analysis will contribute to the public health effort in preventing oncogenic HPV and related cancers in sub-Saharan Africa. Bayesian Nonlinear Model Selection for Cancer Center yangni87@yahoo.com Gene regulatory networks represent the regulatory relationships be- tween genes and their products and are important for exploring and dening the underlying biological processes of cellular systems. We develop a novel framework to recover the structure of non- linear gene regulatory networks using semiparametric spline-based directed acyclic graphical models. Our use of splines allows the model to have both flexibility in capturing nonlinear dependencies as well as control of overtting via shrinkage, using mixed model representations of penalized splines. We propose a novel discrete mixture prior on the smoothing parameter of the splines that allows for simultaneous selection of both linear and nonlinear functional relationships as well as inducing sparsity in the edge selection. Us- ing simulation studies, we demonstrate the superior performance of our methods in comparison with several existing approaches in terms of network reconstruction and functional selection. We ap- ply our methods to a gene expression dataset in glioblastoma mul- tiforme, which reveals several interesting and biologically relevant nonlinear relationships. Session 83: Dose Response/Finding Studies in Cheung2 1Boehringer-Ingelheim Pharmaceuticals Inc. 2Columbia University xiaoyu.jia@boehringer-ingelheim.com The continual reassessment method (CRM) is an adaptive model- based design used to estimate the maximum tolerated dose (MTD) in phase I clinical trials. The method is generally implemented in two-stage approach, whereby the model based dose escalation is activated after an initial sequence of patients are treated. We estab- lish a theoretical framework for building a two-stage CRM basedon coherence principle, and proved the unique existence of the most conservative and still coherent initial design given a CRM model. To facilitate implementation of such design, we also propose a sys- tematic approach to calibrate the initial design and model parameter in the second stage based on the theoretical framework. We demon- strate the application of the proposed design using an oncology dose nding study currently conducted at Columbia University Medical Center. The systematic calibration approach simplies the model calibration process for the two-stage continual reassessment method and yields competitive design performance comparing to the tradi- tional trial-and-error approach. A Practical Application with Interim Analysis in a Dose Rang- ing Design Xin Wang AbbVie Inc. xin.wang@abbvie.com In a clinical development program with a test drug, an interim anal- ysis is proposed for a dose-ranging study. The proposal is to per- form a trend test based on interim data to facilitate a \"Go/No Go\" decision. If the interim results are promising, then the study con- tinues to completion, and various doses can be studied using the entire data set. This presentation describes the background of this study design. Simulations are preformed to evaluate this proposal under various settings. Probabilities of Go and No Go at interim are assessed, along with the probabilities of nding the efcacious dose(s). Simulation results indicate the proposed approach is robust, model independent, and easy to communicate with non-statisticians. Design Considerations in Dose Finding Studies Xin Zhao Johnson & Johnson xzhao121@gmail.com Dose nding is of the utmost importance during clinical develop- ment of a new drug. Depending on the specic therapeutic area, a dose-nding study is usually conducted at late Phase I or early Phase II stage of a drug development. The main objective of such studies is to elucidate clinical efcacy in the intended patient population and to dene the dosage and dosage schedule. An adequate dose-nding study shows the optimal doses in late stage Phase II and/or III trials, thereby saving time and effort and reducing the number of patients required. In this talk, we will explain how to design a dose-nding study and the impact of such design in overall drug development strategy. Specically some design considerations utilizing adaptive and/or Bayesian techniques will be presented. Case studies from various therapeutic areas will be shared to illustrate the concepts. Dose Response Relationship in a Phase 1b Dose Ranging Study in Subjects with Chronic Hepatitis C Virus Infection Di An Gilead Sciences, Inc. di.an@gilead.com Clinical trials for the development of new drug involve several phases, with different doses of study drug, participant population, and numbers of participants. Phase 1b studies are generally de- signed to evaluate safety and tolerability of multiple doses of the study drug, and assess the pharmacokinetics and pharmacodynam- ics. In a recent phase 1b multiple dose-ranging study, subjects with Chronic Hepatitis C Virus Infection from several cohorts have re- ceived 3 days of dosing of the study drug. Subjects in each cohort were administered multiple levels of doses or matching placebo. Blood samples were collected at various time points prior to, dur- ing and after treatment. Antiviral activity of the drug was evaluated 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j99Abstracts by the HCV RNA level and analyzed as categorical and continuous endpoints. Dose-response relationship across dose levels was ex- plored for different cohorts and efcacy endpoints. An Emax model was applied using the SAS PROC NLMIXED procedure. Simula- tion was also performed with similar data setting and larger sample size. Session 84: Design More Efcient Adaptive Clinical Tri- als Using Biomarkers Sequential Designs for Individualized Dosing in Phase I Cancer Clinical Trials \u0007Xuezhou Mao1and Ying Kuen Cheung2 1Sano-aventis U.S. LLC. 2Columbia University maoxuezhou@gmail.com This research presents novel dose-nding designs that adjust for in- dividual pharmacokinetic variability inphase I cancer clinical trials. Extending from a single compartmental model in pharmacokinetic theory, we postulate a two-effect linear model to describe the rela- tionship between the area under concentration-time curve, dose and predicted clearance.We propose a repeated least squares procedure that aims to sequentially determine dose according to a subject's ability of metabolizing the drug. To guarantee consistent estimation of the individualized dosing function at the end of a trial, we apply repeated least squares subject to a constraint based on an eigenvalue theory for stochastic linear regression. We empirically determine the convergence rate of the eigenvalue constraint using a real data set from an irinotecan study in colorectal carcinoma patients, and calibrate the procedure to minimize a loss function that accounts for the dosing costs of study subjects and future patients. When com- pared to the standard dosing method using a patient's body surface area, our simulation results demonstrate that our proposed proce- dures control the overall dosing cost and allow for precise estima- tion of the individualized dosing function. Stratication Free Biomarker Designs for Randomized Trials with Adaptive Enrichment Noah Simon University of Washington nrsimon@uw.edu The biomedical eld has recently focused on developing targeted therapies, designed to be effective in only some subset of the pop- ulation with a given disease. However, for many new treatments, characterizing this subset has been a challenge. Often, at the start of large-scale trials the subset is only rudimentarily understood. This leads practitioners to either 1) run an all-comers trial without use of the biomarker or 2) use a poorly characterized biomarker that may miss parts of the true target population and potentially incorrectly indicate a drug from a successful trial. In this talk we will discuss a class of adaptive enrichment designs: clinical trial designs that al- low the simultaneous construction and use of a biomarker, during an ongoing trial, to adaptively enrich the enrolled population. For poorly characterized biomarkers, these trials can signicantly im- prove power while still controlling type one error. However there are additional challenges in this framework: How do we adapt our enrollment criteria in an \"optimal\" way? (what are we trying to op- timize for?) How do we run a formal statistical test after updating our enrollment criteria? How do we estimate an unbiased treatment effect-size in our \"selected population\"? (combating a potential se-lection bias) In this talk we will give an overview of a class of clin- ical trial designs and tools that address these questions. Bayesian Predictive Modeling for Personalized Treatment Se- lection in Oncology Junsheng Ma, Francesco Stingo and\u0007Brian Hobbs M. D. Anderson Cancer Center bphobbs@mdanderson.org Cancer is a complex dynamic microevolutionary process. Treatment requires understanding of the alterations within cell signaling path- ways that enable cancer cells to evade cell death, proliferate, and migrate. Moreover, the extent of variation in the genomes of cancer patients and among cancer cells within the same tumor make the disease inherently heterogeneous. Future breakthroughs in person- alized medicine will rely on molecular signatures that derive from synthesis of multifarious interdependent molecular quantities. In this presentation, I will introduce a Bayesian predictive approach to personalized treatment selection for new patients based on the treat- ment histories and molecular measurements of previously treated patients. The method formalizes the process for choosing an op- timal therapy in consideration of the extent to which the new un- treated patient's tumor exhibits similarity with previously treated patients. Optimal Marker-Adaptive Designs for Targeted Therapy Based Zang1, Suyu Liu2and\u0007Ying Anderson Cancer Center yyuan@mdanderson.org Targeted therapy revolutionizes the way physicians treat cancer and other diseases, enabling them to adaptively select individualized treatment according to the patient's biomarker prole. The imple- mentation of targeted therapy requires that the biomarkers are accu- rately measured, which may not always be feasible in practice. In this article, we propose two optimal marker-adaptive trial designs in which the biomarkers are subject to measurement errors. The rst design focuses on a patient's individual benet and minimizes the treatment assignment error so that each patient has the highest prob- ability of being assigned to the treatment that matches his/her true biomarker status. The second design focuses on the group benet, which maximizes the overall response rate for all the patients en- rolled in the trial. We develop a Wald test to evaluate the treatment effects for marker subgroups at the end of the trial and derive the corresponding asymptotic power function. Simulation studies and an application to a lymphoma cancer trial show that the proposed optimal designs achieve our design goal and obtain desirable oper- ating characteristics. Session 85: Advances in Nonparametric and Semipara- metric Statistics Quantile Regression for Extraordinarily Large Data Stanislav Volgushev1and\u0007Guang Cheng2 1Cornell University 2Purdue University guang.cheng.stat@gmail.com One complexity of massive data comes from the accumulating er- rors that are often unknown and may even have varying shapes as data grows. In this talk, we consider a general quantile-based mod- elling that even allows the unknown error distribution to be arbitrar- ily different across all sub-populations. A delicate analysis on the 100j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts computational-and-statistical tradeoff is further carried out based on nonparametric sieve estimation. A Validated Information Criterion to Determine the Structural Dimension in Dimension Reduction Models \u0007Yanyuan Ma1and Xinyu Zhang2 1University of South Carolina 2Chinese academy of sciences yanyuanma@gmail.com A crucial component in performing sufcient dimension reduction is to determine the structuraldimension of the reduction model. We propose a novel information criterion-based method to achieve this purpose, whose special feature is that when examining the goodness-of-t of the current model, we need to obtain model eval- uation by using an enlarged candidate model. Although the pro- cedure does not require estimation under the enlarged model with dimension k + 1, the decision on how well the current model with dimension k ts relies on the validation provided by the enlarged model. This leads to the name validated information criterion, cal- culated as VIC(k). The method is different from existing informa- tion criteria based model selection methods. It breaks free from the dependence on the connection between dimension reduction mod- els and their corresponding matrix eigen-structures, which heavily relies on a linearity condition that we no longer assume. Its consis- tency is proved and its nite sample performance is demonstrated numerically. Systematic Clustering and Network Structures: a New Non- parametric Approach that Reveals Unprecedented Structures and Patterns, with Applications to Large CMS Data. Junheng Ma,\u0007Jiayang Sun and Gq Zhang Case Western Reserve University jsun@case.edu Mining valuable clinical information for new discovery fromhuge medical data challenges modern analytics in both statistics and com- puter science. Combining modern statistics and computer science techniques we developed a Numerical Formal Concept Analysis (nFCA) technique, with an R package that interfaces with Ruby and Graphviz, a beautiful computer science visualization software. nFCA overcomes the limitation of FCA and standard statistical clustering techniques by delivering systematic clustering and net- work structures based on numerical data. In this talk, we introduce the building methodology of nFCA, showcase the functionality of our nfca() package and then focus on an innovative translation re- search, building disease/risk factor networks using nFCA on thou- sands ICD-9 codes. from large HCFA data from CMS. We reveal new ndings, and discuss limitations and future possibilities. Semiparametric Model Building for Regression Models with Time-Varying Parameters Ting Zhang Boston University tingz@bu.edu We consider the problem of semiparametric model building for lin- ear regression models with potentially time-varying coefcients. By allowing the response variable and explanatory variables be jointly a nonstationary process, the proposed methods are widely applicable to nonstationary and dependent observations. We propose a local linear shrinkage method that can simultaneously achieve parameter estimation and variable selection. Its selection consistency and the favorable oracle property are established. Due to the fear of losing efciency, an information criterion is further proposed for distin- guishing between time-varying and time-constant components. Nu-merical examples are presented to illustrate the proposed methods. Session 86: Cutting-Edge New Tools for Statistical Anal- ysis and Modeling Web-based Analytics for Business Decision Making Sam Weerahandi Pzer Inc. Weerahandi@aol.com Web-based Business analytics are of interest and practical impor- tance in most areas of statistical practice, especially in Business Decision Making. Web-publishing of analytics should also prove to be of interest to professors who provide external and internal consulting to non-statisticians, and will come handy when they de- liver results in a manner that clients could run them with their own scenarios and parameters. This approach does not require clients to have any knowledge in Statistical Techniques or Statistical Pro- gramming languages - they need to know only the business prob- lem.This presentation will provide an overview of State-of-the-Art in Corporate America, Business Intelligence software, and how to web-publish your analytics, followed by a demo of some interesting analytics showing how almost any business could benet from web- based analytics in a variety of applications Business Management. A GUI Software for Synchronizing Study Design, Statistical Analyses, and Reporting into Simple Clicks Yanwei Zhang Pzer Inc. yanwei.zhang@pfizer.com This talk will introduce a very powerful yet extremely easy to use software, iSTAT, that allows lab scientists, clinicians, pharmacolo- gists, and statisticians to calculate sample size (from both Frequen- tist's and Bayesian perspectives) for planning studies, perform a wide range of statistical analyses, conduct statistical and pharma- cological modeling and simulation, monitor trials (using group se- quential method, Bayesian predictive approach, and prediction in- terval plots), visualize data interactively, and generate study report instantaneously through a Graphical User Interface (GUI) by simple clicks Bayesian Mechanism to Enhance Financial Value of Clinical De- velopment Portfolio Shu Han Pzer Inc. shu.han2@pfizer.com Bayesian statistics has been increasingly used in clinical develop- ment as it provides distinctive advantages in devising adaptive clin- ical trials, modeling flexibility, and incorporation of totality data for decision making. This presentation focuses on employing Bayesian mechanism for dynamically measuring and optimizing nancial value of a clinical development portfolio which can include inves- tigational therapeutics and/or medical devices. The Bayesian mod- eling of and the link to key nancial measurements of a portfolio, such as expected net present value (eNPV) and expected internal rate of return (eIRR), will be introduced, and the mechanism of us- ing Bayesian adaptive clinical development platform to maximize portfolio nancial value will be described. An R Package Suite for Meta-analysis in Differentially Ex- pressed 2University of Pittsburgh 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j101Abstracts jli4@hfhs.org With the rapid advances and prevalence of high-throughput genomic technologies, integrating information of multiple relevant genomic studies has brought new challenges. Meta-analysis has become a frequently used tool in biomedical research. Little effort, however, has been made to develop a systematic pipeline and user-friendly software. Here we present MetaOmics, a suite of three R pack- ages MetaQC, MetaDE and MetaPath with the focus on MetaDE package. MetaDE was developed for candidate marker detection by integrating data from multiple sources. The system allows flexible input of experimental data, various clinical outcome (case-control, multi-class, continuous or survival). It generates informative sum- mary output and visualization plots, operates on different operation systems and can be expanded to include new algorithms or com- bine different types of genomic data. This software suite provides a comprehensive tool to conveniently implement and compare various genomic meta-analysis pipelines. Session 87: Advanced Methods for Graphical Models Learning Causal Networks bxl9@psu.edu In this paper we introduce a statistical model, called additively faith- ful directed acyclic graph (AFDAG), for causal learning from ob- servational data. Our approach is based on additive conditional independence (ACI), a recently proposed three-way statistical re- lation that shares many similarities with conditional independence. However, the nonparametric characterization of ACI does not in- volve multivariate kernel, so is distinct from conditional indepen- dence. Due to this special feature, AFDAG enjoys the flexibility of a nonparametric estimator but avoids the curse of dimensional- ity when handling high-dimensional networks. We develop an es- timator for AFDAG based on a linear operator that characterizes ACI. We propose a modied PC-algorithm to implement the esti- mating procedures efciently, so that their complexity is determined by the density of edges and grows only in a polynomial order of the network size. We also establish the consistency and conver- gence rates of our estimator. Through simulation studies we show that our method outperforms existing methods when commonly as- sumed conditions such as Gaussian or Gaussian copula distribu- tions do not hold. Finally, the usefulness of AFDAG formulation is demonstrated through its application on a proteomics data set. Statistical Modeling of RNase-seq for Genome-wide Inference of RNA Structure Zhengqing Ouyang The Jackson Laboratory zhengqing.ouyang@jax.org Recent studies have revealed signicant roles for RNA structure in almost every step of RNA processing, including transcription, splicing, transport, and translation. RNase footprinting coupled with high-throughput sequencing (RNase-seq) has emerged to dis- sect RNA structures at the genome scale. Combining structure- specic RNases together can provide complementary information on the structural features (such as single-strand or double-strand). However, the inference of RNA structural features from RNase-seq remains challenging because of the issues of data sparsity, signal variability, and correlation as well as contradiction among multi- ple RNases. We present a probabilistic modeling framework thatsystematically captures the correlation structure and variability of multiple RNase proles along the transcripts. We apply our method on simulated datasets and genome-wide We demonstrate that our joint modeling approach outputs interpretable RNA structural features, while approaches that analyze the V1 or S1 prole separately do not. Furthermore, com- paring to simple thresholding, our probabilistic modeling approach probes 53% more nucleotides in the yeast transcriptome without compromising accuracy, and resolves the structural ambiguity of 300,000 nucleotides with overlapping V1 and S1 peaks. We also demonstrate that using a shared latent variable for modeling RNA accessibility, our model reveals the prevalent influence of three- dimensional conformation of RNA on RNase footprinting. Distance Shrinkage and Euclidean Embedding via Regularized Kernel Estimation Ming Yuan University of Wisconsin-Madison myuan@stat.wisc.edu Although recovering an Euclidean distance matrix from noisy ob- servations is a common problem in practice, how well this could be done remains largely unknown. To ll in this void, we study a sim- ple distance matrix estimate based upon the so-called regularized kernel estimate. We show that such an estimate can be characterized as simply applying a constant amount of shrinkage to all observed pairwise distances. This fact allows us to establish risk bounds for the estimate implying that the true distances can be estimated con- sistently in an average sense as the number of objects increases. In addition, such a characterization suggests an efcient algorithm to compute the distance matrix estimator, as an alternative to the usual second order cone programming known not to scale well for large problems. Numerical experiments and an application in visualizing the diversity of Vpu protein sequences from a recent HIV-1 study further demonstrate the practical merits of the proposed method. Detecting Overlapping Communities in Networks with Spectral Methods Yuan Zhang, Elizaveta Levina and\u0007Ji Zhu University of Michigan jizhu@umich.edu Community detection is a fundamental problem in network analy- sis. In practice, it often occurs that the communities overlap, which makes the problem more challenging. Here we propose a general, flexible, and interpretable generative model for overlapping com- munities, which can be thought of as a generalization of the degree- corrected stochastic block model. We develop an efcient spectral algorithm for estimating the community memberships, which deals with the overlaps by employing the K-medians algorithm rather than the usual K-means for clustering in the spectral domain. We show that the algorithm is asymptotically consistent when networks are not too sparse and the overlaps between communities not too large. Numerical experiments on both simulated networks and many real social networks demonstrate that our method performs well com- pared to a number of benchmark methods for overlapping commu- nity detection. Session 88: Advanced Development in Big Data Analytics Tools Clique-based Method for Social Network Clustering \u0007Dipak Dey and Guang Ouyang 102j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts University of Connecticut dipak.dey@uconn.edu Many networks in real life are found to divide naturally into small communities. Examples include Facebook, LinkedIn, computer networks, and metabolic network etc. The problem of detecting clusters or communities is of great importance. An effective and commonly used measurement on the quality of a clustering is called modularity, and algorithms that maximize this quantity are among the most popular network clustering approaches nowadays. Un- fortunately, modularity method has resolution limit when network is large. Here, we propose a novel network clustering algorithm, which provides user control on this limitation. In addition, we pro- vide detailed results of applying our algorithm in various networks. An analysis about the validity of our algorithm is also included. Sparse Partially Linear Additive Models additive model (GPLAM) is a flex- ible and interpretable approach to building predictive models. It combines features in an additive manner, allowing each to have either a linear or nonlinear effect on the response. However, the choice of which features to treat as linear or nonlinear is typically assumed known. Thus, to make a GPLAM a viable approach in situations in which little is known a priori about the features, one must overcome two primary model selection challenges: deciding which features toinclude in the model and determining which of these features to treat nonlinearly. We introduce the sparse partially linear additive model (SPLAM), which combines model tting and both of these model selection challenges into a single convex op- timization problem. SPLAM provides a bridge between the Lasso and sparse additive models. Through a statistical oracle inequality and thorough simulation, we demonstrate that SPLAM can outper- form other methods across a broad spectrum of statistical regimes, including the high-dimensional setting. We develop efcient algo- rithms that are applied to real data sets with half a million samples and over 45,000 features with excellent predictive performance. Clustering by Propagating Probabilities Between Data Points \u0007Guojun Gan, Yuping Zhang and Dipak Dey University of Connecticut guojun.gan@uconn.edu In this paper, we propose a graph-based clustering algorithm called \"probability propagation,\" which is able to identify clusters having spherical shapes as well as clusters having non-spherical shapes. Given a set of objects, the proposed algorithm uses local densities calculated from a kernel function and a bandwidth to initialize the probability of one object choosing another object as its attractor and then propagates the probabilities until the set of attractors become stable. Experiments on both synthetic data and real data show that the proposed method performs as expected. Clustering Time Series: University pkohli@conncoll.edu Time series clustering is common in various areas ranging from sci- ence,engineering, business, nance, economics, health care,to geo- physical studies. Considerable research has been carried out to ad-dress the clustering of stationaryand linear time series. However, in most real situations the time series rarely satisfy the assump- tions of stationarity and/or linearity. In time series literature, there is a scarcity of methods for nonstationary, nonlinear time series, and there are several open research questions with respect to their statistical and computational efciency. In this work, we propose a clustering scheme based on the use of Polyspectral Smooth Lo- calized Complex Exponential (PSLEX) approach which can han- dle the challenges arising with nonstationarity and/or nonlinearity of the time series. The orthogonality property of the SLEX library of complex-valued orthogonal transforms facilitates the analysis of high dimensional massive time series data due to its mathemati- cal elegance. We illustrate our approach using simulated time se- ries from several nonstationary and/or nonlinear models. We also demonstrate the use of proposed method to an interesting area of nance which has applications in portfolio evaluation and diversi- cation, identifying misclassied stocks, and quantifying the effect of trends to draw interesting conclusions about specic stocks. Session 89: Recent Advances in Biostatistics Promoting Similarity of Sparsity Structures in Integrative Analysis Shuangge Ma Yale University shuangge.ma@yale.edu For data with high-dimensional covariates but small to moderate sample sizes, the analysis of single datasets often generates unsat- isfactory results. The integrative analysis of multiple independent datasets provides an effective way of pooling information and out- performs single-dataset analysis and some alternative multi-datasets approaches including meta-analysis. Under certain scenarios, mul- tiple datasets are expected to share common important covariates, that is, their models have similarity in sparsity structures. However, the existing methods do not have a mechanism to promote the sim- ilarity of sparsity structures in integrative analysis. In this study, we consider penalized variable selection and estimation in integra- tive analysis. We develop a penalization based approach, which is the rst to explicitly promote the similarity of sparsity structures. Computationally it is realized using a coordinate descent algorithm. Theoretically it has the much desired consistency properties. In sim- ulation, it signicantly outperforms the competing alternative when the models in multiple datasets share common important covariates. It has better or similar performance as the alternative when there is no shared important covariate. Thus it provides a \"safe\" choice for data analysis. Applying the proposed method to three lung cancer datasets with gene expression measurements leads to models with signicantly more similar sparsity structures and better prediction performance. Graphical Models and its in Genomics \u0007Zhandong Undirected graphical models, also known as Markov networks, en- joy popularity in a variety of applications. The popular instances of these models such as Gaussian Markov Random Fields (GMRFs), Ising models, and multinomial discrete models, however do not cap- ture the characteristics of data in many settings. We introduce a new class of graphical models based on generalized linear mod- els (GLMs) by assuming that node-wise conditional distributions 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j103Abstracts arise from exponential families. Our models allow one to estimate multivariate Markov networks given any univariate exponential dis- tribution, such Poisson, negative binomial, and exponential, by tting penalized GLMs to select the neighborhood for each node. When applied to Genomics data, we demonstrated that our models can capture important dependency structures that are undetectable by other traditional network models. Threshold Regression with Censored Covariates \u0007Jing Qian1, qian@schoolph.umass.edu The problem of censored covariates arises frequently in family his- tory studies, in which an outcome of interest is regressed on an age of onset, as well as in cohort studies in which it may be necessary to adjust for duration of disease. We develop new threshold regres- sion approaches for linear regression models with covariates subject to random censoring. Compared with existing methods, the pro- posed methods are simple but effective as they avoid complicated modeling in dealing with censored covariate values. In addition to estimating the regression coefcient of the censored covariates, the threshold regression methods can also be used to test whether the effect of a censored covariate is signicant. We discuss the choice of optimal threshold which yields the most powerful test. The nite sample performance of the proposed methods are assessed through extensive simulation studies. The methods are illustrated by analyz- ing a study in Alzheimer's disease. Jointly Analyzing Spatially Correlated Visual North Carolina at Chapel Hill 3Northwestern University joshua.warren@yale.edu Glaucoma is a leading cause of irreversible blindness worldwide. Once a diagnosis is made, careful monitoring of the disease is re- quired to prevent vision loss. However, determining if the disease is progressing remains the most difcult task in the clinical setting. A common method for detecting progression includes the analysis of a time series of peripheral visual elds (VF) for a patient by expert clinicians. We introduce new methodology in the Bayesian setting in order to properly model the progression status of a patient (as de- termined by a group of expert clinicians) as a function of changes in spatially correlated sensitivities at each VF location jointly. Past modeling attempts include the analysis of global VF measures or the separate analyses of sensitivities at individual VF locations over time. The rst set of methods ignores important spatial information regarding the location of vision loss on the VF while the second set is inefcient and fails to account for spatial similarities in vision loss across the VF. Our spatial probit regression model jointly in- corporates all highly correlated VF changes in a single framework while accounting for structural similarities between neighboring VF regions. Results indicate that our method provides improved model t and predictions when compared to previously introduced mod- els. Additionally, the mapping of spatially referenced parameters across the VF provides insight into the clinicians' decision making process. This model may be clinically useful for detecting the glau- coma progression status of an individual.Session 90: Adaptive Designs and Personalized Medicine Interpretable and North Carolina State University laber@stat.ncsu.edu A treatment regime formalizes personalized medicine as a func- tion from individual patient characteristics to a recommended treat- ment. A high-quality treatment regime can improve patient out- comes while reducing cost, resource consumption, and treatment burden. Thus, there is tremendous interest in estimating treatment regimes from observational and randomized studies. However, the development of treatment regimes for application in clinical prac- tice requires the long-term, joint effort of statisticians and clinical scientists. In this collaborative process, the statistician must inte- grate clinical science into the statistical models underlying a treat- ment regime and the clinician must scrutinize the estimated treat- ment regime for scientic validity. To facilitate meaningful infor- mation exchange, it is important that estimated treatment regimes be interpretable in a subject-matter context. We propose a simple, yet flexible class of treatment regimes whose members are repre- sentable as a short list of if-then statements. Regimes in this class are immediately interpretable and are therefore an appealing choice for broad application in practice. We derive a robust estimator of the optimal regime within this class and demonstrate its nite sample performance using simulation experiments. The proposed method is illustrated with data from two clinical trials. Regression Analysis for Cumulative Incidence Function yucheng@pitt.edu In this talk we focus on regression analysis under a two-stage ran- domization setting. Even though extensive research is being car- ried out by researchers on the regression problem for dynamic treat- ment regimes, few research have been done on modeling the cu- mulative incidence function (CIF) when a two-stage randomization has been carried out. We extend the multi-state, the Fine and Gray, and the Scheike et al. regression models for modeling the CIF of dynamic treatment regimes and provide ways to implement the pro- posed models in R using the existing packages. We show the im- provement our methods provide by simulation. Optimal, Two Stage, Adaptive Enrichment Designs for Ran- domized Trials, using Sparse Linear Programming \u0007Michael Han Liu2 1Johns mrosen@jhu.edu Adaptive enrichment designs involve preplanned rules for modify- ing enrollment criteria based on accruing data in a randomized trial. These designs can be useful when it is suspected that treatment ef- fects may differ in certain subpopulations, such as those dened by a biomarker or risk factor at baseline. Two critical components of adaptive enrichment designs are the decision rule for modifying en- rollment, and the multiple testing procedure. We provide a general method for simultaneously optimizing both of these components for two stage, adaptive enrichment designs. The optimality criteria are dened in terms of expected sample size and power, under the con- straint that the familywise Type I error rate is strongly controlled. 104j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts It is infeasible to directly solve this optimization problem since it is not convex. The key to our approach is a novel representation of a discretized version of this optimization problem as a sparse lin- ear program. We apply advanced optimization tools to solve this problem to high accuracy, revealing new, optimal designs. Session 91: Recent Developments of High-Dimensional Data Inference and Its Applications Segmenting Multiple Time Series by Contemporaneous Linear Transformation: PCA for Time Series School of Economics jinyuan.chang@unimelb.edu.au We seek for a contemporaneous linear transformation for a p-variate time series such that the transformed series is segmented into sev- eral lower-dimensional subseries, and those subseries each other both contemporaneously and serially. The method may be viewed as an extension of principal component anal- ysis (PCA) for multiple time series. Technically it also boils down to an eigenanalysis for a positive denite matrix. When pis large, an additional step is required to perform a permutation in terms of either maximum cross-correlations or FDR based on multiple tests. The asymptotic theory is established for both xed p and diverg- ing p when the sample size n tends to innity. Numerical experi- ments with both simulated and real datasets indicate that the pro- posed method is an effective initial step in analysing multiple time series data, which leads to substantial dimension-reduction in mod- elling and forecasting high-dimensional linear dynamical structures. The method can also be adapted to segment multiple volatility pro- cesses. Projection Test for High-Dimensional Mean Vectors with Opti- Runze State University 2University of Minnesota huangyuan.stat@gmail.com Testing the population mean is fundamental in statistical infer- ence. When the dimensionality of a population is high, traditional Hotelling's T2test becomes practically infeasible.In this paper, we propose a new testing method for high-dimensional mean vectors. The new method projects the original sample to a lower-dimensional space and carries out a test with the projected sample. We derive the theoretical optimal direction with which the projection test pos- sesses the best power under alternatives. We further propose an esti- mation procedure for the optimal direction, so that the resulting test is an exact t-test under the normality assumption and an asymptotic chi2-test with 1 degree of freedom without the normality assump- tion. Monte Carlo simulation studies show that the new test can be much more powerful than the existing methods, while it also well retains Type I error rate.The promising performance of the new test is further illustrated in a real data example. Thresholding Tests for Signal Detection on High-Dimensional Count University yumouqiu@unl.eduWe consider the problem of detecting rare and faint signals in high- dimensional count data. This problem arises, for example, in the analysis of RNA sequencing (RNA-seq) data to detect genes dif- ferentially expressed across multiple conditions. In this paper, we consider the signal detection problem under generalized linear mod- els (GLMs) and their extensions, which include the linear model as a special case. Based on maximum likelihood estimators (MLEs), a thresholding statistic with a single threshold level is proposed to test for the existence of rare and faint signals. A Cramer type moderate deviation result for multi-dimensional MLEs with non-identically distributed data is derived, which is the prerequisite to study the properties of thresholding test statistics. For the case of linear re- gression, the detection boundary is determined, and it is shown that the proposed thresholding test can attain the boundary. A multi- threshold test is constructed by maximizing the standardized thresh- olding statistic over a set of thresholds. Extensions to generalized linear mixed models are made, where Gaussian-hermite quadrature and data cloning are used to approximate the MLEs of such mod- els. Numerical simulations and a case study on maize RNA-seq data are conducted to conrm and demonstrate the proposed testing approaches. Projected Principal Component Analysis in Factor Models Jianqing Fan1, Yuan Liao2and\u0007Weichen Wang1 1Princeton University 2University of Maryland weichenw@princeton.edu This paper introduces a Projected Principal Component Analysis (Projected-PCA), which is based on the projection of the data matrix onto a given linear space before performing the principal compo- nent analysis. When it applies to high-dimensional factor analysis, the projection removes idiosyncratic noisy components. We show that the unobserved latent factors can be more accurately estimated than the conventional PCA if the projection is genuine, or more pre- cisely, when the factor loading matrices are related to the projected linear space, and that they can be estimated accurately when the dimensionality is large, even when the sample size is nite. In an effort to more accurately estimating factor loadings, we propose a flexible semi-parametric factor model, which decomposes the factor loading matrix into the component that can be explained by subject- specic covariates and the orthogonal residual component. The co- variates effect on the factor loadings are further modeled by the ad- ditive model via sieve approximations. By using the newly pro- posed Projected-PCA, the rates of convergence of the smooth factor loading matrices are obtained, which are much faster than those of the conventional factor analysis. The convergence is achieved even when the sample size is nite and is particularly appealing in the high-dimension-low-sample-size situation. This leads us to devel- oping nonparametric tests on whether observed covariates have ex- plaining powers on the loadings and whether they fully explain the loadings. Finally, the proposed method is illustrated by both simu- lated data and the returns of the components of the S&P 500 index. Session 92: Issues in Probabilistic Models for Random Graphs Exponential-family Random Hypergraph Models for Group Relations \u0007Ryan Haunfelder, Haonan Wang and Bailey Fosdick Colorado State University bailey@stat.colostate.edu 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j105Abstracts Social network data is often recorded and studied as pairwise re- lationships. However, in many scenarios these relationships are originally observed at a group level, involving two or more actors. Representing the relations as dyadic associations is a misrepresen- tation of the data, which can affect inference and understanding of the social system under study. Hypergraphs are graph structures which allow relations that involve more than two actors. In this talk we discuss extensions of common social network features such as transitivity, shared partners, shortest paths and centrality for hyper- graphs. We also introduce a probabilistic exponential random hy- pergraph model, which builds off the exponential-family random graph model (ERGM) for dyadic relations. We discuss proper- ties of our hypergraph model and describe how the Markov depen- dence structures have nice interpretations related to social theories of group dynamics. Exponential-family Random Graph Models with Local Depen- dence Michael Schweinberger Rice University m.s@rice.edu Dependent phenomena, such as relational, spatial, and temporal phenomena, tend to be characterized by local dependence in the sense that units which are close in a well-dened sense are de- pendent.However, in contrast to spatial and temporal phenomena, relational phenomena tend to lack a natural neighborhood struc- ture in the sense that it is unknown which units are close and thus dependent. An additional complication is that the number of observations is 1, which implies that the dependence structure cannot be recovered with high probability by using conventional high-dimensional graphical models. Therefore, researchers have assumed that the dependence structure has a known form. The best-known forms of dependence structure are inspired by the Ising model in statistical physics and Markov random elds in spatial statistics and are known as Markov random graphs. However, ow- ing to the challenge of characterizing local dependence and con- structing random graph models with local dependence, conventional exponential-family random graph models with Markov dependence induce strong dependence and are not amenable to statistical infer- ence. We take rst steps to characterize local dependence in random graph models and show that local dependence endows random graph mod- els with desirable properties which make them amenable to statis- tical inference. We show that random graph models with local de- pendence satisfy a natural domain consistency condition which ev- ery model should satisfy, but conventional exponential-family ran- dom graph models do not satisfy. In addition, we discuss concen- tration of measure results which suggest that random graph models with local dependence place much mass in the interior of the sam- ple space, in contrast to conventional exponential-family random graph models. We discuss how random graph models with local dependence can be constructed by exploiting either observed or un- observed neighborhood structure. In the absence of observed neigh- borhood structure, we take a Bayesian view and express the uncer- tainty about the neighborhood structure by specifying a prior on a set of suitable neighborhood structures. We present simulation re- sults and applications to two real-world networks with ground truth. Local Structure Graph Models with Higher-Order Dependence \u0007Emily Casleton1, Mark Local Structure Graph Models (LSGMs) provide a Markov Ran- dom Field (MRF) modeling approach for random graphs, whereby each edge in the graph has a specied conditional distribution, i.e., probability of edge occurrence, dependent on explicit neighbor- hoods, or subcollections of other graph edges, that dene a con- ditional distribution. As a consequence of the conditional speci- cation, LSGMs have the advantage of allowing direct control and separate interpretation of parameters influencing large-scale (e.g., marginal means) and small-scale (i.e., dependence) structures in a graph model. This is possible through centered parameterization of MRF models, which are applied in LSGMs. However, current tech- nology for centered parameterizations in MRFs assumes pairwise- only dependence, i.e, dependence is modeled between pairs of ran- dom variables only. This creates limitations in specifying condi- tional distributions for graph edges in LSGMs. As a remedy, we ex- tend the centered parameterization for MRFs to account for triples of dependent edges in LSGMs. We also explain and numerically illustrate the importance of centered parameterizations when inter- preting model parameters and, using a MRF framework. Centered parameterizations and their increased interpretation are particularly crucial when attribute/covariate information is included in a graph model. This work advances the modeling of graph data in several important ways related to conditional model specications, state- of-the-art parameterizations and inclusions of higher-order appropriate model incorporation of covariates. Session 93: Negotiation Skills Critical for Statistical Ca- reer Development Negotiation Skills Critical for Statistical Career Development \u0007Ivan Gary2,\u0007Susan Murphy3and\u0007Wei and Company ivan chan@merck.com; mgray@american.edu; samurphy@umich.edu; shen weix1@lilly.com With the growing diversity and topics in the statistical eld, career development becomes an important topic. According to J. E. Miller and J. Miller (2011), three keys to successful negotiation are as fol- lows: (1) be condent, (2) be prepared, and (3) be willing to walk away. In this invited panel discussion session, a group of distin- guished statisticians from three different sectors, academia, indus- try and government, will address key questions regarding how to better negotiate in one's statistical career. This panel consists of esteemed statisticians and leaders from various sectors of the sta- tistical profession. Specically, the panelists will provide their per- sonal experience and guidance on successful negotiations based on their own career journeys and advancements. For example, they will discuss the issues arising from initial position, mid-career roles, and in leadership positions. Throughout the entire paths of their distinguished careers in different career sectors, and inevitably the associated struggles, the panelists will give their suggestions and recommendations on improving one's negotiation skills for atten- dees who are post-graduate statisticians, faculty members, research statisticians, practicing consultants, as well as those in leadership positions in their institutions or in professional associations includ- ing the International Chinese Statistical Association. This session will have a wide appeal to many junior, mid-career and senior atten- dees regardless of gender and career tracks. It will be particularly 106j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts benecial to those junior statisticians who are encouraged to ask important questions such as: To negotiate or not to negotiate? and How to negotiate effectively? Session C01: Disease Models, Observational Studies, and High Dimensional Regression Evaluate the Most Accurate Animal Model With Application to Pediatric Medulloblastoma \u0007Lan Gao1, Behrouz Shamsaei2and Stan Pounds3 1University of 2The University of Tennessee at Chattanooga 3St. Jude Children's Research Hospital cuilan-gao@utc.edu Animal models of human disease are commonly utilized to gain pre- clinical insight into the potential efcacy and action mode of novel drugs. The development and selection of an animal model that ac- curately mimics the human disease profoundly reduces the research timeline and resources needed to make meaningful advances in the treatment and prevention of the human disease under study. Here, we propose a statistical procedure to select the animal model that most accurately mimics the human disease in terms of genome-wide gene expression. Our procedure is designed for studies that have gene expression proles for a cohort of human disease tissue spec- imens from different subjects and gene expression proles for co- horts of disease tissue specimens for each of several animal models. First, we dene and compute a metric of similarity between each human gene expression prole and animal gene expression prole which result in multiple groups of similarities. Then a random block ANOV A model is used to compare the group means of similarities between different animal models. Finally post-hot multiple com- parison is applied to seek the \"best\" animal model of the human disease. The advantages of the proposed method are observed in simulation studies and a real example of pediatric Medulloblastoma. A Generalized Mover-Stayer Model for Disease Progressions with Death in Consideration of Age at the Study Entry \u0007Yi-Ran Lin1, Wei-Hsiung Chao2and Chen-Hsin Chen1;3 1Institute Dong Hwa University 3National Taiwan University yrlin@stat.sinica.edu.tw Multi-state models have been widely used in assessing the dynamic disease progression under investigation. In some scenarios, a frac- tion of the population may be risk free for disease progression. A mover-stayer model is used to t longitudinal data based on a mix- ture combining the sub-population of \"movers\" (for those who un- dergo a specic disease progression) with the sub-population of \"stayers\" (for those who do not). Conventional statistical litera- ture of mover-stayer models deal with the stayers only in the ini- tial state. We generalize it to a mover-stayer model, via viewing as a mixture of nite Markov models, by allowing study subjects to have various disease progressions with probabilities of staying in some subsequent state before death. The maximum likelihood es- timation procedure is implemented with the Fisher scoring method and some relevant diagnostic tools are developed for model check- ing. Using the longitudinal follow-up data from the REVEAL-HBV study which is a community-based cohort study carried out in seven townships of Taiwan, we pursue the risk evaluation of viral load elevation and associated liver disease/cancer with hepatitis B virus (HBV). A six-state mover-stayer model taking account of subjects'different ages at study entry is presented to analyze the multi-path progression from chronic hepatitis B to hepatocellular carcinoma, possibly via cirrhosis, and ending with HBV-related death or non- HBV-related death. The proposed regression analysis method can also be applied to the analysis and interpretation for studying other diseases in research of epidemiology and biobanks. Improving Cancer Mortality Rate Estimation Using Population-specic Structure in Age-standardization \u0007Beverly fuw@math.uh.edu Age-standardization is a popular statistical procedure in comparing cancer mortality rate among different populations or estimating the rate of a population across periods of time. It summarizes the age- specic mortality rates of each population through a weighted aver- age using a common population age structure as reference weights and makes the summary rates comparable across different age struc- ture. Although such practice has been employed in demography and public health studies for more than a century, the practice of select- ing a standard population structure as the reference, such as the US year 2000 population, has been shown to be lack of theoretical jus- tication, leading to a series of problems. In this study, we exam- ine cancer mortality rate of given US populations by sex and racial group. We found that although age-standardization is necessary in comparing mortality rate in different periods of a given population, taking the age structure of the given population in the year 2000 as reference largely corrects the bias introduced by using the US year 2000 population as reference, leading to improved accuracy in esti- mating the mortality rate and its trend of a given population across periods. An Augmented ADMM Algorithm for Linearly Regularized Statistical Estimation Problems Yunzhang Zhu The Ohio State University zhu.219@osu.edu We present a fast and stable algorithm for solving a class of lin- early regularized statistical estimation problem. This type of prob- lems arises in many statistical estimation procedures, such as high- dimensional linear regression with fused lasso regularization, con- vex clustering, and trend ltering, among others. We propose a so-called augmented alternating direction methods of multipliers (ADMM) algorithm to solve this class of problems. As compared to the standard ADMM algorithm, our proposal signicantly reduces the amount of computation at each iteration, while maintaining the same overall rate of convergence. We demonstrate the superior performance of the augmented ADMM algorithm on a generalized lasso problem. We also consider a new acceleration scheme for the ADMM algorithm, which works quite well in practice, especially when solving a sequence of similar problems. Finally, we discuss a possible extension and some interesting connections to two well- known algorithms in imaging literature. Public Health Impacts Following the World Trade Center At- tacks of September 11th 2001; Statistical analyses of data from residents of lower Manhattan, New York \u0007L. l\u00b4aszlo Pallos, Vinicius Antao, Jay Sapp and for Toxic Substances and Disease Registry LLaszlo.Pallos@cDc.hhs.gov Public Health Impacts Following the World Trade Center Attacks of September 11th 2001:Statistical analyses of data from residents of 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j107Abstracts lower Manhattan, New York L. C Antao, Surveillance Branch Divi- sion of Toxicology and Human Health Sciences Agency for Toxic Substances and Disease Registry Introduction In the aftermath of the World Trade Center attacks of September 11th 2011, many tens of thousands of people, aside from occupants of the Twin Towers, were exposed to dust and debris when the Towers collapsed. Owing to concerns of potential long- term health impacts, a large-scale longitudinal health registry was established with a planned duration of at least twenty years. In this present endeavor, we analyzed health effect impacts on residents of lower Manhattan. Data: The World Trade Center Health Registry (WTCHR) was ini- tially established by means of Agency for Toxic Substances and Dis- ease Registry (ATSDR) funds. The Registry consists of a baseline or wave 1 (2003-2004), rst adult follow-up or wave 2 (2007) and a third wave (2011+). At the time these analyses were developed and conducted, wave 3 data was not yet available.) The Registry was im- plemented by New York City Department of Health and Mental hy- giene and the data is administered by them. For this study, we have obtained wave 1 and wave 2 of the survey data. Wave 1 consists of 71,437 registrants of whom 14,665 were residents of lower Manhat- tan. Wave 2 consists of 46,602 registrants, all of whom had to have participated in wave 1. Of these, N=7,219 were residents of lower Manhattan and constituted the observations used in these analyses. As our focus was persistent and long term effects, we included those registrants who were residents of lower Manhattan and had com- pleted both waves. The case denitions of the six health outcomes involve several variables with differing time aspects; however, in a nutshell, by persistent we mean being identied in both wave 1 and wave 2. Seven data points which were clearly 'outliers' were initially removed. An additional 24 data points were removed since they were coded as being in Census blocks with zero population and were thus judged as not good data. We point out that the Registry remains supported by NIOSH. Methods: Our focus was to study possible associations between in- door exposures (e.g., presence of dust or debris, cleaning practices, and replacement of damaged household items) and new or worsened respiratory symptoms and diseases (persistent shortness of breath, persistent wheezing, persistent chronic cough, persistent upper res- piratory symptoms, asthma, and chronic obstructive pulmonary dis- ease or COPD). Logistic regression was used to identify key de- mographic and exposure (explanatory) variables which potentially impact and explain the self-reported health effects observed in the surveys. The Registry contains hundreds of variables, and the pool of several dozens of potential exposure and demographic variables were reduced by logical considerations to 21 for statistical model building (stepwise selection). We performed multivariate logistic regression analyses, controlling for demographics, smoking status, and exposure to the outdoor cloud of dust and debris that was gener- ated by the collapse of the WTC Towers. As the number of potential explanatory variables was rather large, stepwise model building was used to nd the parsimonious models which captured the relevant information content of the data. Results: We found signicant (p=0.05) Odds Ratios (ORs) - for each of the six outcomes we examined - for age (ranging 1.11-1.33 per decade of age), for avoiding dust cloud exposure (0.32-0.73), and for priority group (0.36-0.58 versus the lowest number, or the group closest to ground zero). Other factors signicantly affect- ing some but not all of the health outcomes (ORs different from 1.0) were as follows: Having ever smoked (1.19-1.45); race (1.54-2.35 compared with whites); sex (1.36-1.82, females compared to males); education (0.41-0.61 compared to not nishing high school); income (0.50-0.70 compared to the lowest income); expo- sures in the home such as debris, damage, ne dust, or heavy dust (1.36-1.82); cleaning behavior (1.31-1.65 elevated OR for those having dusted or mopped or vacuumed); and replacement of house- hold items such as carpeting, air-conditioning, drapes, or furniture (1.23-1.70 elevated ORs for having replaced various items). Conclusions Although these are all self-reported data, this analysis indicates that Lower Manhattan residents who suffered home dam- age and other exposures in their homes following the 9/11 attacks are more likely to report new or worsened persistent respiratory symptoms and diseases in the WTCHR. Estimation of Discrete Survival Function through the Modeling of Diagnostic Accuracy for Mismeasured Outcome Data 2Boehringer-Ingelheim Inc. hee-koung.joeng@uconn.edu Standard survival methods are inappropriate for mismeasured out- comes. Previous research has shown that outcome misclassication can bias estimation of the survival function. We develop methods to accurately estimate the survival function when the diagnostic tool used to measure the outcome of disease is not perfectly sensitive and specic. Since the diagnostic tool used to measure disease out- come is not the gold standard, the true or error-free outcomes are latent, they cannot be observed. Our method uses the negative pre- dictive value (NPV) and the positive predictive values (PPV) of the diagnostic tool to construct a bridge between the error-prone out- comes and the true outcomes. We formulate an exact relationship between the true (latent) survival function and the observed (error- prone) survival function as a formulation of time-varying NPV and PPV . We specify models for the NPV and PPV that depend only on parameters that can be easily estimated from a fraction of the observed data. Furthermore, we conduct an in depth study to accu- rately estimate the latent survival function based on the assumption that the biology that underlies the disease process follows a stochas- tic process. We further examine the performance of our method by applying it to the VIRAHEP-C data. Session C02: Design and Analysis of Clinical Trials Sample Size Re-Estimate of BE Studies with Adaptive Design Peng Roger Qu Pzer China R&D Center peng.qu@pfizer.com Adequate sample size is essential to the success of clinical trials. Within the paradigm of adaptive design, sample size re-estimate (SSRE) is relatively mature and may have seen the most applica- tions with adaptive design. Most of the SSRE methodology is un- der hypothesis testing frame work. This presentation will focus on the SSRE for bioequivalence trials, built upon the repeated con- dence interval of group sequential design. Closed form of sample size determination based on conditional power of nal analysis is derived which ensures the desired power. Hybrid version suitable to the practical consideration is recommended, with simulation results demonstrating the desired operating characteristics. Sequential Phase II Clinical Trial Design for Molecularly Tar- 108j2015 ICSA/Graybill Joint Conference, Fort Collins, \u0007Yong Zang1and Ying Yuan2 1Florida Atlantic University 2M. D. Anderson Cancer Center zangyong2008@gmail.com In the early phase development of molecularly targeted agents (MTAs) for targeted therapy, a commonly encountered situation is that the MTA is expected to be more effective for a certain biomarker subgroup, say marker-positive patients, but there is no adequate evidence to preclude that the MTA does not work for the other subgroup, i.e., marker-negative patients. After establishing that marker-positive patients benet from the treatment, it is often of great clinical interest to determine whether the treatment benet extends to marker-negative patients. We propose multi-stage opti- mal sequential trial (MOST) designs to address this practical issue in the context of phase II clinical trials. The MOST designs evalu- ate the treatment effect rst in marker-positive patients and then in marker-negative patients if needed. The designs are optimal in the sense that they minimize the expected sample size or the maximum expected sample size when the MTA is futile for both the marker- positive patients and marker-negative patients. We proposed an ef- cient, accurate optimization algorithm to nd the optimal design parameters. On Sensitivity Analysis for Missing Data using Control-based Imputation Frank Liu Merck & Co. guanghan frank liu@merck.com Control-based imputation (CBI) methods have been proposed as sensitivity analyses for longitudinal clinical trials with missing data. The CBI methods multiply impute the missing data in treatment group based on an imputation model built from the control group data. This will yield a conservative treatment effect estimate com- pared to multiple imputation (MI) under missing at random (MAR). However, the CBI analysis based on regular MI approach can be overly conservative because it not only applies discount to treatment effect estimate but also posts penalty on the variance estimate. In this talk, we will investigate the statistical properties of CBI meth- ods, and propose approaches to get accurate variance estimates us- ing both frequentist and Bayesian methods. Simulation studies un- der various missing data mechanism are conducted to illustrate the statistical properties and performance of the methods. Choosing Covariates for Adjustment in Non-Inferiority Trials Based on Influence and Disparity \u0007Katherine Nicholas, Viswanathan Ramakrishnan and Valerie Durkalski Medical University of South Carolina nicholk@musc.edu It has been shown that type I error is inflated when important co- variates are excluded from a non-inferiority analysis (Nicholas et al, 2014). Traditionally, whether or not to adjust for a covariate in a model is based solely on statistical signicance or some other crite- ria such as AIC that relates to the magnitude of the effect. In addi- tion, one may also check for colinearity with other covariates (using VIF for example) or perform tests of baseline imbalance. However, several authors suggest that these aspects should be considered si- multaneously. For example, Canner et. al. (1991) developed a statistic to determine the relative importance of including a covariate in a model based on both its effect on the outcome (which he calls influence)and its association with treatment (which he calls dispar- ity). Although Canner et. al.'s approach is under the null, Beachet. al. (1989) extended this to non-null treatment effects in the con- text of linear regression. The current research seeks to combine the methods of Canner et. al for binary outcomes with the methods of Beach et. al for non-null treatment effect in order to quantify the relative importance of including covariates in a non-inferiority trial with a binary outcome. Theoretical results are presented and applied via simulation, followed by practical application. Statistical Assessment for Establishing Biosimilarity in Follow- Medical University 4Foxconn International Company 981052@nhri.org.tw Various biological drugs will lose patent protection for upcoming few years, such as Avastin for metastatic colon cancer, Remicade for rheumatoid arthritis, and Herceptin for breast cancer. The expensive biological agent could be replaced with affordable cost follow-on biologics (biosimilar products). Biosimilar development is a com- parison of a complex biosimilar and the approved biologic agents (reference products). However, there may exist the structural differ- ences and functional differences between the two products, unlike resemble of generic drugs. There is enthusiasm that to establish an invented statistical approach for evaluating the biosimilarity be- tween a biosimilar products and a reference product. This presen- tation considers a complex design including K minus 1 reference groups, and an experimental biosimilar product. We propose a con- dence interval approach to determine if observed treatment effect are within an acceptable range for claiming consistency (highly sim- ilarity) of a primary treatment effect between two biological prod- ucts. The determination of the sample size is considered to ensure that the similarity is maintained at a desired power level, say 80 or 90%. Accordingly, a simulation result of power for claiming biosimilarity is also given. The proposed condence interval ap- proach and the general moment-based criterion are compared nu- merically. A real example is given for illustrating the applications of the proposed approach in the biosimilarity assessment between a re- combinant human growth hormone (rhGH) drug and a new biosim- ilar product. Session C03: Functional Data, Semi-parametric and Non-parametric Methods An Unbiased Measure of Integrated Volatility in the Frequency Domain Fangfang Wang University of Illinois at Chicago ffwang@uic.edu We propose an unbiased measure of ex-post price variation in the frequency domain. It is periodogram-based. When intraday prices are contaminated by market microstructure noise, the proposed esti- mator behaves like a lter: it removes the noise by ltering out high frequency periodograms. In other words, the proposed estimator converts the high frequency data into low frequency periodograms. We show, via a simulation study and an application to Microsoft transaction prices, that the proposed estimator is insensitive to the choice of sampling frequency and it is competitive with other exist- ing noise-corrected volatility measures. 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j109Abstracts Functional data analysis for density functions by transforma- tion to a Hilbert space \u0007Alexander Petersen and Hans-Georg M \u00a8uller University of California, Davis alxpetersen@gmail.com Functional data that are non-negative and have a constrained inte- gral can be considered as samples of one-dimensional density func- tions or derivatives of distributions. Such data are ubiquitous. Due to the inherent constraints, densities do not live in a vector space and therefore common Hilbert space based methods of functional data analysis are not applicable. To address this problem, we introduce a transformation approach, mapping probability densities to a Hilbert space of functions through a continuous and invertible map. Com- mon methods of functional data analysis, such as the construction of functional modes of variation, functional regression or classi- cation, are then implemented by using representations of the densi- ties in this linear space. Representations of the densities themselves are obtained by an application of the inverse map from the linear functional space to the density space. Transformations of interest include log quantile density and log hazard transformations, among others. Rates of convergence are derived for the representations that are obtained for a general class of transformations that satisfy cer- tain structural properties. If the subject-specic densities need to be estimated from data, these rates correspond to the optimal rates of convergence for density estimation. The proposed methods are illustrated through simulations and applications in brain imaging. Cross-covariance Functions for Divergence-free and Curl-free Tangent Tomoko Matsuo2 1University of California, Davis 2University of Colorado at Boulder mjfan@ucdavis.edu In this paper, we introduce valid parametric models of cross- covariance functions for divergence-free and curl-free tangent vec- tor elds on the sphere. They are constructed by applying the sur- face curl or the surface gradient operator to a univariate sufciently smooth random eld in the quadratic mean sense. Based on the celebrated Helmholtz-Hodge decomposition, we further propose a flexible parametric model for general tangent vector elds on the sphere called Mixed Matern. It has a close connection with the non- stationary covariance models through differential operators (Jun and Stein (2008); Jun (2011)) and thus fast likelihood evaluation is avail- able for large datasets when the observations are on a regular grid. The application of the Mixed Matern model is illustrated by an ocean surface wind dataset called QuikSCAT. The results show that some important characteristics of the data are captured by our pro- posed model. Empirical Likelihood-based Inference for Linear Components in Partially Linear Models Haiyan Su Montclair State University suh@mail.montclair.edu We propose an empirical likelihood (EL)-based inference for the linear component coefcient in partially linear models and partially linear mixed-effect models. The proposed method combines the projection method with the EL method. The project method is used to remove the nuisance parameter in the model and then EL method is used to construct condence intervals for the linear component. Bartlett correction method is used to correct the EL-based con- dence intervals. The test statistic is shown to follow regular chi- square distribution asymptotically. The numerical performance ofthe method under normal and non-normal error terms is evaluated through simulation studies and a real data example. Consistency of Bayesian Semiparametric Models through Joint Density Estimation Yuefeng Wu University of Missouri-St. Louis wuyue@umsl.edu The studies on the consistency of the Bayesian semi-parametric models are limited to the models that either have the priors on the parametric and nonparametric parts separately or model the para- metric part by some smooth or linear functionals on the space of the density functions of the observable random variables. A group of Bayesian semi-parametric models fall in neither of the two, e.g., a Bayesian ordinal regression based on Bayesian nonparametric es- timation for the joint probability of latent responses and observed covariates. These models are excellent in both the flexibility and the interpretation power, and getting more and more popular. The consistency of them is obtained by showing the L1consistency of the joint density estimation and the smoothness of the correspond- ing functionals from the density function space to the parametric space. Due to the latent variables, new technique is necessary and has been developed to show the L1consistency. Analysis of Water Quality in New Jersey \u0007Kaitlyn Scrudato and Haiyan Su Montclair State University scrudatok1@mail.montclair.edu To model the quality of the water at any given time with available predictors, data from bodies of water across New Jersey from 1999 to 2013 was collected from the database STORET. The water qual- ity parameters studied were Escherichia coli (E. coli) and entero- coccus with the predictors as Dissolved oxygen (DO), pH, Salinity, Temperature, Total Dissolved Solids (TDS) and Total Suspended Solids (TSS). Multiple linear regression was tted rst but didn't t the data well. Logistic regression models indicated that the odds of having unsafe water (having more than 35 cfu of enterococcus) for salt water is 0.176705 times the odds of fresh water when all other values are held constant. To improve the poor t of the mul- tiple regression models, the lasso regression method was also used to model the data. The lasso method concluded that DO, TDS and TSS were signicant to predict the amount of E. coli. Where as for enterococcus, the lasso method concluded that DO, temperature and TSS were signicant in prediction. For both enterococcus and E. coli, DO had a negative relationship with the amount of bacteria in the water. Session C04: Multiple Comparisons, Meta-analysis, and Mismeasured Outcome Data Generalized Holm's Procedure for Multiple Problem Li1, Yi hzhou@astate.edu Holm's procedure is a stepwise multiple testing procedure that can reject only one null hypothesis at each step. A generalized Holm's procedure is proposed in this article. It has been proven that this new procedure has the ability to reject several null hypotheses at each step sequentially and also strongly controls the familywise er- ror rate regardless of the dependence of individual test statistics. An 110j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Abstracts example in the clinical trial is illustrated using the newly proposed procedure. Generalized Condence Interval Approach for Combining Mul- tiple Comparisons \u0007Atiar Rahman and Ram Tiwari U.S. Food and Drug Administration mohammad.rahman@fda.hhs.gov Abstract: We introduce a generalized condence approach for com- bining multiple comparisons using techniques originally proposed by Weerhandi (1993) with application to long term animal carcino- genicity studies. We describe estimation methods of (1-alpha)% generalized condence interval. This method controls the overall false positive rate. Considerations for Two Correlated Cochran-Armitage Trend Tests \u0007Yihan Li, Su Chen, Ying Zhang and Yijie Zhou AbbVie Inc. yihan626@gmail.com In Phase 2 clinical trials, it is often of interest to investigate the re- lationship between the increasing dosage and the effect of the drug. The Cochran-Armitage trend test (Cochran, 1954; Armitage, 1955) is one of the most frequently used methods to study the underly- ing trends for binary endpoints. In some cases, more than one dose response relationship is studied within one trial. For example, a phase 2 dosing nding trial that includes both BID and QD regi- mens (with the same total daily doses) that share a common placebo control arm. Cochran-Armitage trend tests can be used to test for the dose response relationships for BID and QD regimens respec- tively, resulting in two correlated trend tests. We derived the joint distribution of the two trend test statistics under both the null and the alternative hypotheses. We then investigated the impact of the cor- relation on the type I error as well as the power. Simulation studies were conducted to verify the theoretical results. Goodness-of-t Test for Mexico zc3@indiana.edu Meta-analysis is a very useful tool to combine information from different sources. Fixed effect and random effect models are widely used in meta-analysis. Despite their popularity, they may give us misleading results if the models don't t the data but are blindly used. Therefore, like any statistical analysis, checking the model t- ting is an important step. However, in practice, the goodness-of-t in meta-analysis is rarely discussed. In this paper, we propose some tests to check the goodness-of-t for the xed and random effect models in meta-analysis. Through simulation study, we show that the proposed tests control type I error rate very well. To demon- strate the usefulness of the proposed tests, we also apply them to some real data sets. Our study shows that the proposed tests are useful tools in checking the goodness-of-t of the models used in meta-analysis. Pitfalls in Assessing Relative Efcacy Across Trials Xiao Sun Merck & Co. xiao sun@merck.com Although it is well known that the gold standard for assessing relative efcacy of treatments A vs. B is by way of head-to- head comparison in a randomized controlled trial (RCT), there is a widespread use of cross-trial comparisons in HIV and oncologywhen the only available data are from two independent trials A vs. C and B vs. C. A synthesis method is used to assess the relative efcacy of treatment arms A vs. B through the common reference arm C. The synthesized across-trial is observational in nature and subject to pitfalls of various confounding factors. In this presenta- tion, we will show even the two trials seemed very similar in terms of baseline prognostic factors; the missing data actually introduced bias and made the comparison invalid. Therefore, caution should be exercised when interpreting results from cross-trial comparisons. Cross-trial comparisons may have some role in hypothesis generat- ing such as identifying promising treatments for further investiga- tion, RCTs are still essential to make important clinical decisions. Session P01: Poster Session Correction for Confounding Effect in Random Forests Analysis \u0007Yang Zhao and Donghua Lou Nanjing Medical University zhaoyang@njmu.edu.cn Random Forests (RF) is an emsemble machine learning method, which is a powerful tool in analyzing high dimensional data. It can be used to screen for risk factors and build predictive mod- els.We found that RF may produce inacurate result if the dataset includes variables with confounding effect. Failing to remove the confouding effect may produce spurious association. We propose to correct for the confounding effect by using a residual based method.Simulations demonstrate that the proposed method can im- prove the probability that the causal factor to be identied. We also provide an example on genome-wide assoication studies to illustrate the application of the proposed method. Strategies of Genetic Risk Prediction with Lung Cancer GWAS Data \u0007Donghua Lou, Weiwei Duan, Zhibin Hu and Feng Chen Nanjing Medical University loudonghua@sohu.com Objective To investigate the performance of 3 genetic risk predic- tions methods-weighted genetic risk score(wGRS), support vector machine(SVM) and random forest(RF)-applied to high dimensional data of lung cancer with two strategies. Methods This study served Nanjing and Beijing samples of GWAS data as training set and test- ing set respectively. We made use of the two strategies of Full predictive subset\"(FS) and \"Best predictive subset\"(BS) and com- pared the prediction accuracy within the three methods mentioned above with the value combination of Linkage Disequilibrium(LD) and hypothesis testing levels(?).Results Under a high LD structure, the prediction accuracy of wGRS was on the rise with the increasing -log(?). RF and SVM are not sensitive to LD structures as wGRS, but the predictive accuracy of each method applied with a low LD structure(r2\u00a10.2) was mainly better than itself with a high LD struc- ture. Moreover, BS were slightly better than, approximately equal to or tiny less than and worse than FS when the methods were respec- tively wGRS, SVM and RF. Conclusion The prediction accuracy can be improved with the condition of LD-pruning and adopting a proper ?-value, meanwhile, wGRS is better than SVM and RF in that condition. Hierarchical Model for Genome-wide Association Study \u0007Honggang Yi, Hongmei Wo, Yang Zhao, Ruyang Zhang, Junchen Dai, Guangfu Jin and Hongxia Ma Nanjing Medical University ohcepf@163.com 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j111Abstracts With the rapid development of high-throughput genotyping tech- nologies in recently years, genome-wide association study (GWAS) has emerged as one of the most important tools for identifying ge- netic variants involved in complex diseases. Although this has im- proved our understanding of genetic basis of these complex diseases and trait, there are still many analytic challenges in GWAS. Most existing methods for GWAS are single-locus-based approaches, in which each variant is tested individually for association with a spe- cic phenotype in the whole genome-wide. However, such a single- locus-based analysis strategy of GWAS has many limitations. There are many statistical challenges for GWAS, such as how to incorpo- rate biological information into a GWAS and how to mine from GWAS data for getting more information, and so on. It is very apparent that new strategies and methods are urgently needed for GWAS.Here we proposed a hierarchical model GWAS strategy with the inclusion of prior biological information and applicated it in a real GWAS data. With the help of computer simulations, the sta- tistical properties and the effectiveness for actual GWAS data were evaluated from application's point of view, and the research details were as follows: In Section 1, two simulated studies were con- ducted based on the prior biological information which simulated by binomial distributions and the results of gene function classi- cation from a real GWAS data, respectively. Base on the two sim- ulation studies, the effects of different prior biological information for hierarchical model (HM) were evaluated thoroughly. The re- sults showed that both hierarchical model and logistic regression (LR) model are less powerful and perform similarly when OR equal to or less than 1.1 at the GWAS signicance level of 1E-5 and 1E- 7. However, HM always performs powerful than LR when the OR great than 1.1. In Section 2, three simulation studies were imple- mented to explore the effect of applying HM when incomplete in- formation, additional noisy information and uninformative informa- tion were included, respectively. The results showed as follows: The true relevant biological information have a major impact on the per- formance of HM. If the true relevant biological information were included in HM, even though other incomplete information or unin- formative information were also included, the power of HM always greater than LR's. On the contrary, HM lost more power than LR without true relevant biological information. The results of the area under the ROC curve for HM had the similar conclusions. In Sec- tion 3, the HM GWAS strategy was applied in a real GWAS data of lung cancer in Chinese Han populations. A Review of Nonparametric Methods for Testing Isotropy in Spatial Data \u0007Zachary Weller and Jennifer Hoeting Colorado State University zdweller@cord.edu One of the most important aspects of modeling spatial data is appro- priately specifying the second order properties of the random eld. A practitioner working with spatial data is presented a number of choices regarding the structure of the dependence between observa- tions. One of these choices is determining whether or not the covari- ance function is isotropic. Misspecication of isotropy properties could lead to misleading inferences, such as inaccurate predictions and parameter estimates. In a fashion similar to checking assump- tions for simple linear regression by looking at residual plots, a re- searcher may use graphical diagnostics, such as directional sample variograms, to decide whether the assumption of isotropy is reason- able. These graphical techniques can be difcult to assess, open to subjective interpretations, and misleading. An objective hypothesis tests of the assumption of isotropy may be more desirable. To thisend, a number of tests of isotropy have been developed using both the spatial and spectral representations of random elds. We provide an overview of nonparametric methods used to test the hypotheses of isotropy and symmetry in spatial data. We include a summary of key test properties, give insights on important considerations in choosing and implementing a test, and provide a brief simulation study comparing some of the methods. A Bayes Testing Approach to Metagenomic Proling in Washington Next-Generation Sequencing data, we use a multi- nomial with a Dirichclet prior to detect the presence of bacterial genomes in Metagenomic samples via marginal Bayes testing for bacterial strains in a reference database. The NGS data (sequencing reads) per strain are counted fractionally, with each sequencing read contributing an equal amount to each strain that it might represent. The threshold for detection is strain-dependent, and we apply a cor- rection for the dependence amongst the sequencing reads by nding the knee in a curve representing a tradeoff between detecting too many strains, and not enough strains. As a check, we evaluate the joint posterior probabilities for the presence of two strains in bacte- ria, and nd relatively little dependence. We apply our techniques to two human metagenomic data sets, and compare our results with the results found by the Human Microbiome Project (HMP). A Dynamical Model for Networks of Neuron Spike Trains \u0007Hongyu Tan, Phillip Chapman and Haonan Wang Colorado State University hytsky@gmail.com Recurrent event data arise in elds such as medicine, business and social sciences. In general, there are two types of recurrent event data. One is from a relatively large number of processes exhibiting a relatively small number of recurrent events, and the other is from a relatively small number of processes generating a large number of events. Many statistical models and methods have been developed to analyze the rst type of data, but few approaches are available for the second one. We focus on situations in which one process gen- erates a large number of events over the observational period.Our motivating application is a collection of neuron spike trains from a rat brain, recorded during performance of a task. The goal is to model the relationship between a response spike train and a set of predictor spike trains, as well as the spike history of the response itself. We propose a multiplicative intensity model, based on mod- ulated renewal processes, for a single realization from the response spike train. The model includes time-dependent neural spike histo- ries and extrinsic variables. The impact strengths of the functional predictors are modeled by coefcient functions that could be ap- proximated by B-spline basis functions. Spareness of the estimated coefcient functions is achieved by using the penalized partial like- lihood principle. Performance of the proposed method is demon- strated through simulation and real data analysis. Bio-insecticidal Effects of Two Plant Nationale Sup \u00b4erieure de ICSA/Graybill Joint Conference, Fort Collins, Colorado, Plants extracts of Marrubium vulgare and Artemisia herba alba were tested against 4th instar larvae of the mosquito Culex pipiens L. The obtained results indicated a sensitivity of Culex pipiens larvae for plant species aroused. This sensitivity is even higher when expo- sure of the larvae to insecticides is extended in time. Among the extracts used Artemisia herba-alba generates the greatest mortality rate 94Based on the percentage mortality, LC50 value of leaf ex- tract of Marrubium vulgare and Artemisia herba-alba on Culex pip- iens by calculating the regression line em- ploying Probit analysis of (Finney 1971) as described by (Busvine 1971). The probit regressions are used to model the effect of doses to determine the LC50 and their 95 Keywords: Plants extracts, Mortality, LC50, 4th instars, Statistical analysis, Culex pipiens Hypothesis Testing for an Extended Cox Model with Time- varying Coefcients \u0007Takumi Saegusa, Chongzhi Di and Ying Chen Fred Hutchinson Cancer Research Center tsaegusa@uw.edu The log-rank test has been widely used to test treatment effects un- der the Cox model for censored time-to-event outcomes, though it may lose power substantially when the model's proportional haz- ards assumption does not hold. In this presentation, we consider an extended Cox model that uses B-splines or smoothing splines to model a time-varying treatment effect and propose score test statistics for the treatment effect. Our proposed new tests com- bine statistical evidence from both the magnitude and the shape of the time-varying hazard ratio function, and thus are omnibus and powerful against various types of alternatives. In addition, thenew testing framework is applicable to any choice of spline ba- sis functions, including B-splines, and smoothing splines. Simu- lation studies conrm that the proposed tests performed well in - nite samples and were frequently more powerful than conventional tests alone in many settings. The new methods were applied to the HIVNET 012 Study, a randomized clinical trial to assess the ef- cacy of single-dose Nevirapine against mother-to-child HIV trans- mission conducted by the HIV Prevention Trial Network. The Stragety for Selecting Target Population Using Adaptive Phase II/III Seamless Design Based on Time-to-event Data \u0007Hao Yu, Dandan Miao and Feng Chen Nanjing Medical University njyuhao@vip.sina.com ABSTRACT Objective: Although subgroups can be identied on the basis of post-analysis, it needs an additional conrmatory trial and this may lead to an inflation in development time and cost. We present an approach that view treatment comparisons in both a pre- dened subgroup and the full population in the design period of a seamless trial, then evaluate the statistical characteristics. Method: It is based on the adaptive phase IIIII design. The decision of con- tinuing seamlessly either in a subgroup or the full population is on the basis of analysis of PFS and OS obtained from the rst stage. Final analysis is conducted only for OS using Fisher combination method after the second stage trial. Results: It is shown that the type-I-error rate is less than 2.5% and is independent of the cor- relation of OS and PFS. The simulations demonstrate that correct conclusions are reached sufciently often in the various scenarios. Conclusion: In oncology trials if there is an priori hypothesis about increased efcacy in a dened subgroup and this subgroup can be well characterized ,our design can shorten the time and cost. 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j113Index of Authors Abanto-Valle, C, 27, 50 Abanto-Valle, CA, 27, 50 Abecasis, G, 30, 49 Adeniji, AK, 36, 108 Afri-Mehennaoui, F, 31, 77 Albert, P, 38, 95 Allen, G, 32, 103 Amaravadi, L, 25, 71 An, D, 42,99 An, L, 37, 66 Anderes, E, 25,72 Anderson, A, 35, 85 Antao, V , 36, 107 Antonijevic, Z, 27, 27, 44, 44 Aouati, A, 33, 112 Asakura, K, 28, 58 Atem, F, 32, 104 Aue, A, 37, 70 Azevedo, CL, 27, 50 Baladandayuthapani, V , 31, 35, 90, 99 Ball, G, 40,82 Ban, Y , 37,66 Banerjee, S, 25, 73 Bell, J, 41, 47 Bentellis, A, 31,77 Berchi, S, 33,112 Berg, E, 25, 31, 73, 78 Berger, JO, 38, 95 Betensky, R, 32, 104 Bhat, KS, 40,76 Bretz, F, 30, 59 Brock, M, 36, 64 Brown, E, 42, 63 Brutnell, T, 24, 46 Budenz, D, 32, 104 Burdick, R, 33,45 Cai, L, 36,57 Cai, T, 35, 38, 77, 82 Candille, S, 38, 79 Cao, G, 38,92 Cao, H, 34,80 Cao, J, 41,62Carmichael, O, 29, 35, 71, 91 Caruana, R, 43, 103 Casanova, R, 28, 62 Casleton, E, 39,106 Cassese, A, 37, 74 Castellanos, L, 39, 70 castro, MD, 41, 51 Castro-Nallar, E, 37, 66 Chakraborty, B, 31, 86 Chan, AH, 26, 97 Chan, G, 42,85 Chan, I, 106 Chan, K, 30, 49 Chan, KW, 37, 70 Chan, T, 39, 66 Chang, C, 33,39,53,67 Chang, J, 24, 41, 43, 47, 56, 105 Chang, W, 27, 109 Chao, W, 36, 107 Chapman, P, 33, 112 Chatterjee, N, 27, 54 Chaturvedi, P, 27, 44 Chekouo, T, 37, 74 Chen, B, 41,60 Chen, C, 27, 36, 107, 109 Chen, CI, 36,53 Chen, CT, 30, 39, 59, 76 Chen, F, 32, 33, 111, 113 Chen, G, 34, 36, 64, 81 Chen, H, 41,47 Chen, J, 27, 42, 54, 85 Chen, K, 30,35, 42, 49, 63, 92 Chen, L, 30,49 Chen, M, 27, 29, 34, 36, 41, 50, 51, 79, 82, 108 Chen, P, 28, 65 Chen, Q, 41, 41, 52, 60 Chen, S, 26, 32, 43, 90, 93, 111 Chen, SX, 43, 105 Chen, T, 37, 74 Chen, Y , 33, 113 Chen, Z, 43,111 Cheng, G, 32, 39, 96, 100 Cheng, Y , 26, 41, 52, 104Chesi, A, 28, 62 Cheung, K, 31, 42, 86, 99 Cheung, YK, 40, 100 Chiang, A, 24, 54 Chiavacci, R, 28, 62 Chien, W, 36, 36, 52, 53 Christopher, D, 27, 45 Chu, L, 36, 64 Chun, H, 24,54 Chung, D, 41,47 Churpek, MM, 34, 80 Ciarleglio, A, 29, 71 Clarke, B, 33, 51, 112 Clarke, J, 33, 33, 51, 112 Cook, B, 25, 73 Cook, T, 31,86 Coram, M, 38, 79 Costagliola, D, 31, 76 Countryman, P, 26, 96 Cox, N, 30, 49 Crainiceanu, C, 24, 46 Crandall, K, 37, 66 Crimin, K, 25, 71 Crowe, B, 40,83 Crowley, J, 37, 74 Cui, Y , 28, 55 Dai, J, 32, 111 Davidian, M, 26, 104 Davidson, K, 31, 86 Davis R, 98 Davis, R, 33 Demuth, G, 25, 73 Deng, H, 36, 53 Deng, Q, 34, 63 Devanarayan, V , 32, 95 Dey, D, 27, 42, 43, 43, 50, 63,102, 103 Di, C, 33, 113 Di, Y , 41,56 Dicarlo, J, 26, 96 Ding, W, 41,60 Ding, Y , 25, 83 Do, K, 30, 37, 61, 74 Dobra, A, 33, 51, 112 Doecke, J, 37, 74 Dongmo Jiongo, V , 31, 78 Du, J, 30,61 Du, Y , 31, 68Duan, F, 34,57 Duan, W, 32, 111 Duchesne, P, 31, 78 Durkalski, V , 26, 109 Emerson, S, 41, 56 Espeland, M, 28, 62 Evans, S, 28, 58 Fan, J, 27, 35, 43, 48, 82, 105 Fan, M, 40,110 Fan, S, 42,65 Fan, Y , 27, 48 Fang, X, 26, 104 Feng, S, 25,71 Feng, X, 33, 54 Feng, Y , 38, 96 Feng, Z, 34, 57 Fine, J, 41, 52 Fine, JP, 34, 80 Finley, A, 25,73 Fishman, E, 36, 64 Flaherty, P, 41,48 Fong, Y , 36,58 Fosdick, B, 39, 105 Foster, J, 38,95 Frommlet, F, 28, 59 Fu, B, 36,107 Fu, H, 34, 38, 80, 91 Fu, W, 36, 107 Fuentes, M, 39, 70 Furrey, T, 29, 79 Gamazon, E, 30, 49 Gan, G, 43,103 Gao, B, 28, 55 Gao, C, 24, 27, 46, 48 Gao, F, 34,63 Gao, L, 36,107 Gao, X, 33, 38, 51, 92 Garcia, T, 41,52 Gardiner, J, 42,85 Gardner, I, 27, 50 Gardner, J, 27,44 Gary, M, 106 Gehrke, J, 43, 103 Gelernter, J, 41, 47 Gelfand, A, 30, 61 114Bold-faced are presenting authors. Index of Authors George, S, 29, 89 Ghosh D, 73 Ghosh, D, 37,77 Gill, M, 37,67 Gilmore, D, 26, 90 Glimm, E, 30, 59 Godin, O, 31, 76 Greven, S, 24, 46 Gu, X, 25,84 Guan, Y , 35, 91 Guindani, M, 37, 74 Guinness, J, 39,70 Guo, B, 42, 43, 65, 105 Hamasaki, T, 28, 28, 58, 58 Han, F, 24,47 Han, P, 32,93 Han, S, 30,101 Han, X, 27,48 Harrell, L, 36, 57 Harrington, P, 37,67 Harris, S, 27, 44 Harvill, J, 43, 103 Haunfelder, R, 39,105 Haynes, B, 36, 58 Haziza, D, 31,78 He, C, 33, 39, 51, 66 He, J, 29, 71 He, X, 37, 39, 67, 69 He, Z, 29,94 Heitjan, D, 42,85 Hennessey, V , 31, 90 Hobbs, B, 30, 40, 61, 100 Hochberg, M, 37, 69 Hoeting, J, 33, 112 Hoffman, E, 30, 49 Holan, S, 39,71 Holland, D, 30, 61 Holland, E, 39, 66 Honerkamp-Smith, G, 37, 69 Hong, C, 37, 66 Hong, F, 32,95 Horrell, M, 39,70 Hsiao, C, 27, 28, 30, 39, 58, 59, 76, 109 Hsieh, D, 28, 65 Hsieh, R, 36, 52, 53 Hsu, F, 28,62 Hsu, J, 25,83 Hu, F, 35,84 Hu, J, 30, 40, 61, 88 Hu, M, 29, 34, 63, 79 Hu, T, 29, 87 Hu, Y , 40,88 Hu, Z, 32, 111 Huang, C, 32,93 Huang, J, 31, 38, 39, 67, 75, 92 Huang, ML, 28,65 Huang, P, 36,64Huang, S, 28, 28, 65, 65 Huang, W, 28, 58 Huang, Y , 26,38,43,79,97, 105 Hung, H, 28, 65 Hung, HMJ, 28,58 Hunt, K, 33, 53 Im, HK, 30, 49 Jaeger, A, 35, 88 James, G, 38, 96 Janes, H, 26,97 Jeong, J, 37,68 Ji, H, 41, 47 Ji, Y , 37, 42, 65, 66 Jia, X, 42,99 Jiang, D, 41,56 Jiang, F, 38, 94 Jiang, H, 37, 38, 41, 41, 56, 56, 66, 91 Jiang, X, 27,50 Jiao, F, 30, 49 Jin, F, 29, 79 Jin, G, 32, 111 Jin, P, 29, 79 Jin, Z, 38,77 Joeng, H, 36,108 Johns, D, 25, 71 Johnson, B, 41, 60 Johnson, D, 30,61 Johnson, W, 27, 50 Johnson, WE, 37,66 Jung, Y , 40, 88 Kaiser, M, 39, 106 Kang, G, 28,55 Kashyap, V , 31, 74 Kass, R, 39, 70 Kechris, K, 40,87 Kelly, A, 28, 62 Kim, M, 41, 60 Koch, A, 28, 59 Kohli, P, 43,103 Kong, L, 24, 39, 46, 66 Kong, S, 25,84 Kong, X, 35,98 Kong, Y , 27,48 Kosorok, M, 34, 81 Kundu, S, 42, 83 Kuo, L, 33, 37, 51, 67 Laber, E, 26,104 Lachos Davila, VH, 27,50 Lai, H, 37, 68 Lai, Y , 27, 109 Lan, KKG, 39, 76 Lazar, N, 35,88 Lee, BL, 42, 65 Lee, C, 36, 53 Lee, J, 27, 37, 50, 66Lee, JJ, 31,68 Lee, K, 40, 102 Lee, M, 37, 69 Lee, S, 42, 99 Lee, T, 31,74 Lee, TCM, 25, 69 Lei, J, 35, 92 Lei, S, 38, 91 Levina, E, 40, 102 Li, B, 29, 40, 79, 102 Li, C, 41, 47 Li, D, 27, 42, 48, 73 Li, G, 25, 31,38, 83, 86,91 Li, H, 35, 43, 82, 110 Li, J, 30,41,56,101 li, J, 43, 111 Li, M, 38,78 Li, Q, 29, 93 Li, R, 35, 41, 43, 52, 82, 105 Li, S, 36,64 Li, X, 25, 71 Li, Y , 29, 35, 35, 42, 43 ,65, 79, 85, 91,111 Li, Z, 29,94 Liang, L, 30, 49 Liao, Y , 43, 105 Liberles, D, 35, 98 Lim, C, 28, 56 Lim, CY , 28, 56 Lin, C, 30, 36, 49, 53 Lin, L, 29, 79 Lin, N, 39, 67 Lin, Q, 26, 90 Lin, X, 30, 49 Lin, Y , 28, 36, 61, 65, 107 Lin, YK, 36, 36, 53, 53 Lindborg, S, 25, 71 Lipkovich, I, 25, 75 Liu, A, 26, 29, 38, 89, 95, 98 Liu, B, 34, 80 Liu, D, 25, 26 , 38, 71, 95, 97 Liu, F, 26,109 Liu, G, 34, 63 Liu, H, 24, 26, 29, 47, 94, 104 Liu, J, 27,29, 36, 39, 42, 53, 63, 76, 80,94, 109 Liu, L, 27,35,42,44,85,98 Liu, P, 24,46 Liu, Q, 33, 53 Liu, R, 26, 97 Liu, S, 40, 100 Liu, T, 40, 102 Liu, X, 28, 38, 55, 77 Liu, Y , 34, 37, 63, 74, 81 Liu, Z, 32, 35, 84, 103 Loh, W, 38,94 Lok, A, 34, 57 Long, Q, 41,60 Loo, GY , 37, 69Lou, D, 32, 32, 111, 111 Lou, Y , 43, 103 Lu, K, 35, 98 Lu, N, 39, 76 Lu, S, 28,61 Lu, W, 34, 80 Lu, Y , 24,26, 42, 54, 65, 96 Luo, C, 42,63 Luo, S, 24,33,53, 54 Luo, Z, 42, 85 M\u00a8uller, H, 40, 110 Ma, C, 30, 61 Ma, H, 32, 111 Ma, J, 29, 32, 40, 89, 100, 101 Ma, P, 31, 37,66, 75 Ma, S, 28, 32, 34, 36, 55, 64, 81,103 Ma, X, 24, 54 Ma, Y , 32, 41, 43, 52, 101, 110 Ma, Z, 24, 27, 46, 48 Maadooliat, M, 38, 92 Maceachern, S, 27,50 Mahapatra, P, 25, 69 Mai, Q, 34,81 Maiti, T, 28, 56 Mallinckrodt, C, 25,75 Manimaran, S, 37, 66 Manner, D, 38, 91 Mao, X, 40,100 Marcy, P, 25,69 Marder, K, 41, 52 Mary-Krause, M, 31, 76 Matsuo, T, 40, 110 Maurer, W, 30, 59 Mayer, C, 38,91 Mazroui, Y , 31,76 Mcauliffe, J, 41, 48 Mcpeek, MS, 41, 56 Mebane, D, 25, 69 Mebirouk, O, 31, 77 Mehennaoui, S, 31, 77 Mehta, C, 27, 44 Mentch, F, 28, 62 Mesbah, M, 26,96 Meyer, M, 31, 78 Miao, D, 33, 113 Millen, B, 30,59 Miller, E, 27,44 Mitchell, J, 28, 62 Mizera, I, 24, 46 Molenberghs, G, 25,75 Montes, R, 33,45 Morris, J, 29, 72 Mueller, H, 29, 35, 71, 91 Mueller, P, 37, 42, 65, 66 Murphy S, 98 Murphy, S, 24, 106 Murray, S, 38,78 2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17 j115Index of Authors Bold-faced are presenting authors. Mwanza, J, 32, 104 Myers, DB, 39, 71 Najibi, SM, 38, 92 Nandy, S, 28,56 Nettleton, D, Ni, Y , 35,99 Nicholas, K, 26,109 Nicolae, DL, 30, 49 Nie, L, 26, 98 Ning, J, 30, 34, 57, 68 Nordman, D, 39, 106 Norris, M, 27,50 O'Kelly, M, 25, 75 Ogden, T, 29,71 Opsomer J, 98 Opsomer, J, 31,78 Ouyang, G, 43, 102 Ouyang, Z, 40,102 Paik, M, 41, 60 Pallos, LL, 36,107 Pan, Q, 33, 53 Pan, W, 38, 79 Park, Y , 29,80 Paul, D, 37,70 Peng, H, 26, 35, 38, 79, 88, 90 Peng, J, 41,48 Peng, L, 34, 39, 67, 81 Perel, S, 39, 70 Permar, S, 36, 58 Perou, C, 40, 88 Peterfy, C, 26, 96 Petersen, A, 35, 40, 91, 110 Petkova, E, 29, 71 Pfeiffer, R, 34,57 Posch, M, 28, 59 Potard, V , 31, 76 Pounds, S, 36, 107 Price, K, 38, 91 Qian, J, 32,104 Qiao, X, 38, 39,96, 96 Qin, G, 25,90 Qin, J, 32, 41, 60, 93 Qin, Y , 35,85 Qin, Z, 29, 79 Qiu, Y , 43,105 Qu, PR, 26,108 Qu, S, 29,72 Quinlan, M, 27,45 Rached, O, 31, 31, 77, 77 Radchenko, P, 38,96 Rahman, A, 43,111 Ramakrishnan, V , 26, 109 Raman, S, 36, T, 36,58 Rathbun, S, 35,84Ratitch, B, 25,75 Ravishanker, N, 43, 103 Ren, J, 32,93 Ren, Z, 24,46 Revzin, E, 38, 92 Ristl, R, 28,59 Rosenblum, M, 26,104 Rosner, G, 31,90 Roy, S, 28, 62 Ruberg, S, 25, 83 Saddiki, H, 41, 48 Saegusa, T, 33,113 Sahli, L, 31, 77 Sapp, J, 36, 107 Saville, B, 38,91 Schindler, J, 34, 63 Schliep, E, 30,61 Schroeder, J, 36, 64 Schwartz, A, 39, 70 Schweinberger, M, 39,106 Scrudato, K, 41,110 Seetharaman, I, 30, 49 Sen, K, 30, 59 Sengupta, S, 37, 66 Seshan, V , 25, 71 Severini, T, 42, 85 Shamsaei, B, 36, 107 Shao, J, 29, 93 Shao, Q, 35, 82 Shao, Y , 42, 83 She, Y , 27,48 Shen, J, 28, 61 Shen, L, 25, 25, 38, 84, 84, 95 Shen, R, 25,71 Shen, W, 34,57, 106 Shen, Y , 35, 35, 85, 85 Shi, M, 36, 64 Shi, X, 28, 55 Shih, M, 31,68 Shih, T, 42, 85 Shih, W, 31, 86 Shih, WJ, 28, 61 Shim, Y , 36, 107 Shou, H, 24,46 Sidor, L, 27,44 Siegmund, K, 29, 80 Simon, N, 40,100 Sinha, R, 35, 85 Siska, C, 40, 87 Song, C, 32, 95 Song, P, 41, 60 Song, R, 24, 54 Soon, G, 26, 98 Stein, M, 39, 70 Stingo, F, 33, 35, 37, 40, 51, 74, 99, 100 Storlie, C, 25, 69 Storlie, CB, 25, 69 Stranger, B, 30, 49Street, RC, 29, 79 Stroup, W, 27, 45 Su, H, 40, 41, 110, 110 Su, S, 42,74 Su, X, 30, 33, 53, 59 Suchard, M, 37, 67 Sudduth, K, 39, 71 Sullivan, P, 29, 79 Sun, D, 25, 33, 51, 73 Sun, J, 29, 32, 87, 101 Sun, W, 31,37, 39, 40, 74, 75, 88, 96 Sun, X, 43,111 Swartz, M, 33,51 Szulwach, KE, 29, 79 Tan, F, 26, 35,88, 90 Tan, H, 33,112 Tan, L, 26,97 Tan, M, 31,89 Tang, H, 38,79 Tang, X, 42, 85 Tanna, A, 32, 104 Tarpey, T, 29, 71 Tayob, N, 38, 78 Teufel, A, 35, 98 Tian, L, 26, 38,94, 96 Ting, N, 34, 36, 63, 108 Tiwari, R, 43, 111 Tong, X, 38,96 Trippa, L, 31, 42, 65, 68 Tsai, H, 32, 93 Tseng, GC, 30, 101 Tsiatis, A, 26, 104 Tsou, H, 27, 39,76, 109 Tu, I, 28, 28, 64, 65 Tu, W, 29, 94 Tzeng, C, 27, 39, 76, 109 Tzeng, J, 28, 40, 65, 88 Valdes, C, 33, 33, 51, 112 van Dyk, DV , 31, 74 Vandergrift, N, 36,58 Vannucci, M, 33, 37, 51, 74 Verde, F, 36, 64 V olgushev, S, 32, 100 Vu, V , 39,70 Wahed, A, 26, 104 Wan, Y , 32, 103 Wang, B, 25, 90 Wang, C, 28, 30, 31 , 41, 49, 60, 65, 90 Wang, D, 26,90 Wang, F, 40,109 Wang, H, 33, 33, 35, 39, 54, 88, 105, 112 Wang, J, 29, 29, 30, 38, 49, 71, 72, 92 Wang, L, 24, 30, 37, 38, 43, 46, 68, 70, 92, 96, 105Wang, M, 24,34, 36, 54,63, 64 Wang, N, 35,91 Wang, P, 34,81 Wang, R, 41, 48 Wang, S, 28, 29,35, 39, 58, 66, 67, 88,93 Wang, W, 37, 40, 43, 68, 82, 105 Wang, X, 25, 29, 29, 30, 38, 42, 72, 73, 89, 95,99, 101 Wang, Y , 30, 31, 39, 41, 52, 61, 66, 86 Warren, J, 32,104 Weerahandi, S, 30,101 Wei, LJ, 38, 94 Wei, Y , 36,64 Wei, Z, 33, 51 Weller, Z, 33,112 Weng, Y , 30, 59 Whitmore, GA, 37, 69 Wiens, B, 42,73 Wikle, C, 39, 71 Wittes, J, 40,83 Wo, H, 32, 111 Wong, R, 31, 74 Wong, RKW, 25,69 Wu H, 73 Wu, C, 28,55 Wu, H, 29, 79, 80 Wu, J, 31, 78 Wu, M, 32, 40,87, 95 Wu, W, 28,56 Wu, Y , 37, 41, 74, 110 Xi, D, 30,59 Xi, Y , 42, 62 Xiao, R, 28,62 Xie, J, 35,82 Xie, M, 26, 97 Xu, C, 43, 105 Xu, G, 39,67 Xu, J, 42, 73 Xu, N, 42,83 Xu, R, 37,69 Xu, T, 29,79 Xu, Y , 25, 37, 39,42,65, 66, 76, 84 Xu, Z, 29, 79 Xue, L, 34,81 Yan, J, 37,68 Yang, C, 29, 41, 47, 94 Yang, H, 35, 98 Yang, J, 34,81 Yang, W, 39, 71 Yang, Y , 34, 81 Yao, B, 29, 79 Yao, Q, 43, 105 Yau, CY , 37,70 116j2015 ICSA/Graybill Joint Conference, Fort Collins, Colorado, June 14-17Bold-faced are presenting authors. Index of Authors Yavuz, I, 26, 104 Yee, L, 42, 85 Yi, H, 32,111 Yi, M, 33, 53 Yin, Y , 34,63 Yu, C, 31,78 Yu, D, 24,46 Yu, H, 33,113 Yu, L, 35, 98 Yu, M, 29, 93 Yu, T, 38,79 Yu, Z, 29, 94 Yuan, M, 40,102 Yuan, Y , 26, 34, 40, 57, 100, 109 Zang, Y , 26, 40, 100, 109 Zavala, N, 41, 47 Zeng, D, 29, 34, 80, 87 Zhan, X, 30, 49 Zhang H, 73 Zhang, B, 29, 86Zhang, C, 24, 46 Zhang, D, 42, 85 Zhang, F, 29, 79 Zhang, G, 29, 32, 43, 79, 101, 111 Zhang, H, 27, 32, 54, 95 Zhang, J, 28, 34,42, 61, 63, 80 Zhang, L, 33,45 Zhang, M, 29,86 Zhang, N, 31, 41, 47, 75 Zhang, P, 29,89 Zhang, R, 32, 111 Zhang, S, 41, 42,62, 62 Zhang, T, 32,101 Zhang, X, 29, 32, 38, 71, 92, 101 Zhang, Y , 26, 30,37, 40, 41, 43, 51, 67,101, 102-104, 111 Zhang, Z, 26,34, 37, 57, 68, 98Zhao, A, 38, 96 Zhao, H, 36, 37, 40, 41, 47, 64, 67, 102 Zhao, J, 35, 98 Zhao, M, 29, 79 Zhao, N, 32, 40, 87, 95 Zhao, S, 35,82 Zhao, X, 42,99 Zhao, Y , 32, 32, 33,53,111, 111 Zhao, YC, 26, 35, 90, 98 Zhao, YQ, 29, 36, 64, 87 Zheng, Q, 39,67 Zheng, T, 26, 97 Zheng, W, 24, 26, 54, 90 Zheng, Y , 38,77 Zheng, Z, 27, 48 Zhong, J, 25, 71 Zhong, P, 26, 32, 35, 88, 90, 93 Zhong, S, 41, 56 Zhong, W, 31,75Zhou, H, 24, 27, 37, 43, 46, 48, 74, 110 Zhou, L, 38,92 Zhou, M, 42,83 Zhou, Q, 29,87 Zhou, W, 24, 24, 35, 46, 47, 47,82 Zhou, X, 30,49 Zhou, Y , 25, 43, 73, 111 Zhu L, 73 Zhu, H, 24, 29, 30 , 54, 68, 72 Zhu, J, 28,40,57,102 Zhu, R, 36,64 Zhu, Y , 36, 37, 66, 107 Zhu, Z, 25,73 Zhuo, B, 41, 56 Zimmerman, D, 25,72 Zipunnikov, V , 24, 46 Zou, F, 37, 74 Zou, H, 34, 81 2015 ICSA/Graybill Joint Conference, Fort Collins, "}