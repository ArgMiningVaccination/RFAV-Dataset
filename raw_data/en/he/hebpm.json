{"title": "PDF", "author": "PDF", "url": "https://faculty.cc.gatech.edu/~turk/paper_pages/2018_symmetric_locomotion/symmetric_low_energy_locomotion.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "Learning Symmetric and Low-Energy Locomotion WENHAO YU, Georgia Institute of Technology GREG TURK, Georgia Institute of Technology C.KAREN LIU, Georgia Institute of Technology Fig. 1. Locomotion Controller trained for different creatures. (a) Biped walking. (b) Quadruped galloping. (c) Hexapod Walking. (d) Humanoid running. Learning locomotion skills is a challenging problem. To generate realistic and smooth locomotion, existing methods use motion capture, finite state machines or morphology-specific knowledge to guide the motion generation algorithms. Deep reinforcement learning (DRL) is a promising approach for the automatic creation of locomotion control. Indeed, a standard benchmark for DRL is to automatically create a running controller for a biped character from a simple reward function [Duan et al .2016]. Although several different DRL algorithms can successfully create a running controller, the resulting motions usually look nothing like a real runner. This paper takes a minimal- ist learning approach to the locomotion problem, without the use of motion examples, finite state machines, or morphology-specific knowledge. We in- troduce two modifications to the DRL approach that, when used together, produce locomotion behaviors that are symmetric, low-energy, and much closer to that of a real person. First, we introduce a new term to the loss func- tion (not the reward function) that encourages symmetric actions. Second, we introduce a new curriculum learning method that provides modulated physical assistance to help the character with left/right balance and forward movement. The algorithm automatically computes appropriate assistance to the character and gradually relaxes this assistance, so that eventually the character learns to move entirely without help. Because our method does not make use of motion capture data, it can be applied to a variety of character morphologies. We demonstrate locomotion controllers for the lower half of a biped, a full humanoid, a quadruped, and a hexapod. Our results show that learned policies are able to produce symmetric, low-energy gaits. In addition, Authors' addresses: Wenhao Yu, School of Interactive Computing, Georgia Institute of Technology, wyu68@gatech.edu; Greg Turk, School of Interactive Computing, Geor- gia Institute of Technology, turk@cc.gatech.edu; C.Karen Liu, School of Interactive Computing, Georgia Institute of Technology, karenliu@cc.gatech.edu. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a92018 Author. This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in ACM Transactions on Graphics 37(4), August 2018. 0730-0301/2018/8-ART144 $15.00 https://doi.org/10.1145/3197517.3201397speed-appropriate gait patterns emerge without any guidance from motion examples or contact planning. CCS Concepts: Computing methodologies Animation ;Reinforce- ment Learning ; Additional Key Words and Phrases: locomotion, reinforcement learning, curriculum learning ACM Reference Format: Wenhao Yu, Greg Turk, and C.Karen Liu. 2018. Learning Symmetric and Low-Energy Locomotion. ACM Trans. Graph. 37, 4, Article 144 (August 2018), 12 pages. https://doi.org/10.1145/3197517.3201397 1 INTRODUCTION Creating an animated character that can walk is a fascinating chal- lenge for graphics researchers and animators. Knowledge from biomechanics, physics, robotics, and animation give us ideas for how to coordinate the virtual muscles of a character's body to move it forward while maintaining balance and style. Whether physical or digital, the creators of these characters apply physics principles, borrow ideas from domain experts, use motion data, and under- take arduous trial-and-error to craft lifelike movements that mimic real-world animals and humans. While these characters can be engi- neered to exhibit locomotion behaviors, a more intriguing question is whether the characters can learn locomotion behaviors on their own, the way a human toddler can. The recent disruptive development in Deep Reinforcement Learn- ing (DRL) suggests that the answer is yes. Researchers indeed showed that artificial agents can learn some form of locomotion using advanced policy learning methods with a large amount of com- putation. Even though such agents are able to move from point A to point B without falling, the resulting motion usually exhibits jerky, high-frequency movements. The motion artifacts can be mitigated by introducing motion examples or special objectives in the reward function, but these remedies are somewhat unsatisfying as they sacrifice generality of locomotion principles and return partway to heavily engineered solutions. ACM Transactions on Graphics, Vol. 37, No. 4, Article 144. Publication date: August 2018.arXiv:1801.08093v3 [cs.LG] 12 May 2018144:2 Yu, Turk and Liu This paper takes a minimalist approach to the problem of learning locomotion. Our hypothesis is that natural locomotion will emerge from simple and well-known principles in biomechanics, without the need of motion examples or morphology-specific considerations. If the agent can successfully learn in this minimal setting, we fur- ther hypothesize that our learning method can be generalized to agents with different morphologies and kinematic properties. Our approach is inspired by the recent work that applies reinforcement learning to train control policies represented as neural networks, but aims to address two common problems apparent in the motions produced by the existing methods. First, we observe that the motions appear much more energetic than biological systems. Second, an agent with perfectly symmetrical morphology often produces visibly asymmetrical motion, contradicting the observation in biomechan- ics literature that gaits are statistically symmetrical [Herzog et al . 1989]. Therefore, we propose a new policy learning method that minimizes energy consumption and encourages gait symmetry to improve the naturalness of locomotion. While most existing methods in motor skill learning penalize the use of energy in the reward function, the weighting of the penalty is usually relatively low for fear of negatively impacting the learning of the main task (e.g. maintaining balance). As a result, the energy term serves merely as a regularizer and has negligible effect on preventing the agent from using excessive joint torques. We in- troduce a new curriculum learning method that allows for a high energy penalty while still being able to learn successfully using the existing policy learning algorithms. The curriculum provides modulated physical assistance appropriate to the current skill level of the learner, ensuring continuous progress toward successful loco- motion with low energy consumption. Our algorithm automatically computes the assistive forces to help the character with lateral bal- ance and forward movement. The curriculum gradually relaxes the assistance, so that eventually the character learns to move entirely without help. In addition to energy consumption, gait symmetry offers both stable and adaptive locomotion that reduces the risk of falling [Pat- terson et al .2008]. The symmetry of walking trajectories can be measured by established metric used in clinical settings [Nigg et al . 1987]. However, directly using this metric in the reward function leads to two complications for policy gradient methods. First, the requirement of evaluating an entire trajectory introduces a delayed and sparse reward, resulting in a much harder learning problem. Second, the policy, especially at the beginning of learning, might not be able to produce structured, cyclic motion, rendering the sym- metry metric ineffective in rewarding the desired behaviors. Our solution departs from the conventional metrics that measure the symmetry of the states. Instead, we measure the symmetry of actions produced by the policy. We propose a mirror symmetry loss in the objective function to penalize the paired limbs for learning different control strategies. Our evaluation shows that the agent can indeed learn locomo- tion that exhibits symmetry and speed-appropriate gait patterns and consumes relatively low-energy, without the need of motion examples, contact planning, and additional morphology-specific terms in the reward function. We further show that the same re- ward function with minimal change of weighting and the learningmethodology can be applied to a variety of morphologies, such as bipeds, quadrupeds, or hexapods. We test our method against three baselines: learning without the mirror symmetry loss, learning without the curriculum, and learning without either component. The comparisons show that, without the curriculum learning, the trained policies fail to move forward or/and maintain balance. On the other hand, without the mirror symmetry loss, the learning process takes significantly more trials and results in asymmetric locomotion. 2 RELATED WORK 2.1 Physically-based Character Control Obtaining natural human and animal motor skills has been a long- standing challenge for researchers in computer animation. Existing algorithms in character control usually require breaking the mo- tion into more manageable parts or using motion data in order to generate natural-looking results [Geijtenbeek and Pronost 2012]. One approach is the use of finite state machines (FSMs) [Coros et al . al .2009; Wang et al .2009, 2012; Yin et al .2007]. Yin et al. used FSMs to connect keyframes of the character poses, which is then combined with feedback rules to achieve balanced walking, skipping and running motion for hu- manoid characters [Yin et al .2007]. A different application of FSMs can be seen in [de Lasa et al .2010], where they formulated the locomotion task as a Quadratic Programming (QP) problem and used an FSM model to switch between different objective terms to achieve walking motion. Although this class of techniques can suc- cessfully generate plausible character motions, it is usually difficult to generalize them to non-biped morphologies or arbitrary tasks. An alternative approach to obtain natural character motions is to incorporate motion data such as videos [Wampler et al .2014] or motion capture data [da Silva et al .2008; Lee et al .2010, 2014; Liu et al.2005; Muico et al .2009; Sok et al .2007; Ye and Liu 2010]. Despite the high fidelity motion this approach can generate, the requirement of motion data does not allow its application to arbitrary character morphologies as well as generalization to novel control tasks. Apart from designing finite state machines or using motion data, reward engineering combined with trajectory optimization has also been frequently applied to generate physically-based character mo- tions [Al Borno et al .2013; .2012, 2013; Wampler and et al. demonstrated a va- riety of humanoid motor skills by breaking a sequence of motion into shorter windows, and for each window a task-specific objective is optimized [Al Borno et al .2013]. Mordatch and periodicity of the motion was explicitly enforced to generate realistic locomotion. Trajectory-optimization based methods provide a general framework for synthesizing char- acter motions. However, a few common drawbacks for this category of methods include: they are usually off-line methods, they are sen- sitive to large perturbations, and they require explicitly modeling of system dynamics. To overcome the first two issues, Mordatch et al. trained a neural network policy using data generated from ACM Transactions on Graphics, Vol. 37, No. 4, Article 144. Publication date: August 2018.Learning Symmetric and Low-Energy Locomotion 144:3 a trajectory optimization algorithm and demonstrated of character locomotion with different morphologies [Mor- datch et al .2015]. In this work, we aim to develop an algorithm that can synthesize plausible locomotion controllers with minimal prior knowledge about the character, the target motion, and the system dynamics. 2.2 Reinforcement Learning Reinforcement Learning (RL) provides a general framework for mod- eling and solving control of physics-based characters. In computer animation, policy search methods, a sub-area of RL, have been suc- cessfully applied in optimizing control policies for complex motor skills, such as cartwheels [Liu et al .2016], monkey-vaults and Koltun 2014; Peng et al .2015, 2016, 2017; Won et al .2017]. Two major classes of policy search methods used in the previous work include sampling-based methods [Geijtenbeek et al .2013; Tan et and methods [Levine and Koltun 2014; Liu et al. 2016; Peng et al. 2016, 2017]. A representative example of sampling-based method is CMA- ES, which iterates between evaluating a set of sampled parameters and improving the sampling distribution [Hansen and Ostermeier 1996]. This class of policy search methods is relatively robust to local minima and does not require gradient information. However, the sample number and memory requirement usually scales with the number of model parameters, making it less suitable to optimize models with many parameters such as deep neural networks. To combat this constraint on policy complexity, researchers resort to specialized controller design and a fine-tuned reward structure. On the other hand, gradient-based methods naturally fit into the stochastic gradient descent framework, an algorithm that has been successfully demonstrated to train deep neural networks with hun- dreds of millions of parameters [LeCun et al .2015]. Gradient-based methods like REINFORCE exhibit large variance in the estimated gradient, limiting their application to relatively simple control prob- lems [Sutton et al .2000]. Recent developments in deep reinforcement learning have seen significant progress on improving the gradient estimation accuracy and stability of training, leading to algorithms such as DDPG [Lillicrap et al .2015], and PPO [Schulman et al .2017], which can be used to solve complex motor control problems. For exam- ple, by combining TRPO with Generalized Advantage Estimation [Schulman et al .2015b], Schulman et al. demonstrated learning of locomotion controllers for a 3D humanoid character. Later, they pro- posed Proximal Policy Optimization (PPO), which further improved the data efficiency of the algorithm [Schulman et al .2017]. Despite the impressive feat of tackling the 3D biped locomotion problem from scratch, the resulting motion usually looks jerky and unnatu- ral. Peng et al. achieved significantly more natural-looking biped locomotion by combining motion capture data with an actor-critic algorithm [Peng et al. 2017].2.3 Curriculum Learning Our approach is also inspired by works in curriculum learning (CL). The general idea behind CL is to present the training data to the learning algorithm in an order of increasing complexity [Bengio et al.2009]. Researchers in machine learning have shown that CL can improve performance and efficiency of learning problems such as question-answering [Graves et al .2017], classification [Bengio et et .2017; Matiisen et al. 2017] and game playing [Narvekar et al. 2016]. Curriculum learning has also been applied to learning motor skills in character animation and robotics. Florensa et al. demon- strated successful training of robot manipulation and navigations controllers, where the policies are trained with initial states that are increasingly farther from a goal state [Florensa et al .2017]. Pinto et al. applied CL to improve the efficiency of learning of a robot grasping controller [Pinto and Gupta 2016]. Karpathy et al trained a single legged character to perform acrobatic motion by decompos- ing the motion into a high-level curriculum with a few sub-tasks and learned each sub-task with a low-level curriculum[Karpathy and Van De Panne 2012]. In learning locomotion controllers, van de Panne et al. applied external torques to the character in order to keep it upright during training and demonstrated improved learning performance [Van de Panne and Lamouret 1995]. Similarly, Wu et al. used helper forces to assist in optimizing a set of locomotion con- trollers, which are then used by a foot-step planner to controller the character to walk on uneven terrains [Wu and Popovi 2010]. Our method shares the similar idea of using an external controller to as- sist the character in learning locomotion gaits, but differs from their works in two key aspects. First, in addition to balance assistance, our curriculum learning also provides propelling assistance which shows significant improvement over balance assistance alone. Second, our method gradually reduces the strength of the assistance forces with- out affecting the reward function, while their method explicitly minimizes the assistances force in the optimization which might require objective function tuning. Yin et al. applied continuation method to search for a curriculum path that gradually adapt a nom- inal locomotion controller to different tasks such as stepping over obstacle or walking on ice [Yin et al .2008]. More recently, Heess et al. demonstrated learning of agile locomotion controllers in a complex environment using deep Reinforcement Learning [Heess et al .2017]. They applied PPO to train the agents on environments with increas- ing complexity, which provided a environment-centered curriculum. Similarly, we also find that efficiency of curriculum learning can be improved by using a environment-centered curriculum. 3 BACKGROUND: POLICY LEARNING The locomotion learning process can be modeled as a Markov De- cision Process (MDP), defined by a tuple: (S,A,r,0,P,), where Sis the state space,Ais the action space, r:S\u00d7A7 Ris the reward function, 0is the initial state distribution, P:S\u00d7A7S is the transition function and is the discount factor. Our goal is to solve for the optimal parameters of a policy :S\u00d7A7 R, which maximizes the expected long-term reward: =argmax Es0[V(s)], (1) ACM Transactions on Graphics, Vol. 37, No. 4, Article 144. Publication date: August 2018.144:4 Yu, Turk and Liu where the value function of a policy, V:S7R, is defined as the expected long-term reward of following the policy from some input state st: V(st)=Eat,st+1P, . . .[\u00d5 i=0ir(st+i,at+i)]. (2) 3.1 Learning Locomotion Policy In the context of locomotion learning problem, we define a state ass=[q,\u00dbq,c,v], where qand\u00dbqare the joint positions and joint velocities. cis a binary vector with the size equal to the number of end-effectors, indicating the contact state of the end-effectors ( 1in contact with the ground and 0otherwise). vis the target velocity of the center of mass in the forward direction. The action ais simply the joint torques generated by the actuators of the character. Designing a reward function is one of the most important tasks in solving a MDP. In this work, we use a generic reward function for locomotion similar to those used in RL benchmarks 2017]. It consists of three objectives: move forward, balance, and use minimal actuation. r(s,a)=wvEv(s)+Eu(s)+wlEl(s)+Ea+weEe(a). (3) The first term of the reward function, Ev=|\u00afv(s)v|, encour- ages the character to move at the desired velocity v.\u00afv(s)denotes the average velocity in the most recent 2seconds. The next three terms are designed to maintain balance. Eu=(wux|x(s)|+wuy|y(s)|+ wuz|z(s)|)rewards the character for maintaining its torso or head upright, where (s)denotes the orientation of the torso or head. El=|cz(s)|penalizes deviation from the forward direction, where cz(s)computes the center of mass (COM) of the character in the frontal axis. Eais the alive bonus which rewards the character for not being terminated at the current moment. A rollout is terminated when the character fails to keep its COM elevated along the for- ward direction, or to keep its global orientation upright. Finally, Ee=apenalizes excessive joint torques, ensuring minimal use of energy. Details on the hyper-parameters related to the reward function and the termination conditions are discussed in Section 6. 3.2 Policy Gradient Algorithm Policy gradient methods have demonstrated success in solving such a high-dimensional, continuous MDP. In this work, we use Proxi- mal Policy Optimization (PPO) [Schulman et al .2017] to learn the optimal locomotion policy because it provides better data efficiency and learning performance than the alternative learning algorithms. Our method can also be easily applied to other learning algorithms such as Trust Region Policy Optimization (TRPO) [Schulman et al . 2015a] or Deep Deterministic Policy Gradient (DDPG) [Lillicrap et al. 2015]. Like many policy gradient methods, PPO defines an advantage function as A(st,a)=Q(s,a)V(s), where Qis the state- action value function that evaluates the return of taking action aat state sand following the policy thereafter. However, PPO mini- mizes a modified objective function term that enables us to use data sampled under an old policy oldto estimate expectation for the current policy . The minand the clipopera- tors together ensure that does not change too much from old. More details in deriving the objective functions can be found in the original paper on PPO [Schulman et al. 2017]. 4 LOCOMOTION CURRICULUM LEARNING Learning locomotion directly from the principles of minimal energy and gait symmetry is difficult without additional guidance from motion examples or reward function shaping. Indeed, a successful locomotion policy must learn a variety of tasks often with contradic- tory goals, such as maintaining balance while propelling the body forward, or accelerating the body while conserving energy. One approach to learning such a complex motor skill is to design a cur- riculum that exposes the learner to a series of tasks with increasing difficulty, eventually leading to the original task. Our locomotion curriculum learning is inspired by physical learn- ing aids that provide external forces to simplify the motor tasks, such as exoskeletons for gait rehabilitation or training wheels for riding a bicycle. These learning aids create a curriculum to ease the learning process and will be removed when the learner is suffi- ciently skilled at the original task. To formalize this idea, we view the curriculum as a continuous Euclidean space parameterized by curriculum variables xRn. The learning begins with the simplest lesson x0for the beginner learner, gradually increasing the difficulty toward the original task, which is represented as the origin of the curriculum space (i.e. x=0). With this notion of a continuous cur- riculum space, we can then develop a continuous learning method by finding the optimal path from x0to the origin in the curriculum space. Similar to the standard policy gradient method, at each learning iteration, we generate rollouts from the current policy, use the rollout to estimate the gradients of the objective function of policy optimization, and update the policy parameters based on the gradient. With curriculum learning, we introduce a virtual assistant to provide assistive forces to the learner during rollout generation. The virtual assistant is updated at each learning iteration such that it provides assistive forces appropriate to the current skill level of the learner. Two questions remain in our locomotion curriculum learning algorithm. First, what is the most compact set of parameters for the virtual assistant such that locomotion skills can be effectively learned through curriculum? Second, what is the appropriate cur- riculum schedule, i.e. how much assistive force should we give to the learner at each moment of learning? 4.1 Virtual Assistant Our virtual assistant provides assistive forces to simplify the two main tasks of locomotion: moving forward and maintaining lateral balance. The lateral balancing force is applied along the frontal axis (left-right) of the learner, preventing it from falling sideway. The propelling force is applied along the sagittal axis, pushing the learner forward to reach the desired velocity. With these two assistive forces, ACM Transactions on Graphics, Vol. 37, No. 4, Article 144. Publication date: August 2018.Learning Symmetric and Low-Energy Locomotion 144:5 the learner can focus on learning to balance in the sagittal plane as well as keeping the energy consumption low. Both lateral balancing force and propelling force are produced by a virtual proportional-derivative (PD) controller placed at the pelvis of the learner, as if an invisible spring is attached to the learner to provide support during locomotion. Specifically, the PD controller controls the lateral position and the forward velocity of the learner. The target lateral position is set to 0for maintaining lateral balance while the target forward velocity is set to the desired velocity \u00afvfor assisting the learner moving forward. Different levels of assistance from the virtual assistant create different lesson for the learner. We use the stiffness coefficient kpand the damping coefficient kdto modulate the strength of the balancing and propelling forces respectively. As such, our curriculum space is parameterized by x=(kp,kd). Any path from x0to(0,0)constitutes a curriculum for the learner. Our implementation of the virtual assistant is based on the sta- ble proportional-derivative (SPD) controller proposed by Tan et al. [2011]. The SPD controller provides a few advantages. First, it does not require any pre-training and can be applied to any character morphology with little tuning. In addition, it provides a smooth assistance in the state space, which facilitates learning. Finally, it is unconditionally stable, allowing us to use large controller gains without introducing instability. 4.2 Curriculum Scheduling Fig. 2. (a) The learner-centered curriculum determines the lessons adap- tively based on the current skill level of the agent, resulting in a piece-wise linear path from x0to the origin. (b) In each learning iteration, the learner- centered curriculum finds the next point in the curriculum space using a simple search algorithm that conducts 1D line-searches along five direction. The goal is to find the largest step size, , such that the current policy can still reach 60%of the original return \u00afR. (c) Environment-centered curriculum follows a series of predefined lessons along a linear path from x0to the origin. It introduces the learner to a range of lessons in one curriculum learning iteration, resulting in a set of co-linear, overlapping line segments. The goal of curriculum scheduling is to systematically and grad- ually reduce the assistance from the initial lesson x0and ultimately achieve the final lesson in which the assistive force is completely removed. Designing such a schedule is challenging because an ag- gressive curriculum that reduces the assistive forces too quickly can fail the learning objectives while a conservative curriculum can lead to inefficient learning.We propose two approaches to the problem of curriculum sched- uling (Figure 2): Learner-centered curriculum and Environment- centered curriculum. The learner-centered curriculum allows the learner to decide the next lesson in the curriculum space, resulting in a piece-wise linear path from x0to the origin. The environment- centered curriculum, on the other hand, follows a series of prede- fined lessons. However, instead of focusing on one lesson at a time, it exposes the learner to a range of lessons in one curriculum learning iteration, resulting in a set of co-linear, overlapping line segments from x0to the origin of the curriculum space. 4.2.1 Learner-centered curriculum. The learner-centered curricu- lum determines the lessons adaptively based on the current skill level of the agent (Algorithm 1). We assume that the initial lesson x0is sufficiently simple such that the standard policy learning algo- rithm can produce a successful policy , which generates rollouts B with average return, \u00afR. We then update the lesson to make it more challenging (Algorithm 2) and proceed to the main learning loop. At each curriculum learning iteration, we first update by running one iteration of the standard policy learning algorithm. If the average of the rollouts from the updated policy can reach h%of the original return \u00afR(h=80), we update the lesson again using Algorithm 2. The curriculum learning loop terminates when the magnitude of x is smaller than (=5). We run a final policy learning without any assistance, i.e. x=(0,0). At this point, the policy learns this final lesson very quickly. Given the current lesson xi, the goal of Algorithm 2 is to find the next point in the curriculum space that is the closest to the origin while the current policy can still retain some level of proficiency. Since it is only a two-dimensional optimization problem and the solution xi+1lies inxi+1 xi<0and is strictly positive component-wise, we implement a simple search algorithm that conducts 1D line-searches along each direction d, the line-search will return the largest step size, , such that the current policy can still reach l%of the original return \u00afR(l=60) under the assistance xi+d. Among five line-search results, we choose the largest step size, along the direction dto create the next lesson: xi+1=xi+d. 4.2.2 Environment-centered curriculum. Instead of searching for the next lesson, the environment-centered curriculum updates the lessons along a predefined linear path from x0to the origin (Algo- rithm 3). In addition, the learner is trained with a range of lessons [xbein,xend]in each curriculum learning iteration. Specifically, the learner will be exposed to an environment in which the virtual assistant starts with xbeinand reduces its strength gradually to xendat the end of each rollout horizon. The formula of strength reduction from xbeintoxendcan be designed in several ways. We use a simple step function to drop the strength of the virtual assistant by k%every pseconds ( k=25andp=3). Each step in the step function can be considered a learning milestone. Training with a range of milestones in a single rollout prevents the policy from overfitting to a particular virtual assistant, leading to more efficient learning. In each learning iteration, we first run the standard policy learning for one iteration, with the environment programmed to present the current range of lessons [xbein,xend]. After the policy is updated, ACM Transactions on Graphics, Vol. 37, No. 4, Article 144. Publication date: August 2018.144:6 Yu, Turk and Liu Algorithm we evaluate the performance of the policy using two conditions. If the policy meets both conditions, we update the range of lessons tok%\u00b7[xbein,xend]. Note that the updated xbeinis equivalent to the second milestone of the previous learning iteration, result- ing in some overlapping lessons in two consecutive curriculum learning iterations (See Figure 2c). The overlapping lessons are an important aspect of the environment-centered curriculum learning because they allow the character to bootstrap its current skill when learning a new set of predefined lessons. Similar to the learner- centered curriculum, the curriculum learning loop terminates when the magnitude of xbeinis smaller than (=5), and we run a final policy learning without any assistance, i.e. xbein=(0,0)and xend=(0,0). The first condition for assessing the progress of learning checks whether the learner is able to reach the second milestone in each rollout. That is, the agent must stay balance for at least 2pseconds. Using this condition alone, however, might result in a policy that simply learns to stand still or move minimally in balance. Therefore, we use another condition that requires the average return of the policy to reach a pre-determined threshold, \u00afR, which is%of the return from the initial policy trained with full assistance ( =70). 5 MIRROR SYMMETRY LOSS Symmetry is another important characteristic of a healthy gait. Assessing gait symmetry usually requires at least an observation of a full gait cycle. This requirement poses a challenge to policy learning because the reward cannot be calculated before the end of the gait cycle, leading to a delayed reward function. We propose a new way to encourage gait symmetry by measuring the symmetryAlgorithm 3 states , avoiding the potential issue of delayed reward. Imaging a person who is standing in front of a floor mirror with her left hand behind her back. If she uses the right hand to reach for her hat, what we see in the mirror is a person with her right hand behind her back reaching for a hat using her left hand. Indeed, if the character has a symmetric morphology, the action it takes in some pose during locomotion should be the mirrored version of the action taken when the character is in the mirrored pose. This property can be expressed as: (s)=a((o(s))), (5) where a(\u00b7)ando(\u00b7)maps actions and states to their mirrored versions respectively. We overload the notation to represent the mean action of the stochastic policy. Enforcing Equation 5 as a hard constraint is difficult for standard policy gradient algorithms, but we can formulate a soft constraint and include it in the objective function for policy optimization: Lsym()=B\u00d5 i=0||(si)a((o(si)))||2, (6) where Bis the number of simulation samples per iteration. We use 20,000samples in all of our examples. Since Equation 6 is differen- tiable with respect to the policy parameters , it can be combined with the standard reinforcement learning objective and optimized using any gradient-based RL algorithm. Incorporating the mirror symmetry loss, the final optimization problem for learning the locomotion policy can be defined as: =argmin LPPO()+wLsym(), (7) where wis the weight to balance the importance of the gait sym- metry and the expected return of the policy ( w=4). Note that the mirror symmetry loss is included in the objective function of the policy optimization, rather than in the reward function of the MDP. This is because Lsymexplicitly depends on the policy parameters , thus adding Lsymin the reward function would break the assump- tion in the Policy Gradient Theorem [Sutton et al .2000]. That is, changingshould change the probability of a rollout, not its return. If we included Lsymin the reward function, it would change the return of the rollout when is changed. Instead, we include Lsym ACM Transactions on Graphics, Vol. 37, No. 4, Article 144. Publication date: August 2018.Learning Symmetric and Low-Energy Locomotion 144:7 in the objective function and calculate its gradient separately from that of the LPPO, which depends on the Policy Gradient Theorem. 6 RESULTS We evaluate our method on four characters with different mor- phologies and degrees of freedom. The input to the control policy includes s=[q,\u00dbq,c,v]and the output is the torque generated at each actuated joint, as described in Section 3.1. Because the charac- ter is expected to start at zero velocity and accelerate to the target velocity, the policy needs to be trained with a range of velocities from zero to the target. We include vin the input of the policy to modulate the initial acceleration; vis set to zero at the beginning of the rollout and increases linearly for the first 0.5|v|seconds, encour- aging the character to accelerate at 2m/s2. The parameters of the reward functions used in all the experiments are listed in Table 1. We demonstrate the environment-centered curriculum learning for all the examples and selectively use the learner-centered curriculum learning for comparison. The resulting motions can be seen in the supplementary video. We set the starting point of the curriculum x0to(kp,kd)= (2000 ,2000)in all examples. Note that kpandkdare the proportional gains and damping gains in two independent SPD controllers that provide balancing and propelling forces respectively. The damping gain used to compute the balancing force is 0.1kpand the propor- tional gain used to compute the propelling force is 0. We use Pydart [Ha 2016], a python binding of the DART library [Liu and Jain 2012] to perform multi-body simulation. We simulate the characters at 500Hz, and query the control policy every 15sim- ulation steps, yielding a control frequency of 33Hz. We use the PPO algorithm implemented in the OpenAI Baselines library [Dhariwal et al.2017] for training the control policies. The control policies used in all examples are represented by feed-forward neural networks with three fully-connected hidden layers, and each hidden layer consists of 64units. We fix the sample number to be 20,000steps per iteration for all examples. The number of iteration required to obtain a successful locomotion controller depends on the complex- ity of the task, ranging from 500to1500 , yielding a total sample number between 10and30millions. We perform all training using 8 parallel threads on an Amazon EC2 node with 16 virtual cores and 32G memory. Each training iteration takes 2545s depending on the degrees of freedoms of the character model, leading to a total training time between 4and15hours. 6.1 Locomotion of Different Morphologies Simplified biped. Bipedal locomotion has been extensively studied in the literature, and it is a familiar form of locomotion to everyone. Thus we start with training a simplified biped character to perform walking and running. The character has 9links and 21DOFs, with 1.65m in height and weighs in total 50kg. The results can be seen in Figure 3. As expected, when trained with a low target velocity (1m/s), the character exhibits a walking gait. When trained with a high target velocity ( 5m/s), the character uses a running gait indicated by the emergence of a flight phase. Quadruped. Quadrupeds exhibit a large variety of locomotion gaits, such as pacing, trotting, cantering, and galloping. We appliedour approach to a quadruped model as shown in Figure1(b). The model has 13links and 22DOFs, with a height of 1.15m and weight of88.35kg. As quadrupeds can typically move faster than biped, we trained the quadruped to move at 2m/s and 7m/s. The results are shown in Figure 5. The trained policy results in a trotting gait for low target velocity consistently. For high target velocities, the character learns either trotting or galloping, depending on the initial random seed of the policy. Hexapod. We designed a hexapod creature that has 13links and 24DOFs, inspired by the body plan of an insect. We trained the hexapod model to move at 2m/s and 4m/s. As shown in Figure 6, the hexapod learns to use all six legs to move forward at low velocity, while it lifts the front legs and use the middle and hind legs to 'run' forward at higher velocity. Humanoid. Finally, we trained a locomotion policy for a full hu- manoid character with a detailed upper body. The character has 13 links and 29DOFs with 1.75m in height and 76.6kg in weight. We trained the humanoid model to walk at 1.5m/s and run at 5m/s. In addition, we trained the model to walk backward at 1.5m/s. We kept the same reward function parameters between forward and backward walking. Results of the humanoid locomotion can be seen in Figure 4. During walking forward and backward, the character learns to mostly relax its arms without much swinging motion. For running, the character learn to actively swing its arms in order to counteract the angular momentum generated by the leg movements, which stabilizes the torso movements during running. 6.2 Comparison between Learner-centered and Environment-centered Curriculum Learning We compare the learner-centered and environment-centered curricu- lum learning algorithms on the simplified biped model. As demon- strated in the supplementary video, both methods can successfully train the character to walk and run at target velocities with symmet- ric gaits. We further analyze the performance of the two algorithms by comparing how they progress in the curriculum space, as shown in Figure 7. We measure the progress of the curriculum learning with the l2norm of the curriculum parameter x, since the goal is to reach 0as fast as possible. We can see that environment-centered curriculum learning shows superior data-efficiency by generating a successful policy with about half the data that is required for the learner-centered curriculum learning. 6.3 Comparison with Baseline Methods To demonstrate the effect of curriculum learning and mirror sym- metry loss, we compare our method with environment-centered curriculum learning and mirror symmetry loss (ECL + MSL) to three baseline methods: with environment-based curriculum learn- ing only (ECL), with mirror symmetry loss only (MSL) and using vanilla PPO (PPO) with no mirror symmetry loss nor curriculum learning. The baseline methods are trained on the simplified biped character and the humanoid character for both walking and run- ning. The learning curves for all the tests can be seen in Figure 8. In all four of these tasks, our approach learns faster than all of the baseline methods. Without curriculum learning (i.e. blue and cyan ACM Transactions on Graphics, Vol. 37, No. 4, Article 144. Publication date: August 2018.144:8 Yu, Turk and Liu Table 1. Task and reward parameters Character v wvwuxwuywuzwlEawe Simplified Biped 0to1m/s 3 1 1 1 3 4 0.4 Simplified Biped 0to5m/s 3 1 1 1 3 7 0.3 Quadruped 0to2m/s 4 0.5 0.5 1 3 4 0.2 Quadruped 0to7m/s 4 0.5 0.5 1 3 11 0.35 Hexapod 0to2m/s 3 1 1 1 3 4 0.2 Hexapod 0to4m/s 3 1 1 1 3 7 0.2 Humanoid 0to1.5m/s 3 1 1.5 1 3 6 0.3 Humanoid 0to5m/s 3 1 1.5 1 3 9 0.15 Humanoid 0to1.5m/s 3 1 1.5 1 3 6 0.3 Fig. 3. Simplified biped walking (top) and running (bottom). Results are trained using environment-centered curriculum learning and mirror sym- metry loss. Fig. 4. Humanoid walking (top), running (middle) and backward walking (bottom). Results are trained using environment-centered curriculum learn- ing and mirror symmetry loss. curves), the algorithm typically learns to either fall slowly or stand still (as is shown in the supplementary video). On the other hand, without mirror symmetry loss, the resulting policy usually exhibits asymmetric gaits and the training process is notably slower, which is mostly evident in the running tasks, as shown in Figure 9 and Figure 10. Fig. 5. Dog trotting (top, middle) and galloping (bottom). Results are trained using environment-centered curriculum learning and mirror symmetry loss. Fig. 6. Hexapod moving at 2m/s (top) and 4m/s (bottom). Results are trained using environment-centered curriculum learning and mirror symmetry loss. In addition to the three baseline methods described above, we also trained on the simplified biped walking task using vanilla PPO with a modified reward function, where weis reduced to 0.1. This allows the character to use higher torques with little penalty (\"PPO high torque\" in Table 2). While the character is able to walk forward without falling, the motion appears jerky and uses significantly more torque than our results (see supplementary video). To compare the policies quantitatively, we report the average actuation magnitude i.e. Eeand use the Symmetry Index metric proposed by Nigg et al. to measure the symmetry of the motion [Nigg et al. 1987]: ACM Transactions on Graphics, Vol. 37, No. 4, Article 144. Publication date: August 2018.Learning Symmetric and Low-Energy Locomotion 144:9 Fig. 7. Comparison between environment-centered and learner-centered curriculum learning for simplified biped tasks. (a) Curriculum progress over iteration numbers. 0in theyaxis means no assistance is provided. (b) Points in the curriculum space visited by the two curriculum update schemes. SI(XL,XR)=2|XLXR| XL+XR%, where XLandXRare the average of joint torques produced by the left and right leg respectively. The smaller the value SIis, the more symmetric the gait is. The results can be seen in Table 2. As expected, policies trained with our method uses less joint torque and produces more symmetric gaits. Table 2. Comparison of trained policies in action magnitude and symmetry. ECL denotes using environment-centered curriculum learning, MSL means mirror symmetry loss and PPO means training with no curriculum learning or mirror symmetry loss. We present results for the successfully trained policies. Task Training Setup Ee SI Simplified Biped walk ECL+MSL 2.01 0.0153 Simplified Biped walk ECL 2.98 0.1126 Simplified Biped walk PPO high torque 5.96 0.0416 Simplified Biped run ECL + 6.4 Learning Asymmetric Tasks One benefit of encouraging symmetric actions rather than symmet- ricstates is that it allows the motion to appear asymmetric whendesired. As shown in Figure 11, we trained a humanoid that is walk- ing while holding a heavy object ( 10kg) in the right hand. The character uses an asymmetric gait that moves more vigorously on the left side to compensate for the heavy object on the right side. If we chose to enforce symmetry on the states directly, the character would likely use a large amount of torque on the right side to make the poses appear symmetric. 7 DISCUSSION In this work, we intentionally avoid the use of motion examples to investigate whether learning locomotion from biomechanics princi- ples is a viable approach. In practice, it would be desirable to use our policy as a starting point and improve the motion quality by further training with motion examples or additional reward terms. For example, we took the network of the biped walking policy to warm-start another policy learning session, in which the charac- ter learns to walk with knees raising up high (Figure 12 TOP). We also warm-started from the humanoid running policy to obtain a controller using a bigger stride length (Figure 12 BOTTOM). Be- cause the starting policy is already capable of balancing and moving forward, the refinement learning takes only 200 iterations to train. Our experiments show that both learner-centered and environment- centered curricula are effective in training locomotion controllers. The learner-centered curriculum is designed to bootstrap learning from the character's existing skill, but the curriculum might end up taking a long and winding path to reach the origin of the curriculum space. On the other hand, the environment-centered curriculum takes a straight path to the origin, but it presents the character with predetermined lessons and ignores the character's current skill, which might result in an overly aggressive curriculum. However, our scheduling algorithm for the environment-centered curricu- lum ensures that the lessons in two consecutive learning iterations overlap, reducing the chance of the character being thrown into a completely unfamiliar environment and failing immediately. While the learner-centered curriculum requires less hyper parameter tun- ing, we prefer the environment-centered curriculum for its data efficiency. The main design choice of the environment-centered cur- riculum lies in the formula of reduction from xbeintoxend. Our arbitrarily designed step function works well in all of our examples, but it may be possible to design a different reduction formula that can lead to an even more data-efficient learning algorithm. One of the most encouraging results of this work is the emer- gence of different gaits for different target velocities. Most previous work obtains different gait patterns through contact planning or constraint enforcement [Coros et al .2010; Ye and Liu 2010]. In contrast, with different target velocity v, our characters learn speed- appropriate gaits using nearly identical reward functions (Table 1), without additional engineering effort. In addition to the parameters of the reward function and hyper parameters in the network, the style of the motion also depends on the kinematic and dynamic properties of the character. A com- prehensive sensitivity analysis is beyond the scope of this work. However, we notice that the resulting motion is particularly sensi- tive to the range of joint torques that each actuator is allowed to generate. In this paper, the torque range is set heuristically based on ACM Transactions on Graphics, Vol. 37, No. 4, Article 144. Publication date: August 2018.144:10 Yu, Turk and Liu Fig. 8. Learning curves for the proposed algorithm and the baseline methods. Fig. 9. Simplified biped walking (top) and running (bottom). Results are trained using environment-centered curriculum learning only (no mirror symmetry loss). Fig. 10. Humanoid walking (top) and running (bottom). Results are trained using environment-centered curriculum learning only (no mirror symmetry loss). Fig. 11. Humanoid walking holding heavy object in right hand. Results are trained using environment-centered curriculum learning and mirror symmetry loss. the perceived strength of each joint. Our preliminary results show that different locomotion gaits can result when different torque range settings. For example, we observed hopping behavior when we greatly increased the leg strength of the humanoid character. There are many different ways to design the virtual assistant, and some are more effective than others. For example, we tried to provide lateral balance using two walls, one on each side of the Fig. 12. Biped walking with high knees (TOP) and humanoid running with large stride length (BOTTOM) warm-started from our approach. character. The initial lesson presented a narrow passage between the two walls so that the character could not possibly fall. As the curriculum proceeded, we then widened the gap between the walls. The results showed that the character learned to intentionally lean on the wall to decrease the penalty of falling. As the gap widened, the character leaned even more until the character fell because the walls were too far apart. We have also attempted to provide propelling assistance through the use of a virtual treadmill. This experiment was unsuccessful even when learning the initial lesson x0. Our speculation is that the treadmill does not provide the necessary assistance for learning how to move forward; the learner still needs to put a significant amount of effort towards matching the speed of moving ground. In other words, it is arguable that learning with assistance ( x0) is just as difficult to learn as without assistance. 8 CONCLUSION We have demonstrated a reinforcement learning approach for creat- ing low-energy, symmetric, and speed-appropriate locomotion gaits. One element of this approach is to provide virtual assistance to help the character learn to balance and to reach a target speed. The sec- ond element is to encourage symmetric behavior through the use of an additional loss term. When used together, these two techniques provide a method of automatically creating locomotion controllers for arbitrary character body plans. We tested our method on the lower half of a biped, a full humanoid, a quadruped, an a hexapod and demonstrated learning of locomotion gaits that resemble the natural motions of humans and animals, without prior knowledge about the motion. Because our method generalizes to other body plans, an animator can create locomotion controllers for characters and creatures for which there is no existing motion data. ACM Transactions on Graphics, Vol. 37, No. 4, Article 144. Publication date: August 2018.Learning Symmetric and Low-Energy Locomotion 144:11 Although our characters demonstrate more natural locomotion gaits comparing to existing work in DRL, the quality of the motion is still not on a par with previous work in computer animation that exploits real-world data. Further investigation on how to incorpo- rate motion capture data, biological-based modeling, and policy refinement (see Figure 12) is needed. In addition, our work is only evaluated on terrestrial locomotion with characters represented by articulated rigid bodies. One possible future direction is to ap- ply the curriculum learning to other types of locomotion, such as swimming, flying, or soft-body locomotion. ACKNOWLEDGMENTS We thank Charles C. Kemp, Jie Tan, Ariel Kapusta, Alex Clegg, Zackory Erickson and Henry M. Clever for the helpful discussions. This work was supported by NSF award IIS-1514258 and AWS Cloud Credits for Research. REFERENCES Mazen Al Borno, Martin De Lasa, and Aaron Hertzmann. 2013. Trajectory optimization for full-body movements with complex contacts. IEEE transactions on visualization and computer graphics 19, 8 (2013), 1405-1414. Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. 2009. Curricu- lum learning. In Proceedings of the 26th annual international conference on machine learning . ACM, 41-48. Greg Brockman, Vicki Cheung, Ludwig Schulman, Jie Tang, and Wojciech OpenAI and Michiel 2010. Generalized Biped Walking In ACM SIGGRAPH 2010 Papers (SIGGRAPH '10) . ACM, New Coros, Andrej Panne. 2011. Locomotion Skills for Simulated Quadrupeds. ACM Transactions on Graphics 30, 4 (2011), Article TBD. Marco da Silva, Yeuhi Abe, and Jovan Popovi. 2008. Interactive Simulation of Stylized Human Locomotion. ACM Trans. Graph. 27, 3, Article 82 (Aug. 2008), 10 pages. https://doi.org/10.1145/1360612.1360681 Martin de Lasa, Igor Mordatch, and Aaron Hertzmann. 2010. Feature-based Locomotion Controllers. ACM Trans. Graph. 29, 4, Article 131 (July 2010), 10 pages. https: //doi.org/10.1145/1778765.1781157 Prafulla Dhariwal, Christopher Hesse, John Schulman, and Pieter Abbeel. 2016. Bench- marking deep reinforcement learning for continuous control. In International Con- ference on Machine Learning . 1329-1338. Martin L Felis and Katja Mombaur. 2016. Synthesis of full-body 3-D human gait using optimal control methods. In Robotics and Automation (ICRA), 2016 IEEE International Conference on . IEEE, 1560-1566. David Held, Markus Wulfmeier, and Pieter Abbeel. 2017. Reverse curriculum generation for reinforcement arXiv preprint arXiv:1707.05300 (2017). T. Geijtenbeek and N. Pronost. 2012. Character Animation Using Simulated Physics: A State-of-the-Art Review. Comput. Graph. Forum 31, 8 (Dec. 2492- 2515. https://doi.org/10.1111/j.1467-8659.2012.03189.x Thomas Geijtenbeek, Michiel van de Panne, van der Stappen. 2013. Flexible Muscle-based Locomotion for Bipedal Creatures. ACM Trans. Graph. 32, 6, Article 206 (Nov. 2013), 11 pages. https://doi.org/10.1145/2508363.2508399 Alex Graves, Marc G Bellemare, Jacob Menick, Remi Ha and C Karen Liu. 2014. Iterative training of dynamic skills inspired by human coaching techniques. ACM Transactions on Graphics 34, 1 (2014). Nikolaus Hansen and Andreas Ostermeier. 1996. Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation. In Evolu- tionary Computation, Nicolas Ziyu Wang, Carlos Florensa, and Pieter Abbeel. 2017. Automatic Goal Generation for Reinforcement Learning Agents. arXiv:1705.06366 (2017). WALTER Herzog, Benno Read, and EWA Olsson. 1989. Asymmetries in ground reaction force patterns in normal human gait. Med Sci Sports Exerc 21, 1 (1989), 110-114. Jessica K. Hodgins, Wayne L. Wooten, David C. Brogan, and James F. O'Brien. 1995. Animating Human Athletics. In Proceedings of the 22Nd Annual Conference on Com- puter Graphics and Interactive Techniques (SIGGRAPH '95) Yuting Ye, and C. Karen Liu. 2009. Optimization-Based Interactive Motion Synthesis. ACM Transaction on Graphics 28, 1 (2009), 1-10. Andrej Karpathy and Michiel Van De Panne. 2012. Curriculum learning for motor skills. InCanadian Conference on Artificial Intelligence . Springer, 325-330. Yann LeCun, (2015), 436-444. Yoonsang Lee, Sungeun Kim, and Jehee 2010. Data-driven biped control. ACM Transactions on Graphics (TOG) 29, 4 (2010), 129. Yoonsang Lee, Moon Seok Park, Taesoo Kwon, and Jehee Lee. 2014. Locomotion Control for Many-muscle Humanoids. ACM Trans. Graph. 33, 6, Article 218 (Nov. 2014), 11 pages. https://doi.org/10.1145/2661229.2661233 Sergey Levine and Vladlen Koltun. 2014. Learning Complex Neural Network Policies with Trajectory Optimization. In ICML '14: Proceedings of the 31st International Conference on Machine Learning . Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015). C. Karen Liu, Aaron Hertzmann, and Zoran Popovi. 2005. Learning Physics-Based Motion Style with Nonlinear Inverse Optimization. ACM Transactions on Graphics 24, 3 (July 2005), 1071-1081. C. Karen Liu and Sumit Jain. 2012. A Short Tutorial on Multibody Dynamics. Tech. Rep. GIT-GVU-15-01-1, Georgia Institute of Technology, School of Interactive Computing (2012). http://dartsim.github.io/ Libin Liu and Jessica Hodgins. 2017. Learning to schedule control fragments for physics- based characters using deep q-learning. ACM Transactions on Graphics (TOG) 36, 3 (2017), 29. Libin Liu, Michiel Van De Panne, and Kangkang Yin. 2016. Guided Learning of Control Graphs for Physics-Based Characters. ACM Trans. Graph. 35, 3, Article 29 (May 2016), 14 pages. https://doi.org/10.1145/2893476 Tambet Matiisen, Avital Oliver, Taco Cohen, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning . 1928-1937. Igor Mordatch, Kendall Lowrey, Galen Andrew, Zoran Popovic, and Emanuel V Todorov. 2015. Interactive control of diverse complex characters with neural networks. In Advances in Neural Information Processing Systems . 3132-3140. Igor Mordatch, Emanuel Todorov, and Zoran Popovi. 2012. Discovery of Complex Behaviors Through Contact-invariant Optimization. ACM Trans. Graph. 31, 4, Article 43 (July 2012), 8 pages. https://doi.org/10.1145/2185520.2185539 Igor Mordatch, Jack M. Wang, Emanuel Todorov, and Vladlen Koltun. 2013. Animating Human Lower Limbs Using Contact-invariant Optimization. ACM Trans. Graph. 32, 6, Article 203 (Nov. 2013), 8 pages. https://doi.org/10.1145/2508363.2508365 Uldarico Muico, Nonlinear Control of Dynamic Characters. ACM Trans. Graph. 28, 3, Article 81 (July 2009), 9 pages. https://doi.org/10.1145/1531326.1531387 Sanmit Narvekar, Jivko Sinapov, Matteo Leonetti, and Peter Stone. 2016. Source task creation for curriculum learning. In Proceedings of the 2016 International Confer- ence on Autonomous Agents & Multiagent Systems . International Foundation for Autonomous Agents and Multiagent Systems, 566-574. B Nigg, R Robinson, and W Herzog. 1987. Use of Force Platform Variables to Ouantify the Effects of Chiropractic Manipulation on Gait Symmetry. Journal of manipulative and physiological therapeutics 10, 4 (1987). Valerie Closson, Mary C Verrier, W Richard Staines, Sandra E Black, and William E McIlroy. 2008. Gait asymmetry in community-ambulating stroke survivors. Archives of physical medicine and rehabilitation 89, 2 (2008), 304-310. Xue Bin Peng, Glen Berseth, and Michiel van de Panne. 2015. Dynamic Terrain Traversal Skills Using Reinforcement Learning. ACM Trans. Graph. 34, 4, Article 80 (July 2015), 11 pages. https://doi.org/10.1145/2766910 Xue Bin Peng, Glen Berseth, and Michiel van de Panne. 2016. Terrain-adaptive Loco- motion Skills Using Deep Reinforcement Learning. ACM Trans. Graph. 35, 4, Article 81 (July 2016), 12 pages. https://doi.org/10.1145/2897824.2925881 ACM Transactions on Graphics, Vol. 37, No. 4, Article 144. Publication date: August 2018.144:12 Yu, Turk and Liu Xue Bin Peng, Glen Berseth, Kangkang Yin, and Michiel Van De Panne. 2017. DeepLoco: Dynamic Locomotion Skills Using Hierarchical Deep Reinforcement Learning. ACM Trans. Graph. 36, 4, Article 41 (July 2017), H Lampert. 2015. Curriculum learning of multiple tasks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 5492-5500. Lerrel Pinto and Abhinav Gupta. 2016. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. In Robotics and Automation (ICRA), 2016 IEEE International Conference on . IEEE, 3406-3413. John Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015a. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15) . 1889-1897. John Schulman, Philipp and Pieter Abbeel. 2015b. continuous control using generalized advantage John Won Sok, Manmyung Kim, and Jehee Lee. 2007. Simulating Biped Behaviors from Human Motion Data. In ACM SIGGRAPH 2007 Papers (SIGGRAPH '07) . ACM, New York, NY, USA, Article 107. https://doi.org/10.1145/1275808.1276511 Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. 2000. Policy gradient methods for reinforcement learning with function approximation. InAdvances in neural information processing systems . 1057-1063. Jie Tan, Yuting Gu, C. Karen Liu, and Greg Turk. 2014. Learning Bicycle Stunts. ACM Trans. Graph. 33, 4, Article 50 (July 2014), 12 pages. https://doi.org/10.1145/2601097. 2601121 Jie Tan, Karen Liu, and Greg Turk. 2011. Stable proportional-derivative controllers. IEEE Computer Graphics and Applications 31, 4 (2011), 34-44. Michiel Van de Panne and Alexis Lamouret. 1995. Guided optimization for balanced locomotion. In Computer animation and simulation , Vol. 95. Springer, 165-177. Kevin Wampler and Zoran Popovi. 2009. Optimal Gait and Form for 2009 Papers (SIGGRAPH '09) . New York, NY, USA, Article 60, pages. https://doi.org/10.1145/1576246.1531366 Kevin Wampler, Zoran Popovi, and Jovan Popovi. 2014. Generalizing Style to New Animals with Inverse Optimal Regression. ACM Trans. Graph. 33, 4, Article 49 (July 2014), 11 pages. https://doi.org/10.1145/2601097.2601192 Jack M. Wang, David J. Fleet, and Aaron Hertzmann. 2009. Optimizing Walking Controllers. ACM Trans. Graph. 28, 5, Article 168 (Dec. 2009), 8 pages. https: //doi.org/10.1145/1618452.1618514 Jack M. Wang, Samuel R. Hamner, Scott L. Delp, and Vladlen Koltun. 2012. Optimizing Locomotion Controllers Using Biologically-based Actuators and Objectives. ACM Trans. Graph. 31, 4, Article 25 (July 2012), 11 pages. https://doi.org/10.1145/2185520. 2185521 Jungdam Won, Jongho Park, Kwanyu Kim, 2017. How to train your dragon: example-guided control of flapping flight. ACM Transactions on Graphics (TOG) 36, 6 (2017), 198. Jia-chi and Zoran Popovi. Terrain-adaptive bipedal locomotion control. ACM Transactions on Graphics (TOG) 29, 4 (2010), 72. Yuting Ye and C. Karen Liu. 2010. Optimal Feedback Control for Character Animation Using an Abstract Model. ACM Trans. Graph. 29, 4, Article 74 (July 2010), 9 pages. https://doi.org/10.1145/1778765.1778811 KangKang Yin, Stelian Coros, Philippe Beaudoin, and Michiel van de Panne. 2008. Continuation methods for adapting simulated skills. In ACM Transactions on Graphics (TOG) , Vol. 27. ACM, 81. KangKang Yin, Kevin Loken, Michiel van de Panne. 2007. SIMBICON: Simple Biped Locomotion Control. ACM Trans. Graph. 26, 3, Article 105 (July 2007). https: //doi.org/10.1145/1276377.1276509 ACM Transactions on Graphics, Vol. 37, No. 4, Article 144. Publication date: August 2018. "}