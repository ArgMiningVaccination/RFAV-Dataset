{"title": "PDF", "author": "PDF", "url": "https://pa.msu.edu/graduate-program/current-graduate-students/draft-dissertation-pdf/Young_thesis_submitted_v2.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "THE PAST, PRESENT, AND FUTURE OF GRADUATE ADMISSIONS IN PHYSICS By Nicholas T. Young A DISSERTATION Submitted to Michigan State University in partial fulllment of the requirements for the degree of Physics - Doctor of Philosophy Computational Mathematics, Science, and Engineering - Dual Major 2021ABSTRACT THE PAST, PRESENT, AND FUTURE OF GRADUATE ADMISSIONS IN PHYSICS By Nicholas T. Young While graduate admissions in physics directly aects only a small number of people on an annual basis, the number of people indirectly aected is many orders of magnitude greater. Those who complete graduate degrees in physics will go on to beyond leaders in industry, government, and academia,withthelattereducatingthenextgenerationofleadersinscienceandengineering. Given the possibly enormous consequences of our decisions in physics graduate admissions, care should betakentoensurethattheprocessisworkingeectively. However,theevidencesuggestsotherwise. Many inequities exist in the admissions process, unfairly keeping potentially great scientists from even pursuing graduate studies. This thesis then seeks to understand what those inequities might beandhowwemightaddressthem. First,Istudytheadmissionsprocessinthephysicsdepartment at a Midwestern, public university using the random forest algorithm, a machine learning method, to understand what drives the admissions process. After nding that test scores and grades drive theprocess,Iinvestigatedwhetheroneofthosetests,thephysicsGRE,givesapplicantsanoutsized advantage that it is claimed to provide, which it did not. Given the components that drove the admissionsprocesscontaininequities,thesecondhalfofthethesisexploreswhetherarubric-based holisticadmissionsprocessmightbeabletoaddressthoseinequities. Preliminaryevidencesuggests that it does. Finally, to ensure that the methods used in the previous chapters were appropriate, the thesis concludes with a simulation study, nding that the methods used might lead to false negatives in the conclusion. Overall, this thesis suggests that the current graduate admissions process in physics contains inequities and that rubric-based admissions might be able to address them. By addressing those inequities, everyone can be given a fair shot in the admissions process andphysicsasadisciplinecanworktowardbecomingmorerepresentativeofthepopulation. Failure to act only perpetuates the inequities that have and will continue to keep people out of physics.ACKNOWLEDGEMENTS Thisthesiswouldnothavebeenpossiblewithoutthehelpandsupportoftoomanyfolkstomention. However, I will attempt to do so here. First, I would like to thank my dissertation committee for all the feedback and direction they have provided over the past few years in making this thesis as strong as possible. Second, I would like to thank my advisor Danny Caballero who consistently provided support andchallengedmetobethebestresearcherIcouldthroughoutthisprocess. WithDanny,whenever Ibroughtupanewresearchideaordirection,hisquestionwasnever'whydoyouwanttodothat?', but 'how can I help you achieve it?' This thesis would not have been possible without his trust in allowing me to take this thesis in the directions I wanted. Third, I would like to thank the many members of PERL who have provided feedback through groupmeetings,practicepresentations,manuscriptreviews,andgeneralconversationsandprovided asenseofcommunity. I'despeciallyliketothankRachelHendersonwhohasservedasmyunocial co-advisor for providing insights and new directions that helped me grow as a researcher. Additionally, I'd like to thank the many members outside of PERL who have also provided feedbackandinspirationthatappearinthisthesis,especiallyDevinSilviawhosedatavisualization class had a signicant impact on the gures appearing in this thesis and Odd Petter Sand whose own data visualizations inspired those that appear in the introduction. Fourth, I would like thank the many people who provided and curated the data that appears in this thesis including Scott Pratt, Kirsten Tollefson, and Remco Zegers for providing data about MichiganState'sgraduateprogramandJuliePosseltandCaseyMillerforprovidingdatacollected by the Inclusive Graduate Education Network. In addition, I'd like to thank Nicole Verboncoeur andTabithaHudsonwhospentmanyhoursoftheirsummerreadingthroughapplicationstoextract the necessary data. Fifth, I would like to thank Kim Crosslan. My experience in the graduate program would not havegoneassmoothlyasitdidwithoutallofthesupportandassistanceKimprovided. Regardless iiiofwhatissueIencounteredorproblemIfaced,thesolutionwasalwaysonlyanemailorphonecall to Kim away. Sixth, I would like to thank my family for all their support over the years, both prior to and throughout graduate school. Seventh, I would like to thank the world's best two dogs, Kali and Xavier. Over the course of doing this thesis, they have (literally) been by my side providing emotional support in between asking for belly rubs and play time. Finally, I would like to thank my partner, Sarah. This thesis would not have been possible without her. From the daily support she provided to her feedback and suggestions, she has helped me grow into a better person and a better researcher. I'm excited to see where our journey will go next. ivTABLE OF CONTENTS LIST How this thesis contributes to the eld of physics education research . . . . . . this thesis contributes to computational mathematics, science, and engineering 5 1.4 Summaries of the graduate admissions committees . . THE BEGINNING: USING MACHINE LEARNING TO UNDER- STAND PHYSICS to address whether the process changed . . 111 CAN TRUST THE RESULTS IN THE PREVIOUS CHAP- TERS: Table 2.1: Variables used in our model and their scale Summary of the comparisons we analyzed, which group needs to stand out and which does not, and the gure number Distribution of applicants scoring in each Physics GRE range by size of insti- tution. ETS only publishes overall score distributions and hence, we cannot report national scores from only domestic ** signies full mediation is present, ysignies moderation is present. However no moderation The three models compared in this chapter and the data that went into each . . . 96 Table 5.2: Minimum,median,andmaximumvaluesofthemetricsobtainedoverthe125 hyperparameter when using Tomek Links and MICE for each of the three data sets . . . . 106 Table 6.1: Log-F data augmentation example for a two feature and m=1 example. The last four rows are the Feature and outcome imbalances for the binary features from actual graduate school admission values for the various models on the of advantages and disadvantages for each algorithm used in this study . 152 ixLIST OF FIGURES Figure 1.1: VisualrepresentationoftheframeworkpresentedinRossandOdden[2]with methods and methodology broken down according to GRE score, and undergraduate GPA, appearing in orange, were the factors found to be meaningful and hence predictive of being admitted. . . . . 20 Figure 2.2: Averaged conditional feature importances over 30 trials. Physics GRE score and undergraduate GPA, appearing in orange, were the factors found to be meaningfulandhencepredictiveofbeingadmittedwhenadjustingforcorre- lations background coloring expresses the prediction of the model had an applicant had that of the 125 hyperparameter combinations in which each feature had a given rank. Notice that there is a block of features that range between 1 and 5 and a block of features that rank between 7 and representation of eqs. (3.1) to (3.3). The top graphic shows eq. (3.1) while the bottom graphic shows eqs. Fraction of applicants admitted by undergraduate GPA and physics GRE score. The number of students in each bin is also shown. 'Any' corresponds to the corresponding row or column totals. The bin label corresponds to the upperboundofvaluesinthebinexclusivewiththeexceptionofthe4.0GPA bin which includes 4.0. Values are colored based on whether they are above, below, or equal to the overall admissions rate. Admissions rates within 10% of the overall rate are colored the same as the overall rate. The above and below average colors are based on being above/below the midpoint between the max/min admission fraction and the overall average. These are based on raw numbers and not a statistical condensed version of Fig. 3.3 showing the fraction of applicants admitted by undergraduate GPA and physics GRE the bootstrapped coecients in eqs. (3.1) to (3.3). We do nd evidence of the physics GRE score mediating the relationship between GPA and coecients in eqs. (3.5) to (3.7). We do nd evidence of the physics GRE score mediating selectivity and admissions status but do not nd evidence GPA mediating selectivity and coecients in eqs. (3.5) to (3.7). We do nd evidence of the physics GRE score mediating institution size and admission status but do not nd evidence of GPA mediating institution size and admissions status. We do not nd evidence of a serial mediating relationship. Statistically evidence of the physics GRE score mediating gender and admission statusbutdonotndevidenceofGPAmediatinggenderandadmissionstatus. We do not nd evidence of a serial mediating relationship. Statistically in eqs. (3.5) to (3.7). WedondevidenceofGPAmediatingraceandadmissionstatusandaserial mediation eect but do not nd evidence of the physics GRE mediating race and admission status. Statistically signicant 52 xiFigure3.14: A revised version of Fig. 3.4 showing the fraction of applicants admitted by undergraduate GPA and physics GRE score when the cuto score for a high physics GRE score is 670. Here, the number of applicants who could benet from a high physics GRE score is approximately equal to the number of applicants who could be penalized by a low physics GRE score. . . . . . . . 58 Figure3.15: A revised version of Fig. 3.4 showing the fraction of applicants admitted by undergraduate GPA and physics GRE score when the cuto score for a high undergraduate GPA revised version of Fig. 3.4 showing the fraction of applicants admitted by undergraduate GPA and physics GRE score when the cuto score for a high undergraduate GPA is 3.6. Here, the number of applicants who could benet from a high physics GRE score is approximately equal to the number of applicants who could be penalized by a low physics GRE score. . . . . . . . 60 Figure 4.1: Faculty ratings of domestic applicants on 18 constructs. In the plot, a larger, darker circle means that more applicants are in that bin. While many appli- cants are in each level of the academic preparation and test score constructs, few applicants are in the \"low\" bin of the research, noncognitive skills, and Faculty ratings of domestic applicants on 18 constructs split by whether the applicant was admitted. The distribution of ratings of all constructs is statistically dierent for admitted applicants compared to non-admitted applicants. Overall, most admitted applicants were rated \"high\" while most non-admitted Faculty ratings of domestic applicants on 18 constructs split by whether the applicant was male or female. Only three of the constructs showed dierences between males and females: physics GRE score where males Faculty ratings of domestic applicants on 18 constructs split by whether the applicantattendedamoreselectiveorlessselectiveundergraduateuniversity. Only the general GRE and physics GRE scores showed . . . . . . 80 Figure 4.5: Faculty ratings of domestic applicants on 18 constructs split by whether the applicantattendedauniversitywithalargerorsmallerphysicsprogram. shows Fig 2.3 with the Tomek Links marked. Filled points represent Tomek Links. Plot B shows the same plot after the Tomek Links have importances over 30 trials. Physics GRE score, un- dergraduate GPA, Quantitative GRE score, Verbal GRE score and proposed research area, appearing in orange, were the factors found to be meaningful and hence predictive of being showing the ranks of each feature before the implementation of the rubric (left) and the after the implementation of the rubric (right) using data sets 0 and 1a respectively. Features toward the top of the plot are more predictive. Features in orange were found to be the meaningful features needed to predict whether the applicant was admitted in their respective model. Notice that the ordering of the more predictive features is largely unchanged. Plot adapted conditional feature importances over 30 trials. Physics GRE score and proposed research area, appearing in orange, were the factors found to bemeaningfulandhencepredictiveofbeingadmittedoncecorrelationswere of the 125 hyperparameter combinations in which each feature had a given rank for data set 1a. Notice that the plot is mostly diagonal and that physics GRE score and GPA are almost always the top two features. . . . . 101 Figure 5.6: ComparisonofthetestingAUCwhenA)DataSet0isusedtotrainthemodel and B) when Data Set 1a is used to train the model. Training refers to the trainingAUCforthemodel. Allerrorbarsare1standarderror. Resultswere Comparison of the testing accuracy when A) Data Set 0 is used to train the modelandB)whenDataSet1aisusedtotrainthemodel. Thenullaccuracy is shown in cyan with the shorter in height error bars. All error bars are 1 standard error. Results were averaged over 30 factors found to be meaningful and predictive of being admitted. . . . . . 103 Figure 5.9: Averaged conditional feature importances over 30 trials for the models of data set 1b. Physics GRE score and achievement orientation, appearing in orange,werethefactorsfoundtobemeaningfulandhencepredictiveofbeing admitted once correlations the 125 hyperparameter combinations in which each feature had a given rank for models of data set 1b. Notice that the plot is mostly diagonalandthatphysicsGREscore,achievementorientation,andqualityof work are always the top A shows data set 0 with the decision boundary for a model with just the physics GRE score and undergraduate GPA (Fig 2.3) while plot B shows the data with the Tomek Links removed and the resulting decision boundary for GRE score and undergraduate GPA. Plot B shows the data with the Tomek Links removed and the resulting decision boundary for the 2D model. . . 108 Figure 6.1: Distribution of binary features in the simulated c1\u00b8=0\u00955,#=1\u0096000model. . 129 Figure 6.2: Distribution of continuous features values for a subset of the random forest models. Feature names shown in black were constructed to be informative while feature names in grey were constructed to be noise. Plot A shows the N=1000 70/30 out- come imbalance case with the standard random forest algorithm and Gini importance, plot B shows the N=1000 50/50 outcome imbalance case with the standard random forest algorithm and accuracy permutation importance, plotCshowstheN=100,50/50outcomeimbalancecasewiththeconditional inference forest and AUC-permutation importance, and plot D shows the N=10,000 60/40 outcome imbalance case with conditional inference forest and accuracy-permutation importance. For all of the permutation impor- tances, features with less imbalance tend to have larger importances than more imbalanced features for The ranks of the informative features for the four importance measures, grouped by the sample size and outcome imbalance. Noise features are not shown and any feature ranked below a noise feature was assigned a rank of0. odds ratios and 95% condence intervals found by logistic regression models compared by outcome imbalance. Our built-in value is represented by the circled plus. Plot A is a sample size of #=100, plot B is a sample size of #=1\u0096000and plot C is a sample size of #=10\u0096000. Condence intervals that span beyond the scale are removed from the plot. Note the log scale on the horizontal for detection, U=0\u009505. Plot A uses the Holm- Bonferroni correction to control for multiple tests while plot that span beyond the scale are removed from the plot. For higher outcome imbalance, Firth and Log-F penalizations can Ridge penalizations on the #=100data. Dots represent the median value. Plot A shows the 50/50outcome imbalance, plot B shows the 70/30 outcome imbalance, and plot C shows the Ridge penalizations on the #=1\u0096000data. Dots represent the median value. Plot A shows the 50/50outcome imbalance, plot B shows the 70/30outcome imbalance, and plot C shows the according to (A) but only Race- Latinx is detectable in (C). RaceLatinx matrix counts the number of each predicted classication by the model and compares that to the what the data indicates. In this case, a two class system with binary classications leads to a 2 x 2 matrix. For \" classes, the matrix continues to be square and grows to be receiver operating characteristic (ROC) curves that demonstrate two models: one that is better than chance (blue) and one that is worse than chance (green). These ROC curves are plotted along with the chance line (orange dotted). Models that are demonstrably better than chance have ROC curves that tend towards the upper-left corner of the space as the arrow indicates. Models that are worse than chance tend towards the bottom-right corner. (b) For both models, the area under the ROC curves (AUC) are shown (blue and green shading) and computed. AUC provides a measure of the quality of the model. It is indicative of the probability of accurately classifying a random sample from the & institutional selectivity for each applicant. . . 177 Figure B.2: Distribution of physics GRE scores and undergraduate GPAs by gender and whethertheapplicantidentiedasamemberofracialorethnicgroupcurrently Admission fractions of applicants split by their gender and the selectivity of their Admission fractions of applicants split by their gender and the size of their undergraduate Admission fractions of applicants split by their race and the selectivity of their Admission fractions of applicants split by their race and the size of their undergraduate Faculty ratings of domestic applicants on 18 constructs split by whether the applicant was male or female and whether they were admitted or not. . . . . . . 186 Figure D.2: Faculty ratings of domestic applicants on 18 constructs split by whether the applicantattendedamoreselectiveorlessselectiveundergraduateuniversity and whether they were admitted Faculty ratings of domestic applicants on 18 constructs split by whether the applicant attended a university with a larger or smaller physics program and whether they were admitted or ratio, Gini importance, and AUC-permutation im- portance for the features in the school Gini importance, and AUC-permutation im- portance for the features in ratio, Gini importance, and AUC-permutation im- portance for the features in the school plots, between 20% and 34% of the points fall outside of the condence intervals, suggesting the logistic regression models might not be tting the data PAST, PRESENT, AND FUTURE OF GRADUATE ADMISSIONS IN PHYSICS: AN OVERVIEW People like us who believe in physics know that the distinction between past, present, and future is only a stubbornly persistent illusion -Albert Einstein 1.1 Establishing the need to study graduate admissions in physics At rst glance, studying graduate admissions in physics may seem like a small, trivial problem. After all, only a relatively small number of people are directly aected by the physics graduate admissions process. Using the number of test-takers of a commonly required applicant exam in physics, the physics GRE, as a proxy for the number of applicants, approximately 7,000 students applytophysicsgraduateprogramsannually[1]. Incomparison,rst-yearphysicscoursesthatare often the focus of physics education research [2] enroll nearly 425,000 annually [3]. However, the number of people indirectly aected by physics graduate admissions is orders of magnitude larger. The applicants who are admitted to programs will go onto to become leaders inacademia, industry, andgovernmentin eldsas diverseas energy,technology, nationaldefense, and medicine [4]. In addition, some of the admitted applicants will go on to beyond faculty who will train the next generation of scientists, engineers, medical doctors, and science teachers. Furthermore,graduateadmissionshaseconomicconsequencesforbothstudentsandtaxpayers. Applicants who are admitted and earn their PhD have higher average salaries than those with only a bachelor's degree [5], meaning that success in graduate admissions influences earning potential laterinlife. Inaddition,asmanygraduatestudentsareindirectlysupportedbytaxpayersviagrants awarded by governmental agencies, departments have a duty to use the tax payer money wisely by admitting applicants who will be successful in their programs. When taking into account tuition, stipend, and overhead, training a single graduate student can cost tax payers between a quarter and half a million dollars. For the department of physics at Michigan State, the estimated cost is 1$80,000 a year per graduate student. However, if admitted students are not supported throughout their programs, neither applicants nor tax payers will see the benets that can be aorded through graduate study. Yet, given the potentially large impacts of graduate admissions in physics, not everyone is given a fair chance in the process. Physics remains largely white and male even as the United States population becomes increasingly less white and higher education is becoming less male- dominated[6,7]. Tostayintouchwiththedemographicsofthecountryandthebroadereconomy, physics as a discipline needs to reevaluate who is allowed to participate. Failing to do so risks physics losing out on the diversity of opinion and perspectives needed to advance as a discipline and do so ethically. Graduate admissions is but one small part of that process. In this thesis, I will explore the historical approaches to graduate admissions in physics as well as oer a possible route toward achieving those goals of diversity and equity in the process. 1.2 How this thesis contributes to the eld of physics education research Unfortunately, there does not exist a consensus about the subelds of physics education research (PER) and various attempts have been made over years, both broadly [2,8-10] and for specic populationsandtopics[11-13]. MyworktsmostnaturallyintoRussandOdden'sframework[2] so I will map onto that. Russ and Odden classied education research along seven dimensions: disciple, phenomenon, population, context, methods/methodology, theoretical/conceptual framework, and epistemology. ThisthesiscontributestoPERinthedimensionsofpopulation,context,andmethods/methodology so I will focus on those. A visual representation is shown in Fig. 1.1. First, this thesis contributes to PER by focusing on graduate students and the graduate admis- sions process. Traditionally, the population of PER studies has been undergraduate students in introductoryphysicscourses[2,13]. Morerecentlyhowever,PERhasexpandedtofocusonupper- division undergraduate students [14-19], non-physics majors [20,21], graduate students [22-26], 2Figure1.1: VisualrepresentationoftheframeworkpresentedinRossandOdden[2]withmethods and methodology broken down according to Ding's genres [12]. Dimensions that are in bold and expanded represent areas this thesis advances in PER based on their framework, including population, context, and methods and methodology. K-12teachersandstudents[27-33],instructors,teachingassistants,andlearningassistants[34-42], andeveninstitutionsthemselves[43,44]. Thisthesisthenaddstothegrowingbodyofworkregard- inggraduatestudentsinphysics. Studyinggraduateadmissionsspecicallyhasonlybecomeanarea of inquiry recently, with most of the relevant research published within the last ve years [45-56]. Second,thisthesiscontributestoPERbyfocusingoncontextsthathavereceivedlessattentionin theliterature. Whenthinkingaboutcontext,RussandOddenleverageBronfenbrenner'secological systems theory [57] to describe the environment that the research study takes place. Ecological systems theory envisions individuals existing in a set of systems that describe the contexts and interactionsthatmayinformthedevelopmentoftheindividual. Forexample,themicrosystemisthe 3individual'sdirectenvironment,themesosystemisconnectionsacrossmicrosystems,theexosystem istheindirectenvironmentoftheindividualandthemacrosystemisthesocietalnormsandcultural values relevant to the individual. Given that PER has traditionally focused on undergraduate students in introductory physics classes, its context is typically the microsystem [2]. Thinking in terms of Nair's mapping of ecological systems theory onto a physics classroom [58], graduate admissions can be thought of as an exosystem and macrosystem phenomenon. Students do not actively participate in the process but are clearly aected by it (exosystem) and cultural norms about who can do sciences as well as beliefs about what is required to do physics (e.g. innate brilliance [59]) might aect faculty's decisions (macrosystem). Finally,thisthesiscontributestoPERbyusingdataanalysistechniquesthathaveonlyrecently been incorporated into PER studies such as machine learning, Tomek Links, and simulation. Traditionally, PER has been divided into qualitative research and quantitative research [2]. Ding [12] then subdivided quantitative research into three genres: measurement, controlled exploration of relations, and data mining, which is the predominant genre in this thesis. As Romero and Ventura [60] and Cope and Kalantzis [61] note, data mining relies on data that has already been collected and hence, the research quesions that can be addressed may be limited. However, instead of thinking of genres of quantitative research, we can think in terms of the specicmethodsused. RussandOddenclaimthatbasicstatisticaltechniquesandlarge- #summary and frequency analyses are the most common in PER with network analysis and methods for large data set analysis becoming increasingly common [2]. To this, I will add the umbrella term of modelinginwhich researcherstrytodevelopsomequantitative representationofthephenomenon of interest. Modelingcanthenbefurtherbrokendownintotwobroadcategories: explanatorymodelingand predictive modeling [62]. While explanatory modeling is a traditional PER method (Theobald et al.[63]providesanoverviewofthesemethods),predictivemodelsareanemergingarea,oftenusing machine learning techniques. Machine learning, a method used throughout this thesis, especially is an emerging area of PER, with most studies utilizing it having been published in the last ve 4years [64-69]. Finally, as a result of modeling methodologies, a new genre of quantitative PER has started to emerge: simulation. In this genre, researchers create articial data to study the methods of PER themselves. Simulation studies are still rare in PER, with only a few such studies published to date [70-72]. This thesis increases that number by one. 1.3 How this thesis contributes to computational mathematics, science, and engineering As computational mathematics, science, and engineering is unique to Michigan State University, a research-based framework to describe the types of work conducted under the umbrella of the department does not exist. Instead, I will use the \"triple junction\" of computation on which it was formed [73]. Thedepartmentdenes\"triplejunction\"ofcomputationasalgorithmdevelopmentandanalysis, high performance computing, and applications to scientic and engineering modeling and data science. This thesis focuses on the nal of those. Data science techniques such as machine learning, feature engineering, and simulation have seen limited use in physics education research. This thesis then brings data science tools into a new eld, adding additional tools to the physics education researcher's tool kit. In addition, the interdisciplinarynatureofthisthesisofcombiningcomputationaltechniquestoanswereducational questions aligns with the broader goals of the department. 1.4 Summaries of the remaining chapters This thesis considers the past, present, and future of graduate admissions. Chapters 2 and 3 focus on the past and present, Chapters 4 and 5 focus on the present and future, and Chapter 6 extends across dimensions of time, seeking to understand the limitations of models used in the previous chapters and to be used in future analyses. In Chapter 2, I analyze 4 years of admissions records to Michigan State University's graduate physics program using a machine learning method known as random forest. I nd that consistent 5with surveys of admissions committees and observations of committees, quantitative parts of the application such as GRE scores and undergraduate GPA hold the most weight in the process. In fact, knowing only the applicant's undergraduate GPA, physics GRE score, and quantitative GRE score was sucient to predict with 75% accuracy whether an applicant would be admitted. InChapter3,IthenexplorethephysicsGREinmoredepthandshowthatacommonargument for keeping the physics GRE in the admissions process despite its documented issues, that it helps applicants who might be otherwise missed \"stand out,\" is not supported by evidence. To reach this conclusion, I analyzed admissions records from ve universities with both large-N frequency analysisandmediationandmoderationmodels. WhetherIdenedapplicantwhomightbemissed as having a low GPA, graduating from a less selective undergraduate school, or graduating from a smaller undergraduate program did not aect the conclusion. Given that the traditional methods of admissions have not resulted in substantial changes in the demographics of physics at the graduate level and those methods contain inequities in and of themselves, graduates admissions do not need to be revised, but rather rethought. Chapters 4 and 5 provide one such approach: rubric based holistic admissions. In chapter 4, I introduce rubric-based holistic admissions, dening them as an approach to admissions where reviewers consider a broad range of applicant characteristics such as academic achievement,researchexperience,twiththeprogram,testscores,andnoncognitivecompetencies, and rate applicants on those categories according to a pre-determined scoring rubric. I then consider the department of physics at Michigan State University's revised admissions process as an example in practice. I compared the distribution of faculty ratings by admission status, sex, and undergraduate background. The results indicate that the rubric does not show any unexpected inequities based on the applicant's sex or undergraduate background. It did however detectsystematicissuessuchastestscoredierencesanddieringservice-workexpectationsbased on sex. Inchapter5,Iconsiderwhetherourrubric-basedholisticadmissionsisactuallyarethinkingof graduate admissions or just a revision. That is, does switching from the traditional admissions to 6rubric-based admissions fundamentally change the process. I again use the random forest method to analyze the admissions data after the implementation of the rubric as well as introduce a new techniquetoPER,TomekLinks. Irstcomparethetwoadmissionsprocessesusingthesamedata extracted from applications and then consider what additional insights might exist by looking at the rubric-ratings data. Across four sets of analysis, the results suggest that rubric-based holistic admissions is a rethinking of graduate admissions, though additional work is needed to provide greater condence in the results. Finally, in chapter 6, I consider the modeling techniques I've used in the previous chapters, in additiontoothers,andhowtheyperformundervariousdatadistributionsencounteredinPERandthe previouschapters. Acrossthetechniques,Indthatthemoreabinaryvariableisimbalanced(e.g. 50/50 split vs 80/20), the less likely a technique is to nd that variable predictive or explanative of anoutcome. IthenshowthattheeectisalsofoundinrealPERdatabyfocusingontheadmissions processes at 3 institutions. 1.5 Key conclusions in this thesis The results of the chapters then suggest three overarching conclusions. First,thetraditionalgraduateadmissionsprocessinphysicsismetricsheavyanddominatedby thephysicsGRE,whosevalueintheadmissionsprocessshouldbequestioned. (Chapters2and3) Second, rubric-based admissions might oer a possible path forward in terms of making the process more equitable. While the components of the rubric seem to be equitable, we were not able to produce denitive evidence that rubric-based admissions suciently departed from the traditional admissions structure. However, the current results are promising. (Chapters 4 and 5) Finally, modeling techniques in PER have biases and may aect what features are counted as statistically signicant or predictive. If these modeling techniques are going to be used more broadly to make educational policy decisions (e.g. in graduate admissions), we need to really understand the models, including how they work and their limitations and caveats. (Chapter 6) 71.6 Key recommendations as a result of this thesis As a result of work done in this thesis, I propose six key recommendations, three targeted toward departments and graduate admissions committees and three targeted toward researchers. 1.6.1 Recommendations for departments and graduate admissions committees First, if the physics GRE is being used to identity applicants who might otherwise be missed, I recommend against that. My study in chapter 3 did not nd evidence that the physics GRE allows that to happen in practice. Second, departments should rethink their graduate admissions process in terms of what ap- plicants are evaluated on, how those are evaluated, and who does the evaluating. Rubric-based admissions, introduced in chapter 4, seems to oer one way to do so. However, implementing rubric-based holistic admissions requires the department to take an active role in rethinking their process. Departments must decide how to address those three points as answers will depend on specics of the department. Finally,departmentsshouldengageinregularself-studyoftheirprocessesandsharetheresults so that the physics community has an idea of what works and what does not work. Currently, data about admissions practices is either reported in aggregate or individually for a limited number of programs. Greaterreportingfrommanyprogramswillallowforabetterpictureofhowadmissions processes are conducted and how they might be made more equitable. 1.6.2 Recommendations for researchers First, researchers should be transparent about the data that goes into their models and how the distribution of those features may aect the results. For researchers and practitioners to evaluate the conclusions presented in papers, they need to understand the data itself. My work in chapter 6 showed the split of a binary feature can aect whether an algorithm nds it statistically signicant or predictive of an outcome. As a result, it is possible that the literature contains false negatives, 8where potentially important variables were missed. Second,researchersshouldconsidermoremoderntechniquesforanalyzingdatasuchasmachine learning and penalized regression. Machine learning techniques are becoming more common in PER but are still a niched area. Penalized regression techniques have seen limited use in PER but appear to handle data as well as if not better than standard logistic regression. In addition, researchers should follow the recommendation of Aiken et al. and compare dierent models to produce the best tting model [74]. Finally, researchers should conduct more simulation studies to examine the methods used in PER. Data in PER is often a mix of binary, categorical, and continuous features and hence, algorithmsdevelopedineldsthatanalyzeprimarilycontinuousdatamaynotperformasexpected. Simulationstudieswouldbeabletoverifyifthisisthecaseandifresearchersshouldbeconcerned. 1.7 Questions remaining unanswered Whilethisthesisextendstheeld'sunderstandingofgraduateadmissionsinphysics,manyquestions are unanswered, specically around rubric-based admissions and equity in graduate admissions, providing an avenue for future work. First,inordertoreachamoredenitiveconclusionastowhetherourdepartment'sadmissions process, future work could consider alternative approaches to analyzing the data such as mixed methods. Onthequantitativeside,thesealternativeapproachescouldbemoretraditionalmethods in PER like logistic regression or clustering-type methods. The latter may be able to tease out whether there are dierent \"types\" of successful applicants and possibly provide evidence as to whether the process became more holistic. On the qualitative side, future work could include interviewing faculty on the admissions committeeorobservingdeliberationsinrealtime. Suchdatawouldprovidegreaterinsightintothe process, especially regarding individual applicants. Quantitative methods try to summarize and simplify the data, causing us to potentially miss data-rich discussions of applicants. Qualitative methodsmightallowustoseethesecases,especiallydiscussionsaroundborderlineapplicantswho 9might expose what faculty are really valuing in an applicant. Second, in order to understand how the results may generalize, future work should extend the analysis done in this thesis to other physics graduate programs. Additionally, future work should consider programs in dierent geographical areas with dierent applicant pools and of dierent research intensities. Michigan State University is a predominantly white institution with a highly- rankedsubdisciplineandhence,theapplicantpopulationmightnotberepresentativeofthebroader population who applies to physics graduate school. Third, future work should examine other aspects of the graduate admissions process such as who is invited and able to apply to graduate school. Prior work has examined barriers current graduate students experienced when they applied [49]. However, studies such as these ignore studentswhowantedtoapplybutforonereasonoranother,didnot. Therefore,futureworkshould explore the barriers these students face in applying and how departments might address them. Thesemightincludesupportsstudentsneedinapplying,howstudentsndoutaboutprograms,and how departments advertise their graduate programs. Fourth,futureworkshouldcontinuealongthepathoutlinedaboveandconsiderundergraduate physics students more broadly in graduate admissions. The evaluation of applications is the nal stepinthegraduateadmissionsprocessthatcouldbearguedtobeginassoonasastudentdeclares a physics major. Between those two steps, potential applicants must decide whether they are interested in attending graduate school, research potential programs and advisors, complete an application, and submit the application. Even after departments have evaluated the applications andmadeoers,studentsmustdecidewhichprogramtoattendorwhattodoiftheyarenotaccepted to any. All of these are ripe for future study. Finally,totrulymakeanimpactondiversityandequityinphysicslonger-term,futureworkneeds toextendbeyondjusttheadmissionsprocessandconsidergraduateschoolasawhole. Departments needtoaddressdiversitybyexaminingwhoisencouragedandinvitedtoapplytograduateschool, equitybyconsideringhowtheyevaluateapplicantsapplyingtotheirprogramandcurrentstudents in their program and what they are doing to retain the students they did admit, and inclusion by 10intentionally addressing the climate in their department, being transparent with decisions being made, and creating support structures for students from underrepresented backgrounds. Simply making the admissions process more equitable and admitting more diverse students will not make animpactifcorrespondingeortsarenotmadetoretainsuchstudentsandpreventthemfrombeing pushed out. Possible areas of future work include studying qualifying and comprehensive exams, student-advisor relationships, mental health, and departmental support for students. Only when diverse students are not only actively welcomed to the academy but also actively retained will real change happen. 11CHAPTER 2 IN THE BEGINNING: USING MACHINE LEARNING TO UNDERSTAND PHYSICS GRADUATE SCHOOL ADMISSIONS The following chapter is adapted and expanded from its published version in the 2019 Physics Education Research Conference [75]. The published version includes Marcos D. Caballero as the secondauthor. FollowingtheContributorRolesTaxonomy(CRediT)[76],myrolesforthisproject include conceptualization, formal analysis, methodology, software, validation, visualization, and writing the original draft. 2.1 Introduction Despite other science, technology, engineering, and mathematics (STEM) elds becoming more diverse over the past few decades, physics has lagged behind with only 20% of bachelor's degrees awardedtowomenandonly11%awardedtoracialminorities[77]. Thesenumbersdonotimprove when considering graduate degrees where 20% of doctoral degrees are granted to women and 7% are granted to racial minorities [77]. While this underrepresentation has both enrollment and retentioncauses,thischapterwillfocusonthefactorsthatmayaectenrollmentinphysicsgraduate programs. When considering enrollment in physics PhD programs, prior work has found that minority students in physics are less likely to apply to programs if they feel that they will not be admitted based on low GPA or GRE scores, or lack of research experience [49]. Further, given that many graduate programs have application fees, nancial concerns might prevent students from applying to graduate programs that they believe they will not be admitted to. Therefore, it is important to understand what matters when applying to physics graduate programs while acknowledging that many factors that are not easily quantiable matter. Previous research into graduate admissions in physics has tended to take a broad approach, characterizingthegraduateadmissionsprocessacrosstheUnitedStates,bothformaster'sandPhD 12programs [47,50] or focusing on a specic subset of universities such as elite universities [46]. These studies nd that faculty consider numerical measures such as undergraduate GPA and GRE scores most important in the admissions process and have been conducted by either observing the admissions process or by surveying faculty about what they believe to be most important in the admissions process. More recent work in physics graduate admissions has explored applicant perceptions of the various components of the application [51]. Missing in this analysis is an investigationoftheactualapplicationsofprospectivephysicsgraduatestudents. Toourknowledge, there has only been one such study [54]. Given that applications to graduate programs consist of numerical data such as GPA and GRE scores, categorical data such as gender, race, and ethnicity, and open-ended data such as letters of recommendation and personal statements, graduate admissions is an ideal target for machine learning. Indeed, machine learning approaches to understanding graduate admissions have been employed in computer science to study self-reported admissions data [78] and to streamline the review process [79]. Machine learning methods have also been employed more broadly in higher education admissions to predict which admitted students will accept an oer to attend a small liberal arts school [80,81] and to predict which students are likely to be admitted and to complete their MBA [82]. Thegoalofthisworkistofurtherthestudyofgraduateadmissionsinphysicsbyanalyzingthe applicationsusingamachinelearningapproach. Specically,weaskwhatfeaturesofanapplication to this physics graduate program are predictive of admission. Unlike other studies in physics graduate admissions, this work represents a case study of a singleinstitutionratherthanabroadlookatthegraduateadmissionslandscape. However,because physics is regarded as a high consensus discipline, that is, there is large agreement about what counts as legitimate admissions practices [83], we expect our results can generalize to similar doctoral programs. 132.2 Methods 2.2.1 Data The data used in this study comes from the admissions records of 512 domestic applicants to the physicsandastronomygraduateprogramatMichiganStateUniversitybetween2013and2016and wouldhaveenrolledbetweenfall2014andfall2017. Domesticandinternationalapplicantsdonot undergo the same review process and hence we only analyze applications from domestic students. Here,domesticstudentisdenedtobeaU.S.citizenorpermanentresident. Theadmissionsprocess is unique at this university in that the applications are not only reviewed by a central committee butalsomembersofthesubdisciplineinwhichthestudentexpressesinterest. Thedataincludethe applicant'sundergraduateinstitutionandgradepointaverage(GPA),theirgeneralandphysicsGRE scores, and their physics subdiscipline of interest. Per a ballot initiative in the state of Michigan, MichiganStateUniversityandtheotherMichiganpublicuniversitiesareexplicitlyprohibitedfrom discriminating against or granting preferential treatment to individuals based on race, sex, color, ethnicity,ornationaloriginineducation[84]. Tocomplywiththislaw,ouruniversity'sadmissions system collects limited demographic data and our department chose not to record the information that was available when evaluating applicants. As such, demographics are not available to us. Overall, 48% of the domestic applicants were oered admission into the program. 2.2.2 Describing Undergraduate Institutions Because the name of the undergraduate institution in itself does not provide useful information to analgorithm,wecreatednewfactorstodescribecharacteristicsoftheinstitutions. Todescribethe overallinstitution,weclassiedeachinstitutionaspublicorprivate,whetheritisaminorityserving institution (MSI), the region of the country it is located in (such as Northeast, Southwest, etc.), and the Barron's selectivity of the institution, which describes how selective the undergraduate program is. We assume that selectivity serves as a proxy for prestige. Classications for the rst three categories were taken from the most recent Carnegie Rankings [85] while the Barron's 14Table 2.1: Variables used in our model and their scale of measurement Factor Measurement Scale Undergraduate GPA Continuous Verbal GRE score Continuous Quantitative GRE score Continuous Written GRE score Continuous Physics GRE score Continuous Proposed research area Categorical Application year Categorical Barron's selectivity Categorical Region of applicant's undergraduate institution Categorical Type of physics program at applicant's undergraduate institution Categorical Size of undergraduate physics program at applicant's undergraduate institutionCategorical Size of doctoral physics program at applicant's undergraduate institutionCategorical Applicant attended a minority serving institution Binary Public or Private Binary Output variable: admitted status Binary classicationcamefromBarron's ProlesofAmericanColleges . Becausetheoverallreputationof the applicant's undergraduate university might not describe the physics program at that university, wealsoincludedfactorsrelatedtothephysicsprogramsuchasthehighestphysicsdegreeoeredat the university and the size of the undergraduate program and PhD program if applicable. The size of the undergraduate and PhD programs were determined by the median number of graduates of the program between the 2012-2013 and 2015-2016 academic years (i.e. the years that applicants appliedtotheprogram). Theprogramswerethenclassiedassmall,medium-small,medium-large, or large based on which quartile they fell into. We used the Roster of Physics Departments with Enrollment and Degree Data to collect this data [86-89]. All factors appearing in our model are shown in Table 2.1 and include the scale of measurement. 2.2.3 Justifying our choices of institutional factors Prior work has documented university pedigree is often considered in the application process because institutional quality is assumed to be a proxy for student quality [46,90]. Here, we 15measureinstitutionalqualitybyBarron'sselectivityandpublicorprivatestatus,withtheassumption that physics faculty view private universities as more prestigious than public universities. We includeregionoftheapplicant'sundergraduateuniversitytoaccountforthefactthattheinstitution being studied is a public university and might therefore show a preference for students from the surrounding region. Prior work has also found faculty exhibit a tendency to admit students like themselves, though it is more common among academics who graduated from elite institutions [46]. Therefore, it is notunreasonabletoexpectthatfacultymayprefertoadmitstudentswhofollowedsimilarpathsas they did, meaning students from large, doctoral institutions might be more likely to be admitted than students from smaller institutions. Additionally, we use the size of the undergraduate and PhD programs as proxies for the perceived prestige of the physics department, assuming a more prestigious physics department attracts more students and hence graduates more students. 2.2.4 Random Forest Model To analyze our data, we used the conditional inference forest algorithm, a variant of the random forestalgorithm[91]showntobelessbiasedwhenthedataincludesbothcontinuousandcategorical variables [92] such as those used in our model (see Table 2.1). Random forest models in general are ensembles of individual decision trees, which use binary splits of the input features in order to makeaprediction. Thepredictionsarethenaveragedovertheindividualtreestoobtaintheoverall prediction of the random forest. While there are multiple metrics used to assess random forest and other machine learning models, two of the most common are the accuracy and the area under the curve (AUC). The accuracy is simply the proportion of correct predictions made by the model. To ensure that the accuracy isn't inflated by overtraining, only a fraction of the available data is used to construct the model while the rest is used to test the predictive power. It is this remaining data that is used to calculate the accuracy of the model. The AUC is dened as the area beneath the receiver operator curve of the model, which 16visualizes the false positive rate against the true positive rate and varies between 0.5 and 1, with values greater than 0.7 signifying an acceptable model [93]. The area describes the proportion of positive cases that are ranked above negative cases in the data set by the model. For example, for ourdata,theAUCwouldrepresenttheproportionofallrandompairsofadmittedandnot-admitted applicants in which the admitted applicant is classied as admitted and the not-admitted applicant is classied as not-admitted. In addition to making predictions, the random forest algorithm can determine the importance of each feature to the model, referred to as the feature importance. For this analysis, we use two importancemeasures. FirstweusedtheAUCpermutation featureimportance[94]asitisclaimed tobelessbiasedthantheaccuracybasedpermutationimportancewheninputfeaturesdierinscale (asdoourfactorslistedinTable2.1)andwhenthepredictedvariableisnotsplitevenlybetweenthe two outcomes. Under this approach, each feature is randomly permuted and then passed through the model to make a prediction. The AUC is then recorded and the dierence between this value andtheoriginalAUCiscomputed. Aspermutingafeaturewithmorepredictiveinformationshould resultisaworsemodelthanpermutingafeaturewithlesspredictiveinformation,alargerdierence betweentheoriginalAUCandtheAUCwithapermutedfeaturesuggeststhatthatfeaturecontained more predictive information. These dierences can then be used to create a relative ordering of features. However, if the features are correlated, it is possible that the orderings may be biased or that permutations of one feature might result in unrealistic combinations of features and hence would cause the model to extrapolate performance [95]. For example, if all students who earned perfect scoresonthephysicsGREalsohadhighGPAs,permutingGPAcouldcausetheretobecaseswhere aperfectphysicsGREscoregoeswithalowGPA,whichwouldbeoutsideoftheregionlearnedby themodel. Topreventthat,aconditionalimportancemeasurehasbeenproposedinwhichfeatures are permuted within a subset of similar cases [96]. Because of the correlations between various sections of the GRE, we also used this conditional approach to compute feature importances. Feature importances are derived from the data and hence, are not assumed to follow any 17statisticaldistribution. Therefore,thereisnosimplewaytoapplytheideaofstatisticalsignicance to feature importances, though Chapter 6 provides some suggestions. We instead applied the recursivebackwardeliminationtechniquedescribedinD\u00edaz-UriarteandAlvarezdeAndr\u00e9s[97]to determinewhichfeaturesarepredictiveofadmissionandwhicharenot. Whenusingthistechnique, the features are ordered according to their importance. A model is then built using all the features andtheaccuracyiscomputed. Asetfractionofthefeatureswiththesmallestimportancesarethen removed and a new model is built and the accuracy computed. This process continues until only 2 features are left. The model with the fewest number of features while maintaining an accuracy within a standard error of the highest accuracy across all models built in this process is then the selectedmodel. Wewillrefertothefeaturesusedinthisselectedmodelasthe meaningful features and interpret them as the features that are predictive of the outcome. For more information about random forest models, biases, and feature importance measures, model 30 times, randomly selecting 70% of our data for training each time, and averaged thefeatureimportancesoverrunssothattheresultingdistributionofindividualfeatureimportances wouldbeapproximatelynormal. Astheconditionalinferenceforestalgorithmhasroutinesbuiltin tohandlemissingdata[101],applicantswithmissinginformationwerenotremovedfromthedata set. However, the conditional importance approach requires there to be no missing values so we usedtheMICEalgorithm[102]tollimputethemissingdatainthatcase,followingNissenetal.'s recommendation for PER [71]. The imputation results were pooled using Rubin's Rules [103]. In addition, to determine if our model was dependent on our choice of hyperparameters, we alsovariedthefractionofdatatotrainthemodel,thenumberoftreesintheforest,andthenumber of randomly selected features to use to build each tree. We set the training fraction to be either 0.5, 0.6, 0.7, 0.8, or 0.9, the number of trees in the forest to be 50, 100, 500, 1000, or 5000, and 18the number of features used for each tree to be 1,p?,?\u009d3,?\u009d2, or?for a total of 125 possible combinations(124newandtheoriginalmodel). ThesechoicesarebasedondingsinSvetniket al. [100]: namely that the error rates level o once the number of trees is on the order of 102and theirchoicesofthenumberoffeaturesineachtree. Inaddition,increasingthetrainingfractionmay improve performance as there is more data for the model to learn from. For each combination, we repeatedtheprocedureinthepreviousparagraph. Duetothecomputationalcostoftheconditional permutation approach, we only calculated the AUC-permutation importance. To determine if the changing the hyperparameters aected our models, we computed the minimum,median,andmaximumvalueofeachmetricoverthe125hyperparametercombinations andrelativeorderingofthefeaturesineachmodel. Wechosetheminimum,median,andmaximum instead of the mean and standard error because 1) we are looking across dierent models rather than getting repeated measurements of the same things so we cannot assume the results will be normally distributed and 2) we are interested in the best and worst performance achieved under hyperparameter tuning to get a sense of the possible values we can achieve which wouldn't be possible using the mean and standard error. If our model is largely unaected by the choice of hyperparameters,wewouldexpectthemetricstoshowminimalvariationandtherelativeordering of the features to be largely unchanged. 2.3 Results Acrossthe30runs,theaverageaccuracyofourmodelpredictingontheheld-outdatawas 75\u00956%\u0006 0\u00956%,theaveragetrainingAUCwas 0\u0095849\u00060\u0095002,andtheaveragetestingAUCwas 0\u0095756\u0006\u0095006. As our model's accuracy is signicantly higher than the null accuracy of 52\u00957%, the percent of students who were not accepted, and our testing AUC is above 0.7, our model can be considered an acceptable model of the data. The feature importances averaged over the 30 runs are shown in Fig. 2.1. We nd numerical factors such as the applicant's score on the physics GRE, the applicant's score on the quantitative GRE, the applicant's undergraduate GPA, the applicant's verbal GRE score, and their proposed 19Attended a public institutionAttended a MSIHighest physics degree offeredRegion over 30 trials. Physics GRE score, Quantitative GREscore,andundergraduateGPA,appearinginorange,werethefactorsfoundtobemeaningful and hence predictive of being admitted. research area to be more important in the application process than any factor describing the applicant's undergraduate institution. Using recursive backward elimination to determine the meaningful factors, we nd the applicant's physics GRE score, quantitative GRE score, and their undergraduate GPA to be the only meaningful factors. To verify that the applicant's physics GRE score, quantitative GRE score, and undergraduate GPA were indeed the only meaningful factors, we then reran our random forest model 30 times usingonlythesethreefactorsasthepredictors. Ouraveragetestingaccuracywasthen 75\u00954%\u00060\u00956% andourtestingaverageareaunderthecurvewas 0\u0095754\u00060\u0095006,whicharenotstatisticallydierent from the values we found using all fourteen factors shown in table 2.1. When we instead used MICE and the conditional importances, and the metrics were slightly 20Attended a public institutionHighest physics degree offeredAttended a MSISize Averaged conditional feature importances over 30 trials. Physics GRE score and undergraduate GPA, appearing in orange, were the factors found to be meaningful and hence predictive of being admitted when adjusting for correlations among the features. higher,likelybecausetheimputingthemissingvaluesprovidedmoredataforthealgorithmtolearn from. Specically,thetestingaccuracywas 77\u00951%\u00060\u00951%andthetestingAUCwas 0\u0095770\u00060\u0095001. The conditional feature importances are shown in Fig. 2.2. Compared to Fig. 2.1, we notice that the verbal and quantitative GRE scores are ranked lower than they were when we did not take correlations into account and proposed research area and year of applying are ranked higher than when we did not take correlations into account. The physics GRE and GPA are still rankly highly however, even after taking correlation into account. Performing the recursive backward elimination, we nd that physics GRE score and GPA are meaningfulfeaturesandquantitativeGREscorenolongeris. Usingonlythesetwofeaturestocreate aconditionalinferenceforestontheimputeddata,wendthatthetestingaccuracyis 75\u00957%\u00060\u00957% 212.02.53.03.54.0 400 500 600 prediction of the model had an applicant had that score. and the testing AUC is 0\u0095757\u00060\u0095007, which are consistent with the full model. Asthereareonlytwomeaningfulfeatures,wecanplotthefeaturesandseeiftheredoesappear tobeaseparationbetweenadmittedandnotadmittedapplicants. Todosowegeneratedallpossible pairs of undergraduate GPA and physics GRE scores and ran them through our model to nd the predictedadmissionsdecision. TheresultisshowninFig2.3. Weseethattheredoesappeartobea boundarybetweenadmittedandnon-admittedstudentsaroundaphysicsGREscoreof700,which dropstoward650forapplicantswithGPAsabove3.5,providingfurtherevidencethatphysicsGRE score and GPA are predictive of admission at this program. Whenwetestthevarioushyperparametercombinations,wendsimilarresults. Lookingatthe metrics (Table 2.2), we see that the testing accuracy varies by 3.3 percentage points between the minimum and maximum values and the testing AUC varies by 0\u0095034between the minimum and 22Table2.2: Minimum,median,andmaximumvaluesofthemetricsobtainedoverthe125hyperpa- rameter metric 1234567891011121314 RankFeature 0.000.250.500.751.00Fraction of Trials Figure 2.4: Proportion of the 125 hyperparameter combinations in which each feature had a given rank. Noticethatthereisablockoffeaturesthatrangebetween1and5andablockoffeaturesthat rank between 7 and 14. maximumvalues. Asthevariationislimitedandthesemetricsarestillwithintheacceptablerange, the results suggest that our choice of hyperparameters has limited impact on the metrics. When we look at the ranks of the features used each hyperparameter combination, we also see limitedvariation. InFig. 2.4,wenoticetheplotismostlydiagonalandthepresenceoftwoblocks. First, we see that physics GRE score, GPA, quantitative and verbal GRE scores, and proposed research area are always the top ve features, regardless of the hyperparameters. Second, we see thattheinstitutionalfeaturesneverrankabovea7,meaningthatnocombinationofhyperparameters can create a model where these features are predictive of admission. Lookingattherstblock,wenoticethatphysicsGREisalwaysthetoprankedfeaturefollowedby eitherGPAorquantitativeGREscore,withGPAbeingthemorecommonselection. Furthermore, 23GPA never ranks lower than third while the quantitative GRE score ranks between second and fourth. For certain choices of hyperparameters, the applicant's proposed area of research ranks higher than the quantitative GRE score. 2.4 Discussion Perhapsunsurprisingly,wendnumericalmeasuresarethemostimportantfactorsfordetermining whether a domestic applicant will be accepted into this physics graduate program, consistent with ndingsthatgraduateprogramswithalargenumberofapplicantsusenumericalmeasuresasarst pass to evaluate applicants [46,47]. Looking across our analyses, we nd physics GRE score and undergraduate GPA are consistently found to be predictive of admission while quantitative GRE score is sometimes found to be predictive based on the choice of hyperparameters. However, once wetakecorrelationsamongthefeaturesintoaccount,thequantitativeGREscoreisnolongerfound to be predictive. While we nd no evidence of a minimum physics GRE score, we do nd evidence of a \"rough cuto\" as described in Potvin et al. around 700 [47]. Nevertheless, some students who scored signicantlyabovethisthresholdwerenotadmitted. Whilewedonotknowthereasonswhythese students were not admitted, Posselt noted that faculty might not admit superior applicants if they do not believe the applicant will actually enroll in their program [46]. Overall,ourndingsofthemeaningfulfactorsforadmissiontoaphysicsgraduateprogramare consistentwithPotvinetal.'sndingsobtainedbysurveyingphysicsgraduateadmissionsdirectors. Notably,wealsondthatthephysicsGREscore,undergraduateGPA,quantitativeGREscore,and proposed research area are more important than other factors while the undergraduate institution, GRE written score, and proximity/familiarity are less important factors. WhiletheverbalGREscorewasnotfoundtobeameaningfulfeature,theprogramstudiedhere appearstoplacemoreemphasisonitthantheaverageprogram. Thismaybebecauseourstudyonly looked at domestic students while Potvin et al.'s looked at all applicants. Because international students also take the TOEFL while domestic students do not and admissions directors ranked the 24TOEFL as more important the verbal GRE, the TOEFL may take the place of the verbal GRE and hence lower the perceived value of the verbal GRE relative to other factors. Despite prior work suggesting institutional characteristics play an important role in graduate admissions, we did not nd institutional or departmental characteristics to be meaningful to our model. Our result could be due to dierences in methodology or due to institutional eects being influential but not dominant factors [104]. Indeed, Posselt suggests institutional factors might be used to dierentiate applicants with similar GPAs and GRE scores [46]. Therefore, we might not have foundinstitutional factors to bemeaningful because they areused when primaryfactors such as GPA and physics GRE scores do not suciently separate applicants. WhilewedidnothaveaccesstoothercriterionincludedinthePotvinetal. suchasapplication essays, research experiences, and recommendation letters, we were still able to create a model thatcorrectlypredictedwhetheranapplicantwouldbeadmittedwithapproximately75%accuracy based solely o the applicant's undergraduate GPA and physics GRE score (and highly higher if we also included the quantitative GRE score). While undergraduate GPA is a signicant predictor of completing a physics PhD, physics GRE is not, as those scoring near the top of the physics GRE only have a 7% higher probability of completing their PhD than those scoring near the bottom [52]. As the GRE is not associated with completing a doctoral degree and is known to favor persons from majoritized groups in science [52,105], the outsized role of the GRE in the admissions process should be questioned. Indeed, the American Association of Physics Teachers andAmericanAstronomicalSocietyhavereleasedrecommendationsagainstusingthephysicsGRE in graduate admissions [106,107]. 2.5 Limitations There are a few limitations to our study. First, the data we used to make our model was not all the data that would be available to a faculty member evaluating an application. In addition, our model did not contain demographic information about the applicants that could also impact the results given the barriers women and people of color face in physics. Therefore, it is possible 25that meaningful features other than GPA and physics GRE score could lie in the data that was unavailable to us. Second,thisstudywasdoneataprimarilywhiteinstitution(PWI).WhileKanimandCidnote that having a relatively homogeneous research sample can be valuable for reducing variability, especially in early studies, they also note that exploring the eects of variability can lead to new results and a greater understanding of the results [13]. Thus, while our result might generalize to many physics graduate programs, it might also hide important dierences in features predic- tive of admission for applicants of dierent demographics groups and institutions with dierent demographics than our own. 2.6 Future Work and Conclusion Our work adds to the broader literature about graduate admissions and the process by which applicants are judged. Because minoritized students might not apply to graduate programs if they donotthinktheywillbeaccepted,elucidatingthefactorsthatdeterminewhetheranapplicantwill be accepted is crucial. Simply increasing the number of applicants from currently and historically underrepresented groups in physics will not increase their representation unless corresponding eorts are made to admit these students. While our result that test scores and GPA are the most predictivepartsofanapplicationintermsofadmissionalignswithpriorwork,theseresultsrepresent only one institution and might not be representative of all United States physics PhD programs. Given the unique structure of the admissions process at this university, graduate programs with a more traditional admissions process might assign dierent weights to the various parts of an application. Therefore, future work should investigate what features of the applicant drives the admissions process at other institutions. Furthermore, our university has recently moved to a rubric-based admissions format, designed totakeintoaccountnon-cognitivecompetenciesandprogramtinadditiontothemoretraditional admissions criteria such as GPA and GRE scores. Our future work will examine how including thesenewcriteriamaychangethefactorsthataremostpredictiveofanapplicantbeingadmittedto 26the program. The results of such analyses are presented in Chapters 4 and 5. 27CHAPTER 3 FURTHER EVIDENCE AGAINST THE PHYSICS GRE: IT DOES NOT HELP APPLICANTS \"STAND OUT\" ThefollowingchapterwaspublishedinPhysicalReviewPhysicsEducationResearchin2021[108]. The published version includes Marcos D. Caballero as second author. Following the Contributor RolesTaxonomy(CRediT)[76],myrolesforthisprojectincludeconceptualization,formalanalysis, methodology, software, validation, visualization, and writing the original draft. 3.1 Introduction Whileapplyingtograduateprogramsrequiresmanycomponents,perhapsnoneisasscrutinizedas theGraduateRecordsExam(GRE),andinphysics,thephysicsGRE.Indeed,researchintograduate admissionsinphysicssuggeststhatthephysicsGREisoneofthemostimportantcomponentsofthe applications for determining which applicants will be admitted, based on both student and faculty perspectives[47,51]andanalysisoftheadmissionsprocess[46,75]. Despiteitsprominenceinthe admissions process, the physics GRE is known to be biased against women and people of color in physics[105],resultinginloweraveragescorescomparedtowhiteandAsianmales. Atleastonein three programs use a cuto score [47], with 700 being a common choice [52], meaning applicants fromgroupsalreadyunderrepresentedinphysicsgraduateprogramscanbefurthermarginalizedas theyarelesslikelytoachievethesescores. Thisisinadditiontotheobservationthatmanyphysics students of color already see the GRE as a barrier to applying to graduate school [49,55,109]. Further, the physics GRE might not even be useful for determining which applicants will be successful in graduate school. For example, Miller et al. suggest that the physics GRE is not useful for predicting which applicants will earn their PhDs [52]. Additionally, Levesque et al. argue that using the common 50th percentile cuto score for the physics GRE would have caused admissions committees to reject nearly 30% of students who would later receive a national prize postdoctoral fellowship, which can be viewed as a proxy for research excellence [110]. Yet 28despite evidence suggesting the physics GRE does not predict these typical ways of measuring \"success\"ingraduateschoolandcallsfromtheAmericanAstronomicalSocietyandtheAmerican Association of Physics Teachers to eliminate the physics GRE from admissions [106,107], most physics graduate programs still require applicants to submit their physics GRE scores. Currently, nearly 90% of physics and astronomy graduate programs still accept the physics GRE, with over halfrequiringorrecommendingsubmittingascore[106]. OfthosethatdonotacceptphysicsGRE scoresfromapplicants,alloftheprogramsaresolelyastronomygraduateprogramsorjointphysics and astronomy graduate programs. While it is uncertain where removing the physics GRE aects anymeasureofgraduateschoolsuccess(e.g. completionrate),initialworkbyLopezsuggeststhat removing the physics GRE does increase the diversity of applicants [111]. Given these documented issues with the physics GRE, why do departments continue to use it? First, given that many programs are seeing a larger number of applicants, the physics GRE providesaquickwaytoltertheapplicationsdowntoamorereasonablenumberforfacultyreview. Unlike in undergraduate admissions, graduate admissions tend to be decentralized and done at the departmental level by a faculty committee. Hence, faculty are asked to review applications in addition to their regular teaching and research duties and thus, might not have the time to read the letters of recommendation and applicant essays for every applicant. Second, some faculty view GRE scores as measures of innate intelligence [46,48] or ability to becomeaPhD-levelscientist[105]. Afterall,theyandotherfacultylikelyhadhighGREscoresin order to be admitted to graduate school, and may exhibit a survivorship bias, believing that a high GRE score is needed to succeed. Further, physics is seen as a \"brilliance-required\" eld, where innate intelligence is required for success [59]. A third argument, and the most interesting one in terms of the scope of this paper, is that standardized tests such as the physics GRE can help students stand out [112]. The ETS, the creatoroftheGREandphysicsGRE,claimsthatsubjectGREs\"canhelpyoustandoutfromother applicantsbyemphasizingyourknowledgeandskilllevelinaspecicarea\"[113]. Forexample,a studentwithanaveragegradepointaverage(GPA)mightbeabletostandoutfromotherapplicants 29if they did exceptionally well on the physics GRE. In addition, applicants from smaller universities or universities that are not known to the ad- missions committee might benet from performing well on a standardized measure. For example, the ETS claims that the GRE provides a \"common, objective measure to help programs compare studentsfromdierentbackgrounds\"[114]andphysicsadmissionscommitteesworrythatremov- ing the GRE would limit their ability to compare applicants from dierent backgrounds [115]. Anecdotally, some faculty claim that a good physics GRE score could aid students from small liberal arts colleges in the admissions process [116]. WealreadyknowthatGPAsareinterpretedincontextoftheapplicant'suniversity. Posselthas shownthatamongmoreprestigiousgraduateprograms,theapplicant'sGPAisviewedinthecontext oftheirundergraduateinstitutionwithhighGPAsfromprestigiousinstitutionsseenfavorably,low GPAs from an unknown school as unfavorably, and high GPAs from unknown schools and middle GPAs from prestigious institutions in the middle [117]. Therefore, a standardized test such as the physicsGREcouldprovideanassumedequalcomparisonforanadmissionscommitteeandmight allow the applicant from an unknown school to stand out or have a similar chance of admission as an applicant from a more well-known school. Finally, graduate admissions have been documented to be \"risk-adverse,\" where admissions committees select applicants most likely to complete their program [46,48]. As applicants from smalleruniversitiesmaybejudgedbasedonhowpreviouslyenrolledstudentsfromtheiruniversity did in the program [117], a risk adverse admissions committee might be less likely to admit applicants from small universities whose students have previously struggled in their program. However, perhaps a high standardized test score could overcome these perceptions and signal that the applicant might indeed be successful in the program. Our goal then is to focus on the third argument. Does the physics GRE help applicants \"stand out\"intheadmissionsprocessinpractice? Ifthatisthecase,wewouldexpectthosedisadvantaged in the admissions process, those who have low GPAs, attended a smaller institution, or identify as part of a group currently underrepresented in physics, to be admitted at similar rates as their more 30advantaged peers with similar physics GRE scores. Specically, we ask: 1. How does an applicant's physics GRE score and undergraduate GPA aect their probability of admission? 2. Howaretheseprobabilitiesofadmissionaectedbyanapplicant'sundergraduateinstitution, gender, and race? As Small points out in his critique of admissions and standardized test studies [118], multiple variablesratherthanjustastandardizedtestmightbestexplainourresultsandtherefore,aframework that allows for substitutions and trade-os between variables is necessary. Therefore, we ask an additional research question: 3. How might the above relationships be accounted for through mediating and moderating relationships? Thispaperisorganizedasfollows: Sec. 3.2providesanoverviewofmediationandmoderation analysis. We then describe our data, how we determined what constitutes \"standing out,\" and how we implemented mediation and moderation analysis in Sec. 3.3. In Sec. 3.4, we describe our ndings and in Sec. 3.5, we use those ndings to answer our research questions and explain our limitations and choices which may aect our results. Finally, we describe our future work in Sec. 3.6 and the implications of our work for graduate admissions in physics in Sec. 3.7. 3.2 Background Before we can answer the third research question, it is important to describe what we mean by mediating and moderating relationships. Inamediatingrelationship,twovariablesareonlyrelatedbecausetheyarealsorelatedtosome common third variable. For example, a student who played video games the night before an exam mightdopoorlybecausetheystayedupplayingvideogamestoolateanddidnotgetenoughsleep. Therefore, video games and doing poorly on the exam are only related due the common factor of lack of sleep. Lack of sleep is then a mediating variable. 31In a moderating relationship, the strength of the relationship between two variables depends on some third variable. For example, the relationship between someone liking dogs and owning a dog likely depends on whether they are allergic to dogs. That is, we would expect someone who likesdogsbutisallergictodogsislesslikelytoownadogthansomeonewholikesdogsbutisnot allergic to dogs is. Being allergic to dogs is then a moderating variable. Mathematically,supposethatsomeinput -hasaneectonoutput .. Wewouldsaythatsome other input\"mediates the relationship on .because- has an eect on \"and\"has an eect on .[119]. For a simple case, we can represent these relationships as .=81\u00b82- (3.1) \"=82\u00b80- (3.2) .=83\u00b820-\u00b81\" (3.3) where8represents the intercepts. These relationships are visually shown in Fig. 3.1. Usingthisrepresentation,thedirecteectof -on.isrepresentedby 20andtheindirecteect is represented by 01. The total eect is then 20\u00b801which for a linear regression models, is equal to2. Equivalently, in the case the linear regression, the indirect eect is 2\u000020. However, if.is binary, linear regression is not appropriate and logistic regression should be used instead. In this case, Rnhart et al. recommend using 01as the indirect eect as their simulation studies found the 01estimate of the indirect eect exhibited less bias than the 2\u000020 estimate [120]. To determine if the indirect eect is statistically signicant, a common approach is to use a Sobeltest. However,simulationssuggestthattheSobeltestisunderpoweredandthatbootstrapping is a good alternative [121]. Specically, those simulations that using the percentiles of a bootstrappedestimateoftheindirecteecttoestimatethecondenceintervalisagoodcompromise 32Figure 3.1: Visual representation of eqs. (3.1) to (3.3). The top graphic shows eq. (3.1) while the bottom graphic shows eqs. (3.2) and (3.3). between avoiding type I errors while maintaining statistical power. In their approach (which has also been used in PER studies before, e.g., [122]), if 01is dierent than zero, then there is some degree of mediation. More specically, there are three cases. 1. If01<0and20=0then\"fully mediates the relationship between -and.. 2. If01<0and20<0, then\"partially the relationship between -and.. Thisapproachcanalsobeadaptedtomultiplemediatorsandthesemediatorscanbepredictors of other mediators. An example of this serial mediation case with two mediators is shown in Fig. 3.2. Equations (3.1) to (3.3) can then be modied to be .=84\u00b82- (3.4) 33Figure3.2: Visualrepresentationofeqs.(3.5)to(3.7)showingserialmediationwithtwomediators. \"1=85\u00b801- (3.5) \"2=86\u00b802-\u00b803\"1 (3.6) .=87\u00b820-\u00b811\"1\u00b812\"2 (3.7) In this case, there are three indirect eects. First, there are the indirect eects of the mediators individually, 0111and0212, and second, there is the indirect eect of the mediators together 010312. The total indirect eect is then 0111\u00b80212\u00b8010312[125]. More generally, for #mediators, we can generate #\u00b81equations where the rst #are of the form \"==8=\u00b80=-\u00b8=\u00001\u00d5 9=109\"9 (3.8) and the nal equation is of the form .=8H\u00b820-\u00b8#\u00d5 9=119\"9 (3.9) 34So far, we've assumed that the relationship between the mediator \"and the output .does not depend on any other variables. However, it is possible that the relationship between \"and. could also depend on -or some other variable, meaning there is a conditional indirect eect (see Preacher et al. [126]). In the case that the relationship between \"and.depends on-, we would say that-moderates the relationship between \"and.. Practically, this means we must add an interaction term to eq. (3.3), which then becomes [126] .=83\u00b820-\u00b811\"\u00b810 1-\" =83\u00b820-\u00b8\u00b911\u00b810 1-\u00ba\"(3.10) We use the prime on 1coecients to denote an interaction coecient for a mediator while an unprimed1coecient is a coecient of a mediator. The conditional indirect eect is then 0\u00b911\u00b810 1-\u00ba. If10 1=0, we would say that there is no moderation and the indirect eect is the standard 01. In the case that there are multiple mediators, eq. 3.10 can be modied to include multiple mediators and interaction terms for all pairs of variables where moderation may be of interest. In the special case that -is binary, eq. (3.10) reduces to .=8G=0\u00b811\"when-=0and .=8-=1\u00b8\u00b911\u00b810 1\u00ba\"when-=1. Therefore,totestifthereismoderation,wecansimplyregress \"on.given-=0and again given -=1and subtract the slopes to calculate 10 1instead of including an interaction term in the model. 3.3 Methods 3.3.1 Data Data for this study comes from the physics departments at ve selective, research-intensive, pri- marily white universities. Four of these universities are public and part of the Big Ten Academic Alliance while the remaining university is a private Midwestern university. During the 2017- 2018 and 2018-2019 academic years, graduate admissions committees at these ve universities recorded all physics applicants' undergraduate GPA, GRE scores, undergraduate institution, and 35demographic information such as gender, race, and domestic status. In addition, the universi- ties recorded whether each applicant made the shortlist, was oered admission, and whether the applicantdecidedtoenroll. Becauseourstudyincludesallapplicantsratherthanonlyadmittedap- plicants,weareunlikelytosuerfromtherangerestrictionsnotedincritiquesofotheradmissions studies (e.g. [118,127]). However, we do address a possible range restriction in the Limitations and Researcher Decisions section (sec. 3.5.2). Duetodierentrequirementsandadmissionsprocessesforinternationalstudentsanddomestic students (e.g. international students need to submit a test of English prociency), we only include domestic students in our study. We then remove any applicant for whom a physics GRE and GPA were not recorded, leaving us with 2537 applicants. While we in theory could use multiple imputationstoaddressthedataasNissenetal. recommends[71],facultyreviewingtheapplications do not, to our knowledge, do this and hence, we would be creating data that wasn't available in theadmissionsprocess. DistributionsandanalysisoftheremainingphysicsGREscoresandGPAs appear in the appendix. As the applicant's undergraduate university does not contain meaning in itself, we needed to categorizetheinstitutions. Wechosetocategorizetheinstitutionsbytheirsizeandtheirselectivity. Wethenusedthenumberofphysicsbachelor'sdegreesawardedperyearasmeasureofthesizeof the university. We assume that universities with more graduates are more well known and hence, would likely be known to the admissions committees. In contrast, universities that produce fewer bachelor'sdegreesmightnotbeknowntotheadmissionscommitteesandhence,mightbeunknown programs. It would then be these applicants from \"smaller\" programs who might need to \"stand out.\" We acknowledge that some programs that produce a small number of physics bachelor's degrees each year might not be unknown to the admissions committees due to previous applicants fromsuchschoolsorresearchcollaborationsorpartnerships. However,thereisnowayinourdata to know if this is the case. To determine whether a university should be counted as a \"small university\", we used the undergraduate institution names to look up the number of typical physics bachelor's degrees from 36Table 3.1: Summary of the comparisons we analyzed, which group needs to stand out and which does not, and the gure number showing the results VariableGroup that tends to be privileged in admissionsGroup that tends not to be privileged in admissionsFigure Program size Applicants from physics programs that rank in top 25% of programs based on yearly graduatesApplicants from physics programs that rank in bottom 75% of programs based on yearly graduatesFig. 3.5 University selectivity Applicants from universities ranked as most selective or highly selectively (Barron's Value of 1 or 2)Applicants from any other university (Barron's Value of 3 or lower)Fig. 3.6 Gender Male applicants Female applicants Fig. 3.7 Race Asian or white applicants Black, Latinx, Multiracial, and Native applicantsFig. 3.8 AIP's public degree data [128,129]. As of this writing, degree data for the 2018-2019 academic year was not available, so we used data from the 2016-2017 and 2017-2018 academic years to quantify the number of bachelor's degrees. Additionally, this would have been the most recent data available when admissions committees would have reviewed applications and many of the applicants would be represented in the data as bachelor degree recipients To account for the institution's prestige, we used Barron's Selectivity Index [130]. Barron's selectivity index is a measure based on the undergraduate acceptance rate of an institution as well as characteristics of its undergraduate incoming classes, such as mean SAT scores, high school GPAs,andclassrank. Weassumeselectivityisaproxyforprestigeasprestigiousinstitutionstend to have low acceptance rates and high SAT scores and GPAs from incoming students. In contrast to the AIP data, Barron's selectivity index applies to the institution as a whole rather than only the physics department. 3.3.2 Probability of admission procedure Determining whether an applicant is more or less likely to be admitted rst requires computing admissions probabilities. To do so, we grouped applicants based on their GPAs and physics GRE scores. Prior work has found that the physics GRE score and undergraduate GPA are two of the 37most important aspects of the applications [46,47,75]. Our previous work specically found that thephysicsGREscoreandundergraduateGPAwereabletopredictwith75%accuracywhetheran applicant would be admitted to one public Midwestern physics graduate program. In addition, physics is a \"high consensus\" discipline, meaning most programs agree on what consists of a successful applicant [46]. Therefore, despite many other components of the applica- tionsthataectwhetheranapplicantwillbeadmitted,webelieveusingthephysicsGREscoreand undergraduate GPA provides a rst-order overview of what admissions committees would use to admit applicants. In order to ensure a reasonable number of applicants in each group to do meaningful analysis, we grouped applicants into bins based on their GPA and physics GRE score. We choose to use GPAbins0.1unitsinwidthandphysicsGREbins50pointsinwidth. TheGPAbinswereselected to ensure that that GPAs with the same tenth digit were in a single bin. That is, 3.50 through 3.59 wouldbeinasinglebin. AllGPAswerealreadyreportedonthe4.0scaleandphysicsGREscores were reported using the standard 200-990 scale so we did not need to do any conversions. We then computed the fraction of applicants in each bin who were admitted to the program they applied. As we are interested in applicants \"standing out,\" we frame our results as whether applicantsinabinareadmittedatahigherratethantheoverallrate(allacceptedapplicantsdivided by all applicants). If applicants are admitted at a higher rate than the overall rate, it suggests that these applicants did in fact stand out to the admissions committee. In our framing of \"standing out,\" we are assuming that graduate admissions graduate students [53], which may be more characteristic of applicants coming 38from better resourced institutions. Using this framing, we created four groups of applicants who might or might not need to stand out, which are summarized in Table 3.1 and explained in detail below. To take into account the size of the institution, we rst used the AIP data to determine the national quartile each applicant's institution ranked in terms of all bachelor's degree recipients for each of the two years of data. Because not all institutions reported data in both years and the number of graduates could vary signicantly between years, we conducted separate analyses rst withthehighestquartileaninstitutionreachedinthetwoyearsandsecondwiththelowestquartile the program reached in the two years. For example, if an institution was ranked in the 3rd quartile therstyearandthe4thquartileinthesecondyear,ourrstanalysiswouldusethe4thquartileand our second analysis would use the 3rd quartile. We then dene the large programs as those in the 4thquartileandsmallprogramsasthoseinthe1stthrough3rdquartiles. Weaddressthischoicein the discussion. WhenusingBarron'sSelectivityIndextotakeintoaccounttheselectivityoftheinstitution,we usedChettyetal.'s[131]vegroupings(IvyLeague+,remainingmostselectiveinstitutions,highly selectiveinstitutions,selectiveinstitutions,andnon-selectiveinstitutions)asaguide. Astherewas a single applicant from a non-selective institution, selective and non-selective were grouped into a single category. Because we are interested in smaller, less known programs compared to larger, well-known programs, we took the selective and non-selective group to be our \"less selective institution\" group and institutions in the rst three of Chetty et al.'s categories as our \"most selective institutions\". This corresponds to grouping institutions with a Barron's Index of 1 and 2 together as the \"most selective institutions\" and all other values together as the \"less selective institutions\". To understand how high physics GRE scores might help applicants identifying as part of a group currently underrepresented in physics, we compared women's admission probability to men's admission probability and applicants of color's admission probability to applicants not of color's admission probability. While it should be noted that gender is not binary [132], the data 39Table 3.2: Counts of applicants by gender and race who provided both GPAs and physics GRE scores Race Gender Asian Black Latinx Multi Native White Unreported Total Men 247 49 99 166 4 1410 112 2087 Women 56 2 19 26 0 308 28 439 Unreported 1 0 0 1 0 5 4 11 Total 304 51 118 193 4 1723 144 2537 the admissions committee recorded is only in terms of the male and female binary and hence, we cannot comment on how high physics GRE scores may impact applicants of other genders. Furthermore, given the limited number of applicants identifying as part of a racial group underrepresentedinphysics,wecombinedallBlack,Latinx,Multiracial,andNativeapplicantsinto a single category, which we will refer to as B/L/M/N following the recommendation of Williams [133]. We acknowledge that this may obscure important distinctions between groups, as Teranishi [134]andWilliamssuggest. Wealsoacknowledgeapplicantsidentifyingasamarginalizedgender and race may face additional barriers and hence could stand out dierently than an applicant identifying as either a marginalized gender or race. However, there are less than 50 applicants (\u00182%of the sample) identifying as a member of both a marginalized gender and marginalized race, limiting statistical power for analysis. Full demographics are shown in Table 3.2. For informationabouthowraceandethnicitycategorieswereconstructedandstandardized,seePosselt et al. [54] who previously used the 2017-2018 academic year application data from this study in their study. 3.3.3 Mediation and Moderation Procedure Given that to some degree, both the physics GRE score and undergraduate GPA measure physics knowledge,weexpectthatthesetwomeasureswillbecorrelatedwitheachother. Therefore,werst testedwhetherthephysicsGREhasanymediatingeectswhenpredictingadmissionandwhether GPAmoderatestherelationshipbetweenthephysicsGREandadmission;thatis,isoneonlyrelated to admission because it influencesthe other and that one influences admission or isthe strength of 40the relationship between one and admission aected by the other. Because admissions status is a binary outcome variable, we need to use logistic regression for eqs. (3.1), (3.3) and (3.10). When taking an applicant's GPA and physics GRE score into account, we rst centered and scaled both variables so they both have means of zero and variances of 1. As we are treating GPA and physics GRE scores as continuous, we can use linear regression for eq. (3.2). Toestimatethecoecientsineqs.(3.1)to(3.3)and(3.10),wegenerated5000bootstrapsamples withreplacementaswasdoneinHayesandScharkow[121]. Foreachtrial,wecomputedtheindirect eect01. To get the estimate of each parameter, we took the average of the 5000 bootstraps. To get the lower end of the 95% condence interval, we used the value that corresponded to the 2.5th percentile of the values generated by the bootstrap. Likewise, to get the upper end of the 95% condence interval, we used the value that corresponded to the 97.5th percentile. For the institutional features, we treat institutional selectivity and institution size as binary inputvariables(mostselectiveorlessselectiveandlargerinstitutionorsmallerinstitution)andfor demographicfeatures,wetreatgenderandraceasbinaryvariables. Again,weuseB/L/M/Nasone category for race and white and Asian as the other. The applicant's physics GRE score and GPA are again treated as continuous mediating and moderating variables. BecausethephysicsGREscoreandGPAcanbothactasmediatorsandGPAmayalsoinfluence the physics GRE score, we used a serial mediation model instead of the simple mediation model (eqs. (3.4) to (3.7)). While moderation by the independent variable -can occur for any of the relations between the other variables, only moderating relationships between GPA and admission and the physics GRE score and admission are within the scope of this work. Therefore, we only include those interaction terms in our models. For all of these analyses, we used the same bootstrapping process used for the simple mediation and moderation cases. 413.4 Results 3.4.1 Probability of admission results WhencomparingtheGPAsandphysicsGREscoresofallapplicants,wenoticethatmostapplicants who are admitted have both high GPAs and high physics GRE scores (Fig. 3.3). Furthermore, whileanearperfectGPAorphysicsGREscoreresultedinthehighestchanceofadmission,having either a high GPA or high physics GRE and a modest score on the other seemed to still oer an admission fraction around the overall average. However, having a low GPA or low physics GRE andamodestscoreontheotherisusuallygroundsforrejection. Overalladmissionsfractionsfora given physics GRE score or GPA are shown in the top and right margins of Fig. 3.3 respectively. InregardtohavingahighphysicsGREscoredespitealowGPA,werstnotethatonlyasmall fraction of all applicants fall in this regime. Second, there appears to be no pattern in terms of higher than average fraction admitted for these applicants. Some combinations of low GPA and high physics GRE score result in a few applicants being admitted, and hence, an above average fraction of applicants being admitted, while other score combinations have no applicants being admitted, and hence, a below average change of admission. For example, having a GPA in the 3.3 bin and a physics GRE score in the 1000 bin resulted in an above average fraction admitted while having a GPA in the 3.4 bin and a physics GRE score in the 1000 bin did not result in an above average fraction admitted, despite the applicants having a higher GPA. TofurtherunderstandwhetherahighphysicsGREscorecanhighlightthosewithlowGPA,we dividedallstudentsintoeitherahighorlowGPAandhighorlowphysicsGREscorebins,Fig. 3.4. Based on Fig. 3.3 in terms of admissions probabilities, a low GPA seems to be below a 3.5, while a high physics GRE score seems to be above 700. However, 700 is a common cuto score which couldexplainwhyadmissionsprobabilitiesincreaseafterthatscore. Becausehittingtheminimum score might not catch the admission committee's eyes, we instead selected a higher score of 880 which represents the 80th percentile. From Fig. 3.4, we notice two things. First, among applicants in the low GPA bin, less than 42<500550 600 650 700 750 800 850 900 950 1000 Any Binned Physics GRE Score<3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 3.3: Fraction of applicants admitted by undergraduate GPA and physics GRE score. The number of students in each bin is also shown. 'Any' corresponds to the corresponding row or columntotals. Thebinlabelcorrespondstotheupperboundofvaluesinthebinexclusivewiththe exception of the 4.0 GPA bin which includes 4.0. Values are colored based on whether they are above, below, or equal to the overall admissions rate. Admissions rates within 10% of the overall rate are colored the same as the overall rate. The above and below average colors are based on being above/below the midpoint between the max/min admission fraction and the overall average. These are based on raw numbers and not a statistical test. half(44%)evenmakeitabovethetypicalcutoscoreof700andlessthan10%ofthoseapplicants with low GPAs score 880 or higher. These represent approximately 11% and 2% of all applicants respectively. Comparing the fraction of admitted applicants in each bin, applicants with high physics GRE scores and low GPAs are admitted at nearly the same rate as applicants with high GPA and low physics GRE scores. Second,wenoticethat16%ofallapplicantsscoreinthehighGPAbutlowphysicsGREscore bin. Thatis,moreapplicantscouldbepenalizedforhavingalowphysicsGREscoredespiteahigh GPA than could benet from a high physics GRE score despite a low GPA. 43[400,700) [700,880) [880,990] condensed version of Fig. 3.3 showing the fraction of applicants admitted by undergraduate GPA and physics GRE score. <500600 700 800 900 1000 Any Binned Physics GRE Score< 3.0 3.3 AverageAbove Average Fraction Admitted <500600 700 800 900 1000 Any Binned Physics GRE Score< 3.0 3.3 Average Fraction Admitted Figure3.5: FractionofapplicantsadmittedbyundergraduateGPAandphysicsGREscoreandsplit by large or small undergraduate university 44<500600 700 800 900 1000 Any Binned Physics GRE Score< 3.0 3.3 Fraction Admitted <500600 700 800 900 1000 Any Binned Physics GRE Score< 3.0 3.3 by selective or non-selective undergraduate university. Table3.3: DistributionofapplicantsscoringineachPhysicsGRErangebysizeofinstitution. ETS only publishes overall score distributions and hence, we cannot report national scores from only domestic size of the applicant's undergraduate program into account, (large or small), using either the highest or lowest quartile of bachelor's graduates over the two year period did not substantially change the results. Therefore, we only present results from the highest quartile reached, which are shown in Fig. 3.5. Due to the much smaller number of applicants per bin, we reduce the number of GPA and physics GRE bins. We use bins of 3.0 or less, which corresponds to a B or lower, 3.0 to up 3.3, a B+, 3.3 up to 3.7, an A-, and 3.7 up to 4, an A under the standard 4.0 scale. Overall, by looking at the bin in the 'Any' row and 'Any' column of Fig. 3.5 and Fig. 3.6, we see that applicants from the largest undergraduate programs are nearly 40% more likely to be 45<500600 700 800 900 1000 Any Binned Physics GRE Score< 3.0 3.3 700 800 900 1000 Any Binned Physics GRE Score< 3.0 3.3 applicant's gender. <500600 700 800 900 1000 Any Binned Physics GRE Score< 3.0 3.3 Admitted <500600 700 800 900 1000 Any Binned Physics GRE Score< 3.0 3.3 the applicant's race. 46admitted(0.28to0.20)whileapplicantsfromselectiveinstitutionsarenearly70%morelikelytobe admitted(0.31to0.18). Lookingattheindividualadmissionfractions,theredoesnotappeartobe any advantage to applicants graduating from smaller institutions or less selective institutions. The physics GRE scores and GPAs where applicants are admitted at higher than average rates are the nearlysameforlargeandsmallprogramsandselectiveandnon-selectiveprograms. Unsurprisingly, these tend to be higher physics GRE scores and higher GPAs. Outside of a few bins with a small numberofapplicants,nocombinationoflowGPA(B+orless)andhighphysicsGREscoreresulted in an above average admission fraction. ForthehighestphysicsGREscores,900andabove,applicantsfromthelargestormostselective universities seem to be admitted at a higher rate and a higher fraction of applicants from large or selective universities achieve these high scores compared to applicants from smaller universities. The fraction of applicants from both large universities, small universities, selective universities, and non-selective universities, as well as nationally, achieving each score is shown in table 3.3. Thus, it appears that even if higher scores did help applicants stand out, applicants from smaller andlessselectiveschoolsmostinneedofstandingoutarelesslikelytoachievethosescoresinthe rst place. Finally, the results from grouping by gender and race are shown in Fig. 3.7 and Fig. 3.8. Interestingly, we nd that for most physics GRE scores, women are admitted at higher rates than men of equal score are. Likewise, we nd that Black, Latinx, Multi-racial, and Native applicants are admitted at higher or similar rates as white or Asian applicants are for similar physics GRE scores. In addition, the same trend seems to hold for GPA as well. However, a high physics GRE score does not seem to help women with a low GPA. For B/L/M/N applicants, there appears to be a few places where applicants may stand out (such as the 800 physics GRE bin and 3.3 GPA bin). Ifthese applicantswere standingout dueto theirphysicsGRE scorethough, wewould expectthat pattern to continue for higher physics GRE scores but the same GPA. This does not appear to be the case, suggesting these applicants stood out for a reason other than their physics GRE scores. We address this in our discussion. 47Figure3.9: Visualrepresentationofthebootstrappedcoecientsineqs.(3.1)to(3.3). Wedond evidence of the physics GRE score mediating the relationship between GPA and admission status. Because these applicants may have stood out for reasons other than their physics GRE score, wedonotdiscussanyinteractionsbetweengenderandraceandselectivityandinstitutionsize. For completeness, plots showing these interactions are included in the supplementary material. 3.4.2 Mediation and moderation results 3.4.2.1 Physics GRE and GPA A visual representation of our mediation results with the physics GRE score and GPA is shown in Fig. 3.9. We nd that all coecients are statistically dierent from zero. From Fig. 3.9, we see that an applicant's physics GRE score and GPA have about the same eect on whether the applicant is admitted. Given that applicants who had either a high physics GRE score or a high GPA had about the same chance of being admitted, this is not a surprising result. Second,wendthattheindirecteectisnotzero,meaningthatthereispartialmediation. That is,whetheranapplicantisadmitteddependsontheirphysicsGREscoreandtheirGPA.Intermsof theamountofmediation,wendthattheindirecteectaccountsfornearly30%ofthetotaleect. Finally, doing moderation analysis, we nd that 10 1=0\u0095024\u00b9\u00000\u0095114\u00960\u0095154\u00ba. As zero is included in the condence interval, we do not nd evidence that GPA moderates the relationship 48Figure 3.10: Visual representation of the bootstrapped coecients in eqs. (3.5) to (3.7). We do ndevidenceofthephysicsGREscoremediatingselectivityandadmissionsstatusbutdonotnd evidence of GPA mediating selectivity and admissions status. We do not nd evidence of a serial mediating relationship. Statistically signicant coecients are in bold. Figure 3.11: Visual representation of the bootstrapped coecients in eqs. (3.5) to (3.7). We do nd evidence of the physics GRE score mediating institution size and admission status but do not ndevidenceofGPAmediatinginstitutionsizeandadmissionsstatus. Wedonotndevidenceof a serial mediating relationship. Statistically bold. betweenanapplicant'sphysicsGREscoreandwhethertheyareadmitted. Thatis,therelationship betweenanapplicant'sphysicsGREscoreandwhethertheyareadmittedisnotinfluencedbytheir GPA. 493.4.2.2 Institutional features A visual depiction of our results is shown in Fig. 3.10 and Fig. 3.11. We nd that the applicant's physicsGREscorepartiallymediatestherelationshipbetweentheselectivityoftheirundergraduate institution and whether they were admitted and fully mediates the relationship between their institution's size and whether they were admitted. The fractions of mediation due to the indirect eects from the physics GRE score were0212 j0111j\u00b8j0212j\u00b8j010312j\u00b8j20j=0\u009525and0\u0095533respectively. In contrast, the applicant's GPA was not found to be a signicant mediator in either case (zero was contained in the indirect eects' 95% condence intervals), meaning that GPA is not a reason that there are dierences in admission based on the applicant's undergraduate institution. Additionally, no serial mediation was observed for either case. When looking at the results of the moderation analysis when the physics GRE is the me- diating variable, we nd that neither 10 2value is statistically dierent from zero ( 10 2\u0096B4;42C8E8CH= 0\u0095136\u00b9\u00000\u0095141\u00960\u0095411\u00baand10 2\u0096B8I4=0\u0095080\u00b9\u00000\u0095204\u00960\u0095361\u00ba), meaning that the relationship be- tween the physics GRE score and admission is the same regardless of the type of institution the applicant attended. Likewise,wedonotndevidenceofmoderationwhenGPAisthemediationvariable. Inthose cases,10 1\u0096B4;42C8E8CH=0\u0095052\u00b9\u00000\u0095314\u00960\u0095394\u00baand10 1\u0096B8I4=\u00000\u0095143\u00b9\u00000\u0095630\u00960\u0095267\u00ba. 3.4.2.3 Demographic features Our results are shown visually in Fig. 3.12 and Fig. 3.13. Because we chose woman to be \"1\" and B/L/M/N to be \"1\" in our logistic regression equation, some of the coecients are negative. Forexample,thenegative 0coecientforgenderandphysicsGREscoremeansthatwomenscore loweronthephysicsGREthanmendo. Becausethesigndependsonourchoiceofwhichcategory should be \"1\" and are in that sense arbitrary, we use the absolute values of 20and0818to calculate the fraction of mediation. We nd that the applicant's physics GRE score partially mediates the relationship between gender and admission but not race and admission meaning that gender aects admission in part 50becauseitaectphysicsGREscores,whichaectadmission. Thefractionofmediationforgender and admission due to the physics GRE score is0212 j0111j\u00b8j0212j\u00b8j010312j\u00b8j20j=0\u0095246 For GPA, we nd the opposite. GPA partially mediates the relationship between race and admission but not gender and admission. The fraction of mediation for race and admission due to GPA is0111 j0111j\u00b8j0212j\u00b8j010312j\u00b8j20j=0\u0095299. Likewise,wendaserialmediationeectforraceandadmissionbutnotgenderandadmission. Thatis,admissionisaectedbyracebothbecauseadmissionisrelatedtoGPAwhichisrelatedto race and because admission is related to the physics GRE score which is related to GPA which is related to race. When investigating whether any moderation eects exist, we do not nd that to be the case. That is, we nd that none of the interaction coecients are statistically dierent from zero and hence, physics GRE scores and GPAs do not have a dierential eect on admission based on the applicant's gender or 10 1\u0096 %\u0016\u0096A024=\u00000\u0095236\u00b9\u00000\u0095586\u00960\u0095143\u00ba. All results and interpretations from the mediation and moderation analyses are summarized in table 3.4. 3.5 Discussion Here, we address each of our research questions and possible limitations or confounding factors. 3.5.1 Research Questions How does an applicant's physics GRE score and undergraduate GPA aect their probability of admission? We nd that scoring highly on the physics GRE and having a high GPA results in the 51Figure3.12: Visualrepresentationofthebootstrappedcoecientsineqs.(3.5)to(3.7). Wedond evidenceofthephysicsGREscoremediatinggenderandadmissionstatusbutdonotndevidence of GPA mediating gender and admission status. We do not nd evidence of a serial mediating relationship. Statistically signicant coecients are in bold. Figure 3.13: Visual representation of the bootstrapped coecients in eqs. (3.5) to (3.7). We do nd evidence of GPA mediating race and admission status and a serial mediation eect but do not nd evidence of the physics GRE mediating race and admission status. Statistically signicant coecients are in bold. 52Table 3.4: Summary of the mediating and moderation results. * signies partial mediation is present, ** signies full mediation is present, ysignies moderation is present. However no moderation eects were found. Independent Mediating Indirect eect Moderating eect GRE 0.300** 0.080 Institution size GPA -0.015 -0.143 Institution size Serial -0.004 NA Gender Physics GRE -0.540* 0.154 Gender GPA -0.022 0.209 Gender Serial -0.018 Physics GRE -0.049 -0.007 Race GPA -0.285* -0.236 Race Serial -0.110* NA highest chance of admission (Fig. 3.4). Likewise, having a low physics GRE score and low GPA results in the lowest chance of admission. If either the applicant's physics GRE score or GPA is high while the other is not, the chance of admission is approximately equal, regardless of which one is high. However, the number of applicants with high GPAs but low physics GRE scores is 9 times as large as applicants with low GPAs and high physics GRE scores (i.e. scoring above the 80th percentile; Fig. 3.4). Even if we consider meeting the minimum cuto score as a high physics GREscore,thenumberofapplicantswhohavehighGPAsbutlowphysicsGREscoresis1.5times greater than the number of applicants with low GPAs but high physics GRE scores. Thus, many more high GPA applicants could be penalized by the physics GRE than low GPA applicants could stand out or benet from a high physics GRE score. Finally, we note that for low-GPA applicants with high physics GRE scores, they are all essentiallyadmittedatthesamerate,regardlessofwhethertheyscoredinthe700-870rangeorthe 880-990range. Iftheseapplicantswerestandingout,wewouldexpectlowGPAapplicantsscoring above 880 to be admitted at a much higher rate than low GPA applicants scoring between 700 and 870. Thus, it is hard to determine if these applicants actually stood out to the committee or if they 53simply met the minimum physics GRE score needed for the committee to review the rest of the application. How are these probabilities of admission aected by an applicant's undergraduate institution, gender, and race? First, we nd that for most physics GRE scores, applicants from larger and smaller institutions are admitted at similar rates (Fig. 3.5). However, for the highest scores (above 900), applicants from larger universities are admitted at higher rates. Interestingly, for applicants from smaller programs, scoring above 900 does not appear to provide any additional benet in terms of the fraction of applicants admitted compared to scoring between 800 and 900. In contrast, applicants from less selective institutions are less likely to be admitted than ap- plicants from more selective institutions for all physics GRE scores above the common cuto score (Fig. 3.6). That is, the physics GRE does not seem to counteract any potential biases from admissions committees toward applicants from less selective institutions. Overall, attending a large or selective institution and scoring highly on the physics GRE does result in a higher chance of admission than scoring highly on the physics GRE and attending a smaller or less selective institution. It is important to note that there might be selection bias in our data because test-takers with high scores from smaller universities might not choose to apply to these schools. However, this seems unlikely because 1) these programs are highly regarded and hence, these would not be \"safety schools\" to high scoring applicants (as indicated by many high scoring applicants from largeprogramsapplyinghere)2)whilethereisresearchsuggestingstudentswithlowphysicsGRE scores might view their scores as barriers to applying [49], to our knowledge, there is no evidence thatstudentswithhighscoresdonotapplytophysicsgraduateprograms. Giventhatstudentswith low test scores might not apply, it is expected that our data is not representative of test-takers on the lower end of scores (as shown in table 3.3). When looking at the demographic variables, we nd that women are admitted at higher rates than men with similar scores (Fig. 3.7) and B/L/M/N applicants are also admitted at higher rates than white or Asian applicants (Fig. 3.8). As prior work has shown [105], women and B/L/M/N 54test-takerstendtoscorelowerthanwhitemenonthephysicsGREandhence,scoringhighlycould cause these applicants to stand out to admissions committees. How might the above relationships be accounted for through mediating and moderating rela- tionships? Our mediation and moderation analysis further supports the results found through the probability of admissions procedure. WendthatthephysicsGREscoreandGPAhavesimilarregressioncoecientswhenmodeling admission, suggesting they have similar eects (Fig. 3.9) and that there is a mediation eect. In addition, we did not nd any evidence of moderation. That means the relationship between GPA and admission is not dierent due to the applicant's physics GRE score. If a high physics GRE score did help a low-GPA applicant stand out, we would expect to see a moderation eect. Combining the results of probability of admission analysis and the mediating and moderation analysis, we nd that there is mediation but no moderation between an applicant's physics GRE score and their GPA when it comes to admission probability. In practice then, an applicant with a low GPA cannot simply overcome that low GPA by scoring highly on the physics GRE. When we performed mediation analysis on the institutional factors, we found that the relation between institutional selectivity and admission was partially mediated by the applicant's physics GRE score and the relation between institutional size and admission was fully mediated by the applicant's physics GRE score (Fig. 3.10 and Fig. 3.11). Neither of these relationships was mediated by the applicant's GPA or serially however. TheresultsofthemediationanalysisshowthatphysicsGREscoresseemtoexplainsomeofthe dierences in admission probability based on the applicant's undergraduate institution. Therefore anapplicantfromasmallerorlessselectiveinstitutionmaybeabletostandoutbyscoringhighlyon the physics GRE. However, looking at the fraction of applicants admitted by physics GRE scores, especially the highest scores, suggests that is not what happens in practice. In terms of gender and race, we do nd some mediating relationships, but no moderation relationships (Fig. 3.12 and Fig. 3.13). We nd that the physics GRE partially mediates the relationshipbetweengenderandadmission. WealsondGPAandGPAplusthephysicsGREscore 55partially mediates the relationship between race and admission. That is, some of the dierences in admission rates between men and women can be explained by the dierences in their physics GRE scores and some of the dierences in admission rates between B/L/M/N applicants and non- B/L/M/N applicants can be explained by dierences in their GPAs or physics GRE scores and GPAs. TheseresultsthensuggestthatafemaleorB/L/M/Napplicantmaybeabletostandoutbydoing well on the physics GRE. In practice, the probability of admission results do suggest that women and B/L/M/N applicants are admitted at higher rates than their male, white, or Asian peers are. However, as the ve programs studied here were interested in increasing their diversity, our data does not allow us to disentangle \"standing out\" from highlighting. Therefore, our results should be interpreted with caution regarding any claims that the physics GRE may help applicants from groups underrepresented in physics stand out. ItshouldalsobenotedthatwomenandB/L/M/Napplicantsarelesslikelytoreachthesehigher scoresthantheirmale,white,andAsianpeers. Inourdata,75%ofmenand72%ofwhiteorAsian applicants scored above 700 compared to 45% of women and 57% of B/L/M/N applicants. Thus, even if the physics GRE does allow these applicants to stand out, any potential benet must be weighed against known scoring discrepancies. Finally, it could be argued that even though we did not show that the physics GRE helps these applicants \"stand out\", doing well on the test could still provide some benet for them in the admissions process. We would agree with that argument not because of any properties of the test butbecauseofthestructureofgraduateadmissionsinphysics. Intheory,anypartoftheapplication could be weighted highly and therefore, doing well on that part would provide some benet. Given that prior work has established that the physics GRE is weighted highly [46,47,51,75], we would expect that good performance on the test would provide some benet to applicants. Our goal, however, was not to determine whether a high physics GRE score benets applicants in any capacity. Instead, our goal was to determine whether a high physics GRE score oers a disproportionalbenetthatwouldjustifyusingitingraduateadmissionsgiventhedisproportional 56harms the physics GRE can cause, which we were unable to show in practice. 3.5.2 Limitations and Researcher Decisions DataBiases Aspreviouslynoted,applicantswithlowerphysicsGREtestscoresmaybelesslikely to apply, resulting in an over-representation of high scoring applicants. In addition, the programs in this study are well-regarded programs and there is likely a secondary bias toward applicants with high GPAs and high physics GRE scores applying overall. As a result, the results may not generalize to graduate programs whose applicants tend to have lower GPAs or low physics GRE scores. In addition, it is possible that an applicant could be represented multiple times in the data set, asanapplicantcouldhaveappliedtomorethanoneoftheveuniversitiesinthisstudy. However, eachapplicantappliestoeachprogramindependentlyandthus,wecantreatthemasseparateevents fortheadmissionsprobabilities. Ontheotherhand,resultsbasedondistributionssuchastable3.3 and Fig. B.1 and Fig. B.2 would be aected by the duplicates. Toseeifpossibleduplicatesaectedourresults,wecomparedthedistributionswithandwithout possible duplicates. We assumed an application represented the same applicant and hence was a duplicate if two records had the same physics, verbal, written, and quantitative GRE scores, GPA, undergraduate university, and demographic features as the chance of all of these matching for a nonduplicate seems exceedingly low. When we compare the distributions both with and without possible duplicates, Kolmogorov- Smirnovtests[135]suggestthedistributionsarenotsignicantlydierent. Therefore,becausewe cannot actually determine which applicants are duplicates and excluding possible duplicates does not change our results, we did not remove possible duplicates. Our choice of low GPA and high physics GRE While percentiles are available for the physics GRE, a \"high score\" is left to interpretation. Even among admissions committees, membersmayhavedierentideasofwhatahighscoreis. Inourwork,wehavetakenthecommon cutoscoreof700astheminimumpossiblehighscore[52]. revised version of Fig. 3.4 showing the fraction of applicants admitted by under- graduate GPA and physics GRE score when the cuto score for a high physics GRE score is 670. Here,thenumberofapplicantswhocouldbenetfromahighphysicsGREscoreisapproximately equal to the number of applicants who could be penalized by a low physics GRE score. numberofapplicantswithlowGPAswhocouldbenetfromscoringhighlyonthephysicsGREis less than the number of high GPA applicants who could be penalized by having a score below the cuto. We nd that the number of low GPA applicants who could benet from a high physics GRE score is greater than the number of high GPA applicants who could be penalized by a low score whenthe highscorecutois lessthanorequal to670,which islowerthanthe typicalcutoscore and is around the 43rd percentile (Fig. 3.14). Assuming a high score should be at least above the 50th percentile, our specic choice of a high score does not aect our result that more applicants could be penalized than could benet. The previous argument is also aected by what we consider a high GPA. We have chosen any GPAslessthan3.5tobelowbasedontheresultsshowninFig. 3.3whereapplicantswithGPAsat or above 3.5 are nearly twice as likely to be admitted to as applicants with GPAs below 3.5. If we were to pick a lower threshold, there would be even fewer applicants in the low GPA-high physics GRE score group and more applicants in the high GPA-low physics GRE score group, meaning 58[400,700) [700,880) revised version of Fig. 3.4 showing the fraction of applicants admitted by under- graduate GPA and physics GRE score when the cuto score for a high undergraduate GPA is 3.4. evenmoreapplicantswouldpossiblybepenalizedratherthanstandout. UsingaGPAcutoof3.4 instead of 3.5, the ratio of applicants who could be penalized compared to stand out changes from the original 9:1 to nearly 19:1 (Fig. 3.15). If we instead picked a higher GPA such as 3.6, there would be more applicants who could potentially benet, but even then, the number of applicants who could benet is only greater than thenumberofapplicantswhocouldbepenalizedaroundaphysicsGREscoreof730,whichisnot a high physics GRE score (approximately 54th percentile) and does not signicantly change our results (Fig. 3.16). If we were to pick an even higher GPA cuto, we could be hard-pressed to justifywhyanythingotherthanan'A'GPAisconsideredalowGPA,especiallybecauseadmissions committeesseemtogroupapplicantswithGPAsbetween3.5and3.6morecloselywithapplicants with GPAs between 3.7 and 3.8 than applicants with GPAs between 3.4 and 3.5 (based on the fraction of applicants admitted). Based on our data and the fact that some universities use 3.5 as the only separation between 3.0and4.0,using3.5seemstorepresentthebestoptionforseparatinghighandlowGPAstudents. Using any other choice either strengthens our claims or seems unrealistic to use as a cuto. 59[400,700) [700,730) revised version of Fig. 3.4 showing the fraction of applicants admitted by under- graduate GPA and physics GRE score when the cuto score for a high undergraduate GPA is 3.6. Here,thenumberofapplicantswhocouldbenetfromahighphysicsGREscoreisapproximately equal to the number of applicants who could be penalized by a low physics GRE score. Our choice of non-selective school We choose to follow a modied version of Chetty et al.'s groupings of programs [131]. However, many large, state universities have a Barron's Selective Index of 3 and fall in Chetty et al.'s fourth group. For our analysis, we would have included these large, state institutions as part of the less selective programs. As we are concerned with whether the physics GRE helps applicants stand out, saying applicants from large, state universities (for example, the University of Colorado-Boulder, the University of Washington, and Michigan State University) may fall in the traditionally missed category may not be correct. We reran the analysis with these large, state institutions as part of what we called the most selective programs. We nd that the conclusions are then more aligned with the large vs small program results. Using this grouping, applicants from less selective programs are admitted at similar rates to applicants from more selective programs for most physics GRE scores. However, applicantsfrommoreselectiveinstitutionswithphysicsGREscoresabove900arestillmorelikely to be admitted than applicants from less selective institutions with similar physics GRE scores. Intermsofthemediatingandmoderationanalysis,ourresultswouldbestrengthenedunderthis 60choice. WhilethephysicsGREscorewouldnolongermediatetherelationshipbetweenselectivity andadmissionstatus,itwouldmoderatetherelationship( 10 2=0\u0095311\u00b90\u0095015\u00960\u0095612\u00ba). Thispositive moderationmeansthatthephysicsGREscorehasagreatereectonadmissionstatusforapplicants from more selective programs. In terms of standing out arguments, the positive moderation result means that doing well on the physics GRE would provide more of a benet to applicants from more selective universities and not to applicants from smaller programs who are the intended beneciaries of the \"standing out\" argument. Thus,eventhoughthedetailschange,theoverallconclusionarenotweakenedbychangingour groupings. In fact, changing the groupings may strengthen our conclusions instead. Our choice of a \"small\" school We chose small schools to be any university not in the top quartile of yearly bachelor's degrees awarded. We acknowledge that using quartiles is an arbitrary decision. However,whenweusedhalvesinsteadofquartilestodividelargeandsmallschools,our results were unchanged, both in terms of the probability of admission analysis and the mediation and moderation analysis. Using the bottom quartile as small schools and all other programs as large schools would not have yielded insightful results as less than 2% of applicants would have attended a small school under this choice. Of the possible physics specic measures, the number of bachelor's degrees seems most ap- propriate because programs with more graduates are more likely to be known by admissions committees simply because there are more students to apply from those programs. For example, theprogramsinthetopquartilebynumberofbachelor'sgraduatesproducenearlytwo-thirdsofall physicsbachelor'sgraduates[128,129]. Inaddition,weassumethatprogramswithstrongphysics reputations attract more students and hence, produce more graduates. While this is likely to be more true at the graduate level, not all physics programs oer graduate degrees and hence, using the number of PhDs awarded would not be useful. Thus, we believe the number of bachelor's graduates serves as a rough proxy for physics reputation. 613.6 Future Work While the ve universities included in this study were interested in increasing their diversity and reducing inequities in their programs, their admissions processes still resembled the traditional metrics-based admissions model. Recently, many programs, including the ones studied here, have begun to employ holistic admissions, which looks at the overall application, taking into account non-cognitive competencies and contextualizes the accomplishments of the applicant in terms of the opportunities that were available to them [136,137]. Often these holistic admissions use rubrics to weight the various components of each applicant (e.g. see [138,139]). Evidence from biomedical science graduate programs suggests that the GRE can even be included in holistic admissions without reproducing its known gender and racial biases [140]. Furthermore, their two-tieredapproachtoholisticadmissionsdidnotsignicantlyincreasetheworkloadofadmission committee members. These ndings could persuade faculty reluctant to remove GRE due to its easeandsupposedabilitytomeasuresomeinnatequalitytotryholisticadmissions. Whetherthese results would hold for decentralized admissions as is typical in physics and for the physics GRE though are still open questions. Our future work will then examine how our results may be aected when a department uses holistic admissions. In theory, we should no longer see the discrepancies between admitted applicants from large and small programs and more selective and less selective universities. In addition, the sample rubric developed by the Inclusive Graduate Education Network (as shown in [138]) suggests ranking applicants by high, medium, or low on each part of their applicant. Therefore, we would expect to see a flatter distribution of admission fractions based on physics GRE scores because, for example, all scores within the 'high' range should be treated equally in the admissions process. Our future work will determine if this is indeed the case. 3.7 Conclusion and Implications Our work suggests that, in practice, scoring highly on the physics GRE does not help applicants fromsmallorlessselectiveschoolsorapplicantswithalowGPA\"standout\". Indeed,havingahigh 62physicsGREandlowGPAisnobetterthanhavingalowphysicsGREscoreandhighGPAinterms of the fraction of applicants admitted. Similarly, for average physics GRE scores, the selectivity or size of the applicant's institution does not oer any advantage. For the highest scores though, attending a smaller or less selective institution does appear to result in an admissions penalty. We also nd that women and B/L/M/N applicants do have higher rates of admission based on physics GRE scores. However, given that the departments included in this study were actively tryingtoimprovethediversityoftheirgraduatestudentpopulation[54],weareunabletoattribute that standing out to the physics GRE. While ETS's claim that the physics GRE can help applicants stand out from other applicants maybetrueintheory,wedonotndevidencetosupportthatclaiminpractice. Infact,ourresults suggesttheopposite: thephysicsGREmaypenalizeapplicantsduetoalowscoreratherthanhelp applicants due to a high score. As Small points out, facts and data do not unambiguously prescribe a course of action [118] and as other have noted, making such courses of action require a framework of assumptions and commitments [141]. Thus, we do not make a specic recommendation regarding whether the physicsGREshouldbekeptorremovedasaresultofourworkbecausetheanswertothatquestion depends on the priorities of the department. However, if departments are using the physics GRE to identify applicants who might be missed by othermetrics to achieve their admissions priorities, we suggest against this practice as it does not appear to be backed by evidence. 63CHAPTER 4 RUBRIC-BASED ADMISSIONS: A NEW APPROACH TO GRADUATE ADMISSIONS IN PHYSICS This chapter is being drafted as a journal article. The working manuscript version includes K. Tollefson as the second author, Remco G. T. Zegers as the third author and Marcos D. Caballero as the fourth author. Following the Contributor Roles Taxonomy (CRediT) [76], my roles for this projectincludeconceptualization,formalanalysis,methodology,software,validation,visualization, and writing the original draft. 4.1 Introduction FemaleandBlack,Latinx,andIndigenousscholarshavebeenandareunderrepresentedatalllevels ofphysics. Thepercentageofphysicsdegreesawardedtowomenhasstagnatedaround20%[142] while the percentage of physics degrees awarded to Black, Latinx, and Indigenous students has remainedlessthan10%despitethesestudentsmakingupalargerportionofthecollegepopulation than in the past [143]. While there are numerous possibilities to address the systematic inequities thesescholarsfaceatalllevelsofacademiathatlimittheirparticipation[144-150],thispaperwill focus on graduate admissions. Specically, if we treat graduate admissions as a four stage process similar to how O'Meara et al. treats faculty hiring as four-stage process [151], this paper focuses on evaluating applicants and making admissions oers stages of the process. While physics departments may be interested in increasing their diversity, the dominant pro- cessesofevaluatingapplicantsforgraduateschooldonotsupportsuchaims. Priorworkhasfound thatdiversityconsiderationsareoftensecondarywhenevaluatingapplicantsandarediscussedafter many diverse candidates have already been cut from the applicant pool [54,152]. Therefore, in- creasingdiversityandequityduringtheadmissionsprocessrequiresrethinkingtheprocessphysics departments use to evaluate applicants. Onepromisingapproachtorethinkingtheadmissionsprocessisholisticreview,whereabroad 64range of candidate qualities are considered [137]. In physics, the use of rubric-based review to facilitate such holistic reviews has been gaining traction though the Inclusive Graduate Education Network [153]. Under this approach, applicants are rated on both traditional metrics such as GPA andtestscoresaswellasnoncognitiveskillssuchasshowinginitiativeanddisplayingperseverance according to a predened rubric. Such an approach is claimed to ensure each applicant is treated fairly and biases by reviewers are checked [154], and hence, it could make graduate admissions more equitable. To our knowledge however, few studies have examined how these rubrics work in practice and whether they fulll such aims. Therefore, the goal of this paper is to empirically examine those claimsinthecontextonourdepartment'sgraduateprogram. Specically,ourpaperaddressesthree questions related to rubric-based review in our department: 1. How do faculty assign rubric scores to applicants and how do those dier between admitted and rejected applicants? 2. How do the scores assigned by faculty dier by applicant's sex? 3. Howdothescoresassignedbyfacultydierbythetypeofinstitutiontheapplicantattended? As Scherr et al. concluded in their study of graduate admissions practices in physics, many departmentsareunawareofwhatotherdepartmentsdoandhence,theymightbewillingtochange their practices if they become aware of successful practices in use elsewhere [48]. Therefore, a secondary goal of this paper is to describe alternative admissions practices in physics and how departments may apply these alternative practices to their own admissions processes. The rest of the paper is organized as follows. In Sec. 4.2, we provide an overview of holistic review, rubric-based review, and evidence from other elds about their potential for success. In Sec. 4.3, we describe how our department transitioned to rubric-based review, how we collected datarelevanttoevaluatingouradmissionsprocess,andhowweanalyzedsuchdata. InSec. 4.4,we share results that suggest our rubric does support equitable admissions practices and in Sec. 4.5, wecontextualizeourresultsandexaminehowourchoicesasresearchersmayaecttheresults. In 65Sec. 4.6 and Sec. 4.7 we examine the limitations of this study and suggest directions for future work. Finally, in Sec. 4.8, we provide recommendations for departments interested in adopting rubric-based review. 4.2 Background 4.2.1 A typical admissions process in physics WhenapplyingtoaphysicsgraduateprogramintheUnitedStates,anapplicantwilltypicallysubmit their undergraduate transcripts, general and physics GRE scores, multiple statements addressing theirbackground,priorpreparation,andresearchinterests,andlettersofrecommendation. Agroup ofphysicsfaculty,theadmissionscommittee,thenreviewstheapplicationsandoersadmissionto some of the applicants. theonestudiedinthispaper,emphasizegradesandtestscoresoverresearch,bothintermsofwhat faculty say they do [47,51] and what faculty actually do [46,75]. Yet, numerous potential equity issues emerge when admissions is focused around test scores andgrades. First,thereisevidencethatGREscoresvarybasedongenderandrace[52,105]andthe type of undergraduate university the test-taker attends [69]. When combined with the practice of usingcutoscores,whichPotvinetal. estimateatleast1in3departmentsdo,despitethecreators of the GRE and physics GRE recommending against it [47], applicants from underrepresented groups in physics may be more likely to not make the rst cut. Second, the tests themselves can be a nancial burden for students [49]. The cost to take the GeneralGREiscurrently$205inmostpartsoftheworld(andupto$255insomeregions)[1]and the cost to the take the physics GRE is $150 [155]. In addition, if the applicant applies to more than 4 programs, they must pay $27 per school to send their scores. As Owens et al. notes, some students also need to travel to a testing center, which may incur travel or lodging costs [55]. Third, grades vary by applicants' demographics and the type of university they attended. 66Whitcomb and Singh found that wealthier, continuing-generation, white students earned higher gradesandthateventhemostprivilegedracially-underrepresentedstudentsinphysicsearnedlower grades than the least privileged white students [156]. Additionally, grades are not standardized measures across universities, with students at private universities tending to earn slightly higher grades than their peers at public universities [157]. Further, evidence has not necessarily supported these metrics as useful predictors of who will earntheirPhD.Forexample,Milleretal. foundthatwhilegradepointaverageswereusefultosome degree for predicting completion, the physics GRE had limited use [52]. More recent evidence suggests that the physics GRE and undergraduate grade point average only have a relation to PhD completion because they are related to graduate grade point average, which is then related to PhD completion [56]. GivenknownissueswithtestscoresandGPA,whydoprogramscontinuetoemphasizethemover the qualitative parts of the application. Perhaps the simplest answer is that comparing numbers is quickandconvenient[46]. Amorenuancedanswermightbethatqualitativepartsofanapplication can contain substantial variability in what is addressed and these parts of an application can have their own inequities (see. Woo et al. for an overview [158]). One possible conclusion is then that all application materials have inequities, after all they are produced in an inequitable society, so what is the point of changing anything. We instead adopt a pragmatic view that some parts of the admissions process are more inequitable than others and therefore, our goal is to develop methods to minimize or eliminate inequities to the best of our ability in an inequitable society. 4.2.2 Holistic Review One possible approach to addressing inequities in the admissions process is holistic review, which Kent and McCarthy dene as \"the consideration of a broad range of candidate qualities including 'noncognitive' or personal attributes\" [137]. Here, we will use holistic review to refer to the generalprocessregardlessofwhattoolsorsystemsareusedtoconductit. Whentalkingaboutour 67department's rubric-based process or similar processes, we will use rubric-based holistic review. While the idea of holistic admissions is hardly new, its implementation is becoming more common due to both greater awareness that quantitative measures may not accurately predict successingraduateschool[159-161]andinstitutionswantingtousethemostpredictivemeasures of success in their programs [137]. In addition, professional societies such as the American Astronomical Society (AAS) have called for programs to implement \"evidence-based, systematic, holistic approaches\" to graduate admissions [138]. Using holistic review has also been claimed to lead to benecial outcomes for universities including increasing diversity and improving student outcomes (see [137]), though most ForSTEMelds,Wilsonetal. foundthatusingholistic review in a biomedical science program resulted in applicant assessments that were independent of gender, race, and citizenship status [140] and Pacheco et al. found that using a composite score that included GPA, test scores, research experience, and publications was correlated with earning a university fellowship and a shorter completion time while applicant's test scores and GPAs individually were not [164]. Whileholisticreviewshowspromise,programsmayhaveconcernsaboutimplementingit. For example, common concerns include limited faculty time to review applications, a lack of data correlating admissions criteria and student success, and limited resources to implement it [137]. In addition, there may be concerns that because the decisions can be more subjective than using a quantitativemeasurelikeatestscore,theremaybevariabilitybasedonwhoreviewstheapplication. However, a study of holistic admissions at the undergraduate level found that only 3% of reviews showed substantial variability in the overall score between reviewers [165], suggesting that in practice, variability in the overall rating between reviewers is exist. For example, in higher education admissions, Sedlacek proposed eight noncognitive traits, which he denes as thingsnotmeasuredbystandardizedtests: positiveself-concept,realisticself-appraisal,understands andknowshowtohandleracism(thesystem),preferslong-rangetoshort-termorimmediateneeds, availability of strong support person, successful leadership experience, demonstrated community service, and knowledge acquired in or about a eld [169]. In terms of their utility, noncognitive skills have been found to be predictive or correlated with academic success, though these studies have happened outside of the context of physics. At the undergraduate level, noncognitive skills in isolation and in concert with test scores have been foundtobemorepredictiveofsuccessandgraduationthantestscoresalone[170-172]. Likewise, at the graduate and professional levels, noncognitive skills have been found to be correlated with GPA and class rank [173,174], clinical performance [175], and overall success in programs [176,177] but were not found to be associated with doing well on a licensing exam [178]. Of the individual noncognitive skills, conscientiousness has been found to be mostly strongly and consistently associated with academic success [179]. In addition to their benets related to academic success, noncognitve skills can be useful for promotingequityinadmissions. Forexample,includingnoncognitiveskillscanincreasediversity without harming validity [180,181] as noncognitive measures have been shown to be just as valid formajoritizedandminoritizedgroups[180,182,183]. Whileincludingnoncognitiveskillsaspart 69ofadmissionsmayseemlikeahardaskoffaculty,manyfacultyalreadyacknowledgetheusefulness of noncognitive skills in Whileapplicant self-reportsorrecommenderratingsaretypicalapproaches,suchmethodsmayresultininflatedor skewedratings[180]. Arecentstudysuggeststhatevensharingdescriptionsofnoncognitiveskills andwhytheyareusefulforpredictinglatersuccesscanarticiallyinflatejudgements[184]. Thus, how best to measure such skills is still an active area of inquiry [185]. 4.2.2.2 Rubric-Based Review One promising approach to implementing holistic review is rubric-based review. Under this approach, applicants are evaluated based on a set pre-dened criteria. By pre-selecting criteria, whatisrequiredforadmissioniscleartoreviewersandprovidesastructuretoassessallapplicants [46,154]. Thisexplicitnesshasbeenshowntoenhancebothvalidityandreliability[158,186,187]. In addition, rubrics can help make the admissions process more equitable [46]. By explicitly layingoutthereviewcriteriaandwhatisrequiredtoachieveeachleveloftherubric,allapplicants can be judged fairly and individual reviewer's expectations can be mitigated [183]. From research into other areas of academic hiring, we know that gender and racial biases exist in the hiring process, including in physics [188,189]. Specically in graduate admissions, faculty, including astronomy and physics faculty, have been documented showing preferences to applicants with similar backgrounds as themselves or within the same research subeld of their discipline [46]. Thus, rubrics oer a possible route to counter those biases. Indeed, a recent study in admissions for a psychiatry residency program found that using rubric-based holistic review led to more underrepresented applicants receiving an oer to interview compared to the traditional approach [190] while a recent study of grade-school writing found that teachers rated writing attributed to a Black author lower than when it was attributed to a white author but did not nd the eect when the teachers were instructed to use a clearly dened rubric [191]. As rubric-based approaches to admission are still relatively new, best practices are still in 70development. Yet, a few recommendations do exist [154]. First, criteria should be selected before reviewinganyapplicationswithindividualprogramsdecidingwhatqualitiesarecriticalforsuccess in their program [138]. Second, rubrics should be coarse-grained in that there are fewer possible scores for each construct such as low, medium, or high instead of 1-10 to limit disagreements over scores [183]. Third, each level of the rubric should be clearly dened so that a reviewer can easily determinewhichscoreanapplicantshouldgetoneachconstruct. Theselevelsshouldbepickedso thateachpossiblescorewillbereceivedbymanyapplicants[154]. Finally,thesecriteriaandlevels shouldallowfordiverseformsofexcellencetobecountedasachievementssothatapplicantswith non-traditional markers of excellence are not excluded [192]. Whilerubric-basedapproacheshavereceivedlittleresearchinphysics,theyhavebeensuccess- fully incorporated into larger physics graduate program initiatives. Two of the most well-known initiativesaretheFisk-Vanderbiltprogram,whichgraduatesoneofthelargestclassesofBlackPhD physicists in the nation [193], and the APS Bridge Program, which has successfully admitted and retained graduate students of color at rates higher than the national average [143]. Even though rubrics in admission were one of many changes made, these programs suggest that rubric-based review has promise. For a more in depth review about equitable admissions practices in STEM doctoral programs, we refer the reader to Roberts et al. [194] 4.3 Methods 4.3.1 Our Rubric and Applicant Evaluation Process In 2018, the Department of Physics and Astronomy at Michigan State University introduced a rubric-based approach to evaluation applications to the graduate program in Physics, informed by the Council of Graduate Schools' 2016 report on Holistic Review in Graduate Admissions [137]. The main goal was to improve the identication of strong candidates for the program and to make theselectionmoreequitable,therebyincreasingtheparticipationofstudentsfromunderrepresented groups in the department. In preparation for the introduction of the rubric, Casey Miller and Julie 71Posselt,theInclusivePracticeHubDirectorandResearchHubDirector,respectivelyoftheNational Science Foundation supported Inclusive Graduate Education Network [153], led a workshop with faculty who served at that time in the Graduate Recruiting Committee. This workshop resulted in a selection of ve rubric categories, which each had several sub categories. Applicants are ranked with a score of either 0, 1, or 2, corresponding to low, medium, or high, for each subcategory, based on dened criteria for each score. The subcategory scores are then averaged per category and category scores summed (with weights as given below) to calculate the overall score. The categories, with subcategories in parenthesis, are: Academic Preparation, with a weight of 25% (Physics coursework, math coursework, other coursework, and academic recognition and honors) Research, with a weight of 25% (Variety and duration, quality of work, technical skills, and research disposition) Non-cognitivecompetencies,withaweightof25%(Achievementorientation,conscientious- ness, initiative, and perseverance) Fit with program, with a weight of 15% (Fit with research programs of the department, t to research programs of specic faculty, (prior) commitment to participation in the depart- ment/school community, and advocacy for and/or contributions to a diverse, equitable, and inclusive physics community GRE scores, with a weight of 10% (General GRE scores, and Physics GRE scores) /one.sup The choice of these categories and subcategories was based on the discussions in the workshop and advice from the workshop leaders, and included considerations based on experiences during previous recruiting cycles. Another consideration for the choice of the categories is a reasonably closealignmentwithcriteriausedatMSUforawardingfellowshippackagestostudents. Therefore, the rubric scoring can also be used for selecting nominations for university fellowships. This is 1This category was not used in 2021, and will not be used in 2022 due to the impacts of COVID-19 on students ability to take these tests. 72important because fellowship nominations are due shortly after the application deadline (January 1). Applications for the graduate program are submitted to MSU's central application system. All folders with a complete or near-complete application package are reviewed. The applications are divided up into several groups, which each are reviewed by dierent members of the graduate recruitingcommittee. Thiscommitteehasarotatingmembershipwithrepresentationfromfaculty inallmajorresearchdirectionspresentinthedepartment. Committeemembersareinstructedabout the use of the rubric and provided with the criteria. As part of the review process, they also sort students by their interest in research area(s). The results from the rubric scoring are compiled by the Graduate Program Director. Students whose folders are near complete, but have a ranking for which an oer is not impossible, are contacted and ask to provide the missing information. If that additional information is provided, the rubric scoring is updated. Subsequently, the spreadsheet is used by committee representatives from each major research areainthedepartmenttomakealistofstudentstheywouldliketomakeanoertoforapositionin thatspecicresearcharea. Thenumberofstudentswhoaremadeanoertodependsonopenings available per research area, the number of teaching assistant slots available, and the historical acceptance rates for each research area. Typically, the process results in a list of oers that will be made and a wait list for additional oers that can be made if recruiting targets are not met in the initial round of oers. In this stage of recruiting process, the match to available positions is revisited as committee members from specic research areas are better aware than general faculty membersabouttherecruitingneedsforthatyear. Inspiteoftheinstructionandcriteriaprovidedto reviewers,thescoringisstillsomewhatsubjecttodierencesinreviewingstylesandinterpretation of the criteria. This is, for example, apparent in the comparison of average summed scores per reviewer. Therefore, this second stage of the review process also allows for another comparison of application based on the rubric by a few faculty members in each research area. Because of these reasons, the list of students whom an oer will be made to, or who are put on a wait list, quite closely follows the original rubric scoring but modications do occur. 73The whole process is organized and overseen by the Graduate Program Director with support from the Graduate Program Secretary. The Graduate Program Director also serves as the point of contact for questions about the use and interpretation of the rubric, reviews applications of likely candidates, and leads the selection of nominations for fellowships. Theoverallresponsefromfacultywhoservedintherecruitingcommitteeandusedtherubrichas beenpositive,asitprovidesclearguidanceforthereviewprocessandreducestheimpactofdierent reviewing styles and biases to what are the most important skills applicants to a physics graduate program should have. On average, the time spent by individual committee members on reviewing the folders has not increased. Faculty reviewers have provided feedback that it would be better if applicants are rst sorted by research area so that the review is done by several faculty from the relevant research areas in the rst step. Given the large number of applications and the limitations of the current software used to manage applications, this could not easily be accomplished in the past. MSU is implementing new software for managing and reviewing applications, which will makepresortingofapplicationsbyresearchareapossible,leadingtoaconsiderableincreaseinthe eciency of the process. 4.3.2 Participants and Data Collection Dataforthisstudycomesfromcompiledrecordsfromapplicantstoourphysicsgraduateprogram for fall 2018, 2019, and 2020. Most admissions decisions for fall 2020 had already been made before coronavirus accommodations took eect, suggesting at most minimal eects on our data. Whenapplyingtotheuniversity,applicantssubmitageneraluniversityapplication,transcripts, testscores,apersonalstatement,anacademicorresearchstatement,andlettersofrecommendation to a central system. As the current admissions system does not allow for records to be compiled across applicants, two researchers manually extracted relevant information for this study. The researchersindependentlyextracteddatafromtherst20applicationsandthencomparedresultsto ensuretheywereinterpretingtheapplicationsthesameandagreeingonanyconventionsforreport- ing the data. Afterwards, the researchers independently went through the rest of the applications. 74Throughthisprocess,theresearcherscollectedtheapplicant'sdemographics,gradepointaverage, GRE scores, degrees earned or in progress, and previous institutions attended. Any information missing from the applications or entered into the application on a non-standard scale (e.g. a GPA onanon4.0scaleoraGREscoreoutsideofthecurrentscoringrange)wastreatedasmissingdata for the analysis. As rubric scores are determined by faculty and are not part of the materials applicants submit, aggregated scores were then matched with individual applicants using the applicant IDs. Through this process, we collected data on 826 applicants, 511 of which were domestic applicants. 4.3.3 Analysis Becauseofdierentapplicationrequirementsandavailabilityofinstitutionaldataforinternational anddomesticstudents,weonlyincludedomesticstudentsinourstudy. Inaddition,weonlyinclude applications suciently complete that faculty were able to rate and were included in the Graduate Program Director compiled records, leaving us with 321 domestic applicants for this study. Forouranalysis,wewereinterestedinhowfacultyrateapplicantsandhence,wecomputedthe fraction of applicants in each level (low, medium, and high) of the rubric. In some cases (<5%), facultyusedaratingthatwasinbetweenlevels(e.g. low-medium). Becauseofthis,weperformed all subsequent analyses by rst rounding up (so low-medium would become medium) and then repeating the analysis by rounding down. First, we computed the fraction of applicants in each level of the rubric for all applicants, all admitted applicants, and all non-admitted applicants. Second,wecomparedapplicantsbasedondemographicsbycomparingthefractionofapplicants in each bin of the rubric. While gender would be more appropriate, the application system only asksapplicantsabouttheirsexandallowsthemtochoosemaleorfemale. Thuswewereonlyable to compare faculty ratings of males and females. We acknowledge that females is not the correct term to use but as being female does not automatically imply being a woman, we do not believe it is appropriate to assume that someone marking female as their sex is necessarily a woman. 75Intermsofrace,theapplicationsystemdoesnotallowapplicantstoentertheirraceorethnicity, so we are unable to compare applicants of dierent races. Finally,wecomparedapplicantsfromdierentundergraduatebackgroundsbecausepriorwork suggests the applicant's background may influence faculty's perceptions of them. For example, facultymaypreferapplicantswithsimilarbackgroundsasthemselves[46]andmayinterpretgrade pointaveragesinthecontextoftheapplicant'sundergraduateprogram,withhighGPAsfrommore prestigiousuniversitiescarryingmore\"weight\"thanahighGPAfromalesserknownschool[117]. Inaddition,graduateadmissionsinphysicshavebeencharacterizedas\"risk-adverse\"wherefaculty prefer to admit applicants who are likely to complete their program rather than take chances on someone who might not [46,48]. As students from smaller program may be viewed as higher risk if previous students from that program struggled [117], it is possible faculty may be less likely to admit students from smaller undergraduate schools. To characterize an applicant's undergraduate background, we used two measures. First, we usedBarron'svalue,whichisameasureofaninstitution'sselectivitybasedonincomingstudents' SATscores,GPAandclassrank,andoverallacceptancerates. Whilenotequivalenttoprestigious, wetreatselectivityasaproxyforprestigebasedonthatassumptionthatmostselectiveinstitutions are also prestigious institutions. For our analysis, we dened institutions with Barron's values of \"most competitive\" or \"highly competitive\" as selective and all other institutions as not selective. Second, we used the number of bachelor's degrees awarded by the physics department at the applicant's undergraduate institution to estimate the size and notoriety of the department, with the assumptionthatadepartmentthatgrantsmoredegreesismorelikelytobeknownbyanadmissions committee member. Due to variability in yearly degrees, we used the median number of degrees overthe2016-2017,2017-2018,and2018-2019academicyearsasthenumberofbachelor'sdegree awarded [3,128,129]. We then dened any program that was in the top quartile of physics bachelor'sdegreesawardedduringthatperiodasalargeprogramandallotherprogramsassmaller programs. For reference, the programs we classied as large produced nearly two-thirds of all physics bachelors degrees over the period. 76Table 4.1: Percent of Missing Data by Rubric Construct Rubric Construct Percent Missing Physics Coursework 20.0 Math Coursework 20.2 All Other Coursework 20.2 Academic Honors 22.1 Variety/Duration of Research 3.4 Quality of work 4.4 Technical Skills 4.1 Research Dispositions 4.7 Achievement Orientation 4.4 Conscientiousness 4.4 Initiative 4.0 Perseverance 4.4 Alignment of Research 7.2 Alignment with Faculty 32.1 Community Contributions 4.0 Diversity contributions 3.4 General GRE Scores 2.2 Physics GRE Score 2.5 To perform the comparisons in all cases, we used Fisher's Exact Test to examine whether the rubric score was associated with any of the metrics of interest (admission status, sex, institution selectivity, institution size). We used the standard choice of U=0\u009505to judge claims of statistical signicance. Because we did 18 comparisons for each metric of interest, it is likely that there wouldbeatleastone falsepositive. Therefore, weusedtheHolm-Bonferroniproceduretocorrect the p-values for multiple comparisons as it is less conservative than the traditional Bonferroni correction while maintaining statistical power [195]. For cases of missing data, we used pairwise deletion so that we could make the most use of the data we had. While Nissen et al recommends using multiple imputations for missing data in physics education research studies [71], the goal of this paper is to understand what faculty did as opposed to estimate a larger trend or predict an outcome. Therefore, we do not believe that using multiple imputations is aligned with the goal of this paper. The percent of missing data per rubric metric is shown in Table 4.1. 77Academic PreparationResearchNoncognitive competenciesFit contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionAll Rated Applicants (N=321)Figure 4.1: Faculty ratings of domestic applicants on 18 constructs. In the plot, a larger, darker circle means that more applicants are in that bin. While many applicants are in each level of the academicpreparationandtestscoreconstructs,fewapplicantsareinthe\"low\"binoftheresearch, noncognitive skills, and program t constructs. 4.4 Results Theresultsarelargelyunchangedbasedonwhetherweroundeduporroundeddownwhenafaculty member gave a rating in-between levels of the rubric so we present only the round up results here. When we examine the faculty's rating of all applicants in Fig. 4.1, we notice two overarching trends. First, for traditional measures of academic success such as grades and test scores, faculty tendtorateapplicantsusingallthreelevelsoftherubric. Fortheacademicpreparationconstructson contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated Admitted Applicants (N=139) A Academic PreparationResearchNoncognitive contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated Notadmitted Applicants (N=182) BFigure 4.2: Faculty ratings of domestic applicants on 18 constructs split by whether the applicant was admitted. The distribution of ratings of all constructs is statistically dierent for admitted applicants compared to non-admitted applicants. Overall, most admitted applicants were rated \"high\" while most non-admitted applicants were rated \"medium.\" Academic contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated Male Applicants (N=252) A Academic PreparationResearchNoncognitive contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated Female Applicants (N=69) B Figure4.3: Facultyratingsofdomesticapplicantson18constructssplitbywhethertheapplicantwas maleorfemale. Onlythreeoftheconstructsshoweddierencesbetweenmalesandfemales: physics GRE score where males scored higher and community contributions and diversity contributions where females scored higher. 79Academic PreparationResearchNoncognitive competenciesFit contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated Applicants from Selective Undergraduate Programs (N=108)A Academic PreparationResearchNoncognitive contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated Applicants from NonSelective Undergraduate Programs (N=104)BFigure 4.4: Faculty ratings of domestic applicants on 18 constructs split by whether the applicant attended a more selective or less selective undergraduate university. Only the general GRE and physics GRE scores showed dierences. Academic PreparationResearchNoncognitive competenciesFit contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated Applicants from Small Undergraduate Programs (N=106)A Academic PreparationResearchNoncognitive contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated Applicants from Large Undergraduate Programs (N=203)B Figure 4.5: Faculty ratings of domestic applicants on 18 constructs split by whether the applicant attended a university with a larger or smaller physics program. Only the physics GRE score and conscientiousness showed dierences between the groups of applicants, with the latter dependent on how larger physics program is dened. 80course grades, around 25% of applicants still scored in the low bin. Of the academic preparation constructs, academic honors follows a dierent structure than the others where faculty ratings are bi-modal,meaningthatapplicantseitherhadnoacademichonorsorhadmultipleacademichonors. Second, for the research, noncognitive, and t constructs, faculty rarely used the \"low\" level of the rubric, with only three of the twelve constructs in those categories having more than 10% of applicants earning a \"low.\" For research, the most common rating was \"high\" while for the noncognitivetraits,themostcommonratingvariedbetween\"high\"and\"medium.\"Intermsofthe t constructs, most applicants were rated as either \"medium\" or \"high\" for alignment of research, alignment with faculty, and community contributions. In contrast, for the diversity contributions construct, \"low\" was the most common rating, meaning that many applicants did not discuss how they promote or advocate for diversity in their applications. Whenlookingathowfacultyrateapplicantswhowouldlaterbeadmittedcomparedtoapplicants whowouldnotbeadmitted,weseestatisticallysignicantdierencesinthedistributionofallratings (Fig. 4.2). Overall, admitted applicants tended to be rated \"high\" on each construct while non- admittedapplicantstendedtoberated\"medium\"oneachconstruct. Therewereafewexceptionsto the general trend however. For academic honors, diversity contributions, and physics GRE scores, mostadmittedstudentswerenotratedas\"High\"and25%ofapplicantsreceiveda\"low\"scorewhile forallothercoursework,variety/durationofresearch,andgeneralGREscores,mostnon-admitted applicants were rated as high. When looking at the ratings broken down by sex (Fig. 4.3), we notice that the results tend to follow the overall patterns of all three ratings on academic success and test scores and mainly \"medium\" and \"high\" ratings on research, noncognitive skills, and t with the program for both males and females. Comparing ratings between males and females, we nd that only physics GRE score, community contributions, and diversity contributions showed statistically signicant dierences. While males tended to score higher on the physics GRE score, females tended to score higher on community contributions and diversity contributions. As we elaborate on in the discussion, dierences in these three constructs do not necessarily mean that faculty are rating 81males and females dierently but instead may be documenting inequities that already exist. Likewise, when looking at the ratings broken down by the selectivity of the university where the applicant earned their bachelor's degree (Fig. 4.4 ) or the size of the department where they earnedtheirbachelor'sdegree(Fig. 4.5),wemayalsobeobservingexistinginequitiesreflectedin the faculty ratings. For example, applicants from more selective universities only had statistically higherratingsthanapplicantsfromlessselectiveuniversitiesonthegeneralGREandphysicsGRE scores. Similarly applicants from larger programs had statistically higher ratings on the physics GRE score than applicants from smaller programs did. However, applicants from larger program were also rated higher on conscientiousness than applicants from smaller programs, though this result is sensitive to how we dene a large program. Whilewecouldconsiderinteractionsbetweenadmissionstatusandsex,institutionalselectivity, orphysicsprogramsize,wedidnotdosogiventhesmallsamplesizes. Forcompleteness,however, we include those plots in appendix D 4.5 Discussion How do faculty assign rubric scores to applicants and how do those dier between admitted and rejected applicants? For academic achievement and test scores, faculty tended to use all three levels of the rubric when assigning scores to applicants. In contrast, faculty tended to use mainly \"medium\"and\"high\"whenassigningscorestoapplicantsintheresearch,noncognitiveskills,and program t categories. We argue that this result is more of a reflection of the rubric than it is of how faculty are using the rubric. Giventhatgradesandtestscoresarewelldenedviatranscriptsandtestscores,rubricconstructs measuringthesetendedtousequantitativemeasurestodeterminewhichscoretheapplicantwould receive. Thatis,ahighscoreorhighgradeswouldcorrespondtoa\"high\"ratingwhilealowscore orlowgradeswouldcorrespondtoa\"lowrating. Additionally,asthecoursesrequiredforaphysics degree tend to be similar regardless of the specic program, most applicants will have taken the courses mentioned in the rubric and hence, faculty can rank applicants based on those grades. 82Incontrast,theresearch,noncognitiveskills,andtwithdepartmentarelessdenedandinstead dependonwhatapplicantswriteintheirstatementsandwhatinformationlettersofrecommendation contain. This means that not all constructs on the rubric may necessarily be addressed. For example, if an applicant takes quantum mechanics it will certainly appear on their transcript but if thatapplicantwasalsoactiveindepartmentalserviceactivities,itmaynotbereflectedinanyparts oftheapplication. Asaresult,therubricneedstotakeintoaccountthatapplicantsmaynotdisplay a trait because either they do not exhibit it or because they did not mention it. Any display of the trait could then not fall into the \"low\" level of the rubric, which would then explain why faculty tended to use only the \"medium\" and \"high\" ratings. Areasonablefollowupisthenwhethercombining\"noevidence\"with\"evidencenotpresented\" as a single level on the rubric represents an issue with the rubric. We argue that it does not, as it providesthebestoptiongiventhedatafacultyhaveavailable. Applicantsareaskedtodiscusscertain topics in their statements that map broadly onto the rubric constructs but that does not necessarily meantheywill. Whileinterviewscouldbeusefulinseparating\"noevidence\"casesfrom\"evidence not presented\" cases, we worry these would increase admissions committee members' work load. In terms of comparing admitted and non-admitted applicants, all 18 rubric constructs showed statisticallysignicantdierences. Giventhegoaloftherubricistoaidfacultyindeterminingwho toadmit,wewouldexpecttherubrictoshowsuchdierences. Thatallconstructsshowdierences suggests that all parts of the rubric are useful for determining who to admit. Howdothescoresassignedbyfacultydierbyapplicant'ssex? Wefoundonlythreeconstructs on the rubric that showed sex dierences: physics GRE score, community contributions, and diversity contributions. Given known scoring gaps on the physics GRE [52], it is not surprising that males are rated more highly than females on the physics GRE score. Given that females performlargeramountsofserviceworkinacademia[196],itisalsonotunexpectedthatconstructs measuring these would show a dierence between sexes. Because the constructs that show sex dierencesarerelatedtoeectsdocumentedintheliterature,webelievethattherubricisreflecting inequities that already exist rather than creating additional ones. Therefore, we conclude that the 83rubric is not providing an advantage to male or female applicants. Additionally,theconstructsoftherubricthatdonotshowdierencesbetweensexesalsoalign with what we would expect based on the literature. The result that physics and math GPA did not dier by sex aligns with the ndings of [156] and the result that noncognitive skills did not dier by sex aligns with the general nding that noncognitive skills do not appear to depend on demographics [180,183]. How do the scores assigned by faculty dier by the type of institution the applicant attended? Whenwecomparedapplicantsbasedonwhethertheirundergraduateinstitutionwasamoreorless selectiveinstitution,wefoundthattheonlyconstructsthatshoweddierenceswerethegeneralGRE and physics GRE scores. This result aligns with the results of our previous work investigating the physics GRE scores by undergraduate institution type [69,197]. We note that if we instead dene more-selective universities to include large state universities, such as Michigan State University, University of Colorado, Boulder, and University of Washington, our results are unchanged. This redenition is equivalent to considering Barron's values of 1-3 as more-selective and everything else as less-selective compared to the our denition of more-selective as Barron's values of 1 and 2 in the Methods and Results sections. The interpretation of the results when comparing applicants from larger or smaller physics departments is less straightforward because the results do depend on how we dene \"larger\" and \"smaller\" departments. When we dene larger programs as those that ranked in the top quartile ofphysicsbachelor'sdegreesgrantedasmeasuredbythemediannumberofdegreesawardedover the last three years of available data and rounded up in-between ratings, we nd that the physics GRE score and conscientiousness showed dierences between applicants from larger and smaller programs. However, if we rounded down on in-between ratings instead, only physics GRE score showed a dierence between applicants from larger and smaller programs. Furthermore, alternative denitions of \"larger programs\" also produced varying results. One couldalsohavereasonablydened\"larger\"tomean1)inthetophalfofphysicsbachelor'sdegrees granted as measured by the median number of degrees awarded over the last three years, 2) in 84the top quartile of physics bachelor's degrees granted as measured by the total number of degrees awarded over the last three years, and 3) in the top half of physics bachelor's degrees granted as measuredbythetotalnumberofdegreesawardedoverthelastthreeyears. Whenwealsoconsider roundinguporroundingdownin-betweenratings,wecouldmakevariouscombinationsofphysics GRE score, general GRE score, physics coursework, and conscientiousness show a statistically signicant dierence. The only rubric construct that always showed a statistically signicant dierence regardless of how we dened \"larger programs\" was the physics GRE score. Therefore, the results suggest that applicants from larger physics programs score higher on the physics GRE thanapplicantsfromsmallerprogramdo,buttheresultsareinconclusiveastowhetherotherareas oftherubricmightshowdierencesbasedonthesizeofthephysicsprogramtheapplicantattended. One area that unexpectedly did not show dierences regardless of how we dened \"larger program\" was the research section. It is often assumed that students at larger programs have more opportunities to engage in research than students at smaller programs. Yet, even if that is true, it does not appear to be reflected in the rubric scores. 4.6 Limitations Ourstudyhasthreemainlimitations. First,ourstudydoesnotincludemanydisadvantagedgroups in higher education who might not have the same opportunities as their more privileged peers and hence,mayscorelowerontherubric. Whileraceisthemostobviousduetothewayouruniversity interprets Proposal 2, our study does also not include a comparison of low-income applicants to higher-income applicants or rst generation applicants to continuing generation applicants. Additionally,thesizeofourstudydoesnotallowustoexploreintersectionsandwherepossible inequitiesmaylie. AsRudolphetal. note,usingsmallsamplesizeswithsub-groupshasinsucient statistical power and could lead to invalid inferences [138]. Hence, we refrained from performing such analyses in this paper. Second,thisstudyincludedonlyasingleprogram. Underamoretraditionalgraduateadmissions system, physics has been called a \"high consensus\" discipline [46], meaning that physics faculty 85tend to agree on what a \"quality\" applicant is and therefore, a single department's admissions process would be more or less representative of graduate admissions processes in physics. When switching to rubric-based admissions, we cannot necessarily make that same claim. As our rubric was created based on what faculty value, it is not unreasonable to assume that the results would generalize to other departments that also use rubric-based admissions. However, until such processes are evaluated at other departments, we cannot make sure a claim. Third, as a result of using only one program, the applicants are likely not representative of the largerpopulation. Thedatainthisstudycomesfrom1)peoplewhoappliedtoourprogramand2) hada nearlycomplete application. Thus, ifwe considerthosewith aninterestin attendingphysics graduate school as our population, we rst selected on those who applied to graduate school, then selected on those who applied to our program, and nally selected on those who provided enough information in their applications for faculty to evaluate. At each step, we are excluding some of the larger population and thus our claims cannot necessarily be expected to hold for the larger population of potential applicants. 4.7 Future Work As notedin the limitations,our study comparedrubric scores ofmales and females andapplicants from larger or more selective programs with applicants from smaller or less selective programs. Future work could then explore how rubric-based admissions may impact other historically and currently underrepresented groups in physics such as Black, Latinx, or Indigenous applicants. Racism, and specically anti-Black racism, is still prevalent in physics [198-202] and therefore might be reflected in rubric-based admissions. While physics faculty tend to think of diversity mainly in terms of race [46], we acknowledge that diversity is broader than race and studies of equity around the rubric should also consider rst generation applicants, low-income applicants, disabled applicants, and veterans. Studies of undergraduate admissions suggest that when extracurriculars and subjective assessments of characterandtalentgleanedfromessaysandrecommendationsareaddedtotheadmissionsprocess, 86existing inequalities may increase [203] and these applicants may become further disadvantaged in the admissions process. Therefore, future work should ensure that rubric-based admissions do increase equity rather than just use a new tool to perpetuate existing inequities. Second, future work should examine how the use of rubrics may aect what parts of an application drive the admissions process. In our prior work, we found that the physics GRE and grade point average were the main drivers of the admissions process [65]. Given the rubric is designed to emphasize more than just grades and test scores, we would hope to see these factors deemphasizedundertherubricsystem. Sucharesultwouldsuggestthattherubricisfundamentally changing how faculty are reviewing applicants. We explore whether that is the case in Chapter 5. Third, future work could examine how rubric-based admissions may change the type of ap- plicant admitted and student outcomes. Faculty skeptical of holistic admissions may worry that by deemphasizing grades and test scores, their program is admitting less academically prepared students. Future work can explore if these fears have any merit. Research at the undergraduate level on holistic admissions has found that adding noncognitive traits increased graduation rates, especially among those from disadvantaged backgrounds [204]. At the graduate level, a study of amaterialsscienceandengineeringprogramfoundthatafterchangingtheiradmissionstoinclude noncognitive skills, their incoming students won more university fellowships, though the authors cautioned they could not attribute the increase in fellowships solely to their changes in admis- sions [205]. Thus, evidence from outside of physics suggests that these fears may be unfounded, but we will not know for sure until physics specic studies are conducted. Additionally, future work can examine noncognitive skills in physics more broadly. Physics hasbeencharacterizedasabrilliance-dominatedeld[59]andhence,itisnotsurprisingthatmost studiesofsuccessinphysicshavealsofocusedoncognitivemeasuressuchasgrades,examscores, and standardized test scores. While such studies could be useful at all levels of physics, studies at the graduate level are especially importance given the limited number of studies exploring their useful for predicting success in graduate school. [138]. Finally, future work around equity in graduate admissions should investigate who is invited 87to apply to graduate school in the rst place, what barriers those who do not apply but wish to encounter, and how those barriers may be removed. In previous work, Cochran et al. investigated what barriers applicants to physics graduate school through the APS Bridge Program perceived, ndingthatGREscores,lackofresearchexperience,lowGPA,programdeadlines,andapplication costs were common concerns [49]. Unless we also work to make the application process more equitable, making the evaluation process more equitable will not result in large-scale changes in equity at the graduate level. 4.8 Recommendations for Departments Theresultsofthisstudysuggestageneralrecommendationtoimplementrubricsinphysicsgraduate schooladmissions. Rubricscanaidreviewingapplicationsbystandardizingtheprocessandlimiting bias and using rubrics does not appear to increase the time to review applications. Of course, simply using a rubric will not result in changes unless it is implemented well. We therefore propose three more specic recommendations. First, we recommend that admissions committees have multiple members review each appli- cation. For a well-constructed rubric, there should be limited uncertainty as to what rating an applicantwillreceive. However,forconstructsthataremoresubjectiveinnature,facultymayhave diering opinions about what counts as achieving each level. For example, for the quality of work construct on our rubric, what counts as \"making signicant contributions to the project\" might vary based on the reviewer. Therefore, having multiple reviewers can reduce potential bias when reviewing applications. Second, following the call of others [194,206,207], we recommend that members of the admissions committee should be of diverse backgrounds and representative of the applicant pool. To accomplish that, departments might also consider adding non-tenure stream faculty, post-docs, and current graduate students to their admissions committees, providing appropriate recognition and compensation as necessary. Prior work has shown that faculty may prefer to admit applicants like themselves [46] and therefore, a representative admissions committee is needed to ensure that 88minoritized applicants are given equal consideration. Finally, we recommend that departments conduct regular self-studies of their graduate admis- sionsprocessesandsharetheresults. WhileRudolphetal. havepreviouslycalledfordepartments toconductself-studiesoftheiradmissionsprocess[138],webelieveitisequallyimportanttoshare the results of those self-studies so that the physics community can know what is and what is not working. This collective knowledge of what is working and what is not working can then be used by all to improve graduate admissions in physics for everyone. For the sharing of results to be impactful however, the results must be easy to access and easy to understand. While individual departments could post their results on their websites, we believe doing so adds an extra layer of complexity and makes the results harder to access. Instead, we advocate for a centralized system to be created so that departments can easily report their data in a standardized way and practitioners can easily see and compare results across programs. Such a system could be maintained by professional societies such the American Physical Society or the American Institute of Physics, or other organizations. A system like this has been designed for research-based assessments [208], but to our knowledge, there exists no such system for graduate admissions. However, when conducting such self-study of what is working well and what is not working well, it is important to consider the question of \"working well for whom?\". As Razack et al. note, \"working well\" depends on one's social positioning [192] and therefore, a change that works well forapplicantsofonebackgroundmaynotbeworkingforapplicantsofadierentbackground. By considering the \"for whom?\", the physics community can ensure that changes made are for the benetofallratherthanasnewmethodstocontinuetheexistingexclusionarypracticesingraduate admissions. 4.9 Conclusion Inthispaper,wedemonstratedthatrubric-basedadmissionsareapromisingavenueforincreasing equity in graduate admissions. We showed that faculty ratings of applicant's grades, research 89experiences, and noncognitive abilities do not dier based on the applicant's sex or undergraduate background. The dierences we did observe in faculty ratings could be explained as observing known systematic issues in physics regarding test scores and service work expectations. Based on the results of this study, we recommend that departments use rubric-based holistic review for their graduate admissions process. Multiple people should review each application and those people should be representative of the applicant pool to limit any bias in the review process. Finally, departments should engage in self-study to see how their graduate admissions process is working and share those results so that the physics community can collectively learn what is working and what is not working in making graduate admissions more equitable. 90CHAPTER 5 A \"NEW APPROACH\" OR THE SAME APPROACH IN NEW PACKAGING? Thischapterisbeingdraftedasajournalarticle. TheworkingmanuscriptversionincludesMarcos D. Caballero as the second author. Following the Contributor Roles Taxonomy (CRediT) [76], my rolesforthisprojectincludeconceptualization,formalanalysis,methodology,software,validation, visualization, and writing the original draft. 5.1 Introduction Physics departments are increasingly interested in making graduate admissions more equitable and rubric-based holistic review has been gaining traction as a possible route to do so. Unlike the traditionalapproachthatemphasizestestscoresandgrades(seeChapter2),rubric-basedadmissions extend the criteria of interest to include noncognitive competencies, t with the department and researchaccomplishments. Intheory,allapplicantsareevaluatedonthesamesetofexplicitcriteria so the process should be more fair and there should be less bias [183]. InChapter4,weintroducedourdepartment'sapproachtorubric-basedadmissions. Theresults suggest that our rubric is equitable among men and women and with respect to applicants from smallerorselectiveschools. However,justbecausetherubricismoreequitabledoesnotmeanthat itmadeouradmissionsprocessmoreequitableorevenchangedthefactorsthatdriveouradmission process. That is, the rubric might just be a new tool to do the same process. Thegoalofthischapteristhentogobeyondcomparingrubricscoresfordierentapplicantsand instead, consider the admissions process as a whole. Specically, we ask how did the introduction of the rubric change our program's admissions process? To operationalize that question, we ask two research questions: 1. How do admissions models before and after the implementation of the rubric dier in terms predictive ability and meaningful features when our models are based on the data contained 91in applications? 2. How does using the data produced by faculty when rating applicants using the rubric aect our ability to create admissions models? Toanswerthesequestions,wecompareadmissionsmodelsofthecurrentprocessusingdatafrom bothfacultyratingsandtheapplicationstothemodelswegeneratedinChapter2oftheprogram's initial process. From Fig 2.3, we notice that there are cases where applicants have similar physics GREscoresandGPAyetoneapplicantisacceptedwhiletheotherisnot. Giventhatcasessuchas these mightadd additional challenges tomodeling the data,removing such applicantsmight allow us to better characterize the general trends in the data. We therefore consider a new approach that detectssimilarapplicantswithdierentadmissionsoutcomesandremovesthemfromthedataset: Tomek Links [209]. We then ask a third research question: 3. How does using Tomek Links aect our ability to model the admissions data, both before and after the implementation of the rubric? 5.2 Background While admissions committees use common criteria for initially judging applicants, deliberations of borderline applicants under the traditional process might come down to subtle distinctions that were not used for other applicants [46]. Thinking in terms of a modeling perspective, this means thatsomeapplicantsmightbeassessedaccordingtoadditionalcriteriaandhence,theseborderline applicants might not be easily classied by a general model of the admissions process. As a result, including these borderline applicants might cause our model performance to suer while excluding these applicants and instead focusing on a more typical applicant could improve model performance. Unfortunately,whetheranapplicantisaborderlineapplicantisnotincludedinfacultyratingsof applicantsandhence,wedonotknowwhoisaborderlineapplicant. Todeterminewhomightbea borderlineapplicant,letusassumethereisapredictivemodelofagraduateadmissionsprocessthat 92perfectly separates those who are admitted and those who are not admitted in some =-dimensional applicationspace. Wecouldthensaythatthoseapplicantswhoarenearthe =-dimensionalboundary that separates the admitted applicants and not admitted applicants are borderline applicants. To dierentiate borderline applicants in the admissions process from borderline applicants in the modelingprocess,wewillrefertothelatteras boundaryapplicants . Suchadenitionof boundary applicants issimilarHoensandChawla'sdenitionofborderlinecasesinclassication,whichare cases where a small change in the features would cause the classication boundary to shift [210]. However,suchanapproachassumesthatthosewhoareadmittedandnotadmittedcanbecleanly splitinsome =-dimensionalspaceandarenotintermixed. Foravarietyofreasons,anapplicantwith astellarapplicationmightberejectedoranapplicantwithaweakerapplicationmightbeadmitted and hence an admitted applicant might fall on the not-admit side of the separating boundary or viceversa. Whiletheseapplicantsmightnotbeborderlineinthetraditionalsense,theiradmission decision likely would have required deliberation and hence, might have gone through a similar processasaborderlineapplicant. Weshouldthereforealsoconsidertheseapplicantsasborderline applicants in the sense of possibility hurting our model's performance. Perhaps more accurately, we should refer to these applicants as noise applicants following Hoens and Chawla's denition of noise cases, which are case that result from random variation and not are representative of the underlying pattern [210]. Whilewehaveoperationalizedborderlineapplicantsintermsofamodelas boundaryapplicants andnoise applicants , we still need a method to determine which applicants these are before constructing any models. Tomek Links oers one possible method as it is a method of identifying the boundary or noise cases in the data [209]. To identify the Tomek Links in a data set, the distance between all cases in the data set are computed. Using the distances, the nearest neighbor of each case is computed. For two cases, e.g. case1andcase2,thecasesareTomekLinksifandonlyifcase1isthenearestneighborofcase2, case 2 is the nearest neighbor of case 1, case 1 and case 2 are of dierent classes. The only way for these conditions to be fullled is if case 1 and case 2 are boundary cases or if case 1 or case 2 932.02.53.03.54.0 500 600 700 800 900 1000 Physics GRE ScoreUndergraduate GPAOffered Admission? NoYesTomek Link? NoYesA 2.02.53.03.54.0 GRE ScoreUndergraduate GPAOffered Admission? NoYesBFigure 5.1: Plot A shows Fig 2.3 with the Tomek Links marked. Filled points represent Tomek Links. Plot B shows the same plot after the Tomek Links have been removed isanoisecase[210]. Therefore,TomekLinksallowsustoidentify boundaryapplicants andnoise applicants in our data. An example of this approach in practice is shown in Fig. 5.1. While Tomek Links have been successfully used in other contexts (e.g. see [211-213]), these approaches have tended to use data augmentation in conjunction with Tomek Links. While data augmentation approaches are valid from a modeling perspective, they might be questionable from a ethics and policy perspective. For example, altering the data set might lead to a model that is highly inaccurate of the underlying process [214]. For our data set, using data augmentation is analogoustocreatingapplicantsandthusourconclusionsabouthowouradmissionsprocessmight or might not have changed would be based on both real and imaginary applicants. For this reason, we will not use data augmentation. As we note in our methods, we do impute our data. Readers may view this as a contradiction of the previous paragraph but we view data imputation and data augmentation as dierent. Data imputationisusingtheexistingdatatollinthemissingvalues. Inthecaseofmultipleimputations, In 94contrast,dataaugmentationisusingtheexistingdatatocreatenewdataratherthanllin\"holes\"in the data. More generally, data imputation is estimating the results as if we knew the values of the missingdatawhiledataaugmentationiscreatingestimatingnewdatatosimulateabiggerdataset. 5.3 Methods 5.3.1 Data Dataforthisstudycomesfromapplicationstoourgraduateprogramtoenrollinfall2014thorough fall2020. ApplicantssubmittedgeneralandphysicsGREscores,transcripts,apersonalstatement, a research statement, and letters of recommendation. Starting for the cohort to begin our program infall2018,theadmissionscommitteeusedarubrictorateapplicantson18criteria. Thosescores are also included in our data. Further details about the data from fall 2014 through fall 2017 are discussed in Chapter 2 and further details about the data from fall 2018 through fall 2020 and the rubric are discussed in Chapter 4. For convention, Iwill refer to datacollected before the implementationof the rubric (fall2014 - fall 2017) as data set 0 following the convention of using \"naught\" for initial time in physics and datacollectedaftertheimplementationoftherubric(fall2018-fall2020)as dataset1 ,following the convention of using \"1\" to be mean the next time the thing was measured. Furthermore, data in data set 1 that comes from the applications will be referred to as the data set 1a while data that comes from the faculty ratings using the rubric will be referred to as data set 1b data. These are summarized in Table 5.1 5.3.2 Modeling To model our data, we used the cforest algorithm [92,96,99] in R [98]. As in Chapter 2, we used 70% of our data to train the model, 500 trees to build the forest andp?of the?features to construct each tree. We ran each model 30 times, selecting a new training and test set each time and averagedthe results over the runs. Foreach trial, we calculated the trainingAUC, testing 95Table 5.1: The three models compared in this chapter and the data that went into each Name Data source and features Where results are reported Data Set 0 Information pulled from the applications before our department implemented a rubric (2014-2017). Features are shown in Table 2.1Section 2.3 Data Set 1a Information pulled from the applications after our department implemented a rubric (2018-2020). Uses the same features as model 1.Section 5.4.1 Data Set 1b Rubric ratings generated byfaculty as they evaluated applications (2018-2020). Features are shown in Table 4.1.Section 5.4.3 AUC, testing accuracy, null accuracy, and the permutation AUC importances. At this stage of the analysis, missing data was handled using the default cforest procedures [101]. Fordatasets0 and1a,thesamefeatures wereusedasinTable2.1, withthesizeofthephysics program factors updated with new data for the post-data models. For data set 1b, all features were treated as categorical (0, 1, or 2) and as in Chapter 4, any values between a rubric level were rounded up. For both data sets 1a and 1b, we also varied our choice of hyperparameters to determine if our conclusionsdependedonourmodelingchoices. AsinChapter2,wesetthetrainingfractiontobe either 0.5, 0.6, 0.7, 0.8, or 0.9, the number of trees in the forest to be 50, 100, 500, 1000, or 5000, andthenumberoffeaturesusedforeachtreetobe1,p?,?\u009d3,?\u009d2,or?foratotalof125possible combinations. Each model was grown using the same procedure listed above. Toaccountforcorrelationsamongthefeatures,wealsocalculatedtheconditionalimportances for the both data sets 1a and 1b. We used the MICE algorithm with the default choices [102] to impute missing data, calculated the conditional importances, and then pooled the results using Rubin's Rules [103]. To compute the Tomek Links, we used the TomekClassif function in the UBLpackage [215]. We rst used MICE to impute the data before calculating the Tomek Links using the function 96defaultswiththeexceptionofthedistancemetrics. Followingtherecommendationofthepackage's documentation, we used the HVDMdistance for data sets 0 and 1a because those data sets contain bothcategoricalandcontinuousdataandweusedthe Overlap distancefordataset1bbecauseall features were categorical. After removing the Tomek Links, we ran each model 30 times and averaged results. Results were then pooled using Rubin's Rules. 5.4 Results 5.4.1 Data Set 1a 0\u0095720\u00060\u0095004,andtheaveragetestingAUCwas 0\u0095626\u0006\u0095006. Our null accuracy was 66\u00950%which suggests that our model is only doing slightly better than if it were to predict everyone was not admitted to our program. The low testing AUC also suggests a poor model. When looking at the feature importances (Fig. 5.2), we see that the physics GRE score, undergraduate GPA, quantitative and verbal GRE scores, and proposed research area are near the top while the institutional features near the bottom. Performing the backward elimination, we nd that physics GRE score, undergraduate GPA, quantitative and verbal GRE scores, and proposed research area are the meaningful features predictive admission after the implementation of the rubric. We can then plot the ranks of the features so that we can compare them to the ranks of the features in data set 0. The resulting slopeplot is shown in Fig. 5.3. We notice that the order of features is largely unchanged for the most predictive features, with only quantitative GRE score and GPA switching places. The major dierence between the features predictive of admission in datasets 0 and 1a is the number of meaningful features. When we take correlations among the features into account however, we the set of meaningful features shrinks. As shown in Fig. 5.4, only the applicant's physics GRE score and proposed 97Barron SelectivityAttended a public GRE scoreRegion of physics PhDYear Research over 30 trials. Physics GRE score, undergraduate GPA,QuantitativeGREscore,VerbalGREscoreandproposedresearcharea,appearinginorange, were the factors found to be meaningful and hence predictive of being admitted. Table5.2: Minimum,median,andmaximumvaluesofthemetricsobtainedoverthe125hyperpa- rameter combinations for models built from data set 1a metric min median max 0.661 0.666 research area were found to be predictive of admission. It is also important to note that the quantitativeandverbalGREscoresarerankedloweroncecorrelationsareaccountedfor,suggesting that their initial importances were inflated. Given the poor performance of our model, hyperparameter tuning might have improved the model. While it did to a degree, the testing accuracy was still only a few percentage points 98Physics GRE Score Quantitative GRE score Grade Point Average Verbal GRE score Proposed Research Area Year of applying Size of UG physics program PhD Barron Selectivity Writing GRE score Size of UG physics program, bach Region of UG program Highest physics degree offered Attended a MSI Attended a public institutionPhysics GRE Score Grade Point Average Quantitative GRE score Verbal GRE score Proposed Research Area Year of applying Size of UG physics program PhD Highest physics degree offered Size of UG physics program, bach Attended a MSI Region of UG program Writing GRE score Attended a public institution Barron SelectivityPre PostFigure 5.3: Slopeplot showing the ranks of each feature before the implementation of the rubric (left) and the after the implementation of the rubric (right) using data sets 0 and 1a respectively. Features toward the top of the plot are more predictive. Features in orange were found to be the meaningful features needed to predict whether the applicant was admitted in their respective model. Notice that the ordering of the more predictive features is largely unchanged. Plot adapted from [216]. above the null accuracy and the testing AUC was still below 0.7 (Table 5.2). Thus, even with hyperparameter tuning, the models of data set 1a were poor. Finally, to see how the feature ranks varied based on the hyperparameters, we plotted the occurrence fraction of each rank for each feature (Fig. 5.5). We notice that across the 125 hyperparametercombinations,physicsGREscoreandGPAarealmostalwaysthetoptwofeatures followedbyquantitativeandverbalGREscores. Inaddition,noneoftheinstitutionalfeaturesnever rankintheupperhalfoftheimportances. Theseresultsaresimilartowhatwefoundfordataset0 physics physics offeredAttended GRE Averaged conditional feature importances over 30 trials. Physics GRE score and proposed research area, appearing in orange, were the factors found to be meaningful and hence predictive of being admitted once correlations were accounted for. 5.4.2 Using a True Testing Set In addition to looking at the feature order to determine if the admissions process changed, we can compare the performance of the models themselves. If the process didn't change, then a model built from data set 0 should perform equally well on a data set 0 testing set as on data set 1 and a model built from data set 1a should perform equally well on a data set 1a testing set as on data set 0. If the process did change, we would expect better performance on the test data pulled from the train/test split than other data set. Whenlookingattheresults,whichareshowninFigures5.6and5.7,weseethatthelattercase UG program physics Research AreaVerbal GRE scoreQuantitative GRE scoreGrade Point AveragePhysics GRE Score 1 2 3 4 5 6 7 8 9 10 11 12 13 14 RankFeature 0.000.250.500.751.00Fraction of TrialsFigure 5.5: Proportion of the 125 hyperparameter combinations in which each feature had a given rank for data set 1a. Notice that the plot is mostly diagonal and that physics GRE score and GPA are almost always the top two features. Data Set 1aData Set 0Training 0.00 0.25 0.50 0.75 1.00 AUCData SetModel built on Data Set 0 A Data Set 1aData Set 0Training 0.00 0.25 0.50 0.75 1.00 AUCData SetModel built on Data Set 1a B Figure 5.6: Comparison of the testing AUC when A) Data Set 0 is used to train the model and B) whenDataSet1aisusedtotrainthemodel. TrainingreferstothetrainingAUCforthemodel. All error bars are 1 standard error. Results were averaged over 30 trials. 101Data Set 1aData Set 0 0.0 0.2 0.4 0.6 0.8 AccuracyData SetModel built on Data Set 0 A Data Set 1aData Set 0 0.0 0.2 0.4 0.6 0.8 AccuracyData SetModel built on Data Set 1a BFigure 5.7: Comparison of the testing accuracy when A) Data Set 0 is used to train the model and B)whenDataSet1aisusedtotrainthemodel. Thenullaccuracyisshownincyanwiththeshorter in height error bars. All error bars are 1 standard error. Results were averaged over 30 trials. nullaccuracywhilethedataset1atestaccuracyissmallerthanthedataset1anullaccuracy. These metrics suggest that the data set 0 model ts data set 0 well but does not t data set 1a well and therefore, that the process might have changed. LookingatFigure5.6B,weseethatnoneofthemetricsareespeciallygood. ThetestAUCsare both in the poor range suggesting that the model built from data set 1a does not t that well in the rst place. It is then not surprising that the model does not predict data set 0 well. Given that the initial model did not t the data well, we cannot use the result to make a claim about whether the process changed. 5.4.3 Data Set 1b Given that after the implementation of the rubric applicants are rated on the rubric constructs, perhapsusingtherubricsconstructsinsteadoftheapplicationdatainamodelwouldleadtobetter performance. Yet, that wasn't the case. We nd that the testing ImportanceFeatureFigure 5.8: Averaged conditional feature importances over 30 trials for the models of data set 1b. Physics GRE score and quality of work, appearing in orange, were the factors found to be meaningful and hence predictive of being admitted. tendedtobenotadmitted,thenullaccuracyissmallerformodelsofdataset1bthanthemodelsof data set 1a When we looked at the feature importances, the results showed similarities to the importances from the models of data set 1a. From Fig 5.8, we notice that physics GRE score is still the top feature. However, measures of GPA such as physics coursework, math coursework, and all other coursework tended to be in the lower half of the rankings, alignment of research (the closest constructtoproposedresearcharea)wastowardthemiddleoftherankings,andgeneralGREscores was toward the bottom despite GPA, proposed research area, and general GRE scores being top ranking features under the models of data set 1a. From the gure, we also notice that measures related to research (quality of work, research 103All Other CourseworkMath Averaged conditional feature importances over 30 trials for the models of data set 1b. PhysicsGREscoreandachievementorientation,appearinginorange,werethefactorsfoundtobe meaningful and hence predictive of being admitted once correlations were accounted for. dispositions,andtechnicalskills)arerankedintheupperhalfasaremeasuresofnoncognitiveskills (achievement orientation, perseverance, and conscientiousness) while measures of t (diversity contributions,communitycontributions,andalignmentwithfaculty)arerankedinthebottomhalf of features. Whenperformingthebackwardelimination,wendthatonlyphysicsGREscoreandqualityof work are selected, suggesting that only these two featuresare needed to produce similar predictive performance as using all 18 features. We then repeated the analysis taking correlations between features into account. The result is shown in Fig. 5.9. We notice that the top features are similar, though the rank of quality of work decreased to fourth. Now, physics GRE score and achievement orientation were found to be the 104Table5.3: Minimum,median,andmaximumvaluesofthemetricsobtainedoverthe125hyperpa- rameter combinations for the models of data set 1b metric min median max OrientationPhysics GRE Score 1 2 3 4 5 6 7 8 910 11 12 13 14 15 16 17 18 RankFeature 0.000.250.500.751.00Fraction of Trials Figure5.10: Proportionofthe125hyperparametercombinationsinwhicheachfeaturehadagiven rankformodelsofdataset1b. NoticethattheplotismostlydiagonalandthatphysicsGREscore, achievement orientation, and quality of work are always the top three features. meaningful and hence predictive features. Finally, we performed hyperparameter tuning to determine if we could create a model with acceptablemetrics. Unfortunately,wecouldnot. EventhebestAUCamongthe125hyperparameter tuning combinations did not exceed 0.7. The full results are shown in Table 5.3. Looking at the feature ranks, we again see a diagonal pattern toward the upper left of the plot (Fig. 5.10, suggesting the same few features are selected as the most predictive. Regardless of our hyperparameter choices, the top three features are the physics GRE score, achievement orientation, and quality of work. However, the pattern becomes less diagonal toward the bottom right, suggesting that these features are more or less noise in the model. 105Table 5.4: Metrics when using Tomek Links and MICE for each of the three data sets Data Set 0 Data Set 1a Data Set 1b Given the limited ability of the conditional inference forest to model data sets 1a and 1b, we used Tomek Links to remove boundary cases. As we were removing cases, we did not compute importancesandfocusedonthemodelmetricsinstead. TheresultsareshowninTable5.4. AsMICE generatesnewvaluesforeachimputationandhence,aectswhichcasesarenearestneighbors,the percent of cases dropped for each trial varies. First, we notice that for data set 0, using Tomek Links increased the testing AUC and testing accuracy by 0.05 over original model reported in Chapter 2. In fact, the testing AUC is now about 0.8 which is considered \"good\" as compared to \"fair\" for the original model [93]. Likewise,usingTomekLinksalsoresultsinanapproximately0.05increaseinthetestingAUC and testing accuracy for data set 1a. However, the AUC is still in the poor range and the testing accuracy is only slightly better than the null accuracy. Fordataset1b,usingTomekLinksincreasesthetestingAUCandtestingaccuracybyapproxi- mately 0.04. This time, the increase to the testing AUC is enough for the model to be classied as \"fair\". To better understand what Tomek Links were doing in the modeling process, we investigated how removing the boundary cases aected the decision boundary. In order to plot the results, we only used the physics GRE score and undergraduate GPA to make a simple model for data sets 0 and 1a. To compute the Tomek Links, we used MICE to create a complete data set rst and then found the Tomek Links. As all the data in data set 1b was categorical, a 2D plot of the decision boundary would have yielded limited insight and hence, we did not do so. The results of a single 106Figure 5.11: Plot A shows data set 0 with the decision boundary for a model with just the physics GRE score and undergraduate GPA (Fig 2.3) while plot B shows the data with the Tomek Links removed and the resulting decision boundary for the 2D model. trial are shown in Figures 5.11 and 5.12. From the gures, we see that removing the Tomek Links does aect the boundary. In the case of Figure 5.11, we see the area with limited data in the lower right switches to not admitted and in general, the overtting is reduced. In addition, the decision boundary matches closer to what we might expect anecdotally and based on Chapter 3 in that having a higher physics GRE score and GPA is more likely to result in admission as opposed to having only one of those being stellar. Likewise, in Figure 5.12, we again see reduced overtting in the decision boundary. We also see that higher physics GRE scores and GPA are predicted to result in admission as was the case before the implementation of the rubric. However, the threshold for what counts as a high physics GREscoreandGPAseemstobehigheraftertheimplementationoftherubricbasedonthedecision boundaries. 5.5 Discussion Here, we rst provide answers to our research question and then use those answers to address the larger question of whether our department's admissions process changed. 107Figure5.12: PlotAshowsdataset1awiththedecisionboundaryforamodelwithjustthephysics GRE the datawith andthe resulting decision boundary for the 2D model. 5.5.1 Research Questions How do admissions models before and after the implementation of the rubric dier in terms predictive ability and meaningful features when our models are based on the data contained in applications? Whilewewereabletomodelthedatabeforetheimplementationoftherubrictoanacceptable degree, we were unable to do so for the data after the implementation of rubric. Even after hyperparamter tuning, we were unable to achieve a testing accuracy more than a few percentage points above the null accuracy or a testing AUC above 0.7, suggesting a poor model. Intermsofthemeaningfulfeaturesfordatasets0and1a,theyweremoreorlessthesame. For data set 0 presented in Chapter 2, we found the applicant's physics GRE score, quantitative GRE score,andGPAtobethemeaningfulfeatureswhilefordataset1a,wefoundthephysicsGREscore, GPA, quantitative GRE score, verbal GRE score, and proposed research area to be meaningful. After taking correlations into account only the physics GRE score and proposed research area were found to be meaningful. However, because conditional inference forests will always return 108importancevaluesregardlessofhowwellthemodelts,weshouldinterpretthedataset1aresults with a degree of caution. Furthermore,ifthetopfeatureswerethesamebeforeandaftertheimplementationoftherubric, we would expected a model trained either data set 0 or data set 1a would work equally well on the other. Yet, that wasn't what we found. Instead, we found that the model trained on data set 0 did notpredictdataset1awellwhilethemodeltrainedondataset1adidnotpredicteitherofthedata sets well. Howdoesusingthedataproducedbyfacultywhenratingapplicantsusingtherubricaectour ability to create admissions models? While using the rubric features does result in increased metrics compared to the traditional features for the data collected after the implementation of the rubric, the metrics are still outside of the acceptable range. The testing AUC was still below 0.7 but the testing accuracy was greater than the null accuracy by a larger amount than the model created from data set 1a. However, that result may be explained by data set 1b having a less imbalanced outcome. To see if that was the case, we created a model using the data in data set 1a that corresponded to the applicants in data set 1b. When we did so, we found that the metrics were comparable, but the original test data set 1b model slightly outperformed this new model ( 0.02 increase in testing AUC and accuracy). Thus, while some of the improvement in metrics might be attributable to the more balanced data set, using the rubric constructs also provided some benet. In terms of the features, we noticed some similarities and some dierences. For the models of data set 1b, the physics GRE was still the top feature. However, measures of the GPA and general GRE scores were ranked in the lower half, suggesting they might not have been as important. Instead, measures of research ability and experience and noncognitive skills tended to be ranked towardsthetop. Againhowever,weshouldinterpretthedataset1aresultswithadegreeofcaution as the model does not t the data especially well. How does using Tomek Links aect our ability to model the admissions data, both before and after the implementation of the rubric? 109UsingTomekLinksresultedinimprovedmodelperformanceforallthreedatasets. Fordataset 0,usingTomekLinksincreasedthetestingAUCover0.8,whichisconsidered\"good,\"andfordata set1b,usingTomekLinksincreasedthetestingAUCover0.7,whichisconsidered\"fair.\"However, while using Tomek Links for data set 1a did improve the testing AUC, it did not do so enough for the model to be considered acceptable. When looking at the decision boundaries for data sets 0 and 1a with and without Tomek Links removed, we found that overtting appeared to be reduced, suggesting that even if the metrics are not largely improved, there still may be benets from using Tomek Links. Thus, while the benets were relatively small, these results suggest that Tomek Links are a promising technique for modeling PER data, especially for data sets where we expect many boundary cases or cases that go against the general trend. For example, if we were to predict who passes an introductory class, Tomek Links might allow us to remove students who earned exam scores around the minimum passing grade and thus might or might not have passed the course or anomalousstudentswhodidpoorlyonthemidtermsbutmanagedtoearnahighgradeonthenal to pass the class. 5.5.2 Addressing whether our process changed Looking across the research questions, we can now address whether the introduction of the rubric changed our department's admissions process. Overall, the evidence points in the direction of the process changing. Intermsofevidencefortheprocesschanging,wendthatthemodelsofdatasets1aand1bdo not t the data well. As we were able to t the data set 0 models to an acceptable degree using the conditional inference forest algorithm butnot the models of data sets 1a or 1b, thisresult seems to implythattheremustbesomethingdierentaboutthedatasets. Becausedataset0anddataset1a used the same features, it is hard to explain why we could model one well but not the other unless the \"true\" models of the data were dierent and hence, the admissions process changed. In addition, a model trained on data set 0 was better able to predict held-out data from data set 1100 compared to data set 1a. If the process hadn't changed, we would have expected the predictive performance to be similar. Finally,usingTomekLinkstoremoveapplicantswhomighthavegoneagainstthegeneraltrend resulted in minimal increases in the metrics for the models of data sets 1a and 1b. If the process didnotchange,wewouldexpectthatremovingapplicantswhomighthavegoneagainsttheoverall trend would have led to a better model because we were able to model the admissions data before the implementation of the rubric. Yet, that isn't what happened, suggesting again there must be something dierent about the data collected after the implementation of the rubric. 5.5.3 Limitations aecting our ability to address whether the process changed Looking at the results, it is possible that someone could instead believe they suggest the process did not change. We address those here. In terms of evidence for the process not changing, our results show that the most predictive features are similar regardless of which data set we used. When using data set 0, we found that the physics GRE, quantitative GRE, and GPA were most predictive of admission. Likewise, when looking at data set 1a, we found that the physics GRE, GPA, quantitative GRE, verbal GRE, and proposed area of research were most predictive. Using data set 1b showed the most dierences in that the measures of grades and the general GRE scores were in the lower half of the rankings. However, the physics GRE was still the top ranked feature. Yet, both models of the data after the implementation of the rubric did not have acceptable testing metrics, suggesting that we should interpret the feature importance orders with caution. Conditional inference forest models will always produce feature importances regardless of how well the model ts the data. Because the metrics to assess t are relatively poor, we should not trust the conclusion that the most predictive features are the same. However,itispossiblethatthelowmetricsmightbearesultoftheconditionalinferenceforest method not being suited for the data we have. Recent work suggests that the conditional inference forest algorithmdoes not performwell with missingdata [217]. However, whenwe used MICEto 111imputethemissingdata,themodelswerestillnotabletoproducetestingmetricsintheacceptable range, suggesting that the missing data was not the issue. In addition, while conditional inference forests were designed to better handle categorical data than traditional random forests do, there could still be issues with categorical data. For example, for data set 1b, there are only three possible values for each feature. Therefore, the model can only split each feature 3 ways, which limits the depth of the trees and the ne tuning of the model. However, when we used the section total (which could take on any integer between 0 and 8), the results did not substantially improve, suggesting that the scale of the data may not be to blame. Even if the number of categories does not matter, the fact that some of the categorical data are discretized continuous features ((e.g. physics GRE score, physics coursework)) could create problems. Priorworkhasshownthatbinningcontinuousfeaturescanleadtoalossofinformation and over- or under-estimation of eect sizes [218,219]. It is possible that such an eect is present in our data. However, models built from data sets 1a and 1b both found the physics GRE score to be the top feature even though the physics GRE score was discretized in data set 1b. Because the model metrics were not great, this rebuttal should be treated with caution. On the other low metrics. It is also possible that the low metrics are not a result of how we handled the data we had but ratherwhatdatawehad. Itispossiblethatcommitteememberswereusingsomethingnotincluded in our data to evaluate applicants and if we had that data, our models of data set 1a and 1b would improve. While such an explanation seems possible for data set 1a, it seems unlikely for data set 1b because members of the department decided what qualities they wanted to evaluate applicants on and added them to the rubric. Finally, it is possible that the low metrics might not be caused by the data or the model and instead, the low metrics could be caused by the admissions process itself. The goal of the rubric is rate applicants along multiple dimensions and hence in a holistic manner. If applicants were actually assessed holistically, we would expect that the model would not generalize well because 112there is no single underlying process. Instead there might be multiple routes an applicant could take to gain admission and hence, the model might encounter diculties modeling this process. The fact that hyperparameter tuning and Tomek Links did not increase the testing metrics to an acceptable range for models of data set 1a and barely did so for the models of data set 1b supports suchaninterpretation. However,claimingtheprocessismoreholisticbasedontheseresultsalone is premature, especially given the relatively small number of applicants in data set 1b. Instead, resultsfromothermodelingattemptswouldeitherneedtopoorpredictiveabilityorshowevidence of multiple routes to admission to support such a claim. 5.6 Future Work In order to provide better address the limitations and consider whether our admissions process became most holistic, future work should examine alternative techniques for analyzing the data. First, instead of taking a predictive approach in our analysis, we could take an explanatory approach where we try to understand what inputs may have caused the outcome. Under this approach,whetherafeatureisrelatedtotheoutcomeisdeterminedbystatisticalsignicancerather thanitspredictiveability[62]. LogisticregressionisacommonexampleofthistechniqueinPER. The results of such future work would provide greater insight into why the models did not t data sets 1a and 1b well. Second, to determine if the process is more holistic, future work could analyze the data using cluster analysis or latent class analysis. While such methods are becoming popular for analyzing learning environments (e.g. see [220,221]), to our knowledge, such methods are less common in studies of graduate admissions processes. To our knowledge, clustering-like techniques have only been used to understand admissions strategies based on surveys of faculty on admissions committees[45]. Iftheprocessismoreholistic,suchmethodsmightbeabletoidentifyclustersof applicants who were admitted for similar reasons. For example, some applicants may be admitted duetostellaracademiccredentials,othersmaybeadmittedduetotheirresearchbackground,while othersmaybeadmittedbasedonwhichfacultymembersareseekingnewstudents. Findingornot 113nding such a result would provide greater clarity as to how the process may have changed. To do so however, would likely require a larger data set, especially if there are a large number of driving results for why an applicant is admitted. Finally,futureworkcouldtakeamixedmethodsapproachbyconsideringqualitativeapproaches to investigating how our admissions process might have changed. Such qualitative approaches couldallowustoobservetheadmissionsprocessitself(similartothestudiesPosseltconductedas documented in [46]) and understand how faculty are evaluating and discussing applicants in real time. Inaddition,aqualitativeapproachwouldallowustoavoidmanyofthemodelinglimitations related to the scale of the data and metrics. Alternatively, future work could directly ask faculty who have served on the admissions com- mittee both before and after the implementation of the rubric about their perception of the process at each time. However, we must be careful of faculty's potential biases when recalling how things were done in the past (see Muggenburg for an overview [222]). For example, given the greater emphasis on diversity and equity in higher education now, faculty's recall may suer from post- rationalization [223] where they justify their decisions using reasons that weren't available at the time but are consistent with their current self image or social desirability [224] where past events events may be distorted to conform to current attitudes and norms. 5.7 Conclusion Overall, the results of this initial investigation are suggestive that our admissions process did change after the implementation of the rubric. We were able to model the data from before the implementation of the rubric to a sucient degree but not the data after the implementation of the rubric. In addition, the model of the admissions process before the implementation of the rubric does not do well predicting the data collected after the implementation of the rubric and vice versa, suggesting that the underlying process did change. However, there are still numerous limitations that need to be addressed before we can make a denitive conclusion, including how we characterize the data and how we model the data. 114Furthermore, the lack of good tting models on the data post implementation of the rubric suggeststhattheprocessmightbeholistic. Inordertomakesuchaconclusionhowever,wewould needeitherevidenceinfavorofholisticadmissionsisoccurringorstrongerevidencethatthecurrent admissions process is not easily modeled by known techniques. Such evidence could be obtained through a variety of quantitative or qualitative approaches. In terms of the modeling approaches, Tomek Links seem like a promising technique for future PER studies. While their use was not enough to provide a more conclusive answer to the question of whether our admissions process changed, their use did provide evidence that the data collected after the implementation of the rubric may be modelable to an acceptable level, leaving open the possibility that other methods may be able to model the data and hence, should be explored. Finally, to truly get a sense of whether admissions processes change after the implementation of a rubric or merely use a new tool to do the same process, studies such as these need to be completed in other physics departments. By doing so, we will have a better idea of how rubric- based admissions might change admissions processes and how well our results generalize to other programs. 115CHAPTER 6 WHY WE CAN TRUST THE RESULTS IN THE PREVIOUS CHAPTERS: A SIMULATION STUDY 6.1 Introduction When working with educational data, we often encounter imbalanced binary input and outcome features, by which we mean the variable is not equally split into its two categories. For example, demographicsinscience,technology,engineering,andmathematics(STEM)areoftenimbalanced duetohistoricalandongoinginjustices. Whiledataminingwithimbalanceddatahasbeenstudied extensively [225], less attention has been paid to the types of imbalanced data that appear in discipline-based education research (DBER) and educational data mining (EDM) studies. For example, educational data sets might consist of a single course on the order of a hundred students (and hence a hundred data points) to the entire university or even multiple universities, resulting in hundreds of thousands of data points. Further, educational data often includes contin- uous, categorical, and binary variables. As a result, an educational data set might contain many featureswithdierentimbalances. ForspecicexamplesoftheseoccurringintheDBERandEDM literature, we refer the reader to the following papers [65,226-238]. Inthecontextoflogisticregression,whichisapopularEDMtechnique[239]andwasacommon technique used in the previously cited studies, much work has focused around outcome imbalance and how to work with such data. When the outcome is imbalanced, the regression coecients and the probabilities generated from the logistic regression model are biased [240]. To correct for these biases, various techniques such as Rare Events Logistic Regression [240], Firth penalized regression [241], and introducing a log-F distributed penalty [242] have been proposed, which we explain in depth in Sec. 6.2. Morerecently,machinelearningtechniqueshavebecomepopularineducationalresearch. One example is random forest [91,239]. Just as logistic regression has biases that might be relevant to 116EDMandDBERdata,randomforestisalsoknowntohavesuchbiases. Inparticular,randomforest favors categorical features with many levels [92] and continuous features [243] when determining which features are most predictive of an outcome. Most interesting for the context of this paper is a study by Boulesteix et al. [244] building on the work of Nicodemus [243]. In their paper, they systematically varied the amount of predictive information that each binary feature contained as well as the feature imbalance and then used random forest as well as a variant better suited for categorical features, conditional inference forest, [92] to compare how well the algorithms could detect the informative features from the noise. Their key nding was that features with higher imbalances were ranked lower than features with lower imbalances even when they had the same \"built-in\" amount of predictive information. A later study [245] extended the work by including continuous features as well as binary features but only examined the case when none of the features contained predictive information. These studiessuggestthatwhenmodelingdata,ourresultsmightbemeasuringspuriouspropertiesofthe features rather than their predictive information. In this study, we seek to extend this line of work by considering the data typical of DBER and EDMstudies. Thatis,datathatincludesamixofcontinuous,categorical,andbinaryfeatureswith varying degrees of predictive or explanatory ability, and a binary outcome feature that might be imbalanced. In addition, new techniques for ranking random forest features have been developed, such as the AUC-importance [94], which are designed for imbalanced data sets and hence, might prove fruitful for DBER and EDM research. Finally, we wish to extend the work to regression techniques commonly used for educational data and explore how these biases might manifest in these techniques. Specically, we ask three research questions: 1. How might known random forest feature selection biases change when the outcome is im- balanced as is often the case in EDM and DBER studies and does the AUC-permutation importance aect those biases? 1172. How might known machine learning biases manifest in traditionally explanatory techniques such as logistic regression? 3. Howmightpenalizedregressiontechniquessuccessfullyappliedinotherdisciplinesbeused in EDM and DBER to combat any discovered biases? Itshouldbenotedthatouroverarchinggoalistocompareexistingapproachestoanalyzingdata typically found in DBER and EDM research and not to introduce our own new promising method for analyzing such data. Therestofthepaperproceedsasfollows. InSec.6.2,weprovideanoverviewofthealgorithms and approaches we mentioned in the introduction and that we use in the rest of the paper. In Sec.6.3,weexplainhowweconstructedoursimulationdataandcarriedoutourneutralcomparison simulation study [246]. In Sec. 6.4, we provide the results of our simulation study. In Sec. 6.5, we apply what we learned in the simulation study to a graduate admissions data set from United Statesuniversities. InSec.6.6,weprovideanswerstoourresearchquestions,compareourndings with similar studies, and consider how our choices might have influenced the results. In Sec. 6.7, we propose future directions for this work, both in terms of the data and algorithms. Finally, in Sec. 6.8, we provide the conclusions from our study and outline a set of recommendations. 6.2 Background Here, we introduce the two paradigms of statistical modeling and then provide an overview of the algorithms we used in our study. 6.2.1 Paradigms of Statistical Modeling When discussing modeling data, there are two prominent paradigms, both of which are used in DBERandEDM:predictionandexplanation[74,247]. Shmueli[62]providesanoverviewofthese approaches and we summarize the outcome and not necessarily the causal eect. Under this paradigm, having two sets of data, one to train the model and one to test the predictive capabilities of the model, is essential as to provide an estimate of the model's predictive ability. Because prediction is not focused on the causal eects, statistical signicance has no role in assessing features in predictive models. Instead, features are assessed based on whether they improve predictions of the model. While a feature with a small eect might be statistically signicant, it might not have predictive power because a predictive model might perform just as well without the feature as with it. Asacorollarytothis,weshouldnotexpectamodelwithhighexplanatorypowertonecessarily have high predictive power or vice versa, and hence, features with high explanatory power might not have high predictive power. 6.2.2 Explanatory Methods 6.2.2.1 Traditional Logistic Regression When the outcome, ., is binary, logistic regression is the standard technique for explanatory modeling. Under this approach, the probability, ?, of nding the outcome of .=1is given by log1? 1\u0000?=V0\u00b8V1G1\u00b8V2G2\u00b8\u0095\u0095\u0095\u00b8V=G= (6.1) 119whenG1\u0096G2\u0096\u0095\u0095\u0096G=are the input features and the Vare the coecients. Under this formula, logistic regression has a similar form as linear regression. We can rearrange the equation to solve for the odds which becomes >33B\u00b9G1\u0096G2\u0096\u0095\u0095\u0095\u0096G=\u00ba=? 1\u0000?=1\u00b9V0\u00b8V1G1\u00b8V2G2\u00b8\u0095\u0095\u0095\u00b8V=G=\u00ba(6.2) where1is traditionally the natural base, 4. Under this formulation, it makes sense to talk about the odds ratio (OR) or the change in odds as a result of increasing an input feature G9by 1 unit. More formally, $'G9=>33B\u00b9G1\u0096G2\u0096\u0095\u0095\u0095G9\u00b81\u0096\u0095\u0095\u0096G=\u00ba >33B\u00b9G1\u0096G2\u0096\u0095\u0095\u0095G9\u0096\u0095\u0095\u0096G=\u00ba=4V9(6.3) which means that the exponentials of the coecients correspond to the odds ratio for each feature. Notice that the odds ratio is independent of the value of G9. Because aVof 0 means no eect, an odds ratio of 1 is equivalent to no eect [63]. Likewise, an odds ratio greater than 1 means an increase in the odds while an odds ratio less than 1 means a decrease in the odds. An important caveat to this is what a unit increase is and what the odds ratio is in reference to. Often, continuous features are normalized so that the mean is 0 and the variance is 1 or scaled so that an increase of a unit has a tangible meaning. For example, SAT scores are only reported in multiples of 10 so scoring one point higher on the SAT is meaningless. Instead, the researcher wouldwanttoadjustthescaleofthescoressothatanincreaseof1unitcorrespondedto10points better on the test (or another meaningful increment). For continuous features, what the odds ratio is in reference to is answered by the scale choice. For categorical features, especially unordered categorical features, the answer is nontrivial. An increase of 1 unit might not be meaningful or even possible (e.g., what would an increase of 1 unit of race mean?). In that case, it is customary to use one-hot encoding and create separate, binary features for each label. For example, for race, we could create 6 features: white, Asian, Black,Latinx,Native,Multi-racial. Underthisapproachwithbinaryfeatures,anincreaseofaunit 120correspondstochangingcategories,suchasBlackcomparedtonon-Blackstudent,whichdepends on the arbitrary choice of which label is assigned G9=1and which is assigned G9=0. As [63] notes,itisoftenpreferabletoinverttheoddsratioswhicharelessthan1toeasilycompareallodds ratios, which is equivalent to swapping our label for G9=0andG9=1. 6.2.2.2 Penalized Regression When the data contains issues that might make modeling dicult (i.e., small sample size, cor- relations, and more features than data points), adding a penalty to logistic regression might be benecial. This idea is based on the bias-variance trade-o in which we can increase the bias of thecoecienttoreduceitsvariabilityorviceversa[248]. Asaresult,penalizedregressioncanbe useful for feature selection, which is often an important rst step in EDM [239]. Because logistic regression does not have a closed-form solution while linear regression does, we will present the penalized algorithms in the context of linear regression. For typical least squares linear regression with <features and =cases, we are trying to solve the expression argminV\u00b9jj.\u0000-)Vjj2\u00ba (6.4) where.is a=\u00021vector of the outputs, Vis a vector of the <\u00021vector coecients, and -is a<\u0002=matrix of the input data. When we use penalized regression instead, we add a penalty, P, that might depend on the coecients or data. argminV\u00b9jj.\u0000-)Vjj2\u00b8P\u00b9V\u0096-\u00ba\u00ba (6.5) In this study, we consider two types of penalization for explanatory methods, Firth and Log-F penalization, although many more exist. See Ensoy et al. [249] for an overview of methods often used in cases of separation, where an input feature perfectly predicts the outcome, or rare events. 121Under Firth penalization, we try to combat the asymptotic bias of the coecient estimates, which inversely depend on the sample size to some power. Specically, the Firth method adds a penaltythatremovestheasymptoticbiastoorder O\u00b9=\u00001\u00ba,makingitespeciallyusefulforsmalldata sets[241]. ItdoessobypenalizingbytheJereysinvariantprior([250],whichisinverselyrelated to the amount of information in the data. That is, the penalty is larger the less the data allows us to determine the coecients. For a simple one-feature model, the penalty is equivalent to adding 0.5 to each cell of the 2x2 contingency table of the feature and the outcome [251], making this penalization especially useful in the case of separation. In theory, this penalization should then shrink the condence intervals of the features with more imbalance because the more uncertainty would have resulted in a higher penalty. TheJereysinvariantpriorisnotwithoutissues,suchasbeingdependentonthedata,whichare summarized in Greenland and Mansournia [242]. To overcome these, Greenland and Mansournia proposed a log-F\u00b9<\u0096<\u00badistributed penalty. The penalty has a tuning parameter, m, that controls theamountofpenalizationwithahigher <providingmoreaccurateestimatesofsmaller Vbutless accurate estimates of larger V. When little is known about the data, Greenland and Mansournia recommend taking <=1to allow for a wider range of possible values. For a single parameter model, the choice <=1makes the Log-F penalty equivalent to the Firth penalty. In addition to overcoming issues with the Jereys prior, the log-F penalty can be implemented via data augmentation, meaning that any software capable of performing logistic regression can also do Log-F penalization. For a chosen <, the researcher adds <pairs of rows to their data for each feature, where one row has outcome .=1and the other has outcome .=0. In the pair of rows, the researcher then selects one feature to have value 1 and all of the other features to have values of 0, with the choice of feature unique to each pair of rows. The weights for each row are set to be<\u009d2and any intercept feature should be set to 0 in these added rows. An example of this for a 2-feature model with <=1is shown in Table 6.1. Itshouldbenotedthatdespitesimilarityinname,log-Fpenalizedregressionhasnorelationto the proposed LogCF framework [252]. 122Table 6.1: Log-F data augmentation example for a two feature and m=1 example. The last four rows are the augmented data. Outcome Feature 1 Feature 2 Intercept Weight 1 0.748 0.10 1 1 ... ... ... ... ... 1 1 0 0 1/2 0 1 0 0 1/2 1 0 1 0 1/2 0 0 1 0 1/2 6.2.3 Predictive Methods 6.2.3.1 Penalized Regression Inadditiontousingpenalizedlogisticregressionasanexplanatorymethod,therearealsopenalties designed for using regression as a predictive tool. Two of the most common are Ridge and Lasso, which are described in detail in Hastie, Tibshirani, and Friedman [248]. Again, we present the penalties in the context of linear regression. Ridge penalization adds a penalty to the regression equation proportional to the square of the Vs. argminV\u00b9jj.\u0000-)Vjj2\u00b8_jjVjj2\u00ba (6.6) Equivalently, it requires the sum of the squared Vcoecients to be less than some value. argminV\u00b9jj.\u0000-)Vjj2\u00ba (6.7) subject to<\u00d5 9=1V2 9\u0014C (6.8) Here,_, or equivalently C, controls the degree of penalization, with a higher value associated with a stronger penalty. Ridgepenalizationisoftenusedincasesofmulti-collinearitybecauseitreducesthevariability ofthecoecients. Thatis,fortwocorrelatedfeatureswithoutpenalization,onecouldbeextremely 123positiveandtheotherextremelynegativetooseteachother. Withthesquaringofthecoecients under Ridge penalization, the coecients can no longer oset each other and hence, must shrink. Mathematically, Ridge penalization is equivalent to scaling each Vby1 1\u00b8_. Instead of penalizing based on the squared V, we can penalize based on the absolute value of theV; this is the premise of Lasso penalization. Mathematically, Lassopenalization seeks to solve Again,_controls the amount of penalization. Here though, the Lasso penalty is designed for feature selection because it shrinks some Vto zero while shifting the values of the others. Lasso is not designed for correlated features, and hence, it can encounter issues in those cases. For example, if two features are correlated, either could be shrunk to zero without reducing the accuracy of the model. Therefore, Lasso can exhibit variability concerns under correlation. Onewayaroundthisistocombinethepenaltiesintoasinglepenalty,whichistheideabetween Elastic net [253]. Mathematically, the Elastic net penalty is argminV\u00b9jj.\u0000-)Vjj2\u00b8_\u00b9UjjVjj2\u00b8\u00b91\u0000U\u00bajVj\u00ba\u00ba (6.12) . where_controlstheoverallpenalizationand UcontrolstheamountofmixingoftheLassoand Ridge penalties, with the special case U=0reducing to Lasso penalization and U=1reducing to Ridge regression. 124While these algorithms are typically used for prediction, various methods for using these algorithms in an explanatory manner have been developed along with corresponding p-values or otherfeatureselectiontechniques[254-258]. Wewillonlyusethesealgorithmsaspredictivetools, but we include references to these approaches here for completeness. 6.2.3.2 Forest Methods RandomforestisanensemblemethodofdecisiontreesbasedontheClassicationandRegression Trees (CART) framework [91]. For each decision tree, a subset of features, often noted <CAH, is randomly selected and used to predict the outcome. To grow the tree, features are split into two groups with the specics of the splits determined by which ones minimize the Gini Index, a measure of variance, the most. After all trees have been grown, the algorithm uses some method ofaggregatingresults,suchasamajorityvoteofthetrees,todeterminewhattheoverallprediction is. Because the features are split, categorical features do not need to be one-hot encoded like they would in logistic regression. To determine which features are relevant to the prediction, the features are often assessed by the mean decrease in the Gini Index across all trees, with a larger value meaning the feature is morepredictiveoftheoutcome. However,Strobletal. showedthattheGiniIndexisbiasedtoward continuous features and features with many categories [92]. That is, because continuous and features with many categories have many possible split points (innite in the case of continuous), itmorelikelythealgorithmcanndanidealsplitthanthealgorithmcouldforabinaryfeaturethat has only 1 split point. Therefore, these features will be viewed as more important because they appear to better separate the classes. Asaresult,alternativemeasuressuchasaccuracypermutationimportancehavebecomepopular. Touseaccuracypermutationimportance,eachfeatureisrandomlypermutedoneatatimeandthe changeinpredictiveaccuracyisrecorded. Theideaisthatwhenafeaturethatismorepredictiveof theoutcomeispermuted,thepredictiveaccuracywilldecreasemorethanwhenafeaturewithless predictive information is permuted. As a result, the changes in predictive accuracy can be used to 125rankthefeaturesinthemodelqualitatively. Morerecently,analternativebasedontheAUC,which is the probability that the positive case ranks higher than the negative case over all possible pairs of positive and negative cases, has been proposed by Janitza et al. [94]. This AUC-permutation importance is claimed to perform better than the accuracy permutation importance measure when the outcome is imbalanced. It is important to note that both these importances only make sense in the context of the model and relative to each other. Because the Gini Index is also used to create feature splits, the entire algorithm can be biased when the data contains binary, categorical, and continuous features (as is often the case in DBER andEDM).Tocorrectthisproblem,Strobletal. proposedconditionalinferenceforests[92],which are based on the conditional inference framework [259]. Rather than minimize the Gini Index to nd ideal splits, conditional inference forests use the conditional inference independence test to determine which feature to split and how to split it. Simulation studies by Strobl et al. have found thatusingconditionalinferenceforestswithsubsamplingwithoutreplacementdoesinfact,correct the biases shown by traditional random forest [92]. For more details about these algorithms, see the appendix A. 6.3 Methodology 6.3.1 Data Creation To conduct our simulation study, we rst needed to generate our simulated data. To create binary features with varying degrees of imbalance and information, we considered a 2x2 contingency table,Table6.2. WeusedlabelingconventionssimilartothoseofOlivier,Bell,andRapalloforthe reader's convenience because we reference their formulas here [260]. Table 6.2: 2x2 contingency table of fractions feature. G9=0G9=1Total .=0c00c01c0\u00b8 .=1c10c11c1\u00b8 Totalc\u00b80c\u00b811.0 126Table 6.3: Examples of changing only one of the feature imbalance, outcome imbalance, or odds ratio for an N=1000 dataset. (a) Reference table G9=0G9=1Total .=0300 200 500 .=1300 200 500 Total 600 400 1000(b) Changing only the feature imbalance G9=0G9=1Total .=0400 100 500 .=1400 100 500 Total 800 200 1000 (c) Changing only the outcome imbalance G9=0G9=1Total .=0450 300 750 .=1150 100 250 Total 600 400 1000(d) Changing only the odds ratio G9=0G9=1Total .=0360 140 500 .=1240 260 500 cases with .=1bec1\u00b8. Then the feature imbalance is represented by the ratioc\u00b80:c\u00b81and the outcome imbalance is represented by c0\u00b8:c1\u00b8. We pickG9=1and .=1to be the minority classes, though the choice is arbitrary. To quantify the amount of information contained in a feature for predicting or explaining a hypothetical data set with 1000 samples is shown in Table 6.3. To determine the values in the 2x2 table, we can rearrange the formula for the odds ratio in terms ofc\u00b81,c1\u00b8, andc11found in the literature to solve for c11[260]. nd that c11=1\u00b8\u00b9c\u00b81\u00b8c1\u00b8\u00ba\u00b9$'\u00001\u00ba\u0000& (6.14) In the case that $'=1, that is the feature contains no predictive or explanatory information for the outcome, the expression for c11is indeterminate. In that case, the feature and outcome are independent so c11=c\u00b81c1\u00b8. Once we know c11, we can use Table 6.2 to compute the remaining values. That is c10=c1\u00b8\u0000c11 (6.15) c01=c\u00b81\u0000c11 (6.16) c00=1\u00b8c11\u0000c1\u00b8\u00b8c\u00b81 (6.17) Tomodelcontinuousfeatures,weassumedthefeatureswerenormallydistributedwithaseparate distribution for each outcome class. For .=0, we modeled the feature as N\u00b90\u00961\u00baand for.=1, we modeled the features as N\u00b9`\u00960\u00bawhere`was a parameter we controlled. By increasing `, the distributions would have less overlap, and hence, the value of a specic point would provide more information about the outcome. For our study, we choose the same feature imbalances and odds ratio as found in Boulesteix et al., which correspond to c\u00b81=f0\u00955\u00960\u00954\u00960\u009525\u00960\u00951\u00960\u009505gand$'=f3\u00961\u00955\u00961g, creating 15 binary features [244]. We then created ve continuous features with `=f0\u009575\u00960\u009550\u00960\u00960\u00960g, for a total of 20 features. As the number of features in DBER and EDM studies are on the order of 10, we choosetokeepthetotalnumberoffeaturesontheorderof10ratherthanontheorderof100asin the Boulesteix et al. study [244]. We then generated these features for ve outcome imbalances, c1\u00b8=f0\u00955\u00960\u00954\u00960\u00953\u00960\u00952\u00960\u00951g, and three sample sizes, #=f100\u00961\u0096000\u009610\u0096000gfor a total of 15 simulated data sets. A visual 128Figure 6.1: Distribution of binary features in the simulated c1\u00b8=0\u00955,#=1\u0096000model. depiction of the binary features in the c1\u00b8=0\u00955and#=1\u0096000case is shown in Fig. 6.1 and a visual depiction of the continuous features in that same case are shown in Fig. 6.2. 6.3.2 Procedures 6.3.2.1 Forest Algorithms To analyze our data set using forest algorithms, we rst randomly selected 70% of the cases for the training set and kept the remaining 30% of the data set for the testing set. Our prior work with randomforestsuggeststhatthesizeofthetrain/testsplitdidnotqualitativelyaecttheconclusions around variable importance and selected features [65]. We then used the randomForest function from the randomForest package [261] to create random forest models and the cforest function from the partypackage [92,96,259] to create conditional inference forests in R [98]. For both models, we set the number of trees to 500 as that is the default in the cforest algorithmandsimulationstudiesofrandomforesthavefoundthaterrorsratesleveloontheorder of a few hundred trees [100]. For the number of features per tree, we pickedp?where?is the number of features, which is also aligned with the recommendations of Svetnik et al. [100]. We have called this <previously to distinguish from probability in the logistic model but use ?here because it is the common symbol in the random forest literature. 129Figure 6.2: Distribution of continuous features in the simulated c1\u00b8=0\u00955,#=1\u0096000model. Fortherandomforestalgorithm,wethencomputedtheGiniimportance. Fortheaccuracyper- mutation importance and the computed the AUC permutation importanceandaccuracypermutationimportance. Werepeatedthisprocedureofsplittingthedata, running the model, and calculating the importances 30 times so that the resulting distribution of the importances would be approximately normal according to the central limit theorem [262]. Next,wedeterminedtherankofeachfeaturebasedonitsaveragevalueoverthe30runs,where the feature with the largest importance value would have rank 1. This type of approach is often used in screening studies to determine relevant features, which is what we are doing here [95]. To evaluate bias in the Boulesteix et al. paper, they approached bias as the dierence from the expected value of zero in the null case and argued that bias when the features have odds ratios dierent 1 was not well dened [244]. Because we are interested in selecting features, we can createadenitionofbiasbasedontherankofthefeature. Ifaforestalgorithmisbiased,wewould expect to see that features with higher imbalance should have larger rank (i.e. be farther from 1) 130than features with identical odds ratios but smaller imbalances. Using these ranks, we can also dene bias in terms of the features detected by the algorithm. Assumingnobias,featureswithidenticaloddsratiosshouldbedetectedatthesamerate,regardless of their imbalance. To determine if a feature was detected, we adopt the convention that detected means dierent from noise. We dene detected as being ranked above the rst noise feature, which has $'=1 or`=0. We picked this convention so that it is somewhat analogous to the denition statistically signicant, which for explanatory models, is that the probability of obtaining a result at least as extreme as the result observed under the assumption of the null hypothesis is less than some threshold, typically 0.05. 6.3.2.2 Regression Algorithms Touselogisticregressioninanexplanatorymanner,wedidnotuseatrain/testsplitasthatapproach is characteristic of a predictive approach and instead, used all of the data as is customary for explanatory modeling. To create a logistic regression model, we used the glmfunction that is part of base R with the option family='binomial' to use logistic instead of linear regression. Because log-F is based on data augmentation, we also used glmfor that approach. Prior work suggests that a choice of <=1performed better than a choice of <=2and<=1is a good starting choice when nothing is known about the size of the odds ratios [242,263]. Even though we \"know\" the true values of the odds ratios because we built them in, we want to approach the problem as if it were real data and we do not have any prior information about the features. We then used the default weights of <\u009d2for the log-F model. To run the Firth penalization, we used the brglmfunction from the brglmpackage [264,265]. Per the function's documentation, the choice of plis irrelevant for logistic regression so we left it at its default value. Forallthreeapproaches,weusedthe confint functiontocomputethecondenceintervals. For the Firth penalization, 'mean'as the brglmdocumentation suggests 131itisalessconservativeapproach. Wethensaythatafeaturewasdetectedorstatisticallysignicant if zero is not in the condence interval or in the case of odds ratios instead of the raw coecients, 1 [266]. To get a sense of how the odds ratio varied based on the data, we also ran a bootstrapped simulation. Thatis,werandomlyselected80%ofthecasesandranthestandardlogisticregression, Firth penalization, and log-F penalization models on that data. We did this 10,000 times. TocreatetheLasso,Ridge,andElasticnetmodels,weagainusedatrain/testsplitbecausethese algorithms are designed for prediction rather than explanation. To align with the bootstrapping procedure, we used 80% of the cases for the training data and 20% for the testing data. Because Lasso and Ridge have a single tuning parameter, _that controls the _. Wethenusedthe glmnetfunctionto train the Lasso and Ridge models with their respective best value of _[267]. We again repeated this process 10,000 times. Finally, we used the trainfunction from the caretpackage to nd the best values of Uand _for Elastic net and create the model [268]. We again did this 10,000 times with 80% of the data used as training cases. Toanalyzethebootstrappedresultsandgeneratecondenceintervalsforthevaluesoftheodds ratios, we used the percentile bootstraps [269]. Under this approach, all of the bootstrap estimates are sorted smallest to highest. For a given U, the bootstrap condence interval Rationale FollowingthecallofBoulesteix,Lauer,andEugsterforneutralcomparisonstudiesinthecomputa- tionalsciences,weaddressthesethreecriteriaandwhywebelievewehavemettheircriteria[246]. A. The main focus of the article is the comparison itself. It implies that the primary goal of the article is not to introduce a new promising method. As stated in the introduction, we are not 132introducing a method that we have developed and the focus of our paper is on comparing dierent methods rather than showing the usefulness of a certain method. B. The authors should be reasonably neutral. We have not developed any of the algorithms or techniques used in this study and hence, we have no stake in which method might perform best. Wealsohaveexperienceusingpredictiveandexplanatorymethodsandhaveusedthesetechniques in our previous work. C. The evaluation criteria, methods, and data sets should be chosen in a rational way, Our methods and simulated data are based on a previously published simulated study, so we believe they are rational. We believe our evaluation criteria for detecting is rational because it is intuitive, objective, and based on prior approaches. We acknowledge that other approaches do exist and we address those in the discussion. 6.4 Simulation Results 6.4.1 Forest Algorithm Results WhenlookingatasubsetoftheresultsinFig.6.3,weseesimilarresultstoBoulesteixetal.[244]. That is, for the Gini importance, represented by plot A, continuous features are ranked higher than binary features regardless of whether they are noise features or not. For the permutation importances,weseethatmorebalancedfeaturestendtohavelargerimportancesthanlessbalanced features even when they have the same odds ratios. For example, when looking at Fig. 6.3D, we seethatthefeatures OR=3,60/40 andOR=3,50/50 havemuchhigherimportancesthanthe OR=3, 90/10andOR=3, 95/5 features. Similar trends are shown in plots B and C. To compare across outcome imbalances, we aggregated all 3 sample sizes and 5 outcome imbalances into a single plot for each importance method. The results are shown in Fig. 6.4. Again, the Gini importance, Fig. 6.4A, shows a preference toward continuous features and against binary features for all sample sizes and outcome imbalances. More specically, there was not a single sample size or outcome imbalance in which any of the categorical features were detected. 133Figure 6.3: Importance values for a subset of the random forest models. Feature names shown in blackwereconstructedtobeinformativewhilefeaturenamesingreywereconstructedtobenoise. PlotAshowstheN=100070/30outcomeimbalancecasewiththestandardrandomforestalgorithm and Gini importance, plot B shows the N=1000 50/50 outcome imbalance case with the standard random forest algorithm and accuracy permutation importance, plot C shows the N=100, 50/50 outcome imbalance case with the conditional inference forest and AUC-permutation importance, and plot D shows the N=10,000 60/40 outcome imbalance case with conditional inference forest and accuracy-permutation importance. For all of the permutation importances, features with less imbalancetendtohavelargerimportancesthanmoreimbalancedfeaturesforidenticaloddsratios. 134Figure6.4: Theranksoftheinformativefeaturesforthefourimportancemeasures,groupedbythe sample size and outcome imbalance. Noise features are not shown and any feature ranked below a noise feature was assigned a rank of 0. Here, a larger circle reflects a higher rank, meaning the feature was more predictive of the outcome. Overall, features with lower imbalance rank higher than features with higher imbalance for a given odds ratio and the result is not aected by the outcome imbalance or the specic permutation importance or forest algorithm used. 135Forthepermutationalgorithms,theresultsaresimilarregardlessofwhethertheaccuracy-based or AUC-based permutation method is used. Regardless of outcome imbalance and sample size, more balanced features with smaller imbalances tend to rank higher than less balanced features with identical odds ratios. This result is reflected in the plots by the decreasing dot size from top tobottominanyoftherectanglesformedbythedottedlines. Incasesofhighoutcomeimbalance, moderatelyimbalancedfeaturesmightrankhigherthanthebalancedfeature(e.g. the BinaryOR=3 features for the N=10,000 90/10 case in Fig. 6.4D), but the most rank higherthanthebalancedfeaturewiththeoddsratio. Infact,the OR=1.550/50 featurerankshigher than theOR=3, 95/5 feature for some of the models. When looking at sections of columns of the plots in Fig. 6.4, we notice that most informative features cannot be detected for #=100, regardless of which algorithm is used. In fact, only the less imbalanced OR=3features and the more predictive continuous feature can be detected and even then, that depends on the level of outcome imbalance. For the#=1000case, most of the OR=3features can be detected. However, only the less imbalanced OR=1.5features are detected in most cases. Across the three permutation-based importances,theredoesnotappeartobeaconsistentpatternforwhich OR=1.5featuresaredetected based on the outcome imbalance. For the#=10\u0096000case, nearly all of the features can be detected, with the exception being the highly imbalanced OR=1.5 95/5 feature. Again, there is not a consistent pattern as to when this featurewill notbe detectedbased onthe outcomeimbalance. While the OR=1.5, 95/5 feature is never detected in the 50/50outcome imbalance, it is sometimes detected in the 70/30and90/10 outcomeimbalancecases,makingapatterndiculttogeneralizebasedontheoutcomeimbalance. 6.4.2 Logistic regression results In addition to detecting features, logistic regression provides an estimate of the odds ratio, which can give us an idea of how accurately algorithms are modeling the built-in odds ratios. Because theoddsratiosandcondenceintervalsdeterminedetection,wepresentthoserst. Theoddsratio 136results are shown in Fig. 6.5. Fromthe#=100case,plotA,weseethatthe95%condenceintervalsformostfeaturesspan atleastanorderofmagnituderegardlessofthefeatureimbalanceoroutcomeimbalance. However, the width of the condence interval tends to increase with both increasing feature imbalance and increasing outcome imbalance. For example, for the OR=3 90/10 andOR=3 95/5 features with a 90/10outcome imbalance, the condence intervals are too wide to t on a plot that spans 6 orders of magnitude. In some cases, the width of the condence interval for a balanced feature with a highly imbalanced outcome can be comparable to a highly imbalanced feature with a balanced outcome such as OR=1.5 60/40 with a90/10outcome imbalance and OR=3; 95/5 with a50/50 outcome balance. Given the width of the condence intervals, our built-in value of the odds ratio is always contained in the condence intervals. However, when looking at the actual estimate of the odds ratio, we see varying degrees of accuracy. For some features, like OR=1.5; 50/50 , the80/20 outcome imbalance was the most accurate estimate while for OR=3; 60/40 , the60/40outcome imbalance was the most accurate imbalance. In general, there was no specic trend where the discrepancy between the estimated value and the built-in value varied with increasing feature or outcomeimbalance. Inaddition,therewasnoconsistenttrendwheretheestimatedoddsratioover- or under-estimated the built-in value. For the#=1\u0096000and#=10\u0096000cases, we notice that the condence intervals have considerably shrunk and now span on the order of a single magnitude. This is true even for most imbalancedfeatureswiththemostimbalancedoutcome. Nevertheless,thewidthofthecondence intervals still tend to increase with both increasing feature imbalance and outcome imbalance. Aswiththe#=100case,thebuilt-invaluesareincludedinthecondenceintervalsandthere is no consistent trend as to whether the estimated odds ratio over- or under-estimates the built-in value. We note that there is an exception to this for cont; noise2 and50/50outcome imbalance on the#=10\u0096000plot where the noise feature is found to have an odds ratio less than 1. Next,wecanconductananalysissimilartowhatwedidwiththeforestalgorithmsanddetermine 137Figure 6.5: Values of the odds ratios and 95% condence intervals found by logistic regression modelscomparedbyoutcomeimbalance. Ourbuilt-invalueisrepresentedbythecircledplus. Plot A isa sample sizeof size of #=10\u0096000. Condence intervals that span beyond the scale are removed from the plot. Note the log scale on the horizontal axis. 138Figure 6.6: Analog of Fig. 6.4 but using logistic regression as the algorithm and statistical signi- canceasthecriteriafordetection, U=0\u009505. PlotAusestheHolm-Bonferronicorrectiontocontrol for multiple uncorrected p-values. whichfeaturesaredetectedbylogisticregression. Here,becauseweareusinglogisticregressionin anexplanatorymanner,weusethep-valuetodeterminewhetherafeatureisdetected,withstatistical signicance meaning less than a chosen cuto, U. Because we are conducting multiple tests of statisticalsignicance,weshouldcontrolforfalsepositives. Therefore,wepresenttheresultswith and without a Holm-Bonferroni correction [195], which is less conservative than the traditional Bonferroni correction and has been used in DBER work before [270-272]. The correction is applied within each data set because for a study with real data, we would only have one data set. The results are shown in Fig. 6.6 For#=100, when we apply the Holm-Bonferroni correction, the continuous feature with the largest`is the only one to be detected and even then, only for minor outcome imbalances. If instead we do not apply any corrections, logistic regression is able to detect a few of the OR=3 features however these tend to be the ones with lower imbalances. That is, even with a generous denition of statistical signicance, logistic regression is unable to detect features with moderate odds ratios or features with large odds ratios but higher imbalances. 139For#=1000, logistic regression is able to detect both continuous features and most of the OR=3features regardless of whether we applied a correction to the p-values or not. Unlike the #=100case, we are able to detect some of the OR=1.5features though only features with lower imbalances and this depends on whether we apply a correction or not. When we apply the correction, we were only able to detect two of the OR=1.5features across any of the ve outcome imbalances, while if we did not apply the correction, we could detect ten. Finally, for#=10\u0096000, we were able to detect all of the informative features, regardless of whetherweappliedacorrectionornot. However,oneofthecontinuousnoisefeatureswasmarked asstatisticallysignicantinthe 50/50outcomeimbalanceandthe 70/30outcomeimbalancecases. One of these disappeared when we applied the p-value correction while one did not, suggesting that with enough data, random variations in the data might appear as signals. 6.4.3 Penalized regression results Given the result from Sec. 6.4.2 that most features are detected for #=10\u0096000even without correction, we chose to focus on the #=100and#=1000cases as areas where penalized regressionmightoerabenet. Togetarepresentativepictureofhowpenalizedregressionmight help, we then applied the algorithms to the 50/50,70/30,90/10imbalanced outcome data sets, representing no imbalance, medium imbalance, and high imbalance. 6.4.3.1 Condence interval approach Because Firth and Log-F penalized regression are designed for explanatory approaches, we can use them to generate condence intervals. The results for the #=100data sets are shown in Fig. 6.7 and the results for the #=1000data sets are shown in Fig. 6.8. Here, we only present the uncorrected 95% condence intervals because if we do not nd a benet on the uncorrected condence intervals, we would not nd one on the corrected versions. For the#=100case, we notice that the Firth and Log-F regressionforthe #=100datasets. PlotAshowsthe 50/50outcomeimbalance,plotBshowsthe 70/30outcome imbalance, and plot C shows the 90/10outcome imbalance. Condence intervals that span beyond the scale are removed from the plot. For higher outcome imbalance, Firth and Log-F penalizations can considerably shrink and Log-F penalized logistic regression for the #=1000data sets. Plot A shows the 50/50outcome imbalance, plot B shows the70/30outcomeimbalance,andplotCshowsthe 90/10outcomeimbalance. Forhigheroutcome imbalance, Log-F penalizations intervals. penalizations do shrink the condence interval with Log-F appearing to oer a greater benet. However, none of the shrinking makes a dierence as to whether the feature would be statistically signicant or not compared to traditional logistic regression. When we instead look at the moderately imbalanced 70/30case, we see similar results. That 142is, the Firth and Log-F penalizations appear to provide a greater benet in terms of shrinking the condence interval for features with greater imbalance, though again, the benet is not enough to change whether a feature would be detected. For the highly imbalanced 90/10case, both penalizations reduce the condence OR=3; Log-F penalization reduces the width of the condence interval by nearly 3 orders of magnitude compared to the traditional logistic regression. As in the50/50and70/30cases, the penalizations do not aect whether a feature would be statistically signicant,butthepenalizationsstilldoproducemoreaccurateestimatesofthebuilt-inoddsratios than traditional logistic regression does. Looking at the #=1000results in Fig. 6.8, we notice that the condence intervals of the penalized regression methods are similar in length to those of traditional logistic regression. This result is true regardless of the feature imbalance or the outcome imbalance. When it comes to estimating our built-in odds ratio, the penalized methods do not oer much of an improvement over traditional logistic regression. Indeed, for lower imbalanced features, all three methods tend to provide similar estimates, while for higher imbalanced features, there is no clear trend as to which method will provide an estimate closest to that of the built-in value. 6.4.3.2 Bootstrap approach In addition to only considering whether the algorithm detects a feature, we can also get a sense of what range the estimated odds ratio will fall in using the ve dierent penalization approaches. The results from the #=100data sets are shown in Fig. 6.9 and the results from the #=1000 data sets are shown in Fig. 6.10. From the#=100plot, we see that spread of the estimated values varies between the dif- ferent methods. For higher feature imbalances, traditional logistic regression and Firth penalized regression often have the widest distributions. Because Lasso shrinks the coecients to zero (or equivalently, odds ratios to 1) and Ridge reduces the variance of the estimate, these two methods 143Figure 6.9: 95% percentile bootstraps of the odds ratio for Elastic net, Firth, Lasso, Log-F, no, and Ridge penalizations on the #=100data. Dots represent the median value. Plot A shows the 50/50outcomeimbalance,plotBshowsthe 70/30outcomeimbalance,andplotCshowsthe 90/10 outcome imbalance often have the most compact distributions. Likewise, in terms of the median estimate of the odds ratio, we see variation between the methods. BecauseLassoshrinksestimatesandRidgescalesestimates,thesetwounderestimatethe built-in odds ratio. We also nd this behavior with Elastic net, which is a middle group between 144Figure 6.10: 95% percentile bootstraps of the odds ratio for Elastic net, Firth, Lasso, Log-F, no, and Ridge penalizations on the #=1\u0096000data. Dots represent the median value. Plot A shows the50/50outcome imbalance, plot B shows the 70/30outcome imbalance, and plot C shows the 90/10outcome imbalance the two. However, Elastic net often includes the built-in odds ratio within its interval even when Lasso and Ridge do not. This result is especially true for higher feature and outcome imbalances. Log-F penalization on the other hand often takes a middle ground on both estimates and distribution width. Regardless of the feature or outcome imbalance, Log-F does not consistently 145over- or under-estimate the built-in odds ratio and does not have the widest distribution of the estimates. From the#=1000results shown in Fig. 6.10, we see that the six methods tend to produce similar results for more balanced features, even at higher outcome imbalances. The exception is the Firth penalization for higher imbalance features (e.g. OR=3; 90/10 ). For these higher imbalancefeatures,theFirthpenalizationestimatescanbenearlyanorderofmagnitudelargerthan the estimates produced by other methods. As in the#=100case, we nd that Lasso and Ridge tend to underestimate penalized and traditional logistic regression do not show a consistent pattern as to whether they over- or under-predict the built-in value. 6.5 Application to Real Data In this section, we apply the results of our simulation study to a graduate admissions data set. Our data set comes from the application records of over 5,000 applicants to 6 Big Ten or Midwestern universities over a two-year period. The data includes the applicant's GRE scores, undergraduateGPA,undergraduateuniversity,demographicssuchasbinarygender,race,domestic status, whether the applicant made the shortlist, and whether the applicant was admitted to the program. Details about these features can be found in Posselt et al. [54]. We can then treat each university as a separate case study, which is an approach we have outcome imbalances for the binary features from actual graduate school admission data Feature School School 1 AdmitSchool 2 AdmitSchool 3 ShortlistSchool size and outcome imbalances are on the same scale as the data we used in our simulation study. We then selected four of the twelve possible combinations of shortlist or admit and the six programs that represent a small and medium data set with a more balanced and less balanced outcome. Specically, we modelled 60/40 and 78/22) respectively. In the initial paper using this data, Posselt et al. analyzed shortlist and admissions separately and hence, we do so here [54]. 6.5.1 Methods To analyze the real data, we used ve approaches. First, we use logistic regression and random forest with the Gini importance as they are the \"default\" methods. Based on the results of the simulation study, we then choose to use Log-F, as it performed either better or no worse than Firth, Elastic net, as it performed better than Lasso or Ridge and retains the benets of both, and conditional inference forest with the AUC importance, as all of the permutation based importance 147Table 6.5: McFadden Pseudo '2values for the explanatory models School 1 School 2 School 3 shortlist School 3 admit similarly. To mimic the simulation study and know which features were certainly noise, we added four binary noise features (imbalances of 60/40, 75/25, 90/10, and 95/5, ryNoise1, BinaryNoise2, BinaryNoise3, BinaryNoise4) and three continuous noise features. The binary features and their imbalances for the four data sets are shown in Table 6.4. To run the models, we used the same R packages as in the simulation study. However, for real data, we should be interested in how well the model ts and hence, need to include some measure of that. For the logistic regression based methods, we used the standard McFadden pseudo- '2 implemented in the DescTools package via the PseudoR2 function [273], where a good value is between 0.2 and 0.4 [274]. While other choices of pseudo- '2exist, Menard suggests that there is little reason to prefer one over another, but McFadden's might be preferable because it is intuitive [275]. Toconnecttheforestmethodswiththelogisticregressionmethods,wealsocomputedtheAUC foreachmodel,whichfollowstherecommendationofAikenetal.[74]. Todoso,weusedthe AUC function from the ModelMetrics package [276]. We interpreted an AUC of at least 0.7 as a good model [93]. For the predictive methods, Elastic net, random forest, and conditional inference forest, we used the same procedure as in the simulation study except now used a 80/20 train/test split for all methods and calculated the AUC on both the training and testing data sets. 6.5.2 Results First,wepresentthemetricsusedtoassessourmodel,whichareshowninTable6.5andTable6.6. We notice that except for school 3 shortlist, all of the pseudo '2are within the accepted range. 148Table 6.6: AUC values for the various models on the four data sets School 1 School 2School 3 shortlistSchool 3 6.11: Comparison of the odds ratio (A), Gini importance (B), and AUC-permutation importance (C) for the features in school 1. Notice that RaceLatinx has a similar odds ratio as RaceBlack and RaceMulti according to (A) but only RaceLatinx is detectable in (C). RaceLatinx is less imbalanced than RaceBlack and RaceMulti. When looking at the AUC values, we notice that the regression models outperform the forest models and in most cases, the forest models do not produce an AUC in the acceptable range. A reviewofphysicseducationresearchliteraturefoundthatlessthan10%ofpapersreportedout-of- sample metrics, so we cannot say if these results are typical for this type of data [74]. As our goal isnottomakethebestmodelbutrathertoextractfeatures,wedidnotdoanyparametertuningfor the forests. We discuss more about these metrics in the discussion. Becausetheconclusionsfromthefourdatasetsaresimilar,weshareonlytheresultsofschool1 andprovideplotsfortheotherdatasetsintheappendixforcompleteness. Theresultsofalgorithms applied to the school 1 data set are shown in Fig. 6.11. 149When looking at plot A, we notice that Log-F noticeably shrinks the condence interval for highlyskewedfeatureslike RaceBlack . Inexchangethough,theestimateoftheoddsratioisshrunk closer to$'=1for nearly all the features. Even though Elastic net is showing the percentile bootstrapped condence interval instead of the statistical condence interval, the results tend to be aligned with the other methods. That is, themedianvalueisonthesameorderofmagnitudeoftheotherestimatesandtheendpointsofthe condence interval are also on the same order of the magnitude as the other estimates. Whencomparingthedierentmethods,weseethatnoneofthethreealgorithmswouldhaveled to dierent conclusions about which features are statistically signicant or not. From plot A, the statistically signicant features would be VGRE,UGPA,RaceMulti ,RaceLatinx ,RaceBlack In the case of BinaryNoise1 , which is supposed to be a noise feature, we note that duetotherandomnatureofgeneratingthefeature,theoddsratiowassmallerthan1andhence,the algorithms appear to have detected that small dierence. When we move to plot B, we note that the continuous features are all ranked above the binary features as expected. As a result, all features except for one rank lower than the rst noise feature. Finally, when we move to plot C, we notice that only four features are detected, which is smaller than the regression approaches. Because prediction and explanation have dierent goals, we would not expect them to identify the same features. Yet, multiple approaches identifying the same features suggest that these features are in fact, distinct from noise. One interesting point to note is that we see some ranking issues based on imbalance. For example, using a 2x2 contingency table to calculate the theoretical odds ratios, RaceMulti should have an odds ratio of 2.96 while RaceLatinx should have an odds ratio of 2.25. However, because RaceLatinx has an imbalance of 80/20 while RaceMulti has an imbalance of 96/4, RaceLatinx is detected by the AUC-permutation importance while RaceMulti is not. 1506.6 Discussion Here we address our research questions and consider how our choices and approaches might have impacted the conclusions we can draw from this study. We include a summary of the advantages and disadvantages of each algorithm based on our study and prior work in Table 6.7. 6.6.1 Research Questions How might known random forest feature selection biases change when the outcome is imbalanced as is often the case in EDM and DBER studies and does the AUC-permutation importance aect those biases? When we vary the outcome imbalance as well as the feature imbalance, we still observe the same general trend as seen in Boulesteix et al. [244]. That is, features with higher imbalance are less likely to be detected compared to features with lower imbalances but the same odds ratio. In fact, the bias might become worse for high outcome imbalances because it is harder to train a \"good\" model when most of the cases have the same outcome. InoppositiontotheclaimsofJanitzaetal,[94],wedonotndtheAUCpermutationimportance to outperform the accuracy permutation importance. In fact, we nd that the AUC permutation importanceandtheaccuracypermutationimportanceperformsimilarly,regardlessoftheoutcome imbalance. Further, we did not nd any consistent dierences in terms of the features detected by either random forest or conditional inference forest even though conditional inference forest is supposed to be better suited for categorical data [92]. We also see this preference for features with smaller imbalances in the real data. For example, for school 1, we saw that the less imbalanced RaceLatinx was detected over the more imbalanced RaceBlack andRaceMulti even though the theoretical odds ratio of RaceLatinx was smaller than that of the other two features. Across the real data and simulated data, we see the expected bias with the Gini importance in advantages and disadvantages for each algorithm used in this study Method Advantages of algorithm Disadvantages of algorithm RF + Gini -Default choice for many random forest implementations-Biased in favor of continuous features, regardless of whether they are informative of the outcome or not RF + accuracy permutation importance, CIF + accuracy permutation importance, CIF+ AUC permutation importance-Can be used with continuous & categorical features -Categorical features do not need to be binarized -Comparable performance to logistic regression for feature selection without needing to check any assumptions-Ability to detect features decreases with increasing feature imbalance and outcome imbalance -Questionable performance for small N Logistic Regression -standard algorithm for classication, implemented in most software -odds ratios have a \"real-world\" interpretation-Width of condence interval increases for increased outcome and feature imbalance and can become innite in some cases Firth penalization -Able to shrink condence intervals for imbalanced features in small N situations-Not widely implemented in software -Advantages compared to logistic regression disappear for larger N Log-F penalizations -Able to shrink condence intervals for imbalanced features in small N situations -Based on data augmentation so no special software needed -Coecient estimates are similar to those of traditional logistic regression-Advantages compared to logistic regression disappear for larger N Lasso -Shrinks some coecients to zero which can be useful for feature selection-Less able to detect less informative features from noise compared to other algorithms in the study Ridge -Eective at shrinking the width of the distribution of estimated odds ratios-All coecients are scaled by the same amount and are underestimated Elastic Net -Combines the benets of Lasso and Ridge penalizations, often performing better than either approach individually-Requires hyperparameter tuning to determine the ideal amount of mixing between Lasso and Ridge 152where all of the continuous noise features outrank all but one feature. How might known machine learning biases manifest in traditionally explanatory techniques such as logistic regression? We see similar biases in logistic regression as we see in the random forestforfeatureselection. Forasamplesizeof #=100withamultiplecomparisoncorrection,we are unable to detect most features and even without correction, we can only detect low imbalance OR=3features. Theuncorrectedresultsaresimilartothoseofthepermutationimportancesforthe forest algorithms. For#=1000, we can detect most OR=3features and without correction, low imbalance OR=1.5features. Again, the uncorrected logistic regression results resemble those of the forest algorithmsbutseemtobemorealignedwiththeconditionalinferenceforestresultsthantherandom forest results. Oncewegettoalargesamplesize, #=10\u0096000,wecandetectnearlyallfeatures,justaswecan fortheforestalgorithms. However,forlogisticregression,wealsogetanoccasionalfalsepositive. Giventhesizeofthedata,itisnotunreasonablethatthelogisticregressionmodelmightbepicking up on minor dierences in the noise features which it treats as a signal. With explanatory techniques like logistic regression, we could also investigate how well they estimated the built-in odds ratio. We found that while the built-in value is almost always in the condence interval, this has more to do with the width of the intervals than the ability of the algorithms. In general, condence interval width increases with feature and outcome imbalance anddecreaseswithsamplesize. Thedecreaseinwidthasthesamplesizeincreasescorrespondsto what we would expect based on the conclusions of Nemes et al. [277]. We also observed the same general trend for the real data. Features with higher imbalances tend to have the widest condence intervals, which can span several orders of magnitude. Howmightpenalizedregressiontechniquessuccessfullyappliedinotherdisciplinesbeusedin EDM and DBER to combat any 153For explanatory methods, Firth and Log-F were found to shrink the condence intervals, especially for highly imbalanced features and highly imbalanced outcomes. While Firth can still showwidecondenceintervals,theFirthcondenceintervalswerefoundtobesmallerthanthose of traditional logistic regression. On the other hand, Log-F provided at worst similar performance to Firth penalization and for higher imbalances, seemed to shrink the width of the condence interval more than Firth penalization did. We found that both of these methods were most useful for smaller data sets, #=100, while for the medium and larger data sets, their performance was similar. When it came to the distributions of the estimated odds ratios, Log-F often showed a smaller distribution. While the results were comparable for the small data sets, for medium data sets and features with high imbalance, Firth penalization overestimated the odds ratio and had more variability. Conversely, Log-F produced more accurate and less variable distributions. For predictive methods, Lasso, Ridge, and Elastic net were only used in a bootstrap, so we cannot cannot discuss the condence interval width. We can however discuss the distribution of estimated odds ratios. ForLassoandsmalldatasets,wendthatmanyofthefeaturesareshrunktozero,especiallyfor higher imbalances. For example, even for a small, balanced sample, many of the OR=1.5features were shrunk to zero while the other methods did not treat them as consistent with noise. Elastic showed similar results although the eect was not as severe. ForRidgeandsmalldatasets,thedistributionoftheestimatedoddsratiowasoftenthesmallest for a given data set. Given that Ridge is designed to shrink the variability of the estimates, this nding is not surprising. For medium data sets, Lasso, Ridge, and Elastic net performed similarly to the other methods in terms of the distribution of estimated odds ratios. Whileourresultsgenerallyagreewithotherstudies,atruecomparisonisdicultbecauseeach studyuseditsownsubsetofthealgorithms,includingonesweusedinourstudyaswellasoneswe didnot. Therefore,whichalgorithmperformedbestandunderwhatcircumstancesdependsjustas 154much on the algorithms it was compared to as the algorithm itself. In general, other studies have tended to nd that Firth penalization does outperform logistic regressioninthecaseofoutcomeimbalance[251,278-280]andLog-Fpenalizationshowspromise when working imbalanced data and can outperform Firth-penalization [263,281]. Likewise,studieslikePavlouetal. havefoundthatRidgepenalizationworkswellexceptwhen there are many noise features while Lasso performs better when there are many noise features but limitedcorrelations,whichisconsistentwithourresultsPavlouetal.[282]. Theirstudyalsofound that elastic seemed to perform well in all cases, which generally matches what we found. In terms of our nding that none of the methods xed the issues around feature or outcome imbalance, Van Calster reported a similar nding for shrinkage techniques [283]. Specically, they found that despite working well on average, shrinkage techniques often did not work well on individual data sets, even in cases where the techniques could have provided the most benet such as in small sample size or low events per variable cases. Even though the techniques did not solve any of the issues in our study, they still showed promise for reducing the scale of the condence intervals and warrant greater adoption by the DBER and EDM communities. 6.6.2 Limitations and Researcher Choices In this section, we shift our focus from the results of the research questions and instead consider howourchoicesaroundconstructingthesimulateddata,tuningornottuningourmodels,dening \"detected features,\" and assessing the models might have impacted the conclusions we can draw from this study. 6.6.2.1 Our data sets For our simulation study, we used the same levels of information as in the Boulesteix et al. study which we wished to extend [244]. We followed their convention that $'=3corresponded to a large eect while $'=1\u00955corresponded to a moderate eect. However, Olivier noted that what constitutes a large, medium, or small odds ratio depends on the feature imbalance, outcome 155imbalance, and correlations [260]. Therefore, even though we are using the same odds ratios for the dierent imbalances, they might not necessarily contain the same amount of predictive or explanatory power in a \"large\", \"medium\", or \"small\" sense. OnenoticeabledierencebetweenourstudyandtheBoulesteixetal. studywasthenumberof features[244]. WhilewearguedthatDBERandEDMstudiesusuallyhavethenumberoffeatures ontheorderof10ratherthan100,onecouldarguethatwestillhadtoomanyfeaturesbasedonour sample size. For example, a rule of thumb is that there should be at least 10 cases of the minority outcomeforeachfeatureinthemodel,referredtoastheeventspervariable[284,285]. Inthatcase, wewouldhaveneededatleastasamplesizeof400forthe50/50outcomeimbalanceandasample size of 2,000 for the 90/10 outcome imbalance case. However, recent work has called into question whether this rule of thumb is supported by evidence [278]. Van Smeden et al found that events per variable did not have a strong relation to predictive performance of models and instead, recommended that a combination of the number of predictors,thetotalsamplesizeandtheeventsfractionbeusedtoassesssamplesizecriteria[286]. Likewise,Courvoisieretal. foundthatlogisticregressioncanencounterproblemseveniftheevents per variable were greater than 10 and concluded that there is no single rule for guaranteeing an accurate estimate of parameters for logistic regression [287]. Even if the rule of thumb were true for logistic regression, Pavlou et al. claims that penalized regression is eective when the events per variable is less than 10 [288]. 6.6.2.2 Hyperparameter tuning For our simulation study, we did not do extensive hyperparameter tuning for the forest algorithms. We did this because 1) Probst, Wright, and Boulesteix found that random forest is robust against hyperparameter specication, its performance depends less on the hyperparameters than other machinelearningmethods,anditsdefaultchoiceofhyperparametersareoftengoodenough[289] and 2) Couronn\u00e9, Probst, and Boulesteix state that for a method to become a standard tool (as random forest is in EDM and is becoming in DBER), it needs to be easy to use by researchers 156without computational backgrounds and cannot involve complex human interaction, which is not true of hyperparameter tuning [290] . In addition, we only do hyperparameter tuning for Lasso, Ridge, and elastic might not have been advisable for our data in the rst place [291]. However, for completeness and to minimize computation time needed, we did experiment with multiple choices for the number of trees in the forest, =CA44, and the number of features used for each tree, <CAH. For the conditional inference forests with the AUC importance, a sample size of #=1000and outcome imbalances of 50/50, 60/40, 70/30, 80/20, and 90/10, we tried=CA44=f50\u0096100\u0096500\u00961000\u00965000gand<CAH =f1\u0096?\u009d3\u0096p?\u0096?\u009d2\u0096?gwhere?is the total number of features in the model. We did not nd any meaningful dierences in which features were selected and no set of the hyperparameters consistently performed better than the default (=CA44=500\u0096<CAH =p?). Therefore, we used the default choices for throughout the study. 6.6.2.3 Determining Detected Features For the forest algorithms, we chose to use the simple and intuitive method of whether the feature rankedabovetherstnoisefeaturetodeterminewhichfeaturesweredetected. Wewereabletodo thisbecauseweknewwhichfeatureswerenoiseandinthecaseoftherealdata,weaddedfeatures wecreatedtobenoise. Wecouldhave,however,usedavarietyofothermethodstodetectfeatures though each has its own limitations in the context of our study. See Hapfelmeier and Ulm for an overview of dierent approaches, some comparisons, and their own novel method [292]. Ingeneral,thetechniquesforfeatureselectioninforestalgorithmsfallintotwobroadcategories. First, there are elimination techniques that pull out a subset of the features based on criteria. For example,D\u00edaz-UriarteandAlvarezdeAndr\u00e9susedarecursivebackwardeliminationtechniquethat removesacertainfractionoffeaturesuntilonly2remain[97]. Thetechniquethenselectsthemodel 157with the fewest features that performs within 1 standard error of the best model using whatever metric the researcher chooses. These type of methods are not appropriate for this study because they can restrict the features too much. That is, by having some cuto or elimination procedure, features which contain only a small amount of predictive information could be eliminated even though they are in fact predictive. The second common approach is to use some type of permutation test to generate a p-value. Underthisapproach,eithertheoutcomeoreachindividualfeatureispermutedandthenrunthrough themodeltoproducesomemetric. Thisisthendonealargenumberoftimestogetadistributionof themetric. Thentheunpermuteddataisrunthroughthemodeltogettheactualvalueofthemetric. Thep-valueisthenthefractionofcaseswherethepermutedmetricisasextremeastheactualvalue of the metric [293]. This approach has been used in various random forest studies [294,295] and has been extended into the PIMP heuristic for correcting the Gini importance bias [296]. While thesemethodsprovideananalogousmethodforcomparingwithexplanatorymethods,theycanbe computationally intensive as they require the distributions to be conducted from scratch for each model. Asawaytoreducethecomputationalcomplexity,Janitza,Celik,andBoulesteixproposedthat the negative importances, which are assumed to be noise features because they are making the predictions worse, could be used to construct a null distribution [297]. Under this approach, the distribution of the negative importances are reflected across the axis to give create the distribution forpositivevalues. Thesameprocedureasabovecanthenbeusedtocalculatethep-values. While this procedure is computationally feasible, DBER and EDM studies often have on the order of 10 features,whichmeanstherearealimitednumberoffeatureswhichcouldhavenegativeimportances and thus, mirroring the distribution would be of little use. 6.6.2.4 Assessing Our Models For our real data, we noticed that many of the models did not produce out of sample AUCs in the acceptable range of at least 0.7. Here we try to address that. 158First, we acknowledge that one type of model should not always perform better than another; this is the basis of the no free lunch theorems for optimization [298]. Various studies comparing logistic regression and random forest nd a similar result where which algorithm performs best dependsonthedataset[290,299,300]. Therefore,thefactthatlogisticregressionmodelsperform better than the forest models is not necessarily a problem. In fact, by comparing multiple models and nding that some work better than others, we can have greater condence in our results that we are detecting a signal in the data and not just modeling the noise. Second,weneedtoacknowledgethatoverttingishappeningwiththeElasticnetandconditional inference forests. This overtting can be detected by looking for dierences in the training and testing set AUCs, where a higher training AUC is characteristic of overtting. The amount of overttingseemsworseforthesmallerdatasetsasshowninTable6.6. Thisresultisnotunexpected because with smaller data sets, there are fewer cases to learn from. The noise in the model might then be seen as a signal and treated as though it contains predictive information. IfwelookattheothermodelsandtheirresultsinTable6.6,wenoticethattheforestmodelsand Elastic net perform best on the school 3 data sets, which correspond to the medium sized data sets in the simulation study and largest of the real data sets. For the forest algorithms specically, they performbestontheschool3shortlistdataset,whichhappenstohaveasmalleroutcomeimbalance than school 3 admit. This result suggests that to eectively use the predictive approach, the data setshouldnotbetooimbalancedandbasedontheresultsforschool1andschool2,theamountof data should be on the order of 1,000 cases. Additionally,thehigherAUCforlogisticregressionandLog-Fmightbethoughtofastheirown type of overtting. Due to the train/test procedure of the predictive paradigm, these two methods areworkingwiththefulldatasetratherthanjust80%ofthecases,correspondingtoa25%increase in data to work with and hence, learn from. With the \"extra\" data, these models might be better able to detect trends in the data and separate them out from noise. Whiletheredoexisttechniquesfordetectingoverttinginlogisticregression,manyofthemuse some type of testing or validation data set. For example, the Copas test of overtting recommends 159splitting the data in half, using one half of the data to develop the regression model, using that modelwiththeotherhalfofthedatatomakepredictionsoftheoutcome,andthenperformalinear regression with the predictions and actual values, testing whether the coecient is dierent from 1 [301]. If it were, that would provide evidence of overtting. However, this approach is nearly equivalent to using logistic regression in a predictive manner rather than the way it is traditionally used in DBER and EDM. For a technique that aligns the explantory nature of logistic regression, we can examine the residualplots. Because,logisticregressionproducesdiscreteresiduals,usingbinnedresidualplots instead might be helpful [302]. Under this approach, cases are divided into bins and the average value in each bin is plotted against the average residual in that bin. This approach allows the otherwisebinaryresidualtotakeonanyvalueoftheform8 =18=where=18=isthenumberofcasesin the bin andf82Z:\u0000=18=\u00148\u0014=18=g. Whenimplementedviathe armpackage[303],95%condenceintervalsaregeneratedandwe can get an idea of how good the model is by examining what fraction of the binned residuals fall within the intervals. When we do so, we nd that the fraction of residuals falling outside of the condenceintervalsarebetween0.20forSchool3shortlistand0.34forSchool3admit,suggesting the models might in fact, not t well. There does not appear to be a pattern based on the sample size or outcome imbalance. The plots are shown in Fig. E.4 in appendix E. 6.7 Future Work While we considered six approaches to logistic regression, two machine learning algorithms, and three importance measures, these are not the only approaches we could have used. Indeed, these arenoteventheonlylogisticregressionorrandomforesttechniqueswecouldhaveusedbutchose these algorithms as a starting point. Future work could then consider how other modications of logistic regression or random forest might improve upon the problems we have identied here. For example, for logistic regression algorithms, Puhr et al. proposed two modications to Firth penalization, a post-hoc adjustment of the intercept and iterative data augmentation, that 160showed promise in their simulation study [304]. Based on their results, they recommend using their methods or penalization by Cauchy Priors [305], which we did not include in this study, as better options than Log-F when condence intervals were of interest. Furthermore, a later study comparinglogisticregression,Firthpenalization,andthemodicationstoFirthpenalizationfound that the modications to Firth's method worked best in terms of parameter estimation bias for rare events and small sample cases [306]. In terms of forest algorithms, there are several variants that might be useful for the data we encounter in DBER and EDM. For example, Balanced Random Forest and Weighted Random Forest have been developed for working with imbalanced outcomes [307] and Oblique Random Forestshavebeendevelopedtoallowfordiagonalcutsinthefeaturespaceratherthanthehorizontal or vertical cuts allowed under traditional random forest algorithms [308]. In their study, Menze et al. found that Oblique Random Forests outperform traditional random forest when the data is numerical rather than discrete, which might show promise for our data depending on the ratio of numerical features to categorical or binary features [308]. Alternatively, there are non-CART-based approaches to random forest [309]. Loh and Zhou conductedasimulationstudyofvariousapproachestorandomforestandvariableimportance[310], ndingthatforestsgrownusingtheGUIDEalgorithm,whichisimplementedforbothclassication [311] and regression [312], was unbiased while the random forest and conditional inference forest approaches we used here were not. In their study, a method was unbiased \"if the expected values of its scores are equal when all variables are independent of the response variable,\" which would correspond to a case in our study where the odds ratios were 1 for all features. Nevertheless, such an approach might still be worth looking into. Therearealsonewerimportancemeasuresthatshowpromise. Inasimulationstudy,Nembrini, Konig, and Wright proposed a modication of the Gini importance, which they claim removed its bias toward features with more categories and the biases observed here regarding feature imbalance [245]. However, their simulated studies with feature importance only considered null cases in which none of the features were predictive of an outcome. Nevertheless, further study of 161this approach might be fruitful. In contrast to the algorithms used to analyze the data, future work should also explore how changes to the data itself might aect algorithm performance. For example, we could use the risk ratio to encode the level of information in a feature instead of the odds ratio. In theory, risk ratio providesamoreintuitivewaytoquantifytheamountofinformationinafeaturebecauseitisbased ontheratioofprobabilitiesratherthanaratioofodds. ZhangandYuproposedamethodtoconvert theoddsratiotoariskratio[313],thoughmorerecentworkhascalledthisapproachintoquestion and suggests alternatives [314,315]. Because these two measures are related but not the same, there might be additional insights related to which features are detected based on how we dene the amount of \"predictiveness\" they have. To better replicate real DBER and EDM data, future work could also explore how the amount ofcorrelationbetweenthefeaturesaectstheresults. Inthecaseofcorrelatedfeatures,newissues withpermutationsemerge,includingthemodelneedingtoextrapolatetoregionswherethemodel was not trained to calculate a feature importance [95]. Various approaches have been developed for correlated data with forest algorithms [95,96,316], which warrant future study, especially for the type of data we see in DBER and EDM studies. Additionally, future work can extend the data beyond binary features and include categorical features. While logistic regression often requires categorical features to be binarized, doing so can cause a loss of information. For example, treating exam responses as correct or incorrect hidesinformationaboutthespecicincorrectanswerthestudentchoseandpossiblepatterns[317] and combining demographics into a single \"underrepresented\" category can hide the struggles of students of dierent races and ethnicities [318]. As random forest implementations can often handlecategoricalfeaturesdirectly,futureworkcanconsiderhowthebiasesexploredinthisstudy mightmanifestincategoricaldataandhowaggregatingorsegregatingfeaturesmightintroduceits own biases. Finally, future work can consider how both the data and algorithms aect how features are detected. Recently, Pangastuti et al. found that a combination of bagging, boosting, and SMOTE 162improved random forest's classication ability on data set [319]. Weneedtobecarefulwithsuchapproaches,however,becauseourdataisnotjustdatabutrepresents actualstudents. Therefore,weneedtobecertainthatourconclusionsarebasedonthestudentdata and not simulated students \"created\" to make the data easier to analyze. 6.8 Conclusion and Recommendations Ourworksuggeststhatforbothpredictiveandexplanatorymodels,featureandoutcomeimbalances can cause algorithms to detect dierent features despite the same built-in amount of information. Wefoundthistobetrueforrandomforest,conditionalinferenceforest,logisticregression,aswell as in various penalized regression algorithms. On a practical level, this means that if we are using these algorithms for determining which features might be related to some outcome of interest, we might be introducing false negatives into our results, potentially missing factors that are related to the outcome. Based on the results of this study, we propose three recommendations for DBER and EDM researchers. First, for smaller data sets with highly imbalanced features, we recommend using a penalized version of logistic regression such as Log-F. Even though Firth penalization was often comparable to Log-F, Firth penalization is not implemented in all statistical software. Log-F however can be used with any statistical software that can perform logistic regression because it is basedondataaugmentation. Iftheoutcomeisalsoimbalanced,itisevenmoreessentialtoconsider penalized approaches. Second, for medium or large data sets ( #\u00151\u0096000) traditional logistic regression and random forest or conditional inference forest with a permutation importance perform similarly, so either approach works. While the algorithms still do not perform perfectly, none of them provided a consistent advantage over another. We recommend that researchers rst consider whether the research questions are best answered using predictive or explanatory techniques and then which aordances of the algorithms are most relevant to the study. Finally, we call on researchers to include information about their features in their publications, 163including the features themselves, their distributions, and in the case of categorical or binary features, their class frequencies as others outside the DBER and EDM communities have done in thepast[320]. AsimpleexampleofhowthismightbedoneisshowninTable6.4. Inaddition,we recommend that researchers include data set characteristics or so-called \"meta-features\" as well. Some examples include sample size, the number of features, the number of numerical features, the number of categorical features, and the percentage of observation of the majority class or outcomebalance[290]. Justastherehavebeencallsforincreasedreportingofdemographicsinthe DBER and EDM communities to understand how results might depend on the sample population or generalize [13,321], we are calling for the same with the explanatory and predictive models we create, partially addressing some of the questions raised by Knaub, Aiken, and Ding in their analysis of physics education research quantitative work [322]. By doing so, we hope for greater acknowledgement of possible sources of bias or false negatives in feature selection as a result of the data or algorithms used in DBER and EDM studies. 164APPENDICES 165APPENDIX A RANDOM FOREST BACKGROUND ThefollowingappendixcomesfromthesupplementalmaterialofYoungetal.[65]. Thepublished version includes Grant Allen, John M. Aiken, Rachel Henderson, and Marcos D. Caballero as co-authors. It is reproduced here without changes. A.1 The Random Forest algorithm A.1.1 Decision tree learning Randomforestshavetheirrootsinthedecisiontreelearning[323]. Decisiontreelearningusesaset ofbinarydecisionstodevelopamodelforthedataset. Forourpurposes,wefocusonclassication trees where the object of the model is a class label (i.e., a particular categorical outcome). The decisiontreealgorithmisprovidedwiththeclassesandthedatathatshouldpredicttheclasses(i.e., input variables). Conceptually, the decision tree algorithm searches the input variables for the one that best segregates the data into separate classes. That choice of \"best\" can be user specied, but if left to the algorithm, it will be the variable for which a majority of members of a class appear on one side of the decision (termed \"branch\") and not on the other side. For input variables that are continuous data, the algorithm further decides on the binary decision that best splits the data. For example, if \"age \u009f55\" was the binary decision, the algorithm both chose \"age\" as the input variable and \"55\" as the cut-o. The algorithm continues to make these decisions, splitting the data into more and more branches until all branches terminate in a single class (termed \"leaves\") or until a user-specied level. Tracing the path back from any leaf (single class or multiclass) to thestartingpointshowsallthedecisionsthatweremadetoobtainthatleaf. Inthissense,decision tree learning is a glass-box algorithm - a researcher can see every step along the path. Althoughtheresearchercanviewallpartsofthemodelandhowitwasconstructed,anysingle decision tree is strongly tied to the data used to construct it. This leads to overtting of the 166data [323,324]. That is, the decision tree algorithm can produce a model that exactly provides unique classications for the data it is given. As such, applying that model to predict classes in a new data set will often produce false predictions as the model was so strongly tied to the initial datasetonwhichitwastrained. Overttingisacommonprobleminmachinelearningtechniques whereasingleanalysisisconducted[325]. Todealwiththis,somedecisionstreesarepruned[248] - reduced to a smaller size by removing leaves that predict only small classes. However, as computational time has become less expensive, ensemble methods that develop a series of models from random selections of data are a more common method for combating overtting [326]. The specied number of times on a random selection of data with replacement [327]. Data selected to beusedinthealgorithmis\"bagged.\" Eachtimethealgorithmruns,itproducesadecisiontreefrom a randomly selected set of data. The input variables are scored based on how well they classify the data. Those that continually classify well earn higher scores, while those that do not are given lowerscores. Theresultisasetofinputvariablesthathavebeentestedonavarietyofdatasetsso that those input variables that are most important for classication can be found. Random forests use the bagging technique, but also randomly select a subset of input variables [91]. That is, data to develop the Random Forest model are randomly selected and only a subset of input variables (again, randomly chosen) are used in the classication. The same scoring procedure for these input variables is used. Both the bagging and Random Forest algorithms combat overtting by leveraging random selection as opposed to pruning. Conceptually, when randomly sampling data, thetrainedmodelwillsometimesproduceagoodmodelandatothertimesitwillnot. Bychecking the quality of each decision tree model after each run, the algorithms ensure that consistently- appearing predictors from good models are carried into the complete bagging or Random Forest 167model and others are not. Due to randomly selecting a subset of the input variables in addition to abootstrappedsampleofthedata,RandomForestshavebeenshowntohavesignicantadvantage over bagging alone [328]. A.1.3 Tuning Random Forest Parameters Random forests have two tuning parameters that can be adjusted to obtain a reliable model, =8=- the number of input variables selected at random, and =CA44B- the number of trees in the forest. A review of how =8=can aect the predictions of a Random Forest has been conducted by Svetnik et al. [100]. For a variety of choices of =8=, Svetnik et al. compared error rates (fraction of false positivesandfalsenegatives)predictionsandfoundthatformostchoicesof =8=errorrateswerewell maintainedbetween20-25%. Forlower =8=,themodeldoesnotdevelopenoughrobustcomparisons between dierent input variables to be reliable leading to slightly elevated error rates. While for higher=8=,themodelovertsthetrainingdataleadingtohigherscoresforlessimportantvariables, which again produce slightly higher error rates. For a given number of input variables, #, Svetnik et al. suggested =8=include#\u009d2,#\u009d4, andp #where each is rounded up or down to the nearest integer. They tested these suggested choices and found that all choices produce similar error rates evenasthenumberofinputvariablesisvariedbetween3and100. After100variables,thechoice =8==p #performs slightly better than the other choices, but only marginally so (2% dierence in error rates). The number of trees in the forest, =CA44B, describes the number of times that the algorithm randomly selects data and input variables to perform the classication task. Here, more is better, butonlyuptoapoint,afterwhichaddingadditiontreesdoesnotimprovetheclassication. There is no penalty for running the algorithm many times besides wasting computational resources. An estimate of the model's performance for a given number of trees is the Out-Of-Bag error (OOB error). Whenthealgorithmselectsdatatotrain,itleavessomedata\"outofthebag.\" Foranygiven tree in the forest, we can predict the classications for the data left out of the bag. The error rate associated with that prediction is an estimate of the Out-Of-Bag error for that tree. The estimate 168Known Classes Yes No Model Yes#)%#\u001b% Predicts No#\u001b##)# FigureA.1: Theconfusionmatrixcountsthenumberofeachpredictedclassicationbythemodel and compares that to the what the data indicates. In this case, a two class system with binary classications leads to a 2 x 2 matrix. For \"classes, the matrix continues to be square and grows to be\"x\". for the total OOB error is the average across all the trees. In their work, Svetnik et al., found that OOB error stabilized when =CA44B\u00a1102. For 3 orders of magnitude beyond that the OOB error remainedflatat0.2,thattheyfounda \u001820%averagemisclassicationofdataleftoutofthebagfor =8=and=CA44Btondthecombination with the strongest validation scores. Typically, one performs a coarse-grain search allowing the tunable parameters to vary greatly. This is much like searching for the appropriate order of magnitude for each parameter. Once a reasonable range is found for each parameter, a ner-grain search is performed within the bounds determined from the coarse grained search. This search can continue ad innitum , but it is typically only done until reasonable estimates of the \"best\" parameters are found given the expected error and available computational time. A.2 Validating the Random Forest model A Random Forest will develop a model of data, but that does not always mean that model is meaningful. Moreover,becausethatmodelisdevelopedfromarandomselectionofdataandinput variables, individual trees in the model might be terrible predictors. By abiding by suggested parameter choices [100], one can be somewhat condent in the model. However, additional validation of the model can be conducted to be able to provide evidence of that condence. These metrics and the associated curves are developed from how well the model predicts classications of the test data - that is, the data that was not used to train the model. 169FigureA.2: (a)Samplereceiveroperatingcharacteristic(ROC)curvesthatdemonstratetwomodels: one thatis better thanchance (blue) andone thatis worse thanchance (green). These ROCcurves are plotted along with the chance line (orange dotted). Models that are demonstrably better than chancehaveROCcurvesthattendtowardstheupper-leftcornerofthespaceasthearrowindicates. Models that are worse than chance tend towards the bottom-right corner. (b) For both models, the area under the ROC curves (AUC) are shown (blue and green shading) and computed. AUC provides a measure of the quality of the model. It is indicative of the probability of accurately classifying a random sample from the data. A.2.1 Confusion Matrix The simplest tool for understanding how well the Random Forest model predicts classications in the new data set is the confusion matrix. Most other measures associated with validity of the model are derived from the confusion matrix. Conceptually, the confusion matrix keeps track of true positives ( #)%), true negatives ( #)#), false positives ( #\u001b%), and false negatives ( #\u001b%). The sum of all these measures is the total number of observations in the data set that is being tested (#C4BC). #C4BC=#)%\u00b8#)#\u00b8#\u001b%\u00b8#\u001b# (A.1) Foratwoclasssystem(e.g.,Yes/No),thesevaluescanbeorganizedintothe2x2matrixwhere the columns describe the known classes in the data set and the rows describe the classes predicted by the model (Fig. A.1). The confusion matrix provides a quick check of the predictions of the Random Forest model. Essentially a good model will have strong diagonal elements, that is, high 170numbers of true predictions, and small o-diagonal elements, low numbers of false predictions. A.2.2 Associated measures From the confusion matrix, a number of associated measures may be derived. Here we provide thosethatarecommontoreportintheRandomForestliterature. Additionalmeasuresexist,butare notreportedhere /one.sup. Theaccuracyofthemodel( \u0016\u0018\u0018)isthefractionoftruepredictionscompared to the total )%'=#)% #)%\u00b8#\u001b#\u0095 (A.3) This rate varies between 0 and 1. For a good model of the data, we expect this number to be closer to 1. The fall-out, or false positive rate ( \u001b%') compares the number of predicted false positives negatives to the total number of actual negatives in the data, \u001b%' =#\u001b% #\u001b%\u00b8#)#\u0095 (A.4) This rate varies between 0 and 1. For a good model of the data, we expect this number to be closer to 0. Pure guessing (i.e., chance) would yield 0.5 for both of these values. Taken together, these values are plotted together for a range of discrimination thresholds in a receiver operating characteristic curve, which indicate how much better (or worse) the model is than chance. 1In some publications, certain likelihood ratios ( !'\u00b8=)%'\u009d\u001b%'and!'\u0000=\u001b#'\u009d)#'), odds ratios ( !'\u00b8 \u009d!'\u0000), and the F 1score are reported, but \u0016\u0018\u0018,)%', and\u001b%'are the most commonly reported metrics. 171A.2.3 Receiver operating characteristic curve The receiver operating characteristic curve (ROC curve) provides a visualization of the quality of abinarymodel[330]. Init, )%'isplottedagainst \u001b%'foravarietyofdiscriminationthresholds. These thresholds vary from 0 to 1 and describe the probability above which an observation is placed into one class compared to another. Conceptually, it determines, at a given probability of classication, the expected rates of true positives compared to false positives. A sample ROC for mock data is plotted in Fig. A.2(a). The chance line, in which )%'and \u001b%'areequalforallthresholds,isplottedinorange. ThebluecurveistheROCcurveforamodel that is better than chance. The humped-shaped of curve is common for good models as this shape areindicativeof )%'valuesabovechanceforallthresholds. Ontheotherhand,thegreencurveis the ROC curve for a model that is worse than chance. This shape is characteristic as well as it is indicative of )%'below chance for all thresholds. A quantitative measure of the quality of the model is the area under the ROC curve (AUC). This measure is visually represented in Fig. A.2(b) where AUC for the better than chance model is indicated by the blue and the green shading taken together. The AUC for the worse than chance model is indicated by the green shading alone. AUC is indicative of the probability that model will rank a randomly chosen positive instance higher than a randomly chosen negative one. From Fig. A.2, the probability for the rst model is approximately 0.71 while for the second it 0.29. AUCs above 0.7 are typically considered reasonable models with 0.8 and above considered to be goodmodels[93]. Aperfectclassierwillhaveanareaunderthecurveof1whilethechancecurve will have an area under the curve of 0.5. A.3 Feature selection One of the more useful aspects of machine learning, and of the Random Forest algorithm in particular, is the ability to determine which features are more important than other features in the data. For classication tasks, that means nding the input variables that consistently separate the resultsintoclasses. Thereisananalogytoregressionanalysiswheretheimportantinputvariables 172in a Random Forest classier act as statistically signicant correlates with the outcome variable. However, because Random Forests are not rooted in traditional statistical analysis, the important features do not arise from correlation - linear or otherwise. Feature selection makes use of these important features to reduce the overall number of input variables needed to classify the data. This is similar to using regression models of increasing complexity to nd the minimal model that explains the outcomes suciently. These important features can be used as the sole input variables and the resulting model can be validated using the techniques described in Sec. A.2. A good reduced model will maintain high accuracy, produce an ROC curve that is still above the chance line for all thresholds, and have an AUC that is similarly well above chance while using the minimum amount of features. To determine the importance of an input variable (termed \"feature importance\"), the standard RandomForestalgorithm(CART)continuouslycompareshowwelleachinputvariableinasingle decision tree separates the data set into classes. For the CART algorithm, the measure of how well this occurs is either the Gini impurity or the information gain, depending on user selection and choice of tool. For the simplest implementation of the Random Forest classier, the feature importanceisrelatedtotheGiniimpurity( )[91],whichisthetotaldecreaseinnodeimpurity. is computedfor over allthe treesintheforest. Conceptually, foraninputvariableisprobabilityoftheinputvariableshowing up in a given class multiplied by the probability of a misclassication within that factor summed over all classes [331]. Thus the higher for an input variable in a given tree, the less favorable choice it is for splitting the tree. For example, a high input variable would not be selected as the input variable for the rst branch in a given tree. For this implementation, important features arethosewhichconsistentlyproducethebestsplitsforalargeproportionoftreesintheforest. The feature importances are often distributed normally around some mean and are reported with error that is inversely proportional to the square root number of trees in which the input variable was randomly selected for use. 173A.4 Bias and improvements While the CART algorithm and the associated Gini-based feature selection are commonly used in Random Forest classication, both are subject to biases that for certain kinds of data (including those analyzed in this paper) can lead to inaccurate models. First, the Gini-based feature selection described above is not reliable when the input variables vary in scale of measurement or in the number of categories (possible responses) [92]. This is because variables with more categories can be split into two groups in more ways than variables withlesscategoriescanandhence,itismorelikelyafavorablesplitcouldbefound. Furthermore, Gini-based feature importances can be biased if the variables are correlated [332]. In such cases, accuracy-based permutation variable importances can be used [243,244]. Accuracy-basedpermutationvariableimportancesarebasedontheideathatifainputvariable is associated with the outcome variable, then permuting the input variable should break that association, and therefore, the accuracy ( \u0016\u0018\u0018) should decrease. The input variables that change the accuracy the most are then said to have the largest variable importance. However,theseaccuracybasedvariableimportancesarebiasedwhenthesampleisunbalanced, thatis,thecategoriesinthepredictedvariabledonotoccurinequalfrequencies[94]. Janitzaetal. suggest modifying the accuracy-based permutation variable importances to be based on the AUC instead of accuracy because accuracy is biased toward the majority class while the AUC is not. Whenapplyingthismodication,theyfoundthattheAUC-basedpermutationfeatureimportances are better able to discriminate between variables that are good predictors and variables that are poor predictors than the accuracy-based permutation feature importances are when the sample is unbalance. When the sample is balanced, they reported no signicant dierent between the two methods. Because the Random Forest algorithm is based o classication and regression trees (CART) [309]andCARTusestheGiniimpuritytodeterminethesplitpoints,theRandomForestalgorithm itselfisbiasedforthesamereasonstheGini-basedfeatureimportanceis. Thisbiascanbecorrected using conditional inference forests based on the framework proposed by Hothorn et al [92,259]. 174The conditional inference forest algorithm breaks the determining the split points into two steps, unlike the Random Forest algorithm, which selects the input variable and its split point in a single step. First, algorithm tests the global null hypothesis that there is no association between any of the input variables and predicted variable at some predetermined level of signicance. If this test fails, the algorithm terminates. If the null hypothesis can be rejected, the algorithm selects the input variable with the strongest association to the predicted variable as measured by its P value. The algorithm then splits the input variable into two groups that maximizes a chosen test statistic. Finally, the algorithm returns to the rst step and retests the global null hypothesis and either terminates or continues with the next input variable with the highest association to the predicted variable. This revised version of the Random Forest algorithm has been found to be unbiased evenwhenthevariablesvaryinscaleofmeasurementorinthenumberofcategoriesprovidedthat boostrapping is not used (subsampling must occur without replacement or the original biases are still present) [92]. A.4.1 Determining \"signicant\" features While we have detailed various improvements to variable selection, none of these methods can by themselves determine whether the variables are actually important, only how important they are withrespecttoeachother. Variousapproachestodeterminetheactualimportantvariableshavebeen proposed such as selecting the top 10% of the variables ordered by an importance measure [333], selectingallvariablesabovetheabsolutevalueofthemostnegativeimportancevalues[328],using recursive backward elimination to select the fewest number of variables that result in an OOB rate within 1 standard error of the best OOB rate [97], generating a null distribution of each variable and then assigning a p-value based on the fraction of null importances greater than the actual importance value [292], and mirroring the distribution of negative importances to generate an overall null distribution for the importances and again assigning a p-value based on the fraction of null importances greater than the actual importance value [297]. Each of these approaches has its own benets and problems such related to ease of implementation and computation time required 175and to our knowledge, there is no standard choice of procedure. As we had a limited number of negativeimportancesandlimitedcomputationalpower,weusedrecursivebackwardeliminationin this study. Since these \"signicant\"factors arenot determinedby tests ofstatistical signicantbut rather by how much the model changes when they are removed, we refer to the selected factors as meaningful factors. 176APPENDIX B CHAPTER 3 ANALYSIS OF FEATURES Inthisappendix,wedescribethedatausedtoanswerresearchquestions2and3togivethereadera betterideaofthedistributionsofphysicsGREscoresandGPAinthedataset. Becausethedataare skewedleftandexhibitceilingeects(manyapplicantshave4.0GPAsor990physicsGREscores), quartiles are used to describe the various features. To maximize the amount of information shown about the data, we use raincloud plots [334,335], which show the distribution, the density plot, and traditional box plot. Kolmogorov-Smirnov tests suggest the distributions are not signicantly dierent whether we include applicants who may have applied to multiple schools in our data set, so we include possible duplicates in our analysis. Fig. B.1 shows the physics GRE scores and undergraduate GPAs of each applicant based on whether they attended a large undergraduate physics program (top 25% nationally in yearly 400 500 600 700 800 900 1000 Physics GRE ScoreSmaller Programs Largest ProgramsSize of UG Physics ProgramPhysics GRE Score by Undergraduate Physics Program Size 400 500 600 700 800 900 1000 Physics GRE ScoreLess Selective Most SelectiveSelectivity of UniversityPhysics GRE Score by Undergraduate Institution Selectivity 2.0 2.5 3.0 3.5 4.0 Undergraduate GPASmaller Programs Largest ProgramsSize of UG Physics Program Undergraduate GPA by Undergraduate Physics Program Size 2.5 3.0 3.5 4.0 Undergraduate GPALess Selective Most SelectiveSelectivity of University Undergraduate GPA by Undergraduate Institution Selectivity Figure B.1: Distribution of physics GRE scores & undergraduate GPAs by the size of the under- graduate physics program & institutional selectivity for each applicant. 177400 500 600 700 800 900 1000 Physics GRE ScoreWomen MenGenderPhysics GRE Score by Applicant's Gender 400 500 600 700 800 900 1000 Physics GRE ScoreB/L/M/N non-B/L/M/NRacePhysics GRE Score by Applicant's Race 2.0 2.5 3.0 3.5 4.0 Undergraduate GPAWomen MenGender Undergraduate GPA by Applicant's Gender 2.0 2.5 3.0 3.5 4.0 Undergraduate GPAB/L/M/N non-B/L/M/NRace Undergraduate GPA by Applicant's RaceFigure B.2: Distribution of physics GRE scores and undergraduate GPAs by gender and whether theapplicantidentiedasamemberofracialorethnicgroupcurrentlyunderrepresentedinphysics. physics bachelor's degrees) or attended a selective university (categorized as most competitive or highly competitive based on Barron's Selectivity Index). We notice that the physics GRE score distributions are shifted to the right for applicants from large physics departments or selective institutions, signifying higher scores. Indeed, the median physics GRE scores of applicants from large programs or selective institutions are nearly 100 points higher than those of applicants from smallerorlessselectiveinstitutions. However,intermsofGPA,themedianGPAisapproximately thesame,regardlessofwhethertheapplicantgraduatedfromalargerorsmallerphysicsdepartment or attended a more or less selective institution. Fig. B.2 shows the physics GRE and undergraduate GPAs by gender and race. As expected, menscorehigheronthephysicsGREthanwomendoandAsianandwhiteapplicantsscorehigher than Black, Latinx, Multiracial, or Native applicants, though the gaps appear larger than those reported in [52]. When comparing GPAs, we nd that men and women have similar GPAs, as recently reported 178in [156] when comparing men and women's STEM GPAs. Likewise, our data also shows a racial GPA gap with non-B/L/M/N applicants having a median GPA higher than that of B/L/M/N applicants by 0.15. Whenlookingacrossbothgures,wenoticethatthephysicsGREscoredistributionsofsmaller and less selective programs resemble the physics GRE distributions of women and B/L/M/N applicants while the physics GRE score distributions of the largest and most selective programs resemble the physics GRE score distributions of men and non-B/L/M/N applicants. To see if genderandraceareconfoundingvariablesinouranalysis,weexaminedthefractionofwomenand B/L/M/N applicants in each group. If this were the case, the smaller and less selective programs shouldhaveagreaterfractionofwomenandB/L/M/Napplicantsthanthelargerandmoreselective programs. However, we did not nd this to be the case. Applicants from more selective institutions were 16% women while applicants from less selective institutions were 18% women (15% and 14% respectively for B/L/M/N applicants). For institution size, applicants from larger institutions were16%womencomparedto21%womenfromsmallerinstitutions(14%and17%forB/L/M/N applicantsrespectively). Thus,itdoesnotappearthatdierencesinwhoattends(intermsofgender andrace)largerormoreselectiveinstitutionsareresponsiblefortheobserveddierencesinscores. 179APPENDIX C CHAPTER 3 SUPPLEMENTAL FIGURES In this appendix, we include plots that show institutional eects by gender and race. As we note inthediscussion,theprogramsincludedinthisstudywereactivelytryingtoincreasethediversity of their graduate programs. Thus, we are unable to determine from our data whether women were admittedathigherratesthanmenwereandB/L/M/Napplicantswereadmittedathigherratesthan non-B/L/M/Napplicantswerebecausetheystoodoutoriftheadmissionscommitteeshighlighted these applicants from the start. Therefore, we do not comment on any possible interactions. For completeness, the plots are shown here. 180Figure C.1: Admission fractions of applicants split by their gender and the selectivity of their undergraduate institutions. 181Figure C.2: Admission fractions of applicants split by their gender and the size of their undergraduate institutions. 182Figure C.3: Admission fractions of applicants split by their race and the selectivity of their undergraduate institutions. 183Figure C.4: Admission fractions of applicants split by their race and the size of their undergraduate institutions. 184APPENDIX D CHAPTER 4 SUPPLEMENTAL FIGURES Here we present gures showing the results split by admission status and gender, undergraduate institution selectivity, and undergraduate physics program size. Given the relatively small sample sizes, we did not conduct tests of statistical signicance. 185Academic PreparationResearchNoncognitive contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated Admitted, Male Applicants (N=101)A Academic PreparationResearchNoncognitive contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated Admitted, Female Applicants (N=36)BAcademic PreparationResearchNoncognitive contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated Notadmitted, Male Applicants Academic contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated Notadmitted, Female Applicants (N=31)DFigure D.1: Faculty ratings of domestic applicants on 18 constructs split by whether the applicant was male or female and whether they were admitted or not. 186Academic PreparationResearchNoncognitive competenciesFit contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated, Admitted Applicants from Selective Undergraduate Programs (N=50)A Academic PreparationResearchNoncognitive contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated, Admitted Applicants from NonSelective Undergraduate Programs (N=47)BAcademic contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated, Not Admitted Applicants from Selective Undergraduate Programs (N=58)C Academic PreparationResearchNoncognitive competenciesFit contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated, Not Admitted Applicants from NonSelective Undergraduate Programs (N=57)DFigure D.2: Faculty ratings of domestic applicants on 18 constructs split by whether the applicant attendedamoreselectiveorlessselectiveundergraduateuniversityandwhethertheywereadmitted or not. 187Academic PreparationResearchNoncognitive contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated, Admitted Applicants from Large Undergraduate Programs (N=97)A Academic PreparationResearchNoncognitive contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated, Admitted Applicants from Small Undergraduate Programs (N=34)BAcademic PreparationResearchNoncognitive contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated, NonAdmitted Applicants from Large Undergraduate Programs (N=106)C Academic contributionsCommunity with FacultyAlignment of Research Physics GRE ScoreGeneral GRE ScoresFraction 0.8 0.6 0.4 0.2 0.0 0.00.20.40.60.8FractionRated, NonAdmitted Applicants from Small Undergraduate Programs (N=72)DFigure D.3: Faculty ratings of domestic applicants on 18 constructs split by whether the applicant attended a university with a larger or smaller physics program and whether they were admitted or not. 188APPENDIX E CHAPTER 6 SUPPLEMENTAL FIGURES Here we provide the plots for the additional three data sets from Sec. 6.5 for completeness. The plots show the same general results as discussed in the main manuscript. We also include the residual plots from the discussion. FigureE.1: Comparisonoftheoddsratio,Giniimportance,andAUC-permutationimportancefor the features in the school 2 admit data set. FigureE.2: Comparisonoftheoddsratio,Giniimportance,andAUC-permutationimportancefor the features in the school 3 shortlist data set. 189FigureE.3: Comparisonoftheoddsratio,Giniimportance,andAUC-permutationimportancefor the features in the school 3 admit data set. 432101230.6 0.2 0.20.40.6School 1 Log OddsAverage residual 5432100.40.20.00.20.4School 2 Log residual 0.00.20.4School Log residual 43210120.40.20.00.20.40.6School 3 Shortlist Log OddsAverage residual Figure E.4: Plots of the Log-odds vs the average residual in each bin for the four schools. Across all plots, between 20% and 34% of the points fall outside of the condence intervals, suggesting the logistic regression models might not be tting the data especially well. 190BIBLIOGRAPHY 191BIBLIOGRAPHY Interpretative Data. [2] RosemaryS.RussandTorOleB.Odden. PhysicsEducationResearchasaMultidimensional Space: Current Work and Expanding Horizons. In Charles Henderson and Kathleen A. Harper,editors, GettingStartedinPER .AmericanAssociationofPhysicsTeachers,College Park, MD, 2018. [3] StarrNicholson and Patrick J.Mulvey. Roster ofPhysics Departments withEnrollment and Degree Data, 2019. Technical report, September 2020. [4] Physicists and Astronomers : Occupational Outlook Handbook: : U.S. Bureau of Labor Statistics. [5] Employment and Careers in Physics, April 2020. [6] U.S. Census Bureau QuickFacts: United States. [7] Patrick J. Mulvey and Starr Nicholson. Physics Bachelor's Degrees: 2018, August 2020. [8] Lillian C. McDermott and Edward F. Redish. Resource Letter: PER-1: Physics Education Research. American Journal of Physics , 67(9):755-767, September 1999. [9] Jennifer L Docktor and Jos\u00e9 P Mestre. Synthesis of discipline-based education research in physics.Physical Special Topics-Physics Education Research , 10(2):020119, 2014. [10] TorOleB.Odden,AlessandroMarin,andMarcosD.Caballero.Thematicanalysisof18years of physics education research conference proceedings using natural language processing. Physical Review Physics Education Research , 16(1):010142, June 2020. [11] National Research Council. Adapting to a Changing World-Challenges and Opportunities in Undergraduate Physics Education . National Academies Press, Washington, D.C., June 2013. [12] Lin Ding. Theoretical perspectives of quantitative physics education research. Physical Review Physics Education Research , 15(2):020101, July 2019. [13] StephenKanimandXimenaC.Cid. Demographicsofphysicseducationresearch. Physical Review Physics Research , 16(2):020106, July 2020. [14] Marcos D Caballero, Bethany R Wilcox, Leanne Doughty, and Steven J Pollock. Unpack- ing students' use of mathematics in upper-division physics: where do we go from here? European Journal of Physics , 36(6):065004, 2015. [15] Brian Farlow, Marlene Vega, Michael E. Loverude, and Warren M. Christensen. Mapping activation of resources among upper division physics students in non-Cartesian coordi- nate systems: A case study. Physical Review September Qing X. Ryan and Benjamin P. Schermerhorn. Students' use of symbolic forms when con- structing equations of boundary conditions. Physical Review Physics Education Research , 16(1):010122, April 2020. [18] Tao Tu, Chuan-Feng Li, Guang-Can Guo. Students' diculties with partial dierential equations in quantum mechanics. Physical Review Physics Education Research, 16(2):020163, December 2020. [19] Bethany R. Wilcox and Giaco Corsiglia. Cross-context look at upper-division student diculties with integration. Physical Review Physics Education Research , 15(2):020136, October 2019. [20] JChristopherMooreandLouisJRubbo. Scienticreasoningabilitiesofnonsciencemajors in Special Topics-Physics Education Research , 8(1):010106, 2012. [21] Jessica Watkins, Janet E Coey, Edward F Redish, and Todd J Cooke. Disciplinary au- thenticity: Enriching the reforms of introductory physics courses for life-science students. Physical Review Special Topics-Physics [23] Elizabeth Gire and Edward Price. Arrows as anchors: An analysis of the material features ofelectriceldvectorarrows. PhysicalReviewSpecialTopics-PhysicsEducationResearch 2014. [24] CD Porter and AF Heckler. Eectiveness of guided group work in graduate level quantum mechanics. Physical Review Physics Education Research , 16(2):020127, 2020. [25] RachelEScherr,MikeALopez,andMarialisRosario-Franco. Isolationandconnectedness among Black and Latinx physics graduate students. Physical Review Physics Education Research, 16(2):020132, 2020. [26] Ben Van Dusen, Ram\u00f3n S Barthelemy, and Charles Henderson. Educational trajectories of graduate students in physics education research. Physical Review Special Topics-Physics Education Research , 10(2):020106, 2014. [27] YingCaoandB\u00e1rbaraMBrizuela.Highschoolstudents'representationsandunderstandings of electric Physical Review Physics Education Research , 12(2):020102, 2016. [28] Emily A Dare and Gillian H Roehrig. \"If I had to do it, then I would\": Understanding early middle school students' perceptions of physics and physics-related careers by gender. Physical Review Physics Education Research , 12(2):020117, 2016. 193[29] Jessie Durk, Ally Davies, Robin Hughes, and Lisa Jardine-Wright. Impact of an active learning physics workshop on secondary school students' self-ecacy and ability. Physical Review Physics Education Research , 16(2):020126, October 2020. [30] Lisa M Goodhew and Amy D Robertson. Exploring the role of content knowledge in responsive teaching. Physical Review Physics Education Research , 13(1):010106, 2017. [31] ErickaLawton,CarrieObenland,ChristopherBarr,MatthewCushing,andCarolynNichol. Improving high school physics outcomes for young women. Physical Review Physics Edu- cation Research , 17(1):010111, March 2021. [32] Amy D Robertson and Abigail R Daane. Energy Project professional development: Pro- moting positive attitudes about science among K-12 teachers. Physical Review Physics Education Research , 13(2):020102, 2017. [33] Xiaoming Zhai, teacherstolow-socioeconomicstatusschools. PhysicalReviewPhysicsEducationResearch American Physical Society. [34] JessicaL.Alzen,LaurieS.Langdon,andValerieK.Otero.Alogisticregressioninvestigation of the relationship between the Learning Assistant model and failure rates in introductory STEM courses. International Journal of STEM Education , 5(1):56, December 2018. [35] StephanieV.ChasteenandRajendraChattergoon. InsightsfromthePhysicsandAstronomy New Faculty Workshop: How do new physics faculty teach? Physical Review Physics Education Research , 16(2):020164, December 2020. [36] Dimitri R Dounas-Frazer and HJ Lewandowski. Electronics lab instructors' approaches to troubleshooting instruction. Physical Review Physics Education 13(1):010102, prescriptiveness of professional development workshops: The real-time professional development ob- servation tool. Physical Education Research , 12(2):020136, 2016. [38] Alanna Pawlak, Paul W. Irving, and Marcos D. Caballero. Learning assistant approaches to teaching computational physics problems in a problem-based learning course. Physical Review Physics Education Research , 16(1):010139, June 2020. [39] Danny Doucette, Russell Clark, and Chandralekha Singh. Professional development com- bining cognitive apprenticeship and expectancy-value theories improves lab teaching as- sistants' instructional views and practices. Physical Review Physics Education Research , 16(2):020102, July 2020. [40] Melanie Good, Emily Marshman, Edit Yerushalmi, and Chandralekha Singh. Graduate teaching assistants' views of broken-into-parts physics problems: Preference for guidance overshadows development of self-reliance in problem solving. Physical Review Physics Education Research , 16(1):010128, May 2020. 194[41] Tong Wan, Constance M Doty, Ashley A Geraets, Christopher A Nix, Erin K H Saitta, and Jacquelyn J Chini. Evaluating the impact of a classroom simulator training on graduate teaching assistants' instructional practices and undergraduate student learning. Physical Review Physics Education Research , 17(1):010146, June 2021. [42] Ben Van Dusen and Jayson Nissen. Associations between learning assistants, passing introductory physics, and equity: A quantitative critical race theory investigation. Physical Review Physics Education Research , 16(1):010117, April 2020. [43] Geraldine L Cochran, Andrea G Van Duzor, Mel S Sabella, and Brian Geiss. Engaging in self-study to support collaboration between two-year colleges and universities. In 2016 Physics Education Research Conference Proceedings , pages 76-79, 2016. [44] Renee Michelle Goertzen, Eric Brewe, Laird H Kramer, Leanne Wells, and David Jones. Moving toward change: Institutionalizing reform through implementation of the Learn- ing Assistant model and Open Source Tutorials. Physical Review Special Topics-Physics Education Research , 7(2):020105, 2011. [45] Jacqueline Doyle and Geo Potvin. In search of distinct graduate admission strategies in physics: An exploratory study using topological data analysis. pages 107-110, December 2015. ISSN: 2377-2379. [46] Julie R Posselt. Inside graduate admissions University Press, 2016. [47] Geo Potvin, Deepa Chari, and Theodore Hodapp. Investigating approaches to diversity in anationalsurveyofphysicsdoctoraldegreeprograms: Thegraduateadmissionslandscape. Physical Review Physics Education Research , 13(2):020142, December 2017. [48] RachelE.Scherr,MonicaPlisch,KaraE.Gray,GeoPotvin,andTheodoreHodapp. Fixed and growth mindsets in physics graduate admissions. Physical Review Physics Education Research, 13(2):020133, November 2017. [49] Geraldine L. Cochran, Theodore Hodapp, and Erika E. Alexander Brown. Identifying barriers to ethnic/racial minority students' participation in graduate physics. In Physics Education Research Conference Proceedings , PER Conference, pages 92-95, Cincinnati, OH, March 2018. [50] Deepa Chari and according to prospective graduate students. Physical Review Physics Education Research , 15(2), September 2019. [52] CaseyW.Miller,BenjaminM.Zwickl,JulieR.Posselt,RachelT.Silvestrini,andTheodore Hodapp. Typical physics Ph.D. admissions criteria limit access to underrepresented groups but fail to predict doctoral completion. Science Advances , 5(1):eaat7550, January 2019. 195[53] Lindsay Owens, Benjamin M. Zwickl, Scott V. Franklin, and Casey W. Miller. Identifying qualities of physics graduate students valued by faculty. In Physics Education Research Conference Proceedings , 2019. [54] Julie Posselt, Theresa Hernandez, Geraldine Cochran, and Casey Miller. Metrics First, Diversity Later? Making the Shortlist and Getting Admitted to Physics PhD Programs. Journal of Women and Minorities in Science and Engineering , 25(4), 2019. [55] Lindsay M. Owens, Benjamin M. Zwickl, Scott V. Franklin, and Casey W. Miller. Physics GRE Requirements Create Uneven Playing Field for Graduate Applicants. In 2020 Physics Education Research Conference Proceedings , pages human development: Experiments by nature and design. Harvard university press, 1979. [58] Abhilash Nair. THE RELEVANCE OF PHYSICS . PhD thesis, Michigan State University, 2018. [59] Sarah-Jane Leslie, Andrei Cimpian, Meredith Meyer, and Edward Freeland. Expecta- tions of brilliance underlie gender distributions across academic disciplines. Science, 347(6219):262-265, January 2015. [60] Cristobal Romero and Sebastian Ventura. Data mining in education. WIREs Data Mining and Knowledge Discovery , 3(1):12-27, 2013. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1075. [61] Bill Cope and Mary Kalantzis. Big Data Comes to School: Implications for Learning, Assessment, and Research. AERA Open , 2(2):2332858416641907, April 2016. [62] Galit 25(3):289-310, August 2010. [63] ElliJ.Theobald,MelissaAikens,SarahEddy,andHannahJordt.Beyondlinearregression: A referenceforanalyzingcommondatatypesindisciplinebasededucationresearch. Physical Review Physics Education Research , 15(2):020110, July 2019. [64] John M. Aiken, Rachel Henderson, and Marcos D. Caballero. Modeling student pathways in a physics bachelor's degree program. Physical Review Physics Education Research , 15(1):010128, May 2019. [65] Nicholas T. Young, Grant Allen, John M. Aiken, Rachel Henderson, and Marcos D. Ca- ballero. Identifying features predictive of faculty integrating computation into physics courses.Physical Review Physics Education Research , 15(1):010114, February 2019. [66] CabotZabriskie,JieYang,SethDeVore,andJohnStewart.Usingmachinelearningtopredict physics course outcomes. Physical Review Physics Education Research , 15(2):020120, August 2019. 196[67] Jie Yang, Seth DeVore, Dona Hewagallage, Paul Miller, Qing X. Ryan, and John Stewart. Using machine learning to identify the most at-risk students in physics classes. Physical Review Physics Education Research , 16(2):020130, October 2020. [68] Seth DeVore, Jie Yang, and John Stewart. Extending Machine Learning to Predict Unbal- anced Physics Course Outcomes. arXiv:2002.01964 [physics] , February 2020. [69] Nils J. Mikkelsen, Nicholas and Marcos D. Caballero. Investigating institutional influenceongraduateprogramadmissionsbymodelingphysicsGraduateRecordExamina- tion cuto scores. Physical Review Physics Education Research , 17(1):010109, February 2021. [70] Lei Bao. Theoretical comparisons of average normalized gain calculations. American Journal of Physics , 74(10):917-922, October 2006. [71] Jayson Nissen, Robin Donatello, and Ben Van Dusen. Missing data and bias in physics educationresearch: Acaseforusingmultipleimputation. PhysicalReviewPhysicsEducation Research, 15(2), July 2019. [72] ColeWalsh,MartinM.Stein,RyanTapping,EmilyM.Smith,andN.G.Holmes. Exploring the eects of omitted variable bias in physics education research. Physical Review Physics Education Research , 17(1):010119, March 2021. [73] Computational Mathematics, Science and Engineering. [74] John M. Aiken, Riccardo De Bin, H. J. Lewandowski, and Marcos D. Caballero. A Frame- work for Evaluating Statistical Models in Physics Research. American Association of Physics Teachers. [76] Amy Brand, Liz Allen, Micah Altman, Marjorie Hlava, and Jo Scott. Beyond authorship: attribution, contribution, collaboration, and credit. Learned Publishing , 28(2):151-155, 2015. [77] RachelIvie.BeyondRepresentation: DatatoImprovetheSituationofWomenandMinorities in Physics and Astronomy, March 2018. [78] N. Gupta, A. Sawhney, and D. Roth. Will I Get in? Modeling the Graduate Admission Process for American Universities. In 2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW) , pages 631-638, December 2016. [79] Austin Waters and Risto Miikkulainen. GRADE: Machine Learning Support for Graduate Admissions. AI Magazine , 35(1):64-64, March 2014. 197[80] ThomasLux,RandallPittman,MayaShende,andAnilShende. ApplicationsofSupervised Learning Techniques on Undergraduate Admissions Data. In Proceedings of the ACM International Conference on Computing Frontiers , CF '16, pages 412-417, New York, event-place: Como, Italy. [81] KanadpriyaBasu,TreenaBasu,RonBuckmire,andNishuLal. PredictiveModelsofStudent College Commitment Decisions Using Machine Learning. Data, 4(2):65, June 2019. [82] James S Moore. An expert system approach to graduate school admission decisions and academic performance prediction. Omega, 26(5):659-670, October 1998. [83] Julie R. Posselt. Disciplinary Logics in Doctoral Admissions: Understanding Patterns of Faculty Evaluation. The Journal of Higher Education , 86(6):807-833, November 2015. [84] Constitution of the State of Michigan - Article I \u00a7 26. [85] Indiana University Center for Postsecondary Research. The Carnegie Classication of Institutions of Higher Education, 2018 edition . Bloomington, IN. [86] StarrNicholson and Patrick J.Mulvey. Roster ofPhysics Departments withEnrollment and Degree Data, 2013. Technical report, American Institute of Physics, August 2014. [87] StarrNicholson and Patrick J.Mulvey. Roster ofPhysics Departments withEnrollment and Degree Data, 2014. Technical report, American Institute of Physics, September 2015. [88] StarrNicholson and Patrick J.Mulvey. Roster ofPhysics Departments withEnrollment and Degree Data, 2015. Technical report, American Institute of Physics, September 2016. [89] StarrNicholson and Patrick J.Mulvey. Roster ofPhysics Departments withEnrollment and Degree Data, 2016. Technical report, American Institute of Physics, September 2017. [90] Pamela Paxton and Kenneth A. Bollen. Perceived Quality and Methodology in Graduate DepartmentRatings: Sociology,PoliticalScience,andEconomics. SociologyofEducation , 76(1):71-88, October 2001. [92] CarolinStrobl,Anne-LaureBoulesteix,AchimZeileis,andTorstenHothorn. Biasinrandom forest variable sources random forests. BMC Bioinformatics , 14:119, April 2013. [95] Giles Hooker and Lucas Mentch. Please Stop Permuting Features: An Explanation importance for Bioinformatics , 9:307, July 2008. [97] Ram\u00f3n de Andr\u00e9s. Gene selection and classication of microarray data using B\u00fchlmann, Sandrine Dudoit, Annette Biostatistics , 7(3):355-373, July 2006. [100] and Bradley P. Feuston. Random Forest: A Regression Tool and Carolin Strobl. A new vari- able importance measure for random forests with missing data. Statistics and Computing , 24(1):21-34, January 2014. [102] StefvanBuurenandKarinGroothuis-Oudshoorn.mice: MultivariateImputationbyChained Equations of Statistical Software , 45(3):1-67, 2011. [103] Donald Rubin. Basic Ideas of Multiple Imputation for Nonresponse. Survey Methodology , 12(1):37-47, 1986. [104] Gregory Attiyeh and Richard Attiyeh. Testing for bias in graduate school admissions. The Journal of Human Resources; Madison , 32(3):524-548, 1997. [105] CaseyMillerandKeivanStassun. Atestthatfails. Nature,510(7504):303-304,June2014. [106] Statement on the Use of the GRE in Admissions to Graduate Physics Programs. American Association of Physics Teacher , July 2019. [107] AAS Statement on Limiting the Use of GRE Scores in Graduate Admissions in the Astro- nomical Sciences, October 2018. [108] NicholasT.YoungandMarcosD.Caballero. PhysicsGraduateRecordExamdoesnothelp June 2021. [109] Raeshanda Wilson. Predicting Graduate School A Critical Race Analysis of the GraduateRecordExamination. DoctorofEducationinSecondaryEducationDissertations , May 2020. [110] Emily M. Levesque, Rachel Bezanson, and Grant R. Tremblay. Physics GRE Scores of Prize Postdoctoral Fellows in Astronomy. arXiv:1512.03709 [astro-ph, physics:physics] , December 2015. 199[111] Laura A Lopez. Demographic Eects of Removing the Physics GRE Requirement in Graduate Admissions, October 2019. [112] KatieLangin.AwaveofgraduateprogramsdropstheGREapplicationrequirement. Science, May 2019. [113] About the GRE Subject Tests (For Test Takers). [114] GRE Guide to the Use of Scores, 2019. [115] NancyD.Morrison, WilliamV.Dixon, CaseyW.Miller, andTheWomeninAstronomyIV Graduate School Admissions White Paper Group. Women in Astronomy IV White Paper: Graduate Admissions in a Post-GRE World. Bulletin of the AAS , 51(4), 2020. [116] Emily M. Levesque, Rachel Bezanson, and Grant R. Tremblay. Why astronomy programs are moving on from the physics GRE. Physics Today , March 2017. [117] Julie R. Posselt. Trust Networks: A New Perspective on Pedigree and the Ambiguities of Admissions. The Review of Higher Education , 41(4):497-521, June 2018. [118] Alexander R. Small. Range restriction, admissions criteria, and correlation studies of stan- Conceptual, strategic, and statistical considerations. Journal of Personality and Social Psychology , 51(6):1173-1182, 1986. [120] JudithJ.M.Rnhart,JosW.R.Twisk,IrisEekhout,andMartnW.Heymans. Comparison of logistic-regression based methods for simple mediation analysis with a dichotomous outcome variable. BMC Medical Research Methodology , 19(1):19, January 2019. [121] Andrew F. Hayes and Michael Scharkow. The Relative Trustworthiness of Inferential Tests of the Indirect Eect in Statistical Mediation Analysis: Does Method Really Science , Andrew F. Heckler. Mediating relationship of dierential products in understanding integration in introductory physics. Physical Review Physics Education Research, 14(1):010105, January 2018. The Mediation Proportion: A Structural Equation Approach for Estimating the Proportion of Exposure Eect on Outcome Explained by an Intermediate Variable. Epi- demiology , 16(1):114-120, January 2005. [124] Laurence S. Freedman. Condence intervals and statistical power of the 'Validation' ratio for surrogate or intermediate endpoints. Journal of Statistical Planning and Inference , 96(1):143-153, June 2001. [125] Andrew F. Hayes. PROCESS: A Versatile Computational Tool for Observed Variable Me- diation, Moderation, and Conditional Process Modeling. Technical B. Weissman. Do GRE scores help predict getting a physics Ph.D.? A comment on a paper by Miller et al. Science Advances , 6(23):eaax3787, June 2020. [128] StarrNicholson and Patrick J.Mulvey. Roster ofPhysics Departments withEnrollment and Degree Data, 2017. Technical report, American Institute of Physics, October 2018. [129] StarrNicholson and Patrick J.Mulvey. Roster ofPhysics Departments withEnrollment and Degree Data, 2018. Technical report, American Institute of Physics, October 2019. [130] NationalCenterforEducationStatistics.NCES-Barron'sAdmissionsCompetitivenessIndex Data Files: 1972, 1982, 1992, 2004, , 2008, January 2017. [131] Raj Chetty, John Friedman, Saez, Nicholas Turner, and Danny Yagan. Mobility ReportCards: TheRoleofCollegesinIntergenerationalMobility.TechnicalReportw23618, National Bureau of Economic Research, Cambridge, MA, July 2017. [132] Adrienne L. Traxler, Ximena C. Cid, Enriching gender inphysics educationresearch: Abinary August 2016. [133] Tiani L. Williams. 'Underrepresented Minority' Considered Harmful, Racist Language, June 2020. [134] Robert T Teranishi. Race, ethnicity, and higher education policy: The use of critical quantitative research. New Directions for Institutional Research , 2007(133):37-49, 2007. [135] Frank J. Massey Jr. The Kolmogorov-Smirnov Test for Goodness of Fit. Journal of the American Statistical Association , 46(253):68-78, March 1951. [136] Casey Miller and Julie Posselt. Broadening Participation in Graduate Education through HolisticReview,November2018. PresentedattheBridgeProgramandNationalMentoring Community Conference. [137] Julia D Kent and Maureen Terese McCarthy. Holistic review in graduate admissions: A Report from the Council of Graduate Schools. Washington DC: Council of Graduate Students, 2016. [138] Alexander Rudolph, Marcel Meghan Donahue,JackieMonkiewicz,AngelaSpeck,KeivanStassun,RachelIvie,ChristinePfund, and Julie Posselt. Final Report of the 2018 AAS Task Force on Diversity and Inclusion in Astronomy Graduate Education. Bulletin of the AAS , 51(1), 2020. [139] Kris Dunlap. Journey to a more holistic admissions review process by implementing an evaluation rubric, July 2018. 201[140] Marenda A. Wilson, Max A. Odem, Taylor Walters, Anthony L. DePass, and Andrew J. Bean. AModelforHolisticReviewinGraduateAdmissionsThatDecouplestheGREfrom Race, Ethnicity, and Gender. CBE Life more inclusive. Nature, 557(7707):629-632, May 2018. Number: 7707 Publisher: Nature Publishing Group. [144] TheAIPNationalTaskForcetoElevateAfricanAmericanRepresentationinUndergraduate Physics & Astronomy. The Time is Now: Findings from TEAM-UP Report to Increase the NumberofAfricanAmericanswithBachelor'sDegreeinPhysicsandAstronomy. Technical report, American Institute of Physics, 2020. Publisher: APS. [145] Brian J. Rybarczyk, Leslie Lerea, Dawayne Whittington, Analysis , 15(3):ar33, September 2016. [146] \u00d6zlemSensoyandRobinDiAngelo.\"WeAreAllforDiversity,but...\": HowFacultyHiring Committees Reproduce Whiteness and Practical Suggestions for How They Can Change. Harvard Educational Review , 87(4):557-580, December 2017. [147] Arri Eisen and Douglas C. Eaton. A Model for Postdoctoral Education That Promotes Minority and Majority Success in the Biomedical Sciences. CBE Life Standford University Press, 2020. [151] KerryAnn O'Meara, Dawn Culpepper, and Lindsey L. Templeton. Nudging Toward Di- versity: Applying Behavioral Design to Faculty Hiring. Review of Educational Research , 90(3):311-348, June 2020. Publisher: American Educational Research Association. [152] Julie R. Posselt. Toward Inclusive Excellence in Graduate Education: Constructing Merit andDiversityinPhDAdmissions. AmericanJournalofEducation ,120(4):481-514,August 2014. Publisher: The University of Chicago Press. 202[153] Inclusive Graduate Education Network. [154] Casey Miller and Julie Posselt. Equitable Admissions in the Time of COVID-19. Physics, 13, December 2020. [155] GRE Subject Tests Fees (For Test Takers). [156] KyleM.WhitcombandChandralekhaSingh. Notalldisadvantagesareequal: Racial/ethnic minority students have largest disadvantage of all demographic groups in both STEM and non-STEM GPA. arXiv:2003.04376 [physics] , March 2020. [157] Stuart Rojstaczer and Christopher Healy. Where A is ordinary: The evolution of American college and university grading, 1940-2009. Teachers College Record , 114(7):1-23, 2012. College, SangEunWoo,JamesLeBreton,MelissaKeith,andLouisTay. Bias,Fairness,andValidity in Graduate Admissions: A Psychometric Perspective. August 2020. [159] Sandra L. Petersen, Evelyn S. Erenrich, Dovev L. Levine, Jim Vigoreaux, and Krista Gile. Multi-institutional study of GRE scores as predictors of STEM PhD degree completion: GRE gets a low mark. PLOS ONE , 13(10):e0206570, October 2018. [160] PLOS ONE , 12(1):e0169121, January 2017. Publisher: Public Sealy, Christina Saunders, Jerey Blume, and Roger Chalkley. The GRE over the entire range of scores lacks predictive ability for PhD outcomes in the biomedical sciences. PLOS ONE , 14(3):e0201634, March 2019. Publisher: Public Library of Science. [162] CarrieHawkins.TheImpactofaHolisticAdmissionsReviewProcessinaDoctorofPhysical Therapy Program. Graduate Theses, Dissertations, and Capstones , August 2020. [163] Annie M. Francis, L. B. Klein, Sharon Holmes Thomas, Kirsten Kainz, and Amy Blank Wilson. HolisticAdmissionsandRacial/EthnicDiversity: ASystematicReviewandImpli- cations for Social Work Doctoral Education. Journal Work Education , 0(0):1-18, April2021.Publisher: Routledge_eprint: https://doi.org/10.1080/10437797.2021.1895927. [164] Wendy I. Pacheco, Richard J. Noel, James T. Porter, Caroline B. Appleyard, and Hannah Sevian. Beyond the GRE: Using a Composite Score to Predict the Success of Puerto Rican Students in a Biomedical PhD Program. CBE\u2014Life Sciences Education , 14(2):ar13, June 2015. [165] Blaire Lauren Moody Rideout. A Study of the Inter-Rater Reliability of University Appli- cation Readers in a Holistic Admissions Review Process . PhD thesis, Bowling Green State University, 2017. [166] Tim Kautz, James J Heckman, Ron Diris, Bas Ter Weel, and Lex Borghans. Fostering and measuringskills: Improvingcognitiveandnon-cognitiveskillstopromotelifetimesuccess. National Bureau of Economic Research , 2014. Publisher:. 203[167] Brent W. Roberts. Back to the Future: Personality and Assessment and Personality Devel- opment.Journal of research in personality , 43(2):137-145, April 2009. [168] Mathilde Almlund, Angela Lee Duckworth, James Heckman, and Tim Kautz. Personality psychology and economics. In Handbook of the Economics of Education , volume 4, pages 1-181. Elsevier, 2011. [169] W. E. Sedlacek. Noncognitive Measures for Higher Education Admissions. In Penelope Peterson, Eva Baker, and Barry McGaw, editors, International Encyclopedia of Education (Third Edition) , pages 845-849. Elsevier, Oxford, January 2010. [170] Terence J. Tracey and William E. Sedlacek. Noncognitive Variables in Predicting Aca- demic Success by Race. Measurement and Evaluation Sedlacek. Prediction of College Graduation Us- ing Noncognitive Variables by Race. Measurement and Evaluation in Counsel- ing and Development SUCCESS . PhD thesis, Temple University, 2014. [173] Stephen Carp, Kyle Fry, Brittany Gumerman, Kevin Pressley, and Alyssa Whitman. Re- lationship Between Grit Scale Score and Academic Performance in a Doctor of Physical Therapy Program: A Case Study. Journal of Allied Health , 49(1):29-36, February 2020. [174] ScottK.Stolte,StephanieB.Scheer,andEvanT.Robinson.TheReliabilityofNon-Cognitive Admissions Measures in Predicting Non-traditional Doctor of Pharmacy Student Perfor- mance Outcomes. American Journal of Pharmaceutical Education , 67(1):18, September 2003. [175] Kristin Zakariasen Victoro and Richard Shandling. Eectiveness of medical school admissions criteria in predicting residency ranking four years later. Medical Edu- cation,41(1):57-64,2007. Mark Yudelev, and Michael Snyder. Correlation of admissions statistics to graduate student success in medical physics. Journal of Applied Clinical Medical Physics , 15(1):375-385, 2014. 204[178] Chan Kulatunga Moruzi and Georey R. Norman. Validity of Admissions Measures in Predicting Performance Outcomes: The Contribution of Cognitive and Non-Cognitive Di- mensions. Teaching and Learning Paunonen. Big Five personality predictors of post- secondary academic performance. Personality and Individual Dierences , 43(5):971-990, October 2007. [180] Patrick Kyllonen, Alyssa M. Walters, and James C. Kaufman. Noncognitive Constructs and Their Assessment in Graduate Education: A Review. Strate- gies Reducing Racioethnic and Sex Subgroup Dierences and Adverse Im- pact in Selection. Personnel Psychology , 61(1):153-172, 2008. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1744-6570.2008.00109.x. [182] William E. Sedlacek. Why should use noncognitive variables with graduate and profes- sional students. The Advisor: The Journal of the National Association of Advisors for the Health Professions , 24(2):32-39, 2004. [183] CaseyWMiller. UsingNon-CognitiveAssessmentsinGraduateAdmissionstoSelectBetter Students and Increase Diversity. page 10, 2015. [184] Yuanyuan Chen, Shuaizhang Feng, James J. Heckman, and Tim Kautz. Sensitivity of self- reportednoncognitiveskillstosurveyadministrationconditions. ProceedingsoftheNational Academy of Sciences , 117(2):931-935, January 2020. Equity in Graduate Education - Non-Cognitive Assessment, 2021. [186] Penny Salvatori. Reliability and Validity of Admissions Tools Used to Select Students for the Health Professions. Advances in Health Sciences Education , 6(2):159-175, May 2001. [187] JacquelineM.Zeeman,JacquelineE.McLaughlin,andWendyC.Cox. Validityandreliabil- ity of an application review process using dedicated reviewers in one stage of a multi-stage admissions model. Currents in Pharmacy Teaching and Learning , 9(6):972-979, 2017. [188] Mark J. Graham, and Jo Handelsman. Science faculty's subtle gender biases favor male students. Proceedings of the National Academy of Sciences , 109(41):16474-16479, October 2012. Publisher: National Academy of Sciences Section: Social Sciences. [189] AsiaA.Eaton,JessicaF.Saunders,RyanK.Jacobson,andKeonWest.HowGenderandRace Stereotypes Impact the DeBonis. Reimagining Merit and Representation: Promoting Equity and Reducing Bias in GME Through Holistic Review. Academic Psychiatry: The Journal of the American Association of Directors of Psychiatric Residency Training and the Association for Academic Psychiatry , 45(1):34-42, February 2021. [191] David M. Quinn. Experimental Evidence on Teachers' Racial Bias in Student Evaluation: The Role of Grading Scales. Educational Evaluation and Policy Analysis , 42(3):375-392, September 2020. Publisher: American Educational Research Torsten Ris\u00f8r, Brian Hodges, and Yvonne Steinert. cultural myth of medical meritocracy. Medical Education , 54(1):46-53, January 2020. [193] KeivanG.Stassun,SusanSturm,KellyHolley-Bockelmann,ArnoldBurger,DavidJ.Ernst, Webb. The Fisk-Vanderbilt Master's-to-Ph.D. Bridge Program: Recognizing, enlisting,andcultivatingunrealizedorunrecognizedpotentialinunderrepresentedminority students. American Journal of Physics , 79(4):374-379, March 2011. [194] Sonia F. Roberts, Elana Pyfrom, Jacob A. Homan, Christopher 6(2):65-70, 1979. Publisher: [Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]. [196] Cassandra M. Guarino and Victor M. H. Borden. Faculty Service Loads and Gender: Are Women Taking Care of the Academic Family? Research in Higher Education , 58(6):672- 694, September 2017. [197] Nicholas T. Young and Marcos D. Caballero. The Physics GRE does not help applicants [physics] , III. Commentary: Disentangling Today, July PhysicalReviewPhysicsEducation Research, 12(2):020113, August 2016. [200] PaulH.Barber,TyroneB.Hayes,TracyL.Johnson,andLeticiaM\u00e1rquez-Maga\u00f1a. in higher education. Science, 369(6510):1440-1441, September 2020. [201] Danielle Dickens, Maria Jones, and Naomi Hall. Being a Token Black Female Faculty MemberinPhysics: ExploringResearchonGenderedRacism,IdentityShiftingasaCoping Strategy, and Inclusivity in Physics. The Physics Kelly Ochs Rosinger, Karly Sarita Ford, and Junghee Choi. The Role of Selective College Admissions Criteria in Interrupting or Reproducing Racial and Economic Inequities. The Journal of Higher Education , pages 1-25, September 2020. [204] David Kalsbeek, Michele Sandlin, and William Sedlacek. Employing Noncogni- tive Variables to Improve Admissions, and Increase Student Diversity and Reten- tion.Strategic Enrollment Best Diversity Paper: Work in Progress: Aligning What We Want With What We Seek: Increasing Comprehensive Review in the Graduate Admissions Process. June 2020. [206] assessments. International Journal of STEM Education , 5(1):21, December 2018. [209] Ivan Tomek. Two Modications of IEEE Transactions on Systems, Man, and Cyber- netics, SMC-6(11):769-772, November 2013. Section: 3 _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118646106.ch3. [211] Min Zeng, Bei Zou, Faran Wei, Xiyao Liu, and Lei Wang. Eective prediction of three commondiseasesbycombiningSMOTEwithTomeklinkstechniqueforimbalancedmedical data. In2016 IEEE International Conference of Online Analysis and Computing Science (ICOACS) , pages 225-228, May 2016. [212] Gustavo E. A. P. A. Batista, Ronaldo C. Prati, and Maria Carolina Monard. A study of the behavior of several methods for balancing machine learning training data. ACM SIGKDD Explorations Thanathamathee. Random Forest with Sampling Techniques for Handling Imbalanced Prediction of University Student Depression. Infor- mation, 11(11):519, November for Discrimination Prevention and Bias Disambiguation. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, AIES '20, pages 358-364, New York, NY, USA, February 2020. Association for Computing Machinery. [215] Paula Branco, Rita P. Ribeiro, and Luis Torgo. UBL: an R Package for Utility-Based Learning. CoRR, abs/1604.08079, Methods , 7(1):19-40, 2002. [219] Julie R. Irwin and Gary H. McClelland. Negative Consequences of Dichotomizing Con- tinuous Predictor Variables. Journal of Marketing Research , 40(3):366-371, August 2003. Publisher: SAGE Publications Inc. [220] M. Stains, J. Harshman, M. K. Barker, S. V. Chasteen, R. Cole, S. E. DeChenne-Peters, M.K.Eagan,J.M.Esson,J.K.Knight,F.A.Laski,M.Levis-Fitzgerald,C.J.Lee,S.M.Lo, L. M. McDonnell, T. A. McKay, N. Michelotti, A. Musgrove, M. S. Palmer, K. M. Plank, T.M.Rodela,E.R.Sanders,N.G.Schimpf,P.M.Schulte,M.K.Smith,M.Stetzer,B.Van Valkenburgh,E.Vinson,L.K.Weir,P.J.Wendel,L.B.Wheeler,andA.M.Young.Anatomy of STEM teaching in North American universities. Science, 359(6383):1468-1470, March 2018. Publisher: AmericanAssociationfortheAdvancementofScienceSection: Education Forum. [221] Kelley Commeford, Eric Brewe, and Adrienne Traxler. Characterizing active learning environments in physics Practice , 145:302-318, March 2021. [223] R Behrens and R Analysing changing personal travel behaviour over time: methodological lessons from the application of retrospective surveys in Cape Town. In 8th international conference on survey methods in transport: harmonisation and data quality, Annecy, 2008. [224] Allen L. Edwards. The social desirability variable in personality assessment and research . The social desirability variable in personality assessment and research. Dryden Press, Ft Worth, TX, US, 1957. Pages: viii, 108. 208[225] NiteshVChawla.DataMiningforImbalancedDatasets: Knowledge 875-886. Springer, Boston, MA, 2009. [226] ElsaVazquezArreolaandJereyR.Wilson. Bayesianmultiplemembershipmultipleclassi- cation logistic regression model on student performance with random eects in university instructors and majors. PLOS ONE , 15(1):e0227343, January 2020. [227] Mei-Shiu Chiu. Gender dierences in STEM Choice by Aective States and BehaviorsinOnlineMathematicalProblemSolving: Positive-Aect-to-SuccessHypothesis. JEDM | Journal of Educational Data Mining , 12(2):48-77, August 2020. [228] Katherine P. Dabney and Robert H. Tai. Comparative analysis of female physicists in the physical sciences: Motivation and background variables. Physical Review Special Topics - Physics Education Research , 10(1):010104, February 2014. [229] CharlesHenderson,MelissaDancy,andMagdalenaNiewiadomska-Bugaj. Useofresearch- basedinstructionalstrategiesinintroductoryphysics: Wheredofacultyleavetheinnovation- decision Review Special - Physics Education Research , 8(2):020104, July 2012. [230] Rachel Ivie, Susan White, and Raymond Y. Chu. Women's and men's career choices in astronomy and astrophysics. Physical Review Physics Education Research , 12(2):020109, August 2016. 2012. [232] CathrynA.Manduca,EllenR.Iverson,MichaelLuxenberg,R.HeatherMacdonald,DavidA. McConnell, David W. Mogk, and Barbara J. Tewksbury. Improving undergraduate STEM education: The ecacy of discipline-based professional Science Advances February [233] Carlos M\u00e1rquez-Vera, Alberto Cano, Crist\u00f3bal Romero, and Sebasti\u00e1n Ventura. Predicting student failure at school using genetic programming and dierent data mining approaches with high dimensional and imbalanced data. Applied Intelligence J. Sax, Kathleen J. Lehman, Ram\u00f3n S. Barthelemy, and Gloria Lim. Women in physics: A comparison to science, technology, engineering, and math education over four decades. Physical Review Physics Education Research , 12(2):020108, August 2016. [237] Kelly Spoon, Joshua Beemer, John C. Whitmer, Juanjuan Fan, James P. Frazee, Jeanne Stronach, Andrew J. Bohonak, and Richard A. Levine. Random Forests for Evaluating Pedagogy and Informing Personalized Learning. JEDM | Journal of Educational Data Mining, 8(2):20-50, December 2016. Number: 2. [238] Robert H. Tai, Xiaoqing Kong, Claire E. Mitchell, Katherine P. Dabney, Daniel M. Read, Donna B. Jee, Dorothy A. Andriole, and Heather D. Wathington. Examining Summer Laboratory Research Apprenticeships for High School Students as a Factor in Entry to MD/PhDProgramsatMatriculation. CBE\u2014LifeSciencesEducation ,16(2):ar37,June2017. [239] AlejandroPe\u00f1a-Ayala. Educationaldatamining: logistic and survival regressions. Statistics in Medicine , 34(23):3133-3143, 2015. [243] Kristin K. Nicodemus. Letter to the Editor: On the stability and ranking of predictors from random forest variable importance measures. Briengs and Manuel J. A. Eugster. A Plea for Neutral Com- parison Studies in Computational Sciences. PLOS ONE , 8(4):e61562, April 2013. [247] Cristobal Romero and Sebastian Ventura. Educational data mining and learning analytics: An updated survey. WIREs Data Mining and Knowledge Discovery , 10(3):e1355, 2020. [248] TrevorHastie,RobertTibshirani,andJeromeFriedman. Theelementsofstatisticallearning: data mining, inference, and prediction . Springer Science & Business Media, 2nd edition edition, 2009. 210[249] C.Ensoy,T.W.Rakhmawati,C.Faes,andM.Aerts.SeparationIssuesandPossibleSolutions: PartI-SystematicLiteratureReviewonLogisticModels-PartII-Comparisonofdierent methods for separation in logistic regression. EFSA Publications , 12(9):869E, 2015. [250] Harold Jereys. An invariant form for the prior probability in estimation problems. Pro- ceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences , 186(1007):453-461, September 1946. [251] Georg Heinze and Michael Schemper. A solution to the problem of separation in logistic regression. Statistics in Medicine , Methodology) , 67(2):301-320, 2005. Paolo Vineis, and Maria De Iorio. Signicance ridge regression for genetic 12(1):372, September 2011. Methodology) , 72(4):417-473, September 2010. [257] Jason D. Lee, Dennis L. Sun, Yuekai Sun, and Jonathan E. Taylor. Exact post-selection inference, with application to the lasso. Annals of Statistics , 44(3):907-927, June 2016. [258] RichardLockhart,JonathanTaylor,RyanJ.Tibshirani,andRobertTibshirani. Asignicance test for the of 42(2):413-468, April 2014. [259] Torsten Hothorn, Kurt Zeileis. Unbiased Recursive Partitioning: A Conditional Inference Framework. Journal of Computational and Graphical Statistics , 15(3):651-674, September 2006. [260] Jake Olivier and Melanie L. Bell. Eect Sizes for 2 \u00d72 Contingency Tables. PLoS ONE , March 2013. [261] Liaw and Wiener. and Regression by randomForest. R News, 2(3):18-22, 2002. C. Heyde. Central Theorem. In Wiley StatsRef: Statis- tics _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat04559. 211[263] M. Firth-and logF-type penal- ized methods in risk prediction for small or sparse binary data. BMC medical research methodology , Ioannis Kosmidis Firth. Jereys-prior Hastie, and Rob Tibshirani. Regularization Paths for Gener- alized Linear Models via Coordinate Descent. Journal of Statistical Software , 33(1):1-22, February 2010. Number: 1. [268] the Bootstrap and Other Resampling Plans . CBMS-NSF Regional Conference Series in Applied Mathematics. Society for Industrial and Applied Mathematics, January 1982. [270] BethanyRWilcoxandHeatherJLewandowski.Students'epistemologiesaboutexperimental physics: ValidatingtheColoradoLearningAttitudesaboutScienceSurveyforexperimental [271] BethanyRWilcoxandHeatherJLewandowski. Students'viewsaboutthenatureofexperi- mental [272] ZhongzhouChen,KyleM.Whitcomb,MatthewW.Guthrie,andChandralekhaSingh. problem solving performance afterstudyinganonlinetutorial. In for Descriptive Statistics . 2020. [274] Daniel McFadden. Quantitative Methods for Analyzing Travel Behaviour of Individuals: Some Recent Developments. Technical Report 474, Cowles Foundation for Research in Economics, Yale University, 1977. [275] ScottMenard. CoecientsofDeterminationforMultipleLogisticRegressionAnalysis. The American Statistician , 54(1):17-24, February 2000. [276] Tyler Hunt. ModelMetrics: Rapid Metrics C. Ekemans, and Johannes B. Reitsma. No rationale for 1 variable per 10 events criterion for binary logistic regression analysis. BMC Medical Research Methodology , 16(1), December 2016. [279] HyungwooKim,TaeseokKo,No-WookPark,andWoojooLee. ComparisonofBiasCorrec- tion Methods for the Rare Event Logistic Regression. Korean Journal of Applied Statistics , 27(2):277-290, April 2014. [280] SamDoerken,MartaAvalos,EmmanuelLagarde,andMartinSchumacher.Penalizedlogistic regression with low prevalence exposures beyond high dimensional settings. PLOS ONE , 14(5):e0217057, May 2019. Publisher: Public Library of Science. [281] Emmanuel O. Ogundimu. Prediction of default probability by using statistical models for rare events. Journal of the Royal Statistical Society: Series A (Statistics in Society) , 182(4):1143-1162, 2019. Iorio, and Rumana Z. Omar. Reviewandevaluationofpenalisedregressionmethodsforriskpredictioninlow-dimensional data with few events. Statistics in Medicine , 35(7):1159-1177, 2016. [283] BenVanCalster,MaartenvanSmeden,BavoDeCock,andEwoutWSteyerberg. Regression shrinkage methods for clinical prediction models do not guarantee improved performance: Simulation study. Statistical Methods in Medical Research , 29(11):3166-3178, November 2020. [284] P. Peduzzi, J. Concato, E. Kemper, T. R. Holford, and A. R. Feinstein. A simulation study of the number of events per variable in logistic regression analysis. Journal of Clinical Epidemiology , 49(12):1373-1379, December 1996. [285] Peter C Austin and Ewout W Steyerberg. Events per variable (EPV) and the relative perfor- mance of dierent strategies for estimating the out-of-sample validity of logistic regression models.Statistical Methods in Medical 26(2):796-808, Joris AH de Groot, Gary S Collins, Douglas G Altman, Marinus JC Ekemans, and Johannes B Reitsma. Sample size for binary logistic prediction models: Beyond events per variable criteria. Statistical Methods in Medical Research, 28(8):2455-2474, August 2019. [287] King, and Rumana Z. Omar. How to develop a more accurate risk prediction model when there are few events. BMJ, 351:h3868, August 2015. 213[289] PhilippProbst,MarvinN.Wright,andAnne-LaureBoulesteix. Hyperparametersandtuning strategies for random Interdisciplinary Couronn\u00e9, [stat], January 2021. [292] A. Hapfelmeier and K. Ulm. A new variable selection approach using Random Forests. Computational Statistics & Data Analysis , 60:50-69, April 2013. [293] Markus Ojala and Permutation Tests for Classier Perfor- mance.Journal of Machine Learning Research , 11:1833-1863, June 2010. [294] Xiang Chen, Ching-Ti Liu, Meizhuo Zhang, and Heping Zhang. A forest-based approach to identifying gene and gene-gene interactions. Proceedings of the National Academy of Sciences of the United States of America , 104(49):19199-19203, 2007. [295] MinghuiWang,XiangChen,andHepingZhang.Maximalconditionalchi-squareimportance and Thomas Lengauer. tance: a corrected feature A variable importance test for random forests for high-dimensional data. Advances in Data Analysis and Classication , 12(4):885-915, November 2016. [298] D. H. Wolpert and W. G. Macready. No free lunch theorems for optimization. IEEE Transactions on Evolutionary Computation , 1(1):67-82, Kaitlin Datasets. , 1(3):25, 2018. [300] Andreas W\u00e5linder. Evaluation of logistic regression and random forest classication based on prediction accuracy and metadata analysis . Linnaeus Cambridge 2006. 214[303] Andrew Gelman and Yu-Sung Su. arm: Data Analysis Using Regression and with rare events: accurate eect and predictions? Statistics in Medicine, 36(14):2302-2317, 2017. Andrew Gelman, Aleks Jakulin, and Yu-Sung Su. A weakly informa- tive default prior distribution for logistic and other regression models. Annals of Applied Statistics, 2(4):1360-1383, December 2008. [306] H\u00fclyaOlmu,EzgiNazman,andSemraErba. Comparisonofpenalizedlogisticregression models for rare event case. Communications in Statistics - Simulation and Computation , 0(0):1-13, October 2019. [307] ChaoChen,AndyLiaw,andLeoBreiman. UsingRandomForesttoLearnImbalancedData. Technical of California, Berkley, July 2004. [308] BjoernH.Menze,B.MichaelKelm,DanielN.Splittho,UllrichKoethe,andFredA.Ham- precht. On Malerba,andMichalisVazirgiannis,editors, 453-469, Berlin, Heidelberg, 2011. Springer. [309] LeoBreiman,JeromeFriedman,CharlesJStone,andRichardAOlshen. tress with unbiased variable selection and interaction detection. Statistica sinica , pages 361-386, 2002. [313] J. Zhang and K. F. Yu. What's the relative risk? A method of correcting the odds ratio in cohort studies of common outcomes. JAMA, 280(19):1690-1691, November 1998. [314] [316] ChristophMolnar,GunnarK\u00f6nig,BerndBischl,andGiuseppeCasalicchio. Model-agnostic Feature Importance and Eects with Dependent Features - A Conditional Subgroup Ap- proach.arXiv:2006.04628 [cs, stat] , June 2020. 215[317] R. PhysicalReviewPhysicsEducationResearch July 2019. [318] Devyn Shafer, Maggie S. Mahmood, and Tim Stelzer. Impact of broad categorization on statistical results: How underrepresented minority designation can mask the struggles of both Asian American and African American students. Physical Review Physics Education Research, 17(1):010113, Mining Approach for Educational Decision Support. EKSAKTA: Journal of Sciences and Data Analysis , 2(1):33-44, February 2021. Number: 1. [320] Sander Greenland, Mohammad Ali Mansournia, and Douglas G. Altman. Sparse data bias: a problem hiding in plain sight. British Medical Journal , 352, April 2016. [321] Luc Paquette, Jaclyn Ocumpaugh, Ziyue Li, Alexandra Andres, and Ryan Baker. Who's Learning? Using Demographics in EDM Research. JEDM | Journal of Educational Data Mining, 12(3):1-30, October 2020. Number: 3. [322] Alexis V. Knaub, John M. Aiken, and Lin Ding. Two-phase study examining perspectives and use of quantitative methods in physics education research. Physical Review Physics Education Research , 15(2):020102, July , CarolinStrobl,JamesMalley,andGerhardTutz. AnIntroductiontoRecursivePartitioning: Rationale, Application and Characteristics of Classication and Regression Trees, Bagging and Random Forests. Psychological methods , 14(4):323-348, December 2009. [329] James Bergstra and Yoshua Bengio. Random Search for Hyper-Parameter Optimization. Journal learning: A test-driven approach . \" O'Reilly Media, Inc.\", 2014. 216[332] Kristin K. Nicodemus and James D. Malley. Predictor correlation impacts machine learn- ing algorithms: implications for genomic studies. Bioinformatics (Oxford, England) , 25(15):1884-1890, August 2009. [333] D. M. Reif, A. A. Motsinger-Reif, B. A. McKinney, M. T. Rock, J. E. Crowe, and J. H. Moore. Integrated analysis of genetic and proteomic data identies biomarkers associated with adverse events following smallpox vaccination. Genes and Immunity , robust data visualization. Wellcome Open Re- search, 4:63, April 2019. [335] Kirstie Whitaker, Tom Rhys Marshall, Tim Van Mourik, Poggiali, Hao Ye, and Marius Klug. RainCloudPlots/RainCloudPlots: WellcomeOpenRe- search, August 2019. 217 "}