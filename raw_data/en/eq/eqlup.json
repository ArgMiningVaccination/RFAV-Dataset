{"title": "PDF", "author": "PDF", "url": "https://authors.library.caltech.edu/114603/3/2205.06908.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "Division of Engineering and Applied Science, California Institute of Technology yThe rst two authors contributed equally to this article. Alphabetical order. \u0003Corresponding authors. Email: sjchung@caltech.edu This is the accepted version of Science Robotics V ol. 7, Issue 66, eabm6597 (2022) DOI: 10.1126/scirobotics.abm6597 Video: https://youtu.be/TuF9teCZX0U Data and training code: https://github.com/aerorobotics/neural-fly Abstract Executing safe and precise flight maneuvers in dynamic high-speed winds is important for the on- going commoditization of uninhabited aerial vehicles (UA Vs). However, since the relationship between various wind conditions and its effect on aircraft maneuverability is not well understood, it is challeng- ing to design effective robot controllers using traditional control design methods. We present Neural-Fly, a learning-based approach that allows rapid online adaptation by incorporating pre-trained representa- tions through deep learning. Neural-Fly builds on two key observations that aerodynamics in different wind conditions share a common representation and that the wind-specic part lies in a low-dimensional space. To that end, Neural-Fly uses a proposed learning algorithm, Domain Adversarially Invariant Meta-Learning (DAIML), to learn the shared representation, only using 12 minutes of flight data. With the learned representation as a basis, Neural-Fly then uses a composite adaptation law to update a set of linear coefcients for mixing the basis elements. When evaluated under challenging wind conditions generated with the Caltech Real Weather Wind Tunnel with wind speeds up to 43:6 km=h(12:1 m=s), Neural-Fly achieves precise flight control with substantially smaller tracking error than state-of-the-art nonlinear and adaptive controllers. In addition to strong empirical performance, the exponential stability of Neural-Fly results in robustness guarantees. Finally, our control design extrapolates to unseen wind conditions, is shown to be effective for outdoor flights with only on-board sensors, and can transfer across drones with minimal performance degradation. 1 INTRODUCTION The commoditization of uninhabited aerial vehicles (UA Vs) requires that the control of these vehicles be- come more precise and agile. For example, drone delivery requires transporting goods to a narrow target area in various weather conditions; drone rescue and search require entering and searching collapsed build- ings with little space; urban air mobility needs a flying car to follow a planned trajectory closely to avoid collision in the presence of strong unpredictable winds. 1arXiv:2205.06908v1 [cs.RO] 13 May 2022Figure 1: Agile flight through narrow gates. (A) Caltech Real Weather Wind Tunnel system, the quadrotor UA V , and the gate. In our flight tests, the UA V follows an agile trajectory through narrow gates, which are slightly wider than the UA V itself, under challenging wind conditions. ( B-C) Trajectories used for the gate tests. In (B), the UA V follows a gure-8 through one gate, with wind speed 3:1 m=sor time-varying wind condition. In (C), the UA V follows an ellipse in the horizontal plane through two gates, with wind speed 3:1 m=s. (D-E) Long-exposure photos (with an exposure time of 5 s) showing one lap in two tasks. ( F-I) High-speed photos (with a shutter speed of 1/200 s) showing the moment the UA V passed through the gate and the interaction between the UA V and the wind. 2Unmodeled and often complex aerodynamics are among the most notable challenges to precise flight control. Flying in windy environments (as shown in Fig. 1) introduces even more complexity because of the unsteady aerodynamic interactions between the drone, the induced airflow, and the wind (see Fig. 1(F) for a smoke visualization). These unsteady and nonlinear aerodynamic effects substantially degrade the performance of conventional UA V control methods that neglect to account for them in the control design. Prior approaches partially capture these effects with simple linear or quadratic air drag models, which limit the tracking performance in agile flight and cannot be extended to external wind conditions [1, 2]. Although more complex aerodynamic models can be derived from computational fluid dynamics [3], such modelling is often computationally expensive, and is limited to steady non-dynamic wind conditions. Adaptive control addresses this problem by estimating linear parametric uncertainty in the dynamical model in real time to improve tracking performance. Recent state-of-the-art in quadrotor flight control has used adaptive con- trol methods that directly estimate the unknown aerodynamic force without assuming the structure of the underlying physics, but relying on high-frequency and low-latency control [4, 5, 6, 7]. In parallel, there has been increased interest in data-driven modeling of aerodynamics (e.g., [8, 9, 10, 11]), however exist- ing approaches cannot effectively adapt in changing or unknown environments such as time-varying wind conditions. In this article, we present a data-driven approach called Neural-Fly, which is a deep-learning-based tra- jectory tracking controller that learns to quickly adapt to rapidly-changing wind conditions. Our method, de- picted in Fig. 2, advances and offers insights into both adaptive flight control and deep-learning-based robot control. Our experimental demonstrates that Neural-Fly achieves centimeter-level position-error tracking of an agile and challenging trajectory in dynamic wind conditions on a standard UA V . Our method has two main components: an offline learning phase and an online adaptive control phase used as real-time online learning. For the offline learning phase, we have developed Domain Adversarially Invariant Meta-Learning (DAIML) that learns a wind-condition-independent deep (DNN) representation of the aerodynamics in a data-efcient manner. The output of the DNN is treated as a set of basis functions that represent the aerodynamic effects. This representation is adapted to different wind conditions by updating a set of linear coefcients that mix the output of the DNN. DAIML is data ef- cient and uses only 12 total minutes of flight data in just 6 different wind conditions to train the DNN. DAIML incorporates several key features which not only improve the data efciency but also are informed by the downstream online adaptive control phase. In particular, DAIML uses spectral normalization [8, 12] to control the Lipschitz property of the DNN to improve generalization to unseen data and provide closed- loop stability and robustness guarantees. DAIML also uses a discriminative network, which ensures that the learned representation is wind-invariant and that the wind-dependent information is only contained in the linear coefcients that are adapted in the online control phase. For the online adaptive control phase, we have developed a regularized composite adaptive control law, which we derived from a fundamental understanding of how the learned representation interacts with the closed-loop control system and which we support with rigorous theory. The adaptation law updates the wind-dependent linear coefcients using a composite of the position tracking error term and the aerodynamic force prediction error term. Such a principled approach effectively guarantees stable and fast adaptation to any wind condition and robustness against imperfect learning. Although this adaptive control law could be used with a number of learned models, the speed of adaptation is further aided by the concise representation learned from DAIML. Using Neural-Fly, we report an average improvement of 66 % over a nonlinear tracking controller, 42 % over anL1adaptive controller, and 35 % over an Incremental Nonlinear Dynamics Inversion (INDI) con- troller. These results are all accomplished using standard quadrotor UA V hardware, while running the PX4's 3Velocity Attitude PWMLearned basis function net from K wind conditionsA Online adaptation B Offline meta-learning C Control diagram Learned basis function Feedforward Gravity Feedback+Model kinematics Flight control unitNominal dynamicsOnline adaptation block Desired trajectory Drone state error Tracking-based adaptationPrediction-based adaptationTracking control +Tracking-based adaptationPrediction-based adaptation Vehicle plantFigure 2: Offline meta-learning and online adaptive control design. (A) The online adaptation block in our adaptive controller. Our controller leverages the meta-trained basis function , which is a wind-invariant representation of the aerodynamic effects, and uses composite adaptation (that is, including tracking-error- based and prediction-error-based adaptation) to update wind-specic linear weights ^a. The output of this block is the wind-effect force estimate, ^f= ^a. (B) The illustration of our meta-learning algorithm DAIML. We collected data from wind conditions fw1;\u0001\u0001\u0001;wKgand applied Algorithm 1 to train the net. ( C) The diagram of our control method, where the grey part corresponds to (A). Interpreting the learned block as an aerodynamic force allows it to be incorporated into the feedback control easily. 4default regulation attitude control. Our tracking performance is competitive even compared to related work without external wind disturbances and with more complex hardware (for example, [4] requires a 10-time higher control frequency and onboard optical sensors for direct motor speed feedback). We also com- pare Neural-Fly with two variants of our method: Neural-Fly-Transfer, which uses a learned representation trained on data from a different drone, and Neural-Fly-Constant, which only uses our adaptive control law with a trivial non-learning basis. Neural-Fly-Transfer demonstrates that our method is robust to changes in vehicle conguration and model mismatch. Neural-Fly-Constant, L1, and INDI all directly adapt to the unknown dynamics without assuming the structure of the underlying physics, and they have similar perfor- mance. Furthermore, we demonstrate that our method enables a new set of capabilities that allow the UA V to fly through low-clearance gates following agile trajectories in gusty wind conditions (Fig. 1). Related Work for Precise Quadrotor Control Typical quadrotor control consists of a cascaded or hierarchical control structure which separates the design of the position controller, attitude controller, and thrust mixer (allocation). Commonly-used off-the- shelf controllers, such as PX4, design each of these loops as proportional-integral-derivative (PID) regulation controllers [13]. The control performance can be substantially improved by designing each layer of the cascaded controller as a tracking controller using the concept of differential flatness [14], or, as has recently been popular, using a single optimization based controller such as model predictive control (MPC) to directly compute motor speed commands from desired trajectories. State-of-the-art tracking performance relies on MPC with fast adaptive inner loops to correct for modeling errors [4, 7], however, this approach requires full custom flight controllers. In contrast, our method is designed to be integrated with a typical PX4 flight controller, yet it achieves state-of-the-art flight performance in wind. Prior work on agile quadrotor control has achieved impressive results by considering aerodynamics [4, 7, 11, 2]. However, those approaches require specialized onboard hardware [4], full custom flight control stacks [4, 7], or cannot adapt to external wind disturbances [11, 2]. For example, state-of-the-art tracking performance has been demonstrated using incremental nonlinear dynamics inversion to estimate aerody- namic disturbance forces, with a root-mean-square tracking error of 6:6 cm and drone ground speeds up to12:9 m=s[4]. However, [4] relies on high-frequency control updates ( 500 Hz ) and direct motor speed feedback using optical encoders to rapidly estimate external disturbances. Both are challenging to deploy on standard systems. [7] simplies the hardware setup and does not require optical motor speed sensors and has demonstrated state-of-the-art tracking performance. However, [7] relies on a high-rate L1adaptive controller inside a model predictive controller and uses a racing drone with a fully customized control stack. [11] leverages an aerodynamic model learned offline and represented as Gaussian Processes. However, [11] cannot adapt to unknown or changing wind conditions and provides no theoretical guarantees. Another recent work focuses on deriving simplied rotor-drag models that are differentially flat [2]. However, [2] focuses on horizontal, xy\u0000plane trajectories at ground speeds of 4 m=swithout external wind, where the thrust is more constant than ours, achieves \u00186 cm tracking error [2], uses an attitude controller running at 4000 Hz , and is not extensible to faster flights as pointed out in [11]. Relation between Neural-Fly and Conventional Adaptive Control Adaptive control theory has been extensively studied for online control and identication problems with parametric uncertainty, for example, unknown linear coefcients for mixing known basis functions [15, 16, 17, 18, 19, 20]. There are three common aspects of adaptive control which must be addressed carefully in any well-designed system and which we address in Neural-Fly: designing suitable basis functions for online 5adaptation, stability of the closed-loop system, and persistence of excitation, which is a property related to robustness against disturbances. These challenges arise due to the coupling between the unknown underlying dynamics and the online adaptation. This coupling precludes naive combinations of online learning and control. For example, gradient-based parameter adaptation has well-known stability and robustness issues as discussed in [15]. The basis functions play a crucial role in the performance of adaptive control, but designing or selecting proper basis functions might be challenging. A good set of basis functions should reflect important features of the underlying physics. In practice, basis functions are often designed using physics-informed modeling of the system, such as the nonlinear aerodynamic modeling in [21]. However, physics-informed modeling requires a tremendous amount of prior knowledge and human labor, and is often still inaccurate. Another approach is to use random features as the basis set, such as random Fourier features [22, 23], which can model all possible underlying physics as long as the number of features is large enough. However, the high-dimensional feature space is not optimal for a specic system because many of the features might be redundant or irrelevant. Such suboptimality and redundancy not only increase the computational burden but also slow down the convergence speed of the adaptation process. Given a set of basis functions, naive adaptive control designs may cause instability and fragility in the closed-loop system, due to the nontrivial coupling between the adapted model and the system dynamics. In particular, asymptotically stable adaptive control cannot guarantee robustness against disturbances and so exponential stability is desired. Even so, often, existing adaptive control methods only guarantee expo- nential stability when the desired trajectory is persistently exciting, by which information about all of the coefcients (including irrelevant ones) is constantly provided at the required spatial and time scales. In prac- tice, persistent excitation requires either a succinct set of basis functions or perturbing the desired trajectory, which compromises tracking performance. Recent multirotor flight control methods, including INDI [4] and L1adaptive control, presented in [5] and demonstrated inside a model predictive control loop in [7], achieve good results by abandoning complex basis functions. Instead, these methods directly estimate the aerodynamic residual force vector. The residual force is observable, thus, these methods bypass the challenge of designing good basis functions and the associated stability and persistent excitation issues. However, these methods suffer from lag in estimating the residual force and encounter the the lter design performance trade of reduced lag versus amplied noise. Neural-Fly-Constant only uses Neural-Fly's composite adaptation law to estimate the residual force, and therefore, Neural-Fly-Constant also falls into this class of adaptive control structures. The results of this article demonstrate that the inherent estimation lag in these existing methods limits performance on agile trajectories and in strong wind conditions. Neural-Fly solves the aforementioned issues of basis function design and adaptive control stability, us- ing newly developed methods for meta-learning and composite adaptation that can be seamlessly integrated together. Neural-Fly uses DAIML and flight data to learn an effective and compact set of basis functions, represented as a DNN. The regularized composite adaptation law uses the learned basis functions to quickly respond to wind conditions. Neural-Fly enjoys fast adaptation because of the conciseness of the feature space, and it guarantees closed-loop exponential stability and robustness without assuming persistent exci- tation. Related to Neural-Fly, neural network based adaptive control has been researched extensively, but by and large was limited to shallow or single-layer neural networks without pretraining. Some early works focus on shallow or single-layer neural networks with unknown parameters which are adapted online [19, 24, 25, 26, 27]. A recent work applies this idea to perform an impressive quadrotor flip [28]. However, the existing neural network based adaptive control work does not employ multi-layer DNNs, and lacks a principled 6and efcient mechanism to pretrain the neural network before deployment. Instead of using shallow neural networks, recent trends in machine learning highly rely on DNNs due to their representation power [29]. In this work, we leverage modern deep learning advances to pretrain a DNN which represents the underlying physics compactly and effectively. Related Work in Multi-environment Deep Learning for Robot Control Recently, researchers have been addressing the data and computation requirements for DNNs to help the eld progress towards the fast online-learning paradigm. In turn, this progress has been enabling adaptable DNN-based control in dynamic environments. The most popular learning scheme in dynamic environments is meta-learning, or \"learning-to-learn\", which aims to learn an efcient model from data across different tasks or environments [30, 31, 32]. The learned model, typically represented as a DNN, ideally should be capable of rapid adaptation to a new task or an unseen environment given limited data. For robotic appli- cations, meta-learning has shown great potential for enabling autonomy in highly-dynamic environments. For example, it has enabled quick adaptation against unseen terrain or slopes for legged robots [33, 34], changing suspended payload for drones [35], and unknown operating conditions for wheeled robots [36]. In general, learning algorithms typically can be decomposed into two phases: offline learning and online adaptation. In the offline learning phase, the goal is to learn a model from data collected in different envi- ronments, such that the model contains shared knowledge or features across all environment, for example, learning aerodynamic features shared by all wind conditions. In the online adaptation phase, the goal is to adapt the offline-learned model, given limited online data from a new environment or a new task, for example, ne tuning the aerodynamic features in a specic wind condition. There are two ways that the offline-learned model can be adapted. In the rst class, the adaptation phase adapts the whole neural network model, typically using one or more gradient descent steps [30, 33, 35, 37]. However, due to the notoriously data-hungry and high-dimensional nature of neural networks, for real-world robots it is still impossible to run such adaptation on-board as fast as the feedback control loop (e.g., \u0018100Hz for quadrotor). Furthermore, adapting the whole neural network often lacks explainability and robustness and could generate unpredictable outputs that make the closed-loop unstable. In the second class (including Neural-Fly), the online adaptation only adapts a relatively small part of the learned model, for example, the last layer of the neural network [38, 36, 39, 40]. The intuition is that, different environments share a common representation (e.g., the wind-invariant representation in Fig. 2(A)), and the environment-specic part is in a low-dimensional space (e.g., the wind-specic linear weight in Fig. 2(A)), which enables the real-time adaptation as fast as the control loop. In particular, the idea of inte- grating meta-learning with adaptive control is rst presented in our prior work [38], later followed by [39]. However, the representation learned in [38] is ineffective and the tracking performance in [38] is similar as the baselines; [39] focuses on a planar and fully-actuated rotorcraft simulation without experiment valida- tion and there is no stability or robustness analysis. Neural-Fly instead learns an effective representation using a our meta-learning algorithm called DAIML, demonstrates state-of-the-art tracking performance on real drones, and achieves non-trivial stability and robustness guarantees. Another popular deep-learning approach for control in dynamic environments is robust policy learning via domain randomization [41, 42, 43]. The key idea is to train the policy with random physical parame- ters such that the controller is robust to a range of conditions. For example, the quadrupedal locomotion controller in [41] retains its robustness over challenging natural terrains. However, robust policy learning optimizes average performance under a broad range of conditions rather than achieving precise control by adapting to specic environments. 72 RESULTS In this section, we rst discuss the experimental platform for data collection and experiments. Second, we discuss the key conceptual reasoning behind our combined method of our meta-learning algorithm, called DAIML, and our composite adaptive controller with stability guarantees. Third, we discuss several experiments to quantitatively compare the closed-loop trajectory-tracking performance of our methods to a nonlinear baseline method and two state-of-the-art adaptive flight control methods, and we observe our methods reduce the average tracking error substantially. In order to demonstrate the new capabilities brought by our methods, we present agile flight results in gusty winds, where the UA V must quickly fly through narrow gates that are only slightly wider than the vehicle. Finally, we show our methods are also applicable in outdoor agile tracking tasks without external motion capture systems. Figure 3: Training data collection. (A) The xyz position along a two-minute randomized trajectory for data collection with wind speed 8:3 km=h(3:7 m=s), in the Caltech Real Weather Wind Tunnel. ( B) A typical 10-second trajectory of the inputs (velocity, attitude quaternion, and motor speed PWM command) and label (offline calculation of aerodynamic residual force) for our learning model, corresponding to the highlighted part in (A). ( C) Histograms showing data distributions in different wind conditions. ( C)Left: distributions of thex-component of the wind-effect force, fx. This shows that the aerodynamic effect changes as the wind varies. ( C)Right: distributions of the pitch, a component of the state used as an input to the learning model. This shows that the shift in wind conditions causes a distribution shift in the input. 8Experimental Platform All of our experiments are conducted at Caltech's Center for Autonomous Systems and Technologies (CAST). The experimental setup consists of an OptiTrack motion capture system with 12 infrared cameras for lo- calization streaming position measurements at 50 Hz , a WiFi router for communication, the Caltech Real Weather Wind Tunnel for generating dynamic wind conditions, and a custom-built quadrotor UA V . The Real Weather Wind Tunnel is composed of 1296 individually controlled fans and can generate uniform wind speeds of up to 43:6 km=hin its 3x3x5 mtest section. For outdoor flight, the drone is also equipped with a Global Positioning System (GPS) module and an external antenna. We now discuss the design of the UA V and the wind condition in detail. UA V Design We built a quadrotor UA V for our primary data collection and all experiments, shown in Fig. 1(A). The quadrotor weighs 2:6 kg with a thrust to weight ratio of 2:2. The UA V is equipped with a Pixhawk flight controller running PX4, an open-source commonly used drone autopilot platform [13]. The UA V incorporates a Raspberry Pi 4 onboard computer running a Linux operation system, which performs real-time computation and adaptive control and interfaces with the flight controller through MA VROS, an open-source set of communication drivers for UA Vs. State estimation is performed using the built-in PX4 Extended Kalman Filter (EKF), which fuses inertial measurement unit (IMU) data with global position estimates from OptiTrack motion capture system (or the GPS module for outdoor flight tasks). The UA V platform features a wide-X conguration, measuring 85 cm in width, 75 cm in length, and 93 cm diagonally, and tilted motors for improved yaw authority. This general hardware setup is standard and similar to many quadrotors. We refer to the supplementary materials (Section S1) for further conguration details. We implemented our control algorithm and the baseline control methods in the position control loop in Python, and run it on the onboard Linux computer at 50 Hz . The PX4 was set to the offboard flight mode and received thrust and attitude commands from the position control loop. The built-in PX4 multicopter attitude controller was then executed at the default rate, which is a linear PID regulation controller on the quaternion error. The online inference of the learned representation is also in Python via PyTorch, which is an open source deep learning framework. To study the generalizability and robustness of our approach, we also use an Intel Aero Ready to Fly drone for data collection. This dataset is used to train a representation of the wind effects on the Intel Aero drone, which we test on our custom UA V . The Intel Aero drone (weighing 1:4 kg) has a symmetric X conguration, 52 cm in width and 52 cm in length, without tilted motors (see the supplementary materials for further details). Wind Condition Design To generate dynamic and diverse wind conditions for the data collection and ex- periments, we leverage the state-of-the-art Caltech Real Weather Wind Tunnel system (Fig. 1(A)). The wind tunnel is a 3 mby3 marray of 1296 independently controllable fans capable of generating wind conditions up to 43:6 km=h. The distributed fans are controlled in real-time by a Python-based Application Program- ming Interface (API). For data collection and flight experiments, we designed two types of wind conditions. For the rst type, each fan has uniform and constant wind speed between 0 km=hand43:6 km=h(12:1 m=s). The second type of wind follows a sinusoidal function in time, e.g., 30:6 + 8:6 sin(t) km=h. Note that the training data only covers constant wind speeds up to 6:1 m=s. To visualize the wind, we use 5smoke generators to indicate the direction and intensity of the wind condition (see examples in Fig. 1 and Video 1). 9Training epoch 0 Training epoch 30 Training epoch 900.0 m/s 1.3 m/s 2.5 m/s Without adversarial loss Training epoch 90t-SNE plots of the linear coefficients (a*) in the training processFigure 4: t-SNE plots showing the evolution of the linear weights ( a\u0003) during the training process. As the number of training epochs increases, the distribution of a\u0003becomes more clustered with similar wind speed clusters near each other. The clustering also has a physical meaning: after training convergence, the right top part corresponds to a higher wind speed. This suggests that DAIML successfully learned a basis function shared by all wind conditions, and the wind-dependent information is contained in the linear weights. Compared to the case without the adversarial regularization term (using = 0 in Algorithm 1), the learned result using our algorithm is also more explainable, in the sense that the linear coefcients in different conditions are more disentangled. Offline Learning and Online Adaptive Control Development Data Collection and Meta-Learning using DAIML To learn an effective representation of the aero- dynamic effects, we have a custom-built drone follow a randomized trajectory for 2 minutes each in six different static wind conditions, with speeds ranging from 0 km=hto22:0 km=h. However, in experiments we used wind speeds up to 43:6 km=h(12:1 m=s) to study how our methods extrapolate to unseen wind conditions (e.g., Fig. 6). The data is collected at 50 Hz with a total of 36;000data points. Figure 3(A) shows the data collection process, and Fig. 3(B) shows the inputs and labels of the training data, under one wind condition of 13:3 km=h(3:7 m=s). Figure 3(C) shows the distributions of input data (pitch) and label data (x\u0000component of the aerodynamic force) in different wind conditions. Clearly, a shift in wind conditions causes distribution shifts in both input domain and label domain, which motivates the algorithm design of DAIML. The same data collection process is repeated on the Intel Aero drone, to study whether the learned representation can generalize to a different drone. On the collected datasets for both our custom drone and the Intel Aero drone, we apply the DAIML al- gorithm to learn two representations of the wind effects. The learning process is done offline on a normal desktop computer, and depicted in Fig. 2(B). Figure 4 shows the evolution of the linear coefcients ( a\u0003) during the learning process, where DAIML learns a representation of the aerodynamic effects shared by all wind conditions, and the linear coefcient contains the wind-specic information. Moreover, the learned representation is explainable in the sense that the linear coefcients in different wind conditions are well disentangled (see Fig. 4). We refer to the \"Materials and Methods\" section for more details. Baselines and the Variants of Our Method We briefly introduce three variants of our method and the three baseline methods considered (details are provided in the \"Materials and Methods\" section). Each of the 10controllers is implemented in the position control loop and outputs a force command. The force command is fed into a kinematics block to determine a corresponding attitude and thrust, similar to [14], which is sent to the PX4 flight controller. The three baselines include: globally exponentially-stabilizing nonlinear controller dynamics [4], and L1adaptive control [5, 7]. The primary difference between these baseline methods and Neural-Fly is how the controller compensates for the unmodelled residual force (that is, each baseline method has the same control structure, in Fig. 2(C), except for the estimation of the ^f). In the case of the nonlinear baseline controller an integral term accumulates error to correct for the modeling error. The integral gain is limited by the stability of the interaction with the position and velocity error feedback leading to slow model correction. In contrast, both INDI and L1decouple the adaptation rate from the PD gains, which allow for fast adaptation. Instead, these methods are limited by more fundamental design factors, such as system delay, measurement noise, and controller rate. Our method is illustrated in Fig. 2(A,C) and replaces the integral feedback term with an adapted learning term. The deployment of our approach depends on the learned representation function , and our primary method and two variants consider a different choice of . Neural-Fly is our primary method using a rep- resentation learned from the dataset collected by the custom-built drone, which is the same drone used in experiments. Neural-Fly-Transfer uses the Neural-Fly algorithm where the representation is trained using the dataset collected by the aforementioned Intel Aero drone. Neural-Fly-Constant uses the online adapta- tion algorithm from Neural-Fly, but the representation is an articially designed constant mapping. Neural- Fly-Transfer is included to show the generalizability and robustness of our approach with drone transfer, i.e., using a different drone in experiments than data collection. Finally, Neural-Fly-Constant demonstrates the benet of using a better representation learned from the proposed meta-learning method DAIML. Note that Neural-Fly-Constant is a composite adaptation form of a Kalman-lter disturbance observer, that is a Kalman-lter augmented with a tracking error update term. Trajectory Tracking Performance We quantitatively compare the performance of the aforementioned control methods when the UA V follows a2:5 mwide, 1:5 mtall gure-8 trajectory with a sin(t) m=s). The flight trajectory for each of the experiments is shown in Fig. 5, which includes a warm up lap and six 6:28 slaps. The nonlinear baseline integral term compensates for the mean model error within the rst lap. As the wind speed increases, the aerodynamic force variation becomes larger and we notice a substantial performance degradation. INDI and L1both improve over the nonlinear baseline, but INDI is more robust thanL1at high wind speeds. Neural-Fly-Constant outperforms INDI except during the two most challenging tasks: 43:6 km=hand sinusoidal wind speeds. The learning based methods, Neural-Fly and Neural-Fly- Transfer, outperform all other methods in all tests. Neural-Fly outperforms Neural-Fly-Transfer slightly, which is because the learned model was trained on data from the same drone and thus better matches the dynamics of the vehicle. In Table 1, we tabulate the root-mean-square position error and mean position error values over the six laps for each experiment. Figure 6 shows how the mean tracking error changes for each controller as the wind speed increases, and includes the standard deviation for the mean lap position error. In all cases, Neural-Fly and Neural-Fly-Transfer outperform the state-of-the-art baseline methods, including the 30:6 km=h,43:6 km=h, and sinusoidal wind speeds all of which exceed the wind speed in the training data. 11All of these results presents a clear trend: adaptive control substantially outperforms the nonlinear baseline which relies on integral-control, and learning markedly improves adaptive control. Figure 5: Depiction of the trajectory tracking performance of each controller in several wind condi- tions. The baseline nonlinear controller can track the trajectory well, however, the performance substantially degrades at higher wind speeds. INDI, L1, and Neural-Fly-Constant have similar performance and im- prove over the nonlinear baseline by estimating the aerodynamic disturbance force quickly. Neural-Fly and Neural-Fly-Transfer use a learned model of the aerodynamic effects and adapt the model in real time to achieve lower tracking error than the other methods. Agile Flight Through Narrow Gates Precise flight control in dynamic and strong wind conditions has many applications, such as rescue and search, delivery, and transportation. In this section, we present a challenging drone flight task in strong winds, where the drone must follow agile trajectories through narrow gates, which are only slightly wider than the drone. The overall result is depicted in Fig. 1 and Video 1. As shown in Fig. 1(A), the gates used in our experiments are 110 cm in width, which is only slightly wider than the drone ( 85 cm wide, 75 cm long). To visualize the trajectory using long-exposure photography, our drone is deployed with four main light emitting diodes (LEDs) on its legs, where the two rear LEDs are red and the front two are white. There are also several small LEDs on the flight controller, the computer, and the motor controllers, which can be seen in the long-exposure shots. Task Design We tested our method on three different tasks. In the rst task (see Fig. 1(B,D,F-I) and Video 1), the desired trajectory is a 3 mby1:5 mgure-8 in the x\u0000zplane with a lap time of 5 s. A gate is 120.0 4.2 8.5 12.1 Wind speed [m/s]3.25.610.017.831.6Tracking error [cm]Interpolation regionExtrapolation region (not covered in training)Mean tracking Nonlinear INDI L1 NF-Constant NF-Transfer NF PIDFigure 6: Mean tracking errors of each lap in different wind conditions. This gure shows position tracking errors of different methods as wind speed increases. Solid lines show the mean error over 6 laps and the shade areas show standard deviation of the mean error on each lap. The grey area indicates the extrapolation region, where the wind speeds are not covered in training. Our primary method (Neural-Fly) achieves state-of-the-art performance even with a strong wind disturbance. placed at the left bottom part of the trajectory. The minimum clearance is about 10 cm (see Fig. 1(I), which requires that the controller precisely tracks the trajectory. The maximum speed and acceleration of the desired trajectory are 2:7 m=sand5:0 m=s2, respectively. The wind speed is 3:1 m=s. The second task (see Video 1) is the same as the rst one, except that it uses a more challenging, time-varying wind condition, 3:1 + 1:8 sin(2\u0019 5t)m=s. In the third task (see Fig. 1(C,E) and Video 1), the desired trajectory is a 3 mby 2:5 mellipse in the x\u0000yplane with a lap time of 5 s. We placed two gates on the left and right sides of the ellipse. As with the rst task, the wind speed is 3:1 m=s. Performance For all three tasks, we used our primary method, Neural-Fly, where the representation is learned using the dataset collected by the custom-built drone. Figure 1(D,E) are two long-exposure photos with an exposure time of 5 s, which is the same as the lap time of the desired trajectory. We see that our method precisely tracked the desired trajectories and flew safely through the gates (see Video 1). These long- exposure photos also captured the smoke visualization of the wind condition. We would like to emphasize that the drone is wider than the LED light region, since the LEDs are located on the legs (see Fig. 1(A)). Figure 1(F-I) are four high-speed photos with a shutter speed of 1/200 s. These four photos captured the moment the drone passed through the gate in the rst task, as well as the complex interaction between the drone and the wind. We see that the aerodynamic effects are complex and non-stationary and depend on the UA V attitude, the relative velocity, and aerodynamic interactions between the propellers and the wind. 13Table 1: Tracking error statistics in cmfor different wind conditions. Two metrics are considered: root- mean-square (RMS) and mean. PPPPPPPPPPPMethodWind speed [m/s]0 4.2 8.5 12.1 8:5 + 2:4 sin(t) RMS Mean RMS Mean RMS Mean RMS Mean RMS Mean Nonlinear 11.9 10.8 10.7 9.9 16.3 14.7 23.9 21.6 31.2 28.2 INDI 7.3 6.3 6.4 5.9 8.5 8.2 10.7 10.1 11.1 10.3 L1 4.6 4.2 5.8 5.2 12.1 11.1 22.7 21.3 13.0 11.6 NF-Constant 5.4 5.0 6.1 5.7 7.5 6.9 12.7 11.2 12.7 12.1 NF-Transfer 3.7 3.4 4.8 4.4 6.2 5.9 10.2 9.4 8.8 8.0 NF 3.2 2.9 4.0 3.7 5.8 5.3 9.4 8.7 7.6 6.9 Outdoor Experiments We tested our algorithm outdoors in gentle breeze conditions (wind speeds measured up to 17 km=h). An onboard GPS receiver provided position information to the EKF, giving lower precision state estimation, and therefore less precise aerodynamic residual force estimation. Following the same aforementioned gure-8 trajectory, the controller reached 7:5 cm mean tracking error, shown in Fig. 7. 3 DISCUSSION State-of-the-art Tracking Performance When measuring position tracking errors, we observe that our Neural-Fly method outperforms state-of- the-art flight controllers in all wind conditions. Neural-Fly uses deep learning to obtain a compact rep- resentation of the aerodynamic disturbances and incorporates that representation into an adaptive control design to achieve high precision tracking performance. The benchmark methods used in this article are nonlinear control, INDI, and L1and performance is compared tracking an agile gure-8 in constant and time-varying wind speeds up to 43:6 km=h(12:1 m=s). Furthermore, we observe a mean tracking error of 2:9 cm in0 km=hwind, which is comparable with state-of-the-art tracking performance demonstrated on more aggressive racing drones [4, 7] despite several architectural limitations such as limited control rate in offboard mode, a larger, less maneuverable vehicle, and without direct motor speed measuremnts. All our experiments were conducted using the standard PX4 attitude controller, with Neural-Fly implemented in an onboard, low cost, and \"credit-card sized\" Raspberry Pi 4 computer. Furthermore, Neural-Fly is robust to changes in vehicle conguration, as demonstrated by the similar performance of Neural-Fly-Transfer. To understand the fundamental tracking-error limit, we estimate that the localization precision from the OptiTrack system is about 1 cm , which is a practical lower bound for the average tracking error in our system (see more details in the supplementary material, Section S8). This is based on the fact that the difference between the OptiTrack position measurement and the onboard EKF position estimate is around 1 cm . To achieve a tracking error of 1 cm , remaining improvements should focus on reducing code execution time, communication delays, and attitude tracking delay. We measured the combined code execution time and communication delay to be at least 15 ms and often as much as 30 ms . A faster implementation (such as using C++ instead of Python) and streamlined communication layer (such as using ROS2's real-time fea- tures) could allow us to achieve tracking errors on the order of the localization accuracy. Attitude tracking delay can be substantially reduced through the use of a nonlinear attitude controller (e.g., [45]). Our method 14weather stationGPS 2.0 1.5 1.0 0.5 0.0 0.5 x [m]7.007.257.507.758.008.258.508.75z [m]Outdoor tracking performance RMS error=8.2cm, mean error=7.5cm 131030>50 Tracking error [cm]Figure 7: Outdoor flight setup and performance. Left: In outdoor experiments, a GPS module is deployed for state estimation, and a weather station records wind proles. The maximum wind speed during the test was around 17 km=h(4:9 m=s).Right: Trajectory tracking performance of Neural-Fly. is also directly extensible to attitude control because attitude dynamics match the Euler-Lagrange dynam- ics used in our derivations. However, further work is needed to understand the interaction of the learned dynamics with the cascaded control design when implementing a tracking attitude controller. We have tested our control method in outdoor flight to demonstrate that it is robust to less precise state estimation and does not rely on any particular features of our test facility. Although control and estimation are usually separately designed parts of an autonomous system, aggressive adaptive control requires minimal noise in force measurement to effectively and quickly compensate for unmodelled dynamics. Testing our method in outdoor flight, the quadrotor maintains precise tracking with only 7:5 cm tracking error on a gentle breezy day with wind speeds around 17 km=h. Challenges Caused by Unknown and Time-varying Wind Conditions In the real world, the wind conditions are not only unknown but also constantly changing, and the vehicle must continuously adapt. We designed the the sinusoidal wind test to emulate unsteady or gusty wind conditions. Although our learned model is trained on static and approximately uniform wind condition data, Neural-Fly can quickly identify changing wind speed and maintains precise tracking even on the sinusoidal wind experiment. Moreover, in each of our experiments, the wind conditions were unknown to the UA V before starting the test yet were quickly identied by the adaptation algorithm. Our work demonstrated that it is possible to repeatably and quantitatively test quadrotor flight in time- varying wind. Our method separately learns the wind effect's dependence on the vehicle state (i.e., the wind-invariant representation in Fig. 2(A)) and the wind condition (i.e., the wind-specic linear weight in Fig. 2(A)). This separation allows Neural-Fly to quickly adapt to the time-varying wind even as the UA V follows a dynamic trajectory, with an average tracking error below 8:7 cm in Table 1. 15Computational Efciency of Our Method In the offline meta-learning phase, the proposed DAIML algorithm is able to learn an effective representation of the aerodynamic effect in a data efcient manner. This requires only 12 minutes of flight data at 50 Hz , for a total of 36,000 data points. The training procedure only takes 5 minutes on a normal desktop computer. In the online adaptation phase, our adaptive control method only takes 10 ms to compute on a compact onboard Linux computer (Raspberry Pi 4). In particular, the feedforward inference time via the learned basis function is about 3:5 ms and the adaptation update is about 3:0 ms , which implies the compactness of the learned representation. Generalization to New Trajectories and New Aircraft It is worth noting that our control method is orthogonal to the design of the desired trajectory. In this article, we focus on the gure-8 trajectory which is a commonly used control benchmark. We also demonstrate our method flying a horizontal ellipse during the narrow gate demonstration Fig. 1. Note that our method supports any trajectory planners such as [1] or learning-based planners [46, 47]. In particular, for those planners which require a precise and agile downstream controller (e.g., for close-proximity flight or drone racing [1, 10]), our method immediately provides a solution and further pushes the boundary of these plan- ners, because our state-of-the-art tracking capabilities enable tighter congurations and smaller clearances. However, further research is required to understand the coupling between planning and learning-based con- trol near actuation limits. Future work will consider using Neural-Fly in a combined planning and control structure such as MPC, which will be able to handle actuation limits. The comparison between Neural-Fly and Neural-Fly-Transfer show that our approach is robust to chang- ing vehicle design and the learned representation does not depend on the vehicle. This demonstrates the generalizability of the proposed method running on different quadrotors. Moreover, our control algorithm is formulated generally for all robotic systems described by the Euler-Langrange equation (see \"Materials and Methods\"), including many types of aircraft such as [21, 48]. 4 MATERIALS AND METHODS Overview We consider a general robot dynamics model: M(q)\u007fq+C(q;_q) _q+g(q) =u+f(q;_q;w) (1) whereq;_q;\u007fq2Rnare thendimensional position, velocity, and vectors, M(q)is the symmetric, positive denite inertia matrix, C(q;_q)is the Coriolis matrix, g(q)is the gravitational force vector and u2Rnis the control force. Most importantly, f(q;_q;w)incorporates unmodeled dynamics, and w2Rm is an unknown hidden state used to represent the underlying environmental conditions, which is potentially time-variant. Specically, in this article, wrepresents the wind prole (for example, the wind prole in Fig. 1), and different wind proles yield different unmodeled aerodynamic disturbances for the UA V . Neural-Fly can be broken into two main stages, the offline meta-learning stage and the online adaptive control stage. These two stages build a model of the unknown dynamics of the form f(q;_q;w)\u0019 (q;_q)a(w); (2) 16where is a basis or representation function shared by all wind conditions and captures the dependence of the unmodeled dynamics on the robot state, and ais a set of linear coefcients that is updated for each condition. In the supplementary material (Section S2), we prove that the decomposition (q;_q)a(w)exists for any analytic function f(q;_q;w). In the offline meta-learning stage, we learn as a DNN using our meta-learning algorithm DAIML. This stage results in learning as a wind-invariant representation of the unmodeled dynamics, which generalizes to new trajectories and new wind conditions. In the online adaptive control stage, we adapt the linear coefcients ausing adaptive control. Our adaptive control algorithm is a type of composite adaptation and was carefully designed to allow for fast adaptation while maintaining the global exponential stability and robustness of the closed loop system. The offline learning and online control architectures are illustrated in Fig. 2(B) and Fig. 2(A,C), respectively. Data Collection To generate training data to learn a wind-invariant representation of the unmodeled dynamics, the drone tracks a randomized trajectory with the baseline nonlinear controller for 2 minutes each in several different static wind conditions. Figure 3(A) illustrates one trajectory under the wind condition 13:3 km=h(3:7 m=s). The set of input-output pairs for the kthsuch trajectory is referred as the kthsubdataset,Dwk, with the wind condition wk. Our dataset consists of 6 different subdatasets with wind speeds from 0 km=hto 22:0 km=h(6:1 m=s), which are in the white interpolation region in Fig. 6. The trajectory follows a polynomial spline between 3 waypoints: the current position and two randomly generated target positions. The spline is constrained to have zero velocity, acceleration, and jerk at the starting and ending waypoints. Once the end of one spline is reached, the a new random spline is generated and the process repeats for the duration of the training data flight. This process allows us to generate a large amount of data using a trajectory very different from the trajectories used to test our method, such as the gure-8 in Fig. 1. By training and testing on different trajectories, we demonstrate that the learned model generalizes well to new trajectories. Along each trajectory, we collect time-stamped data [q;_q;u]. Next, we compute the acceleration \u007fqby fth-order numerical differentiation. Combining this acceleration with Eq. (1), we get a noisy measurement of the unmodeled dynamics, y=f(x;w) +\u000f, where\u000fincludes all sources of noise (e.g., sensor noise and noise from numerical differentiation) and x= [q; _q]2R2nis the state. Finally, this allows input-output pairs with wind condition wk. As we discuss in the \"Results\" section, in order to show DAIML learns a model which can be transferred between drones, we applied this data collection process on both the custom built drone and the Intel Aero RTF drone. The Domain Adversarially Invariant Meta-Learning (DAIML) Algorithm In this section, we will present the methodology and details of learning the representation function . In particular, we will rst introduce the goal of meta-learning, motivate the proposed algorithm DAIML by the observed domain shift problem from the collected dataset, and nally discuss key algorithmic details. 17Meta-Learning Goal Given the dataset, the goal of meta-learning is to learn a representation (x), such that for any wind condition w, there exists a latent variable a(w)which allows (x)a(w)to approximate f(x;w)well. Formally, an optimal representation, the representation function and ak2Rhis the latent linear coefcient. Note that the optimal weight akis specic to each wind condition, but the optimal representation is shared by all wind conditions. In this article, we use a deep neural network (DNN) to represent . In the supplementary material (Section S2), we prove that for any analytic function f(x;w), the structure (x)a(w)can approximate f(x;w)with an arbitrary precision, as long as the DNN has enough neurons. This result implies that the solved from the optimization in Eq. (4) is a reasonable representation of the unknown dynamics f(x;w). Domain Shift Problems One challenge of the optimization in Eq. (4) is the inherent domain shift inx caused by the shift in w. Recall that during data collection we have a program flying the drone in different winds. The actual flight trajectories differ vastly from wind to wind because of the wind effect. Formally, the distribution of x(i) kvaries between kbecause the underlying environment or context whas changed. For example, as depicted by Fig. 3(C), the drone pitches into the wind, and the average degree of pitch depends on the wind condition. Note that pitch is only one component of the state x. The domain shift in the whole statexis even more drastic. Such inherent shifts in xbring challenges for deep learning. The DNN may memorize the distributions ofxin different wind conditions, such that the variation in the dynamics ff(x;w 1);f(x;w 2);\u0001\u0001\u0001;f(x;wK)g is reflected via the distribution of x, rather than the wind condition fw1;w2;\u0001\u0001\u0001;wKg. In other words, the optimization in Eq. (4) may lead to over-tting and may not properly nd a wind-invariant representation . To solve the domain shift problem, inspired by [49], we propose the following another DNN that works as a discriminator to predict the environment index out of Kwind conditions, loss(\u0001)is a classication loss function (e.g., the cross entropy loss), \u00150is a hyperparameter to control the degree of regularization, kis the wind condition index, and (i)is the input-output pair index. Intuitively,hand play a zero-sum max-min game: the goal of his to predict the index kdirectly from (x) (achieved by the outer max); the goal of is to approximate the label y(i) kwhile making the job of hharder (achieved by the inner min). In other words, his a learned regularizer to remove the environment informa- tion contained in . In our experiments, the output of his aK\u0000dimensional vector for the classication probabilities of Kconditions, and we use the cross entropy loss for loss(\u0001), which is given sample DwkfromD 3 Randomly sample two disjoint batches Ba(adaptation set) and B(training set) from Dwk 4 Solve the least squares problem DNN using stochastic gradient descent normalization of the DAIML Algorithm Finally, we solve the optimization problem in Eq. (5) by the proposed algorithm DAIML (described in Algorithm 1 and illustrated in Fig. 2(B)), which belongs to the category of gradient-based meta-learning [31], but with least squares as the adaptation step. DAIML contains three steps: (i) The adaptation step (Line 4-6) solves an least squares problem as a function of on the adaptation setBa. (ii) The training step (Line 7) updates the learned representation on the training set B, based on the optimal linear coefcient a\u0003solved from the adaptation step. (iii) The regularization step (Line 8-9) updates the discriminator hon the training set. We emphasize important features of DAIML: (i) After the adaptation step, a\u0003is a function of . In other words, in the training step (Line 7), the gradient with respect to the parameters in the neural network will backpropagate through a\u0003. Note that the least-square problem (Line 4) can be solved efciently with a closed-form solution. (ii) The normalization (Line 6) is to make sure ka\u0003k\u0014 , which improves the robustness of our adaptive control design. We also use spectral normalization in training , to control the Lipschitz property of the neural network and improve generalizability [8, 10, 12]. (iii) We train hand in an alternating manner. In each iteration, we rst update (Line 7) while xing hand then update h(Line 9) while xing . However, the probability to update the discriminator hin each iteration is \u0011\u00141instead of1, to improve the convergence of the algorithm [50]. We further motivate the algorithm design using Fig. 3 and Fig. 4. Figure 3(A,B) shows the input and label from one wind condition, and Fig. 3(C) shows the distributions of the pitch component in input and thex\u0000component in label, in different wind conditions. The distribution shift in label implies the impor- tance of meta-learning and adaptive control, because the aerodynamic effect changes drastically as the wind condition switches. On the other hand, the distribution shift in input motivates the need of DAIML. Fig- ure 4 depicts the evolution of the optimal linear coefcient ( a\u0003) solved from the adaptation DAIML, 19via the t-distributed stochastic neighbor embedding (t-SNE) dimension reduction, which projects the 12- dimensional vector a\u0003into 2-d. The distribution of a\u0003is more and more clustered as the number of training epochs increases. In addition, the clustering behavior in Fig. 4 has a concrete physical meaning: right top part of the t-SNE plot corresponds to a higher wind speed. These properties imply the learned representation is indeed shared by all wind conditions, and the linear weight acontains the wind-specic information. Finally, note that with 0 training epoch reflects random features, which cannot decouple different wind conditions as cleanly as the trained representation . Similarly, as shown in Fig. 4, if we ignore the adversar- ial regularization term (by setting = 0), differenta\u0003vectors in different conditions are less disentangled, which indicates that the learned representation might be less robust and explainable. For more discussions about we refer to the supplementary materials (Section S3). Robust Adaptive Controller Design During the offline meta-training process, a least-squares t is used to to nd a set of parameters athat minimizes the force prediction error for each data batch. However, during the online control phase, we are ultimately interested in minimizing the position tracking error and we can improve the adaptation using a more sophisticated update law. Thus, in this section, we propose a more sophisticated adaptation law for the linear coefcients based upon a Kalman-lter estimator. This formulation results in automatic gain tuning for the update law, which allows the controller to quickly estimate parameters with large uncertainty. We further boost this estimator into a composite adaptation law, that is the parameter update depends both on the prediction error in the dynamics model as well as on the tracking error, as illustrated in Fig. 2. This allows the system to quickly identify and adapt to new wind conditions without requiring persistent excitation. In turn, this enables online adaptation of the high dimensional learned models from DAIML. Our online adaptive control algorithm can be summarized by the following control law, adaptation law, and covariance update equations, (9) whereuNFis the control law, _^ais the online linear-parameter update, Pis a covariance-like matrix used for automatic gain tuning, s=_~q+ \u0003~qis the composite tracking error, yis the measured aerodynamic residual force with measurement noise \u000f, andK,\u0003,R,Q, and\u0015are gains. The structure of this control law is illustrated in Fig. 2. Figure 2 also shows further quadrotor specic details for the implementation of our method, including the kinematics block, where the desired thrust and attitude are determined from the desired force from Eq. (7). These blocks are discussed further in the \"Implementation Details\" section. In the next section, we will rst introduce the baseline control laws, \u0016uanduNL. Then we discuss our control law uNFin detail. Note that uNFnot only depends on the desired trajectory, but also requires the learned representation and the linear parameter ^a(an estimation of a). The composite adaptation algorithm for^ais discussed in the following section. In terms of theoretical guarantees, the control law and adaptation law have been designed so that the closed-loop behavior of the system is robust to imperfect learning and time-varying wind conditions. Specif- ically, we dene d(t)as the representation error: f= \u0001a+d(t), theory shows that the robot tracking 20error exponentially converges to an error ball whose size is proportional to kd(t)+\u000fk(i.e., the learning er- ror and measurement noise) and k_ak(i.e., how fast the wind condition changes). Later in this section we formalize these claims with the main stability theorem and present a complete proof in the supplementary materials. Nonlinear Control Law We start by dening some notation. The composite velocity tracking error term sand the reference velocity _qrare dened such that s= _q\u0000_qr=_~q+ \u0003~q (10) where ~q=q\u0000qdis the position tracking error and \u0003is a positive denite gain matrix. Note when s exponentially converges to an error ball around 0,qwill exponentially converge to a proportionate error ball around the desired trajectory qd(t)(see Section S5). Formulating our control law in terms of the composite velocity error ssimplies the analysis and gain tuning without loss of rigor. The baseline nonlinear (NL) control law using PID control gain matrices. Note a standard PID controller typically only includes the PI feedback on position error, D feedback on velocity, and gravity compensation. This only leads to local exponential stability about a xed point, but it is often sufcient for gentle tasks such as a UA V hovering and slow trajectories in static wind conditions. In contrast, this nonlinear controller includes feedback on velocity error and feedforward terms to account for known dynamics and desired acceleration, which allows good tracking of dynamic trajectories in the presence of nonlinearities (e.g., M(q)andC(q;_q) are nonconstant in attitude control). However, this control law only compensates for changing wind condi- tions and unmodeled dynamics through an integral term, which is slow to react to changes in the unmodelled dynamics and disturbance forces. Our method improves the controller by predicting the unmodeled dynamics and disturbance forces, and, indeed, in Table 1 we see a substantial improvement gained by using our learning method. Given the learned representation of the residual dynamics, (q;_q), and the parameter estimate ^a, we replace the integral term with the learned force term, ^f= ^a, resulting in our control law in Eq. (7). Neural-Fly uses trained using DAIML on a dataset collected with the same drone. Neural-Fly-Transfer uses trained using DAIML on a dataset collected with a different drone, the Intel Aero RTF drone. Neural-Fly-Constant does not use any learning but instead uses =Iand is included to demonstrate that the main advantage of our method comes from the incorporation of learning. The learning based methods, Neural-Fly and Neural-Fly- Transfer, outperform Neural-Fly-Constant because the compact learned representation can effectively and quickly predict the aerodynamic disturbances online in Fig. 5. This comparison is further discussed in the supplementary materials (Section S7). Composite Adaptation Law We dene an adaptation law that combines a tracking error update term, a prediction error update term, and a regularization term in Eq. (8) and (9), where yis a noisy measurement off,\u0015is a damping gain, Pis a covariance matrix which evolves according to Eq. (9), and QandRare two positive denite gain matrices. Some readers may note that the regularization term, prediction error term, and covariance update, when taken alone, are in the form of a Kalman-Bucy lter. This Kalman-Bucy 21lter can be derived as that variance of the parameter error [51]. The Kalman-Bucy lter perspective provides intuition for tuning the adaptive controller: the damping gain \u0015cor- responds to how quickly the environment returns to the nominal conditions, Qcorresponds to how quickly the environment changes, and Rcorresponds to the combined representation error dand measurement noise fory. More discussion on the gain tuning process is included in Section S6. However, naively combining this parameter estimator with the controller can lead to instabilities in the closed-loop system behavior un- less extra care is taken in constraining the learned model and tuning the gains. Thus, we have designed our adaptation law to include a tracking error term, making Eq. (8) a composite adaptation law, guaranteeing stability of the closed-loop system (see Theorem 1), and in turn simplifying the gain tuning process. The regularization term allows the stability result to be independent of the persistent excitation of the learned model , which is particularly relevant when using high-dimensional learned representations. The adapta- tion gain and covariance matrix, P, acts as automatic gain tuning for the adaptive controller, which allows the controller to quickly adapt to when a new mode in the learned model is excited. Stability and Robustness Guarantees First we formally dene the representation error d(t), as the dif- ference between the unknown dynamics f(q;_q;w)and the best linear weight vector agiven the learned representation (q;_q), namely,d(t) =f(q;_q;w)\u0000 (q;_q)a(w). The measurement noise for the measured force is a bounded function \u000f(t)such thaty(t) =f(t) +\u000f(t). If the environment conditions are changing, we consider the case that _a6= 0. This leads to the following stability theorem. Theorem 1. If we assume that the desired trajectory has bounded derivatives and the system evolves ac- cording to the dynamics in Eq. (1), the control law Eq. (7), and the adaptation law Eq. (8)and(9), then the position tracking error exponentially converges to ball lim t!1k~qk\u0014sup t\u0002 C1kd(t)k+C2k\u000f(t)k+C3(\u0015ka(t)k+k_a(t)k)\u0003 andC3are on ;R;Q;K; \u0003;Mand\u0015. Implementation Details Quadrotor Dynamics Now we introduce the quadrotor dynamics. Consider states given by global posi- tion,p2R3, velocityv2R3, attitude rotation matrix R2SO(3) , and body angular velocity !2R3. Then dynamics of inertia matrix of the quadrotor, S(\u0001)is the skew-symmetric mapping, gis u= [ x; y; z]>are the total thrust and body torques from four rotors predicted by the nominal model, and f= [fx;fy;fz]>are forces resulting from unmodelled aerodynamic effects due to varying wind conditions. We cast the position dynamics in Eq. (13a) into the form of Eq. (1), by taking M(q) =mI,C(q;_q)\u00110, andu=Rfu. Note that the quadrotor attitude dynamics Eq. (13b) is also a special case of Eq. (1) [15, 52], and thus our method can be extended to attitude control. We implement our method in the position control loop, that is we use our method to compute a desired force ud. Then the desired force is decomposed into the desired attitude Rdand the desired thrust Tdusing kinematics (see Fig. 2). Then the desired attitude and thrust are sent to the onboard PX4 flight controller. 22Neural Network Architectures and Training Details In practice, we found that in addition to the drone velocityv, the aerodynamic effects also depend on the drone attitude and the rotor rotation speed. To that end, the input state xto the deep neural network is a 11-d vector, consisting of a the drone velocity (3- d), the drone attitude represented as a quaternion (4-d), and the rotor speed commands as a pulse width modulation (PWM) signal (4-d) (see Fig. 2 and 3). The DNN has four fully-connected hidden layers, with an architecture 11!50!60!50!4and Rectied Linear Units (ReLU) activation. We found that the three components of the wind-effect force, fx;fy;fz, are highly correlated and sharing common features, so we use as the basis function for all the component. Therefore, the wind-effect force fis approximated by f\u00192 4 (x) 0 0 0 (x) 0 0 0 (x)3 52 4ax ay az3 5; (14) whereax;ay;az2R4are the linear coefcients for each component of the wind-effect force. We followed Algorithm 1 to train in PyTorch, which is an open source deep learning framework. We refer to the supplementary material for hyperparameter details (Section S3). Note that we explicitly include the PWM as an input to the network. The PWM information is a function ofu=Rfu, which makes the controller law (e.g., Eq. (7)) non-afne in u. We solve this issue by using the PWM from the last time step as an input to , to compute the desired force udat the current time step. Because we train using spectral normalization (see Algorithm 1), this method is stable and guaranteed to converge to a xed point, as discussed in [8]. Controller Implementation For experiments, we implemented a discrete form of the Neural-Fly con- trollers, given in Section S4. For INDI, we implemented the position and acceleration controller from Sections III.A and III.B in [4]. For L1adaptive control, we followed the adaptation law rst presented in [6] and used in [7] and augment the nonlinear baseline control with ^f=\u0000uL1. SUPPLEMENTARY MATERIALS Section S1. Drone Conguration Details Section S2. The expressiveness of the learning architecture Section S3. Hyperparameters for DAIML and the interpretation Section S4. Discrete version of the proposed controller Section S5. Stability and robustness formal guarantees and proof Section S6. Gain tuning Section S7. Force prediction performance Section S8. Localization error analysis Figure S1. Training and Validation Loss Figure S2. Importance of domain-invariant representation Figure S3. Measured residual force versus adaptive control augmentation Figure S4. Localization inconsistency Table S1. Drone conguration details Table S2. Hardware comparison used of Quadrotor Dy- namics Subject to Rotor Drag for Accurate Tracking of High-Speed Trajectories. IEEE Robotics 2377-3766. doi: High-delity computational aerodynamics of multi-rotor un- manned aerial vehicles. In 2018 AIAA Aerospace Sciences Meeting , page 1266, 2018. [4] Ezra Tal and Sertac Karaman. Tracking Aggressive Quadrotor Trajectories Using Incre- mental Nonlinear Dynamic Inversion and Differential Flatness. IEEE Transactions and Chengyu Cao. L1 Adaptive Controller for Attitude Control of Multirotors. In AIAA Guidance, Navigation, and Control Conference , Minneapolis, Minnesota, August 2012. American Institute of Aeronautics A. Theodorou. L1-Adaptive MPPI Architecture for Robust and Agile Control of Multirotors. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 7661-7666, lander: Stable drone landing control using learned dynamics. In 2019 International Conference on Robotics and Automation (ICRA) , pages 9784-9790. IEEE, 2019. Soon-Jo Chung. Neural-swarm: Decentralized close- proximity multirotor control using learned interactions. In 2020 IEEE International Conference on Robotics and Automation (ICRA) , pages IEEE, Yisong Yue, and Soon-Jo Chung. Neural-swarm2: Planning and control of heterogeneous multirotor swarms using learned interactions. IEEE 6(2):3769-3776, 2021. [12] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. Fraundorfer, and Marc Pollefeys. PIXHAWK -A micro aerial vehicle design for autonomous flight using onboard com- puter vision. Autonomous Robots , Mellinger and Vijay Kumar. Minimum snap trajectory generation and control for quadrotors. In2011 IEEE International Conference on Robotics and May 2011. doi: 10.1109/ICRA.2011.5980409. [15] and Weiping Li. Applied nonlinear control . Prentice Hall, Englewood Cliffs, N.J, 1991. ISBN 978-0-13-040890-7. [16] Petros A Ioannou and Jing Sun. Robust adaptive control , volume 1. Prentice-Hall Upper Saddle River, NJ, . Wiley Sons, Inc., 1995. [18] Kumpati S Narendra and Anuradha M Annaswamy. Stable adaptive systems . Courier Corporation, 2012. [19] Jay A. Farrell and Marios M. Polycarpou. Adaptive Approximation Based Control . Adaptive control of flight: theory, applica- tions, and open problems. In 2006 American Control Conference , 2006. [21] Xichen Shi, Control Fixed-Wing VTOL with Airflow Vector Sensing. In 2020 IEEE In- ternational Conference on Robotics and Automation (ICRA) , Recht. Random features for large-scale kernel machines. In Proceedings of the 20th International Conference on Neural Information Processing Systems Nonlinear Dynamical Systems. In 2021 60th IEEE Conference on Decision S. Schaal. A locally weighted learning composite adaptive controller with structure adaptation. In IEEE/RSJ International Conference on Intelligent Robots and Systems , volume 1, pages 882-889 vol.1, September 2002. doi: 10.1109/IRDS.2002.1041502. 25[25] Fu-Chuang Chen and Hassan K Khalil. Adaptive control of a class of nonlinear discrete-time systems using neural networks. IEEE Transactions on Automatic Control , 40(5):791-801, 1995. [26] Eric N Johnson and Anthony J Calise. Limited authority adaptive flight control for reusable launch vehicles. Journal of Guidance, Control, and Dynamics , 26(6):906-913, 2003. [27] Kumpati S Narendra and using networks and proximate and Taeyoung Geometric Adaptive Control With Neural Networks for a Quadro- tor in Wind Fields. IEEE Transactions on Control Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning , pages 1126-1135. neural networks: A survey. IEEE Transactions on Yue. Meta- adaptive nonlinear control: Theory and algorithms. Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-reinforcement Caluwaerts, Wenbo Gao, Chelsea Finn, and Jie Tan. Rapidly adaptable legged robots via evolutionary meta-learning. In 2020 IEEE/RSJ International Conference on Intelligent Robots and (IROS) , pages 3769-3776. IEEE, and Sergey Levine. Model-based meta-reinforcement learning for flight with Robotics and Automation Letters , 6(2):1471-1478, 2021. [36] Christopher D McKinnon and Angela P Schoellig. Meta learning with paired forward and inverse models for efcient receding horizon control. IEEE Robotics Pieter Abbeel. Model-based reinforcement learning via meta-policy optimization. In Conference on Robot Learning , pages 617-629. PMLR, 2018. [38] Michael O'Connell, Soon-Jo Chung. Meta-learning-based robust adap- terrain. Science robotics , 5(47), 2020. [42] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do- main randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots Chung, and Fred Y Hadaegh. Swarm assignment and trajectory optimization using variable-swarm, distributed auction assignment and sequential convex programming. The International Journal of Robotics Research [45] Soon-Jo Chung. Nonlinear control of autonomous flying cars with wings and distributed electric propulsion. In 2018 IEEE Conference on Decision and Control (CDC) Chung. Chance-constrained trajectory optimization for safe exploration and learning of nonlinear systems. IEEE Robotics and Automation Letters Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research , 17(1):2096-2030, 2016. [50] Ian Goodfellow, Jean Pouget-Abadie, Neural Processing Systems , 27, 2014. [51] R. E. Kalman and R. S. Bucy. New Results in Linear Filtering and Prediction Theory. Journal of Basic Engineering , 83(1):95-108, Li, and S. Shankar Sastry. A Mathematical Introduction to Robotic Manipulation . Kalman. A New Approach to Linear Filtering and Prediction Problems. Journal of Ba- sic Engineering , 82(1):35-45, Hall, 2002. URL https://www.pearson.com/content/one-dot-com/one-dot-com/us/en/ higher-education/program.html . [58] Multicopter A.A. with NVIDIA Corporation, and Y .Y . is also with associated Argo AI. K.A. is currently afliated with Purdue University. We thank J. Burdick and J.-J. E. Slotine for their helpful discussions. We thank M. Anderson for help with conguring the quadrotor platform, and M. Anderson and P. Spieler for help with hardware troubleshooting. We also thank N. Badillo and L. Pabon Madrid for help in experiments. Funding: This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). This research was also conducted in part with funding from Raytheon Technologies. The views, opinions, and/or ndings expressed are those of the authors and should not be interpreted as representing the ofcial views or policies of the Department of Defense or the U.S. Government. The experiments reported in this article were conducted at Caltech's Center for Autonomous Systems and Tech- nologies (CAST). Author contributions: (1) S.-J.C. and Y .Y . directed the research activities. (2) G.S. and M.O'C. designed and implemented the meta-learning algorithm under the guidance of Y .Y ., K.A., A.A., S.-J.C., while the last-layer adaptation idea was started with a discussion by G.S., M.O'C., X.S., and S.-J.C. (3) M.O'C., G.S. designed and the adaptive control algorithm with inputs from S.-J.C., X.S. (4) M.O'C., G.S. performed experiments and evaluated the results. (5) M.O'C. conducted the theoretical analysis of the meta-learning based adaptive controller with input from S.-J.C., G.S., X.S. (6) G.S. ana- lyzed the ., K.A., A.A., and S.-J.C. (7) G.S., M.O'C. made all the gures and videos with input from all the others. (8) All authors prepared the manuscript. Competing interests: The authors declare that they have no competing interests. Data and materials availability: 28All data needed to evaluate the conclusions in the article are present in the article or in the Supplementary Materials. SUPPLEMENTARY MATERIALS Section S1 Drone Conguration Details Table S1 presents the conguration information of the custom built drone (g. 1(A)) and the Intel Aero drone. We use both drones for data collection and use the custom built drone exclusively for experiments. Custom built drone Intel Aero drone Weight 2:53 kg 1 :47 kg Thrust-to-weight ratio 2.2 1.6 Rotor tilt angle 12\u00b0 front, 10\u00b0 rear 0\u00b0 Diameter 85 cm wide, 75 cm long 52 cm wide, 52 cm long Conguration Wide-X4 X4 On-board computer Raspberry Pi 4 Intel Aero computing board (Atom x7 processor) Flight controller Pixhawk 4 running PX4 Aero Flight Controller running PX4 Table S1: Drone conguration details Congurations of the custom built drone and the Intel Aero drone with propeller guards. Precision tracking for drones often relies on specialized hardware and optimized vehicle design, whereas our method achieves precise tracking using improved dynamics prediction through online learning. Al- though most researchers report the numeric tracking error of their method, it can be difcult to disentangle the improvement of the controller resulting from the algorithmic advancement versus the improvement from specialized hardware. For example moment of inertia generally scales with the radius squared and the lever arm for the motors scales with the radius, so the attitude maneuverability roughly scales with the inverse of the vehicle radius. Similarly, high thrust to weight ratio provides more attitude control authority during high acceleration maneuvers. More powerful motors, electronic speed controllers, and batteries together allow faster motor response time further improving maneuverability. Thus, state-of-the-art (SOTA) tracking performance usually requires specialized hardware often used for racing drones, resulting in a vehicle with greater maneuverability than our platform, a higher thrust to weight ratio, and using high-rate controllers sometimes even including direct motor RPM control. In contrast, our custom drone is more representative of typical consumer drone hardware. A detailed comparison with the hardware from some recent work in agile flight control is provided in Table S2. Section S2 The Expressiveness of the Learning Architecture In this section, we theoretically justify the decomposition f(x;w)\u0019 (x)a(w). In particularly, we prove that any analytic function \u0016f(x;w) : [\u00001;1]n\u0002[\u00001;1]m!Rcan be split into a w-invariant \u0016 two the dimension of \u0016a(w)only scales polylogarithmically with 1=\u000f. We rst introduce the following multivariate polynomial MPC [11] Flight computer Raspberry Pi 4 - laptop laptop Flight controller Pixhawk 4 STM32H7 Raceflight Revolt ? (400 MHz ) Flight controller rmware custom ? ? Mass [ kg] 2.53 0.609 0.610 0.8 Total width [ cm] 85 ? ? ? Propeller diameter [ in] 11 5 6 ? Motor Spacing [ cm] 39* 18 - ? Thrust-to-weight ratio [-] 2.2 ? 4 5 Motion capture frequency [ Hz] 100 360 200 100 MPC control frequency [ Hz] - - - 50 Position control frequency [ Hz] 50 ? 55 ? Attitude control frequency [ Hz]<1000 2000 4000 ? Motor speed feedback No Optical encoders No No (5 kHz ) ? indicates information not provided - indicates information not applicable * front to back Table S2: Hardware comparison Hardware conguration comparison with other quadrotors that demon- strate state-of-the-art trajectory tracking. Direct comparisons of performance are difcult due to the varying congurations, controller tuning, and flight arenas. However, most methods require extremely maneuverable quadrotors and onboard/offboard computation power to achieve state-of-the-art performance, while Neural- Fly achieves state-of-the-art performance on more standard hardware with all control running onboard. Lemma 2. (Multivariate polynomial approximation in the hypercube) Let \u0016f(x;w) : [\u00001;1]n\u0002[\u00001;1]m! Rbe a smooth function of Then \u0016fhas a uniformly (1 +t)\u0000ksk2\u0011 : Note i.e., sup[x;w]2[\u00001;1]n+mk\u0016f(x;w)\u0000 Cp(x;w)k\u0014O((1 +t)\u0000ppn+m). Finally to present the following representation theorem. 30Theorem 3. \u0016f(x;w)is a function satisfying the assumptions in Lemma 2. For any \u000f>0, there existh2Z+, and two Chebyshev that sup andh=O((log(1=\u000f))m). Proof. First exists p=O\u0010 log(1=\u000f)pn+m\u0011 such that sup[x;w]2[\u00001;1]n+m 1 +log(1=\u000f)pn+m\u0013m\u0013 can be generalized to vector-valued functions with bounded input space straight- forwardly. Finally, since neural networks are universal approximators for polynomials [54], Theorem 3 immediately guarantees the expressiveness of our learning structure, i.e., (x)a(w)can approximate f(x;w) with arbitrary precision, where (x)is a deep neural network and ^aincludes the linear coefcients for all the elements of f. In experiments, we show that a four-layer neural network can efciently learn an effective representation for the underlying unknown dynamics f(x;w). Section S3 Hyperparameters for DAIML and the Interpretation We implemented DAIML (Algorithm 1) using PyTorch, with hyperparameters reported in Table S3. We iteratively tuned these hyperparameters by trial and error. We notice that the behavior of the learning algo- rithm is not sensitive to most of parameters in Table S3. The training process is shown in g. S1, where we present thefloss curve on both training set and validation set using three random seeds. The floss is de- ned byP i2Bky(i) k\u0000 (x(i) k)a\u0003k2(see Line 7 in Algorithm 1), which reflects how well can approximate 310 200 400 600 800 1000 Training epoch123The f loss on the training data =0.1 =0.0 0 200 400 600 800 1000 Training epoch0.40.60.81.01.2The f loss on the validation dataFigure S1: Training and validation loss. The evolution of the floss on the training data and validation data in the training process, from three random seeds. Both mean (the solid line) and standard deviation (in the shaded area) are presented. Training with the adversarial regularization term ( = 0:1) has similar behaviors as = 0(no regularization) in the early phase before 300 training epochs, except that it converges slightly faster. However, the regularization term effectively avoids over-tting and has smaller error on the validation dataset after 300 training epochs. Architecture of net 11!50!60!50!4with ReLU activation functions Architecture of hnet 4!128!6with ReLU activation functions Batch size of Ba 128 Batch size of B 256 Loss function for h Cross-entropy loss Learning rate for training 0:0005 Learning rate for training h 0:001 Discriminator training frequency \u0011 0:5 Normalization constant 10 The regularization 0:1 Table S3: Hyperparameters used in DAIML (Algorithm 1). the unknown dynamics f(x;w). The validation set we considered is from the gure-8 trajectory tracking tasks using the PID and nonlinear baseline methods. Note that the training set consists of a very different set of trajectories (using random waypoint tracking, see Results) , and this difference is for studying whether and when the learned model starts over-tting during the training process. We emphasize a few important parameters as follows. (i) The frequency 0< \u0011\u00141is to control how often the discriminator his updated. Note that \u0011= 1corresponds to the case that andhare both updated in each iteration. We use \u0011= 0:5for training stability, which is also commonly used in training generative adversarial networks [50]. (ii) The regularization parameter \u00150. Note that = 0 corresponds to the non-adversarial meta-learning case which does not incorporate the adversarial regularization term in Eq. (5). From g. S1, clearly a proper choice of can effectively avoid over-tting. Moreover, another benet of having > 0is that the learned model is more explainable. As observed in g. g:training-tsne, > 0 disentangles the linear coefcients a\u0003between wind conditions. However, if is too high it may degrade the prediction performance, so we recommend using relatively small value for such as 0:1. 32The importance of having a domain-invariant representation. We use the following example to illus- trate the importance of having a domain-invariant representation (x)for online adaptation. Suppose the data distribution in wind conditions 1 and 2 are P1(x)andP2(x), respectively, and they do not overlap. Ideally, we would hope these two conditions share an invariant representation and the latent variables are distinct (a(1)anda(2)in the rst line in g. S2 shown below). However, because of the expressiveness of DNNs, may memorize P1andP2and learn two modes 1(x)and 2(x). In the second line in the following gure, 1and 2are triggered if xis inP1andP2, respectively ( 1x2P1and1x2P2are indicator functions), such that the latent variable ais identical in both wind conditions. Such an overtted is not robust and not generalizable: for example, if the drone flies to P1in wind condition 2, the wrong mode 1 will be triggered. Figure S2: Importance of domain-invariant representation. The key idea to tackle this challenge is to encourage diversity in the latent space, which is why we introduced a discriminator in DAIML. Figure 4 shows DAIML indeed makes the latent space much more disentangled. Section S4 Discrete Version of the Proposed Controller In practice, we implement Neural-Flyon a digital system, and therefore, we require a discrete version of the controller. The feedback control policy uremains the same as presented in the main body of this article. However, the adaptation law must be integrated and therefore we must be concerned with both the numerical accuracy and computation time of this integration, particularly for the covaraince matrix P. During the development of our algorithm, we observed that a naive one-step Euler integration of the continuous time adaptation law would sometimes result Pbecoming non-positive-denite due to a large _Pmagnitude and a coarse integration step size (see [55] for more discussion on the positive deniteness of numerical integration of the differential Riccati equation). To avoid this issue, we instead implemented the adaptation law in two discrete steps, a propagation and an update step, summarized as below. We denote the time at step kastk, the value of a parameter before the update step but after the propagation step with a subscript t\u0000 k, and the value after both the propagation and update step with a subscript t+ k. The value used in the controller is 33the value after both the propagation and update steps, that is ^a(tk) = ^at+ k. During the propagation step in Eq. (15) and (16) both ^aandPare regularized. Then, in the update step in Eq. (18) and (19), Pand^aare updated according to the gain in Eq. (17). This mirrors a discrete Kalman lter implementation [56] with the tracking error term added in the update step. The discrete Kalman lter exactly integrates the continuous time Kalman lter when the prediction error e, tracking error s, and learned basis functions are constant between time steps ensuring the positive deniteness and Robustness Formal Guarantees and Proof We divide the proof of Eq. (12) into two steps. First, in Theorem 4, we show that the combined compos- ite velocity tracking error and adaptation error, k[s; ~a]k, exponentially converges to a bounded error ball. This implies the exponential convergence of s. Then in Corollary 5 we show that when sis exponentially bounded, ~qis also exponentially bounded. Combining the exponential bound from Theorem 4 and the ultimate bound from Corollary 5 proves Theorem 1. Before discussing the main proof, let us consider the robustness properties of the feedback controller without considering any specic adaptation law. Taking the dynamics Eq. (1), control law Eq. (7), the composite velocity error denition Eq. (10), and M_s+ We Lyapunov function V=s>Msunder the assumption of bounded ~ato show that lim t!1ksk\u0014suptkd\u0000 ~ak\u0015max(M) \u0015min(K)\u0015min(M)(21) Taking this results alone, one might expect that any online estimator or learning algorithm will lead to good performance. However, the boundedness of ~ais not guaranteed; Slotine and Li discuss this topic thoroughly [15]. In the full proof below, we show the stability and robustness of the Neural-Fly adaptation algorithm. First, we introduce the parameter measurement noise \u0016\u000f, where \u0016\u000f=y\u0000 a. Thus, \u0016\u000f=\u000f+dand k\u0016\u000fk\u0014k\u000fk+kdkby the triangle inequality. Using the above closed loop dynamics Eq. (20), the parameter estimation error ~a, and the adaptation law Eq. (8) and (9), the combined velocity and parameter-error closed- loop dynamics are given we rely on the fact that P\u00001is both uniformly positive denite and uniformly bounded, that is, there exists some constant matrices AandBsuch thatA\u0017P\u00001\u0017B. Dieci and Eirola [55] show the slightly weaker result that that Pis positive denite and nite when is bounded under the looser assumption Q\u00170. Following the proof from [55] with the additional assumption thatQis uniformly positive denite, one can show the uniform deniteness and uniform boundedness of P. Hence,P\u00001is also uniformly positive denite and uniformly bounded. Theorem 4. Given dynamics that evolve according to Eq. (22) and(23), uniform positive deniteness and the norm of\u0014s ~a\u0015 exponentially converges to the bound and\u0015min(\u0001)and\u0015max(\u0001)are the minimum and maximum eigenvalues of (\u0001)over time, respectively. Given Corollary 5 and Eq. (24), the bound in Eq. (12) is proven. Note \u0015max(P\u00001) = 1=\u0015min(P)and a sufciently large value of \u0015min(P)will make the RHS of Eq.(24) small. Proof. Now consider Lyapunov function by V=\u0014s ~a\u0015>\u0014M 0 0P\u00001\u0015\u0014s _M\u00002Cis skew-symmetric. As K,P\u00001QP\u00001,M, andP\u00001are all uniformly positive denite and uniformly bounded, and >R\u00001 is positive semidenite, there exists some > 0 such that \u0000\u00142K 0 allt. upper bound for the disturbance term Das D= sup t \u0014d >R\u00001\u0016\u000f\u0000P\u00001\u0015a\u0000P\u00001_a\u0015 (33) and dene the function M, M=\u0014M 0 0P\u00001\u0015 (34) By Eq. (32), the Cauchy-Schwartz inequality, and the denition of the minimum eigenvalue, we have the following inequality for _V: _V\u0014\u0000 2 V+ 2s V \u0015min(M)D (35) Consider the related systems, WwhereW=p V,2_WW =_V, and and ball lim t!1 \u0014s ~a\u0015 \u0014D \u0015min(M)(39) This completes the proof. Next, we present a corollary which shows the exponential convergence of ~qwhensis exponentially stable. Corollary 5. Ifks(t)k\u0014Aexp(\u0000 t) +B= lim t!1k~qk\u0014B \u0015min(\u0003)(41) 36Proof. From the Comparison Lemma [57], we can easily show Eq. (40). This can be further reduced as follows. k~qk\u0014e\u0000\u0015min(\u0003)tk~q(0)k+Ae\u0000\u0015min(\u0003)tZt 0e(\u0015min(\u0003)\u0000 ) Taking the limit, we arrive at Eq. (41) With the following corollary, we will justify that is strictly positive even when \u00110, and thus the adaptive control algorithm guarantees robustness even in the absence of persistent excitation or with ineffective learning. In practice we expect some measurement information about all the elements of a, that is, we expect a non-zero . Corollary 6. If \u00110, then the bound in Eq. (24) can (45) \u00110also simplies the _Pequation to a rst-order differential matrix equation. By integrating this simplied _Pequation, we can show Pexponentially converges to the value P=Q 2\u0015. This leads to bound in Eq. (44). We now introduce another corollary for the Neural-Fly-Constant, when =I. In this case, the regu- larization term is not needed, as it is intended to regularize the linear coefcient estimate in the absence of persistent excitation, so we set \u0015= 0. This corollary also shows that Neural-Fly-Constant is sufcient for perfect tracking control when fis constant; though in this case, even the nonlinear baseline controller with integral control will converge to perfect tracking. In practice for quadrotors, we only expect fto be constant when the drone air-velocity is constant, such as in hover or steady level flight with constant wind velocity. Corollary 7. If \u0011I,Q=qI,R=rI,\u0015= 0, andP(0) =p0Iis diagonal, where q,randp0are strictly positive scalar constants, then the bound in Eq. (24) can be \u0015min(K)\u0015min(M)(46) Proof. Under these assumptions, the matrix differential equation for Pis reduced to the scalar differential equation dp dt=q\u0000p2=r (47) 37whereP(t) =p(t)I. This equation be integrated nd that to p=pqr. Then \u0015min(K)=\u0015max(M), then we can take =\u0015min(K)=\u0015max(M). Then, to lim t!1 +r\u00001\u0001 sup tkf\u0000ak+\u000f=r (49) Section S6 Gain Tuning The attitude controller was tuned following the method in [58]. The gains for all of the position controllers tested were tuned on a step input of 1 min the x-direction. The proportional (P) and derivative (D) gains were tuned using the baseline nonlinear controller for good rise time with minimal overshoot or oscillations. The same P and D gains were used across all methods. The integral and adaptation gains were tuned separately for each method. In each case, the gains were increased to minimize response time until we observed having large overshoot, noticeably jittery, or oscilla- tory behavior. ForL1and INDI this gave a rst-order lters with a cutoff frequency of 5 Hz. For each of the Neural-Fly methods, we used R=rIandQ=qI, whererandqare a scalar values. The tuning method gave anRgains similar to the measurement noise of the residual force, a Qvalues on the order of 0:1, and \u0015values of 0:01. Section S7 Force Prediction Performance The section discusses g. S3, which is useful for understanding why learning improves force prediction (which in turn improves control). For the nonlinear baseline method, the integral (I) term compensates for the average wind effect, as seen in g. S3. Thus, the UA V trajcetory remains roughly centered on the desired trajectory for all wind conditions, as seen in g. 5. The relative velocity of the drone changes too quickly for the integral-action to compensate for the changes in the wind effect. Although increasing the I gain would allow the integral control to react more quickly, a large I gain can also lead to overshoot and instability, thus the gain is effectively limited by the combined stability of the P, D, and I gains. Next, consider the two SOTA baseline methods, INDI and L1, along with the non-learning version of our method, Neural-Fly-Constant. These methods represent different adaptive control approaches that assume no prior model for the residual dynamics. Instead, each of these methods effectively outputs a ltered version of the measured residual force and the controller compensates for this adapted term. In g. S3, we observe that each of these methods has a slight lag behind the measured residual force, in grey. This lag is reduced by increasing the adaptation gain, however, increasing the adaptation gain leads to noise amplication. Thus, these reactive approaches are limited by some more inherent system properties, like measurement noise. Finally, consider the two learning versions of our method, Neural-Fly and Neural-Fly-Transfer. These methods use a learned model in the adaptive control algorithm. Thus, once the linear parameters have adapted to the current wind condition, the model can predict future aerodynamic effects with minimal 38Figure S3: Measured residual force versus adaptive control augmentation, ^f.Wind-effect x- and z-axis force prediction for different methods, ^fandKiR ~pdt, compared with the online residual force measure- ment,f. The integral term in the nonlinear baseline method and the ^fterm in the adaptive control methods, including the Neural-Fly methods, all act to compensate for the measured residual force. INDI, L1, and Neural-Fly-Constant estimate the residual force with sub-second lag, however adjusting the gains to de- crease the lag increases noise amplication. Neural-Fly and Neural-Fly-Transfer have reduced the lag in estimating the residual force but have some model mismatch, especially at higher wind speeds. 395 10 15 20 25 30 35 40 t [s]345678||pEFK pmocap|| [cm] Localization inconsistency for PID baseline in no wind actual mean one- two- Figure S4: Localization inconsistency Typical difference between the OptiTrack motion capture position measurement, pmocap , and the EKF position estimate, pEKF, corrected for the Optitrack delay. The mean difference corresponds to a constant offset between the center of mass, which the EKF tracks, and the centroid of reflective markers, which the OptiTrack measures. The standard deviation corresponds to the root-mean-square error between the two measurements. changes to the coefcients. As we extrapolate to higher wind speeds and time-varying conditions, some model mismatch occurs and is manifested as discrepancies between the predicted force, ^f, and the mea- sured force, f, as seen in g. S3. Thus, our learning based control is limited by the learning representation error. This matches the conclusion drawn in our theoretical analysis, where tracking error scales linearly with representation error. Section S8 Localization Error Analysis We estimate the root mean squared position localization precision to be about 1 cm . This is based on a comparison of our two different localization data sources. The rst is the OptiTrack motion capture system, which uses several infrared motion tracking cameras and reflective markers on the drone to produce a de- layed measurement the position and orientation of the vehicle. The PX4 flight controller runs an onboard extended Kalman lter (EKF) to fuse the OptiTrack measurements with onboard inertial measurment unit (IMU) measurements to produce position, orientation, velocity, and angular rate estimates. In offline anal- ysis, we correct for the delay of the OptiTrack system, and compare the position outputs of the OptiTrack system and the EKF. Typical results are shown in g. S4. The xed offset between the measurements occurs because the OptiTrack system tracks the centroid of the reflective markers, where the EKF tracks the center of mass of the vehicle. Although the EKF must internally correct for this offset, we do not need to do so in our offline analysis because the offset is xed. Thus, the mean distance between the the OptiTrack position and the EKF position corresponds to the distance between the center of mass and the center of vision, and the standard deviation of that distance is the root-mean-square error of the error between the two estimates. Averaged over all of the data from experiments in this paper, we see that the standard deviation is 1:0 cm . Thus, we estimate that the localization precision has a standard deviation of about 1:0 cm . 40 "}