{"title": "PDF", "author": "PDF", "url": "https://fic.tufts.edu/assets/PIA-guide_revised-2014-3.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "A Design Guide Andy Catley John Burns Dawit Abebe Omeno SujiParticipatory Impact AssessmentFeinstein International Center Tufts first edition of this guide, released in 2009, was made possible with the support of the Bill and Melinda Gates Foundation under the Impact Assessment of Innovative Humanitarian Assistance Projects initiative. The authors would like to thank Mito Alfieri, Dr. Valerie Bemo, Kathy Cahill and Regine Webster from the Foundation for their extraordinary support and encouragement. We would also like to thank the organizations participating in the project under the Bill and Melinda Gates- funded Sub-Saharan Famine Relief Effort \"Close to the Brink\" for their willing participation and valuable contributions. In particular, we would like to single out the Save the Children (USA) Country Office in Malawi; Country Offices of Catholic Relief Services (CRS) in Mali; International Medical Corps (IMC) Office in Nairobi representing Southern Sudan; the Africare Country Offices in Niger and Zimbabwe; Lutheran World Relief Office in Niger; and the Country Office of CARE International in Zimbabwe. Also, Omar Abdou, Mme Ramatou Adamou, Jacque Lazarus Sithole, Alexa Reynolds and Innocent Takaedza, many thanks for your participation, contributions and support. From the Feinstein International Center many thanks go to Sally Admassu, Yacob Rosa Pendenza, Anita Robbins, Katherine Sadler, Dr Peter Walker and Dr Helen Young for providing technical and administrative support. And to Cathy Watson, many thanks for editing and proofreading the first edition. This second edition of the guide was supported by the Norwegian Ministry of Foreign Affairs, with design by Green Ink. PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE iPhoto credits Adama No 4 School Gardens Group, Adama, Ethiopia: page 21 (bottom) Berhanu Admassu: page 36 John Burns: page 3, page 6, page 25, page 27, page 37 (right), page 52 Andrew Catley: page 10, page 54 Hulugeb Fruit and Vegetable Association, Bahir Dar, Ethiopia: page 21 (top) Alkassoum Kadede: cover Kebele 05 Urban Gardens Group, Bahir Dar, page 21 (middle) Tim Leyland: page 1 Omeno Suji: page 26, page 37 (left) Suggested citation Catley, A., Burns, J., Abebe, D., Suji, O. (2013). Participatory Impact Assessment: A Design Guide. Feinstein International Center, Tufts University, Somerville Contents Abbreviations ..................................................................... v Foreword ......................................................................... vi Introduction ...................................................................... 1 Purpose of the design guide .......................................................... 2 Why measure impact? .............................................................. 3 Some challenges with evaluation, impact assessment and learning ................... 3 Towards better impact assessments .............................................. 4 What is a Participatory Impact Assessment? ........................................... 5 Livelihood changes over time ................................................... Participatory numbers and PIA ................................................. 5 impact assessment .................................... 7 When to do an impact assessment .................................................... 9 Designing a Participatory Impact Assessment - an eight-stage approach ............... 10 Background ....................................................................... 11 Stage 1: Identifying the key questions ................................................ 12 Stage 2: Defining the boundaries of the project in space and time ......................... 13 Defining the project boundary: participatory mapping ............................ 13 Defining the project period: timelines ........................................... 15 Stage 3: Identifying indicators of project impact ....................................... 18 Types of indicators and measurement ........................................... 18 Community-defined indicators of project impact .................................. 19 Quantitative and qualitative indicators ......................................... 21 Changes in coping strategies ................................................... 22 Using monitoring data for impact assessment .................................... 23 Stage 4: Choosing the methods ...................................................... 25 Testing the methods .......................................................... 25 Using visual aids ............................................................. 25 Simple ranking .............................................................. 26 Simple scoring ............................................................... 26 Proportional piling ........................................................... 28 Before and after scoring ....................................................... 28 Before and after proportional piling ............................................ 33 Pair-wise ranking ............................................................ 33 Matrix scoring ............................................................... 33 Impact calendars ............................................................ 37 Radar diagrams ............................................................. 38 Voting ...................................................................... 38 Repeating scoring and ranking to improve reliability ............................. 38 Stage 5: Sampling ................................................................. 40 Who will use the findings of a PIA? ............................................. 40 PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE iii Time and money; questions and methods ........................................ 40 Selecting the sampling method ................................................. 42 Selecting the sample size ...................................................... 42 Stage 6: Assessing project attribution ................................................ 45 Attribution by comparison of project versus non-project factors .................... 46 Using a participatory method to directly compare project and non-project activity or impact ............................................................ 48 Comparing changes in project versus non-project participants ..................... 50 Stage 7: Triangulation ............................................................. 51 Stage 8: Feedback and validation References and further reading .................................................... 54 Abbreviations ALNAP Active Learning Network for Accountability and Performance CAHW Community Animal Health Worker CBO Community Based Organization CHW Community Health Worker CI Confidence Interval GIRA Gokwe Integrated Recovery Action project GTZ German Technical Cooperation HAP-I Humanitarian Accountability Partnership HH Household IIED International Institute for Environment and Development LEGS Livestock Emergency Guidelines and Standards M&E Monitoring and Evaluation NGO Non-Governmental Organization OLP Organizational Learning Partnership PIA Participatory Impact Assessment PRA Participatory Rural Appraisal UN United Nations USAID United States Agency for International Development PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE vForeword This updated guide to Participatory Impact Assessment (PIA) is a very welcome addition to the development field. Working with Tufts University in Ethiopia, I have made use of PIA findings on many occasions to review and develop programs, best practice and policy. PIA became an essential part of the Pastoralist Livelihoods Initiative funded by the United States Agency for International Development in Ethiopia from 2005 to 2013, with over 20 PIAs or related studies undertaken on specific activities in the program. The results helped to formulate best practices which eliminated wasteful or inappropriate activities, such as emergency livestock vaccinations, while promoting effective interventions such as early commercial de-stocking for pastoralists during droughts. With the facilitation of Tufts, these good practices were adopted as policy by the Ethiopian Government through its national guidelines on livelihoods-based drought response, published in 2009. The PIA results were also critical further afield and contributed to the international Livestock Emergency Guidelines and Standards. In Ethiopia, PIA was similarly effective in other sectors, such as water use - both for human needs and agriculture purposes - as well as health and education. I recommend this guide highly for real-time assessments of what works and what doesn't, and for ensuring the wide use of evidence for developing good practice and, ultimately, policy. John Graham Country Director Save the Children International Ethiopia Introduction Researchers at the Feinstein International Center have been developing and adapting participatory approaches to measure the impact of aid projects on people's livelihoods since the early 1990s, covering both development and humanitarian contexts. Drawing upon this experience, this guide aims to provide practitioners with a flexible framework for designing a project-level Participatory Impact Assessment (PIA). Other than in some health, nutrition, and water projects, where indicators of project performance can relate to international standards, for many projects there is no 'gold standard' for measuring impact. This guide aims to bridge the gap by describing a tried and tested approach to measuring the impact of livelihoods projects. The guide avoids standards and indicators, or a blueprint for impact assessment, and instead uses an eight-stage framework which can be adapted to different contexts and project interventions. Similarly, we describe some participatory methods that are particularly useful for understanding impact but which, again, should be adapted to context. In this updated version, the kind and range of examples of PIA has been expanded to include service delivery projects, and the importance of comparisons has been emphasized. Much of the value of PIA is derived from comparing situations at two points in time, or comparing 'project' and 'non-project' impacts, influences and changes. Our experience shows that PIA can be used to overcome some of the well-known weaknesses in conventional humanitarian and development monitoring, evaluation and impact assessment approaches. Common problems include an emphasis on measuring activities as opposed to real impact, over-use of external as opposed to community-defined indicators of impact, and weak or non- existent baselines. The guide also shows how participatory methods can be used to attribute impact or change to project activities, supported by cross-checking. Many of the methods used produce numerical data, and so the guide describes how the systematic use of these methods can produce conventional statistics to describe project impacts. Objectives of the design guide 1 Provide a flexible framework for designing an impact assessment 2 Clarify the differences between measuring process and measuring real impact 3 Show how PIA can be used to measure the impact of different projects in different contexts using community-identified impact indicators 4 Show how participatory methods can be used to measure impact where no baseline data exists 5 Demonstrate how participatory methods can be used to attribute impact to a project 6 Demonstrate how qualitative but numerical data arising from the systematic use of participatory methods can be collected and presented to show project impactPurpose of the design guide Why measure impact? Some challenges with evaluation, impact assessment and learning The ability to define and measure impact is essential if organizations are to strengthen their capacity to systematically evaluate and compare project interventions. Better lesson learning across organizations, operations and time is critical for creating an evidence base which can continue to inform and improve humanitarian and development work. Institutionalizing good practice in the systems and structures of aid organizations relates directly to their ability to meet the growing demands on the aid sector, and the needs of people made vulnerable by disasters and humanitarian crises. Similarly, communicating the effectiveness of impact is necessary for the aid sector to respond to increasing pressure from donors and the general public to demonstrate results (Fritz Institute, 2007). Much has been written on the monitoring, evaluation and impact assessment of aid projects. For example, a common theme in the literature on humanitarian assistance is the limited incentives for international organizations to measure the impact of their work (Roche, 1999: Hofmann et al., 2004; Watson, 2008; Burns, 2009). Recent initiatives such as the Humanitarian Accountability Partnership, Active Learning Network for Accountability and Performance, Organizational Learning Partnership, and the Humanitarian Impact program of the Fritz Institute reflect a growing interest and demand for greater effectiveness, learning and accountability within the humanitarian sector; although, overall, weak evaluation and learning is still widely recognized as a chronic problem. Both humanitarian and development organizations are under growing pressure to demonstrate and measure the real impact of their projects on the livelihoods of recipient communities. Although many aid agencies claim to achieve positive impact, these claims are rarely supported by rigorous evidence (Hofmann et al., 2004; Darcy, 2005), and the \". . . gap between the rhetoric of agencies and what they actually achieve is increasingly met with skepticism and doubt amongst donors and other stakeholders\" (Roche, 1999). Agencies often support claims of project impact using information from their monitoring systems; but, increasingly, these systems focus on reporting project activity rather than impact PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 3- the delivery of inputs does not necessarily translate directly to livelihoods impacts. Other common approaches for explaining impact draw from ad hoc interviews and case studies with both approaches leaning towards reporting 'success stories' rather than understanding or describing wider impacts or project attribution. Towards better impact assessment A well-designed impact assessment can capture many of the real impacts of a project on people's lives, whether positive or negative, intended or unintended. It follows that impact assessments can also show whether project funds have achieved the expected effect on livelihoods, and if not, why not. This alone should create a greater demand from donors and greater incentives for implementing agencies to measure the results of their work. In our experience, even when results show that impact is not as expected or negative, frank and full reporting can be appreciated by donors as it suggests willingness by the implementing agency to learn from its programming. In contrast, less transparent and defensive reporting tends to evoke skepticism. Our experience also shows that when project participants are included in the impact assessment process, this creates an opportunity to develop a learning partnership involving the donor, the implementing partner, and local people. A good impact assessment process can create space for dialogue, and the results can provide a basis for discussions on how to improve programming and where best to allocate future resources. Results from some impact assessments supported by the Center show unintended impacts that differ from, and are possibly more important than, the expected impacts. If these assessments had not been carried out these impacts would not have been captured or documented, and the opportunity to use this information in designing future projects would have been lost. In addition to the internal organizational learning benefits derived from measuring impact, the results from impact assessments can be important, and sometimes critical, for influencing new policy and good practice guidelines. Experience from Ethiopia shows that evidence derived from PIA contributed to the development of national government guidelines for livelihoods-based drought responses (Abebe and Catley, 2013), while also providing an evidence base for sections of the global Livestock Emergency Guidelines and Standards (LEGS, 2009). A more systematic approach to impact measurement helps to improve accountability, not only to donors and external stakeholders, but more importantly to the recipients of aid. It will also answer the fundamental questions that are rarely asked - what impact are we really having, and do aid interventions really work? This can only lead to better programming, more effective use of aid funds, and better credibility of donors and implementing organizations. Indeed, \"In the long term the case for aid can only be sustained by more effective assessment and demonstration of its impact, by laying open the mistakes and uncertainties that are inherent in development work, and by an honest assessment of the comparative effectiveness of aid vis-\u00e0-vis changes in policy and practice\" (Roche, 1999) . Livelihood changes over time Our approach to PIA is based on the principles and methods of Participatory Rural Appraisal (PRA). It involves adapting participatory methods to measure changes in people's livelihoods over time, and to understand how different factors caused these changes. The approach is flexible and so can be adapted to local conditions. It acknowledges that people who receive aid assistance are constantly seeing and discussing its impact, and that \". . . local people are capable of identifying and measuring their own indicators of change\" (Catley, 1999). Most of the definitions of impact in development or humanitarian assistance refer to the concept of change, which can be positive or negative. Consistent with this, a project-level PIA tries to answer three key questions (Watson, 2008): 1 What changes have there been in the community since the start of the project? 2 Which of these changes are attributable to the project? 3 What difference have these changes made to people's lives? In contrast to many traditional project monitoring and evaluation (M&E) approaches, PIA aims to measure the real impact of a project on the lives of project participants. This differs from evaluation because many evaluations focus on measuring project objectives, the extent to which they were achieved, and if they weren't, why not. It follows that if project objectives do not clearly state a proposed effect on livelihoods, it becomes possible to achieve the objectives without changing livelihoods. Therefore, PIA goes beyond typical evaluation and the measurement of objectives, and examines how project activities actually benefited the intended recipients, if at all. Participatory numbers and PIA Many of the methods that we use in PIA are ranking or scoring methods that produce numbers. These numbers can be measures of indicators such as income, health or food consumed, as well as changes in dignity, status, and wellbeing. In general, the reliability of these numbers improves if the same method is repeated with different people, and this involves a level of standardization of the method. However, we also want to ensure flexibility and capture contexts, reasoning and explanation for a particular set of scores or ranks. Therefore, the adapted PIA methods have two main parts - a standardized ranking or scoring, followed by an informal and open- ended interview. Conventional statistics can be used to summarize and analyze the numerical data produced by ranking or scoring methods, and this can include comparisons of different types of activity or support. A further adaptation involves translating measures of project impact into economic values, which, in turn, supports benefit-cost analysis. These aspects of PIA are particularly useful when engaging in What is a Participatory Impact Assessment? PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 5policy reform processes, or developing good practice guidelines. These adaptations and uses of PIA now fit within a broader set of experiences with 'participatory numbers' initially reviewed by Chambers (2007), and with case studies presented in Holland (2013). These accounts show how systematic, well-designed participatory monitoring, evaluations and impact assessments are contributing to, or, in some cases, driving improvements in practice and policy in an increasing number of countries and range of contexts. Cross-checking, triangulating Information and numbers from participatory methods are cross-checked in PIA using various approaches: Revisiting the initial project document to assess if the proposed inputs were likely to lead to the stated changes. In other words, is the 'causal framework' technically sound? Reviewing the project monitoring reports, or discussions with project staff to understand what was implemented and where. Reading secondary reports, statistics and literature related to the project area, and similar, past projects. Using different PIA methods to measure the same indicator. Asking the same question in different ways within a given PIA, e.g. combining a ranking or scoring with an informal interview. What caused the change? A PIA aims to understand the relative importance of project activities against other events, support or processes that occurred independently of the project. In a PIA, participatory methods are used to: Identify the main factors that have led to changes in people's livelihoods during a project. Categorize these factors as 'project' and 'non-project' factors. Measure the relative importance of each factor. In other words, what changes (if any) can be specifically attributed to project activities? This process places project activities within the far wider economic, social and environmental context of an area, alongside a range of non-project factors that influence livelihood strategies and outcomes. Conventional research often approaches the question of attribution using control groups as well as a statistical comparison of outcomes in a treated group (e.g. those receiving project support) and a control group (i.e. those not receiving project support). This approach raises various ethical, practical and resource issues when applied to development and humanitarian aid projects, but the basic concept of comparison is useful and has been an important aspect of our PIA work. Before and after comparison - commonly, PIA has compared two points in time, such as the situation before the project began with the situation towards the end, or at the end, of a project. This approach uses a timeline, developed by local people, to clearly define the project start date and validate this timing against project documents and other events. Before and after scoring methods are then used to compare indicators at the start and end of the project. Results are cross-checked against secondary information and project monitoring reports. Comparing different activities and types of support - when communities have received different types of support it can be possible to compare these using participatory methods such as matrix scoring. Each type of support or input is compared against a set of indicators, developed with communities and project staff. Similarly, if a project supports a new kind of service, such as a community health worker, a PIA can use matrix scoring to compare these workers with other types of health service provider or facility. Comparing recipients of project support with non-recipients - rather than defining 'control groups' and 'treatment groups' at the start of a project, when reviewing project documents and monitoring reports it is often possible to identify a type of comparison group. This arises because many projects lack the resources to target all households in an area, or are designed to target specific groups, such as vulnerable households, or specific types of resources. For example: In a project that aimed to improve women's livelihoods through the creation of income-generating groups, it might be possible to compare group members' changes in income with those of women who were not part of the project. In a project that aimed to protect livestock through a vaccination program, it might be possible to compare livestock mortality in vaccinated and non-vaccinated animals. Using comparisons in PIA can be very useful for improving the credibility of the findings, but needs a good understanding of the project design and activities, and the wider context in which the project took place. For example, women who get milk from goats provided by a project might share some of that milk with their neighbors who did not receive goats. Similarly, people receiving cash during an emergency cash-transfer project might share the cash, or share items that were bought using the cash. Evidence and participatory impact assessment A systematic and well-designed PIA can assist communities to measure impact using their own indicators and their own methods. It can also overcome the weaknesses inherent in the M&E systems of many donors and Non-Governmental Organizations (NGOs), which emphasize the measurement of process and delivery over results and impacts. For many years, good practice guidelines for rural development, public health and other types of development have emphasized the importance of community participation in problem analysis, project design, and project M&E, and PIA fits within and supports these approaches. Similarly, participation is a core standard of the Humanitarian Charters and Minimum Standards in Humanitarian Response (Sphere, 2011). PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 7In an era of evidence-based approaches, one of the challenges for aid donors and implementers is how to improve the quality of evaluation and impact assessment, while also ensuring the ethical and meaningful involvement of aid recipients in gathering, analyzing and using evidence of project impact. Different research or assessment design options can be positioned according to the quality of the evidence they produce. An example of an evidence hierarchy is shown in Figure 1.1, and much of the longstanding concern over the evaluation of aid projects relates to the widespread use of anecdote and selected interviews with relatively weak evidence. In contrast, randomized case- control trials are viewed by some researchers as providing very high quality evidence, although a range of practical, ethical and resource issues can limit the use of this type of design. When PIA is well-designed, with a good understanding of local context and the systematic use of comparisons and triangulation, it seems to produce evidence that is of reasonable quality and which a range of people - from community members to policy makers - can understand and use. Figure 1.1 Levels of evidence and participatory impact assessment Blind randomized case control trials Evidence ++++++ Use - Randomized case control trials Evidence +++++ Use + Randomized survey Evidence +++ Use + Selected interviews Evidence + Use +++ Anecdote Evidence - Use +++Systematic PIA Evidence ++++ Use +Can be blind or double-blind. Provides strong evidence of cause and effect (attribution) but very rarely practical or needed in the assessment of development or relief projects A common approach used in epidemiological studies; provides strong evidence of cause and effect (attribution). Rarely used or needed for assessing relief or development projects, except for some human health and disease control projects Can produce useful descriptive information, but usually provides limited evidence of cause and effect when evaluating aid projects. Often used in the assessment of development or relief projects. Involves interviews with purposively or conveniently selected people, including project beneficiaries. The case study material used by some NGOs can fall into this category, with best-case examples often used. Ad hoc informal pieces of information and stories which are not collected in any systematic way. Sometimes direct quotations in reports are anecdotal. When considering the timing of a PIA, impacts can be seen as occurring during a project, and in the months or years after a project. For example: In a drought response program, households can acquire cash from a de-stocking activity and can spend this cash immediately on food, animal fodder and veterinary drugs. In contrast, participants in a sheep restocking project may strategize to build their herds and eventually exchange some sheep for cattle, but this exchange and the benefits of cattle may not be evident for years after the project ended. Similarly, most agricultural interventions need at least one crop cycle to achieve any impact, and training projects may take even longer to see impact. Typically, when resources are available to support impact assessment, these resources are tied to a particular project and have to be used before the project ends. It follows that most PIA is done towards the end of a project, or soon after, and the main impacts that are measured are immediate, real impacts or proxy impacts. However, it can also be possible to identify future impact indicators, even if these cannot be measured. In the restocking example (see previously) informants might identify cattle ownership as an indicator of long-term impacts and thereby show how the initial transfer of sheep might create other assets. In situations when resources become available to support impact assessment some years after a project ended, PIA can still be used. However, the design of the PIA becomes more complex because various other factors and projects may have taken place after the project in question ended, and contributed to changes in livelihoods. Impacts from a specific project become blurred over time and attribution becomes more difficult to measure. In cases where local people are recalling a situation, recall tends to become less reliable as time increases, and so more effort is needed to triangulate information. When to do an impact assessment PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 9Designing a Participatory Impact Assessment - an eight-stage approach Our approach to assessing impact emphasizes the participation of project households, and uses an eight-stage framework. The framework is intended to be generic and flexible, so that users can adapt it to local needs and conditions. It combines participatory approaches and methods with some basic epidemiology or 'good science' principles. The PIA framework draws on various bodies of experience such as: The 'soft systems' participatory impact assessment approaches of Action-Aid Somaliland during the mid-1990s. Guidance on participatory M&E from the International Institute for Environment and Development (IIED), and case studies of participatory M&E in the journal Participatory Learning and Action . Our use of PIA since the late 1990s, particularly in complex emergencies, and as a strategy for using evidence to influence policy reform. Experiences with participatory epidemiology, supported by IIED, the Feinstein International Center and the University of Edinburgh. A more recent and broad-ranging body of experiences with participatory numbers in Africa and Asia, and how participatory processes and methods can produce conventional statistics. Although these eight stages for designing a PIA are presented sequentially (see box on right), much of the decision-making and activity for different stages takes place simultaneously. For example, in a workshop with project staff and community representatives you might cover Stages 1 and 2, part of Stage 4, and Stages 5-8. Then with community informants you might complete Stage 3, and test the methods under Stage 4. In other words, although it is important to set the questions first (Stage 1) and feedback results at the end (Stage 8), you do not necessarily have to complete the other stages in the numerical order in which they are presented.Background Eight stages for designing a Participatory Impact Assessment Stage 1 Define the questions to be answered Stage 2 Define the geographical and time limits of the project Stage 3 Identify and prioritize locally defined impact indicators Stage 4 Decide which methods to use for measuring change, and test them Stage 5 Decide which sampling method and sample size to use Stage 6 Decide how to assess project attribution Stage 7 Decide how to triangulate results from participatory methods with other information Stage 8 Plan the feedback and final cross- checking of results with communities PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 11Setting the questions: an example from a livestock distribution project Assume that a project has provided sheep or goats to female headed households. For such a project, the impact assessment may only need to answer three questions: 1 How has the project impacted, if at all, the livelihoods of the women involved in the project? 2 How has the project impacted, if at all, the nutritional status of the women's children? 3 How might the project be changed to improve its impacts in future?One of the most important, but often the most difficult, parts of designing an impact assessment is deciding which questions should be answered. Defining the questions for an impact assessment is similar to defining the objectives of a project - unless you know specifically what you are trying to achieve, you are unlikely to achieve it. Many impact assessments and evaluations try to answer too many questions, leading to superficial or inconclusive results on a wide range of issues and, therefore, uncertainty on how to use this information. Although it is tempting to try and capture as much information about a project as possible, it is usually better to limit the impact assessment to a maximum of five key questions, and answer these questions with confidence. This approach is similar to the notion of 'optimal ignorance' in Participatory Rural Appraisal (PRA) and assumes that we do not need to know everything about a project in order to identify key impacts and make improvements. If you have already worked with communities to identify their impact indicators at the beginning of the project, the assessment questions will be framed according to these indicators. More likely, you will be using a retrospective approach, and working with project participants to jointly define the assessment questions. Stage 1: Identifying the key questions Defining the geographical (spatial) boundaries of a project aims to ensure that everyone understands the physical limits of the area in which impact is supposed to take place. Defining the project's time boundaries aims to ensure that everyone is clear about the time period being assessed. Defining the project boundary: participatory mapping Participatory mapping is a useful visualization method to use at the beginning of an assessment to define the geographical boundary of the project area. It also acts as a good ice-breaker as many people can be involved. Maps produced on the ground using locally available materials are easy to construct and adjust until informants are content that the information is accurate. Mapping is a useful method for the following reasons: Both literate and non-literate people can contribute to the construction of a map, as it is not necessary to have written text on it. When large maps are constructed on the ground, many people can be involved in the process and contribute ideas. People also correct each other and make sure that the map is accurate. Maps can represent complex information that would be difficult to describe using text alone. Maps can be used as a focus for discussion. Many variations on the basic mapping can be used. These include projecting images of local maps derived from Google Earth onto flip chart screens and asking people to add layers of detail to the maps, including community boundaries - but note that annotation using marker pens can be difficult to adjust relative to maps on the ground. Guidance for participatory mapping for PIA 1. Mapping is best used with a group of informants, say 5-15 people. Find a piece of open ground and explain that you would like the group to produce a picture showing features such as: Geographical boundaries of the community. In pastoral areas, these should include the furthest places where people go to graze their animals. Main villages or human settlements. Roads and main foot paths. Rivers, lakes, dams, wells and other water sources. Crop production farmed areas, fishing areas, forests and other natural resources. Market centers. Services, clinics, schools, shops, seed and fertilizer distribution outlets, veterinary clinics, government offices. Ethnic groups. Seasonal and spatial human and livestock movements. Areas of high risk, flooding, insecurity, tsetse flies, ticks and other parasites. Stage 2: Defining the boundaries of the project in space and time PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 13Explain that the map should be constructed on the ground using materials that are to hand. For example, lines of sticks can be used to show boundaries, and stones may be used to represent human settlements. In some communities people may be more comfortable using flip charts and colored markers to construct the map. If in doubt, ask the participants which option they would prefer to use. 2. When you are confident that the group understands the task they are being asked to perform, it is often useful to explain that you will leave them alone to construct the map and return in 30 minutes. At that point, leave the group alone and do not interfere with the construction of the map. 3. After about 30 minutes, check on progress. Give the group more time if they wish. 4. When the group is happy that the map is finished, ask them to explain the key features of the map. The process of 'interviewing the map' enables assessors to learn more about the map and pursue interesting spatial features. As mentioned, a map can be a useful focus tool for discussions and follow-up questioning. It is important that one member of the team takes notes during this discussion. During this part of the exercise ask the participants to include any project infrastructure on the map in relation to the other features. For example, if the project constructed wells or a cereal bank, or established a community vegetable garden, ask the participants to illustrate these on the map. In many cases these may already have been included, which already tells us something about the importance of the project from the perspective of the participants. Similar or other types of physical assets may have been established by the government or another NGO in the project area and it is important to also include these on the map. 5. It is often useful to add some kind of scale to the map. This can be done by taking a main human settlement and asking how many hours it takes to walk to one of the boundaries of the map. In less remote communities people may already know how many kilometers it is from one settlement to another and can define this on the map. A north-south orientation can also be added to the map, or arrows pointing to a major urban center or natural feature lying outside of the boundary of the map. 6. Make two large copies of the map on flip chart paper. Give one copy to the group of participants. When maps are used to show seasonal variations, such as flooding, livestock movements, or crop production, these can be cross-checked using seasonal calendars. The increasing use of computer scanners and digital cameras means that copies of maps can easily be added to reports. Defining the project period: timelines Defining the project boundaries in time, sometimes called the 'temporal boundary', aims to ensure that everyone is clear about the time period that is being assessed. A timeline is an interviewing method that captures the important historical events in a community, as perceived by the community itself, and positions the project start date and end date against these events. This method helps to ensure that everyone involved in the assessment understands which project is being assessed as well as the project timeframe, and helps people to recall events and changes during the project.1 The following timeline was produced by five key informants in a rural community in Zimbabwe, participating in a drought recovery project. Key political events were used as reference points for the timeline. The timeline (Figure 2.4) shows when the project started and a consequent improvement in food security shortly thereafter. Note that the timeline also shows external factors that might have contributed to food security, such as improved rainfall and other NGO interventions. Where applicable, a timeline should highlight non-project factors in order to help isolate the impact of the project from other relevant variables. Examples of participatory mapping Figure 2.1 Map of Pyutar Village Committee area, Ward 9 by Krishna Bahadur and Iman Singh Ghale (source: Young et al., 1994) This map was produced by two farmers in a sedentary community in Nepal . The map shows the location of the main livestock types, areas of cultivation and other features.Figure 2.2 Map of Kipao village, Garsen Division, Tana River District (source: Catley and Irungu, 2000) This map was constructed by Orma herders . It shows the dry season grazing areas for cattle around Kipao and proximity to areas infested by biting flies called tsetse. During the wet season, the area became marshy and cattle were moved to remote grazing areas . 1 In other words, a timeline reduces recall bias during impact assessment. PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 151986 Prosopis (an invasive tree) introduced by the Natural Resource and Wildlife Protection Organization of the former government 1991 Downfall of Ethiopian government (Derg) 1992 Livestock deaths due to the disease 'sole' in addition to drought 1993 Boya-hagay in which a large number of livestock deaths were remembered 1994 Widespread camel deaths due to diseases 'goson' and 'Kahu' 1995 Woder-Temere in which the widespread death of goats occurred due to diseases 'gublo' and 'korboda' , leaving the kraal empty 1996 Good rain and milk, life was good; 'waybo' disease detected for the first time in Afar and killed many cattle 1998 School, clinic and water reservoir construction started by SATCON; people employed as daily labor and earned a lot of money Lahibiak (swelling) around the neck (possibly anthrax) occurred and killed over 100 people 1999 Some herders selected to be trained as community-animal health workers (CAHWs) 2000 Human health clinic started 2001 Second round of CAHWs selection 2002 Extensive farming initiated by private investors and governmentFigure 2.3 Timeline from an impact assessment in Telalak, Ethiopia (source: Ethiopia Participatory Impact Assessment Team, 2002) This timeline was used at the start of an impact assessment of a community animal health project in Ethiopia . It shows the start of the project in 1999, and the project was still ongoing at the time of the impact assessment in 2002 . Presidential Elections Parliamentary Elections Gokwe Integrated Recovery Action (GIRA) project started in December 2005 PIA May/June2000 2002 2003 2004 2005 2006 2007 National Referendum & Parliamentary Elections Harvest was OK DROUGHT year, little or no harvest (March). Grains (maize) ran out by November. People started selling livestock to buy grain and eating fewer meals. They also started consuming 'svovzo' . Some people moved to more productive neighboring areas in search of agricultural work. Concern started distributing in kind food assistance from December through to March 2003 Small harvest in March. Grains (maize) ran out by November, people started exchanging household items for grain; some sold ox carts, ploughs, window frames and roofs in order to purchase maize Good harvest DROUGHT year, little or no harvest, people selling livestock and belongings to purchase grains. In August Africare started developing the GIRA project proposal in partnership with the community. Concern started distributing in-kind food assistance in November through to April 2006. Africare reinitiated the GIRA project in December 2005 - distributing soy bean, sorghum and sweet potato seeds. Although late in the planting season, many farmers managed to plant at least some of these seeds. Distributions continued through to January 2006 Good harvest in March, particularly for sorghum, sweet potato and soy beans. This was attributed to high rainfall, and the seeds distributed by Africare. Two bad years and one medium year implied that most farmers either had no seeds left or at least no good quality seeds. Africare did a second round of seed distributions in September/October (soya beans, sweet potato, sunflower, maize and groundnuts) Bad maize harvest, as a result of poor rainfall. Soya beans and sweet potato did well, groundnuts did okay. By June people already having to purchase maizeFigure 2.4 Timeline of recent events, Nemangwe, Zimbabwe (source: Burns and Suji, 2007b) PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 17Types of indicators and measurement A key feature of all types of project assessment is that inputs, activities, outputs, change or impact are measured. The things that we measure are usually called 'indicators' and there are two main types of indicators as follows: Process indicators2 usually measure a physical aspect of project implementation, for example the procurement or delivery of inputs such as seeds, tools, fertilizer, livestock or drugs, the construction of project assets and infrastructure, such as wells or home gardens, the number of training courses run by the project or the number of people trained. Process indicators are useful for showing that project activities are actually taking place according to the project work plan and budget. However, this type of indicator may not tell us much about the impact of the project activities on the participants or community. Impact indicators measure changes that occur in people's lives and can be qualitative or quantitative. Many projects involve some sort of direct or indirect livelihoods asset transfer, such as infrastructure, knowledge, livestock, food or income. These asset transfers sometimes represent impact, but usually it is the benefits or changes realized through the use of these assets that represents more meaningful impact on the lives of project participants. For example, if a project provides training in new, improved farming practices, a transfer of skills and knowledge (human capital) would be expected. While this new knowledge indicates a certain level of impact, it is the application of new knowledge and practices that may ultimately result in higher yields and income as well as financial assets among participating farmers. In this example, the changes in knowledge and the improved yields that are attributable to this knowledge are effectively only 'proxy indicators' of impact. If some of the extra food produced is consumed by the farmer and his family, this utilization can represent a real food security and nutritional benefit, or livelihoods Stage 3: Identifying indicators of project impact Process indicators measure the implementation of the project activities. These indicators are usually quantitative e.g. 'number of government staff trained' is a process indicator which might be reported as '15 agricultural extension officers trained'. Impact indicators look at the end result of project activities on people's lives. Ideally, they measure the fundamental assets, resources and feelings of people affected by the project. Therefore, impact indicators can include household measures of income and expenditure, food consumption, health, security, confidence and hope. 2 Some organizations call process indicators 'outcome indicators'. impact. Alternatively, if increased income derived from crop sales allows for livelihoods investments in health, education, food and food production, or income generation, these expenditures would represent a real impact on the lives of the project participants. Community-defined indicators of project impact Where possible a PIA should use impact indicators that are identified by the community or intended project participants. Communities have their own priorities for improving their lives, and their own ways of describing and measuring change. Sometimes these local indicators are similar to those identified by project staff, but, often, local people also suggest important indicators that otherwise would have been overlooked. In general, the M&E systems of organizations and projects emphasize 'our indicators' not 'their indicators'. For example, selected drought response projects in Zimbabwe and Niger aimed to measure project impact against specific household food security indicators, such as increased crop production and dietary diversity. When project participants were asked to identify their own benchmarks of project impact they included the following indicators: The ability to pay for school fees using project-derived income. The ability to make home improvements. Improved skills and knowledge from the project training activities. Improved social cohesion. Time saving benefits provided by the project. Identifying community-defined indicators - one way of collecting community indicators of impact is simply to ask project participants, when the project starts, what changes in their lives they expect to occur as a direct result of the project. Alternatively, in cases where the project has already been implemented you can ask what changes have already occurred. This should be done separately for each project activity that you plan to assess. If the project has a technical focus, for example, natural resource management, the provision of agricultural inputs or livestock, ask the participants how they benefit from the ownership or use of the resources in question. These benefits are impact indicators. The difference between having assets and using assets - typically, local people will refer to changes in assets when asked to 6% milk 34%Figure 3.1 Benefits derived from cattle, Dinka Rek Community Animal Health Project, Tonj County, South Sudan (source: Catley, 1999) Proportional piling was used with 10 community groups to explore the benefits derived from cattle, and therefore, the potential impact of controlling cattle disease . This example shows the value of enabling communities to describe impacts and benefits. Technical project staff had previously noted the importance of milk from cattle to these communities, but had failed to recognize that the use of cattle for marriage payments was the second most important reason for keeping cattle and, therefore, an important impact indicator . PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 19describe project impacts. However, it can be useful to look at deeper aspects of 'having assets' and, specifically, how assets are used. When identifying the impact indicators try to be specific, not general. For example, 'The goats give me milk' is not very specific. A better and more specific indicator is 'The children drink the goats' milk' or 'I use the income from selling milk to pay school fees' . Similarly, the indicator 'I have more status in the community' is not very specific. A better indicator might be 'I can now join the local savings and credit group in the village'. Gender perspectives - when collecting community indicators it is important to capture the views of different groups of people within the community. Women will often have different priorities and expectations of project impact than men. The same might apply to different groups. For example, pastoralists are likely to attach greater importance to the livestock health Unpacking how assets are used: an example from a restocking project Participants in a restocking project may tell you that they now have more goats as a result of the project. Although an increase in goat ownership would be a good community indicator of impact, this alone doesn't tell us how the goats will benefit that person or household. Therefore, when discussing these kinds of indicators it is important to follow up with additional questions. It may be that the actual benefit derived from the goats is an increase in milk production which 'we feed to our children' . In this case, increased milk production, or increased milk consumption by children are better indicators of impact than simply an increase in the number of goats.3 You can then go a step further and ask how milk is beneficial to their children, and people might mention the health and nutritional benefits that milk provides. At this point, it may be that the best indicator of impact is improved child nutrition. Alternatively, the participants may have received income from the sale of the goats or goat products. If this is the case, you will want to ask how they used this extra income. Expenditures on food, education, clothes, medicine, ceremonies, and investments in livestock, agricultural inputs, or income-generating activities are all good livelihood indicators of impact that can be measured easily. Again, investigating how livestock, livestock products, and the income earned from these are utilized can be a useful way of unpacking and identifying livelihood impact indicators. 'I sell young goats and use the money for food. ' 'I now have more status in the community. ' 'I've now joined the savings and credit group. ''I feed goat milk to my children. ' Asset transfer - more goatsProject activities Use process indicators to assess project implementation'More goats' is an initial impact indicator identified by local people The asset has been unpacked to reveal specific uses of the asset . These are stronger impact indicators identified by local people 3 If the impact assessment takes place before the desired project impact is expected, you may have no choice but to use proxy indicators such as an increase in the number of livestock. Although not ideal, at least if these have been identified by project participants, they can, to some extent, be validated as community indicators. benefits from a project well, compared to other users who are crop farmers. Handling a large number of impact indicators - if the community or participants produce many impact indicators, ask them to prioritize the indicators using ranking. It is important not to have too many indicators: as with the key assessment questions, it is better to have a few good indicators than too many poor ones. Try to limit the number of indicators therefore to no more than five per project activity being assessed. Using photographs to reveal community impact indicators - an excellent method for understanding local perceptions of impact is to lend or give digital cameras to project participants, and ask them to take photographs of any aspect of a project which they feel is important. They can be asked to photograph project activities as well as positive and negative impacts. After a few days or weeks, the photographs are collected and reviewed, and informants asked to explain why they took the photograph, or 'tell us the story about this picture' . The photographs and the stories will reveal a range of local impact indicators. Quantitative and qualitative indicators Community impact indicators may be quantitative , such as income earned from crop sales, or qualitative , such as improved skills, knowledge or social status . An important aspect of PIA is that opinions, perceptions and feelings can be expressed numerically. Therefore, qualitative indicators are measured using participatory ranking or scoring methods, and the methods are repeated with different informants to improve reliability. These three photographs were taken by people involved in an urban gardens project in Ethiopia and illustrate impacts such as the enjoyment of gardening, selling vegetables for cash, and uses of cash to buy household items, food and coffee. Each of these indicators can be measured systematically using scoring methods (source: Schroff et al., 2011) . PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 21Changes in coping strategies During a crisis, people will often use various livelihood strategies to cope. These 'coping strategies' are often good indicators for measuring change or impact. For example, during a drought people may sell most of their livestock (usually at a reduced price) and use the money to buy food and cover other important expenses. When the drought ends, they will then often re-invest in livestock assets. By capturing these changes you can determine whether the situation has improved and to what extent the project played a role in facilitating this change. To identify these coping strategies, simply ask people what they did during the period leading up to and during the crisis. For most livelihoods projects, community indicators of project impact will often relate to changes or improvements in income, food security, health and education. Impact against these indicators as well as changes in coping strategies can often be broadly captured by looking at changes in income and food sources, as well as household expenditure. For example, using the strategies in Table 3.1: Compared to a normal year, in a year with a poor cereal harvest we might expect a greater portion of household food to come from wild foods (strategy #8) relative to cereals. Examples of impact indicators Quantitative increased milk consumption by children income from crop sales value of financial assets Qualitative trust confidence hope status participation voice security dignity social cohesion wellbeing Coping mechanisms 1 De-stocking to save remaining livestock and purchase grain (early stages of drought) 2 Stress sale of livestock at reduced prices in order to purchase grain (later stages of drought) 3 Sale of household assets (including roofing, doors, windows and cooking utensils) in order to purchase grain. 4 Migrate to other areas in search of better pasture for livestock 5 Increase vegetable production for consumption and sale 6 Migration of young men to urban areas as well as to other countries in search of employment 7 Expand on informal income-generating activities such as mat weaving, brick making, firewood collection 8 Increase production/collection and consumption of wild foods 9 Reduce the number of meals consumed (even down to one meal a day) 10 Engage in agricultural work in neighboring communities less affected by the drought, or for wealthier farmers 11 Participate in food-for-work projects or public safety net program 12 Permanently migrate to urban areas and give up agro-pastoralist livelihoods practicesTable 3.1 Examples of common coping strategies We might also expect a greater portion of income to come from the sale of household assets (strategy #3) relative to other income sources during this period. In terms of household expenditure, after a poor harvest we might expect a greater proportion of household income to be spent on food to compensate for the decline in farm production. During a recovery period following a drought, we might expect households to spend more of their income on livestock assets, as they re-stock after suffering livestock losses due to the death of their animals or stress sales. Therefore, tracking changes in food, income and expenditure can often be a useful way of measuring impact against community indicators of impact and against coping strategies. Many livelihoods projects also have food security, income generating, or livelihoods diversification objectives and, again, food, income and expenditure changes can be a useful way to measure change against these objectives. We emphasize that an understanding of the context is essential in deriving meaning from these indicators, as livelihoods and coping strategies will vary depending on the type of crisis being experienced. They will also change over time and between different communities. Simply measuring changes in livelihoods impact indicators will not tell us much about impact unless you understand the reasons behind those changes. An understanding of livelihoods and context is therefore an important part of any impact assessment. Using monitoring data for impact assessment Process monitoring - most project M&E systems measure the delivery of inputs and activities, as opposed to the real impact of the project on people's livelihoods. However, process monitoring data is still very useful during impact assessment because it allows a comparison of local people's description of impact with the items or types of support that were actually delivered. This type of comparison is a useful way to cross-check (triangulate) PIA findings. By reviewing what was implemented and where, it is usually possible to estimate likely impacts and, therefore, compare these expected impacts with local views. For example, if a food security project introduces high-yielding crop varieties into a community and an impact assessment shows an overall improvement in food security, the process monitoring reports should tell us whether the improved seed varieties were indeed delivered and planted at a sufficient level to achieve impact, and that harvests were consistent with changes in food security. Proxy indicators - in addition to measuring process indicators, some M&E systems measure proxy indicators of impact. For example: Knowledge transfers from a farmer training course might be measured by testing the participants to see if they have learned new techniques. A project that introduces high-yielding crop varieties might measure crop yields as a proxy for impact, assuming that increased production automatically translates into improved household food security. Although proxy indicators of impact can be useful and easy to quantify, they may provide misleading measures of impact. Using the previous examples, these indicators do not describe the use of new assets (knowledge or crop yields) or the actual changes to people's lives that resulted from the transfers. There are many reasons why an assumed benefit, as PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 23measured using a proxy indicator, does not materialize into a real benefit. For example: A farmer may have been unable to use a new farming technique because the seed varieties or fertilizer was not available, or were only available through high-risk credit schemes - the farmer has new knowledge but cannot apply it. Although a new cereal variety produced a better yield, food aid distributions in the area reduced market demands and prices for cereals. The project area became insecure and crop harvests were looted. In this case, the asset transfer actually put people at risk of violence with a negative impact. Households were in debt and so crops were sold to pay off loans rather than being consumed and improving food security. Excessive post-harvest losses occurred due to problems with grain storage and pests. Therefore, proxy indicators need to be interpreted with care. In particular, although people might have gained new assets in the form of knowledge, skills, food or income, were these assets actually used and if not, why not? If so, what was the impact? After selecting the impact indicators, you will need to decide which methods to use to measure the indicators. This section provides guidance on participatory methods, but in common with the overall PIA framework, each of the methods can be adapted to suit a particular need or context. Some useful methods for measuring impact are: simple ranking simple scoring before and after scoring before and after proportional piling pair-wise ranking matrix scoring impact calendars radar diagrams All these methods produce numbers, but also involve the use of semi-structured interviews as part of the method. Each method has strengths and weaknesses, and some methods are more appropriate for certain cultures and contexts. For additional resource materials on participatory tools and methods see Annex 1. Testing the methods At first sight, many participatory methods look simple to use. However, even for practitioners who are very familiar with the methods, it is important to test each method before it is used in a PIA. The testing helps to ensure that informants can be given clear guidance on how to provide the information needed, including a clear description of the ranking and scoring system. Similarly, among the informants there needs to be a clear and common understanding of the different items that are being ranked or scored, and a common interpretation of any diagrams that are used (see 'Using visual aids', below). Typically, testing the methods takes place in one of the communities where the PIA will be conducted. Using visual aids An advantage of many participatory methods is that illiterate people can be involved. Commonly, the methods use diagrams Stage 4: Choosing the methods PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 25and pictures to illustrate the different items that are being ranked or scored and, in the case of PIA, the indicators. Diagrams can be produced using sketches on pieces of card or more elaborate approaches can be used, such as photographs or printed diagrams. Alternatively, local materials can be used to represent each indicator or item. For example, a head of sorghum might represent rain-fed production, a broad green leaf might represent vegetable production, and a feather might be used to represent poultry production. For all types of visual aids, the value of the ranks or scores provided by informants will partly depend on a clear and common interpretation of each diagram or picture. The visual aids should be explained, and informants should verify that the meaning of each visual aid is understood. Informants can also produce their own visual aids and this approach helps to ensure good understanding of their meaning. During the testing of the methods, diagrams or pictures may need to be amended to improve clarity. Simple ranking Simple ranking requires informants to assess the relative importance of different items, usually by placing the items in order of importance (1st, 2nd, 3rd etc.). Simple ranking is a useful way of prioritizing the impact indicators you wish to use in an assessment, or to get an understanding of which project benefits or activities are perceived to be of greatest importance, with reasons. Simple scoring Simple scoring requires informants to use counters such as seeds, stones, nuts or beans to attribute a specific score to each item or indicator. For example, ten counters per item might be used and people asked to assign scores of between 0 and 10 depending on the importance of the item. Note that with Table 4.1 Simple ranking of overall project benefits by focus group participants (source: Burns and Suji, 2007b) Benefit Ranking in order of importance (n=16 groups) Better farming skills 1st More food (fewer hunger months) 2nd Increased variety of food/dietary diversity (improved nutrition) 3rd Improved health 4th Increased income from sale of food 5th Data derived using the summary of ranks from 16 focus group discussions. The original data was collected using simple ranking. Cereal crops Project garden Livestock Poultry Fishing Wild foods Purchased Food aid simple scoring, a relatively small number of counters are used and informants are asked to count out the actual number of counters assigned to each item (compare this approach with proportional piling - see below). The method is more sensitive than simple ranking because it shows the relative size or amount of difference between the items or indicators being scored. In the example in Figure 4.1, we assumed that a food security project established a Table 4.2 Simple ranking of livestock assets (source: Burns, 2006) Ranking of livestock assets Women Men Cattle 1st Cattle 1st Sheep 2nd Goats 2nd Goats 3rd Sheep 3rd Camels 4th Camels 4th Donkeys 5th Donkeys 5th Horses 6th Horses 6th In this example pastoralists were asked what benefits they derived from different livestock. They were then asked to rank them in terms of the overall benefits they provided. The exercise was done with both women and men's groups to ensure that any gendered differences were captured. In this example the only variation was that women ranked sheep higher than goats as they fetched a higher market price. The men valued goats slightly higher than sheep as they are more resilient to drought. Cereal crops 30% Project garden 10% Livestock 13% Poultry 7% Fishing 10% Wild foods 10% Purchased 17% Food aid 3% Figure 4.1 Simple scoring of food sources PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 27community nutrition garden. In this case, we can measure the impact of the garden on household food security using simple scoring. This could be done by asking project participants to identify all the food sources that contribute to the household food basket. Using visual aids to represent each of the different food sources, w e would then ask them to distribute counters against the different food sources to show the relative importance of household food derived from each food source. The results can be presented using a pie chart as shown, with the scores converted into proportions (percentages). In a restocking project in Niger, women identified increased milk production as an important project benefit. Simple scoring was used to show how the extra milk was actually used in their villages (Figure 4.2). Proportional piling Proportional piling is useful if a large number of items need to be compared: The method starts with a large number of counters, usually 100. This means that the results can easily be converted into percentages. The method does not ask informants to physically count out the number of counters for each item, but more to distribute the counters to show a visual pattern that illustrates the relative importance of each item. Therefore, proportional piling is a type of visualization method where the results are recorded numerically. An example is shown in Figure 4.3 on page 29, where households sold cattle to private traders during a drought, and so received income from cattle sales. The chart shows how the income was used at household level. Before and after scoring Before and after scoring adapts and expands simple scoring to compare impacts or items at two points in time, typically before a project and then during or after a project. Definitions of 'before', 'after' or 'during' are derived from timelines. This method is particularly useful for measuring impact where project baseline data is weak or non-existent. In the example shown in Figure 4.4, some interpretations of the results were as follows: In terms of impact, the results indicate that food produced in the project garden contributed to the household food basket by the Consumed 50% Consumed 40%Given away 20% Given away 30%Sold 30% Sold 30%Figure 4.2 Scoring the uses of milk in a restocking project (source: Burns et al., 2008) Milk utilization from restocking Fadama Village Milk utilization from restocking Marafa Village Participants identified three different ways in which the milk was being utilized. They were then asked to distribute ten counters amongst the three categories to illustrate what portion of the milk was utilized in each way . The ways in which the milk is being utilized implies a nutritional benefit (consumed), an income benefit (sold) and a social benefit (given away) . These are all project impacts . 35 \u2014 30 \u2014 25 \u2014 20 \u2014 15 \u2014 10 \u2014 5 \u2014 0 \u201427.7 11.78.8 1.93.26.618.8 5.36.05.3 4.3 careBuy clothesOthersMean proportion (%) of expenditure (95% confidence interval) Type of expenditureFigure 4.3 Scoring income utilization - household use of income derived from a commercial de-stocking project (n=114) (source: Abebe et al., 2008) Cereal cropsBefore (36) After (30) Project gardenBefore (0) After (10) LivestockBefore (11) After (13) PoultryBefore (2) After (7) FishingBefore (10) After (10) Wild foodsBefore (14) After (10) PurchasedBefore (20) After (17) Food aidBefore (7) After (3)Figure 4.4 Before and after scoring of food sources Steps 1 The simple scoring method from Figure 4.1 has been adapted into a before and after scoring. Participants were asked to count and distribute counters to show their food sources before the project started. 2 Once they were happy with the distribution of the counters, the results were recorded. 3 They were then asked to repeat the scoring to show the situation after the project. 4 Informants were then asked to explain the scores - the reasons for the scores are as important as the scores themselves. PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 29end of the project. By summating all of the 'after' scores, it can be calculated that project garden food made up about 10% of all food by the end of the project. The reduced reliance on rain-fed crops, wild foods, and relief aid was partly explained by the new use of food from the project garden. Increased wild food consumption is often cited as a food security coping mechanism, and so a reduced dependency on this food source, as well as on food aid, indicates a positive impact on food security. However, it is also possible that a reduction in food aid may have been due to supply issues, and the reduction in rain-fed crops and wild foods may have been the result of inadequate rainfall and a poor harvest. In this case, production from the project garden may have helped people to cope with the bad harvest, and project impact would be framed more in terms of improving people's resilience to food shocks, rather than an improvement in food security. Consistent with this, the results do not show an overall increase in food, or even an improvement in food security, only the relative change in the contributions of the different food sources. The increase in the food contribution from poultry production may be due to the fact that the respondent was able to invest in hens using income from the sale of crops produced in the project garden. This livelihood investment would represent a project impact, and the increase in the contribution from this source is a useful indicator of this impact. Alternatively, the income to invest in poultry may be attributed to project- related savings as opposed to direct project-derived income. It is possible that before the project, people would have to purchase some of the food they now produce in the project garden. This saving may account for the results showing a relative reduction in the amount of household food now being purchased. While all or none of these interpretations may be true, there is no way of knowing unless informants are asked to explain the scores. On their own, the scores have limited meaning and so it is very important to follow up the scoring process with further questions, as part of a semi-structured interview. Figure 4.5 on page 31 shows the results from a similar before and after scoring, but where the method was standardized and repeated with 145 informants. Relative to Figure 4.4 on page 29, the results are presented using conventional statistics with mean scores and 95% confidence intervals. This type of presentation allows statistically significant differences between the before and after scores for each crop to be identified, as well as differences between crops. For example, for groundnuts there is no overlap between the 95% confidence intervals of the before and after scores and, therefore, a significantly higher contribution of groundnuts after the project. However, in common with Figure 4.4, the scores are arbitrary and depend on the scoring system used; the results and changes only really become meaningful when informants explain the reasons behind the changes. A further aspect of before and after scoring is that the scoring method that is used has a considerable effect on how to interpret the results. In general, two main approaches are used: Option 1: using the same number of counters 'before' and 'after' - an informant is given a number of counters, e.g. 20, and asked to assign these counters to show the importance of different sources of income before a project. The informant is then given another 20 counters to show the situation after the project. This method uses the same number of counters before and after, and, therefore, cannot indicate if the total income increased, decreased, or stayed the same during the project. Option 2: allowing informants to select the number of counters for scoring 'after' - an informant is given 20 counters as before, and asked to assign these counters to show the importance of different sources of income before a project. The informant is then asked to select a total number of counters of their choice, to show the situation after the project - they can select another 20 counters, or opt for more or less counters in total. This scoring system allows changes in total income to be assessed, as well as the relative importance of the different sources of 6 4 2 0 Cow peas Groundnuts Livestock Maize Other Poultry Sorghum Soya Sunflower Sweet potatoVegetablesMean score (95% confidence interval) Type of foodFigure 4.5 Before and after scoring of food basket contributions (n=145) (source: Burns and Suji, 2007b) Before After Comparing two types of before and after scoring Assume that a farmer in Zimbabwe earns one hundred percent of his income from selling cotton and in a typical year can expect to earn $US900 from cotton sales. An NGO then runs a project to promote soya and sweet potato and, through involvement in the project, the farmer earns an additional $US300 from sales of soya and sweet potato. Two different before and after scoring methods are used during a PIA. Option 1 uses the same total number of counters before and after. Option 2 allows the farmer to select the total number of counters to show the 'after' situation. The results could be as follows: Source of income Scoring method, option 1, income Scoring method, option 2, income Before After Before - nominal baselineAfter Cotton Soya and sweet potato20 015 520 020 5 Total 20 20 20 25 The results from option 1 do not show the overall increase in income, but only the relative importance of cotton versus soya and sweet potato after the project. The results from option 2 are more revealing and useful. They show both the overall increase in income (from 20 to 25 counters in total, or a 25% increase in total income), as well as the relative importance of the different crops. PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 31income. The 20 counters at the start of the project represent a nominal baseline. The kind of results that are produced from these two different scoring methods are illustrated in the box 'Comparing two types of before and after scoring' on page 31. Further examples showing the value of nominal baselines are shown in Table 4.3 and Figure 4.6. Scoring against a nominal baseline can be useful for measuring changes in sensitive impact indicators such as income, livestock numbers or crop yields. People may be unwilling or uncomfortable discussing exact amounts in these cases. But with this scoring method sensitive questions like, \"How much money did you make?\" or, \"How many cattle do you own?\" are not necessary.Table 4.3 Using a nominal baseline to show changes in income (source: Burns and Suji, 2007b) Location (number of informants) Mean proportional increase were asked to show if there had been any increase or decrease in actual income since the project started. This was done by placing ten counters in one basket which represented their income before the project. The participants were then given another ten counters and asked to show any relative changes in household income, by either adding counters to the original basket of ten, or removing them. For example, if someone were to add four counters to the original basket this would represent a forty percent increase in income . Alternatively, if they were to remove four counters it would represent a forty percent decrease in income. The participants were then asked to explain the changes. Figure 4.7 Impact scoring - how do different cattle diseases affect milk production? (source: Catley, 1999) Awet - Rinderpest Daat - Foot and mouth disease and foot rot Guak - Probably fasciolosis Joknhial - Anthrax Abuot - Contagious bovine pleuropneumonia Ngany - Internal parasites Liei - Mixed infection; includes trypanosomosis and fasciolosis Makieu - unknown Proportional piling was used to compare milk production in a healthy cow as opposed to those suffering from different types of cattle disease. The black dots represent the piles of counters . A hundred counters (in the center of the diagram) were the nominal baseline, and were used to represent milk production from healthy cattle . The smaller piles on the periphery represent milk production in the cow if it suffered from different diseases.The chart shows the results from a scoring method that estimated changes in crop yields against a nominal baseline of 10 counters . The project had been promoting production of groundnuts, sweet potatoes, and drought resistant varieties of maize .Figure 4.6 Scoring changes in crop yields against a nominal baseline (source: Burns and Suji, 2007b) Maize Sorghum Ground- nutsSweet potatoMilletMean score16 14 12 10 8 6 4 2 0 A variation of a nominal baseline is shown in Figure 4.7. Here, the PIA was looking at the impact of a veterinary project in South Sudan and trying to understand how the prevention or control of cattle diseases might affect milk production. Before and after proportional piling Before and after proportional piling is very similar to before and after scoring, but typically: The method starts with a large number of counters, usually 100. This means that the results can be converted easily into percentages. The method does not ask informants to physically count out the number of counters for each item, but more to distribute the counters to show a visual pattern that illustrates the relative importance of each item. Therefore, proportional piling is a type of visualization method but where the results are recorded numerically. The method is most useful when informants start with 100 counters to show the 'before' situation, but then can choose the total number of counters for the 'after' situation. An example of before and after proportional piling is shown in Figure 4.8. Pair-wise ranking In pair-wise ranking, items are compared in pairs for importance or preference. Table 4.4 on page 34 shows some results from pair-wise ranking of food sources in an integrated livelihoods project in Niger. The project had several components, including restocking of small ruminants as well as the establishment of cereal banks and vegetable gardens. After the pair-wise comparisons and rankings, people are asked to explain the reasons why they prefer one item over another. Matrix scoring Matrix scoring involves the comparison of different items, project activities or services using a list of indicators. In cases where project and non-project items, impacts, activities or services are compared, the comparison can often be a powerful way of understanding project impacts against pre-existing services or activities. Like other scoring methods, matrix scoring can be standardized and repeated with different individual informants or groups of informants. Also in common with other methods, matrix scoring uses semi-structured interviews to understand the reasoning behind people's scores. Figure 4.8 Before and after proportional piling of cattle deaths due to disease, South Sudan (source: Catley, 1999) Before Now A community-based animal health project had been working in South Sudan for three years. Before and after proportional piling was used to measure changes in cattle deaths due to seven important diseases that were present at the start of the project . The 'before' situation used 100 counters, divided against the seven diseases . To show the situation 'now', after three years, informants were asked to select the number of counters they wanted to use . The method was repeated with six groups of informants . The before and now scores from the six groups were summated to produce the pie charts . Each slice of the pie charts illustrates a different disease. Results were cross-checked against project monitoring reports of cattle treatment GUIDE 33Matrix scoring draws heavily on visual aids, such as line drawings to depict both the items being scored and the indicators. The main steps are: Designing the matrix Identify and illustrate the items to be compared and the indicators. The items might be different types of food, different service providers, different crops, or different types of income-generating activity. Where possible, include in the list of items some items or activities that are not part of, or related to, the project. Pair-wise comparison can be used to identify the indicators as follows: Select two items and ask people which item is more important and why? They will state a preference and give reasons why one item is more important than another . These reasons are indicators, showing differences between items. Repeat the comparison using different pairs of items, until the informants are no longer offering new indicators, but referring back to the indicators they have already identified. You will now have a full list of indicators to use . In the case of service delivery projects and PIA there are five useful indicators to include in the matrix should informants not mention them. These five indicators are: Accessibility The physical distance between the service worker or facility and the intended users of the service; this can be measured using matrix scoring Table 4.4 Pair-wise ranking of food source preferences (source: Burns, 2007) Food source Millet1Vegetables1Purchases Cereal bank MilkNumber of times preferred (overall rank) Millet1 Millet Millet Millet Millet 4 (1st) Vegetables1 Vegetables Vegetables Vegetables 3 (2nd) Purchases Cereal bank Purchases 1 (4th) Cereal bank Cereal bank 2 (3rd) Milk 0 (5th) 1 Own production Reasons given for food source preferences: Millet vs. vegetables We prefer millet, as vegetables require a lot of water, which is hard to come by in this area, making vegetables difficult to grow. Millet vs. purchase Millet is easier to come by, in that we can grow it and it is cheaper as we don't have to pay for it. Millet vs. cereal bank We don't pay for the millet we grow; therefore it's cheaper than the cereal bank millet. Millet vs. milk It's easier to sell millet than milk. Vegetables vs. purchase If we get a good harvest we can earn good income from selling the vegetables. Vegetables vs. cereal banks Vegetables are cheaper. Vegetables vs. milk Vegetables are easier to sell than milk, and are therefore better at generating income for the poor. Cereal bank vs. purchase Cereal banks are cheapest. and cross-checked using participatory mapping . Availability The presence of a worker or facility and their ability to function; this depends heavily on adequate supplies of medicines and equipment for health and veterinary services, or books for education. Similarly, is the worker available to provide a service at the right time of day, or for a sufficient number of days during the year? Supplies of equipment, medicines or books can be cross-checked using direct observation of facilities . Affordability The cost of the service to the intended users; this can be measured using matrix scoring, and cross-checked using official price lists. Acceptance Covers cultural and political factors . Are the service providers accepted and trusted according to local preferences? Acceptance will be measured qualitatively . Quality Relates to the impact of a service . Does a new community-based health worker system result in disease prevention and better health? Do new schools actually produce children who can read and write? Conducting the scoring The pictures that depict the items to be scored are usually placed in a row on the ground and the meaning of each picture is verified with the informant(s). One of the indicator pictures is then selected and its meaning also verified; the picture is placed adjacent to the item pictures. Using a pile of around 30 stones, informants are then asked to score the items against the indicator, using all of the stones. The scores are then checked and questions are asked to reveal the reasons behind the scoring. Select the second indicator and place this below the first, and repeat the scoring with this indicator; again, ask questions to check the scores and show the reasons for the scores. Taking each indicator in turn, repeat the scoring and gradually add more rows to the matrix until all of the indicators have been scored. Ask further questions to clarify, probe and explore the scores, so that the reasons for each set of scores are explained fully. Table 4.5 shows the results from matrix scoring of different foods in the same project in Niger where pair-wise ranking was used (Table 4.4 on page 34). Although milk (from livestock) ranked lowest during pair-wise ranking, when scored using indicators such as 'income potential' and 'nutritional value', it scored higher than other foods and, overall, Table 4.5 Matrix scoring of different food sources against preference indicators (source: Burns, 2007) Indicator Food type Millet Vegetables Purchases Cereal bank Milk Availability (quantity/ volume)15 12 5 13 5 Access (easy to come by) 22 8 3 13 4 Income earning and savings potential12 13 0 8 17 Nutritional value 6 17 6 6 15 Total 55 50 14 40 41 Data derived from a matrix scoring exercise using 50 counters . As all of the indicators were positive attributes of a food, the score for each food were summated . PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 35milk achieved the third highest score. This shows how matrix scoring can be a more useful method for PIA than pair-wise ranking. Figure 4.9 and Table 4.6 shows how matrix scoring was used to assess the introduction of a new community-based animal health worker (CAHW) system into southern Ethiopia. The method was Figure 4.9 A completed matrix scoring of veterinary service providers, southern Ethiopia The different service providers are positioned along the top of the matrix, and are represented using picture cards . The different indicators of service provision are gradually added to the matrix, and scored in turn . The indicators are represented visually using items to hand . Indicator Govern - ment veterinary serviceDrug dealers (black market)Traditional medicineCAHWs Others 'Service is near to us, so our animals are treated quickly' (W=0.69***) 11 (6-15)0 (0-16) (2-6) 4 (3-9) 12 (7-19)0 animals recover we this (W=0.73***) 1 (1-3) 5 (1-17) 4 (2-8) 19 (6-23) 'We good from service (W=0.62***) 7 (1-10) 7 (3-9) 12 (5-15) 4 (2-14) 'This service can treat all our animal health problems' (3-12) 4 (0-9) 20 (5-24) 2 Kendal coefficient of concordance (**p<0.01; ***p<0.001). W values vary from 0 to 1; the higher the value, the higher the level of agreement between informants. The black dots represent the scores (number of small stones) that were used during the matrix scoring . Median values (range) are presented . A higher number of dots indicate a relatively strong association between an indicator and service provider, whereas a low number of dots indicate a weak association.Table 4.6 Matrix scoring of veterinary service providers in southern Ethiopia (source: Admassu et al., 2005) standardized and repeated in ten locations where CAHWs had been used. Impact calendars Impact calendars have been used to measure the duration of impact provided by projects with food security objectives. In the example shown below: Project participants were given 25 counters representing a households' total amount of maize, post-harvest. Using 12 cards to represent each month of the year, participants were asked to distribute the counters along a 12-month calendar to show the monthly household utilization of the harvested maize, up until depletion. The method was done with project participants for the agricultural year before the project started, and again for the agricultural year after the project had started. The method was then repeated with community members who had not participated in the project. Table 4.7 Example of a food security impact calendar Year April May June July Aug Sept Oct Nov Dec Jan Feb Mar 2004-2005: Year before project 2006-2007: Year of project activity/project participants 2006-2007: Non-project participants 25 counters were used for each year; the Table shows results from a one-off use of the method, but it could be standardized and repeated . PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 37Radar diagrams Radar diagrams are another variation of before and after scoring, where results are easily visualized. The method often uses a relatively small scale for scoring, of either 0 to 5, or 0 to 10. In the example in Figure 4.10, participation in five aspects of a primary healthcare project was measured over a five-year period. Figure 4.11 shows important time-saving benefits for water collection reported by women and associated with the construction of a new village dam. Voting Voting has been used in PIA to prioritize indicators at the start of an assessment. For example, in a PIA of a food security project in Zimbabwe, indicators were prioritized by asking participants to vote using a secret ballot. After a discussion about all the potential impact indicators that applied to the project, participants were asked to write down their single most important indicator of project impact. These notes were then collected and tallied, and disaggregated by gender. Clearly, this method would need to be adapted in non-literate communities. It is possible that other voting methods could be applied to impact measurement. In many ways, project impact assessment is no different from a consumer survey or a polling exercise. Simple voting exercises include getting people to stand in lines or groups representing different indicators, or getting them to raise their hands in response to a specific question comparing two variables. These kinds of exercises lend themselves to focus group discussions. However, public voting can be problematic as peer pressure may influence the vote, or the views of minority groups or less powerful individuals in the community may not come through. Nevertheless, there is scope for experimentation with these kinds of exercises, particularly where the objective is to capture a quick vote on a non-sensitive issue. Repeating scoring and ranking to improve reliability If a ranking or scoring method is standardized, field-tested and then repeated with different informants, the reliability of the results increases. This aspect of PIA is shown in Table 4.8 on page 39. Year 5 Year 3 Year 1Leadership Needs assessmentOrganization ManagementResource mobilizationFigure 4.10 Using a radar diagram to show changing participation in a primary healthcare project (source: Rifkin et al., 1988) In this example levels of participation are measured against five components of the project cycle . This was done by asking participants to gauge their own level of participation in each of the activities identified on a scale of 0-5; each activity or aspect of the project was represented by the spokes on the radar diagram. The results show increasing levels of participation over time . This is a good illustration of how a qualitative impact indicator such as participation can be measured . 60 50 40 30 20 10 Before AfterFigure 4.11 Measuring time- saving benefits for water collection from a new dam This radar diagram shows how much time 8 women spent on water collection before and after a dam was constructed by a project in Zimbabwe . The scale is from 0 minutes to 60 minutes, and each spoke of the radar represents the results from 1 woman . Table 4.8 Reliability and repetition Before and after scoring of reliance on food aid following a community-based agriculture project. In this example, the results from 6 repetitions and 10 repetitions of the scoring method can be summarized using the median score; the median is a way of averaging the data . with 6 repetitions the median 'after' score was 4.5 and the range of scores was 4-7. with 10 repetitions the median 'after' score was 5.5 and range of scores was 4-8. The reliability of the results improves with the number of repetitions - but only if the method is tested and standardized .1 Repetition 3 Repetitions 6 Repetitions 10 Repetitions Before After Before After Before After Before After 10 4 10 4 10 4 10 4 10 6 10 6 10 6 10 5 10 5 10 5 10 4 10 4 10 7 10 7 10 4 10 4 10 5 10 7 10 8 10 6 PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 39Who will use the findings of a PIA? Practitioners often find it difficult to know how to select a sample of informants, and how to decide how many informants to involve in a PIA. One approach is to first decide what level of evidence is needed by the organizations and people who will use the findings of a PIA, and design the sampling approach accordingly. Users who are closer to the field and working day-to-day at the community level are often confident that qualitative evaluation is sufficient. These users include field staff working for community-based organizations and local or international NGOs. In these situations, PIA results are intended primarily for local use on a small scale and relatively small samples can be used. The validity of the PIA findings is based on the correct selection and use of participatory methods, cross-checking against monitoring data and the experience of fieldworkers with prolonged engagement in the field. This process can also inform country programs. This type of PIA is often cost-effective, timely and appropriate, and can lead to revised programming that is well-grounded in field realities. Occasionally, however, more systematic approaches are also needed. Moving from field-level to country or regional offices and to the operational or funding policies of donors, governments, United Nations agencies and NGOs, information needs tend to change. More central actors often require results based on a mix of qualitative and quantitative data, and which apply to wider areas. Similarly, technical experts in some sectors have a preference for quantitative analysis and results, statistics and economic analysis. The impact assessment might also be used for a large-scale project or program, covering many districts or even a whole country. In these cases, larger, more representative sampling is often needed. Time and money; questions and methods In addition to thinking about the type of evidence needed according to the end-users, a common reality is that sample sizes are often determined by time and resource limitations, the number and type of questions asked, and the methods needed to answer the questions. For example: A set of PIA questions and methods points to a need to spend two hours with each informant group. The PIA budget and time period indicates that 15 informant group sessions are possible. If the questions and methods are then revised and indicate a need for a three-hour session per informant group, only ten informant group sessions are possible for the same time and money. As the number of assessment locations, individual informants or informant groups increases, so does the amount of travel time between locations and informants. In general, the larger the number of people involved in a PIA, the more time it will take and costs will increase. Similarly, when the scope of a PIA increases and more quantitative approaches are used, more specialist support may be needed e.g. for statistical analysis. This further increases the time and funds that are required. Stage 5: Sampling Table 5.1 Sampling methods for impact assessment Type of samplingDescription Examples of assessments using this approach fully, or in part Random samplingl Associated with quantitative research and evaluation design l Uses the principle that any location or informant has an equal chance of being selected relative to any other location or informant l Generally viewed as the most representative type of sampling and, therefore, the most rigorous; results more likely to be used by central policy makers, technical experts and academics l Allows results from the sample to be extrapolated to a wider project area l Sample size(s) are determined using mathematical formulae which include the level of statistical confidence (error) required and estimates of the amount of change expected in the population in question l Data analysis and summaries usually based on conventional statistical tests l Tends to be less participatory than other approaches l Tends to lead to more expensive and time-consuming assessment design l Randomization can miss key informants i.e. individuals who have particular knowledge about an area or project l Can be used in humanitarian contexts when lists of targeted households are available, and when all selected locations or households are accessiblel Commercial de-stocking, Ethiopia (Abebe et al, 2008) l Primary healthcare, Ethiopia (Catley et al., 2008) l Restocking, Kenya (Lotira, 2004) l Microfinance and value chains Ethiopia (Burns and Bogale, 2011) Purposive l Uses the judgment of community representatives, project staff or the assessors to select representative locations and/or informants l Useful if no sampling frame is available l Results cannot be extrapolated to a wider area, but extrapolation may not be needed l Moderately rigorous if conducted well, and clear criteria for sampling are described and followed l Data can often be analyzed and summarized using conventional statistical tests if needed l Can include a comparison of impacts in areas judged to be 'weak' , 'moderate' or 'strong' in terms of implementation l Can be participatory if community members are involved in selection of assessment sites and informants l Subject to bias, particularly towards more successful project areas or householdsl Gokwe Recovery Action, Zimbabwe (Burns and Suji, 2007b) l Chical Recovery Action, Niger, (Burns and Suji, 2007a) l Pastoralist Survival and Recovery, Niger, (Burns et al, 2008) l Veterinary services, Ethiopia (Admassu et al., 2005 ) l Livestock feed supplementation (Bekele and Abera, 2008) Convenience l Easily-accessible, convenient locations or informants are sampled l The least rigorous sampling option and unlikely to be representative, particularly in larger projects l Commonly used for the evaluation of aid projects l Sometimes the only option, especially during wet season with poor road access, or in insecure areas Various - this type of sampling is commonly used in project evaluations PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 41Selecting the sampling method There are three types of sampling method that we have used for PIA, and these relate to end-users, time and money considerations, and accessibility issues, as outlined in Table 5.1 on page 41. Although random sampling is considered to be the most scientific, and convenience sampling the least, each method has its strengths and weaknesses. For example, convenience sampling may save time, but all the selected villages being easily accessible may not be representative of the greater project area (roadside bias). Alternatively, random sampling may give more truthful results, but it can be costly and time consuming. With purposive sampling there is a risk of being directed to villages where project staff think the project has worked well, but where the results will show an impact that is not representative of other villages in the project area. In addition to these issues, the sampling method partly depends on the type of questions that are being asked in the PIA, a good understanding of the context, and a review of the project design and activities. For example: In a PIA of a primary animal health project in South Sudan, lack of security prevented access to much of the project area. A mix of purposive and convenience sampling was used to select sites for the PIA (Catley, 1999). For a PIA of a drought-response project, a list of participating households became available and so it was decided to use a random sample of these households (Abebe et al., 2008); it was also hoped that the results would influences the development of a national guideline on drought response. For a PIA of community-based animal health workers, it was known that these workers had been trained to prevent or treat specific diseases. Therefore, the PIA design included a comparison of disease impacts from 'diseases handled by CAHWs' versus 'diseases not handled by CAHWs'. Purposive sampling was used because project monitoring indicated similar levels of activity across all project locations and it was a relatively small project (Admassu et al., 2005). For an impact assessment of a primary healthcare project, it was important to compare how women and men were using community health workers and other health service options in project and non-project areas. A random selection of project and non-project locations was used, followed by a random selection of women and men as informants in each location (Catley et al., 2008). Selecting the sample size Random sampling - when a random sampling method is used, sample size is usually calculated using mathematical formulae that take into account the design of the assessment, the expected amount of change or impact, the level of statistical significance needed, and other issues. In general, the higher the amount of change or impact expected, the smaller the sample size needed to detect the change within a given level of statistical confidence. There are now various sample-size calculators available online. These allow you to enter information such as population size and the required level of confidence (usually a 95% confidence level is used), and then a sample size is provided. However, if you are not familiar with sample-size calculations it may be better to seek support from specialists in statistics, social science, epidemiology or other disciplines, and be prepared to explain the context in which you are working, the project, the number of project households or participants, and the levels and types of impact which are expected. Although often perceived as the most scientific approach for estimating sample size, the use of mathematical formulae still involves judgments and, therefore, subjectivity. For example, when using a sample-size formula for a PIA of a restocking project, the assessment team may need to estimate the volume of extra animal milk consumed by children in restocked households, and the number or proportion of children who will consume this milk. In common with purposive sampling then, random sampling also involves judgments. Similarly, the confidence level that is used for calculating sample size usually follows statistical convention and, as indicated earlier, a 95% confidence interval is popular. However, in conventional, quantitative project evaluations there are numerous cases where results are reported that show changes Livestock ownership as a meaningful indicator of change (or not) A large-scale safety net program in northern Kenya aimed to measure the impact of regular cash transfers. As the program covered pastoralist areas, the ownership of livestock was used as an indicator of key assets. The evaluation used a control group and compared livestock ownership in households receiving cash transfers versus households that did not receive cash. A quantitative case-control evaluation design was used, and some results were as follows: Households owning livestock (% of households) (source: adapted from Oxford Policy Management, 2012) Type of livestockHouseholds receiving cash transfers (n=1434)Control households (n=1433)Difference between project and controlsBaseline After cash transfersDifference Baseline After no cash transfersDifference Goats/sheepCamelsCattle58.428.015.863.728.416.75.40.40.977.531.520.773.431.922.3-4.10.31.69.464*0.064ns-0.708ns * Significant at 90% confidence level ns Not significant The results show that: l There was a significantly higher proportion of cash-recipient households owning sheep or goats relative to control households at the time of the evaluation. l There was no difference in the number of households owning camels between cash recipients and control households. l More control households owned cattle than cash recipient households, though the difference was not significant. Despite the limited changes in camel and cattle ownership, the evaluation report concluded that the program was, \". . . having a significant impact on livestock ownership\" . However, the evaluation results had limited meaning. This is because the critical indicator for assets in a pastoralist household is not whether livestock are owned or not, but the number of livestock owned. For example, if the actual number of goats/sheep owned is very low (e.g. 5 animals per household) among cash-recipient households, then: l these animals are unlikely to make a substantial contribution to household food or income. l there is still a considerable asset gap in terms of what people own, and what they need to own to pursue a livelihood that is independent of external support; typically, this requires a minimum herd size of around 35-40 goats or sheep (or more), plus other types of livestock such as camels or cattle. It also seems possible that although sheep and goat ownership may have declined in control households, this was because they were converting these assets to cattle and camels and, overall, becoming wealthier. This example shows how a subset of results with statistical significance has been highlighted in an evaluation report, although these results - and the results overall - have limited livelihoods significance. PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 43that are statistically significant, but have limited livelihoods significance. Non-random sampling - estimating sample sizes for a purposive or convenience sample assumes that there is no intention of extrapolating the results of the PIA to a wider area. There are few fixed rules in these situations and, in our experience, sample size is set according to the PIA design and methods, the size of the project in terms of number of units such as villages, households and people, and practicalities, cost and time. Similarly, in these situations it is more likely that statistical tests will be used that do not depend on data from a representative random sample and, if so, sample size can partly depend on the minimum number of repetitions needed to make these tests meaningful.4 An example of how this mix of factors can determine sample size is shown in the box 'Sample size in a PIA using purposive sampling'. Referring to the example in the box, the process of standardizing and repeating a participatory method enabled meaningful results to be produced. In this example, added value was evident because the method was based on a comparison of project and non-project service providers. Repetition of the method also improved the reliability of the results. Sample size in a PIA using purposive sampling (Admassu et al., 2005) A PIA was designed to assess a CAHW project in a remote area of southern Ethiopia. Matrix scoring was used to compare the accessibility, availability, affordability, acceptance and quality of new CAHWs against existing government clinics, traditional healers, informal drug dealers and others. This approach meant that 'project CAHWs' were compared with 'non-project clinics, traditional healers, drug dealers' and so on. The project had trained 30 CAHWs in different villages and so, working with project staff, ten villages were selected purposively and judged to be representative of the project. Within each village, matrix scoring was repeated with one group of informants and ten sets of matrix scoring data were produced in total. The ten sets of results were summarized and analyzed as follows (see Table 4.6 on page 36): l In each cell of the matrix, scores were summarized as the median (average) score, and minimum and maximum score (also called the range). l The level of agreement between the ten informant groups (one group in each of ten villages) was assessed for each indicator. It was assumed that the scores were more likely to be accurate if there was agreement between the ten groups and because project monitoring data indicated that all CAHWs had received the same training and support. In this example, a sample size of only ten informant groups allowed some level of conventional statistical analysis of the data. 4 For more information, see texts and guidance on statistics and, in particular, how parametric statistics are used for data that is normally distributed (e.g. data that may be derived from randomized, quantitative surveys), whereas non-parametric statistics are used for data that is not normally distributed (e.g. data that can arise from convenience or purposive samples). In any community or area where a project is implemented, changes will take place over time. Some of these changes may have nothing to do with the project and would have happened regardless of whether or not the project ever existed. Other changes occur as a result of the project and these changes can be attributed to the project. The assessment of attribution is an important aspect of PIA. For example, an NGO implements an agricultural recovery project in a food insecure area, affected by periodic drought and conflict. A survey shows improvements in the food security and nutritional status of the participating community, and concludes that the project was a success. Is this a correct assumption, or might other factors such as rainfall, seasonality or security have been more important in influencing food security and nutrition outcomes? The objective of assessing attribution is to isolate and contextualize the impact of the project from non-project factors. In PIA we have used three main approaches for understanding project attribution: 1. Within a project area, assessing the relative importance of project and non-project factors that contributed to changing livelihoods. 2. Within a project area, using participatory methods to compare project and non-project activities or service providers. 3. Comparing changes in project participants with non-project participants. As outlined earlier in the guide, all three of these approaches involves the concept of comparison, but does not use a conventional case-control approach in which a control group is defined at the start of a project and then excluded from project support. Some Stage 6: Assessing project attribution Non-project factors Improved rainfall Improved security Improved government extension services Less inflation Project factors Use of project seeds Use of project tools Use of project fertilizerLivelihoods 'before' Livelihoods 'after'Figure 6.1 Attribution in an agricultural recovery project PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 45of the issues facing randomized case-control studies are summarized in the box 'Issues affecting the use of randomized case-control studies'.6 Attribution by comparison of project and non-project factors This approach aims to understand and prioritize the project and non-project factors which contributed to changes in the impact indicators in a given project (see Figure 6.1 on page 47). These factors can often be identified during the informal interviews that are part of before and after scoring and proportional piling methods (see Stage 4 on page 25). Using ranking and scoring - for example, in a village with an NGO income-generation project, a woman uses proportional piling to show how her income changed during the project. As part of the method she is then asked to explain why her income changed with probing questions to reveal all of the factors that contributed to the change. When the method is repeated with other women, including women who were not involved in the project, a list of 'project' and 'non-project' factors appears. Simple scoring, ranking or proportional piling can then be used to assess the relative importance of these factors. Using the example of the agricultural recovery project in Figure 6.1 on page 45, Table 6.1 on page 47 illustrates how informants might have scored the project and non-project factors. Table 6.2 on page 47 shows how informants described and ranked the factors contributing to improved livestock health in an area with a CAHW project. Using frequency of responses - another way of understanding attribution factors is by asking people to list all the factors that contributed to a particular impact, and record Issues affecting the use of randomized case-control studies5 l How can a control group really be controlled in the real world? Why should control group members refuse to receive assistance from another source? l Can a control group participate openly and honestly? If incentives are given for their participation, is this really a true control group? If they are aware of project assistance to others, how will this affect their behavior? l Is there a potential security risk for NGO staff when excluding a control group from project support? l If a control group is selected from non-project participants in the same community, how can indirect benefits from the project be understood and measured? l Does the increased cost and time of data collection from a control group justify the added value of the study findings and, if so, who decides that this is the case? l Exclusion contradicts humanitarian principles. l The use of a control group is disrespectful of people's time. l Raising expectations of the control group - will the information be reliable? l Using controls could potentially create tensions or fuel conflict between recipient and non-recipient communities. Case-control studies were originally intended to detect relatively small changes between 'treatment groups' and 'control groups' in medical trials. In development and humanitarian aid projects, the aim is often to achieve substantial changes to people's lives - these changes can often be measured and understood without the use of case control studies. 5 The list of issues draws on a set of practical and ethical concerns identified by participants during a workshop in Addis Ababa, Ethiopia, in 2006. The workshop participants were mainly project staff, program managers and country representatives from six international NGOs. 6 Although randomized case-control studies were attracting increasing support from donor governments when this guide was being revised in 2013, it was also evident that the same governments rarely used data from case-control studies when setting their own domestic policies. each response. Every time the same reason is repeated, put a check or a cross next to it. At the end of the process, tally the number of times each factor was mentioned. The assumption here is that the most frequently mentioned factors hold a greater weight or importance than those mentioned less frequently. This method is a convenient way of quickly attributing impact when using a fairly large sample. Also, by not pre-defining the factors that contribute to impact, informants are free to propose any factors they wish. Table 6.1 Measuring attribution using simple scoring Factor Type of factor Score Rank Improved Rainfall Non-project 33 1st Improved Security Non-project 26 2nd Improved Seeds Project 19 3rd Government Extension Services Non-project 12 4th Provision of Fertilizer Project 8 5th Provision of Tools Project 2 6th Table 6.2 Ranking of project and non-project factors associated with improved livestock health (source: adapted from Admassu et al., 2005) Factor Type of factor Median Rank Increase usage of modern veterinary drugs due to attitudinal change of the community for modern veterinary medicineProject 1st Biannual vaccination by Community Animal Health Workers; vaccine supplied by governmentPart-project 2nd Good rain and better availability of pasture (during 2002)Non-project 3rd Reduced herd mobility and herd mixing due to increasing settlement; reduced transmission of some diseasesNon-project 4th N=10 informant groups; there was a high level of agreement between the groups (W=0.75; p<0.001). Table 6.3 Example of an Attribution Tally Form List reasons Frequency (number of responses) Tally 1 Improved seeds 333333333333 12 2 Provision of tools 33 2 3 Provision of fertilizer 3333 4 4 Improved rainfall 333333333333333333333 21 5 Improved security 3333333333333333 16 6 Extension services 333333333 9 PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 47Table 6.4 Reasons given for improvements in household food security (source: Burns and Suji, 2007a) Factors Number of responses (n=74) Cereal banks (available and affordable food supply) 68 Better farm inputs (seeds and fertilizers, and fast maturing millet) 59 More income to purchase food (from cereal bank savings, micro credit and vegetable sales)50 Restocking (income from sales and milk from livestock) 46 Vegetable production (more diverse foods, less dependency on millet) 38 Food aid 10 Decrease in crop infestations and pests 8 Improved rainfall 5 Data was derived using semi-structured interviews following a before and after scoring of food sources . Some people gave more than one response; others gave none. Total number of responses = 284. Table 6.4 shows the results from an impact assessment of a drought-response project in Niger. The five most frequently mentioned factors contributing to improved food security were directly related to the project. Using a participatory method to directly compare project and non-project activity or impact Matrix scoring - a comparative participatory method, such as matrix scoring, can provide a measure of project attribution if an item or service that is only provided or supported by a project is compared with other items or services that have no relationship with the project. An example is shown in Figure 6.2 on page 49, where matrix scoring with 200 women was used to compare community health agents, introduced by a project, with pre-existing service providers. In this example, the new community health agents improved the accessibility, availability, affordability and acceptance of health services, but there were concerns about the quality and health outcomes from the service. Simple scoring - sometimes it is possible to use a participatory method that compares an activity or expected impact that was assigned to a project versus an activity or impact that the project never aimed to achieve. For example, during the design of a CAHW system in Ethiopia it was agreed with communities that new CAHWs could only be trained to prevent or treat a certain range of livestock diseases of local importance. This was because regulations prevented the use of some medicines by CAHWs or, for some diseases, the correct medicine was not available or even known. In this case, it might be expected that CAHW activity should lead to a reduced impact of the diseases they were supposed to cover, but there would be limited or no change in the impact from other diseases. Using this approach, a simple scoring method with a nominal baseline was used to measure the impact of different diseases over time. Proportional piling - using a similar approach to that described above, proportional piling was used during a PIA of an emergency cattle feed supplementation project during drought in Ethiopia. During the project design, herders chose to select specific types of cattle to receive the feed because it was not possible to feed all of the livestock. Therefore, the PIA was able to compare mortality in cattle receiving feed during the drought versus mortality in cattle 12 10 8 6 4 2 0 Community Health Agent Health Clinic Other Traditional Birth AttendantMean score (95% confidence interval)Project Non-project Non-project Non-projectFigure 6.2 Comparison of project and non-project health service providers by women, Somali Region, Ethiopia (n=200) (source: Catley et al., 2009) Accessibility Availability Affordability Acceptance Quality Type of service A matric scoring method was used to compare 4 types of health service provider against 5 indicators of service provision viz. accessibility, availability, affordability, acceptance and quality. The method was standardized and repeated with 200 women, randomly selected, in the project area. The project had introduced Community Health Agents, and these were compared with pre-existing government health clinics, traditional birth attendants, and 'other' providers, such as informal medicine -6 -8 -10Change in median disease scoreHandled by CAHWs Not handled by CAHWsFigure 6.3 Comparing impacts from diseases of camels 'handled' and 'not handled' by community animal health workers in Ethiopia (source: Admassu et al., 2005) Results produced from a before and after scoring method, using a nominal baseline of 10 counters to represent the situation before the project . Informants could reduce the baseline of 10 counters to show reduced disease impacts, increase the 10 counters using up to 10 additional counters to show increased disease impact, or, leave the baseline of 10 unchanged . On the chart, a median score of 0 represents no change in disease impact . The method was repeated with 10 groups of informants . For all 5 diseases handled by CAHWs there was a reduced disease impact. For diseases not handled by CAHWs, impacts varied by disease. PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 49not receiving feed. Some results are shown in Table 6.5 and this example also shows how PIA results can support benefit-cost analysis. Comparing changes in project versus non-project participants This approach to assessing attribution is often problematic in PIA due to issues such as: the need to identify and involve non-recipients of project assistance; the limited incentives for them to spend time providing information; the need to match project and non-project participants in terms of wealth, vulnerability and other criteria; and the additional time and resources needed to gather information from non-participants. There are also issues such as raising expectations of future assistance among non-participants and understanding if and how this might affect the information they provide. In a PIA in Zimbabwe, non-project participants attended focus group discussions which included the use of impact calendars to measure the duration of food security in a given year. The method was repeated with project and non-project participants, as illustrated in Figure 6.4. Table 6.5 Comparing mortality rates in cattle receiving and not receiving project feed during drought (source: adapted from Bekele and Abera, 2008) Location/Group Mortality Difference in mortality Benefit-cost ratio Bulbul area - affected by moderate drought Unfed cattle moved to grazing areas Cows fed using project feed108/425 (25.4%) 13/161 (8.1%)17.3% 1.9:1 Web area - affected by severe drought Unfed cattle moved to grazing areas Cows fed using project feed139/407 (34.2%) 49/231 (21.2%)13.0% 1.6:1 Note - the benefit-cost ratio depended not only on the value of the cattle saved by project feeding, but also the costs of delivering the feed and other costs; these costs varied by location . 14 12 10 8 6 4 2 0Average score Apr May June July Aug Sept Oct Nov Dec Jan Feb Mar The impact calendar used 25 counters to represent a household's total post-harvest cereal balance. The counters were then distributed along a calendar to indicate utilization up until depletion . The data was collected during focus group discussions, and the distribution of the counters was agreed upon by consensus of participants from each group .Figure 6.4 Food security impact calendar (source: Burns and Suji, 2007b) 2004-05 Project participants 2006-07 Project 2006-07 Non-project Triangulation is a crucial aspect of PIA and typically involves: Comparing the results from participatory methods with secondary information, including project monitoring data, and direct observation. Comparing the results from different participatory methods. There is no standard way of triangulating, but it is good practice to cross-check information whenever possible. In the PIA report, agreement or consistency between results can then be described as can areas of disagreement with reasons. Of the various sources of secondary information which might be available, project monitoring reports are particularly important as these should record the level and type of project activity in each area. Other secondary information might include previous studies and reports, and external surveys done by the government, other organizations or research institutes. Example 1 To illustrate how triangulation is used, assume that a primary healthcare project trains and supplies new community health workers. During a PIA, women report improvements in the number of children receiving treatment for diarrhea and pneumonia. In this example: The project monitoring data should be reviewed to check that the project actually provided training and medicines for TimelinesSecondary data Direct observation Mapping Matrix scoringInformal interviews Before and after proportional piling Before and after scoringFigure 7.1 Options for cross-checking information in participatory impact Triangulation PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 51diarrhea and pneumonia in children, and sufficient quantities of relevant medicines were supplied to community health workers (CHWs) in the projects areas. Similarly, if the CHWs were submitting monthly activity reports to the project or government, do these reports reflect levels of activity that support the PIA findings? Different participatory methods can be used and the results can be compared. For example: A proportional piling method could be used to measure proportions of sick children receiving treatment before and after the project, by important disease. The proportional piling could be followed up with questions to understand the different factors that contributed to the changing treatment pattern, and then a ranking method to show the relative importance of the factors. In other words, did CHWs contribute to the change and, if so, to what extent? A matrix scoring of CHWs against other health service providers could show relative accessibility, availability, affordability, acceptance and quality.Example 2 Assume a PIA is used to understand the impacts of a food security project. If you were to do a before and after scoring of food sources, income and expenditure, the results from the first exercise may show an increase in cereal production, the second may show an increase in the proportion of household income from the sale of millet, and the expenditure exercise may show a relative reduction in the amount of household income spent on millet purchases. When the results of the scoring are supported by informants' explanations, the results from all three exercises are consistent with each other. Direct observation can also be used to triangulate data. The photos above from an impact assessment report show before and after views of a project garden site illustrating changes in crop production. This is the final stage of the assessment and involves the presentation of the findings back to the community and local partners such as community-based organizations, local NGOs or local government partners. This stage of a PIA is the final opportunity for the community and project participants to verify that the results are correct and provide further explanations and information about the project. If there is to be a second phase of the project, or if the same project activities are to be implemented in another community, the feedback process can support the design and planning of the new or expanded project. Stage 8: Feedback and validation PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 53References and further reading Abebe, D. (2005). Participatory review and impact assessment of the community-based animal health worker system in pastoral and agropastoral areas of Somali and Oromia Regions, Ethiopia. Save the Children USA, Addis Ababa Abebe, D. and Catley, A. (2013). Participatory impact assessment in drought policy contexts. In: Holland, J. (ed.), Who Counts? The power of participatory statistics. Practical Action Publishing, Rugby Abebe, D., Cullis, A., Catley, A., Aklilu, Y., Mekonnen, G. and Ghebrechirstos, Y. (2008). Impact of a commercial destocking relief intervention in Moyale district, southern Ethiopia. Disasters 32/2, 167-189 ActionAid-Somaliland (1993). Programme review by the Sanaag Community-based ActionAid, London Admassu, B., Nega, S., Haile, T., Abera, B., Hussein, A. and Catley, A. (2005). Impact assessment of a community-based animal health project in Dollo Ado and Dollo Bay districts, southern Ethiopia. Tropical Animal Health and Production 37/1, 33-48 Ashley, C. and Hussein, K. (2000). Developing methodologies for livelihood impact assessment: Experience of the African Wildlife Foundation in East Africa. Sustainable Livelihoods Working Paper 129. Overseas Development Institute, London Bayer, W. and Waters-Bayer, A. (2002). Participatory Monitoring and Evaluation (PM&E) with Pastoralists: A review of Abera, T. (2008). Livelihoods- based Drought Response in Ethiopia: Impact assessment of livestock feed supplementation. Feinstein International Center, Tufts University and Save the Children USA Burns, J. (2006). Mid-term visit to Lutheran World Relief Project, Niger. Feinstein International Center, Tufts University, Medford Burns, J. (2007). Field testing visit to Africare Project, Niger, March 2007. Feinstein International Center, Tufts University, Medford Burns, J. (2009). Measuring the impact of aid at the project level. Addressing challenges - from theory to practice. Paper presented at the World Conference on Humanitarian Studies, Groningen, Netherlands, February Burns, J. and Bogale, S. (2012) Impact assessment of livestock value chain interventions. Final impact assessment of the PSNP Plus project in Raya Azebo. Feinstein International Center, Tufts University, Medford Burns, J. and Catley, A. (2010). Participatory approaches to impact assessment: Experiences from humanitarian interventions in Zimbabwe. In: Ozerdem, A. and Bowd, R. (eds.), Participatory Research Methodologies: Development and post disaster/conflict Ashgate Publishing, Farnham Burns, J.C. and Suji, O.W. (2007a). Impact assessment of the Chical Integrated Recovery Action Project, Niger. Feinstein International Center, Medford Burns, J.C. and Suji, O.W. (2007b). Impact assessment of the Gokwe Integrated Recovery Project, Zimbabwe. Feinstein International Center, Medford https://wikis.uit.tufts.edu/confluence/ display/FIC/Impact+Assessment+of+the+ Gokwe+Integrated+Recovery+Action+Project Burns, J.C., Suji, O.W. and Reynolds, A. (2008). Impact assessment of the Pastoralist Survival and Recovery Project, Dakoro, https://wikis.uit.tufts.edu/confluence/ display/FIC/Impact+Assessment+of+the+ Pastoralist+Survival+and+Recovery+ A. (1999). Monitoring and impact assessment of community-based animal health projects in southern Sudan: Towards participatory approaches and methods. A report for V\u00e9t\u00e9rinaires sans fronti\u00e8res A guide for trainers. African Union/Interafrican Bureau for Animal Resources, Nairobi http://www.participatoryepidemiology.info/ PE%20Guide%20electronic%20copy.pdf Catley, A. and Irungu, P. (2000). Participatory research bovine trypanosomosis in Orma cattle, Tana River District, Kenya: Preliminary findings and identification of best-bet solutions. International Institute for Environment and Development, Kenya Trypanosomiasis Research Institute, Nairobi http://www.participatoryepidemiology.info/ Tana%20River%20research.pdf Catley, A., Bekele, G. and Napier, A. (2008). Impact assessment of the Save the Children USA LEAP Health Program, Afdher and Dolobay Woredas, Somali Region, Ethiopia. Feinstein International Center, Tufts University, Addis Ababa Catley, A., Alders, R.G. and Wood, J.L.N. (2012). Participatory epidemiology: Approaches, methods, experiences. The Veterinary Journal 191, 151-160 Catley, A., Abebe, D., Admassu, B., Bekele, G., Abera, B., Eshete, G., Rufael, T. and Haile, T. (2009). Impact of drought-related livestock vaccination in pastoralist areas of Ethiopia. Disasters 33/4, 665-685 Chambers, R. (2007). Who counts? The quiet revolution of participation and numbers. IDS Working Paper 296. Institute of Development Studies, University of Sussex, Brighton http://www.ids.ac.uk/files/Wp296.pdf Cohen, M. and Gaile, G. (1997). Highlights and recommendations of the First Virtual Meeting of the CGAP Working Group on Impact Assessment Methodologies, April 7-19, 1997. Management Systems International, Washington, DCCromwell, E., Kambewa, P., Mwanza, R. and Chirwa, R., with Kwera Development Centre (2001). Impact assessment using participatory approaches: 'Starter pack' and sustainable agriculture in Malawi. Agricultural Research and Extension Network Paper No. 112. Overseas Development Institute, London Darcy, J. (2005). Acts of faith? Thoughts on the effectiveness of humanitarian action. Prepared for the Social Science Research Council seminar series 'The Transformation of Humanitarian Action', New York, 12 April 2005. Discussion Paper. Humanitarian Policy Group, Overseas Development Institute, London http://www.odi.org.uk/sites/odi.org.uk/files/ odi-assets/publications-opinion-files/4850. pdf Estrella, M. and Gaventa, J. (1998). Who counts reality? Participatory monitoring and evaluation: A literature review. Working Paper 70. Institute for Development Studies, Brighton Ethiopia Participatory Impact Assessment Team (2002). Impact assessment of community-based animal health workers in Ethiopia: Initial experiences with participatory approaches and methods. http://www.participatoryepidemiology.info/ PMIA%20Afar%20&%20Wollo.pdf Feinstein International Center (2006). Tufts University, Participatory Impact Assessment Training Workshop, Aide Memoir, Addis Ababa, September 2006 Fritz Institute (2007). Partners for effective relief http://www.fritzinstitute.org/ prgHumanitarianImpact.htm (accessed on April 20, 2007) Gosling, L. with Edwards, M. (1999). Toolkits - A practical guide to planning, monitoring, evaluation and impact assessment. Save the Children, London Guijt, I. (1998). Participatory monitoring and impact assessment of sustainable agriculture initiatives: An introduction to the key elements. SARL Discussion Paper No. 1. International Institute for Environment and Development, London Guijt, I., Arevalo, M. and Saladores, K. (1998). Participatory monitoring and evaluation: Tracking change together. PLA Notes 31, 28-36 Hofmann, C.A., Roberts, L., Shoham, J. and Harvey, P. (2004). Measuring the impact of humanitarian aid: A review of current practice. Humanitarian Policy Group report 17. Overseas Development Institute, London Holland, J. (ed.) (2013). Who Counts? The power of participatory statistics. Practical Action Publishing, Rugby Hulme, D. (1997). Impact assessment methodologies for microfinance: A review. http://pdf.usaid.gov/pdf_docs/PNACB901.pdf INTRAC (2001). NGOs and impact assessment. NGO Policy Briefing Paper No. 3. International NGO Training and Research Centre, Oxford INTRAC (1999). Evaluating Impact: The search for appropriate methods and instruments. International NGO Training and Research Centre, Oxford Kumar, S. (2002). Methods for Community Participation: A complete guide for practitioners. ITDG Publishing, London OPM and IDS (2012). Kenya Hunger Safety Net Programme: Monitoring and evaluation component. Impact analysis synthesis report. Oxford Policy Management, Oxford Pretty, J., Guijt, I., Thompson, J. and Scoones, I. (1995). Participatory Learning and Action: A trainer's guide. International Institute for Environment and Development, London http://www.iied.org/pubs/display. php?o=6021IIED&n=1&l=1&k= Participatory%20Learning%20and%20 Action%20A%20trainer's%20guide Rifkin, S.B., Muller, F. and Bichmann, W. (1988). Primary healthcare: On measuring participation. Social Science and Medicine 26/9, 931-940 Roberts, L. (2004). Assessing the impact of humanitarian assistance: A review of methods and practices in the health sector. Humanitarian Policy Group Background Paper. Overseas Development Institute, London Roche, C. (1999). Impact Assessment for Development Agencies: Learning to value change. Oxfam GB, Oxford Schroff, R., Mengistu Dessalegn and Hanna Teshome (2011). Understanding the Social Impacts of Urban Gardening in Ethiopia. Feinstein International Center, Tufts University, Medford Shoham, J. (2004). Assessing the impact of humanitarian assistance: A review of methods in the food and nutrition sector. Background Paper for HPG Research Report No 17. Overseas Development Institute, London Simanowitz, A. with Johnson, S. and Gaventa, J. (2000). Making impact assessment more participatory. Ford Foundation Sponsored Development Finance Impact Assessment Planning Workshop. Improving the Impact of Microfinance on Poverty Working Paper no. 2. Imp-Act, Institute of Development Studies, University of Sussex, Brighton Watson, C. (2008). Impact Assessment of Humanitarian Response: A review of the literature. Feinstein International Center, Medford White, S. and Petit, J. (2004). Participatory Methods and the Measurement of Wellbeing. Participatory Learning and Action 50 Young, J., Dijkema, H-P., Stoufer, K., Ojha, N., Goma Shrestha, G. and Thapa, L. (1994). Evaluation of an animal health improvement programme in Nepal. RRA Notes 20 PARTICIPATORY IMPACT ASSESSMENT: A DESIGN GUIDE 57 "}