{"title": "PDF", "author": "PDF", "url": "aclanthology.org/2022.naacl-main.427.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 10-15, 2022 \u00a92022 Association for Computational Linguistics A Holistic Framework for Analyzing the COVID-19 Vaccine Pennsylvania, USA {pachecog,islam32,mmahaja,ashor,mingyin,dgoldwas}@purdue.edu ungar@cis.upenn.edu Abstract The Covid-19 pandemic has led to infodemic of low quality information leading to poor health decisions. Combating the outcomes of this in- fodemic is not only a question of identifying false claims, but also reasoning about the de- cisions individuals make. In this work we pro- pose a holistic analysis framework connecting stance and reason analysis, and fine-grained en- tity level moral sentiment analysis. We study how to model the dependencies between the dif- ferent level of analysis and incorporate human insights into the learning process. Experiments show that our framework provides reliable pre- dictions even in the low-supervision settings. 1 Introduction One of the unfortunate side-effects of the Covid- 19 pandemic is a global infodemic flooding social media with low quality and polarizing informa- tion about the pandemic, influencing public percep- tion on it (Tagliabue et al., 2020). As studies have shown (Montagni et al., 2021), these influences have clear real-world implications, in terms of pub- lic acceptance of treatment options, vaccination and prevention measures. Most computational approaches tackling the Covid-19 infodemic view it a misinformation de- tection problem. In other words, they look at identi- fying false claims and analyzing reactions to them on social media (Hossain et al., 2020; Alam et al., 2021; Weinzierl et al., 2021). This approach, while definitely a necessary component in fighting the infodemic, does not provide policy makers and health-professionals with much needed informa- tion, characterizing the reasons and attitudes that underlie the health and well-being choices individ- uals make. Our goal in this paper is to suggest a holistic analysis framework, providing multiple inter- connected views of the opinions expressed in *Equal contribution Figure 1: Holistic Analysis Framework of Social Me- dia Posts, Connecting entity-level Moral Perspectives, Stance and Arguments Justifying it. text. We specifically focus on a timely topic, attitudes explaining vaccination hesitancy. Fig- ure 1 describes an example of our framework. Our analysis identifies the stance expressed in the post ( anti-vaccination ) and the rea- son for it ( distrust of government ). Given the ideologically polarized climate of social media discussion on this topic, we also aim to characterize the moral attitudes expressed in the text ( oppression ), and how different entities mentioned in it are perceived ( \"Biden, Government\" are oppressing, \"citizens, us\" are oppressed ). When constructing this framework we tackled three key challenges. 1. How should these analysis dimensions be operationalized? While stance prediction is an established NLP task, constructing the space of possible arguments justifying stances on a given topic, and their identification in text, are still open challenges. We take a human-in-the-loop approach to both problems . We begin by defining a seed set of relevant arguments based on data-driven studies (Weinzierl et al., 2021; Sowa et al., 2021), where each reason is defined by a single exemplar sentence. In a sequence of interactions, we use a pre-trained textual inference model to identify paraphrases in a large collection of Covid-19 vac- cination tweets, and present a visualization of the results to humans. Humans then perform an error5821analysis, and either add more sentences to help characterize the existing reason better, or add and characterize additional reasons, based on examples retrieved from the large corpus. We explain this process in detail in Section 2. Our morality analysis is motivated by social sci- ence studies (Pagliaro et al., 2021; D\u00edaz and Cova, 2021; Chan, 2021) that demonstrate the connection between moral foundation preferences (Haidt and Graham, 2007; Graham et al., 2009) and Covid- related health choices. For example, studies show that the endorsement of the fairness andcare moral foundations is correlated with trust in science. To account for fine-grained patterns, we adapt the re- cently proposed morality-frame formalism (Roy et al., 2021) that identifies moral roles associated with moral foundation expressions in text. These roles correspond to actor/target roles (similar to agent/patient) and positive or negative polarity, which should be understood in the context of a spe- cific moral foundation. In Figure 1 \"Biden\" is the negative actor in the context of Oppression, making him the oppressor. We explain this formalism in Section 3. 2. How should the dependencies between these dimensions be captured and utilized? The combination of stance, reason and moral attitudes provides a powerful source of information, allow- ing us to capture the moral attitudes expressed in the context of different stances and their reasons. These connections can also be used to help build expectations about likely attitudes in the context of each stance. As a motivating example, consider the reason \"distrust in government\" , which can be associated with the \"oppression\" moral foun- dation only when its actor is an entity related to government functions (rather than oppression from Covid-19 illness). We model these expectation as a probabilistic inference process (Pacheco and Gold- wasser, 2021), by incorporating consistency con- straints over the judgements made by our model, and predicting the most likely analysis jointly, con- sisting of all analysis dimensions. The full model, described using a declarative modeling language, is provided in Section 5. 3. How can text analysis models be adapted to this highly dynamic domain, without exten- sive and costly manual annotation? While our analysis in this paper focuses on a specific issue, vaccination hesitancy, we believe that our analysis framework should be easily adaptable to new issues.Relying on human insight to characterize and op- erationalize stance and reason identification is one aspect, that characterizes issue-specific considera- tions. Moral Foundation Theory, by its definition, abstracts over specific debate topics, and offers a general account for human morality. However, from a practical perspective, models for predict- ing these highly abstract concepts are trained on data specific to a debate topic and might not gen- eralize well. Instead of retraining the model from scratch, we hypothesize that given an initial model constructed using out-of-domain data, and a small amount of in-domain labeled data, we can obtain acceptable performance by modeling the interac- tion between reasons, stances and moral founda- tions. We study these settings, along with the fully supervised setting in Section 6. The data, code and tools used in this paper are publicly available *. 2 Opinion Analysis To analyze opinions about the Covid-19 vaccine, we model the vaccination stance expressed in each tweet (i.e. pro-vaccine, anti-vaccine, neutral) and the underlying reason behind such stance. For example, in Figure 1 the tweet expresses an anti- vaccine stance, and mentions their distrust of the Biden administration as the reason to take this stance. There are three main challenges involved in this analysis: 1) predicting the stance, 2) construct- ing the space of possible reasons, and 3) mapping tweets to the relevant reasons. Stance prediction is an established NLP classification task (Glandt et al., 2021). However, uncovering latent themes from text automatically remains an open challenge, traditionally approached using noisy unsupervised techniques such as topic models (Zamani et al., 2020b), or by manually identifying and annotating them in text (Hasan and Ng, 2014). Instead, we combine computational and qual- itative techniques to uncover the most frequent reasons cited for pro and anti vaccination stances. We build on previous health informatics studies that characterized the arguments made against the Covid-19 vaccine in social media (Wawrzuta et al., 2021). In this work, researchers come up with a code-book of 12 main themes, frequently used as reasons to refuse or cast doubt on the vaccine. We *https://gitlab.com/mlpacheco/ covid-moral-foundations5822show_reasons() lists the current list of reasons (e.g. Govern- ment Distrust, Natural Immunity.) show_closest_tweets(reason, K) lists the Ktweets closest to a given reason , based on their embedding similar- ity. wordcloud(reason) Renders a word cloud to visualize the arguments associated to a given reason, based on bigram and trigram TF-IDF features. show_assignments(threshold) Renders a bar plot show- ing the assignment of tweets to reasons, based on embedding similarity. An optional threshold can be used to limit assignments. tsne(threshold) Renders a visualization of the reason clus- ters in a 2D map. Threshold is optional. silhouette_score(threshold) Measures the overlap- ping degree between clusters. Threshold is optional. add_reason(reason, phrase) Adds a new reason with a phrase that characterizes it in natural language remove_reason(reason) Removes a given reason add_phrase(reason, phrase) Adds an additional phrase to an existing reason. Table 1: Interactive API Operations propose an interactive, humans-in-the-loop pro- tocol to learn representations for these 12 initial reasons, ground them in data, evaluate their quality, and refine them to better capture the discussion. To do this, we build a tool to explore repeating arguments and their reasons in the Covid-19 vac- cine debate. The tool consists of an interactive Google Colab notebook equipped with a custom API to query current arguments, ground them in data, and visualize them. To initialize the system, we use the 12 reasons suggested by Wawrzuta et al. (2021), and represent them using the one-sentence explanation provided. Our main goal is to ground these reasons in a set of approximately 85,000 un- labeled tweets about the Covid-19 vaccine (details in Section 4). To map tweets to reasons, we use the similarity between their SBERT embeddings (Reimers and Gurevych, 2019). The interaction is centered around the operations outlined in Table 1. Intuitively, the first six operations allow humans to diagnose how reasons map to text, and the last three allows them to act on the result of this diagnosis, by adding and removing reasons, and modifying the phrases characterizing each reason. We follow a simple protocol during interaction, where three human coders use the operations above to explore the initial reasons. The coders start by looking at the global picture: the reasons distribu- tion, the 2D visualizations (van der Maaten and Hinton, 2008) and the silhouette score (Rousseeuw, 1987). Then, they query the reasons one by one, looking at the word cloud (characterizing the dis- tribution of short phrases over all texts assigned to the reason) and the 10 closest tweets to each reason. Following these observations, there is a discussionPRO VAXgovernment distrust, vaccine dangerous, covid fake, vaccine oppression, pharma bad, natural immunity effective, vaccine against religion, vaccine does not work, vaccine not tested, bill gates' micro chip, vaccine tested on dogs, vaccine has fetal tissue, vaccine makes you sterile ANTI VAXgovernment trust, vaccine safe, covid real, vaccine not op- pression, pharma good, natural immunity ineffective, vac- cine not against religion, vaccine works, vaccine tested Table 2: Resulting Reasons phase in which the coders follow a thematic analy- sis approach (Braun and Clarke, 2012) to uncover the overarching themes that are not covered by the current set of reasons, as well as the argumentation patterns that the method fails to identify. Then, they are allowed to add and remove reasons, as well as explanatory phrases for them in natural lan- guage. Every time a reason or phrase is added or removed, all tweets are reassigned to their closest reasons. This process was done over two one-hour sessions. The coders were NLP and Computational Social Science researchers, two female and one male, between the ages of 25 and 40. In the first session, the coders focused on adding new reasons and removing reasons that were not prevalent in the data. For example, they noticed that the initial set of reasons contained mostly anti- vaccine arguments, and added a positive reason for each negative reason (e.g. government distrust government trust ). In addition to this, they broke down the reason \"Conspiracy Theory\" into specific conspiracy theories, such as Bill Gates' micro chip , the vaccine contains fetal tissue , and the vaccine makes you sterile .They also removed infrequent reasons, such as the swine flu vaccine . The final set of reasons can be observed in Table 2. In the second session, the coders focused on identifying the argumentative patterns that were not being captured by the original reason expla- nations, and came up with overarching patterns to create new examples to improve the represen- tation of the reasons. For example, in the case of thegovernment distrust reason, the coders found that phrases with strong words were needed (e.g. F the government ), examples that suggested that the government was \"good at being bad\" (e.g. the government strong record of screwing things up ), and examples with explicit negations (e.g. the gov- ernment does not work logically ). Once patterns were identified, each coder contributed a set of 2 to 5 examples, which were introduced to the reason representation.5823CARE/HARM :Underlies virtues of kindness, gentleness, and nurturance. FAIRNESS /CHEATING :Generates ideas of justice, rights, and autonomy. LOYALTY /BETRAYAL :Underlies virtues of patriotism and self- sacrifice for the group. It is active anytime people feel that it's \"one for all, and all for one.\" AUTHORITY /SUBVERSION :Underlies virtues of leadership and followership, including deference to legitimate authority and re- spect for traditions. PURITY /DEGRADATION :Underlies religious notions of striving to live in an elevated, less carnal, more noble way. It underlies the widespread idea that the body is a temple which can be desecrated by immoral activities and contaminants. LIBERTY /OPPRESSION :The feelings of reactance and resent- ment people feel toward those who dominate them and restrict their liberty. Table 3: Moral Foundations (Haidt and Graham, 2007) In Appendix A.1, we include screenshots of the interactive notebook, and tables enumerating the full list derived patterns and phrases. To visualize the impact of interaction, we also show the overall distribution of reasons before and after interaction, and word clouds for a select set of reasons. The methodology and tool we developed are broadly applicable for diagnosing NLP models. 3 Morality Frame Analysis Moral Foundations Theory (Haidt and Graham, 2007) suggests that there are at least six basic foun- dations that account for the similarities and recur- rent themes in morality across cultures, each with a positive and negative polarity (See Table 3). To analyze moral perspectives in tweets, we build on the definition of morality frames proposed by Roy et al. (2021), where moral foundations are regarded as frame predicates, and associated with positive and negative entity roles. While Roy et al. (2021) defined different roles types for each moral foundation (e.g. entity caus- ing harm ,entity ensuring fairness ), we aggregate them into two general role types: actor andtarget , each with an associated polarity (positive, nega- tive). An actor is a \"do-er\" whose actions or in- fluence results in a positive or negative outcome for the target (the \"do-ee\"). For each moral foun- dation in a given tweet, we identify (positive/negative actor) and tity benefiting/suffering\" (positive/negative target). For example, the statement \"We are suffering from the pandemic\" expresses harm as the moral foun- dation, where \"pandemic\" is a negative actor , and \"we\" is a negative target (i.e. the entity suffer-ing from the actor's actions). There can be zero, one or multiple actors and targets in a given tweet. Entities can correspond to specific individuals or groups (e.g., I, democrats, people of a given de- mographic), organizations (e.g., political parties, CDC, FDA, companies), legislation or other politi- cal actions (e.g., demonstrations, petitions), disease or natural disasters (e.g., Covid, global warming), scientific or technological innovations (e.g., the vaccine, social media, the Internet), among others. We break down the task of predicting moral- ity frames into four classification tasks. For each tweet, our goal is to predict whether it is making moral judgement or not, and identify its prominent moral foundation. For each entity mentioned in the tweet, we predict whether it is a target or a role, and whether it has positive or negative polarity. 4 Data Collection and Annotation There is no existing corpus of arguments about the Covid-19 vaccine annotated for morality frames and vaccination stance, so we collected and an- notated our own. First, we searched for tweets between April and October of 2021 mentioning specific keywords, such as covid vaccine andvac- cine mandate . The full list of keywords, as well as the procedure to obtain them, can be seen in Appendix A.2. Then, we created an exclusive web application for annotating our task. Moral foundation and vac- cination stance labels can be annotated directly. To identify entities, annotators were able to highlight the relevant text spans, and choose its role label (i.e. positive/negative actor or target). We annotated our dataset using three in-house annotators pursuing a Ph.D. in Computer Science. We awarded anno- tators $ 0.75per tweet and bonus ( 2$0.75 = $1 .5) for completing two practice examples. Our work is IRB approved, and we follow their protocols. To ensure quality work, we provided annotators with eight examples covering all six moral foun- dations and non-moral cases. Before starting the annotation task, the annotators had to read the in- structions, go through the examples, and annotate two practice questions. The annotation interface, examples and practice questions can be seen in Appendix A.3. Inter-annotator agreement We calculated the agreement among Krippendorff's (Krippendorff, 2004), where = 1suggests per- fect agreement, and = 0suggests chance-level5824(a) Reasons and Vax Stance (b) Reasons and Moral Foundations (c) Moral Foundations and Vax Stance Figure 2: Correlation Heatmaps agreement. We found = 60.82for moral foun- dations, and = 78.71for stance. For roles, we calculated the character by character agreement be- tween annotations. For example, if one annotator marked \"Dr Fauci\" as a target in a tweet, and an- other marked \"Fauci\", it was considered to be an agreement on the characters \"Fauci\" but disagree- ment on \"Dr\". Doing this, we found = 83.46. When removing characters marked by all three an- notators as \"non-role\", the agreement dropped to = 67.15. Resulting annotated dataset We used a major- ity vote to get moral foundation and vaccination stance labels, and obtained 750 annotated tweets. Similarly, we defined a text span to be an entity mention E, having a moral role R and polarity P, in a tweet T, if it was annotated as such by at least two annotators. Our resulting dataset contains 891 (T,E,R,P) tuples. Statistics can be seen in Table 4. MORAL NUM. VACCINATION STANCE FOUNDATION TW. PRO ANTI NEUT NOAGREE Care/Harm 96 77 17 2 0 Fairness/Cheating 75 33 28 14 0 Loyalty/Betrayal 33 26 2 5 0 Authority/Subversion 114 26 72 13 3 Purity/Degradation 24 2 22 0 0 Liberty/Oppression 93 9 78 6 0 Non-moral 304 188 68 44 4 No Agreement 11 6 5 025 0 TOTAL 750 367 292 84 7 Table 4: Dataset Summary To evaluate the correlation between the different dimensions of analysis, we calculate the Pearson correlation matrices and present them in Figure 2. We can interpret reasons as distributions over moral foundations and stances (and vice-versa). This anal-ysis provides a useful way to explain each of these dimensions. For example, we see that care/harm is strongly correlated with reasons such as covid is real,the vaccine works , and natural immunity is in- effective . Other expected trends emerge, such as pu- rity/degradation being highly correlated with vac- cine against religion . To evaluate the modeling ad- vantage of our opinion analysis framework, we look at the correlation between stance, moral founda- tions and topics extracted in an unsupervised fash- ion using Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We find that the reasons extracted in- teractively have higher correlations with both vac- cination stance and moral foundations. The LDA correlation matrices can be seen in Figure 3. In Table 5 we show the top four reasons for fair- ness/cheating . We choose this moral foundation given that it is evenly split among stances and it is active for different reasons. We show the top two (E,R,P) tuples for each reason. We can appreciate that while this moral foundation is used by peo- ple on both sides, the reasons offered and entities used vary. On the anti-vax side, authority figures and vaccine trials are portrayed as negative actors, while women and children are portrayed as targets. On the pro-vax side, Covid and unvaccinated peo- ple are portrayed as negative actors, and the general public is portrayed as a target. Unlabeled Covid-19 vaccine corpus In addition to our annotated dataset, we collected a corpus of 85,000 tweets in English mentioning the covid vaccine, uniformly distributed between January and October of 2021. These tweets are unlabeled, and are used to ground arguments (Section 2) and to augment data for indirect supervision (Section 5).5825(a) LDA Topics and Vax Stance (b) LDA Topics and Moral Foundations Figure 3: Correlation Heatmaps for LDA Topics VAXNOTOPPRESSION VAXDANGER 70% Pro-Vax actor, neg) (COVID, actor, neg) Table 5: Top 4 reasons for Fairness/Cheating , and their most frequent opinions and entity roles 5 Joint Probabilistic Model We propose a joint probabilistic model that rea- sons about the arguments made, their morality frames, stances, reasons, and the dependencies between them. We implement our model using DRaiL (Pacheco and Goldwasser, 2021), a declara- tive modeling framework for specifying deep rela- tional models. Deep relational models combine the strengths of deep neural networks and statistical relational learning (SRL) to model a joint distri- bution over relational data. This hybrid modeling paradigm allow us to leverage expressive textual encoders, and to introduce contextualizing informa- tion and model different interdependent decisions. SRL methods have proven effective to model do- mains with limited supervision (Johnson and Gold- wasser, 2018; Subramanian et al., 2018), and ap- proaches that combine neural networks and SRL have shown consistent performance improvements (Widmoser et al., 2021; Roy et al., 2021). Following the conventions of statistical rela- tional learning models, we use horn-clauses of the form p0p1...pnhto describe relational properties. Each logical rule defines a probabilistic scoring function over the relations expressed in itsbody and head. Base rules/classifiers We define three base rules to score whether a tweet tihas a moral judgment, what is its prominent moral foundation m, and what is its vaccination stance. r0:Tweet (ti)IsMoral (ti) r1:Tweet (ti)HasMF (ti,m) r2:Tweet (ti)VaxStance moral role of an entity eimentioned in tweet ti, we write two rules. The first one scores whether the entity eiis an actor or a target, and the second one scores its polarity (positive or negative). r3:Mentions (ti,ei)HasRole (ei,r) r4:Mentions (ti,ei)EntPolarity (ei,p) (2) Note that these rules do not express any depen- dencies. They function as base classifiers that map tweets and entities to their most probable labels. Dependency between roles and moral founda- tions The way an entity is portrayed in a tweet can be highly indicative of its moral foundation. For example, people are likely to mention children as anegative actor in the context of care/harm . To capture this, we explicitly model the dependency between an entity, its moral role, and the moral foundation. r5:Mentions (ti,ej)HasRole (ei,r) EntPolarity (ei,p)HasMf (ti,m)(3) Dependency between stances and moral founda- tions As we showed in Section 4, there is a signif- icant correlation between the stance of a tweet with5826respect to the vaccine debate, and its moral founda- tion. For example, people who oppose the vaccine are more likely to express the liberty/oppression moral foundation. To capture this, we model the dependency between the stance of a tweet and its moral foundation. r6:VaxStance (ti,s)HasMf (ti,m)(4) Dependency between reasons and moral foun- dations/stances Explicitly modeling the depen- dency between repeating reasons and other deci- sions can help us add inductive bias into our model, potentially simplifying the task. For example, we can enforce the difference between two opposing views that use similar wording, and that could oth- erwise be treated similarly by a text-based model (e.g. \"natural methods of protection against the disease are better than vaccines\" vs.'vaccines are better than natural methods of protection against the disease\" ). We add two rules to capture this dependency, one between reasons and moral foun- dations, and one between reasons and stances. r7:Mentions (ti,r)HasMf (ti,m) r8:Mentions (ti,r)VaxStance (ti,s)(5) Hard constraints To enforce consistency be- tween different decisions, we add two unweighted rules (or hard constraints). These rules are not as- sociated with a scoring function and must always hold true. We enforce that, if a tweet is predicted to be moral, then it needs to also be associated to a specific moral foundation. Likewise, if a tweet is not moral, then no moral foundation should be assigned to it. c0:IsMoral (ti) \u00acHasMf (ti,none ) c1:\u00acIsMoral (ti)HasMf (ti,none )(6) Whenever different tweets have the same stance, we include a constraint to enforce consistency be- tween the polarity of different mentions of the same entity. Roy et al. (2021) showed that enforcing con- sistency for mentions of the same entity within a political party was beneficial. Given the polar- ization of the Covid-19 vaccine, we use for each rulewr:p0p1...pnhmeasure the importance of each rule in the model and can be learned from data. For example, when attempt- ing to predict care/harm for a tweet ti, we would like the weight of rule instance IsTweet (ti) HasMf (ti,care/harm )to be greater than the weight of rule instance IsTweet (ti) HasMf (ti,loyalty /betrayal ). In DRaiL, these weights are learned using neural networks with parameters r. The collection of rules represents the global decision, and the solution is obtained by running a maximum a posteriori (MAP) infer- ence procedure. Given that horn clauses can be expressed as linear inequalities corresponding to their disjunctive form, the MAP inference problem can be written as a linear program. DRaiL supports both locally and globally normalized structured pre- diction objectives. Throughout this paper, we used the locally normalized objective. For details about the learning procedure, we refer the reader to the original paper (Pacheco and Goldwasser, 2021). Learning with low-supervision To learn DRaiL models in the low-supervision setting, we use an Expectation-Maximization style protocol, outlined in Algorithm 1. First, we initialize the parameters of base rules using distant supervision classifiers. For moral foundations, we use the Johnson and Goldwasser (2018) dataset and the Moral Foun- dation Twitter Corpus (Hoover et al., 2020). For roles, we use the Roy et al. (2021) dataset. For polarity, we combine the Roy et al. (2021) dataset with the MPQA 3.0 entity sentiment dataset (Deng and Wiebe, 2015). For vaccination stances, we annotate our 85K unlabeled tweets using a set of prominent antivax and provax hashtags. Details about these datasets are provided in Appendix A.4. Once the base rules have been initialized using distant supervision, we turn our attention to learn- ing DRaiL models over the Covid-19 dataset pre- sented in Section 4. We alternate between MAP in- ference to obtain training labels (expectation step), and training the neural nets using these labels (max- imization step). We receive an optional parameter kindicating the amount of direct supervision to be used. When kis provided, k%of the annotated labels are seeded during inference. 6 Experimental Evaluation The goal of our framework is to identify morality frames and opinions in tweets by modeling them5827MODEL MORAL /NM MORAL FOUND .ACTOR /TARGET ENT. POLARITY VAXSTANCE Macro Weighted Macro Weighted Macro Weighted Macro Weighted Macro Weighted Random 35.85 - - - - - Scores). NM: Non Moral Algorithm 1 Low Supervision Learning Protocol 1:Random initialization for all r 2:forrbase rules do 3: rdistant 4:end for 5:while not converged do 7: Train all rules locally using Ygold 8:end while jointly. In this section, we perform an exhaustive experimental analysis to evaluate the performance of our model and each of its components. Experimental settings In DRaiL, each rule ris associated with a neural architecture, which serves as a scoring function to obtain the rule weight wr. We use BERT-base-uncased (Devlin et al., 2018) for all classifiers. For the rules that model depen- dencies (Eqs. 3, 4, 5), we concatenate the CLS token with a one-hot vector of the symbols on the left hand side of the rule (i.e. role, sentiment, stance and reason), before passing it through a classifier. For rules that have the entity on the left-hand side (Eqs. 2, 3), we use both the tweet and the entity as an input to BERT, using the SEP token. We trained supervised models using local normaliza- tion in DRaiL, and leveraged distant supervision using protocol outlined in Algorithm 1. In all cases, we used a learning rate of 2e5, a maximum sequence length of 100, and AdamW. In all experi- ments, we perform 5-fold cross-validation over the annotated dataset and report the micro-averaged results. General results Table 6 shows our general re- sults for morality frames and vaccination stance. We evaluate our base classifiers and show the im- pact of modeling dependencies using DRaiL. The joint model results in a significant improvement for morality, moral foundation and vaccination stance.For entities, role and polarity remain stable. We also measure the impact of explicitly modeling rea- sons (Eq. 5) and present results in Table 7. We show the performance for the initial reasons pro- posed by Wawrzuta et al. (2021), which are all from the anti-vaccine perspective, and the impact of our two rounds of interaction, expanding and refining reasons (round 1) and augmenting argu- mentative patterns (round 2). We find that moral foundations improve from 60.07 to 62.27 and vac- cination stance improves from 67.72 to 72.53 after interaction. MODEL MF V AX. STANCE 7: Contribution of reasons at different interaction rounds (Weighted F1) Ablation study We show an ablation study in Table 8. First, we can see how all dependencies contribute to the performance improvement, role- MF being the most impactful. We can also see that explicitly modeling morality constraints im- proves both the morality prediction and the moral foundation prediction, suggesting an advantage to breaking down this decision. We observe that the stance-polarity constraint does not have a signifi- cant impact, but does not hurt performance either, suggesting that our classifiers already capture this information. Lastly, we can see that the perfor- mance for roles and polarity remains stable, poten- tially because these classifiers have a strong starting point. Distant supervision In Figure 4 we evaluate the impact of our indirect supervision protocol by slowly augmenting the amount of direct supervi- sion available. We can see that by leveraging out of domain-data and dependencies, we can obtain a5828MODEL Constraint, SPC: Stance-Polarity Constraint Figure 4: Performance in low-supervision settings competitive model using just 25% of the annotated labels, and we can outperform the fully supervised classifiers using 50% of the annotations. 7 Related Work Recent studies have noted the prevalence of rumors and misinformation in the context of the Covid- 19 pandemic (Loomba et al., 2021; Shahi et al., 2021; Lazarus et al., 2021; Ahmed et al., 2020). Following this trend, several computational ap- proaches have been proposed to detect misinfor- mation related to Covid in news outlets and social media (Weinzierl and Harabagiu, 2021; Bang et al., 2021; Serrano et al., 2020; Al-Rakhami and Al- Amri, 2020). In this paper, we take a different approach and look at the problem of identifying opinions surrounding the Covid-19 vaccine, and ex- plicitly modeling the rationale and moral sentiment that motivates them. Some recent works also look at analyzing ar- guments about Covid and vaccine hesitancy more broadly. In most cases, they either take a traditional classification approach for predicting stances (Al- liheibi et al., 2021; Lyu et al., 2021), or use topic modeling techniques to uncover trends in word us- age (Skeppstedt et al., 2018; Lyu et al., 2021; Sha et al., 2020; Zamani et al., 2020a). In contrast, wepropose a holistic framework that combines differ- ent methodological techniques, including human- in-the-loop mechanisms, classification with distant supervision, and deep relational learning to connect stance prediction, reason analysis and fine-grained entity moral sentiment analysis. 8 Discussion We introduce a holistic framework for analyzing social media posts about the Covid-19 vaccine. We model morality frames and opinions jointly, and show that we can obtain competitive performance. The main limitation of our work is the size of the annotated dataset studied. Annotating for morality is a difficult and costly task, as it requires signifi- cant domain expertise. This motivates the need for methods that perform well under limited supervi- sion, and that can leverage external and unlabeled resources. We took a first step in this direction by combining a wide range of methodological strate- gies. Given the amount of data generated daily about Covid, there are broader opportunities for ex- ploiting these resources than what were explored in this paper. While we provided a preliminary analy- sis of the correlation between stances, reasons and morality, our current work looks at leveraging this framework to analyze opinions at scale. We also presented a first step towards interactive exploration of opinions on social media. While we explored this approach in a limited scenario, there is a lot of potential for using this paradigm for diag- nosing NLP models and adapting to new domains. More research is required to devise protocols and evaluation strategies for this process. 9 Acknowledgements We gratefully acknowledge our in-house annota- tors for their work annotating the dataset presented in this paper. We also thank the anonymous re- viewers of this paper for their insightful comments. This project was partially funded by a Microsoft Research Dissertation Grant, a Purdue Graduate School Summer Research Grant and an NSF CA- REER award IIS-2048001. Downing, L\u00f3pez et al. 2020. Covid-19 and the 5g conspiracy theory: social network analysis of twitter data. Journal of medical internet research , 22(5):e19458.5829Mabrook S Al-Rakhami and Atif M Al-Amri. 2020. Lies facts save: Hassan Sajjad, Kareem Darwish, et al. 2021. Fighting the covid-19 infodemic in social media: A holistic perspective and a call to arms. In Proceedings of the Interna- tional AAAI Conference on Web and Social Media , volume 15, pages 913-922. Fahad M Alliheibi, Abdulfattah Omar, and Nasser Al- Horais. 2021. Opinion mining of saudi responses to covid-19 vaccines on twitter. International Journal of Advanced Computer Science and Applications , 12(6):72-78. Yejin Bang, Pascale Fung. 2021. on covid-19 fake news detection. arXiv preprint arXiv:2101.03841 . David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of machine Learning research , 3(Jan):993-1022. Virginia Braun and Victoria Clarke. 2012. Thematic analysis. , pages 57-71. Eugene Y Chan. 2021. Moral foundations underlying behavioral compliance during the covid-19 pandemic. Personality and individual differences , 171:110463. Lingjia Deng and Janyce Wiebe. 2015. MPQA 3.0: An entity/event-level sentiment corpus. In Proceedings of the 2015 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies , pages 1323-1328, Denver, Colorado. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, The relationship between af- fective dispositions and compliance with official health recommendations during the covid-19 pan- demic. Cognition tweets. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Papers) , pages 1596-1611, Online. Association for Computational Linguistics. Jesse Graham, Jonathan Haidt, and Brian A Nosek. 2009. Liberals and conservatives rely on different sets of moral foundations. Journal of personality and social psychology , 96(5):1029.Jonathan Haidt and Jesse Graham. 2007. When morality opposes justice: Conservatives have moral intuitions that liberals may not recognize. Social Justice Re- search , 20(1):98-116. Kazi Saidul Hasan and Vincent Ng. 2014. Why are you taking this stance? identifying and classifying reasons in ideological debates. In Proceedings of the 2014 Conference on Empirical Methods in Natu- ral Language Processing (EMNLP) , pages 751-762, Doha, Qatar. Association for Computational Linguis- tics. J. Hoover, G. Portillo-Wightman, L. Yeh, S. Haval- dar, A.M. Davani, Y . Lin, B. Kennedy, M. Atari, Z. Kamel, M. Mendlen, G. Moreno, C. Park, T.E. Chang, J. Chin, C. Leong, J.Y . Leung, A. Mirinjian, and M. Dehghani. 2020. Moral twit- ter corpus: A collection of 35k tweets annotated for moral sentiment. Psychological and Personal- ity Science , 11(8):1057-1071. Sean Young, and Sameer Singh. 2020. COVIDLies: Detecting COVID-19 misinformation on social media. In Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020 , Online. Association for Computa- tional Linguistics. Kristen Johnson and Dan Goldwasser. 2018. Classifica- tion of moral foundations in microblog political dis- course. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 720-730, Melbourne, Australia. Association for Computational Linguistics. Klaus Krippendorff. 2004. Measuring the reliability of qualitative text analysis data. Quality and quantity , 38:787-800. Jeffrey V Lazarus, Scott C Ratzan, Adam Palayew, Lawrence O Gostin, Heidi J Larson, Kenneth Rabin, Spencer Kimball, and Ayman El-Mohandes. 2021. A global survey of potential acceptance of a covid-19 vaccine. Nature medicine , 27(2):225-228. Sahil Loomba, Alexandre de Figueiredo, Simon Graaf, and Heidi J Larson. 2021. Measur- ing the impact of covid-19 vaccine misinformation on vaccination intent in the uk and usa. Nature human behaviour , 5(3):337-348. Joanne Chen Lyu, Eileen Le Han, and Luli. 2021. Covid-19 vaccine-related discussion on twit- ter: topic modeling and sentiment analysis. Journal of medical Internet research , 23(6):e24435. Ilaria Tzourio, et al. 2021. Acceptance of a covid-19 vaccine is associated with ability to detect fake news and health literacy. Jour- nal of public health (Oxford, England) .5830Goran Muric, Yusong Wu, and Emilio Ferrara. 2021. COVID-19 Vaccine Hesitancy on Social Media: Building a Public Twitter Dataset of Anti-vaccine Content, Vaccine Misinformation and Conspiracies. Maria Leonor Pacheco and Dan Goldwasser. 2021. Modeling content and context with deep relational learning. Transactions of the Association for Compu- et al. 2021. Trust predicts covid-19 prescribed and discretionary behavioral in- tentions in 23 countries. PloS using Siamese BERT- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP) , pages 3982-3992, Hong Kong, China. Association for Com- putational Linguistics. Peter Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. J. Comput. Appl. Math. 2021. Identifying morality frames in political tweets using relational learning. In Proceedings of the 2021 Conference on Empirical Methods in Natu- ral Language Processing , pages 9939-9958, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Juan Carlos Medina Serrano, Orestis Papakyriakopou- los, and Simon Hegelich. 2020. Nlp-based feature extraction for the detection of covid-19 misinforma- tion videos on youtube. In Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020 . Hao Sha, Mohammad Al Hasan, George Mohler, and P Jeffrey Brantingham. 2020. Dynamic topic mod- eling of the covid-19 twitter narrative among us . Ma- jchrzak. 2021. An exploratory study of covid-19 misinformation on twitter. Online social networks and media , 22:100104. Maria Skeppstedt, Andreas Kerren, and Manfred Stede. 2018. Vaccine hesitancy in discussion forums: computer-assisted argument mining with topic mod- els. In Building Continents of Knowledge in Oceans of Data: The Future of Co-Created 9(8):876. Shivashankar Subramanian, Trevor Cohn, and Timothy Baldwin. 2018. Hierarchical structured model for fine-to-coarse manifesto text analysis. In Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long Papers) , pages 1964-1974, New Orleans, Louisiana. Association for Luca Galassi, medicine 1289. Laurens van Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal Machine arguments against covid-19 vaccines run on facebook in poland: Con- tent analysis of comments. Vaccines , Suellen Hopfer, Misinformation adoption or re- jection in the era of covid-19. In Proceedings of the International AAAI Conference on Web and Social Media , volume 15, pages 787-795. Maxwell A Weinzierl and Sanda M Harabagiu. 2021. Automatic detection of covid-19 vaccine misinforma- tion with graph link prediction. Journal of structured prediction for discourse-level processing. InProceedings of the 16th Conference of the Euro- pean Chapter of the Association for Computational Linguistics: Main Volume , pages 1174-1184, Online. Association Understanding weekly covid-19 concerns through dynamic content-specific lda topic modeling. In Proceedings of the Conference on Empirical Methods in Natural Language Process- ing. Conference on Empirical Methods in Natural Language Processing , volume 2020, page 193. NIH Public Access. Mohammadzaman Understanding weekly COVID-19 concerns through dynamic content- specific LDA topic modeling. In Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science , pages 193-198, Online. Association for Computational Linguistics.5831A Appendix A.1 Reasons and Phrases Tables 10 and 11 show the full list of phrases for anti-vax and pro-vax reasons. The interactive task interface is presented in Figures 5 and 6. Bar plots for reason assignments before and after interaction are shown in Figure 7. (a) Theme: VaccineDanger (b) Theme: VaccineSafe Figure 8: (b) VaccineSafe Figure 9: Wordclouds for reasons after interaction. A.2 Data Collection To create the list of keywords used to collect tweets about the Covid-19 vaccine, we read multiple ar- ticles about Covid mentioning vaccination status, vaccine hesitancy, misinformation, vaccine con- straints, health issues, religious sentiment and other vaccine-related debates, and made a list of re- peating statements. Then, we consulted three re- searchers, two in Computational Social Science and one in Psychology, and constructed a list of relevant keywords that are indicative of morally charged discussions. The full list of keywords can observed in Table 12. covid vaccine, covid vaccination, covid vaccine tyranny, covid vaccine oppression, covid vaccine mandate, covid vaccine conspiracy, covid vaccine anti-vax, covid vaccine religion, covid vaccine satan, covid vaccine god, covid vaccine jesus, covid vaccine islam, covid vaccine muslim, covid vaccine christianity, covid vaccine christian, covid vaccine hindu, covid vaccine jews, covid vaccine catholic, covid vaccine buddhism, covid vaccine religious, covid vaccine biden failure, covid vaccine passport, covid vaccine loyalty, covid vaccine cheating, covid vaccine freedom, covid vaccine betrayal, covid vaccine liberty, covid vaccine black people, covid vaccine propaganda, covid vaccine hesitancy, covid vaccine hesitant, covid vaccine microchip, covid vaccine bill, covid vaccine pregnancy, covid vaccine pregnant, covid vaccine approval, covid vaccine biden, covid vaccine fda, covid vaccine cdc, covid vaccine fauci, Covid-19 china, vaccine passport, vaccination mandate, covid vaccine death, covid vaccine military, experimental covid vaccine, covid vaccine authorization, vaccine oppression, vaccine satan, covid vaccine bill gates, covid vaccine side effect, covid vaccine adverse events Table 12: List of the keywords for data collection.A.3 Data Annotation Task The steps for annotating tweets using our graphical interface are (See Figure 10). 1.Select the moral foundation of the text using the checkbox . You can see the definition of each moral foundation by hovering your mouse over them. If the tweet does not make any moral judgement, check \"none\". In this case, you don't have to highlight actor-target polarity. 2.After selecting any moral foundation other than \"none\", text highlighting for actor-target role with polarity will be visible below. If you select a moral foundation other than \"none\", you can highlight the actor-target polarity. 3.Choose the color-coded label Positive Ac- tor/Positive Target/Negative Actor/Negative Target to highlight the text with the color of the selected label. You can see the definition of actor-target-polarity role by hovering your mouse over them. 4.Highlight words, phrases, or sections of the text for the actor-target role with polarity of the corresponding moral foundation. 5.If you made any mistakes in highlighting, se- lect the \" Unhighlight \" button to unhighlight the previously highlighted text. 6.Finally, click the \" Submit \" button to submit the task. We provided eight examples (Figure 11) cov- ering six moral principles and non-moral cases to make our annotation task more understandable. An- notators could see the explanations for choosing a moral foundation and an. actor-target polarity by clicking the \" See Explanation \" button. Annotators had to complete two practice exam- ples before starting the real task. If they made any mistake, our practice session provided them the correct result with an explanation. Figure 12 shows the interface for one of the two practice examples. A.4 Out-of-Domain Datasets For moral foundation prediction, we use the dataset proposed by Johnson and Goldwasser (2018), con- sisting of 2K tweets by US congress members an- notated for the five core moral foundations. We also use the Moral Foundation Twitter Corpus5832Themes Overarching Patterns GovDistrust Add phrases with strong word for distrust \"Good at being bad\" Explicit negations GovTrust Hedging phrases (sort-of trust) VaxDanger Closer connection between vaccine words and danger words (related to sickness, bad effects) Explit negations Rhetorical questions Refusing the vaccine for medical reasons VaxSafe Explicit mentions of safety Explicit negations CovidFake Stronger relevant negative words (fake, scam, hoax) Explicit negations CovidReal Trust the science References to Covid hospitalization on the rise, explicit mentions of hospitals Explicit negations VaxOppression Legal language Explicit mentions of discrimination and oppression Sarcasm VaxNotOppression Justifying mandates Freedom to be protected Criticizing others using \"you/people\" language, focus freedom on me/my/I BigPharmaAnti Stronger words against pharmaceutical companies (corrupt, evil) Not accountable / irresponsible past behavior Mentions of negative side-effect of other products (cancer) BigPharmaPro Trust science/research and vaccine development process Language about intent, the vaccine was created to do something good, explicit names of companies NaturalImmunityPro The vaccine is not enough Explicit mentions to population immunity, herd immunity and antibodies NaturalImmunityAnti Emphasis on global look, collective entities, society Natural immunity characterized as dangerous or not effective Mentions of experts and trusting science VaxAgainstReligion I put it in god hands (god is deciding) Treating pro-vax as another religion VaxNotAgainstReligion \"Religious\" in quotes Bugus exemptions is your faith\" Call to action: get tested/get vaccinated/put a mask on (mentions of compassion) No religion ask members to refuse vaccine VaxDoesntWork Reference to \"magic Why is corona not going away? Why are vaccinated people dying? VaxWorks \"ask a doctor\", consult with an expert Research on the vaccine is good/has been going on for a long time Capture differences, e.g. \"good trials\" vs. rushed ones. VaxNotTested Language suggesting \"rushed through trials\" and \"experimental vaccine\" VaxTested trust the research and development process Testing can be confused with covid-test, use other language. Table 9: Overarching argumentation patterns uncovered by coders during interaction5833Themes Phrases in the government\" government\", is dishonest\", \"The government always lies\", \"The government has a strong record of screwing things up\", \"The government is good at screwing things up\", \"Do people's health\", \"The tell you the truth about the vaccine\" VaxDanger\"the vaccine will be dangerous to health\" , \"Covid vaccines can cause blood clots\", \"The vaccine is a greater danger to our children's health than COVID itself\", \"The vaccine will kill you\", \"The experimental covid vaccine is a death jab\", \"The covid vaccine causes cancer\", \"The covid vaccine is harmful for pregnant women and kids\", \"The vaccine increases health risk\", \"The vaccine isn't safe\", \"What are vaccines good for? Nothing, rather it increases risk\", \"I and many others have medical exemptions\", \"The vaccine is dangerous for people with medical conditions\", \"I won't take the vaccine due to medical reasons\", \"The vaccine has dangerous side effects\" lie\", \"I don't think that covid is real\", \"I don't buy that covid is real\", \"I don't think there is a pandemic\", \"I don't think the pandemic is real\", \"I don't buy that there is a pandemic\" VaxOppression\"I do not want to be vaccinated because I have freedom of choice\" \"Forcing people to take experimental vaccines is oppression\", \"The vaccine has nothing to do with Covid-19, it's about the vaccine passport and tyranny\", \"The vaccine mandate is unconstitutional\", \"I choose not to take the vaccine\", \"My body my choice\", \"I'm not against the vaccine but I am against the mandate\", \"I have freedom to choose not to take the vaccine\", \"I am free to refuse the vaccine\", \"It is not about covid, it is about control\", \"Medical segregation based on vaccine mandates is discrimination\", \"The vaccine mandate violates my rights\", \"Falsely labeling the injection as a vaccine is illegal\", \"Firing over vaccine mandates is oppression\", \"Vaccine passports are medical tyranny\", \"I won't let the government tell me what I should do with my body\", \"I won't have the government tell me what to do\" BigPharmaAnti\"the vaccine was created only for the profit of pharmaceutical companies\" , \"We are the subjects of massive experiments for the Moderna and Pfizer vaccines\", \"Pharmaceutical companies are is evil\", would you trust big pharma with the COVID vaccine? They haven't been liable for vaccine harm in the past\", \"Covid vaccines are not doing what the pharmaceutical companies promised\", \"Pharmaceutical companies have a history of irresponsible behavior\", \"I don't trust Johnson & Johnson after knowing their baby powder caused cancer for decades\" NatImmunityPro\"natural methods of protection against the disease are better than vaccines\" , \"Herd immunity is broad, protective, and durable\", \"Natural immunity has higher level of protection than the vaccine\", \"Embrace population immunity\", \"I trust my immune system\", \"I have antibodies I do not need the vaccine\", \"Natural immunity is effective\" VaxAgainstReligion\"The vaccine is against my \"The vaccines are the mark of the beast\", \"The vaccine is a tool of Satan\", \"The vaccine is haram\", \"The vaccine is not halal\", \"I will protect my body from a man made vaccine\", \"I put it all in God's hands\", \"God will decide our fate\", \"The vaccine contains bovine, which conflicts with my religion\", \"The vaccine contains aborted fetal tissue which is against my religion\", \"The vaccine contains pork, muslims can't take the vaccine\", \"Jesus will protect me\", \"The vaccine doesn't protect you from getting or spreading Covid, God does\", \"The covid vaccine is another religion\" VaxDoesntWork\"the vaccine does not work\" , \"covid vaccines do not stop the spread\", \"If the vaccine works, why are deaths so high?\", \"Why are vaccinated people dying?\", \"If the vaccine works, why is covid not going away?\" VaxNotTested\"the vaccine is not properly tested, it has been developed too quickly\" , \"Covid-19 vaccines have not been through the same rigorous testing as other vaccines\", \"The Covid vaccine is experimental\", \"The covid vaccine was rushed through trials\", \"The approval of the experimental vaccine was rushed\", \"How was the vaccine developed so quickly?\" VaxExperimentDogs\"Animal shelters are empty because Dr Fauci allowed experimenting of various Covid vaccines/drugs on dogs and other domestic pets\", \"Fauci tortures dogs and puppies\" BillGatesMicroChip\"The covid vaccine is a ploy to microchip people\", \"Bill Gates wants to use vaccines to implant microchips in people\", \"Globalists support a covert mass chip implantation through the covid vaccine\" VaxFetalTissue \"There is aborted fetal tissue in the Covid Vaccines\", \"the Covid vaccines contain aborted fetal cells\" VaxMakeYouSterile \"The covid vaccine will make you sterile\", \"Covid vaccine will affect your fertility\" NoResponsibility no one is responsible for the potential side effects of the vaccine SwineFluVax mentioning the past development of the swine flu vaccine VaxResistance the vaccine has existed before the Covid-19 epidemic, now there is too much resistance ConspiracyTheories conspiracy theories, hidden vaccine effects (e.g., chips) Table 10: AntiVax Themes and phrases for Covid-19 talking points. Themes that were added during interaction are shown in blue. Themes that were removed during interaction are shown in red. The original explanations/examples are presented in bold.5834Themes Phrases GovTrust\"We trust the government\", \"The government cares for people\", \"We are thankful to the government for the vaccine availability\", \"Hats off to the government for tackling the pandemic\", \"It is a good thing to be skeptical of the government, but they are right about the covid vaccine\", \"It is a good thing to be skeptical of the government, but they haven't lied about the covid vaccine\", \"The government can be corrupt, but they are telling the truth about the covid vaccine\", \"The government can be corrupt, but they are not lying about the covid vaccine\" VaxSafe\"The vaccine is safe\", \"Millions have been vaccinated with only mild side effects\", \"Millions have been safely vaccinated against covid\", \"The benefits of the vaccine outweigh its risks\", \"The vaccine has benefits\", \"The vaccine is safe for and kids\", \"The won't make you vaccine vaccine isn't a death jab\", \"The covid doesn't harm \"Scientist know what are doing\", \"Scientist know what they are saying\", \"Covid hospitalizations are on the rise\", \"Covid hospitalizations are climbing as fourth stage surge continues\", \"Covid's death toll has grown faster\", \"Covid is not a hoax\", \"The pandemic is not a lie\", \"The pandemic is not a lie, hospitalizations are on the rise\" VaxNotOppression\"The vaccine mandate is not oppression because vaccines lower hospitalizations and death rates\", \"The vaccine mandate is not oppression because it will help to end this pandemic\", \"The vaccine mandate will help us end the pandemic\", \"We need a vaccine mandate to end this pandemic\", \"I support vaccine mandates\", \"If you don't get the vaccine based on your freedom of choice, don't come crawling to the emergency room when you get COVID\", \"If you refuse a free FDA-approved vaccine for non-medical reasons, then the government shouldn't continue to give you free COVID tests\", \"You are free not to take the vaccine, businesses are also free to deny you entry\", \"You are free not to take the vaccine, businesses are free to protect their customers and employees\", \"If you choose not to take the vaccine, you have to deal with the consequences\", \"If it is your body your choice, then insurance companies should stop paying for your hospitalization costs for COVID\" BigPharmaPro\"I trust the science and pharmaceutical research\", \"Pharmaceutical companies are not hiding anything\", \"The research behind covid vaccines is public\", \"The Pfizer vaccine is saving lives\", \"The Moderna vaccines are helping stop the spread of covid\", \"The Johnson and Johnson vaccine was created to stop covid\", \"Pharmaceutical companies are seeking FDA approval\", \"Pharmaceutical companies are following standard protocols\" NatImmunityAnti\"Only the vaccine will end the pandemic\", \"Vaccines will allow us to defeat covid without death and sickness\", \"The vaccine has better long term protection than to natural immunity\", \"Natural immunity is not effective\", \"Natural immunity would require a lot of people getting sick\", \"Experts recommend the vaccine over natural immunity\" VaxReligionOk\"The vaccine is not against religion, get the vaccine\", \"No religion ask members to refuse the vaccine\", \"Religious exemptions are bogus\", \"When turning in your religious exemption forms for the vaccine, remember ignorance is not a religion\", \"Disregard for others' lives isn't part of your religion\", \"Jesus is trying to protect us from covid by divinely inspiring scientists to create vaccines\" VaxWorks\"The vaccine works\", \"Vaccines do work, ask a doctor or consult with an expert\", \"The covid vaccine helps to stop the spread\", \"Unvaccinated people are dying at a rapid rate from Covid-19\", \"There is a lot of research supporting that vaccines work\", \"The research on the covid vaccine has been going on for a long time\" VaxTested\"Covid vaccine research has been going on for a while\", \"Plenty of research has been done on the covid vaccine\", \"The technologies used to develop the Covid-19 vaccines have been in development for years to prepare for outbreaks of infectious viruses\", \"The testing processes for the vaccines were thorough didn't skip any steps\", \"The vaccine received FDA approval\" ProVax positive attitude Table 11: ProVax reasons and phrases. Reasons that were added during interaction are shown in blue. Reasons that were removed during interaction are shown in red. The original explanatory phrases are presented in bold.5835Figure 5: Interactive task interface. (Hoover et al., 2020), consisting of 35k tweets an- notated for moral foundations. The topics across these two datasets span political issues (e.g. gun control, immigration) and events (e.g. Hurricane Sandy, Baltimore protests). Given that neither of these two datasets contain examples for the liberty/oppression moral foundation, we curate a small lexicon by looking for synonyms and antonyms of the words liberty and oppression . Then, we use this lexicon to annotate the con- gresstweets dataset. We annotate a tweet as lib- erty/oppression if it contains at least four keywords, which results in around 2K tweets. The derived lex- icon for liberty/oppression can be seen in Table 13 To learn to predict roles, we use the subset of Johnson and Goldwasser (2018) dataset annotated for roles by Roy et al. (2021), which contains roughly 3K tweet-entity-role triplets. For polar- ity, we combine the Roy et al. (2021) dataset with the MPQA 3.0 entity sentiment dataset (Deng and Wiebe, 2015), which contains about 1.6K entity- sentiment pairs. https://github.com/alexlitel/congresstweetsliberty, independence, freedom, autonomy, sovereignty self-government, self-rule, self-determination, home-rule civil liberties, civil rights, human rights, autarky, free-rein, latitude, option, choice, volition, democracy, oppression, persecution, abuse, maltreatment, ill treatment, dictator, dictatorship, autocracy, tyranny, despotism, repression, suppression, subjugation, enslavement, exploitation, dependence, constraint, control, totalitarianism Table 13: Liberty/Oppression Lexicon. For stance, we annotate our dataset of 85K unla- beled covid tweets using a set of prominent antivax and provax hashatgs. For the antivax case, we rely on the hashtags proposed by Muric et al. (2021). For the provax case, we manually annotate hash- tags that have a clear provax message, and that are used in at least 50 tweets in our unlabeled dataset. The full set of hashtags used can be found in Tables 14 and 15.5836Figure 6: After querying the themes (i.e., CovidFake, CovidReal), interface shows the wordcloud. (a) Without threshold before (b) Without threshold after (c)threshold 0.3before (d)threshold 0.3after Figure 7: Cluster assignment before and refining arguments interactively.5837Figure 10: Annotation task interface. Figure 11: Examples provided to the annotators.5838Figure 12: One of the two practice examples provided to the annotators before starting the "}