{"title": "YouTube starts mass takedowns of videos promoting \"harmful or ineffective\" cancer cures", "author": "Jon Porter", "url": "https://www.theverge.com/2023/8/15/23832603/youtube-cancer-treatment-misinformation-policy-medical", "hostname": "theverge.com", "description": "It's attempting to streamline its health moderation policies.", "sitename": "The Verge", "date": "2023-08-15", "cleaned_text": "YouTube will remove content that promotes \"cancer treatments proven to be harmful or ineffective\" or which \"discourages viewers from seeking professional medical treatment,\" [the video platform announced today](https://blog.youtube/inside-youtube/a-long-term-vision-for-medical-misinformation-policies/). The enforcement comes as YouTube is attempting to streamline its medical moderation guidelines based on what it's learned while attempting to tackle misinformation around topics like covid-19, vaccines, and reproductive health. Going forward, Google's video platform says it will apply its medical misinformation policies when there is a high public health risk, when there is publicly available guidance from health authorities, and when a topic is prone to misinformation. YouTube hopes that this policy framework will be flexible enough to cover a broad range of medical topics, while finding a balance between minimizing harm and allowing debate. Videos are not allowed to discourage viewers from seeking professional medical treatment In its blog post, YouTube says it would take action both against treatments that are actively harmful as well as those that are unproven and are being suggested in place of established alternatives. A video could not, for example, encourage users to take vitamin C supplements as an alternative to radiation therapy. YouTube's updated policies come a little over three years after it [banded together with some of the world's biggest tech platforms](/2020/3/16/21182726/coronavirus-covid-19-facebook-google-twitter-youtube-joint-effort-misinformation-fraud) to make a shared commitment to fighting covid-19 misinformation. Although the video platform had previously taken action against vaccine misinformation such as [pulling ads from anti-vax conspiracy videos](/2019/2/22/18236839/youtube-demonetization-anti-vaccination-conspiracy-videos-dangerous-harmful-content), it strengthened its approach in light of the pandemic, [removing videos with covid vaccine misinformation](/2020/10/14/21515796/youtube-covid-vaccine-misniformation-policy) in October 2020 and [banning vaccine misinformation from its platform entirely in late 2021](/2021/9/29/22700171/youtube-vaccine-misinformation-antivax-ban). The platform has also taken action against other videos deemed harmful under its medical misinformation policy, including those that provide [\"instructions for unsafe abortion methods\" or promote \"false claims about abortion safety.\"](/2022/7/21/23273200/youtube-alternative-abortion-medical-misinformation-policy-crackdown) While the major tech platforms stood united in early 2020, their exact approaches to covid-19 misinformation have differed since that initial announcement. Most notably, Twitter [stopped enforcing its covid misinformation policy](/2022/11/29/23483836/twitter-covid-misinformation-policy-enforcement-end) in late 2022 following its acquisition by Elon Musk. Meta has also softened its moderation approach recently, [rolling back its covid misinformation rules](https://www.washingtonpost.com/politics/2023/06/16/meta-rolls-back-covid-misinformation-rules/) in countries (like the US) where the disease is no longer considered a national emergency. "}