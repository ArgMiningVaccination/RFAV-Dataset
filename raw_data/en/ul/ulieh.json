{"title": "PDF", "author": "PDF", "url": "https://let.uvt.nl/general/people/bunt/docs/ISA-10_proceedings.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "Proceedings 10th Joint ISO - ACL SIGSEM Workshop on Interoperable Semantic Annotation May 26, 2014 Reykjavik, Iceland Harry Bunt, editor i i isa-10: 10th Joint ACL - ISO Workshop on Interoperable Semantic Annotation Workshop Programme 08.30 - 08:50 Registration 08:50 -- 09:00 Opening by Workshop Chair 09:00 -- 10:30 Session A 09:00 -- 09:30 Hans-Ulrich Krieger, A Detailed Comparison of Seven Approaches for the Annotation of Time-Dependent Factual Knowledge in RDF and OWL and GAF: Linking Linguistic Annotations 10:00 -- 10:15 Johan Bos, Semantic Annotation Issues in Parallel Meaning Banking 10:15 --10:30 Annotation Platform of Textual Entailment 10:30 - 11:00 Coffee break 11:00 -- 13:00 Session B 11:00 -- 11:15 Bolette 11:45 Kiyong Lee, Semantic Annotation Anaphoric Links -- 12:00 Laurette Pretorius and Sonja Bosch, Towards extending the ISOcat Data Category Registry with Zulu Morphosyntax 12:00 -- 13:00 Harry Bunt, Kiyong Lee, Martha Palmer, Rashmi Prasad, James Pustejovsky and Annie Zaenen, ISO Projects on the development of international standards for the annotation of various types of semantic information 13:00 - 14:00 Lunch break C Questions and Finding Answers: Semantic Relation Annotation to Compute the Expected Answer Type 14:30 -- 14:45 Susan Windisch Brown, From Visual Prototypes of Action to Metaphors: Extending the IMAGACT Ontology of Action to Secondary 15:15 -- 16:00 Poster session: elevator pitches followed by poster visits Leon Derczynski and Kalina Bontcheva: Spatio-Temporal Grounding of Claims ii ii Made on the Web in Pheme Mathieu Roche: How to Exploit Paralinguistic Features to Identify Acronyms in Texts Sungho Shin, Hanmin Jung, Inga Hannemann and Mun Yong Yi: Lessons Learned from Manual Evaluation of NER Results by Domain Experts Milan Tofiloski, Fred Popowich and Evan Zhang: Annotating Discourse Zones in Medical Encounters Yu Jie Seah and Francis Bond: Annotation of Pronouns in a Multilingual Corpus of Mandarin Chinese, English and Japanese 16:00 --16:30 Coffee break 16:30 -- Modality: Identifying Triggers Modal Rui Correia, Nuno Mamede, Jorge and Maxine Eskenazi, Using the Crowd to Annotate Metadiscursive Acts 18:00 Workshop Closing iii iii Editor Harry Bunt Tilburg University Workshop Organizers/Organizing Committee Harry Bunt Tilburg University Nancy Ide Vassar College, Poughkeepsie, NY Kiyong Lee Korea University, Seoul James Pustejovsky Brandeis University, Waltham, MA Laurent Romary INRIA/Humboldt Universit\u00e4t Programme Committee Jan Alexandersson DFKI, Saarbr\u00fccken Paul Buitelaar National University of University Hong Kong Anette Frank Universit\u00e4t Heidelberg Robert Gaizauskas University of Sheffield Koiti degli Studi di University of Applied Sciences, Augsburg Inderjeet Mani Yahoo, Sunnyvale Martha Palmer University of Colorado, University INRIA/Humboldt Universit\u00e4t Vrije Universiteit Amsterdam Annie Zaenen Stanford University iv iv Table of contents Hans-Ulrich Krieger A Detailed Comparison of Seven Approaches for the Annotation of Time-Dependent Factual Knowledge in RDF Linking Linguistic Annotations 9 Johan Bos Semantic Annotation Issues in Parallel Meaning Banking 17 Assaf Kiyong Lee Semantic Annotation of Anaphoric Links in Language 29 Laurette Pretorius and Sonja Bosch Towards extending the ISOcat Data Category Registry with Zulu Morphosyntax 39 Volha Petukhova Understanding Questions and Finding Answers: Semantic Relation Annotation to Compute the Expected Answer Type 44 Susan Windisch Brown From Visual Prototypes of Action to Metaphors: Extending the IMAGACT Ontology of Action to Secondary Meanings 53 Ekaterina Lapshinova-Koltunski and Kerstin Anna Kunz Annotating Leon Derczynski and Kalina Bontcheva Spatio-Temporal Grounding of Claims Made on the Web, in Pheme 65 Mathieu Roche How to Exploit Paralinguistic Features to Identify Acronyms in Texts 69 Sungho Shin, Hanmin Jung, Inga Hannemann and Mun Yong Yi Lessons Learned from Manual Evaluation of NER Results by Domain Experts 73 Milan Tofiloski, Fred Popowich and Evan Zhang Annotating Discourse Zones in Medical Encounters 79 v v Yu Jie Seah and Francis Bond Annotation of Pronouns in a Multilingual Corpus of Mandarin Chinese, Hendrickx and Teresa Gon\u00e7alves Automatic Tagging Triggers Modal Rui Correia, Nuno Mamede, Jorge Baptista and Maxina Eskenazi Using the Crowd to Annotate Metadiscursive Acts 102 vi vi Author Index Alexandropoulou, Stavroula 21 Baptista, Jorge 102 Beloki, Zuhaitz 9 Bond, Francis 88 Bontcheva, Kalina 65 Bos, Johan 17 Bosch, Sonja 39 Brown, Susan Windisch 53 Chesney, Sophie 21 Correia, Rui 102 Derczynski, 102 Fokkens, Antske 9 Gangemi, 88 Gon\u00e7alves, Hage, 9 Hannemann, Inga 73 Ockeloen, Niels 9 Olsen, Sussi 25 vii vii Pedersen, Bolette 25 Volha 44 Popowich, Fred 79 Pretorius, 95 Rigau, German 9 Roche, Mathieu 69 Seah, Yu Jie 82 88 Vossen, 21 Yi, Mun Yong 73 Zanzotto, Fabio Massimo 88 Zhang, Evan 79 A Detailed Comparison of Seven Approaches for the Annotation of Time-Dependent Factual Knowledge in RDF and OWL Hans-Ulrich Krieger German Research Center for Saarbr \u00a8ucken, Germany krieger@dfki.de Abstract Representing time-dependent factual knowledge in RDF and OWL has become increasingly important in recent times. Extending OWL relation instances or RDF triples with further temporal arguments is usually realized through new individuals that hide the range arguments of the extended relation. As a result, reasoning and querying with such representations is extremely complex, expensive, and error-prone. In this paper, we discuss several well-known approaches to this problem and present their pros and cons. Three of them are compared in more detail, both on a theoretical and on a practical level. We also present schemata for translating triple-based encodings into general tuples, and vice versa. Concerning query time, our preliminary measurements have shown that a general tuple-based approach can easily outperform triple-based encodings by several orders of magnitude. Keywords: temporal & diachronic relations; binary vs. N-ary schemata for factual state- ments. 1. Introduction Representing temporally-changing information becomes increasingly important for reasoning and query services de- ned on top of RDF and OWL, for practical applications such as business intelligence in particular, and for the Se- mantic Web/Web 2.0 in general. Extending binary OWL ABox relation instances or RDF triples with further tem- poral arguments translates into a massive proliferation of useless \"container\" objects. Reasoning and querying with such representations is extremely complex, expensive, and error-prone. In this paper, we critically discuss several well-known ap- proaches to the encoding of time-dependent information in RDF and OWL. We present seven approaches and ex- plain their pros and cons. Three of them are then com- pared in more detail, both theoretically and practically w.r.t. space consumption and answer time for simple queries. Two of the three approaches stay within the existing RDF paradigm, whereas the third proposal argues for replacing the RDF triple by a more general tuple in order to ease rea- soning and querying, but also to come up with ontologies that have a smaller memory footprint when compared to semantically equivalent triple-based encodings. In order to make the measurements for the three approaches comparable, we have used the rule-based semantic reposi- toryHFC (Krieger, 2013) that we have developed over the last years and which is comparable to popular engines, such as Jena, OWLIM, or Virtuoso. We also present schemata for translating temporal triple-based encodings into general tuples, and vice versa. Concerning query time, our prelim- inary measurements have shown that a general tuple-based approach can easily outperform a triple-based encoding by 1 to 5 orders of magnitude. 2. Synchronic and Diachronic Relations Linguistics and philosophy make a distinction between syn- chronic and diachronic relations in order to characterizestatements whose truth value do (or do not) change over time. Synchronic relations, such as dateOfBirth , are rela- tions whose instances do not change over time, thus there is no direct need to attach a temporal extent to them. Con- sider, e.g., the natural language sentence Tony Blair was born on May 6, 1953. Assuming a RDF-based N-triple representation (Grant and Beckett, 2004), an information extraction (IE) system might yield the following set of there is one unique date of birth, this works per- fectly well and properly capture the intended meaning. Diachronic relationships, however, vary with time, i.e., their truth value do change over time. Representation frameworks such as OWL that are geared towards unary and binary relations can not directly be extended by a fur- ther (temporal) argument. Consider the following sentence: Christopher Gent was Vodafone's chairman un- til July 2003. Later, Chris became the chairman of GlaxoSmithKline with effect from 1st January 2005. Given this, an IE system might discover the following time- RDF graph mixes up the association between the original statements and their temporal extent [????-??-??,2003-07-??]: the second and third association is not supported by the above natural language quotation. 3. Approaches to Diachronic Representation Several well-known techniques of extending binary rela- tions with additional arguments have been proposed in the literature. 3.1. Equip Relation With Temporal Arguments This approach has been pursued in temporal databases (called valid time ) and the logic programming community. For instance, a binary relation, such as worksFor between a person pof type Person and a company cof type Com- pany becomes a quaternary relation with two further tem- poral arguments sande, expressing the temporal interval [s, e]in which the atemporal statement worksFor (p, c)is true represented by stating that s=e): worksFor (p, c)/mapsto worksFor (p, c, s, e ) Unfortunately, OWL and description logic (DL) in general only support unary (classes) and binary (properties) rela- tions in order to guarantee decidability of the usual in- ference problems. Thus forward chaining engines (such as OWLIM and Jena) as well as tableaux-based reasoners (e.g., Racer or Pellet) are unable to handle such descrip- tions. We note here that this approach is clearly the silver bullet of representing binary factual statements, since it is the easi- est and most natural one, although a direct interpretation is incompatible with RDF and almost all currently avail- able reasoners. We will favor this kind of representation in the second part of the paper when presenting the measure- ments, using HFC (Krieger, 2013). 3.2. Apply a Meta-Logical McCarthy & Hayes' situation calculus, James Allen's in- terval logic, and the knowledge representation formal- ism KIF use variants of the meta-logical predicate holds Hence, our worksFor (p, c)relation instance becomes holds (worksFor (p, c), t). McCarthy & Hayes call a state- ment whose truth value changes over time a fluent (Mc- Carthy and Hayes, 1969). The extended quaternary rela- tion from the previous subsection can be seen as a rela- tional fluent, whereas the holds expression here, however, embodies a functional fluent, meaning that worksFor (p, c) is assumed to yield a situation-dependent value. Such kinds of relations are notpossible in OWL, since de- scription logics limit themselves to subsets of function-free rst-order logic and because only a weak form of relation composition is possible in OWL. However, we can reify the atemporal fact worksFor (p, c)in RDF, so that the aboveholds relation instance can at least be encoded by intro- ducing a new individual o, represented as an RDF blank node. We note that in the original calculus, situations were dened at an instant of time, thus we use only a single tem- poral argument there. holds (worksFor (p, c), t)/mapsto o .holds (o, t) type(o,AtemporalFact )subject (o, p) predicate (o,worksFor )object (o, c) As an alternative, we might turn the worksFor relation into a class: holds (worksFor (p, c), t)/mapsto o .holds (o, t) type(o,WorksFor )subject (o, p)object (o, c) However, this would require to always introduce a new class for the representation of each diachronic relation. 3.3. Reify the Original Relation Reifying a relation instance again leads to the introduction of a new object and ve additional new relationships. In addition, a new class needs to be introduced for each rei- ed relation, plus accessors to the original arguments, very similar to the approach directly above. Furthermore, and very important, relation reication loses the original re- lation name, thus requiring a massive modication of the original ontology. Coming back to our worksFor example, we obtain (WorksFor is the newly introduced class) worksFor (p, c, s, e )/mapsto o .type(o,WorksFor ) person (o, p)company (o, c) starts (o, s)ends(o, e) It is worth noting that this encoding can be seen as a kind of \"owlcation\" of Neo-Davidsonian semantics (Parsons, 1990), as the original relation is turned into an event . 3.4. YAGO's Fact Identier The approach YAGO (Hoffart et al., 2011) takes is related to Approach 2 and 3 directly above, as it is a kind of ex- ternal reication. YAGO uses its own extension of the N3 plain triple format, called N4, which associate unique iden- tiers iwith each time-dependent fact. The above quaternary relation instance then is represented as follows: worksFor (p, c, s, e )/mapsto i . i:worksFor (p, c) occursSince (i, s)occursUntil (i, e) Note that the association i:worksFor (p, c)has the disad- vantage of notbeing part of the triple repository (as it is a quadruple technically; we guess that there exists a sepa- rate extendable mapping table). Thus, entailment rules and queries will never have access to these quadruples, unless some custom functionality has been implemented in the se- mantic repository. Nevertheless, this is a valid and proper annotation schema, however notexpressible in OWL. Rather, such a kind of association can be seen as an exten- sion of the idea behind annotation properties in OWL in2that not only classes, properties, and individuals can be an- notated with information, but also binary relation instances (= triples), thus occursSince andoccursUntil from above can be regarded as relation instance annotation properties. Unfortunately, we are not aware of such an extension. 3.5. Wrap Range Arguments Wrapping the range arguments of a relation instance, i.e., grouping them in a new object, allows us to keep the orig- inal relation name, although the approach still requires to rewrite the original ontology: worksFor (p, c, s, e )/mapsto o .worksFor (p, o) type(o,CompanyTime )company (o, c) starts (o, s)ends(o, e) Again, a new object ( o), a new class ( CompanyTime ), and new accessors ( need to be intro- duced. W3C suggests this obvious pattern to be used to encode arbitrary N-ary relations (Hayes and Welty, 2006). Alternatively, instead of dening a new class for each range type of the original relation, one might dene a general class, say RangePlusTime , together with three accessors value ,starts , and ends , in order to avoid a reduplication of the original class hierarchy on the property level. We use the latter renement in our measurements below. 3.6. Encode the 4D View in OWL (Welty and Fikes, 2006) have presented an implementation of the 4D or perdurantist view in OWL, using so-called time slices (Sider, 2001). Relations from the original ontology no longer connect the original entities, but instead connect time slices that belong to those entities. A time slice here is merely a container for storing the time dimension of space- time. At least, the original relation name is kept, although such a representation requires a lot of rewriting and even introduces twonew container objects: worksFor (p, c, s, e s)ends(t, e) starts (t/prime, s)ends(t/prime, e) We note here that thisapproach and the approach below only work for binary relations. This restriction, however, do no harm to RDF-encoded OWL ontologies, since an RDF triple encodes a binary relation. 3.7. Interpret Original Entities as Time Slices In (Krieger et al., 2008), we have slightly extended and at the same time simplied the perdurantist/4D view from di- rectly above. pandcfrom the example above are still rst- class citizens, now called perdurants which possess time slices, explaining the behavior of an entity within a certain temporal extent (e.g., being a Person or aCompany ) and are able to group multiple facts that stay constant within the same period of time. In the extended relation instance, p andcare then replaced by new IDs p/primeandc/prime(similar to the approach above), but these new individuals are still typed to the original classes, here: Person andCompany , resp.Keeping the original typing thus allows us to superimpose the original class hierarchy with the notion of a time slice. worksFor (p, c, s, starts (c/prime, s)ends(c/prime, e) The nice thing with this reinterpretation is that it does not require any rewriting of the TBox and RBox of an ontol- ogy and makes it easy to equip arbitrary upper and domain ontologies with a concept of time, supplied by an indepen- dent time ontology (e.g., OWL-Time) that only needs to talk about instants and/or intervals; see (Krieger, 2010). Perdurants pandcabove only need to be introduced once, independent of which time slice they are linked to. For example, assuming perdurant ppossesses three time slices for a/prime, s, e). Since the starting and ending time coincide in the three statements, p/prime,p/prime/prime, and p/prime/prime/primecan be identied, and the temporal extent needs to be specied only once (and not three times). 4. Theoretical Considerations Within this section, we will consider three of the above seven approaches (Sections 3.1.-3.7.) which we nd to be the most promising ones. On a theoretical level, we will count how many bytes, tuple elements, and triples/tuples overall are needed to represent a diachronic relation in- stance, using approaches 1,5, and 7. During the last years, we have gained some experience with all three formats in several German and European projects. In the European project NIFTi andTrendMiner , we have applied Approach 1 (Krieger and Kruijff, 2011; Krieger and Declerck, 2014). The German TAKE project has used Approach 5 to store biographical knowledge. The ontol- ogy which backs up the LT-World language portal had been rewritten to adhere to Approach 5, as it lacked an explicit treatment of time. In MUSING , we have used Approach 7 to equip the PROTON upper ontology with a notion of time (Leibold et al., 2010). For the MONNET project, we have also chosen Approach 7 to represent the Web content of companies, listed on Deutsche B \u00a8orse's DAX and NYSE's Euronext. In the following, we will restrict ourself to quaternary re- lations pD\u00d7R\u00d7T\u00d7T, where Tis used to describe the starting and ending point of a fluent. The reason for this is that approach 7 (and 6) only works for binary re- lations that are extended by one or two further temporal arguments. Thus a quaternary diachronic relation instance p(d, r, s, e )encodes a truth value for p(d, r)within inter- val[s, e]. We are neutral as to whether temporal intervals are convex (i.e., contain \"holes\") or whether the temporal metric utilizes N,Q, orRforT\u2014this is unimportant for the presentation above and the measurements below. We - nally note that Tcan be easily extended by a further disjoint element, say ?, in order to permit left-open or right-open temporal intervals. Given this, comparison operators over time instants or the Allen relations over intervals, however,3no longer will be Boolean, but instead become three-valued relations. 4.1. Approach 1: Quintuples The quaternary relation instance p(d, r, s, e )is represented as a tuple in HFC by an extension of the plain N-triple for- mat (Grant and Beckett, 2004): d p r s e This tuple consists of 5 elements/arguments and requires (at least) 20 (= 54) bytes, assuming an (internal) int[] representation with 4 byte integers (which is the case in HFC ). Using integer arrays is a common way to represent triples/tuples internally, since the external representation of URIs and XSD atoms needs to be addressed only during input and output. Overall, we obtain 1 object (the inte- ger array) to represent the whole tuple. This last number is very important, since it is desirable to access informa- tion directly in a semantic repository, instead of \"ddling\" around with helper structures (container objects) that blow up the memory. In addition, the overall number of ele- ments is equally important, since triple repositories usually build up large index structures to efciently access all those triples that match a specic element at a certain position in a triple. 4.2. Approach 5: W3C's N-ary Relations As we have indicated in Section 3.5., the triple repre- sentation of the quaternary relation instance results in 5 triples/complex objects: d p o o rdf:type nary:RangePlusTime o nary:value r o nary:starts s o nary:ends e Overall, 5 triples translate into 15 (= 53) elements or 60 (=512) bytes. Furthermore, for each p, we might need an additional class for the type of o, as well as accessors value ,starts , and ends . Since these tuples need to be specied only once, we do not count them here. This approach introduces onebrand-new individual o(a blank node) which turns out to be problematic , since it might lead to anon-terminating closure computation during the appli- cation of entailment rules; not covered here, see (Krieger, 2012). 4.3. Approach 7: Time Slices As described in Section 3.7., perdurants dandrneed only be introduced once, so we do not take them into account. As is the case for approach 5 above, new individuals d' andr'are introduced here; in fact, twofor each fluent we like represent: d' p fourd:hasTimeSlice d' r fourd:hasTimeSlice r' This representation utilizes 9 triples, leading to 27 elements or 108 bytes per fluent in the worst case. We note here that r'only needs to be equipped with a temporal extent and linked to perdurant riffpis an OWL object property, i.e., notmapping to XSD atoms (best case: 5 triples). The below measurements assume the worst case. 4.4. Comparison: When to Apply Which Approach Let us now summarize the pros and cons of the three ap- proaches. Approach 1. This is\u2014for us\u2014the most intuitive ap- proach: ABox relation instances are simply extended by two further temporal arguments. Existing ontologies (TBox and RBox) can be easily equipped with a treatment of time. RDFS/OWL entailment rules as well as custom rules are more intuitive, easier to formulate, and less error-prone when compared to approach 5 and 7. Approach 1 per- forms best in terms of memory consumption and query- ing/reasoning time. Contrary to approach 5 and 7, it does notintroduces new individuals, a precondition for guaran- teeing the termination of the materialization process; see (Krieger, 2012). Approach 5. This approach, recommended by the best practice group of W3C, is able to encode arbitrary n-ary relations (as is trivially the case for approach 1). The en- coding is worth to consider if ontologies are dened from scratch and require time-dependent relations. Contrary to approach 1, approach 5 is compliant with the triple model of RDF. Unfortunately, standard RDFS and OWL reasoning is no longer possible which is also the case for approach 7. This approach introduces a new blank node for each ABox relation instance. Approach 7. This treatment is great if an ontology is al- ready given, but misses a notion of time. The approach does not require to rewrite the TBox and the RBox of an ontol- ogy (contrary to approach 5) and also stays inside RDF. The time slices are possessed by perdurants view is attractive, but is the worst of the three approaches in terms of memory consumption. Two further individuals are introduced here. 5. Practical Measurements In order to compare the three approaches on a practical level, we need a semantic repository that is able to directly encode arbitrary n-ary relations (in our special case: quin- tuples). Popular engines, such as RACER, Pellet, Jena, OWLIM, or Virtuoso which are geared towards binary re- lations/RDF triples can thus notbe applied here. As men- tioned in Section 1., the experiments were performed using HFC , a forward chaining engine and semantic repository that we have developed over the last years and that is used in our lab. 5.1. Initial Numbers The numbers below are computed against the mid-size on- tology that backs up an earlier version of the LT-World4Figure 1: Rewrite schema for obtaining data sets for ap- proaches 1, 5, and 7. size [MB] #tuples RAM [GB] time [s] 1 53 548,132 129 2,740,660 1.67 14.3 7 273 4,360,428 2.15 25.9 Figure 2: Initial numbers for approaches 1, 5, and 7. language portal ( www.lt-world.org ). The measure- ments are obtained Intel Core i7 (2.8 GHz), using Java 1.6 with an initial heap of 4GB. The unex- panded ABox consists of 204,959 RDF triples. Fully materialized, 548,132 triples are obtained. Since tempo- ral information is missing, we randomly attach tempo- ral starting and ending points to ABox relation instances through XSD int atoms which we let vary between 0 and 1,000 using a random generator (implemented by java.lang.Math.random() ). This synthetical data (without the original triples) is used for approach 1. We have then produced two further meaning-preserving data sets by rewriting the quintuples to RDF triples, com- pliant with the formats that are used in approach 5 and 7 (see Figure 1). For approach 5, we have used blank nodes of type Range- PlusTime to group the original value and the starting and ending time of each ABox relation instance. To address ap- proach 7 properly, we have chosen the subject and object URIs of the original triples as names for the perdurants and have attached ascending integers to the original names in order to generate new URIs for the time slices themselves. Given apporach 1, 5, and 7, Figure 2 then describes the three ontologies in terms of space (le size, number of triples/quintuples, main memory requirement) and loading time in order to set up HFC as a repository on which queries are carried out, as described in the next section. Given these \"offline\" numbers, approach 1 seems to be far superior. The next section amplies this judgment through further numbers obtained from \"online\" measurements for relatively easy queries. 5.2. Querying the Ontologies This section presents measurements for six SPARQL-like queries posted in HFC , given approach 1, 5, and 7. Thequeries were originally written for approach 1 (see Figure 3) and were transformed manually to the format required by approach 5 (see Figure 4) and 7. Notranslation is depicted here for approach 7 (this would require a further half page). The rst and second query obtains the starting as well as the starting and ending times over all fluents. Query three selects those objects whose fluents are true intervals (l- ter: start/negationslash=end). The next query searches for subjects in symmetric relation instances that might differ in their starting and ending time. Query ve simply accesses all time-stamped information for a specic individual (here: ltw:obj 68081 ). Finally, query six nds those subjects that have an ending time equal to a specic instant (here: 936). As can be seen in Figure 4, the queries for approach 5 (as is the case for approach 7) are no longer easy to read and take much longer to complete; in some cases this divergency can make a difference between doable and intractable applica- tions which employ such kind of queries. 5.3. Comparison As can be easily recognized from the measurements de- picted in Figure 5, approach 1 easily outperforms approach 5 and 7 by 1 to 5 orders of magnitude . We are not only convinced that querying is faster, intu- itive and less error-prone for approach 1, but have shown in (Krieger, 2012) that the same happens, even drastically for a more complex case, viz., reasoning over a temporal extension of the RDFS and OWL entailment rules (Hayes, 2004; ter Horst, 2005). 6. Summary We hope to have shown that a general tuple-based approach for annotating time-dependent factual knowledge on the Web is far superior to triple-based approaches. We are con- vinced that the time is ripe to move towards this conserva- tive extension of the RDF data model. We note here that even ontologies that utilize approaches 2 to 7 can be easily rewritten to format 1. Due to space requirements, neither are we able to depict and explain any temporal RDFS and OWL entailment rules (Krieger, 2012), nor complex cus- tom rules in the different formats. We are certain that a closer comparison of such rules would even amplify our position, since Semantic Technologies not only are inter- ested in accessing already externalized information (this paper), but also require inferential capabilities to make im- plicit knowledge explicit. The attentive reader of this paper might ask him-/herself how we address instantiations of the above schemata in a different external representation format, such as XML, and how we handle relations with more than two arguments. We will speculate about this in the next two addenda. 7. Addendum 1: XML Representation In order to use harvested data from the Web outside the RDF universe and a specic reasoner (in our case: HFC ), it might be interesting to have an XML exchange represen- tation for the above approaches. Unfortunately, due to the additional degree of freedom in XML to specify a value,5(1) SELECT DISTINCT ?start WHERE \"936\"xsd:int Figure 3: Queries ?start WHERE ?blank Processing time for the three approaches w.r.t. queries 1-6. The numbers in parentheses at the head of the table list how many results are returned by each query. Query 4 for approach 7 runs out of memory (4GB) after 96 seconds. Queries 5 and 6 are performed 100 times to measure total time.6even more kinds of representations are possible here (ex- amples are related to approach 1 and 3, given our running worksFor example): (1) <worksFor person=\"p\" take a liberal stance here as our interest is not in den- ing an \"external\" exchange format, but in deciding which \"internal\" format performs best in terms of (i) memory con- sumption , (ii) running time (querying and reasoning), and (iii)human readability . Nevertheless, we would probably opt for either the \"external\" solution (4) or (5) which are related to the \"internal\" approach (3). 8. Addendum 2: Beyond Binary Relations The approaches above were investigated on how well they perform w.r.t. binary relations whose two arguments can be considered to be obligatory . Such kind of relations are the default case in today's popular knowledge resources, such as YAGO, DBpedia, BabelNet, or Google's Knowl- edge Graph. In case more and especially optional arguments are in- vestigated, our verdict concerning the different approaches will probably turn into a different direction, so the repre- sentation format needs to be updated (in the best case) or changed (in the worst case). Consider the following exam- ple, taken from (Davidson, 1967, p. 83) Jones buttered the toast in the bathroom with a knife at midnight . The binary base relation butter (we assume a direct map- ping of the transitive verb to the relation name here) now needs to be split and/or extended by further optional argu- ments, as the following sentences are perfectly legal: Jones buttered the toast. Jones buttered the toast in the bathroom . Jones buttered the toast with a knife . Jones buttered the toast at midnight . Jones buttered the toast in the bathroom with a knife . Jones buttered the toast with a knife in the bathroom .Jones buttered the toast in the bathroom at midnight . ..... In principle, the number of adjuncts is not bounded, thus adding a large number of potentially underspecied direct relation arguments is probably a bad solution. Today's tech- nologies often address such \"hidden\" arguments through a kind of relation composition , viz., dening further proper- ties such as instrument (to access knife ) orlocation (to access bathroom ) on the object ( toast ) of the relation in- stance: instrumentbutter locationbutter We think that modeling the optional arguments in such a way is unsatisfactory asinstrument orlocation \"operate\" on the object of the binary relation instance and noton the relation instance itself! Our personal solution would model the obligatory argu- ments, including (under- or unspecied) time and perhaps space, as direct arguments of the corresponding relation in- stance or tuple. A further argument, an event identier, also takes part in the relation. Optional arguments, however, would be addressed through binary relations, now working on the event argument. Applying this kind of Davidsonian orevent representation to the above example gives us (in- formal relational notation) e .butter (e,Jones,toast,at midnight ) location (e,bathroom ) instrument (e,knife) 9. Acknowledgements The research described in this paper has been nanced by the European Integrated project TrendMiner under contract number FP7 ICT 287863. The author Thierry Declerck and Bernd Kiefer\u2014thank you guys! Finally, I would like to thank the three reviewers for suggestions and support. 10. References Davidson, Donald. (1967). The logical form of action sentences. In Rescher, Nicholas, editor, The Logic of Decision and Action , pages 81-95. University of Pittsburgh Press. Grant, Jan and Beckett, Dave. (2004). RDF test cases. Technical report, W3C, 10 February. Hayes, Patrick and Welty, Chris. (2006). Dening N-ary relations on the semantic web. Technical report, W3C. Hayes, Patrick. (2004). RDF semantics. Technical report, W3C. Hoffart, Johannes, Suchanek, Fabian M. Berberich, Klaus, Kel- ham, Lewis, de Melo, Gerard, and Weikum, Gerhard. (2011). YAGO2: Exploring and querying world knowledge in time, space, context, and many languages. In Proceedings of the 20th International World Wide Web Conference (WWW 2011) , pages 229-232. Krieger, Hans-Ulrich and Declerck, Thierry. (2014). TMO\u2014the federated ontology of the TrendMiner project. In Proceedings of the 9th edition of the Language Resources and Evaluation Conference (LREC) . Krieger, Hans-Ulrich and Kruijff, Geert-Jan and description logic rule-based reasoning7in situation-aware robots. In Proceedings of the A framework for temporal representation and reason- ing in business intelligence applications. In AAAI 2008 Spring Symposium on AI Meets Business Rules and Process Manage- ment, pages 59-70. AAAI. Krieger, Hans-Ulrich. (2010). A general methodology for equip- with time. In Proceedings LREC 2010 . Krieger, Hans-Ulrich. (2012). A temporal extension of the Hayes/ter Horst entailment rules and an alternative to W3C's n-ary relations. In Proceedings of the 7th International Confer- ence on Formal Ontology in Information Systems pages 323-336. Krieger, Hans-Ulrich. (2013). An efcient implementation of equivalence relations in OWL via rule and query rewriting. In Proceedings of the 7th IEEE International Conference on Se- mantic Computing (ICSC) , pages and reasoning in opera- tional risks. In Kenett, S. and Raanan, Yossi, editors, Op- erational Risk Management: A Practical Approach to Intelli- gent Data Analysis , chapter 3, pages 41-59. Wiley. McCarthy, John and Hayes, Patrick J. (1969). Some philosoph- ical problems from the standpoint of articial intelligence. In Meltzer, B. and Michie, D. editors, Machine Intelligence 4 , pages 463-502. Edinburgh University Press. Parsons, Terence. (1990). Events in the Semantics of English. A Study in Subatomic Semantics . MIT Press, Cambridge, MA. Sider, Theodore. (2001). Four Dimensionalism. An Ontology of Persistence and Time . Oxford University Press. ter Horst, Herman J. (2005). Combining RDF and part of OWL with rules: Semantics, decidability, complexity. In Proceed- ings of the International Semantic Web Conference , pages 668- 684. Welty, Christopher and Fikes, Richard. (2006). A reusable on- tology for fluents in OWL. In Proceedings of 4th FOIS Network Institute, VU University Amsterdam, The Netherlands. IXA NLP Group, University of the Basque Country, Donostia, Spain. Innovation Lab SynerScope B.V and the Semantic Web is increasing. The NLP community makes more and more use of information presented as Linked Data. At the same time, an increasing interest in representing information from text as Linked Data can be observed in the Semantic Web community. It is however not necessarily straightforward to adapt existing NLP modules so that they can read in and produce linguistic annotations in RDF. This paper presents the representations we use in two projects that involve both directions of interaction between NLP and the Semantic Web. In previous work, we have shown how instances represented in RDF can be linked to text and linguistic annotations using GAF. In this paper, we address how we can make further use of Linked Data by using its principles in linguistic annotations. 1. Introduction Research involving computational linguistics and Linked Data is increasing. The Semantic Web community is look- ing into Natural Language Processing (NLP) to include in- formation from text to the Semantic Web. At the same time, more and more NLP applications make use of Linked (Open) Data. These research directions call for representa- tions that facilitate interaction between Resource Descrip- tion Framework (RDF) and linguistic annotations. The idea of using Linked Data in linguistic representations has al- ready been suggested by Ide et al. (2003) for the Linguistic Annotation Framework. Several terminology repositories for NLP have been developed such as the ISO TC37/SC4 Data Category Registry,1or the Ontologies for Linguis- tic Annotation, OLiA (Chiarcos, 2008). It is however not necessarily straightforward to adapt existing linguistic pipelines so that they represent the information they gener- ate in RDF. In this paper, we describe our approach to facilitate com- munication between the linguistic annotations produced by our NLP tools and representations in RDF. We describe our framework developed in previous work which allows us to link RDF statements that describe interpretations of text to linguistic annotations. We then go beyond this basic link between linguistic annotations and semantic interpretation and introduce an approach for representing the linguistic annotations themselves in RDF in such a way that does not require complete revisions of our NLP tools or the develop- ment of complex conversion wrappers. Statements about the world presented as Linked Data are linked to linguistic analyses of text using the Grounded Annotation Framework (Fokkens et al., 2013, GAF). As Fokkens et al. (2013) explain, GAF provides a natural way to represent (cross-document) coreference, possibly grounded in the Semantic Web. Together with possibilities of modeling provenance provided by the PROV-O (Moreau 1http://www.isocat.org/et al., 2012), it indicates the source of information making it particularly suitable to model alternative perspectives. GAF links RDF statements to linguistic annotations repre- sented in any format, as long as they have unique identiers. Representing linguistic annotations in RDF facilitates this and has the additional advantage that we can dene links between linguistic annotations. These links can help us to combine evidence from different modules and hence im- prove our semantic interpretation. We describe our ongoing work on making linguistic annota- tions RDF-based through revisions of the KYOTO Annota- tion Format (Bosma et al., 2009, KAF), while we continue to use a wide range of NLP modules including 3rd party software. The revised version of KAF, the so-called NLP Annotation Format (NAF), can easily be converted to RDF by assigning Internationalized Unique Identiers (IRIs)2to each annotation and by providing a uniform approach to include provenance information and condence scores. The rest of this paper is structured as follows. In Section 2., we provide background information on NewsReader and BiographyNet, the two projects that provided the main re- quirements for our representation. This is followed by an overview of related work in Section 3. Section 4. provides a brief introduction to GAF. This is followed by an expla- nation of advantages and challenges in using RDF for lin- guistic annotations in Section 5. Section 6. describes NAF and is followed by our conclusion in Section 7. 2. Background and Motivation NAF and GAF were developed as part of two interdis- ciplinary projects involving NLP and the Semantic Web: NewsReader3and BiographyNet.4These projects involve both information extraction and the use of Semantic Web technologies for NLP analyses. The requirements set out 2The use of IRIs rather than URIs is introduced in RDF 1.1. IRIs accept a wider range of unicode characters than URIs. 3http://www.newsreader-project.eu 4http://www.biographynet.nl9for NAF and GAF are mainly dened by these two projects. They are described in Section 2.1. We then present the main requirements these projects impose in Section 2.2. 2.1. NewsReader and BiographyNet NewsReader develops technology to process daily news streams in four languages. A range of modules extract what happened to whom ,when andwhere , removing duplica- tion, complementing information, registering inconsisten- keeping track of original sources. Incoming infor- mation is integrated with the past, distinguishing new in- formation from old and storylines are unfolded. Output is stored as RDF triples in a central repository called Knowl- edgeStore (Corcoglioniti et al., 2013) that is also used for reasoning over knowledge. BiographyNet is centered around the Biography Portal of the Netherlands,5a collection of Dutch biographical dic- tionaries. It is an interdisciplinary project where NLP and Semantic Web technologies are used to support historic re- search on biographical data. One of the roles of NLP in this project is to interpret text from the biographies auto- matically and translate it to RDF triples. These projects have several goals in common that influ- ence the requirements of our representation. First, we cre- ate RDF representations of information expressed in natural language in both projects. Second, both projects combine information coming from several sources which partially cover the same topics. Different sources may conrm in- formation, but they can also contradict each other and pro- vide different perspectives on the same topic. We attempt to reveal such differences in perspective in both News- Reader and BiographyNet. Third, NewsReader and Biog- raphyNet both involve several highly challenging tasks that involve multiple NLP components (event detection, cross- document coreference, opinion mining, etc.). Therefore, these projects make use of existing state of the art tools as much as possible. 2.2. Representation requirements When representing different perspectives, it is essential for the representation schema to allow us to keep track of the provenance of all annotations. Provenance information provides insight into where the data came from, what was done with it, what sources and tools were used in the pro- cess of creating annotations for the data and who was re- sponsible for the data, tools and execution of the process. Knowing the source of annotations is particularly impor- tant when dealing with contradictory or conflicting infor- mation. Because information may be used for historic re- search (BiographyNet) or decision makers monitoring the news (NewsReader), users need to have a general indica- tion of the reliability of information. This includes the pa- per, person or publisher that provided information, but also information on the NLP modules that were involved in ex- tracting the information. Provenance information should thus, whenever possible, be accompanied by condence scores .6 5http://www.biografischportaal.nl/en/zoek 6Several of our NLP modules assign condence scores. Only scores directly assigned by our modules are represented.The connection between information in data and the origi- nal source forms an essential part of indicating the prove- nance. We establish this link through GAF. Furthermore, we want to use information represented as Linked Data to support disambiguation. Our representation format should thus be conform to RDF principles as much as possible. The tasks we set out to do within the projects involve both new representations and existing ones. We are exploring several new challenging topics including complex relations between events, (changing) perspectives and storylines. For several of these topics, there are no existing standard rep- resentations. This means that it should be easy to integrate new representations in our format but also that new layers are built on top of previous annotations, resulting in deeper hierarchical representations. The format should thus be simple andflexible to allow for new additions. On the other hand, we have more than one tool available for some of the tasks we are carrying out. We want to investigate if we can improve our results by combining the output of different tools. This means that it must be possible to include alternative analyses on the same object next to each other and we need a method to link similar information through appropriate relations (in case the tools are not based on the same theoretical framework). Finally, we are dealing with a massive amount of data in NewsReader.7The format should thus be as compact as possible, and has to allow for parallel execution of the NLP modules. It should be noted, however, that the formalism should rst and foremost include all required information and be practical. Structure and content will thus not be compromised for the sake of compactness when including essential information or practicality is at stake. 3. Related Work During the last two decades, several proposals have been made for representing linguistic annotations in such a way that they can be processed by a variety of NLP tools. Dif- ferences in theoretical insights and assumptions make stan- dardization challenging. Recent efforts therefore mainly aim for interoperability among formats (Ide and Suderman, 2012). In this section, we will describe several formats that serve this purpose. We then discuss efforts of representing linguistic information in RDF. 3.1. Linguistic Annotations The General Architecture for Text Engineering (Cunning- ham, 2002, GATE) provides an infrastructure for integrat- ing NLP tools. The architecture aims at providing an en- vironment for building robust NLP tools and resources. It supports creating NLP pipelines by providing a basic set of NLP tools that can easily be extended and an environment that makes it relatively easy to integrate new components. Internally, GATE uses a unied format that is based on TIP- STER format (Grishman, 1997), the Atlas format (Bird et al., 2000) and uses Thompson and McKelvie (1997)'s pro- posal for stand-off markup. Information is represented in Annotation Graphs (AGs). Annotations form the labels of 7LexisNexis estimates that each working day around 1 million news Semantic Role Analysis the edges in the graph that go from one node to another. These nodes have pointers to locations in the annotated text. Annotations furthermore consist of an identier, a type and additional feature-value pairs. Because nodes can only point to locations in the text and not to other anno- tations, the annotation does not form a true graph. It is difcult to represent hierarchical annotations (Ide and Sud- erman, 2012) making it less suitable for our purposes. The Unstructured Information Management Architecture (Ferrucci and Lally, 2004, UIMA) provides data representa- tions and interfaces that are platform independent. Its main purpose is to provide interoperability. Information is repre- sented in the Common Analysis Structure (CAS). In CAS, annotations are dened as typed objects. For each type, one supertype and a set of features associated with the type are dened. Types have a is-arelation with their supertype and inherit the supertype's features. Annotations are associated with a \"subject of analysis\" (sofa), which corresponds to the annotated data. In the case of NLP, this is usually the text. Annotations are identied by their start and end posi- tion in the annotated data. Compared to NAF, UIMA seems less flexible. For instance, when running a pipeline that uses multiple modules for se- mantic representations, we postpone the decision on what is likely to be the best interpretation until we have collected as much evidence as possible. This includes the relations between alternative analyses. It is not straightforward to model these relations, which might be fuzzy, in a type hi- erarchy where relations between types and their supertypes areis-arelations and no multiple inheritance is allowed. For instance, Figure 1 illustrates an analysis of the semantic roles of Daimler in the sentence Daimler takes 40%. There is overlap between the role \"steal-10.5#Agent\" and combi- nation of \"Removing#Agent\" and \"Removing#Cause\", but none of these roles is a more general or more specic type than the others. The role \"bring-11.3#Instrument\" contra- dicts the other outputs. The three similar roles are, in this case, closer to the correct interpretation than the contradic- tory role. Formally dened relations between these roles would reveal that the semantic role analyses provide more evidence for the interpretation where Daimler ends up with the 40% than the one where Daimler is the instrument for bringing it. However, the relations between these analyses cannot be expressed by subtyping.8 8This example deals with the representation of a single men- tion in the text, however, other mentions expressing the same statement may add further evidence and/or futher contradictions. This becomes apparent when representing the instances in a se-Bosma et al. (2009) followed the principles dened as part of LAF. The basic idea is that linguistic annotations are stand-off annotations represented in XML. The represen- tation is layered: different linguistic entities have their own layer. Annotations can assign properties to these entities, including links between entities in a different layer. Infor- mation can be added incrementally by introducing new lay- ers. KAF provided hierarchical annotations (not provided by GATE) and the flexibility to provide different and possi- bly conflicting annotations (not provided by UIMA). KAF was used successfully to glue NLP tools together in KYOTO9and subsequent projects such as OpeNER.10It still has some limitations that needed to be addressed. First and foremost, it is not RDF compatible, nor designed in a way that it is easy to convert to RDF. In some layers infor- mation is lumped together in a way that makes it difcult to add provenance and condence scores to individual anno- tations. Finally, information is sometimes repeated several times in the same representation leading to unnecessary in- crease in space. NAF, the sequal of KAF, was designed to address these limitations. The Graph Annotation Format (Ide and Suderman, 2007, GrAF) is a serilizaion of LAF that can represent merged annotations in a single graph. Its interoperability is demon- strated by Ide and Suderman (2012) who show how GrAF representations can be converted to GATE and UIMA and vice versa. The fact that this is possible with other LAF- based formats indicates that it is also likely to be feasible to integrate GATE and UIMA representations in NAF. 3.2. RDF in Linguistic Annotations The idea of using Linked Data and RDF to repre- sent linguistic annotations for achieving interoperability among linguistic resources has been discussed for several years (Chiarcos et al., 2012). Following Linked Data and RDF principles provide a way to address the so-called con- ceptual interoperability among resources, i.e. the ability of heterogeneous NLP resources and tools to talk and under- stand each other. Ide et al. (2003) explicitly mention RDF as a possible for- mat to provide semantic coherence in representations. Fur- thermore, linking annotation categories to URIs belonging to a shared terminology is a fundamental part of LAF. ISO- cat is completely compatible with RDF (Kemps-Snijders et al., 2008). The NLP2RDF initiative collects a number of efforts for representing NLP related information in RDF, including notable efforts such as OLiA (Ontologies for Lin- guistic Annotation (Chiarcos, 2008)). Still, to there are relatively few implemen- tations of RDF-compatible annotation formats that are ac- tively used or produced by NLP modules. Notable ex- ceptions are the NLP Interchange Format (Hellmann et al., 2013, NIF), which is tightly linked to OLiA, UIMA mantic layer that collects evidence form all mentions. It is at this level, where we will ultimately have to resolve conflicting infor- mation from mentions. The mentions in the text layer often remain undecisive about these interpretations. 9http://www.kyoto-project.eu 10http://www.opener-project.org11Clerezza,11and Cassidy (2010)'s conversion of GrAF to RDF. Hellmann et al. (2013) provide an elaborate description of NIF and a user evaluation. NIF uses RDF to represent linguistic annotations. Annotations are related to strings which are dened by their start and end offsets in the text. These representations are simple andcompact and it is easy to represent information from different tools. It is straightforward to include information on provenance us- ing PROV-O (Moreau et al., 2012) and condence. Many of these advantages are the result of NIF's RDF compati- bility. We will elaborate on the advantages of using RDF in linguistic representations in Section 5. NIF has the disadvantage that it is not easy to integrate its representations in NLP tools, as shown by Hellmann et al. (2013)'s user evaluation. Because linguistic annotations are linked to strings it is furthermore not practical for represent- ing hierarchical structures. NIF Stanbol12addresses this problem by assigning an identier to annotations, but this variation of NIF is still in its initial stages of development and is not ready to be used in a complex NLP architecture. UIMA Clerezza provides a basic mapping mechanism to convert CAS to RDF. We are not aware of a publication that provides in-depth information on this mapping or on how these representations are used. It is therefore not clear whether representations in CAS can easily be represented in RDF or whether such representations are practical to use. It seems, nevertheless, that UIMA together with UIMA Clerezza offers a functionality similar to NAF. Apart from the restrictions of CAS outlined above, it would however been signicantly more time consuming to adapt our cur- rent NLP modules to UIMA than revising KAF. Further- more as we pointed out, we still need to deal with conflict- ing annotations. Cassidy (2010) describes the process of converting GrAF to a representation in RDF.13His motivation is similar to ours. He addresses the advantage of using URIs for linguistic an- notations which are dened in ontologies. The implemen- tation is a direct mapping from GrAF's XML representa- tion to XML. Cassidy (2010) shows that GrAF can be con- verted to RDF, but also points out that a data model that de- nes information captured by the GrAF's XML schema in a format-neutral way would be preferable, but this had not been developed at the time. To our knowledge, this has not changed. Cassidy (2010)'s work is similar to the work pre- sented in our paper, because he also converts a LAF-based format to RDF. However, he does not address what the re- sulting data model should look like and how this relates to the original GrAF representation. NAF is a revision of KAF that addresses several of KAF's limitations by improving its compatibility with RDF. This step is in line with the vision of LAF presented by Ide et al. (2003), who already suggest RDF as a XML compatible format that can be used for semantic coherence. We adapt stimulate interop- erability between tools that work with NIF representations. However, we avoid the challenges related to integrating NIF in our own tools or building NIF wrappers, since our rep- resentation maintains a large part of the XML schema that was used in KAF. We thus continue to use a LAF-based for- mat, but have structured it in a way that it can be converted to RDF by simple generic rules resulting in a data model that is particularly suitable for representing provenance and condence scores. The following section describes GAF, a framework that can link annotations in any of the formats described above to instances in RDF. 4. Linking Linguistic Annotations to the Semantic Web In this section, we provide a brief introduction to the Grounded Annotation Framework (GAF). A more elabo- rate description and motivation can be found in Fokkens et al. (2013). As mentioned in Section 2.1., we aim to extract what hap- pened to whom, when and where. The information we seek is thus centered around events. We use the Simple Event Model (Hage et al., 2011, SEM) to represent this informa- tion at the instance level as opposed to the mention level in text. There are several RDF schemas and OWL ontologies for representing events, but SEM is among the most flexi- ble. In particular, it can contain contradictory information as required by our goal to model different perspectives. Events are formally represented as instances in a seman- tic layer, just like the participants, locations and times re- lated to the events. GAF introduces the gaf:denotes andgaf:denotedBy relations. This allows us to link the instances represented in SEM to mentions of these in- stances in text. This approach has several advantages over other approaches to model events in NLP. First, the approach provides a natural way to model coref- erence. A set of mentions in text that corefer all denote the same instance. This avoids the (arbitrary) selection of one specic mention as the \"anchor\", \"trigger\" or \"main referent\" to which other partic- relevant for modelling cross-document coreference in NewsReader and BiographyNet where many different sources from different times may refer to the same event making it even more challenging to identify which mention should function as the \"anchor\". Second, not all information on events comes from text. Videos, pictures, sensors, or data registration containing mobile phone data may also provide information on events. Because GAF can link SEM representations to any kind of mention, it provides a natural way to integrate information from various kinds of sources. Third, the instance layer can combine information from many different mentions in a unied repersentation, resolv- ing possible conflicts and complementing information that is lacking in individual representations of mentions. As such, it provides the possibility to override interpretations of individual mentions that lack the evidence for the correct interpretation. It therefore enables us to be more robust and12underspecied when representing semantic information for mentions in NAF. Fourth, GAF can link an instance or RDF statement to any mention that has a unique identier. We can thus link the statement that a specic person is an agent in an event to a semantic role or syntactic relation or combine information from different event models proposed by the NLP commu- nity. In summary, GAF provides a straightforward way to link linguistic annotations to semantic representations in RDF. The only requirement is that these annotations have a unique identier making it widely applicable. The next sec- tion will discuss advantages and challenges to make more extensive use of RDF in linking linguistic annotations. 5. Linguistic Annotations in RDF RDF is a useful data model for linguistic representations for several reasons. However, RDF representations pose a challenge when these representations are used as input for NLP modules. This section addresses both sides of using RDF for representing linguistic information. 5.1. Advantages of RDF representations RDF is by nature a graph model, which makes declarative specication of dependency patterns easy, for instance in SPARQL. Triple stores are typically optimized for queries that require multiple joins. That makes evaluation of de- pendency graph queries, which are typically long branched chains, efcient. This facilitates the communication be- tween representations in RDF and linguistic processing tools. Another advantage of RDF is that it uses IRIs14for identi- cation and IRIs are not limited to the scope of a document, but have a global validity. This makes it easy to represent coreference relations across documents as done in GAF as explained in Section 4. Furthermore, RDF forms the basis on which RDFS and OWL ontology reasoning is possible. This allows for some very useful operations, such as subclass, subproperty and property chain reasoning. We therefore propose to use IRIs more extensively than is currently done in NIF. NIF rep- resents most linguistic attributes and values as strings. In NAF, we try to use IRIs as much as possible while repre- senting linguistic information. Schuurman and Windhouwer (2011) note the challenges in- volved in dening standardized sets of linguistic proper- ties. ISOcat (Kemps-Snijders et al., 2008) provides stan- dards with useful denitions, but because of differences in linguistic theories or cross-linguistic properties it is not al- ways possible to use existing sets. New, sometimes closely related, categories will be introduced as linguistic annota- tions. If we can represent linguistic properties with ontolo- gies, we can dene how output of different tools relate to each other. If there are differences in granularity between output of cer- tain tools, reasoning can be used to generalize over linguis- tic information. It is also possible to dene equivalence or 14Recall that IRIs are the new internationalized variant of URIs used in RDF 1.1.near equivalence. The possibility of dening relations be- tween linguistic classes increases the interoperability and comparability of tools (Hellmann et al., 2013). For in- stance, Agirre et al. (2009) dene a basic set of nine Part of Speech (PoS) tags which are used in KAF. Several other modules that use PoS tags as their input assume that this set is used. If we include a PoS tagger that is trained on the Penn Treebank, this will assign tags according to the set dened by Santorini (1990). We can dene that a common plural noun (NNS) and a common singular or mass noun (NN) from Santorini's set are both subtypes of the nominal class (N) used by Agirre et al. (2009). RELcat (Schuurman and Windhouwer, 2011; Windhouwer, 2012) provides a set of basic relations specically designed for this purpose. We can make use of these relations in NAF. 5.2. The challenge of using RDF Several challenges exist when it comes to creating linguis- tic representations in RDF. In fact, Hellmann et al. (2013) state that \"RDF can hardly be used efciently for NLP in the internal structure of a framework\". We will dene those in two categories: Those caused by generic differences in structure and expressivity between RDF and XML repre- sentations, and those caused by practical use of these differ- ent interpretations in NLP tools and pipelines, such as the ability to read in and (re)use annotation information from other tools with relatively low cost. Comparing XML and RDF is a bit like comparing apples and oranges; while XML in itself is a data format and se- rialization format, RDF is an abstract data model which can be serialized using several data formats and syntaxes. While RDF is meant to express semantic relations between objects, \"XML is rst and foremost a means to dene gram- mars\" (Decker et al., 2000). Often, intrinsic properties of a dened XML grammar are used to express important information, e.g. the nesting of elements is used to denote a hierarchical relation within the data. Furthermore, concepts such as \"document order\" are intrinsic to XML related technologies including DOM, XPath and XSLT. Since multiple serialization formats are available for RDF, syntactical grammar properties can not implicitly be used to encode information such as ordering, hierarchy, etc. Hence, the information encoded in such grammar based features needs to be modeled explicitly in the data model. Though this is very well possible and one could argue that this is a more sound solution to start with (Cassidy, 2010), it does not alter the fact that adopting current NLP tools and pipelines to use such a data model is a non trivial task. As mentioned above, NIF Stanbol offers the basic structure for such a model, but it is still in its initial stages of develop- ment. Current representations of linguistic annotations in RDF have a radically different structure from the one used in LAF-based models making it challenging to build wrap- ping tools around NLP modules that use LAF-based repre- sentations. 6. The NLP Annotation Format The previous section showed why RDF can be useful for representing linguistic annotations, but also that there Excerpt of a NAF document showing the text, term and entity layers. some challenges involved in adapting tools to use RDF. We propose a solution where we maintain a LAF-based repre- sentation similar to KAF, but revise it so that it can easily be converted to RDF. The structure remains easy integratable in our existing tools, but also allows us to take advantage of possibilities offered by RDF. In this section, we describe the current status of the format. 6.1. Current Status of NAF Like KAF, NAF comprises several annotations over a text at different linguistic levels (morphosyntactic, syntactic, se- mantic and pragmatic),15adopts a stand off strategy for an- notating the source text and is XML based. The following general rules are followed in all layers: <span> elements are used to dene the range of lin- guistic elements to which an annotation applies. Linguistic annotations of a particular level always span elements of previous levels. Linguistic annotations of different levels are not mixed. The \"levels\" in the general rules refer to different types of linguistic information, which can be groupments of linguis- tic entities (e.g. tokens vs. terms vs. chunks), relations be- tween linguistic entities (e.g. dependencies, semantic roles) 15Currently, NewsReader uses 12 different layers for process- ing text ranging from low level analysis, such as tokenization, to high-level analysis such as semantic roles and factualityor information about a linguistic entity (e.g. disambiguated word sense). Figure 2 shows an excerpt of a NAF docu- ment comprising three layers: text, terms and entities. The span elements for the entities point to identiers in the term layer, while the span elements in the term layer point to the identiers of the tokens in the text layer. In order to reduce unnecessary duplication of information and facilitate conversion to NAF-RDF, the following addi- tional rules were dened for NAF: No duplicate representations of xed properties of a specic linguistic annotation Consistent structure of different linguistic layers Usage of IRIs whenever possible to refer to external entities and linguistic properties Consistency is important so that generic rules can be used to convert standard NAF to NAF-RDF. For instance, in NAF we always use the <naf:span> element to point to lower linguistic entities and <naf:from> and<naf:to> at- tributes to dene a relation from one linguistic entity (the source) to another (the target). IRIs are used as much as possible so that we can make use of the advantages of RDF conform representations, as outlined above. In the example in Figure 2, the entity \"New York\" is recognized by the NER module and is linked to the appropriate DBpedia page. The association between the entity and the external reference is represented using an IRI (http://dbpedia.org/page/New York City ). We strive to indicate attributes and their values through IRIs as well. Representing information by IRIs has the advantage that we can dene properties of linguistic values formally in RDF. This avoids repetitions of such properties as found in KAF. A requirement that all IRIs should also be represented in ontologies can however form a hindrance to quickly integrate new annotations. Creating ontologies and use their denitions in NAF is therefore optional, though highly recommended. The principles behind NAF are mostly followed by the NLP modules that currently use NAF. There are however a few additional revisions needed to meet all our requirements. We will outline them in the next subsection. 6.2. Further simplifying RDF conversion NAF layers can easily be converted to RDF, but it is cur- rently not possible to do so with a generic script that applies to all layers. In NAF-RDF, all annotations are represented as triples. A typical triple would have the identier of a linguistic object as subject, an attribute as predicate and the attribute's value as its object. Figure 3 provides an example of NAF annotations in RDF. Triples can be placed in named graphs. We can provide provenance information and con- dence values for each named graph. Triples will thus be placed in the same named graph according to their prove- nance and condence values. Note that this will often mean that a named graph contains only one triple. An XML ele- ment in NAF can be translated to RDF by taking the identi- er as subjects, attributes as RDF predicates and values as objects. This means that an XML element in representation (in RDF TriG) always provide a unique identier and may only contain attributes that belong in the same named graph, i.e. that have the same provenance and condence scores. If there are annotations associated with an object that have different provenance or condence scores, multiple XML elements must be used to represent this information. As can be seen in the term layer in Figure 2, this is cur- rently not the case. The term element indicates the type, lemma and Part of Speech (PoS). Even though lemma and PoS are often determined by the same tool and have the same provenance, one could imagine that they do not al- ways have the same condence score. The type indicates whether the word is a member of a closed class and will denitely have different condence scores from lemma and PoS. According to the requirement outlined above, the type should thus be indicated in its own element and the same may apply to PoS and lemma. Note that it is possible to assign more than one condence score to the same named graph. In this case, however, all condence scores apply to all triples in the graph. It is therefore not an option for the scenario outlined above. In Figure 3, we represent the output of one tool which is a lemmatizer and PoS-tagger and can indicate whether a word is closed or open class. In this case, the tool assigns identical condence scores to lemmas and PoS-tags and a different score to the type. Even if provenance can be pro- vided for each linguistic object, it is typically provided for a set of annotations that are created by the same activity. Be- cause PoS-tags, lemmas, types and their condence scores are provided by running the same module, they have the same provenance. Most annotations in NAF are represented as attributes in elements. There are two notable exceptions: <span> and<externalReferences> . A <span> is a child of a linguistic element and includes one or more <target> elements. These targets refer to linguistic elements from other layers. As their name indicates, <externalReferences> can link linguistic elements to annotations dened in external resources. This element can contain one or more <externalRef> elements that always consists of a reference and a resource. Because the structure of these two exceptions is consistent across lay-ers, they too can be converted to RDF representations by generic rules. However, note that both a reference and a resource are indicated for <externalRef> . If we use a IRI to indicate the reference, we no longer need to provide the resource. The resource is an invariable property of the reference and need not be provided for individual elements. Resource attributes can be removed from external refer- ences as soon as we start making use of IRIs more exten- sively. Revisions concerning the attributes that may occur in the same element will be implemented as we start adding more modules to our architecture and experimenting with more than the top-ranking outcome of tools. Provenance and condence information for individual annotations play a signicant role in this step and may lead to further re- visions of the structure. It should however be anticipated when new layers of information are added to NAF. Modeling the provenance of information is essential for GAF. We can evaluate the value of informaton coming from many different layers and across different mentions (within the same document and accros documents) to be unied at the instance level in SEM. The ultimate goal is to come to an adequate representation in SEM which can be based on many different pieces of evidence. This also allows us to model interpretation of text given diffent background knowledge that may complement the partial and vague in- formation in text, which is the rule rather than the excep- tion. 7. Conclusion and Future work In this paper, we presented ongoing work to link linguis- tic annotations using RDF and, vice versa, to convert tex- tual information to RDF. We introduced NAF, a LAF-based representation format that is specically structured in a way so that it can easily be converted to RDF. NAF aims to be a consistent and compact representation schema, easy to con- vert to RDF and it facilitates the integration of provenance and condence scores into the model. Our work on NAF can be seen as a continuation of our pre- vious work on GAF which we also described in this pa- per. This generic framework can relate any instance to a mention of this instance and any RDF triple to a mention of the relation expressed by the triple. GAF provides a link between the Semantic Web and linguistic annotations and forms at the same time a natural way to model (cross- document) coreference. NAF forms the next step in facil- itating the representation of linguistic annotations in RDF. Within GAF, NAF should provide the pieces of linguisti- cally grounded evidence with their provenance. We argued that we need to allow for flexibility, redudancy and even conflicts at the level of linguistics annotation of mentions in NAF, to be resolved at the semantic level in SEM rea- soning over the provenance of the evidence. We outlined the general advantages of using RDF for lin- guistic representations. They include efcient graph search, straightforward coreference representations, and the possi- bility of using reasoning to link linguistic representations. This last property is particularly important since it supports interoperability. We also point out some challenges due to differences between RDF and XML representations typi- cally used for representing linguistic annotations.15The solution NAF offers is to maintain properties of an ex- isting linguistic annotation format that has proven its prac- ticality for complex linguistic pipelines and adapt it so that it can easily be converted to RDF. We have outlined a set of principles for NAF that do not harm the flexibility, interop- erability or practicality of KAF, but does facilitate conver- sion of NAF to RDF. As pointed out in Section 6.2., a few more steps need to be made in NAF to make it fulll all our requirements. The next steps will therefore mainly focus on replacing infor- mation by IRIs. While creating ontologies and IRIs for representing linguistic annotations, we aim to look at alter- native representation formats as much as possible in order to improve interoperability of NAF. In particular, we will try to make use of NIF representations ideally by joining the NLP2RDF initiative as suggested by Hellmann (p.c.). Acknowledgements We thank Harry Bunt and three anonymous reviewers for valuable feedback. All remaining errors are our own. This work was supported by the European Union's 7th Framework Programme via the NewsReader (ICT-316404) project and the BiographyNet project (Nr. 660.011.308), 8. A. D., Rigau, G., Soroa, A., and Bosma, W. (2009). KAF: Kyoto annotation framework. Bird, S., Day, D., Garofolo, J., Henderson, J., Laprun, C., and Liberman, M. (2000). ATLAS: A flexible and ex- tensible architecture for linguistic annotation. In Pro- ceedings of the Second International Conference on Lan- guage Resources and Evaluation . Bosma, W., A., Marchetti, A., Aliprandi, a generic semantic annotation format. In Proceed- ings of the 5th International Conference on Generative Approaches to the Lexicon GL 2009 , Pisa, Italy. Cassidy, S. (2010). An RDF realisation of LAF in the DADA annotation server. In Proceedings of ISA-5 , Hong Kong. Chiarcos, C., Nordhoff, S., and Hellmann, S. (2012). Linked Data in Linguistics. Representing and Connect- ing Language Data and Language Metadata . Springer, Heidelberg. Chiarcos, C. (2008). Serani, L. (2013). Interlinking unstructured and structured knowledge in an integrated framework. In 7th IEEE International Conference on Semantic Computing (ICSC), Irvine, CA, USA . Cunningham, H. (2002). GATE, a general architecture for text engineering. Computers and the Humanities , 36(2):223-254. Decker, S., Melnik, S., Harmelen, F. V ., D., Klein, M., and Horrocks, I. (2000).The semantic web: The roles XML and Internet Computing, IEEE , 4(5):63-73. Ferrucci, D. and Lally, A. (2004). UIMA: an architectural approach to unstructured information processing in the corporate research environment. Natural Language En- gineering , 10(3-4):327-348. Fokkens, A., van Erp, M., V ossen, Tonelli, S., van Hage, W. R., Serani, L., Sprugnoli, R., and Hoeksema, J. (2013). GAF: A Grounded Annotation Framework for events. In Proceedings of the rst Workshop on Events: Denition, Dectection, Coreference and Representation , Atlanta, USA. Grishman, R. (1997). TIPSTER architecture design docu- ment version, 2.3. Technical report. Hage, V ., \u00b4e, V ., Segers, R., Hollink, L., and Schreiber, G. (2011). Design and use of the Simple Event Model (SEM). J. Web Sem. , 9(2):128-136. http://dx.doi.org/10.1016/ j.websem.2011.03.003 Br \u00a8ummer, M. (2013). Integrating NLP using Linked Data. In Proceed- ings of the 12th International Semantic Web Conference . Ide, N. and Suderman, K. (2007). GrAF: a graph-based format for linguistic annotations. In Proceedings of the Linguistic Annotation Workshop , pages 1-8, Prague, Czech Republic. Ide, N. and Suderman, K. (2012). Bridging the gaps: inter- operability for language engineering architectures using GrAF. Language Resources and Evaluation , (46):75- 89. Ide, N., Romary, L., and Villemonte de La Clergerie, E. (2003). International standard for a linguistic annotation framework. In Proceedings of the HLT-NAACL 2003 Workshop on Software Engineering and Architecture of Language Technology Systems (SEALTS) . Association for Computational Linguistics. Kemps-Snijders, M., Windhouwer, M., Wittenburg, P., and Wright, S. (2008). Isocat: Corralling data categories in the wild. In LREC . Moreau, L., Missier, P., Belhajjame, K., B'Far, S., Cresswell, S., Gil, Y ., Groth, P., Klyne, G., Lebo, T., McCusker, J., Miles, S., Myers, J., Sahoo, S., and Tilmes, C. (2012). PROV-DM: The PROV data model. Technical report. Santorini, B. (1990). Part-of-speech tagging guidelines for the penn treebank project (3rd revision). Technical re- port, Penn Engineering. Schuurman, I. and Windhouwer, M. (2011). Explicit se- mantics for enriched documents. what do ISOcat, REL- cat and SCHEMAcat have to offer. In 2nd Support- ing Digital Humanities conference (SDH 2011), Copen- hagen . Thompson, H. and McKelvie, D. (1997). Hyperlink se- mantics for standoff markup read-only documents. In Proceedings of SGML Europe-97 . Windhouwer, M. (2012). RELcat: a relation registry for isocat data categories. In Proceedings of LREC 2012 , pages 3661-3664.16Semantic Annotation Issues in Parallel Meaning Banking Johan Bos University of Groningen johan.bos@rug.nl Abstract If we try to align meaning representations of translated sentences, we are faced with the following problem: even though concepts and relations ought to be independent from specic natural languages, the non-logical symbols present meaning representations in usually resemble language-specic words. In faithful translations, such symbols can be easily aligned. In informative translations (where more information is provided by the target translation), symbols can be aligned by a symbol denoting an inclusion relation. In loose translations, we need a third combinator to combine symbols with similar but not identical meanings. We show how this can be done with several concrete, non-trival English-German translation pairs. The resulting formalism is a rst step towards constructing parallel meaning banks. Keywords: Semantic Representations, Meaning Banking, Parallel Corpora 1. Introduction The ingredients of meaning representations can roughly be divided into two categories: the logical symbols, and the non-logical symbols. To the rst category belong the quantiers, the variables, and the boolean operators (nega- tion, conjunction). The members of the second category, the non-logical symbols, are based on the language that is undergoing semantic analysis. For example, a mean- ing representation for a simple sentence like \"John doesn't smoke\" would contain the logical symbols \u00acand, several variables, and the non-logical symbols JOHN (representing the entity referring to John) and SMOKE (representing the event of smoking). But now suppose I have a good transla- tion of this English sentence into, say, German or Dutch. Arguably, the meaning representation for this translation should not differ a great deal. But what would it look like precisely? One possible solution is to take a (neutral) auxiliary language for dening the vocabulary of non-logical sym- bols. But soon one will discover that this option isn't feasi- ble. In natural (non-literal) translations, the source is some- times more general, sometimes more specic than the target translation. This information will be lost when one relies on a single language. Moreover, phrasal translations will be hard to capture by a single language of symbols.1The alternative, and one that will be explored in this paper, is to combine the non-logical symbols of the source and target of a translated sentence into a single meaning representation. In order to investigate this possibility, we follow a strongly data-driven method. We take non-trivial transla- tion examples from an existing corpus (see Figure 1) and produce the meaning representations for each language. Then we will compare the respective meaning representa- tions, and examine how we could align the two representa- tions. Here we will just consider pairs of English-German translations \u2014 the choice for these two close languages makes sense for a pilot study of this kind. 1Although there are initiatives, notably the Abstract Meaning Representation project (Banarescu et al., 2013), pursuing closely related goals.We employ Discourse Representation Theory, DRT (Kamp and Reyle, 1993), as the formal theory of meaning, mainly because it is well-known among semanticists and has cov- ers many linguistic phenomena, but we would like to em- phasize that any meaning representation with variables and n-place relations could have been adopted to integrate the ideas put forward in this paper. We will introduce new machinery for representing parallel meanings. We will bring three new operators into play for combining non- logical symbols dealing with faithful translations, informa- tive translations, and loose translations. To make this more readable, we just assume that the non-logical symbols rep- resent the right sense of the concepts expressed by the sur- face strings. We also assume that each non-logical symbol carries the information of its source language (here: En- glish or German), but don't explicitly show it in the mean- ing representations for reasons of clarity. 2. Faithful Translations: Faithful translations are among the easiest to align, because they are often based on word-by-word translations. Con- sider the examples and corresponding meaning representa- tions given below in Example 1. Here, and in the exam- ples that follow, we show the meaning representation for an English expression and one its German translation, and a parallel meaning representation comprising both source and target language. The mono-lingual meaning represen- tations also show the mappings of discourse referents to surface strings (where dotted variables indicate substitu- tions that need to take place) for the reader's convenience. EXAMPLE 1 x p x/mapsto\"the chance to (x) TOZU(x,p)17English (en) German (de) Pubs also provide good value for money, Pubs bieten auch ein gutes Preis-Leistungsverh \u00a8altnis, the chance to taste of beer die Gelegenheit ein Glas Bier zu trinken and have a chat with the locals. und mit den Einheimischen zu plaudern. The \"Magpies\", Die \"Elstern\", Newcastle United Football Club, wie der Newcastle United Football Club auch genannt wird, have produced some players. besten Fu\u00dfballspieler Gro\u00dfbritanniens hervor. Due to the possibility of animals and birds Da Haustiere und V \u00a8ogel Krankheiten nach bringing disease to the UK, Gro\u00dfbritannien einschleppen k \u00a8onnen, bringing them with you on holiday is not recommended. wird davon abgeraten, sie mit in die Ferien zu nehmen. Figure 1: Examples considered in this study. Source: The English-German Translation Corpus, http://ell.phil. tu-chemnitz.de/ . This example illustrates a faithful, literal translation, and as a pleasant consequence there is a simple one-to-one mapping between the non-logical symbols of the source and target language. To arrive at a parallel meaning represen- tation, we combine the non-logical symbols (with the same arity) originating from different languages by simply con- catenating them with the help of a new operator: . For instance, the German-originating two-place relation ZUand the English-originating two-place relation TOare combined to yield a new compound non-logical symbol TOZU. Now consider Example 2, illustrating some basic neo- Davidsonion event structure.2It makes sense to assume that the thematic roles are universal and therefore language in- dependent. Therefore it is not necessary to align the con- ditions for the roles in the parallel meaning representation: they are shared. However, it could be the case that there are languages that explicitly express a role (for instance, by a preposition), in which case the non-logical symbol denot- ing that role could be based on it.3 EXAMPLE 2 x e y x/mapsto\"The Magpies\" e/mapsto\" xhave y\" MAGPIES AGENT (e,x) PRODUCEHERVORBRINGEN (e) THEME (e,y) We will give meaning to this new operator by extending a translation function from the meaning representation to 2For simplicity we assume that proper names introduce one- place relations. 3An example that comes to mind is the passive construction in English, where the agent role is marked by the preposition \"by\". A further example is the semantic role of recipient expressed by the preposition to, in constructions like \"Mary gives the book to John\". See also Example 6.rst-order logic, [.]fol, on the same lines as earlier work in Discourse Representation Theory (Bos, 2004; Kamp and Reyle, 1993). We ,u n)) This simply says that all these symbols are synonyms, and applied to nof variables, result in logically equivalent meanings. One could compare this to a WordNet (Fell- baum, 1998) synset: given the compound symbol A B, then A and B belong to the same cross-lingual synset. 3. Informative Translations: < Translations, however, are rarely as literal and faithful as the previous examples suggest. Consider for instance Ex- ample 3, where the English noun \"players\" is translated into German with the more specic \"Fu\u00dfballspieler\". Even though it is clear from the context in the English sentence that we talk about players that practice the game of foot- ball, it isn't stated explicitly. It would therefore be wrong to align the meanings of these words with the operator. What we propose to do instead is introducing a new oper- ator,<, that combines two symbols and species that the rst is more specic (carries more information) than the second. EXAMPLE 3 x x/mapsto\"player\" PLAYER (x)x x/mapsto\"Fu\u00dfballspieler\" FUSSBALLSPIELER (x) /arrowsoutheast /arrowsouthwest x FUSSBALLSPIELER <PLAYER (x) As can be seen in the parallel meaning representation in Example 3, we specied that FUSSBALLSPIELER is more informative than PLAYER . This seems to be a common phe- nomenon in translation. What's left to do is giving a formal denition for <, and we dene it instance, given the compound symbol A <B applied to x, then A(x) holds, and if A(x) holds then also B(x) holds. In the parlance of WordNet (Fellbaum, 1998) practitioners, A would be a hyponym of B. 4. Loose Translations: An old proverb says that a translation cannot be both faith- ful and beautiful. Loose translations often just sound better. A case in point is \"taste a pint of beer\" and its German ren- dering \"ein Glas Bier trinken\": a pint (a unit of measure- ment) isn't the same as a glass (a container), and tasting isn't the same as drinking, although in WordNet (Fellbaum, 1998) they are both co-troponyms of consume . To align such loose translations we propose a new operator for sym- bol alignment:, illustrated by Example 4. EXAMPLE 4 e x y e/mapsto\"taste x\" x/mapsto\" apint of y\" is used to align non-logical symbols that have approximately the same meaning, and therefore cannot be described by or<. It is n)) 5. Aligning Embedded Contexts So far we have looked at what we believe are the basic ways to align meaning representations for parallel texts. But there are further issues in meaning alignment, and as a matter of fact the machinery proposed so far isn't able to account for some problems that we encounter when we con- sider modals and negation. Consider the English sentence \"The possibility of animals and birds bringing disease to the UK\" and its German translation \"Haustiere und V \u00a8ogel nach Gro\u00dfbritannien einschleppen.\" Both sentences contain a modal expression, expressed by a noun in English, and by a modal verb in German. Analo- gously to Example 1, we could analyze the English modal by introducing a hybrid modal operator (Bos, 2004). Now suppose that the German modal verb is semantically inter- preted by the modal possibility operator /diamondmath. This would give the meaning representation as shown in Example 5.EXAMPLE 5 x p x/mapsto\"the possibility of p\" POSSIBILITY (x) OF(x,p) p:x (e,z) THEME (e,u) some discrepancy between the monolingual se- mantic analyses: the hybrid modal operator (the colon :) that connects a propositional discourse referent with an em- bedded context in the English case, and the modal operator /diamondmathin the German case. We could say that in such a case we would need to revise the semantics analysis either on the English or on the German side, to arrive at the same logical operator. An alternative solution, shown here in Example 5, is to decorate logical operators with a language mode . This way, we can combine several operators triggered by differ- ent languages into one and the same parallel meaning rep- resentation. A similar semantic mismatch arises with trans- lating \"not recommended\" with the German verb \"abraten\". On the one side we face an explicit negation, and on the other side an implicit negation. Further empirical study is required to shed more light on this issue and evaluate the various possibilities for semantic alignment. 6. Discussion In this paper we proposed a new formalism to align mean- ing representations of translated texts. We illustrated the formalism with several non-trivial examples for English- German translations. Certainly, there are many things that we did not consider: light verbs, tense, aspect, discourse re- lations, pronouns, anaphoric phenomena. Hence, a sensible question to ask is how representative the examples consid- ered in this pilot study are and how and whether this method19scales up to other phenomena and languages more distant from English than German. The only answer we can give to this question is that one just needs to try and investigate, using the empirical method explored here. It is probably fair to point though that the examples that we discussed were not selected be- cause they were easy to model. In fact we tried deliberately to nd challenging examples with syntactic mismatches (such as the implicit vs. explicit negation). It seems that for closely related languages such as English and German the approach put forward in this paper is promising. For more distant languages, it could be that the same message is conveyed with very different syntactic structures, as the English-Korean pair4(\"I transla- tion \"nan-nun meri-ka aphuta\") in x/mapsto\"I\" y/mapsto\"head\" e/mapsto xhave a (e) RECIPIENTNUN(e,x) THEMEKA(e,y) HEADMERI (y) This is an interesting example because to ensure a smooth alignment between the English and Korean sen- tence, it forces us to produce a non-literal semantic analysis of the English sentence. It also shows that thematic roles, at least under the analysis put forward here, are more com- monly overtly expressed in languages other than English. But then, even within a single language, paraphrases with different syntactic structure should receive similar meaning representations: consider for instance \"my head hurts\" and \"I have a headache\". In this particular case, a proper analy- sis of light verbs would strengthen semantic alignment. Finally, we would like to remark that the assumptions that we have made for semantic representations are humble: meaning is described with the help of variables, n-place relations, a stock of non-logical symbols, and a couple of logical operators (the usual suspects, i.e. negation, disjunc- tion, modalities). This is standard practice carried out by formal semanticists studying Germanic languages, and we don't see any reason why it wouldn't extend to more dis- tant languages. It is an exercise that could lead not only to interesting language resources for machine translation ap- plications, but also to get a better general understanding of cross-lingual semantic analysis. 4This example was kindly suggested to me by one of the anonymous reviewers of this paper.7. References Banarescu, L., Bonial, C., Cai, S., Georgescu, M., Griftt, K., Hermjakob, U., Knight, K., Koehn, P., Palmer, M., and Schneider, N. (2013). Abstract Meaning Represen- tation for Sembanking. In Proceedings of the 7th Lin- Workshop and Interoperability with Discourse , pages 178-186, Soa, Bulgaria, August. Bos, J. (2004). Computational Semantics in Discourse: Underspecication, Resolution, and Inference. Journal of Logic, Language and Information , 13(2):139-157. Fellbaum, C., editor. (1998). WordNet. An Electronic Lexi- cal Database . The MIT Press. Kamp, H. and Reyle, U. (1993). From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and }@gmail.com platform for annotating inferential phenomena in entailment data, buttressed by a formal semantic model and a proof-system that provide immediate verication of the coherency and completeness of the marked annotations. By integrating a web-based user interface, a formal lexicon, a lambda-calculus engine and an off-the-shelf theorem prover, the platform allows human annotators to mark linguistic phenomena in entailment data (pairs made up of a premise and a hypothesis) and to receive immediate feedback whether their annotations are substantiated: for positive entailment pairs, the system searches for a formal logical proof that the hypothesis follows from the premise; for negative pairs, the system veries that a counter-model can be constructed. This novel approach facilitates the creation of textual entailment corpora with annotations that are sufciently coherent and complete for recognizing the entailment relation or lack thereof. A corpus of several hundred annotated entailments is currently being compiled based on the platform and will be available for the research community in the foreseeable future. Keywords: Annotation Platform, Semantic Annotation, Proof System, Formal Model, Textual 1. Introduction The Recognizing Textual Entailment (RTE) corpora (Da- gan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2008, a.o) present the challenge of automatically de- termining whether an entailment relation obtains between a naturally text Tand a manually composed hy- pothesis H.1These corpora, which are the only textual entailments, mark entailment candidates as positive/negative.2For example: their discovery of ulcer-causing bacteria, Aus- tralian doctors Robin Warren and Barry Marshall have received the 2005 Nobel Prize in Physiology or Medicine. H: Robin Warren was awarded a Nobel Prize.3 Entailment: Positive However, the linguistic phenomena that underlie entailment in each particular case and their contribution to inferential processes are not indicated in the corpora. In the absence of a gold standard that identies linguistic phenomena trigger- ing inferences, the inferential processes employed by en- tailment systems to recognize entailment are not directly 1A short software demonstration paper describing the Se- mAnTE annotation platform is included in the EACL 2014 pro- ceedings. 2Pairs of sentences in RTE 1-3 are categorized in two classes: yes- orno-entailment ; pairs in RTE 4-5 are categorized in three classes: entailment ,contradiction andunknown . We label the judgments yes-entailment from RTE 1-3 and entailment from RTE 4-5 as positive , and the other judgments as negative . 3Pair 222 from the development set of RTE 2.accessible and, as a result, cannot be evaluated or improved straightforwardly. We address this problem through the SemAnTE (Semantic Annotation of Textual Entailment) platform introduced in this paper. The platform allows human annotators to elu- cidate some of the central inferential processes underlying entailments in the RTE corpus. In 80.65% of the positive pairs in RTE 1-4, annotators found the recognition of en- tailment to rely on inferences stemming, inter alia , from the semantics of appositive, restrictive or intersective modica- tion (Toledo et al., 2013). We decided to focus on the above three phenomena for two reasons. First, they are prevalent in the RTE datasets and, second, their various syntactic ex- pressions can be modeled semantically using a limited set of logical concepts, such as equivalence, inclusion and con- junction. The annotation platform allows the annotators to mark the above three modication patterns when they are involved in the recognition of entailment by binding the words and constructions in sentences to a lexicon of abstract semantic denotations. The proposed semantic modeling offers an im- portant advantage: it licenses the system to search for for- mal proofs that substantiate manual annotations and to de- scribe how the modeled phenomena interact and contribute to the recognition process. This is achieved by employing a lambda-calculus engine and a theorem prover. The platform is currently employed for the preparation of a new corpus of several hundred annotated entailments com- prising both positive and negative pairs. In the future, we plan to extend the semantic model to cover other, more complex phenomena.212. Semantic Model We model entailment in natural language based on order theory, on a working assumption that entailment describes a preorder relation on the set of all possible sentences. Thus, any sentence trivially entails itself (reflexivity); and given two entailments T1H1andT2H2whereH1andT2 are identical sentences, we assume T1H2(transitivity). We use a standard model-theoretical extensional semantics, whereby each model Massigns sentences a truth-value in the set{0,1}- the domain of truth-values on which we as- sume the simple partial order. We adapt Tarski's (1944) theory of truth to entailment relations and consider a the- ory of entailment adequate if the intuitive entailment pre- order on sentences can be described as the pairs of sen- tencesTandHwhose of annotations is to link between textual rep- resentations in natural language and model-theoretic repre- sentations. To this end, the words and structural congu- rations inTandHare marked with lexical labels that en- code semantic meanings for the linguistic phenomena be- ing modeled. These lexical labels are dened formally in a lexicon, as illustrated in Table 1 for major lexical cate- gories over types: eforentities ,tfortruth-values , and the functional compounds of eandt. Category Type Example Denotation Proper Name e Dan dan Indef. Article (et)(et) a A Def. Article (et)e the Copula (et)(et) is IS Noun et bacteria bacteria Intrans. verb et sit sit Trans. verb eet receive receive (et)((et)(et))and (Pred)) et Dutch dutch Exist. Quant. (et)(et)t some SOME Table 1: Lexicon Illustration Denotations that are assumed to be arbitrary are given in boldface. For example, the intransitive verb sitis assigned the typeet, which describes functions from entities to truth- values, and its denotation sitis an arbitrary function of this type. The denotations of several other lexical items are re- stricted by the given model M. As illustrated in Figure 1, the coordinator andis assigned the type (et)((et)(et)), and its denotation is a function that takes a function Aof type etand returns a function that takes a function B, also of typeet, and returns a function that takes an entity xof type eand returns 1if and only if xsatises both Attaching labels words and syntactic construc- tions enables annotators to mark the linguistic phenomena manifested in the data. Moreover, by virtue of its formal foundation, this approach allows annotators to verify that the entailment relation (or lack thereof) that obtains between the textual forms of TandHis also Functions in the Lexicon between their respective semantic forms. This latter step ensures that the annotations provide sufcient information for recognizing the entailment relation in a given pair based on the semantic abstraction. For example, consider the simple entailment Dan is short and thin Dan is short and assume annotations of Dan as a proper name, short andthin as restrictive modiers in predicate position, and and as predicate conjunction. The formal model can be used to verify these annotations by constructing a proof as follows: For each )(dan) def. IS(Pr(short )))(dan) def. of IS =[ [Dan is short ] ]Manalysis 3. Platform Architecture The platform's architecture is based on a client-server model, as illustrated in Figure 2. Figure 2: Platform Architecture The user interface (UI) is implemented as a web-based client using Google Web Toolkit (Olson, 2007) and allows multiple annotators to access the RTE data, to annotate22them, and to substantiate their annotations. These opera- tions are done by invoking corresponding remote procedure calls at the server side. We describe the system components as we go over the work-flow of annotating Example 1. Data Preparation : We extract T-Hpairs from the RTE datasets XML les and use the Stanford CoreNLP (Klein and Manning, 2003; Toutanova et al., 2003; de Marneffe et al., 2006) to parse each pair and to annotate it with part- of-speech tags.4Subsequently, we apply a naive heuristic to map the PoS tags to the lexicon.5This process is per- formed as part of the platform's installation and when an- notators need to simplify the original RTE data in order to avoid syntactic/semantic phenomena that the semantic en- gine does not support. For example, the fronted for-phrase For their discovery. . . is moved after the object of the verb receive as fronted adjuncts are not supported. Additionally, the phenomenon of distributivity manifested in the infer- ence Robin Warren and Barry Marshall have received. . . Robin Warren has received. . . , which is required for rec- ognizing the entailment in this example. We do not model this inference and the construction must therefore be sim- plied. These simplications yield Tsimple andHsimple as follows: Tsimple : The Australian doctor Robin Warren has re- ceived the great Nobel Prize in Physiology-Medicine for the discovery of the ulcer-causing bacteria. Hsimple : Robin Warren was awarded a Nobel Prize. Annotation : The annotation is done by marking the tree- leaves with entries from the lexicon. For example, receives is annotated as a transitive verb, ulcer-causing is anno- tated as of the bacteria , andAustralian is annotated as an intersective modier of the noun doctors . In addition, annotators add leaves that mark semantic relations. For instance, a leaf that indicates the apposition between The Australian doctor andRobin Warren is added and annotated as WHOA. Furthermore, the annotators x parsing mistakes as in the great Nobel Prize in Physiology-Medicine server stores a list of all annotation actions. Figure 3 shows the tree-view, lexicon, prover and annotation history panels in the UI. Dening Lexical Relations : Our modeling of modication phenomena does not address inferences that rely on lexi- cal knowledge, as in: \"Robin Warren has received a prize\" \"Robin Warren was awarded a prize\". Such lexical re- lations between the text and hypothesis are marked by the annotators and translated into logical formulas by the proof- system. Proving : Once all leaves are annotated and the tree struc- tures ofTsimple andHsimple are manipulated, the annota- tors use the prover interface to request a search for a proof 4Stanford CoreNLP version 1.3.4 5This heuristic is naive in the sense of not disambiguating verbs, adjectives and other types of terms according to their se- mantic features. It is meant to provide a starting point for the manual annotation process.indicating that their annotations are substantiated. First, the system uses lambda calculus reductions to create logical forms that represent the meanings of Tsimple andHsimple in higher-order logic. At this stage, type errors may be re- ported due to erroneous parse-trees or annotations. In this case an annotator will x the errors and re-run the prov- ing step. Second, once all type errors are resolved, the higher-order representations are lowered to rst order and Prover9 (McCune, 2010) is executed to search for a proof between the logical expressions of Tsimple andHsimple .6 The proofs are recorded in order to be included in the cor- pus release. Figure 4 shows the result of translating Tsimple andHsimple to an input to Prover9. 4. Corpus Preparation We have so far completed annotating 40 positive entail- ments based on data from RTE 1-4. The annotators are thoroughly familiar with the data and have extensive expe- rience in recognizing entailments stemming from apposi- tive, restrictive and intersective modication. While com- piling a corpus of several hundred entailment pairs, we are also working to extend our model to recognize inferences produced by a wider range of linguistic phenomena. The objective is to minimize the need for simplifying the input utterances so as to make them compatible to the model. formulas(assumptions). % Pragmatics: all x0 (((nobel prize(x0) & in nobel prize(Physiology Medicine, x0)) & great nobel prize in(Physiology Medicine, endoflist. Figure 4: Input for Theorem Prover 5. Conclusions This paper proposes a novel concept for an annotation plat- form buttressing a proof-system designed to substantiate a semantic annotation scheme for inferences stemming from modication phenomena. This method guarantees that the manual annotations constitute a complete description of a given entailment relation and facilitates the creation of a 6Prover9 version 2009-11A23Figure 3: User Interface Panels: Annotation History, Tree-View, Prover Interface and Lexicon Toolbox gold-standard of such phenomena. A new corpus is cur- rently being developed and will be publicly available for the research community in the foreseeable future. Acknowledgments The work of Stavroula was supported by a VICI grant number 277-80-002 by the Netherlands Organisation for Scientic Research (NWO). 6. References Bar Haim, Roy, Dagan, Ido, Dolan, Bill, Ferro, Lisa, Gi- ampiccolo, Danilo, Magnini, and pascal recognising textual en- tailment challenge. In Proceedings of the Second PAS- CAL Challenges Workshop on Recognising Textual chal- lenge. Learning Challenges. Christopher D. (2006). Generating Typed Depen- dency Parses from Phrase Structure Parses. In Proceed- ings IEEE / ACL 2006 Workshop on Spoken Lan- guage Technology . The Stanford Natural Language Pro- Giampiccolo, Danilo, Dang, Hoa Trang, Magnini, Bernardo, Dagan, recognising textual entailment challenge. InTAC 2008 Proceedings . Klein, Dan and Manning, Christopher D. (2003). Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1 , ACL '03, pages 423-430, O'Reilly Media.Tarski, Alfred. (1944). The semantic conception of truth: and the foundations of semantics. Philosophy of Textual ment. In Proceedings the 10th International Confer- ence on Computational Semantics (IWCS 2013) - As- sociation for Computational Linguistics. Toutanova, Kristina, Klein, Dan, D., and Singer, Yoram. (2003). Feature-rich part-of-speech tagging with a cyclic dependency network. In Pro- ceedings of the 2003 Conference of the North Ameri- can Chapter of the Association for Computational Lin- guistics on Human Language Technology - Volume 1 NAACL '03, pages 173-180, Stroudsburg, PA, USA. ACL.24Semantic project \"Semantic Processing across Domains\" is granted by the Danish Research Council for Culture and Communication and runs for the period 2013 -2016 . It focuses on Danish as a low -resourced language and aims at increasing the level of technological resources available for the Danish HLT community . A primary project goal is to provide semantically annotated text corpora of Danish following agreed standards and to let these serve as training data for advanced machine l earning algorithms which will address data scarcity and domain adaptation as central problem areas . The D anish CLARIN Reference Corpus - supplemented by a selection of additional text types from social media and the web - are being sense and role annotated . We experiment with an adaptation of PropBank roles to Danish a s well as with a scalable sense inventory of Danish. This inventory spans from supersense annotation s (semantic classes) to wordnet -derived sense annotation s which rely on a distinction between ontological types and main and subsenses . The annotation t ool WebAnno , which is being developed as part of the German CLARIN project , is applied for the annotation task. 1. Semantic annotation and Danish HLT resources The newly initiated project \"Semantic Processing across Domains\" is granted by the Danish Research Council for Culture and Communication and runs for the period 2013 -2016. It is a collaborate project between the University of Cop enhagen and The Society for Danish Language and Literature (DSL) , which is an independent institution editing and publishing Danish texts and dictionaries on a scholarly basis . The project focuses on Danish as a low -resourced language and aims at increasin g the level of technological resources available for the Danish HLT community , which according to the META -NET White Paper Series (See Pedersen et al. 2012) falls in the category 'fragmentary' compared to other European languages . To this en d, a primary project goal is to provide semantically annotated text corpora of Danish following agreed standards and to let these serve as training data for machine learning algorithms which address data scarcity and domain adaptation as central problem areas (S\u00f8gaard 2013) . 2. CLARIN resources used and developed in the project The DK -CLARIN project (2008 -2011) aimed at initiating a Danish research infrastructure for the Humanities by integrating written, spoken, and visual records in a common technological infrastructure. Th is work is currently continued within the Danish DigHumLab1 and European CLAR IN2 project s, and several resources deriving from this work serve as background resources for the present annotation project. This regards in particular the CLARIN Reference Corpus (Asmussen 2012) which 1 See http://dighumlab.com/ 2 See http://www.clarin.eu/ is the main target for our annotation even if the material is augment ed with a selection of additional text types from social media . The CLARIN Reference Corpus contains approx. 45 million words covering newspapers, magazines, oral (but transcribed) congress debates, web pages, blogs etc. DK -CLARIN also financed the finalization of the Danish wordnet, DanNet (Pedersen et al. 2009) , which is applied as a lexical resource for word sense annotation in the project and which - together with a medium -sized dictionary of Danish, The Danish Dictionary (DDO , developed by DSL ), form s the basis for experiments with a scalable sense inventory . Both the corpus and the wordnet are available via the META -SHARE and DK -CLARIN platform s. 3. Three semantic annotation tasks The project is concerned with semantic annotation at word and sentence level . Currently, three specific annotation tasks are embarked : A lexical sample task with a selected subset of noun s and verbs to be sense anno tated on the basis of DanNet and DDO3 An all -words task with coarse -grained sense annotations based on so -called supersenses (or 'semantic classes') Annotation of semantic roles relying on a transfer of PropBank roles (Palmer et al. 2005) to Danish . In the following the three tasks are described in more detail. 3.1 Word sense annotation based on DanNet One aim of the project is to experiment with a s calable 3 For this task, the CLARIN Reference Corpus will be augmented with manually selected examples from additional corpora and from the web. 25sense inventory for word sense disambiguation (wsd) . In contrast to most other wordnets, DanNet is compiled from the corpus -based definitions and sense distinctions provided in D DO with clear distinction s between main and sub -senses. This distinction opens for the possibility of automatic generation of sense inventories of vary ing granularity and for an examination of the inter -coder agreement that is achieved with these individual inventories (a comparable approach is employed in the MASC project at a larger scale, cf. Passonneau et al. 2012 and de Melo et al. 2012 ). We hypothesize that the main senses in DanNet in combination with its ontological types (such as Person, Semiotic Artifa ct, Building, Time, Measurement4) will provide the basis for a practically more adequate and theoretically well-founded s ense inventory for word sense disambiguation than what is seen in several comparable wordnet resources where rather finegrained and unstructured sense enumerations are applied (for discussion s, see Ide & Wilks 2007, Kilgarrif f 2007, Brown et al. 2010 , V ossen et al. 2011 , de Melo et al. 2012 among many others ). An initial approach is therefore to automatically collapse sub -senses of a word with the ir main sense, unless a sub -sense has another ontologi cal type or topic than the main sense - a case which is typically seen with metaphorical or very specialized senses . For comparison to an even coarse r inventory , and in order to provide also sense annotations that are directly comparable and interoperable across semantic corpus resources for other languages, all DanNet senses are mapped to the so -called 'supersense' -inventory (corresponding roughly to semantic classes) derived from the wordnet lexico -grapichal classes ( cf. Ciaramita & Johnson 2003 and http://wordnet.princeton.edu/wordnet/man/lexnames.5W N.html )5 using a transfer scheme (Figure 1) . Figure 1. Extract of transfer scheme from EuroWordNet ontological types used in DanNet to supersenses (numbers in middle column indicate number of synsets) 4 The ontological types in DanNet are adapted from the EuroWordNet Ontology (V ossen et al. 1999). 5 For the all -words t ask, only the supersenses are annotated. To exemplify the distinction between supersenses and the finer sense inventory that we generate from DanNet and DDO , consider the lexemes pande (pan, forehead) and kort (map , card, playing card ). The two unrelated meanings of pande will be maintained in both approaches (noun.artifact and noun.body in supersense terms), whereas the different meanings of kort will be collapsed into one with the supersense approach (corresponding to the supersense noun.artifact) , but maintained in the more fine-grained appro ach. However, both approaches will generali ze over the dictionary distinctions between postcards, admissions cards and id cards since these are all considered to be subsenses in DDO with the same ontological type in DanNet. Previous to the manual annotation, all corpus data are pre-annotated based on DanNet (for annotation tool, see Section 4) . In cases of more than one sense or supersense, the annotator will choose betw een the pre -annotated ones. In all situations, the annot ator can overrule a pre-annotated sense and assign an alternative sense. Unknown words (which are mostly compounds) are obviously not pre -annotated ; in these cases the annotators will pick the most appropriate sense from a pick list . Figure 2 shows an anno tation task with pre -annotated data. Figure 2 . Pre-annotated corpus extract for the annotator to refine (all-words task) (lit: 'The boy is considered by the police as dangerous ') Three annotators will work on each corpus extract and a gold standard annotation will be decided upon by the third annotator, wh o has the role of curator. 3.2 Annotation of semantic roles This part of the annotation project plans to relate to recent ISO standards for semantic role annotation (Bunt & Palmer 2013), and will include a transfer of PropBank roles to Danish, relying in addition on existing descriptive works on Danish verbs . Th ese include The Danish Thesaurus (DT) which is recently being published by DSL (cf. Nimb et al. 2013) and which group verbs in a FrameNet -like fashion. The thesaurus consists of scenarios described by the same words as the ones evoking a semantic frame. Information about the function of these group s in terms of arguments in a frame description is implicitly given in the metadata of the semantic subsections, i.e. in the type of g roup and from assigned relations of the type involved_agent and involved_patient, making DT a rich background resource for the constructin g a lexical resource for semantic role 26annotation . Further, a FrameNet -like resource has been developed by Bick (2011 ). This resource uses the semantic verb classification of DanNet and includes a set of all in all 38 semantic roles . Follo wing roughly a transfer approach tested by the Dutch Language Corpus Initiative (D -Coi) (Monachesi & Chapman 2006), we foresee the following steps: 1. Localize the verb sense 2. If the verb sense is not yet part of the Danish frames file, translate it to Englis h 3. Check the verb's frames file in PropBank 4. Locali ze the arguments and modifiers of the verb ; compare and adjust according to DT and Danish FrameNet 5. Extent the Danish frame file with synonyms from DT In contrast to PropBank which employs phrase structure trees for the syntactic structure, we plan to assign semantic roles onto a dependency parsed version of the corpus . 4. Applied annotation tool: Web Anno For the first annotation task, which was initiated in January 2014 and co ncerns the lexical all -words task, we apply the browser -based tool Web Anno. This tool is being developed as part of the German CLARIN project (CLARIN -D), cf. Yimam et al. 2013. Figure 3. The curator function in WebAnno where the gold standard is developed based on previous manual annotations ( 'When one of the two has admitted to have had a sexual relationship, how can.. ') The tool supports the annotation of a variety of linguistic levels and it is interoperable with a va riety of data formats. Further it supports project management of the annotation tasks and allows for dynamic quality judgments b y integrat ing measures of inter -annotator agreement and curator facilities . Figure 3 shows how annotation disagreements are detected and the gold standard developed. Another central feature for the tasks foreseen in the project is the previously mentioned possibility of including pre -annotation s as seen in Figure 2 . To our knowledge , we are the first team to apply WebAnno for semantic annotation. Since new features and some optimization is still foreseen in the experimental phase, the task is currently being hosted by Darmstadt University (where WebAnno is being developed) , but the hosting role will be taken over by the University of Copenhagen during spring 2014. 5. Concluding remarks and future work The original DK -CLARIN resources included both text and lexical -semantic resources, but hardly any links between the two. With this pro ject we move towards an integration of text and lexical resources by annotating a part of the CLARIN Reference Corpus with word senses and semantic roles following agreed standards . Further, the annotations will serve as training data to machine learning algorithms that will enable us to automatically annotate larger amounts of data with a certain margin of correctness . The main focus in this part of the project will be to investigat e methods for making joint learning of SRL-WSD less sensitive to the amounts of annotated data available and to domain differences (cf. S\u00f8gaard 2013) . DSL foresees to use the manually and automatically annotated corpora as an extra on -line citation resource for their dictionaries, illustrating the specific dictionary sense s and realizations via direct links from the dictionary sense to examples in a corpus . At present DSL already offers advanced searching for part-of-speech, morphological features and some syntactical ones. Adding the possibility of enhancing the search pat tern with word senses and semantic roles is the next logical step, and such an extension will allow for example to observe semantic meaning changes over time in a more systematic way. References Asmussen, J \u00f8rg (2012) . CLARIN Copenhagen October 31, 2012. http://cst.ku.dk/Workshop311012/sprogtekno2012.pdf Bick , Eckhard (2011). A FrameNet for Danish . In: Proceedings of NODALIDA 2011, May 11 -13, Riga, Latvia. NEALT Proceedings Series, Vol 11 , pp.34 -41. Brown, S.W., T. Rood, & M. Palmer. (2010). Number or Nuance: Which factors restrict reliable word sense 27annotation? Proceedings of the 7th Internationa l Conference on Language Resources and Evaluation (LREC'10) . Valetta, Malta. Bunt, H. and M. Palmer (2013) Conceptual and representational choices in defining an ISO standard for semantic semantic role annotation. In: Proceedings Ninth Joint Supersense Tagging of Unknown Nouns in WordNet Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing . Ida, Nancy & Yorick Wilks (2007) Making Sense About Sense. In: E. Agirre & P. Edmonds: Word Sense Disambiguatio n - Algorithms and Applications . Springer. Kilgarriff, Adam (2007). Word senses. In: E. Agirre & P. Edmonds: Word Sense Disambiguatio n - Algorithms and Applications . Springer. Melo, Gerhard de, Collin F. Baker, Nancy Ide, Rebecca J. Passonneau, Christiane Fellbaum (2012) Empirical Comparisons of MASC Word Sense . Proceedings from LREC 2012 , Istanbul, Turkey. Monachesi, and J . Trapman. (2006). Merging FrameNet and PropBank in a corpus of written Dutch. In Proceedings of the workshop Merging and layering linguistic information. Workshop held in conjunction with LREC 2006, Genoa - Italy, 23 May 2006. pp. 32-39. Nimb, Sanni, Bol ette S. Pedersen, Anna Braasch, H. S\u00f8rensen and Thomas Troelsg\u00e5rd (2013) Enriching a wordnet from a thesaurus. Workshop Proceedings on Lexical Semantic Resources for NLP from the 19th Nordic Conference on Computational Linguistics (NODALIDA ). Link\u00f6ping Electronic Conference Proceed ings; Volume 85. Oslo, Norway. Palmer, M. D. Gildea, P. Kingsbury. (2005). The Proposition Bank: A Corpus (2012). The MASC Word Sense Sentence Corpus . Proceedings from LREC 2012 , Istanbul, Turkey. Pedersen, , L. DanNet - the challenge of compiling a WordNet f or Danish by reusing a monolingual dictionary. Language Resources and Evaluation, Computational Linguistics Series. Volume 43, Issue 3, Page 269 -299. B.S, -Andresen, B. Maegaard (2012). The Danish Language in the Digital Age - danske sprog i den (2013 ). Semi -supervised learning and domain adaptation in natural language processing . Morgan & Claypool. Yimam, S.M., C. (2013): WebAnno: A Flexible, Web -based and Visually Supported System for Distributed Annotations . In Proceedings of ACL -2013, demo session V ossen, EuroWordNet, annotated corpus for Dutch . Proceedings of Electronic Lexicography in the 21st century: New Applications for new users ( eLEX2011 ), Slovenia. 28Semantic Annotation of Anaphoric Links Kiyong Lee Korea University Seoul 137-767, Korea ikiyong@gmail.com Abstract This paper attempts to integrate several existing coreference annotation schemes into an extended annotation scheme ASana. The proposed ASanaallows some other types of the anaphor-antecedent relation, called 'anaphoric link', than the canonical type of coreference that implies the referential identity between an anaphor and its antecedent. The structure of ASanaitself is very simple, consisting of a single entity type for mentions and a single anaphoric relation, each of which is characterized by a small set of attribute-value specications. Constrained by these specications, ASanasupports a two-step annotation procedure: For a given text T, (1) identication of a set of mentions Min the text Tthat refer to something in the universe of discourse referents as its markables , (2a) identication of a set of pairs of the mentions in Mthat are anaphorically related, and (2b) specication of the type of such a relation. anaphor, annotation, annotation scheme (AS), antecedent, coreference, discourse referent, markable, mention 1. Introduction There appeared a little gossip item in The Telegraph . (1)By Lucy Kinder 5:45 PM GMT 24 Feb 2014 head {pie charts }mt9to detail how much pain mt10shemt11had caused him mt12. {Graham Jones }mt13, 44, more come and used {a spatula }mt42to deliver them mt43. Crying and screaming, {Mrs Jones }mt44was left with bruising and reddening. This short news item contains at least 44 noun phrases (NPs) of various forms that mention or refer to something in the universe of discourse, a non-empty set of discourse referents (see Kamp (1981) and Kamp & Ryele (1993)). Each of these phrases, which are technically called 'men- tions', is uniquely identied with an integer and its prex mtthat stands for mention in the text.1 Most of the mentions are referentially grounded, namely re- ferring expressions, and some of them are also identied as having coreferentially related to others. To understand the whole story told by this news article, one should recognize these coreferential relations among the so-called mentions. The noun phrase A former head teacher mt1or the larger phrase A former head teacher of a primary school mt3and the name Graham Jones mt13, for instance, refer to the same person and so do the name Mrs Jones mt44and the noun phrase his wife mt6. 1Not every mention refers to an (individual) entity (e.g., no man).Each of such pairs that refer to the same entity in the discourse referents consists of two mentions, one called 'anaphor' and the other its 'antecedent'. They are also said to corefer or be coreferential. The antecedent of an anaphor may be split into more than one. The pronoun their mt25in the news article, for instance, has its antecedent split into two mentions, Jones mt21andhis wife mt23.2There are half a dozen oc- currences of pronouns such as he, his ,she, and herthat re- fer to either Graham Jones mt13orhis wife mt6and there is also a locative pronoun There mt37that refers to the kitchen mt36. The two expressions, an affair mt8andher relationship mt24, may also be understood to corefer, for they can be interpreted as referring to the same event or state of affairs, provided events are included as rst-class citizens among discourse referents. Reference or coreference resolution is a big issue in com- putational linguistics, especially in the area of information extraction ( IE). There have been several important publi- cations that deal with that issue: to cite some, we have: Hirschman & Chinchor (1997), Chen & Hacioglu (2006), Haghighi & Klein (2009), Haghighi & Klein (2010), Rah- man & Ng (2011), Stoyanov et al. (2010), Stoyanov & Eisner (2012), Ratinov & Roth (2012), and van Deemter & Kibble (2000). While referring to these works and others to be cited, this paper aims at constructing a semantic annotation scheme (AS) for coreference and other anaphoric link phenomena in a language (English) that may be proposed as an ISO standard for language resources management. First, this paper reviews the four existing ASs: (1) the TEI-based ASs, Bruneseaux and Romary (1997) and MMAX multi-level AS, and (4) Pustejovsky et al. (2013)'s ISO-Space AS. 2Pustejovsky et al. (2013) calls this a case of split antecedent, whereas Rullmann (2003) views this as an instance of multiple antecedent.29Second, this paper proposes to integrate all these ASs into a two-level AS: Given an input text, it species (1) ways of identifying the whole set of mentions as possible mark- ables and then (2) ways of (a) selecting possible anaphors from the set of possible markables, (b) pairing each of the selected anaphors with their antecedent(s), and (c) speci- fying the type of each anaphor-antecedent relation, called 'anaphoric' link. The second level is complex: it consists of three steps (a), (b), and (c). Each of these steps is con- strained and triggered by a small set of attribute-value spec- ications for each anaphor-antecedent pair of the mentions and each type of the anaphoric link. The rest of the paper develops as follows: Section 2 Review of Existing Annotation Schemes, Section 3 Specication of the Proposed Integrated Annotation Scheme, and Section 4 Illustrations, Section 5 Semantic Interpretations, and Sec- tion 6 Concluding Remarks. 2. Review of Existing Annotation Schemes 2.1. Preliminaries There are several types of the anaphoric link. The best known type is coreference, an equivalence (symmetric, transitive, and reflexive) relation, that holds between two terms, called 'mentions', if and only if their denotations are identical. (2) Two terms t1andt2corefer iff [|t1]M=[|t2|]M, where [|ti|]Mis the denotation or referent of a term ti with respect to a model M. Coreference and binding are two different, but related lin- guistic phenomena, often discussed together. Consider: (3) a. John loves hismother. [coreference] b.Everyone loves hismother. [binding] In (a), the pronoun hismay be understood as coreferring with the name John as its antecedent. In (b), on the other hand, the pronoun hisdoes not corefer with the quantied noun phrase everyone , but is treated in formal semantics as a variable bound by the universal quantier. This paper attempts to accommodate both the type of coreference, as dened in (2), and some other types of anaphoric phenom- ena into the proposed ASana. 2.2. Coindexing In linguistics, coreference and binding are both annotated in the same manner by coindexing, as shown below. (4) a. John iloves his imother. [coreference] b. Everyone iloves his imother. [binding] Split antecedents can also be represented by coindexing with a set index such as {x, y}. Here are some examples, taken from Rullmann (2003), (5a,b,c): (5) a. Mary 1told John 2that they {1,2}should invest in the stock market. b. Every woman 1told [her 1husband] 2that they {1,2} should invest in man 1told [each of his 1girlfriends] 2that they {1,2}were going to get married. Coindexing is not, however, expressively powerful enough to mark up details of anaphoric relations. The treatment of reciprocal pronouns is one of such cases. (6) a. They ilove each other i. b. If everyone iwere to love one another i, then theyi wouldn't want me to make a sacrice, ... More than mere coindexing is called for an adequate inter- pretation of the anaphoric phenomena shown here. 2.3. The TEI-based Annotation Schemes There are two almost identical ASs for coreference: Brune- seaux and Romary (1997) and TEI P 5 (2014). They are both based on XML and also on theTEIGuidelines . They differ from each other mainly because Bruneseaux and Ro- mary (1997) followed a much earlier version of the TEI Guidelines , while TEI P 5 (2014) is the most recent version, updated January 2014. 2.3.1. Bruneseaux and Romary (1997) The AS, proposed by Bruneseaux and Romary (1997), consists of two ( XML) elements: <rs> for referring strings and <link> for coreference. For each of the two elements, we can specify their associated at- tributes and possible values:3: (1) Attributes for the Element <rs> :attributes = type, key; type = \"object\"; key = ID;and (2) Attributes for the Element <link> :attributes = type, targets; type = \"coref\"; targets coreference relations may not preserve the original identity. Here the live chicken 01was killed and cut into four pieces 04and became roasted chicken 05.5 3As a specication language, we adopt ISO 14977 (1996) Ex- tended BNF (Backus-Naur Form) that can be converted into its document type denition ( DTD). 4Bruneseaux and Romary (1997) annotated a French version of the original English example (16) that occurs in Brown and Yule (1983), page 202. 5The original annotation of the French version given in Brune- seaux and Romary (1997) was modied here to suit the English example.302.3.2. TEI P 5(2014) TEI P textual segments: (8)<title xml:id=\" SHIRLEY \">Shirley</title>, which made its Friday night debut only a month ago, was not listed on <name xml:id=\" NBC\">NBC</name>'s \">the show</seg> still is being considered. In this textual fragment, the name Shirley is annotated as the title of a show being broadcast over NBC, a television network. The text contains no pronominal forms, but the two nominal forms, the show andthe network , are under- stood as corresponding to the two names Shirley and NBC, respectively. The annotation of the two segments, namely those nominal forms, right above introduces the attribute @corresp to indicate such a coreferential relation for each of the two. As shown below, the use of the elements <linkGrp> and <link> makes instances of the anaphoric link involving anaphors and their antecedents. The attribute @target has two arguments, as speci- ed by the attribute @targFunc : the rst argument is antecedent and the second anaphor . The element <linkGrp> allows several instances of the anaphoric link to be grouped together, while simplifying the specication of the element <link> with a single attribute @target Task Denition Hirschman & Chinchor (1997)'s CDT lists four purposes of constructing an AS in the order of their importance. The rst two are: (1) to support the MUC (Message Understand- ing Conference) information extraction tasks and (2) to be able to achieve good (ca. 90%) inter-annotator agreement. Creation of a corpus for research on coreference and dis- course phenomena is the last goal. 2.4.1. Markables Hirschman & Chinchor (1997)'s CDT restricts the set of its markables to nouns, that is, names, noun phrases, or pro- nouns6. Noun phrases include dates ( January 23 ), currency expressions ( $1.2 sive, demonstrative, and reflexive pronouns are markables. So are the rst person pronouns. The interrogative pro- nouns ( who, which engine ) are not markables. 6All of the examples given in subsection 2.4 are copied from Hirschman & Chinchor (1997).In Hirschman & Chinchor (1997), verbs and other ver- bal forms such as gerunds ( Slowing the economy ) are not markables.7Implicit pronouns, is, null anaphora ( Billi called John and eispoke with him for an hour. ) and pre- sumptive or intrusive pronouns ( the movie iwhich I saw ti)8as well as relative pronouns (complementizers) are not treated as markables. 2.4.2. Extents The extent of a markable is a maximal string, while its head is marked with an attribute MIN(minimal string). The max- imal noun phrases thus include their modiers, appositional phrases, non-restrictive relative clauses, and prepositional phrases ( Fred Frosty, the ice cream is not re- stricted to referential identity. Here is the general principle for annotation coreference that they proposed: (10) Two markables are coreferential if they both refer to sets, and the sets are identical, or they both refer to types, and the types are identical. This principles thus allows the possible coreferentiality be- tween bound anaphora and quantied NPs that are their an- tecedents. Here are examples for the bound anaphoric relation that are treated in Hirschman & tomorrow. 2.4.4. An <COREF >for the annotation of markables and also of their coreferential link type IDENT with the following specica- tion of attribute-values: (12) List of Attributes and Possible Values for <COREF > ID=INTEGER ; MIN =CDATA ;{* Head of the whole extent * } REF=IDRef;{* Antecedent * } TYPE =IDENT ; STATUS =OPT;{* if the reader is uncertain about the iden- tity relation.* } 7The phrases program trading, excessive trading, slowing of the economy are noun-like, so they are treated as markables. 8See Cooper (1979), Evans (1977), Evans (1980), and Wech- sler (2006) for detailed discussions of interpreting various uses of pronouns and Sells (1984) for presumptive and intrusive pro- nouns. 9Each extent is marked with a pair of stars (*) in Hirschman & Chinchor (1997), but these stars are replaced with curly brackets in this paper.312.4.5. Illustrations Here are some illustrations, copied from Hirschman & (1997): (13) <COREF of Education\">Board of Education</ COREF > budget is just too high, the Mayor said <COREF ID =\"102\" STATUS =\"OPT\"TYPE=\"IDENT \" REF=\"102\">Livingstone Street</ COREF > has lost control. Here, the reader is uncertain about the IDENT relation of the Board of Education and Livingstone Street, although they are locally identical. That is why STATUS =\"OPT\"is introduced into the annotation. (15) <COREF ID =\"1\" MIN=\"boys and girls\">The sleepy COREF > breakfast. Conjoined noun phrases treated as one extent. (16) <COREF ID =\"5\">Fred</ COREF > resigned be < The chain of coreference links <ID=\"5\", ID=\"6\", ID=\"7\"> is cut off by not the in<COREF > The entire string every man who knows his own mind and the pronoun hisare annotated as coreferring with IDENT . There are many other illustrations, for instance, for the annotation of apposition, predicate nominals and time- dependent identity, types and tokens, functions and values, and metonymy. 2.5. The MMAX2 Multi-level Annotation Scheme Sch\u00a8afer et al. uses & (2006)'s GUI- based MMAX 2 annotation tool for coreference resolution to build a fully coreference-annotated large corpus of 266 scholarly papers from the ACL anthology. Here we briefly introduce the MMAX 2 coreference AS.2.5.1. Markables Only proper names, noun phrases, and pronouns are mark- ables, called possible entity (1) def-np (denite NPs), (2) pper (personal pro- nouns), (3) ne (proper names including citations), (4) ppos (possessive this AS who, which, whose, excludes bound anaphora ( {Every van Deemter & Kibble (2000) and their two other related works, Kibble & van Deemter (1999) and Kibble & van Deemter (2000), this AS differentiates coref- erence from other types of the anaphoric link. It suggests that the annotation of coreference proper be separated from other tasks such as annotation of bound anaphors and of the relation between a subject and a predicative NP. It calls for a division of labor that achieves better inter-annotator agreement. 2.6. The Brandeis ISO-Space Annotation Guidelines The ISO-Space Working Group at Brandeis University pro- duced a manual of annotation guidelines for spatial infor- mation (Pustejovsky et al., 2013) and proposed it as an annex of ISO-Space (2013), an ISO international standard for spatial annotation. First, the 2013 version of this an- nex annotates spatial entities ( I am sitting in the carse.)10 as referring expressions. Second, it introduces an ele- ment <metaLink> to annotate the three different types of coreference between these spatial entities: (1) corefer- ence, (2) subcoreference, and (3) split coreference. Third, it species a set of attributes and their possible values for the element <metaLink> , as shown below: (18) Attributes and Possible Values [object ID1], [object ID2], implied. *} id = \"meta\" INTEGER ; object ID1 \"splitCoref\"; comment = CDATA ; Instead of using XML as its representation language, Puste- jovsky et al. (2013) adopts the predicate-logic-like forms to represent the three different types of coreference. Here are examples: (19) a. {Two cars }se1are on the street. One se2of them se3 turns left. 10A spatial entity is introduced into ISO-Space (2013) as an element of a type of entity that participates in location-involving motions or non-motion events. Its identier is marked can be interpreted as stating that the referents of two spatial entities cars se1andthem se3are identical, while onese2partially corefers with the spatial entity type expressions cars se1. Here is another example: (20) a. {John se6and Mary se7}se8met at the the names John se6andMary se7are each treated as a referring expression. At the same time, the whole phrase {John se6and Mary se7}se8as a group is also treated as a referring expression. And then the antecedent of the plural pronoun They se9is split into two: John se6andMary se7. 3. Specication of the Proposed Integrated Annotation Scheme 3.1. Overview Given an input text, the task of coreference or other anaphoric link annotation is three-fold: (1) identication of a set of mentions in the text that refer to something in the domain of discourse referents as its markables , (2a) iden- tication men- tions that are anaphorically related and (2b) specication of the type of such a relation. To trigger and constrain these annotation steps, the entity type of mentions and the anaphoric relation are assigned a set of required or implied attribute-value specications. (2a) and (2b) constitute two sub-processes unied into one, for they depend on each other. The input text can be of any size. It can range from a short sentence to a very large corpus. 3.2. Identifying Markables and Extents The set of possible markables consists of terms or men- tions, which comprise both referring and non-referring ex- pressions in a text. As attested quantitatively by various reference resolution experiments such as Chen & Ha- cioglu (2006), Haghighi & Klein (2010), Stoyanov et al. (2010), and Raghunathan et al. (2010), these mentions are mostly noun phrases of the following four forms: (1)proper names, (2) denite or indenite nominals with plu- rality and other agreement specications, (3) (generalized) universal or existential quantiers, and (4) denite or in- denite pronouns reflexives, reciprocals, and demonstratives.11 The list of these features just species what morphosyn- tactic features are required or implied for the identica- tion of mentions. The annotation of these features could be done at earlier stages of annotating raw data such as to- kenization and morphosyntactic annotation. The process, whether manual or automatic, of marking up these men- tions as markables should be straightforward at this basic level. 3.3. Anaphoric Links The main task of annotating coreference and other types of the anaphoric link is to recognize antecedent-anaphor pairs among the set of markables and also to identify the type of their anaphoric link. 3.3.1. Anaphor-antecedent Pairs Anaphors are part of the set of mentions, being mostly pro- nouns and other pronominal forms (see Keenan (1993a)). They are thus easily identied. (21) a. Bob loves Jane, but shedoesn't love him. b. Bob was tired, and sowas I. Some denite noun phrases can be anaphors, too. Here are some examples: (22) a. {The project leader }iis refusing to help. {The jerk }ithinks only of himself. b.{Hilary Clinton }i,{Bill's wife }i. Among the list of pronouns, we may also include the use ofitreferring to propositions, facts, actions, etc., or the use ofsothat may involve so-called sloppy identities, as shown below: (23) a. John said {he has been to heaven }i, but I don't believe {it}i. b. John {loves his wife }iand{does}i?Bob. Examples such as these are often discussed in linguistic literature, but have been seldom treated in computational work. The so-called expletive itandthere , the complementizer that, and the impersonal use of the pronoun it, as shown below, are excluded from the list of possible anaphors as well as from the list of possible markables. (24) Itexp's impossible to go out now, for itimp's raining cats and dogs. Itexpis also reported thatcomp there exp is a storm approaching from the south. The identication of anaphors as well as mentions can also be triggered by the morphosyntactic features of markables. 11Interrogatives are excluded.333.3.2. Types of the Anaphoric Link Unlike anaphors, antecedents can be of any class of a word, phrase, or clause. It should, however, be a subset of mark- ables as specied by the rst step of annotation. If ver- bal forms are excluded from the set of markables for some practical reasons, then they would not be in the set of pos- sible anaphors or antecedents. The extent of antecedents is not restricted to a single word or phrase, but may extend to larger phrases such as con- joined phrases: (25){The boys iand the girls j}kmet at a party and they k danced all night. Antecedents may not be contiguous, either, but split into two or more phrases, as in: (26) I imet{a farmer }jand{hisjdog}kand we {i,j,k}all walked together. There are at least two uses of pronouns: (27) a. anaphoric: John iloves his iwife. b. indexical or deictic: Look at him 1. He 1is naked. Context: the speaker pointing to a person over there. In the anaphoric use, the pronoun hisinds its antecedent John iin the given text. In the indexical use, the antecedent of the pronoun him 1is not found in the text, but provided contextually. Pronouns can be antecedents as well as anaphors in the chain of an anaphoric link. (28) John i1loves {hisi2wife}jand she jalso loves him i3. Anaphoric links may be forward or backward. The term that corefers with a pronoun normally precedes it, thus be- ing called 'antecedent'. This so-called antecedent may also come after its related anaphor, as in: (29) When sheireturned home, Sue iwas surprised to nd her dog gone. In such a case, the pronoun is often called 'cataphor'. Sometimes it is difcult to decide which is an anaphor and which is its antecedent, as especially in appositive cases (Seoul i,{the capital of South Korea }j, where iandjcore- fer.). In such cases, we simply have to state that they corre- spond to or corefer with each other. The antecedent-anaphor relation is normally a one-to-one relation, but there are cases in which the antecedent of an anaphor is split into many. Besides this case of split coref- erence, Pustejovsky et al. (2013) lists subCoreference as another type of coreference: (30) I have {two cars }i, but one jof them ibroke down. Here onejis a member of the set of two cars i.3.4. Formal Description Bunt (2010) provides a formal description of the annota- tion structure, consisting of two levels of syntax: one is an abstract level of an annotation, called 'abstract syntax', and another, a concrete level of representing annotations, called 'concrete syntax'. Every abstract syntax for seman- tic annotations must be supported by an explicit (formal) semantics. An XML-serialization of an abstract syntax is an instance of a concrete syntax. The semantics of a concrete syntax is dened as the semantics of the abstract syntax for which it denes a concrete representation. (Different rep- resentations of the same abstract syntax thus have the same semantics.) 3.4.1. Abstract Syntax The abstract syntax of an annotation scheme consists of two parts: (1) a conceptual inventory, that species the ba- sic concepts from which annotation structures are built up; (2) a specication of the possible ways of combining el- ements of the conceptual inventory into annotation struc- tures. An annotation structure is a set consisting of two kinds of elements: entity structures andlink structures . En- tity structures provide linguistic information about a region of primary data; link structures provide information about the semantic relation between regions of primary data. In the case of annotating coreference and other anaphoric link types, entity structures correspond to the entities that are re- lated by anaphoric links, and link structures to the linkings of anaphoric expressions to their antecedents. An entity structure is a pair m, awhere mis a markable that identies a region of primary data, and ais the speci- cation of the semantic information that the annotation pro- vides about that region of primary data. In the annotation scheme ASanafor coreference and other anaphoric link types the acomponent of an entity structure is an n-tuple, 3n6consisting maximally of a semantic type t, a deniteness d, a morphosyntactic form f, a natural gender g, a plurality p, and a collectiveness c(more about these el- ements below). The fact that the length nof these n-tuples may vary, reflects the optionality of some of the elements. A link structure is a triplet 1, 2, rconsisting of two en- tity structures (for anaphor and antecedent) and a relation corresponding to the type of anaphoric link between them. For the abstract syntax of the annotation scheme ASana the conceptual inventory is a 9-tuple M, T, D, F, G, P, C, Q, R , where (1) Mis a non- empty set of markables, (2) Tis a set of semantic types; (3)Dis a set of deniteness values; (4) Fis a set of morphosyntactic forms; (5) Gis a set of natural genders; (6)Pis a set of singular/plural values; (7) Cis a set of 'collectivity values'; (8) Qis a set of generalized quantiers and (9) Ris a set of binary relations over the set of entity structures, corresponding to the various types of anaphoric links. The annotation structures are dened by an assignment @that species the semantic components of entity structures. For each markable min M,@(m)generates n-tuple, 3n7, of of morphosyntactic forms, and several of343.4.2. Concrete Here is an XML-based concrete syntax ASanX, corre- sponding to the abstract syntax of the proposed anno- tation scheme ASana. First, it introduces two ele- ments <entity> and<anaLink> that correspond to entity structures and link structures, respectively, as de- ned in ASana. Both of these XML elements have an @identifier attribute in order to allow references from within the representation of a certain link structure to the representations of specic entity structures or other link structures. Moreover, <entity> structures have a @target attribute for representing the markables that they associate linguistic information with. Second, the assign- ment @forMcan be transduced into ASanXas below: (31) Attributes and Values for the element <entity> attributes = identifier, quant = CDATA; comment = CDATA; By allowing the value quant as a possible value for the attribute form ,ASanXtreats the anaphoric link between a quantier and a bound anaphor (e.g., pronoun). It does not treat reciprocals (e.g., each other, one another ). (32) Attributes and Values for the element <anaLink> attributes = identifier, a star. *} antecedent = IDRef*; {*This allows multiple antecedents. The indexical use of a pronoun may not have an antecedent in the element <entity>. *} type = \"ident\", \"partIdent\", \"setIdent\", \"qBound\"; comment = CDATA ; the other elements may refer to other levels of annotation. 13Attributes in square brackets are optional or implied. 14The identier is tagged xml:id extent IDin a tokenized source text or the extent itself as its value. This value can be a (possibly null or non-contiguous) sequence of tokens or their IDs. 16Verbal forms including sentential or adjectival forms are ex- cluded. 17Optional attributes have a value 'unspecied' as default.The attribute @type introduces values other than ident for referential identity. These values allow the types of the anaphoric link other than the type of coreference proper. The use of each of the values of the attribute @type is illustrated below: (33) a. ident : referential individual-level identity; John 1loves Jane 2, but she 2dislikes 1. b.partIdent : referential partial identity; John cars }i. i1of them ibroke down. c.setIdent : set or group-level identity between an anaphor and its identity; {Every farmer }1owns a donkey. They 1beat it. {The whole army }2surrendered themselves 2. d.qBound : case of bound anaphors; Every xfarmer loves his xwife. The set-level anaphoric identity assumes that the denotation of an anaphor is a set and also that that set is also the de- notation of its antecedent so that they are identical as sets. For example, the denotation [|every farmer |]Mofevery farmer with respect to a model Mis understood to be a set {X|[|farmer |]MX}of supersets of the set of farmers. We introduce <isoAna> as the root element for XML doc- uments in the concrete XML annotation scheme ASanXfor coreference and other types of anaphoric link. 4. Illustrations Here is a segment of the news item given earlier. The pro- posed ASanXcan annotate it, as shown below: (34) a. the are well-known donkey sentences: (35) a. Every farmer who owns a donkey beats it.18 b. If Pedro owns a donkey, he beats it. Example (b) can be annotated as below: (36) a. If and<it,a donkey >are treated as coreferring. Note that indenite descriptions are treated as referential terms, not existential quantiers (see Kamp (1981).) 5. Semantic Interpretations As stated earlier, every semantic annotation must be ac- companied by an explicitly dened semantics. The use of the lambda calculus in the line of Montague (1974) or that of the discourse representation structures (DRSs), proposed by Kamp (1981) and Kamp & Ryele (1993), can, for instance, be linked to the abstract syntax to provide such a semantics for semantic annotations. Attempts have been made by Katz (2007), Pratt-Hartman (2007), Bunt (2007), Bunt & Overbeeke (2008a), Bunt & Overbeeke (2008b), Lee (2008) to develop an annotation-based se- mantics with the use of lambda calculus or by Bunt (2010) with the use of DRSs . The use of lambda abstraction has run into the problem of complexity especially in dealing with multiple quantication and embedded adjunct struc- tures. This should be the case with the treatment of var- ious anaphoric phenomena. There are at least two inter- esting works to overcome this complexity problem: One 18Originally, from Geach (1962).is an earlier work by Muskens (1996) which proposed a way of combining Montague semantics with DRSs and another is the most recent work by Bunt (2014) which directly addresses to the treatment of anaphoric phenom- ena by combining underspecied representation ( USR) that arises because of the presence of context-dependent expres- sions such as pronouns with representation of annotation in- formation ( AIR). In constructing these representation struc- ture, Bunt (2014) shows how useful and necessary it is to combine the introduction of discourse referents in DRSs with markables in the annotation into USR and AIR, es- pecially when there are multiple occurrences of identical anaphoric expressions, that is, pronouns, in a text. We leave detailed discussion of ways of interpreting anaphoric links as a work item for the future. Here are some remarks on the interpretation of various anaphoric ex- pressions. First, names and denite descriptions are refer- ential terms, both referring to some unique entities in the domain of discourse referents. Indenite descriptions are also treated as referential terms, as mentioned earlier. Second, as proposed and discussed in formal model- theoretic semantics (see Montague (1974), Barwise and Cooper (1981), Link (1987), and Keenan & Westerstahl (2010)), proper names, denite descriptions, indenite sin- gular dog ) or bare plural ( donkeys ) noun phrases as well as quantied noun phrases ( three students, every man ) are also interpreted as referring to sets of sets or properties, in the world. In our treatment, universally quantied expres- sions are differentiated from other types of generalized, but existentially quantied expressions. Third, pronouns, on the other hand, do not refer directly to any entities in the world, but only through being coref- erential with some other terms in the text (anaphoric use) or by referring to some entities that are provided contex- tually in a discourse situation (indexical use) (see Keenan (2007)). Nevertheless, pronouns are also marked up as re- ferring expressions or mentions in coreference annotation (see Cooper (1979), Evans (1977), Evans (1980).) 6. Concluding Remarks The purpose of this paper has been to integrate several ex- isting ASs for anaphoric links into a unied ASanathat may be accepted as an ISOstandard for language resources management. One big issue in designing ASanais a choice between theoretical granularity and practical sustainability. If an AS is theoretically ne-grained, then the range of its applications may be wider, provided that various conditions of its sustainability are guaranteed such as the ease of its use with a high score of inter-annotator agreement and the cost- effectiveness of developing language resources through its use, as mentioned in Hirschman & Chinchor (1997). The aspect of granularity here mainly concerns (mor- phosyntactic) forms of anaphors and types of anaphoric link. The pronoun hisin Examples (3a,b), for instance, is treated as a typical anaphor with a reasonable claim that ev- ery pronominal form is an anaphor. The name John and the quantier Everyone are easily recognized as their respec- tive antecedents. A question now is whether their anaphoric links are of the same type or not. The proposed ASana treats them both as instances of the anaphoric link, but of36different types, without elevating the notion of coreference from the level of referential identity to that of set-identity or type-identity as in Hirschman & Chinchor erence as referential identity, ASanacan easily modify its scheme in the line of van Deemter & Kibble (2000) with a division of labor. 7. Acknowledgments Partially supported by the ID R&Dprogram of MSIP /KEIT No. 10044457 \"Development of Autonomous Intelligent Collaboration Framework for Knowledge Bases and Smart Devices\". I owe thanks to Harry Bunt for his great sug- gestions and also to Roland H. Hausser, Hwan-Mook Lee, and Do-sam Hwang for reading the pre-nal version of this paper with encouraging comments. 8. References Muskens, Reinhard. 1996. Combining Montague seman- tics and Phi- losophy 19: 143-186. Barwise, Jon, and Robin Cooper. 1981. Generalized quan- tiers and natural language. Linguistics and Philosophy , 4: 159219. Brown, Gillian, and George Yule. 1983. Discourse Analy- sis. Cambridge University Press, Cambridge. Bruneseaux, Florence, and Laurent Romary. 1997. Codage des r DHM . ACHALLC'97 semantic annotation. InProceedings of the 21st Pacic Asia Conference on Language, Information, and Computation (PACLIC-21), pp. 13-29. Korean Society for Language and Informa- tion, Seoul. Bunt, Harry. (2010). A methodology for designing seman- tic annotation languages exploiting syntactic-semantic iso-morphisms. In: A. Fang, N. Ide and J. Webster (eds.), em Proceedings of ICGL 2010, the Second International Conference on Global Interoperability for Language Re- sources, pp. 29-45. City University of Hong Kong, Hong Kong. Bunt, Harry. 2011. Introducing abstract syntax + seman- tics in semantic annotation, and its consequences for the annotation of time and events. pp. 157-205. In Eunry- oung Lee and Aesun Yoon (eds.), Trends in Lan- guage and Knowledge Processing . Hankookmunhwasa, Seoul. Bunt, Harry. 2014. Annotations that effectively contribute to semantic interpretation. In Harry Bunt, Johan Bos and Stephen Pulman (eds.), Computing Meaning , vol. 4, 49-69. Springer, Berlin. Bunt, Harry, and C. Overbeeke. 2008a. An extensible, compositional semantics of temporal annotation. In Pro- ceedings of LAW-II: The Second Linguistic Annotation , LREC-2008. Marrakech. Bunt, Harry, and C. Overbeeke. 2008b. Towards formal interpretation of sematnic annotation. In Proceedings of the 6th Edition of LREC (Language Resources and Eval- uation Conference) 2008. Marrakech.Burnard, Lou and Syd Bauman (eds.). 2014. TEI P 5: Guidelines for Electronic Text Encoding and Inter- change , Version2.6.0. Last updated on 20th January 2014, revision 12802. Text Encoding Initiative Consor- tium, Charlollotesville, V A. Chen, Ying, and Kadri Hacioglu. 2006. Exploration of coreference resolution: The ACE entity detection and recognition task. In Petr Sojka, Ivan Kope[ c]ek, and Karel Pala (eds.), Conference, TSD 2006 , LNAI 4188, pp.301-308. Cooper, Robin. 1979. The interpretation of pronouns. In Frank Heny and Helmut S. Schnelle (eds.), Syntax and Semantics 10: Selections from the 3rd Groningen Round Table , 61-92. Academic Press, New York. Evans, Gareth. 1977. Pronouns, quantiers, and relative clauses (I). The Canadian Journal of Philosophy , 7.3, 467-536. Evans, Gareth. 1980. Pronouns. Linguistic Inquiry 353375. Geach, Peter. 1962. Reference Generality . Cornell University Press, Ithaca, N.Y. Haghighi, Aria, and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic Features. Proceedings of the 2009 Conference on Empirical Meth- ods in Natural Language Processing , pages 11521161. Singapore, 6-7 August 2009. ACL and and Klein. 2010. Coreference resolu- tion in a modular, entity-centered model. In Proceedings of HLT '10 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics , 385-393. denition, version 3.0. Updated 13 July 1997. ISO/IEC JTC 1 Information Technology. 1996. ISO/IEC 14977:1996(E), CD 24617-7 resource management - Semantic annotation framework - Part 7: Spatial information (ISO-Space) . The Interna- tional Organization for Standardization, Geneva. Kamp, Hans. 1981. A theory of truth and semantic repre- sentation. In Formal Methods in the Study of Language , part 1, 277-322. MC Tract 135. Stichting Mathematisch Centrum, Amsterdam. Kamp, Hans and Uwe Reyle. to Logic . Kluwer, Dordrecht. Karttunen, Lauri 1969. Pronouns and variables. In Robert Binnick, Alice Davidson, Georgia Green, and Jerry Mor- gan (eds.), Proceedings from the fth regional meeting of the Chicago Linguistic Society , pp. 108-116. University of Chicago Department of Linguistics. Katz, Graham. 2007. Towards a denotational semantics for TimeML. In F. Schilder, Graham Katz, and James Pustejovsky (eds.), Annotation, Extraction, and Reason- ing about Time and Events , 88-106. Springer, Dordrecht. Keenan, Edward. 1993a. Identifying anaphors. In J Guenter, B. Kaiser, and C. Zoll (eds), Proceedings of37BLS 19 , 503-516. Berkeley Linguistics Society, UC Berkeley. Keenan, 1993b. asymmetry. In Utpal Lahiri and Zachary Linguistic Theory , III: 117-144. Dept. of Modern Languages and Linguistics, Cornell University. Keenan, Edward. 2007. On the denotations of anaphors. Research on Language and Computation 5.1:5-17. General- ized quantiers in linguistics and logic, In Johan van Benthem and Alice ter Meulen (eds.), Handbook of Logic and Linguistics , 2nd rev. ed., 859-910. Springer, Berlin. Kibble, Rodger, and Kees van Deemter. is what should coreference annotation be? Proceedings of ACL workshop on Coreference and its applications . University of Maryland, June 1999. 90-96. Kiyong. 2008. Formal for interpreting tem- poral annotation. In Piet van Sterkenburg (ed.), Unity and Diversity of Languages: Special Lectures for the 18th International Conference of Linguists pp. P. G \u00a8ardenfors (ed.), Generalized Quantiers: 151-180. Reidel, Montague, Richard. 1974. The proper treatment of quan- tication in ordinary English. In Richmond Thomason (ed.), Formal Philosophy: Selected Papers of Richard Montague . Yale University Press, New Haven. M\u00a8uller, Christoph, and Michael Strube. 2006. Multi-level annotation of linguistic data with MMAX2. In S. Braun, K. Kohn, and J. Murkherjee (eds.), Corpus Technology and Language Pedagogy: New Resources, New Tools, New Methods , pages 197-214. Peter Lang, Frankfurt a.M.. Pratt-Hartman, Ian. From TimeML to Interval Tempo- ral Logic. In Proceedings of the Seventh Interna- tional on Computational The ISO-Space Working Group, Brandeis University. Raghunathan, K., H. Lee, S. Rangarajan, N. Chambers, M. Surdeanu, D. Jurafsky, and C. Manning. 2010. A multi- pass sieve for coreference resolution. In EMNLP. Rahman, Altaf, and Vincent Ng. 2011. Coreference res- olution with world knowledge. In Proceedings of the 49th Annual Meeting of the Association for Compu- tational Language Technologies (ACL-HLT) based multi-sieve co-reference resolution with knowl- edge. Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processingand Computational Natural Language Learning , pages 12341244. Agbayani, Paivi Koski- nen, and Vida Samiian (eds.), Proceedings of the Western Conference on Linguistics: WECOL 2002 . Department of Linguistics, California State University, 243-254. Sch\u00a8afer, Ulrich, Christian Spurk, and J coreference-annotated corpus of scholarly pa- pers from the ACL Anthology. Proceedings of COLING 2012: Posters , pages 10591070. Sell, Peter. 1984. Syntax and semantics of resumptive pronouns . Doctoral thesis, University of Nathan David Hysom. 2010. Corefer- ence resolution with Reconcile. Proceedings of the ACL 2010 Short Papers , pages 156-161. Stoyanov, Veselin, 2012: Technical Papers , pages 2519-2534, COLING 2012, Mumbai, December 2012. Soon, Wee Meng, Daniel Chung Yong Lim, and Hwee Tou Ng. 2001. A machine learning approach to coreference resolution of noun phrases. van Deemter, pp. 615-623. Wechsler, Stephen. 2006. Why G & Between 40 and 60 Puzzles for Krifka . Centre for General Linguistics, Typology and Universals Research ( ZAS), Berlin.38Towards Extending with Zulu Morphosyntax Laurette Pretorius, Sonja Bosch University of South Africa Box 392, UNISA, Pretoria, South Africa 0003 pretol@unisa.ac.za, boschse@unisa.ac.za Abstract The importance of the semantic annotation of morphological data for agglutinating languages is the departure point of this paper. It discusses the principled extension of the ISOcat data category registry (DCR) to include Zulu morphosyntactic data categories. The focus is on the Zulu noun. Where existing data categories are found appropriate they are used and where new additions are required the published guidelines are followed. The expectation is that these extensions will also be useful for languages that are related to Zulu and share its morphosyntactic structure. The inclusion of the other Zulu word categories forms part of future work. Keywords: ISOcat, data categories, Zulu, morphosyntax, semantic interoperability 1. Introduction Our point of departure is the increasing emphasis on the se- mantic interoperability of linguistic data, and more specif- ically, morphological data. In languages with agglutinative morphologies much information, both syntactic and seman- tic, is encoded in the morphology of words. The accurate annotation of this information is often key to the reliability of language processing technologies, tools and applications for these languages and for their interoperability with other languages and, for example, in the Semantic Web. Interoperability can be dened as a measure of the degree to which diverse systems or language resources are able to work or be used together to achieve a common goal (Ide and Pustejovsky, 2010). Interoperability is typically dened semantic interoperability. \"Whereas syntactic interoperability provides for the ex- change of clearly dened classes of data, semantic inter- operability enables the automatic recognition of the indi- vidual data exchanged.\" Also in linguistics, \"syntax refers to the grammar and formal rules for dening sets of data, while semantics dene the meaning and the use of these data. In other words, on the semantic layer data becomes information\" (Kubicek et al., 2011). This is also true for morphological data. In the project, reported on in this paper, we follow a princi- pled approach to the creation of data categories to facilitate the syntactic and semantic interoperability of morphologi- cal information of the Bantu language family. For this pur- pose we use the ISOcat Data Category Registry1(DCR), based on the ISO 12620 standard. Indeed, ISOcat may be considered a key resource and a de facto standard in its own right. In terms of language we focus on Zulu, an agglutina- tive Bantu language spoken in Southern Africa. As a rst step towards future syntactic and semantic anno- tation it is nec- essary to ensure that all the relevant linguistic concepts, re- ferred to as data categories, occurring in Zulu morphology are available in ISOcat. This paper therefore reports on a 1ISOcat data category registry, http://www.isocat. org/rst attempt to extend the ISOcat DCR for Zulu by restrict- ing our attention to the Zulu noun and its morphological structure. The structure of the paper is as follows: Section 2 briefly introduces ISOcat and shows how it facilitates syntactic and semantic annotation and interoperability of linguistic data. Since our focus is on the Zulu noun, section 3 provides a short exposition of Zulu noun morphology. Section 4 rep- resents the core contribution. It discusses the proposed Zulu morphosyntax extension to the ISOcat DCR. Section 5 con- cludes the paper by sharing acquired insights, discussing further extensions to the ISOcat Zulu morphosyntax and identifying future work, also for other related languages. 2. ISOcat DCR and interoperability The main aim of the ISOcat DCR is to dene widely ac- cepted linguistic concepts in a stable and persistent way. Each concept is assigned a so-called persistent identier (PID) in the form of a cool Uniform Resource Identier (URI). It provides a \"framework for dening data cate- compliant with the ISO/IEC 11179 family of stan- dards. According to this model, each data category is as- signed a unique administrative identier, together with in- formation on the status or decision-making process asso- ciated with the data category. In addition, data category specications in the DCR contain linguistic descriptions, such as data category denitions, statements of associated value domains, and examples. Data category specications can be associated with a variety of data element names and with language-specic versions of denitions, names, value domains and other attributes.\" (ISOcat, nd). Although data categories are stored as a flat list, there is the option of creating customized registry instances for specic sub- disciplines of interest. This is achieved through the so- called Data Category Selections (DCSs). Interoperability is achieved when users annotate their data with references to registered concepts, thereby allowing others to interpret their data. Interoperability is further en- hanced by ensuring that there is the minimum of duplica- tion in the registry. The ISOcat DCR makes it possible to39work across projects, disciplines and languages by provid- ing a mechanism to make the semantics of different tag sets explicit through referencing of registered ISOcat concepts. 3. Morphology of the Zulu noun The morphological structure of the Zulu noun is charac- terised by a nominal classication system that categorises nouns into a number of noun classes, as determined by pre- xal morphemes also known as noun prexes. These noun prexes have, for ease of analysis, been divided into classes with numbers by scholars who have worked within the eld of the Bantu language family. Table 1 shows examples of Meinhof's (Meinhof, 1932, 48) numbering system of some of the noun class prexes: Prex Class Word form English of noun class prexes Noun prexes usually indicate number, with the uneven class numbers designating singular and the corresponding even class numbers designating plural. However, this is not always the case, since some nouns in so-called plural classes do not have a singular form; plurals of class 11 nouns are found in class 10, while a class such as 14 is not associated with number at all. The noun prex typi- cally constitutes two parts, namely a preprex (the initial vowel) and a basic prex, but in some classes such as 1a and its plural class 2a a basic prex does not feature. In other instances such as classes 11 and 14 the basic prexes are often discarded, with the result that only the preprex appears in the surface form. Other morphemes that may be sufxed to the noun in Zulu are the diminutive, augmentative and feminine, or combi- nations thereof: Diminutive nouns are usually formed by the sufxa- tion of a diminutive sufx (\"time\") >isikhashana nouns ation of an augmentative (\"stone\") >itshekazi Feminine nouns are sometimes sufxa- tion of a feminine diminutive and feminine: A femi- nine noun formed by the sufxation of a diminutive sufx -kazi or-azimay be followed by having given birth\"). Other examples of the use of these sufxes are as follows: Diminutives addi- tional greatness). Feminine sufx with adjective stem added to the ad- jective stem -deto bring about harmony with feminine that are explicit in the morphological analysis ofizimvukazana are shown in Table 2. Morpheme Syntactic concept Semantic concept izin- Prex of class 10 Class 10 indicates has syntactic role in plural sentence (nominal classication) -vu Stem Sufx 2: Morphological in izimvukazana Other relevant concepts are the augmentative sufx, class gender, which subsumes class 10 and all the other classes, and afx which subsumes prex and sufx. At the lexi- cal level relevant concepts for izimvukazana are, for exam- ple, diminutive noun and noun, but this forms part of future work. 4. Extending the DCR Now that we have established the concepts that we need for the syntactic and semantic annotation of the morphological information in izimvukazana and, in general the Zulu noun, we proceed to select or create data categories for these con- cepts in order to extend the ISOcat DCR. 2Usually a common gender noun, while a different noun rep- resents the masculine form, e.g. inqama cf. (Taljaard and Bosch, 1998, 144).404.1. The procedure Our extension is based on the principle that if an appropri- ate data category already exists, we use it, if not, we add a new data category to the DCR. If there are multiple op- tions available we consider those (see section 4.2) and se- lect an appropriate DC. If not, we extend the registry, as de- scribed in (Anon., 2010). For now the DCs that are added to the DCR via the ISOcat web interface are marked as pri- vate categories. Once they have been tested, moderated and evaluated they will be marked as public. Since the broader aim is to add Zulu data categories in such a way that these categories are also useful for other related languages that share morphosyntactic structure with Zulu, we attempt to provide the most general denitions without losing essen- tial information. Table 3 shows the DCs for the concepts in Table 2, for the augmentative sufx and the mentioned subsuming concepts. DC Existing/New Existing 3417 Stem Existing 3485 Sufx Existing 3501 Class New Feminine Existing 1880 Diminutive Existing 3046 Augmentative New 6542 Class gender New 6016 Afx Existing 3072 Table 3: DCs for Zulu noun morphology, both existing and new 4.2. Discussion of the selection of existing DCs In cases where more than one DC is appropriate, we opted for the DC of the GOLD ontology (Farrar and Langendoen, 2003) by way of consistency and coherence. Prex : The selected DC is 3417: \"An afx which is added to the front of a root or stem\". This denition goes hand in hand with that of the afx (3072), and does not necessarily serve to change the meaning of a word as indicated in 293 and 1365, but may also serve to \"change a word according to the grammatical context.\" (Kosch, 2006, 8). Stem : The selected DC is 3485: \"Stem is the class of morphological units that are analysable into a root and possibly one or more derivational units. Stems can occur alone and are the basis for adding inflectional units.\" This denition provides more information on the nature of the stem vs. the root than for instance 1389. Sufx : The selected DC is 3501: \"An afx, consist- ing of a letter, syllable, or syllables that follow a stem or word modifying its meaning. Sufxes may be in- flectional or derivational.\" This denition is closer to the general function of sufxes in the Bantu languages than for instance 294 and 1395.Afx : The selected DC is 3072: \"An afx is a mor- pheme with an abstract meaning which can only be used when added to a root morpheme. These are clas- sied in four different ways, depending on their posi- tion with reference to the root: sufx, prex, circumx and inx.\" The signicant part of this denition for the case of the Bantu languages is that the term referred to as an afx is a morpheme that cannot occur inde- pendently (cf. (Kosch, 2006, 8)). Some of the other denitions such as those in 291 and 1234 do not make explicit reference to this dependency. Feminine : The selected DC is 1880: \"Of, relating to, or constituting the gender that ordinarily includes most words or grammatical forms referring to females.\" This denition caters for the feminine sufx in Zulu which may occur with nouns as well as with adjec- tives compared to 3197 which caters more specically for languages with grammatical gender. Diminutive : The selected DC is 3046: \"Form express- ing smallness\". This denition covers the diminutive sufx in Zulu which is not only sufxed to nouns but also to adjectives, and is therefore more appropriate than for instance 2225 where only a diminutive noun is mentioned in the denition. 4.3. The addition of new DCs In the ISOcat online template for dening new data cate- gories a justication, prole and status have to be provided for each new addition. For all the new DCs, given below, the justication is that they are \"Bantu language identi- ers\", their prole is \"morphosyntax\", and their status is \"private\". The essential concept specic information, re- quired in the template, is as follows: English name : class 10 Key: 6171 PID:http://www.isocat.org/datcat/DC-6171 Identier : class 10 Denition : Class designation used in the Bantu languages generally for miscellaneous nouns, including many animal names, in the plural. Source : (Doke, 1967). English name : augmentative Key: augmentation. Source : (Doke, 1967). It was deemed necessary to create a new DC for the aug- mentative in particular for the Bantu languages since an existing DC such as 3094 \"A special form of a noun that signals that the object being referred to is large relative to the usual size of such an object\" would not make provision for augmentatives in adjective stems.41English name : class gender of nouns in the Bantu languages that generate grammatical agreement by means of class prexes, also termed gender number prexes. Source : (Kosch, 2006). Conceptual domain : class gender is a complex closed DC, which assumes as values the simple DCs class 1, class 1a, class 2, ..., class 15 and class 16-18. The complete list of DCs relating to class number is shown in Table 4, which follows the references in section 6. 4.4. Dening a Data Category Selection for Zulu As a nal step we dene a Data Category Selection (DCS) specically for Bantu to ensure optimal interoperability be- tween the Bantu languages. By selecting existing DCs, where possible, wider interoperability with other languages will also be achieved. 5. Conclusion and future work In this paper we described our rst efforts in extending the ISOcat DCR for the future annotation, both syntactic and semantic, of Zulu noun morphology. This forms part of a larger, longer term project towards extending the DCR with all the morphological concepts for Zulu and even other Bantu languages. There seems to be a good mix of existing and new data cat- egories, attesting to, on the one hand, the commonality be- tween languages and the potential for interoperability and, on the other hand, the inherent difference between language families, which is also to be expected. 6. References Anon. (2010). DCR Style Guidelines. http://www. isocat.org/manual/DCRGuidelines.pdf . Doke, C. M. (1967). The Southern Bantu Languages . Dawsons of Pall Mall, London. Farrar, S. and Langendoen, D. T. (2003). A linguistic ontology GLOT . J. in- teroperability mean, anyway? Toward an opera- tional denition of interoperability. In Proceedings of the Second International Conference on Global In- teroperability for Language Resources (ICGL 2010) , Kosch, I. M. (2006). Topics in Morphology in the African Language Context . University of South Africa, Pretoria. ISBN 9781868883691.Kubicek, H., Cimander, R., and Scholl, H. J. (2011). Layers of interoperability. In Kubicek, H., Cimander, R., and Scholl, H. J., editors, Organizational Inter- operability in E-Government: Lessons from 77 Euro- Springer, Berlin. ISBN: 9783642225024 (Online). C. (1932). Introduction to the phonology the Bantu languages . Dietrich Reimer/Ernst Vohsen, Berlin. Taljaard, P. Denition class 1 6017 class 1 Class designation used in the Bantu languages generally for nouns denoting human beings in the singular. class 1a 6540 class 1a Class designation used in the Bantu languages generally for proper names, kinship terms, names of personied animals and objects, and nouns of foreign origin in the singular. class 2 6163 class 2 Class designation used in the Bantu languages generally for nouns denoting human beings in the plural. class 2a 6541 class 2a Class designation used in the Bantu languages generally for proper names, kinship terms, names of personied animals and objects, and nouns of foreign origin in the plural. class 3 6164 class 3 Class designation used in the Bantu languages generally for impersonal nouns, including names of plants, trees, some body parts, names of spirits, diseases, rivers and abstract nouns in the singular. class 4 6165 class 4 Class designation used in the Bantu languages generally for impersonal nouns, including names of plants, trees, some body parts, names of spirits, diseases, rivers and abstract nouns in the plural. class 5 6166 class 5 Class designation used in the Bantu languages generally for miscellaneous nouns, including majority of names of fruits in the singular. class 6 6167 class 6 Class designation used in the Bantu languages generally for miscellaneous nouns, including majority of names of fruits in the plural; nouns denoting things occurring in pairs, fluids, abstract nouns. class 7 6168 class 7 Class designation used in the Bantu languages generally for miscellaneous nouns, including names of languages, customs and habits, nature and the physical world, material objects and instruments in the singular. class 8 6169 class 8 Class designation used in the Bantu languages generally for miscellaneous nouns, including names of languages, customs and habits, nature and the physical world, material objects and instruments in the plural. class 9 6170 class 9 Class designation used in the Bantu languages generally for miscellaneous nouns, including many animal names, in the singular. class 10 6171 class 10 Class designation used in the Bantu languages generally for miscellaneous nouns, including many animal names, in the plural. class 11 6172 class 11 Class designation used in the Bantu languages generally for miscellaneous nouns and long objects, in the singular. Class 11 nouns take their plurals mainly in three classes, viz. 6, 10 and 14. The choice of the plural class is language dependent. class 14 6173 class 14 Class designation used in the Bantu languages generally for abstract nouns and non-abstract nouns mostly collective, usually in the singular. Some nouns in this class take plural forms and these are mainly found in class 6. class 15 6174 class 15 Class designation used in the Bantu languages for verbal innitives. By the very nature of their meaning, innitive forms do not show a distinction between singular and plural. class 16-18 6175 class 16-18 Class designation used in the Bantu languages for the so-called locative noun classes. Very few nouns occur in these classes. No longer productive noun classes. Class 16 and 17 prexes indicate locative adverbials. Table 4: Complete data categories for class number added to the ISOcat Data Category Registry (Doke, 1967)43Understanding questions and nding answers: semantic relation annotation to compute the Expected Answer Type Volha Petukhova Spoken Language Systems, Saarland University, Germany v.petukhova@lsv.uni-saarland.de Abstract The paper presents an annotation scheme for semantic relations developed and used for question classication and answer extraction in an interactive dialogue based quiz game. The information that forms the content of this game is concerned with biographical facts of famous people's lives and is often available as unstructured texts on internet, e.g. Wikipedia collection. Questions asked as well as extracted answers, are annotated with dialogue act information (using the ISO 24617-2 scheme) and semantic relations, for which an extensive annotation scheme is developed combining elements from TAC KBP slot lling and TREC QA tasks. Dialogue act information, semantic relations and identied focus words (or word sequences) are used to compute the Expected Answer Type (EAT). Our semantic relation annotation scheme is dened and validated according to ISO criteria for design of a semantic annotation scheme. The obtained results show that the developed tagset ts the data well, and that the proposed approach is promising for other query classi- cation and information extraction applications where structured data, for example, in the form of ontologies or databases, is not available. Keywords: semantic annotation, annotation scheme design, semantic relations 1. Introduction According to the ISO Linguistic Annotation Framework (ISO, 2009), the term 'annotation' refers to linguistic infor- mation that is added to segments of language data and/or nonverbal communicative behaviour. Semantic annotations have been proven to be useful for various purposes. An- notated data is used for a systematic analysis of a variety of language phenomena and recurring structural patterns. Corpus data annotated with semantic information are also used to train machine learning algorithms for the automatic recognition and prediction of semantic concepts. Finally, semantically annotated data is used to build computer- based services and applications. One of the rst steps in ob- taining such annotations is the design of a semantic annota- tion scheme that ts the data well. The International Orga- nization for Standards (ISO) has set up a series of projects for dening standards for the annotation of various types of semantic information, together forming the so-called Se- mantic Annotation Framework (SemAF). Different parts of SemAF are concerned with (1) time and events; (2) di- alogue acts; (3) semantic roles; (4) spatial and (5) They general theoreti- cally and empirically well-founded domain- and language- independent concepts. This presents a good starting point for designing domain-specic schemes, if desired. In this paper we discuss the design of a domain-specic an- notation scheme for semantic relations used for a domain- specic Question Answering (QA) application. In a domain-specic QA, questions are expected about a certain topic; if a question outside that topic is asked, it will not be answered by the system. The system described here is an interactive guessing game in which players ask questions about attributes of an un- known person in order to guess his/her identity. The player may ask ten questions of various types, and direct questions about the person's name or alias are not allowed. More- over, the system is a Question Answering Dialogue System(QADS), where answers are not just pieces of extracted text or information chunks, but full-fledged natural language di- alogue utterances. The system has all components that any traditional dialogue system has: Automatic Speech Recog- nition (ASR) and Speech Generation (e.g. TTS) modules, and the Dialogue Engine. The Dialogue Engine, in turn, consists of four components: the interpretation module, the dialogue manager, the answer extraction module and the utterance generation module. The dialogue manager (DM) takes care of overall communication between the user and the system. It gets as input a dialogue act representation from the interpretation module (IM), which it is usually about a question which is uttered by the human player. Questions are classied according to their communicative function (e.g. Propositional, Check, Set and Choice Ques- tions) and semantic content. Semantic content is deter- mined by Expected Answer Type (EAT), e.g. LOCATION as semantic relation, and the focus word, e.g. study . To extract the requested information, a taxonomy is designed comprising 59 semantic relations to cover the most impor- tant facts in human life, e.g. birth, marriage, career, etc. The extracted information is mapped to the EAT, and both the most relevant answer and a strategy for continuing the dialogue are computed. The DM then passes the system re- sponse along for generation, where the DM input is trans- formed into a dialogue utterance (possibly a multimodal and multifunctional one). The paper is structured as follows. Section 2 gives an overview of previous approaches to designing semantic re- lation tagsets for QA applications. Section 3 discusses design criteria for the new semantic relation annotation scheme. Section 4 denes the semantics of the relations and groups them into a hierarchical taxonomy. Section 5 describes the collection of dialogue data and annotations, with indicated reliability of the dened annotation scheme in terms of inter-annotator agreement. In Section 6 classi- cation results using semantic relations in questions and for answer extraction are presented. Section 6 concludes the44reported study and outlines future research. 2. Related work A major breakthrough in QA has been made by (Moldovan et al., 2000) when designing an end-to-end open-domain QA system. This system achieved the best result in the TREC-8 competition1with an accuracy of 77.7%. Their system contains the three components: question process- ing, paragraph indexing and answer processing. First, the question type, question focus, question keyword and ex- pected answer type are specied. There are 9 question classes (e.g. 'what' ally, expected answer type is determined, e.g. person , money ,organization ,location . Finally, a focus word or a sequence of words is identied in the question, which dis- ambiguates it by indicating what the question is looking for (see Moldovan et al., 2000 for an overview of dened classes for 200 of the most frequent TREC-8 questions). Li and Roth (2002) proposed another question classica- tion scheme, also based on determining the expected an- swer type. This scheme is a layered hierarchical one hav- ing two levels. The rst level represents coarse classes like Date ,Location ,Person ,Time ,Denition ,Manner ,Number , Price ,Title,Distance ,Money ,Organization ,Reason and Undened . The second level has 50 ne-grained classes likeDescription ,Group ,Individual andTitle for the upper- level class of Human . The most recent work comes from the TAC KBP slot lling task (Joe, 2013) aiming to nd ller(-s) for each identied empty slot, e.g. for a person (e.g. date ofbirth, age, etc.) and/or for an organization (e.g. member of, founded by, etc). Pattern matching, trained classiers and Freebase2are used (Min et al., 2012) and (Roth et al., 2012) to nd the best ller. The best system performance achieved in terms of F-score is 37.28% (see Surdeanu, 2013 and Roth et al., 2013 ). We see that semantic relations are commonly used to com- pute an expected answer type. Our task, domain and data differ from the above mentioned approaches in that (1) our domain is closed, (2) the content is mainly unstructured in- ternet articles, and (3) the answers are not just extracted chunks or slot llers, but rather full dialogue utterances. These aspects cannot be captured by existing annotation approaches. Therefore, we propose a new semantic rela- tion annotation scheme and when developing it we rely on criteria formulated for semantic annotation ISO standards design (see e.g. ISO 24617-2). These criteria support well- founded decisions when designing the conceptual content and structure of the annotation scheme. We discuss the cri- teria in the next Section. 3. Annotation scheme design criteria The design of a scheme for annotating primary language data with semantic information is subject to certain method- ological requirements, some of which have been made ex- plicit in various studies (Bunt and Romary, 2002; Ide et 1http://trec.nist.gov/pubs/trec8 2http://www.freebase.com/al., 2003; Bunt and Romary, 2004), and some of which have so far remained implicit. For example, Bunt and Ro- mary (2002) introduce the principle of semantic adequacy , which is the requirement that semantic annotations should have a semantics. This is because a semantic annotation is meant to capture something of the meaning of the anno- tated stretch of source text, but if the annotation does not have a well-dened semantics, then there is no reason why the annotation should capture meaning any better than the source text itself. A semantic annotation scheme is intended to be applied to language resources, in particular to collections of empiri- cal data. It should therefore contain concepts for dealing with those phenomena which are found in empirical data, allowing good coverage of the phenomena of interest. Finally, an annotation scheme should be practically useful, i.e. be effectively usable by human annotators and by auto- matic annotation systems; it should not be restricted in ap- plicability to source texts in a particular language or group of languages; and it should incorporate common concepts of existing annotation schemes where possible. From these considerations, the following general criteria can be distilled: compatibility : incorporate common concepts of exist- ing annotation schemes, thus supporting the mapping from existing schemes to the new one, and ensuring the interoperability of the dened scheme. theoretical validity : every concept dened has a well- dened semantics. empirical validity : concepts dened in the scheme cor- respond to phenomena that are observed in corpora. completeness : concepts dened in the scheme provide a good coverage of the semantic phenomena of inter- est. distinctiveness : each concept dened in the scheme is semantically clearly distinct from the other concepts dened. andeffective usability : concepts dened in the scheme are learnable for both humans and machines with ac- ceptable precision. We will show in this paper that each of these criteria is ful- lled, supporting well-founded decisions when designing the conceptual content and structure of the proposed anno- tation scheme. 4. Semantic relations In order to nd the answer to a certain question, semantic role information can be used. A semantic role is a rela- tional notion (between an event and its participant) and de- scribes the way a participant plays in an event or state (rst dened as such in (Jackendoff, 1972) and (Jackendoff, 1990)), as described mostly by a verb, typically providing answers to questions such as \"who\" did \"what\" to \"whom,\" Several semantic role annotation schemes have been developed in the past, e.g. FrameNet (ICSI, 2005), PropBank (Palmer et al., 2002), VerbNet (Kipper, 2002) and Lirics (Petukhova and Bunt, 2008).45 Time Human description Name Alternative Name Age_Of Body Gender Nationality Religion Title Profession Degree Icon modifiers Topic Manner Purpose Reason Definition Duration Frequency Period Initial Time InitialTime:Birth Final Time FinalTime:Death Initial Time (Li and Roth, 2002); and 4/circlecopyrtin LIRICS semantic role set) Communicative function % Propositional Questions 22.4 Set Questions 38.8 Choice Questions 10.4 Check Questions 23.9 Unspecied Question Type 4.5 Table 1: Distribution of information-seeking communica- tive functions in the annotated data. Along with semantic roles, relations between participants are also relevant for our domain, e.g. the relation between Agent and Co-Agent (or Partner) involved in a 'work' event may be a COLLEAGUE OFrelation. To decide on the set of relations to investigate, we anal- ysed available and collected new dialogue data. As a start- ing point, we analysed recordings of the famous US game 'What's my line?' that are freely available on Youtube (www.youtube.org ). However, the latter differs from our scenario: during the TV-show participants may ask only propositional questions with expected 'yes' or 'no' an- swers;, our game allows any question type from the user. Therefore, we collected data in pilot dialogue experiments, where one participant was acting as a person whose name should be guessed and the other as a game player. 18 dia- logues were collected of total duration of 55 minutes com- prising 360 system's and user's speaking turns. To evaluate the relation set and to train classiers, we performed large scale gaming experiments in a Wizard of Oz setting (see Section 4). Pilot experiments showed that all players tend to ask simi- lar questions about gender, place and time of birth or death, profession, achievements, etc. To capture this information we dened 59 semantic relations. We proposed a multi- layered taxonomy: a high level, coarse annotation com-prising 7 classes and a low-level, ne-grained annotation, comprising 52 classes. This includes the HUMAN DE - SCRIPTION class dened for basic facts about an individ- ual like age, title, nationality, religion, etc.; HUMAN RELA - TIONS for parent-child and other family relations; HUMAN GROUPS for relations between colleagues, friends, enemies, etc.; EVENTS &NON -HUMAN ENTITIES class for awards, achievements, products of human activities, etc.; EVENT MODIFIERS for specifying manner, purpose, reasons, etc.; the TIME class to capture temporal information like dura- tion, frequency, period, etc.; and the LOCATION class to capture spatial event markers for places where events oc- cur. Some of the second-level classes are broken down into even more specic classes. For example, TITLE has three classes such as PROFESSION for ofcial name(s) of the em- unof- cial and ofcial names of obtained degrees and degrees within an organization, e.g. 'highest paid athlete', 'doctor in physics', 'senior leader', etc.; and ICON for unofcial or metaphorical titles that do not refer to an employment or membership position, e.g. 'public gure', 'hero', 'sex symbol', etc. Figure 1 shows the dened hierarchical tax- onomy with an indication of what concepts can be found in existing schemes for annotating semantic relations and semantic roles. It should be noted here that the majority of the concepts dened here are domain-specic, i.e. tailored to our quiz game application. The approach could however be adapted for designing comparable annotation schemes for other domains; this has for example been done for the food domain (see Wiegand and Klakow, 2013). From a semantic point of view, each relation has two argu- ments and is one of the following types: RELATION (Z,?X), where Zis the person in ques- tion and Xthe entity slot to be OF(einstein,? X); (E1, event in tion and E2is the event slot to be lled, e.g. REA-46RELATION % RELATION % RELATION % RELATION % ACTIVITY OF 10.21 LOC BIRTH 2.34 AGE OF 3 LOC DEATH 1.69 AWARD 4.4 LOC RESIDENCE 1.69 BODY 1.5 MANNER 1.12 CHARGED FOR 4.21 MEMBER OF 2.43 CHILD OF 1.5 NAME 1.87 COLLEAGUE OF 1.03 NATIONALITY 1.22 CREATOR OF 6.09 OWNER OF 1.97 DESCRIPTION 4.12 PARENT OF 1.31 DURATION 1.31 REASON 1.22 EDUCATION OF 3.65 RELIGION 2.53 EMPLOYEE OF 1.59 SIBLING OF 0.94 ENEMY OF 1.12 SPOUSE OF 1.4 FAMILY OF 1.59 SUPPORTED BY 0.94 FOUNDER OF 1.87 TIME 7.96 FRIEND OF 1.03 TIME BIRTH 2.06 GENDER 1.69 TIME DEATH 1.59 LOCATION 4.68 TITLE 11.14 Table 2: Question types in terms of dened semantic relations and their distribution in data (relative frequency in %). SON(death,? E2); and RELATION (E,?X) where Eis the event in question and Xthe entity slot to be lled, e.g. DURA - TION (study,? X). The slots to be lled are categorized primarily based on the type of entities which we seek to extract information about. However, slots are also categorized by the content andquantity of their llers. Slots are labelled as name ,value , orstring based on the content of their llers. Name slots are required to be lled by the name of a person, organization, or geo-political en- tity (GPE). Value slots are required to be lled by either a numerical value or a date. The numbers and dates in these llers can be spelled out (December 7, 1941) or written as numbers (42; 12/7/1941). String slots are basically a \"catch all\", meaning that their llers cannot be neatly classied as names or values. Slots can be single-value orlist-value based on the number of llers they can take. While single-value slots can have only a single ller, e.g. date of birth, list-value slots can take multiple llers as they are likely to have more than one correct answer, e.g. employers. 5. Data collection and annotations In order to validate the proposed annotation scheme em- pirically, two types of data are required: (1) dialogue data containing player's questions that are more realistic than youtube games and larger than our pilots; and (2) descrip- tions containing answers to player's questions about the guessed person. This data is also required to build an end- to-end QADS. To collect question data we explored different possibilities. There is some question data publicly available, e.g. ap- proximately 5500 questions are provided by the University Illinois3annotated according to the scheme dened in (Li and Roth, 2002). However, not all of this data can be used for our scenario. We ltered out about 400 questions for our purposes. Since this dataset is obviously too small, we generated questions automatically using the tool provided by (Heilman and Smith, 2009) from the selected Wikipedia articles and ltered them out manually. Out of the gener- ated 3000 questions relevant ones were selected: grammat- ically questions were xed and synonyms from WordNet4were used to gen- erate different variations of questions for the same class. Questions collected in pilot experiments were added to this set as well. The nal question set consists of 1069 ques- tions. These questions are annotated with (1) communica- tive function type according to ISO 24617-2; (2) with se- mantic relations as dened in Section 3; and (3) with ques- tion focus word or word sequence. Table 1 provides an overview of the types of information-seeking communica- tive functions in the collected data and those relative fre- quencies. Table 2 illustrates the distribution of question types based on the EAT's semantic relation. A focus word or word sequence describes the main event in a question, usually specied by a verb or eventive noun. The focus word (sequence) is extracted from the question to compute the EAT and formulate the query. For example, (1)Question: When was his rst album released? Assigned semantic relation: TIME Focus word sequence: rst album released EAT: TIME release(rst album) Query: TIME album) :: (E, ?X) :: QUALITY(V ALUE) :: QUANTITY(SINGLE) The question set is currently enriched with questions from large scale Wizard of Oz experiments. The data collection procedure was similar to that of pilots. A Wizard (English native speaker) simulated the system's behaviour and the other participant played the game. 21 unique subjects, un- dergraduates of age between 19 and 25, who are expected to be related to our ultimate target audience, participated in these experiments. 338 dialogues were collected of a total duration of 16 hours comprising about 6.000 speak- ing turns. An example from this dialogue collection can be found in the Appendix. Answers were retrieved from 100 selected Wikipedia articles in English containing 1616 sentences (16 words/sentence on average), 30.590 tokens (5.817 unique tokens). Descriptions are annotated using complex labels consisting of an IOB-prex ( Inside, Outside, and Beginning), since we aim to learn the exact answer boundaries, and semantic relation tag, the same as used for classifying questions. We mainly focus on labeling nouns and noun phrases. For example: 4urlhttp://wordnet.princeton.edu/47RELATION % RELATION % RELATION % RELATION % RELATION % ACCOMPLISHMENT 4.0 DURATION 1.8 LOC DEATH 0.8 PART IN 3.6 TIME 14.6 AGE OF 2.1 EDUCATION OF 4.2 LOC RESIDENCE 3.2 RELIGION 0.7 TIME BIRTH 2.8 AWARD 2.5 EMPLOYEE OF 2.2 MEMBER OF 1.8 SIBLING OF 2.3 TIME DEATH 1.0 CHILD OF 3.6 FOUNDER OF 1.2 NATIONALITY 3.1 SPOUSE OF 1.9 TITLE 14.2 COLLEAGUE OF 1.7 LOC 5.6 OWNER OF 1.1 SUBORDINATE OF 1.3 CREATOR OF 8.5 LOC BIRTH 5.0 PARENT OF 3.7 SUPPORTEE OF 1.1 Table 3: Answer types in terms of dened semantic relations and their distribution in data (relative frequency in %) (2)Gates graduated from Lakeside School in 1973. The word Lakeside in (2) is labeled as the beginning of an EDUCATION OFrelation ( B-EDUCATION OF), and school is marked as inside of the label ( I-EDUCATION OF). Table 3 illustrates the distribution of answer types based on the identied semantic relation. Since the boundaries between semantic classes are not al- ways clear, we allowed multiple class labels to be assigned to one entity. For example: (3)Living in Johannesburg, he became involved in anti- colonial politics, joining the ANC and becoming a founding member of its Youth League . Here, Youth League is founded by a person ( FOUNDER OF relation), but the person is also a member of the Youth League . There are also some overlapping segments de- tected as in example ( 4): (4)He served as the commander-in-chief of the Conti- nental Army during the American Revolutionary War. The entity commander-in-chief of the Continental Army in (4) is marked as TITLE , while the Continental Army is rec- ognized as MEMBER OF. Both of these relations are cor- rect, since if a person leads an army he/she is also a member of it. To assess the reliability of the dened tagset, the inter- annotator agreement was measured in terms of the standard Kappa statistic (Cohen, 1960). For this, 10 randomly se- lected descriptions and all 1069 questions were annotated by two trained annotators. The obtained kappa scores were interpreted as annotators having reached good agreement (averaged for all labels, kappa = .76). 6. Semantic relation classication and learnability To investigate the learnability of the relations we dened in a data-oriented way and to evaluate the semantic relation set, we performed a number of classication experiments. Moreover, we partition the training sets in such a way that we can assess relation learnability by plotting learn- ing curves for each relation given an increasing amount of training data. Classiers used were statistical ones, namely, Conditional Random Fields (CRF) (Lafferty et al., 2001) and Support Vector Machines (SVM) (Joachims et al., 2009).5. The selected feature set includes word & lemma tokens ; n-grams andskip n-grams for both tokens and tags from the Stanford tagger (Toutanova used two CRF Question classication results et al., 2003); NER tags from three different NER tools: Stanford NER (Finkel et al., 2005), Illinois NER (Rati- nov and Roth, 2009), and Saarland NER (Chrupala and Klakow, 2010); chunking using OpenNLP7to determine NP boundaries; key word to determine the best sentence candidate for a particular relation, e.g. marry, married, marriage, husband, wife, widow, spouse for the SPOUSE OF relation. To assess the system performance standard evaluation met- rics are used, precision (P), recall (R) and F-score (F1). In particular, precision is important, since it is worse for the system to provide a wrong answer than not to provide any answer at all, e.g. to say it cannot answer a question.8It should be noted that for answer extraction sequential clas- siers were trained and their predictions were considered as correct iff both the IOB-prex and the relation tag fully correspond to those in the referenced annotation. 6.1. Question classication In the 10-fold cross-validation classication experiments, classiers were trained and evaluated in two different set- tings: (1) Baseline , where classication is based solely on the bag-of-words features; (2) and System 1 : best system performance after trying different sets of features and se- lection mechanisms, namely, on bag-of-words plus bigrams generated from bag-of-lemmas. Table 4 presents the clas- sication results. It may be observed that System 1 clearly outperforms the baseline. The results are also better than those of the state- of-art systems on this task. To compare, the system reported in (Dell and Wee Sun, 2003) using SVM reached 80.2% accuracy (using bag-of-words) and 79.2% (using bag-of- ngrams) for the 50 question classes dened in (Li and Roth, 2002) and on their data. The reported in (Huang et al., 2008) the accuracies of SVM and Maximum Entropy (ME) classiers were 89.2% and 89.0% respectively on the data and taxonomy of (Li and Roth, 2002). The best perfor- mance in terms of accuracy reported by Li and Roth (2006) 7http://opennlp.apache.org/ 8Each WoZ experiment participant lled in a questionnaire, where among other things they indicated that 'not-providing' an answer was entertaining; giving wrong information, by contrast, was experienced as annoying.48Baseline System 1 System 2 P R F1 P R F1 P R F1 CRF 0.74 0.62 0.67 Table 6: Overall system performance. *) applied only to 12 most frequently occurring relations of the tagset was 89.3% using the SNoW learning architec- ture for a hierarchical classier . The performance of the classiers (System 1 setting) on each relation in isolation has also been assessed. Table 5 presents the obtained results. Our classiers achieved reasonably high accuracy in detect- ing all relations. In terms of F-score, three relations were rather problematic, namely OWNER OF, DESCRIPTION and SUPPORTEE OF. For the latter, the number of train- ing instances was rather low as we will show in our learn- ability experiments (see Section 5.3). For the rst one, we have concluded that this relation requires a more clear def- inition to make better distinctions with other classes, e.g. it is often confused with CREATOR OF and FOUNDER OF. Similarly, the DESCRIPTION relation has a rather vague denition and tends to be applied for many unclassiable instances. We introduce two relations instead: DEFINI- TION and TOPIC (see Figure 1). 6.2. Answer extraction In the 5-fold cross-validation classication experiments, classiers were trained and evaluated in three different set- tings: (1) Baseline obtained when training classiers on word token features only; (2) System 1 where classica- tion is based on automatically derived features such as n- grams for tokens and lemmas (trigrams), POS, NER tags and chunking; joint classication on all relations; (3) and System 2 : pattern matching and classication on the same features as System 1 applied for each relation separately. Both CRF++ and SVM-HMM classiers in System 1 and 2 settings show gains over the baseline systems. To appre- ciate how good statistical classiers generally are on rela- tion recognition for answer extraction, consider the perfor- mance of distant supervision SVM9with precision of 53.3, recall of 21.8 and F-score of 30.9 (Roth et al., 2013 ) on the TAC KBP relations. However, we emphasize that our task, relation set, application and data are different from those of TAC KBP. As can be observed from Table 6, the CRF++ classier achieves the best results in terms of precision and F-score. Although the running time was not measured, the classica- tion runs faster than the SVM-HMM. System 2 outperforms System 1 (6-11% increase in F-score). When training on each relation in isolation, feature weights can be adjusted more efciently, while not affecting other classiers' per- formances. More from CRF++ on each semantic rela- tion classication can be seen in Table 7. 9Distant supervision method is used when no or little labeled data is available, see (Mintz et al., 2009).Relation P R F1 Relation P R F1 ACCOMPLISHMENT 0.73 OF 0.79 0.54 0.63 CHILD OF 0.74 0.58 0.65 PART IN 0.25 0.05 0.08 COLLEAGUE OF 0.78 0.32 0.43 RELIGION 0.60 0.16 0.24 CREATOR OF 0.64 0.17 0.26 SIBLING OF 0.92 SUBORDINATE EMPLOYEE OF 0.77 0.19 0.28 SUPPORTEE OF 1.00 0.40 0.54 FOUNDER OF 0.65 0.26 0.36 MEMBER OF 0.65 0.14 0.21 LOC 0.77 0.33 0.90 System 2. 6.3. Learnability The outcome from the learnability experiments is presented in Figure 2. From these graphs, we can clearly observe that larger training data positively correlates with higher F- score. The SUPPORTEE OFis the most sensitive relation to the amount of training data, followed by LOC DEATH and SUBORDINATE OF. 7. Discussion and conclusions We propose an annotation scheme for question classi- cation and answer extraction from unstructured textual data based on determining semantic relations between en- tities. Semantic relation information together with the fo- cus words (or word sequences) is used to compute the Ex- pected Answer Type. Our results show that the relations that we have dened help the system to understand user's questions and to capture the information, which needs to be extracted from the data. The proposed scheme ts the data and is reliable, as evidenced by good inter-annotator agree- ment. Semantic relations can be learned successfully in a data-oriented way. We found the ISO semantic annotation scheme design criteria very useful. Following them sup- ported our decisions when dening concepts and the struc- ture of the scheme. The proposed approach is promising for other query classication and information extraction tasks for domain-specic applications. There is a lot of room for further research and development, and the annotation scheme is far from perfect. For instance, observed inter-annotator agreement and classication re- sults indicate that some relations need to be re-dened. We will test how generic the proposed approach is by testing it on the TAC and TREC datasets. Moreover, since some relations, in particular of RELATION (E1, ?E2) and RELA - TION (E,?X) types, are very close to semantic roles, there is a need to analyse semantic role sets (e.g. ISO seman- tic roles (Bunt and Palmer, 2013)) and study the possible overlaps. From the QADS development point of view, we need to evaluate the system in real settings. For this, the ASR is cur- rently retrained, i.e. generic language and acoustic models are adapted to our game scenario. For now, all classication experiments were run on data transcribed by a human. It is a semi-automatic process, when the ASR output has been corrected. The real system, however, needs to operate on ASR output lattices (list of hypotheses for each token with49Relation P R F1 Accuracy (in %) Relatio P R F1 Accuracy (in %) ACTIVITY OF EDUCATION 0.85 98.97 EMPLOYEE OF 0.91 0.75 0.83 99.49 ENEMY OF 0.81 0.56 0.66 99.35 FAMILY 0.88 0.59 98.07 0.99 results for each relation in isolation.(*presented in alphabetic order) 20 30 40 50 60 70 8020406080 % training dataF-scoreLearning Curve ACCOMPLISHMENT AWARD COLLEAGUE OF AGE OF CHILD OF CREATOR OF20 30 40 50 60 70 8020406080 % training dataF-scoreLearning Curve DURATION EMPLOYEE OF LOC EDUCATION OF FOUNDER OF LOC BIRTH20 30 40 50 60 70 8020406080 % training dataF-scoreLearning Curve LOC DEATH MEMBER OF OWNER OF LOC RESIDENCE NATIONALITY PARENT OF 20 30 40 50 60 70 80020406080 % training dataF-scoreLearning Curve PART IN SIBLING OF SUBORDINATE OF RELIGION SPOUSE OF SUPPORTEE OF20 30 40 50 60 70 80020406080 % training dataF-scoreLearning Curve SUPPORTER OF TIME BIRTH TITLE TIME TIME DEATH Figure 2: Learning curves for the dened relations the recognizer's condence scores). Therefore, in the near- est future we will test the question classiers performance on the actual ASR output. 8. Acknowledgments The research reported in this paper was carried out within the DBOX Eureka project under number E! 7152. The author is particularly thankful to her Master students Desmond Darma Putra and Humayun Faiz who performed machine-learning classication experiments.9. References H. Bunt and M. Palmer. 2013. Conceptual and represen- tational choices in dening an iso standard for semantic role annotation. In Proceedings Ninth Joint ISO Towards multimodal se- mantic representation. In Proceedings of LREC 2002 Workshop on International Standards of Terminology and Language Resources Management , pages 54-60,50Las Palmas, Spain. H. Bunt and L. Romary. 2004. Standardization in multi- modal content representation: Some methodological is- sues. In Proceedings of LREC 2004 , pages 2219-2222, Lisbon, Portugal. G. Chrupala and D. Klakow. 2010. A named entity labeler for german: Exploiting Language Resources Association (ELRA). J. Cohen. 1960. A coefcient of agreement for nomi- nal scales. Education and Psychological Measurement , 20:37-46. Z. Dell and L. Wee Sun. 2003. Question classication us- ing support vector machines. In Proceedings of SIGIR , pages 26-32, Toronto, Canada. J. Finkel, T. Grenager, and C. Manning. 2005. Incorpo- rating non-local information into information extraction systems by gibbs sampling. In Proceedings of ACL '05 , pages 363-370, Stroudsburg, PA, USA. Association for Computational Linguistics. M. Heilman and N. Smith. 2009. Question generation via overgenerating transformations and ranking. Lan- guage Technologies Institute, Carnegie Mellon Univer- sity Technical Report CMU-LTI-09-013. Z. Huang, M. Thint, and Z. Qin. 2008. Question classica- tion using head words and their http: N. Ide, Romary, and E. de la Clergerie. 2003. Interna- tional standard for a linguistic annotation framework. In Proceedings of HLT-NAACL Workshop on The Software Engineering and Architecture of Language Technology , Edmunton. ISO. 2009. ISO 24612:2009 Language resource manage- ment: Linguistic annotation framework (LAF) . ISO, Geneva. ISO. 2012. Language resource management - Seman- tic annotation framework - Part 2: Dialogue acts. ISO 24617-2 . ISO Central Secretariat, Geneva. R.S. Jackendoff. 1972. 1990. Semantic . MIT Press. T. Joachims, T. Finley, and C.-N. Yu. 2009. Cutting- plane training of structural svms. Machine Learning , 77(1):27-59. E. Joe. 2013. Tac kbp 2013 slot descriptions. K. Kipper. 2002. Verbnet: random elds: Probabilistic models for segment- ing and labeling sequence data. In Proceedings of ICML '01, pages 282-289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. X. Li and D. Roth. 2002. Learning question classiers. InProceedings of the COLING '02 Computational Linguis- tics.X. Li and D. Roth. 2006. Learning question classiers: the role of semantic information. Natural Language Engi- neering , pages 229-249. B. Min, X. Li, R. Grishman, and S. Ang. 2012. New york university 2012 system for kbp slot lling. In Proceed- ings of the 5th Text Analysis Conference (TAC 2012) . M. Mintz, R. Bills, S.and Snow, and Jurafsky D. 2009. Distant supervision for relation extraction without la- beled data. In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL/IJCNLP) , page 10031011. D. Moldovan, R. Girju, R. Goodrum, and V . Rus. 2000. The structure and performance of an open-domain question answer- ing system. In Proceedings of ACL '00 , pages 563-570, Stroudsburg, PA, USA. Association for Computational Linguistics. M. Palmer, D. Gildea, and P. Kingsbury. 2002. The propo- sition Bunt. 2008. Lirics semantic role an- notation: Design and evaluation of a set of data cate- gories. In Proceedings of the sixth international con- ference on language resources and evaluation (LREC 2008) . Paris: ELRA. L. Ratinov and D. Roth. 2009. Design challenges and mis- conceptions in named entity recognition. In Proceedings of CoNLL '09 , pages 147-155, Stroudsburg, PA, USA. Association for Computational Linguistics. B. Roth, G. Chrupala, M. Wiegand, M. Singh, and D. Klakow. 2012. Saarland university spoken language systems at the slot lling task of tac kbp 2012. In Pro- ceedings of the 5th Text Analysis Conference (TAC 2012) , Gaithersburg, Maryland, USA. B. Roth, T. Barth, M. Wiegand, M. Singh, and D. Klakow. 2013. Effective slot lling based on shallow distant supervision methods. In TAC KBP 2013 Workshop , Gaithersburg, Maryland USA. National Institute of Stan- dards and Technology. M. Surdeanu. 2013. Overview of the tac2013 knowledge base population evaluation: English slot lling and tem- KBP 2013 Workshop , Gaithers- burg, Maryland USA. National Institute of Standards and Technology. K. Toutanova, D. Klein, C. Manning, and Y . Singer. 2003. Feature-rich part-of-speech tagging with a NAACL '03 , pages 173-180, Stroudsburg, PA, USA. Association for Com- putational Linguistics. M. Wiegand and D. Klakow. 2013. Towards the detection of reliable food-health relationships. In Proceedings of the NAACL-Workshop on Language Analysis in Social Media (NAACL-LASM) .51Appendix: dialogue example S: Hello P: Hello S: Good afternoon almost evening S: What is your name P: My name is James S: Hello James it's nice to meet you P: Nice to meet you S: How are you doing today? P: Good, thank you S: Alright S: Today we are going to play a game and here are the rules S: I'm a very famous person and you need to guess my name you can ask whatever questions you want of me except for my name directly S: You have at most ten questions and then you get to guess my name exactly once S: So you can ask whatever questions you want but then if you want to guess my name you only get one try S: If you get my name correct you win if you get my name incorrect or choose to pass then you lose and then we'll move on to the next round S: Do you understand and are comfortable with the rules? P: Yeah yeah P: So the name is kind of a famous person P: Okay P: I'm not sure how good am I in this area S: Yes S: I am a famous person and I am male P: Okay okay good S: Alright S: And what is your rst question? P: What is the rst question P: What do you do? S: I am a leader P: A leader P: What is your nationality? S: I am American P: Are you alive? S: I am not alive P: Are you leading a company? S: I am not leading a company P: okay P: You're not a company leader P: When are you born? S: I was born on February twenty second seventeen thirty two P: Seventeen thirty two P: Ok P: Eehm P: Are politician? S: I am a politician P: Okay P: So then it is not my area but I will try to guess P: When were you in the government? S: Uhm S: Let's see S: I retired from the presidency in seventeen ninety seven P: Ninety seven P: George Washington S: Is that your nal guess? P: Yes, Washington S: Very good, excellent job! S: Congratulations!52From Visual Prototypes of Action to Metaphors Extending the IMAGACT Ontology of Action to Secondary Meanings Susan Windisch Brown University of Florence Piazza Savanarola, 1, Florence, Italy E-mail: susanwbrown@att.net Abstract This paper describes an infrastructure that has bee n designed to deal with corpus -based variations that do not fall within the primar y, physical variation of action verbs. We have first e stablished three main categories of secondary varia tion -- metaphor , metonymy and idiom -- and criteria for creating types within these catego ries for each verb. The criteria rely heavily on th e images that compose the IMAGACT ontology of action and on widely accepted p rocesses of meaning extension in linguistics. Altho ugh figurative language is known for its amorphous, subjective nature, we have endeavoured to create a standard, justifiable proc ess for determining figurative language types for individual verbs. We specificall y highlight the benefits that IMAGACT's representat ion of the primary meanings through videos brings to the understanding and anno tation of secondary meanings. Keywords: semantic annotation, Introduction IMAGACT is a cross -linguistic ontology of action concepts that are represented with prototypic 3D animations or brief films. This format makes use of the universal language of images to identify action typ es, avoiding the under -determinacy of semantic definitions. This ontology has been induced from the references to physical actions found in English and Italian spoke n corpora (Moneglia et al. 2012) and gives a picture of the variety of activities that are prominent in our eve ryday life, specifying the language used to express each one in ordinary communication. IMAGACT uses prototypic scenes to represent the ran ge of variations that natural language verbs can recor d in a language and maps different languages onto the same ontology of visually represented concepts. Each ver b can express one or more concepts, while each concept ca n refer to one or more verbs. (Moneglia in press). For example, the verb to cross ranges over four main action types (Figure 1), identified in corpus occur rences, some of which can be equivalently identified by oth er verbs (pass, climb). The specific way of categorizing actions by the verb to cross does not find direct correspondence in other languages. For instance, in Italian only type 1 and 3 can be in the extension o f the direct translation (attraversare) while 2 and 4 respectively require other Italian verbs (incrociare, superare). The IMAGACT ontology has been developed through annotation of English and Italian spoken corpora, i n which reference to actions is frequent. Working in their native languages, linguists identified the variatio n of action -oriented lexicons across different action concepts. 521 Italian verbs and 550 English verbs (i.e., the high -frequency verbal lexicon most likely to be used when referring to action) have been processed (Mone glia et al. 2012). The corpus -based strategy relied on an induction process that separated the metaphorical and phraseological usages from those strictly referring to physical actions. IMAGACT only specifies the various possible interpretations of verbs with respect to physical a ctions, while ignoring the other interpretations. Therefore the possible interpretations of verbs beyond physical a ctions are not considered and are not represented in the o ntology. Figure 1. The four action types of the verb to cross The unique visual format of the ontology makes the representation of abstract concepts difficult or im possible. This limitation, however, also constitutes an impor tant added value, which can benefit our knowledge of act ion verbs in their abstract interpretations and the identification of these meanings within ontologies, as we will show in later sections of this paper. The capacity to refer to many different physical ac tivities with a single verb belongs to the core of the seman tic competence of a language, which has been achieved b y mother -tongue speakers during the early phases of their first language acquisition. A speaker cannot assert knowledge of the meaning of cross if he is not able to judge that the above events can be the object of it s application. At the same time, despite the differen ce between the different actions represented in each c oncept, he will also be able to judge that none of them rep resents the meaning of the verb better than the others and that the 53verb is applied in its own meaning in all cases (pr imary meanings). This is not the case for metaphors, phraseology and abstract meanings. For instance, the semantic competence of the speake r is not affected if she does not understand the meaning of \"John crossed wires with Mary\" (idiom) or \"John nee ds to cross to another account\" (metaphor). Competent speakers are, on the contrary, able to judge that i n these cases the verb is not used in its physical meaning (marked meanings). Nonetheless, roughly half of corpus occurrences of action verbs are not used in their p rimary, physical meanings, and the use of verbal predicatio n extended from physical meanings is one of the more productive means of reference in natural languages. This paper describes the infrastructure that has be en designed to deal with variations that do not fall w ithin the primary, physical variation of an action verb. It w ill specifically highlight the benefits that IMAGACT's representation of the primary meanings through vide os brings to the understanding and annotation of secon dary meanings. 2. Processing Corpus Occurrences in IMAGACT and the Selection of Marked Variation The construction of IMAGACT requires the examinatio n and interpretation of verb occurrences in an oral c ontext, which is frequently fragmented and may not provide enough semantic evidence for an immediate interpretation. To this end, the annotation infrast ructure allows the annotator to read the context of the ver bal occurrence in order to grasp the meaning. The annot ator represents the referred meaning with a simple sente nce in a standard form for easy processing. This sentence is positively formed, in the third person, present ten se, active voice, with the essential arguments of the v erb filled. Crucially, along with the standardization, the annotator assigns the occurrence to a \"variation cl ass\", either PRIMARY or MARKED (Moneglia et al.2012). The decision concerning the status of the occurrenc e makes use of an operational test roughly derived fr om Wittgenstein (1953). The occurrence is judged PRIMA RY if it is possible to say to somebody who does not k now the meaning of the verb V that \"the referred action and similar events are what we intend with V\"; otherwise the occurrence is MARKED. For instance, the occurrence s standardized in \"John crosses the finish line\"; Joh n crosses the street\" and \"John crosses his legs\" are assigned to PRIMARY variation, since all can be poi nted to explain \"what cross means\". Conversely, the instances standardized as \"a though t crossed John's mind\" are not what one uses to insta ntiate the meaning of to cross and therefore have been tagged as MARKED. The annotation of primary versus marked variation has been evaluated at 9.5 K-Cohen agreeme nt (Gagliardi 2014). The positive selection of occurrences in which verb s refer in their own meaning to physical actions preceded t he annotation of action concepts. Only occurrences ass igned to the PRIMARY variation class make up the set of Action Types stored in the ontology. To this end, t he standard IMAGACT infrastructure allows clustering o f occurrences under prototypes representing the vario us action concepts, keeping granularity to its minimal level (8.2 K agreement [Gagliardi 2014]). The full annota tion process can be found in Moneglia et al. 2012. Concepts are represented using the universal langua ge of images, which allows the reconciliation, in the IMAGACT ontology, of the types derived from the annotation of different language corpora. 1010 dist inct action concepts have been identified and visually represented with prototypical scenes, either animat ed or filmed (Frontini et al. 2012; Moneglia et al. 2012) . The cross-linguistic correspondences of those actions w ith the verbs that can refer to them in English and Italian have been established in a MYQL database. 38,462 occurrences have been processed in the Engli sh corpus and 42,723 in the Italian corpus. Respective ly 19,229 and 16,210 (50% and 38%) have been considere d marked. 3. Marked Variation Categories We have established three main categories of marked variation-- metaphor , metonymy and idiom --and criteria for creating types within these categories for each verb. The criteria rely heavily on the images that compos e the IMAGACT ontology of action and on widely accepted processes of meaning extension in linguistics. Alth ough figurative language is known for its amorphous, subjective nature, we have endeavored to create a standard, justifiable process for determining figur ative language types for individual verbs, that we will s how in the following sections on the basis of the verbs to turn and to close. 3.1 Metaphor The process for identifying a metaphoric type for a verb involves several steps and satisfying several relat ed criteria. First we list all the occurrences of a ve rb that were labeled as \"marked\" during the initial corpus annot ation of the IMAGACT project. We then use a standard lexicographic procedure of gathering similar usages together. For each group of occurrences that is a p otential metaphor, we look for an image or \"family\" of relat ed images from the IMAGACT ontology to which the occurrences are related. For example, the following list is a sample of one group of corpus occurrences for the verb to turn: John turns to the question of religion The presenter turns to [the subject of ] the book The colleagues turn to the report The host turns to the other issues We have linked this group to the S4 animated video from the IMAGACT ontology shown in Figure 2. The action is of a woman facing straight ahead then turning her h ead to the right. 54Figure 2. Interface sample for to turn The next step is to identify the property of the ac tion that affords the extension of the verb to a more abstrac t domain. In this video, the actor turning her head n ow sees whatever is to her right rather than whatever is di rectly in front of her. This physical turning of her head can indicate a change in the focus of her attention, say, from a street in front of her to a dog barking on her right. With a metaphorical extension, to turn can be used to indicate a change in the focus of one's attention to abstract things as well, such as the question of religion. One of the most influential theories of metaphor ha s been that of conceptual metaphor (Lakoff 1987), which po sits that a fundamental mechanism of human cognition is the use of a concrete, physical domain to understand a more abstract one. These conceptual metaphors are often revealed in a group of related lexical metaphors. F or example, the conceptual metaphor Life is a Journey can be seen in the sentences \"Mary needs to move on aft er her divorce\" and \"the governor ran into a political roa d block.\" Where it is possible, we identify the gener al conceptual metaphor that supports the specific ling uistic metaphor in question. Using the list of conceptual metaphors maintained by the University of Californi a, Berkeley, we linked the turn metaphor just described to the conceptual metaphors Change is Motion and Ideas are Locations. Thus, a person facing one location (idea) can turn to face another, indicating a change in her at tention from one idea to another. As with the identification of primary, physical typ es in IMAGACT, we use equivalent verbs to help distinguis h metaphorical types. For the marked variation, we distinguish between equivalent verbs that are used in their primary, or non -figurative, meaning and equivalent verbs that are used in a marked or figurative sense. For example, the verb shift has been identified as a verb that can be used in the same situations as turn in \"John turns to the question of religion.\" Both of these verbs are used metaphorically in this situation, with the same metaphorical meaning. This match is relevant for an ontology of abstract concepts and corresponds to ac tion concepts in the IMAGACT database. However, the key means of distinguishing types with in the category of metaphor are the links to the actio n concepts they derive from and the descriptions of t he relevant properties that license the metaphorical extensions. Often, links to different action concep ts are enough to distinguish two marked types of a verb. F or example, \"John turns to the question of religion\" i s linked to type S4, as described above. Another very common metaphor for the verb to turn refers to a change of state, such as \"the witch turns the frog back into a princ e\" or \"the gas turns to a liquid\". The metaphor is linked to the action concept represented by the video in S2. As p art of the conceptual metaphor Change of State is Change o f Direction, the linguistic metaphor for turn in this case uses the property of moving in a new direction from a different action concept and image than the previou s turn metaphor. Sometimes two or more metaphors derive from the sam e action concept but rely on different properties of that concept. Another metaphor of to turn links to the S4 image in Figure 2: \"John turns to Mary for answers\" or \"Mary turns to a psychiatrist\". In this case, the reorientation of the actor's head indicates an appe al for interaction rather than a change in the focus of hi s attention. Identifying the prototype related to the metaphor helps in understanding the properties that license the metaphoric extension. 3.2 Metonymy Metonymy is a less studied phenomenon than metaphor , especially as it pertains to verbs. However, the co rpus data we have gathered suggests that it is a necessary ca tegory to fully account for the marked variation of certai n verbs. For our purposes, we have defined verb metonymy as the use of one action or event to represent a sequence or set of events of which it is a part. For example, many occurrences of to close in our English corpus follow the form of \"John closed the pub\" and \"The management closed the factory.\" This usage of close does not follow the process of metaphorical extension, in which an abstract domain is being understood using propertie s from a physical one. There are actual actions of closing involved in the situations described by these sente nces. When John closes the pub, he does indeed close the door. He probably also takes the cash from the register, turns off the lights, and locks the door as he leaves. This i s not a physical domain being used to understand an abstrac t one, 55but one action in a sequence of events being used t o represent the whole sequence (Goossens 1995). Complicating the situation, the events in such a se quence are not always all physical actions. \"The managemen t closed the plant\" probably is also meant to include the decision to end production at the plant, as well as the action of closing and locking the doors. For our pu rposes, as long as part of the whole event can be described using the verb in its physical sense, we have categorized that type as a metonymic one.1 For this category, we also link the type to an imag e from the action ontology. The type of close described previously is linked to the video in Figure 3. We a lso identify one or more equivalent verbs. As with meta phor, where the equivalent verbs are usually other verbs used in a metaphorical way, the equivalent verbs for metony mic types are often other verbs being used metonymicall y. For example, shut is the equivalent verb for this type. Figure 3. Action type for to close 3.3 Idiom We use a standard definition of idiom: a fixed phra se whose meaning cannot be deduced by combining the meanings of the individual words in the phrase. Bec ause idioms are usually language specific, we have not attempted to link any idioms to the language -independent action concepts in IMAGACT. Instead, we identify an equivalent verb, along with a specific synset in Wo rdNet. For instance, we identify the idiom \"turn a deaf ea r to\" with the , connected to the WordNet synset [neglect, ignore, disregard]. 4. Ongoing Work We have tested our categories and criteria against the full set of corpus occurrences for five verbs (turn, cross, pull, close, combine), creating types to account for all the occurrences. Although this exercise has largely sup ported the applicability of our schema, it has also raised some questions that we are still in the process of resol ving. For some highly frequent verbs, like to turn, we find a few, very common marked types. For others, like to pull, we find a myriad of different marked types, many of wh ich occur only once or twice in the corpus. How to effi ciently account for these rare types remains an open questi on. We have also discovered verbs with marked usages th at 1 In some cases, a metonymic use of a verb seems to h ave been further extended into a metaphor. Rather than creat e a complex annotation scheme where categories can interact, we have provisionally decided to treat these as metaphors. do not seem fit into any of our three categories, s uch as Mary received the wire transfer . In these cases the verbs appear to have the same meaning as one of the prima ry, physical types for that verb, but to be acting on o bjects that are not strictly physical. We are in the proce ss of evaluating a fourth type to account for these usage s. We plan to evaluate further our marked categories a nd methodology for type creation by annotating the ful l set of corpus occurrences for a larger set of action verbs from the IMAGACT ontology, a set that includes verbs tak en from each of the upper level nodes of the ontology. Based on the results, we will finalize the annotation int erface, then use it to process all of the marked occurrence s identified by the original IMAGACT annotation. We anticipate supplementary annotation to account for thematic roles and the possible regularities among types that they may reveal (Brown & Palmer 2012). We expe ct this work to lead to a rich study of the relation b etween the marked and primary types of high -frequency verbs. 5. References Conceptual Metaphor Home Page. http://www.lang.osaka -u.ac.jp/~sugimoto/MasterMeta phorList/MetaphorHome.html IMAGACT. http://www.imagact.it Brown S. W., & Palmer n of metaphorical verbs with VerbNet. Interoperable -8), 8th -ACL/SIGSEM Workshop, Pisa, October. Frontini, F., De Felice, I., Khan, F., Russo, I., M onachini, M., Gagliardi, G. & Panunzi, A. (2012). Verb interpretation for basic action types: Annotation, induction and creation of prototypical sce nes. CogAlex -III Workshop as part of the COLING 2012 conference. Mumbai (India), December. Gagliardi, G. (2014). Validazione dell'ontologia dell'azione IMAGACT per lo studio e la diagnostica del \"mild cognitive impedirment\" (MCI). PhD Dissertation, University of Florence. Goossens, L. (1995). Metaphtonymy: The interaction of metaphor and metonymy in expressions of linguistic action. In L.Goossens, P. Pauwels, B. Rudzka -Ostyn, A. Simon -Vanderbergen & J. Vanparys (eds), By Word of Mouth. Amsterdam: John Benjamins, pp. 159 -- 174. Lakoff, G. (1987). Women, Fire, and Dangerous Things: What Categories Reveal about the Mind. Chicago: University of Chicago Press. Moneglia, M. (in press). Natural language ontology of action. A gap with huge consequences for natural language understanding and machine translation. In Z. Vetulani, J. Mariani (eds), Post LTC 2011 M., G., Panunzi, Frontini, F., Russo, I. & Monachini, M. (2012). IMAGACT: Deriving an action ontology from spoken corpora. 8th Joint ISO -ACL/SIGSEM Workshop, Pisa, October. Wittgenstein, L. (1953). Philosophical Investigations. Heidelberg kerstin.kunz@iued.uni-heidelberg.de Abstract This paper describes a set of procedures used to semi-automatically annotate a multilingual corpus on the level of cohesion, an important linguistic component of effectively organised and meaningful discourse. The annotation categories we operate with base on different degrees of granularity and account for lexico-grammatical and semantic aspects of different types of cohesion. This annotation scheme allows us to compare and differentiate cohesive features across languages, text types and in written and spoken discourse on different levels of abstraction. Our aim is to obtain a ne-grained and highly precise annotation, at the same time avoiding purely manual annotation. Therefore, we decide for corpus-based semi-automatic procedures to identify candidates expressing cohesion in English and in German. The annotated corpus is one of the few existing resources supporting contrastive studies of cohesion. Keywords: cohesion, discourse relations, annotation, corpora 1. Introduction Cohesion is an important component of effectively organ- ised and meaningful discourse, as the message being com- municated in discourse is not just a set of clauses, but forms a unied, coherent whole. While coherence con- cerns the cognitive aspects of establishing meaning rela- tions during text processing, cohesion involves explicit lin- guistic means that signal how clauses and sentences are linked link together to function as a whole. Both con- cepts have been studied in a range of disciplines, includ- ing philology, sociology, philosophy, psychology, computer science and linguistics. The latter analyses inventories of the linguistic markers that are available in a given language, see (Louwerse and Graesser, 2005). Classications of lexico-grammatical markers and their relational potentials are quite often language specic, cf. (Halliday and Hasan, 1976; De Beaugrande and Brinker, 2005), etc. For multilingual analysis, e.g. contrastive linguistics or translation (both human and machine) studies, it is impor- tant to establish categories which enable the comparison of inventories across languages in order to identify common- alities and contrasts. Complex annotations on higher lin- guistic levels which are geared towards high precision are typically carried out manually and hence, are very time- consuming. To our knowledge, existing resources provide annotations of individual cohesive phenomena only, e.g. pronominal coreference in the BBN Pronoun Coreference and Entity Type Corpus, (Weischedel and Brunstein, 2005), verbal phrase ellipsis in (Bos and Spenader, 2011) or con- junctive relations in PDTB, (Prasad et al., 2008), annotation of (abstract) anaphora in (Dipper and Zinsmeister, 2009) and (Dipper et al., 2012). Most of them are monolingual and apply manual annotation procedures only. In the present paper, we suggest procedures to semi- automatically identify and annotate cohesive phenomena.2. Motivation and Theoretical Background As already mentioned above, cohesion plays an important role in discourse organisation and coherence. Our main in- terest lies in comparing the realisation of cohesive strategies in different languages and also in written and spoken text types via empirical methods. Therefore, one major chal- lenge is to dene categories that enable identication of commonalities and differences in terms of various cohesive aspects. Our concept of cohesion is based on Halliday and Hasan's denition as relations of meaning that exist within the text, and that dene it as a text, see (Halliday and Hasan, 1976). Hence, our long-term focus is on the investigation of the semantic or conceptual relation as such. Cohesive rela- tions, however, require a linguistic trigger, a cohesive de- vice which explicitly signals that there is a relation to an- other textual expression. These devices can be grammar- or vocabulary-driven. As claimed by (Louwerse and Graesser, 2005), grammar-driven cohesion refers to the semantic re- duction of expressions to functional items which are syn- tactically obligatory, such as proforms. V ocabulary-driven cohesion refers to the lexical vocabulary of the discourse segment. Halliday and Hasan (1976) describe ve main types of cohesion in English, for which we adopt for our multilingual analysis: reference ,substitution ,ellipsis ,con- junction andlexical cohesion . Although their classication claims to be mainly semantic, it is influenced by lexico- grammatical patterns that reflect systemic features of En- glish. We therefore attempt a more conceptual classica- tion which is suitable for the comparison of English and German: Reference involves identity between instantiated refer- ents/entities, as in example (1). Substitution/ellipsis expresses similarity between different instantiated refer- ents/entities of the same type, see examples (2) and (3) re- spectively. Conjunction concerns the logico-semantic rela- tions between propositions (e.g. addition, contrast, cause)57- see example (4). Lexical cohesion includes similarity between referents/entities of the same type which bases on sense relations between lexical items (e.g. hypernymy, part-whole relations), as in example (5). (1) a. Wir f \u00a8ur Wohlstand und Chancen, weil dasrichtig ist. Wir tun damit das Richtige. b. We work for prosperity and opportunity be- cause they're right. It's the right thing to do. (2) a. Das war ein Problem. Aber keins , mit dem ich mich auseinandersetzen wollte. b. This was a problem. But not oneI chose to deal with. (3) a. Who says that? - My parents/circlemultiplytext. b. Wer sagt das? - Meine Eltern/circlemultiplytext. (4) Sicherheitspolitik mit auf den Weg gebracht. b. They want Europe to be strong in the world. That's why Britain has helped launch a Euro- pean security policy. (5) a. V or allem m Entwicklungsbanken Humankapital aufwen- den. b. First and foremost, the development banks must focus their efforts... To start, the banks should devote more resources to the develop- ment of human capital. According to Halliday and Hasan (1976), what distin- guishes cohesive relations from other semantic relations is that the lexico-grammatical resources, i.e. the cohesive de- vices, trigger relations that transcend the boundaries of the clause. We argue that these semantic relations are realised in both languages under investigation and also across text types in- cluding written and spoken discourse. Systemic compar- and Lapshinova-Koltunski, in press), have shown that they differ in terms of lexico-grammatical pat- terns of realisation, as can be see from the examples above. In addition, we suggest textual contrasts in the frequency of cohesive devices, in types of preferred cohesive relations, in the strength of the cohesive relation, as well as in the breadth of variation. Starting from these considerations, we formulate subcate- gories of the ve phenomena of cohesion dened above, reflecting the lexico-grammatical and semantic features of thecohesive devices that establish these types. Only those categories are dened which are applicable for both English and German, see table 1. Our analysis also includes cohesive relations , which are of- ten described as relations across grammatical domains, e.g. in (Halliday and Hasan, 1976; Eckert and Strube, 2000; andlexical chains . They both involve a textual relation that is created between linguistic expressions.reference type function personal head, modier, it- endophoric demonstrative head, modier, local, temporal, pronomi- sem.type connects, causal, nominal, verbal, clausal lexical cohesion general nouns, collocations Table 1: Cohesive devices and their functions The textual relation of coreference evokes a conceptual re- lation of identity between discourse referents/entities (see above). A coreference relation links at least two corefer- ring expressions: an antecedent, i.e. a linguistic element introducing a new discourse referent, and a cohesive de- vice of reference which functions as an anaphora (or cat- aphora, in the case of forward reference) and which points to the same referent again. The cohesive device of refer- ence serves as a linguistic marker which triggers a search instruction to its antecedent e.g. a semantically weak pro- form or a deictic element. We include all categories dened for reference (see table 1) for the analysis of anaphoras. As several anaphoras may point to the same antecedent, sev- eral textual relations may be created for one referent in the same discourse, hence a coreference chain is the set of all coreferring expressions which refer to the same antecedent. The same applies to lexical cohesion, although the meaning relation established is a different one (see above): a lexical chain contains at least two lexical expressions in different textual parts which are linked by a semantic relation of hy- pernymy (e.g. a specic noun linked to a general noun), meronymy, synonymy, etc. or by repetition of the lexical base. Again, the chain may contain more elements and hence also semantic relations, which tie textual referents that belong to the same experiential or semantic domain. 3. Corpus resources The multilingual corpus we annotate offers a continuum of different text types (registers) from written to spo- ken discourse. More precisely, it includes English and German texts of ten registers, eight of which represent written discourse and include ctional texts (FICTION), political essays (ESSAY), instruction manuals (INSTR), popular-scientic texts (POPSCI), letters to shareholders (SHARE), prepared political speeches (SPEECH), tourism leaflets (TOU) and corporate websites (WEB). This part was imported from the existing corpus CroCo described in58(Hansen-Schirra et al., 2013). The written texts are saved in two subcorpora according to the language: English writ- ten texts (EO), German written texts (GO). The other reg- isters are of spoken discourse and include recorded and transcribed interviews, as well as academic speeches, see (Lapshinova-Koltunski et al., 2012). The spoken texts are also stored in two subcorpora classied according to the language of origin: English spoken texts (EO-SPOKEN) and German spoken texts (GO-SPOKEN). The whole num- ber of words contained in the corpus comprise ca. 730 thou- sand words (see table 2, although not big, but still provides a usefull data set for annotating and analysing cohesion in both languages. register EO 364565 369675 Table 2: Corpus constellation and size The corpus is pre-annotated on several levels, which in- clude information on tokens, lemmas, morpho-syntactic features (e.g. case, number, etc.), parts-of-speech, phrase chunks and their grammatical functions, as well as and sen- tence boundaries. The annotation of the written part was partly imported from CroCo, whereas for the spoken part, we use Stanford POS Tagger (Toutanova et al., 2003) and the Stanford Parser (Klein and Manning, 2003). The corpus is encoded in the CWB format (CWB, 2010) and can be queried with Corpus Query Processor (CQP) (Evert, 2005). These annotation levels allow us to obtain additional infor- mation on cohesive phenomena and cohesive relations, i.e. coreference: morpho-syntactic preferences of antecedents and anaphoras, their positions in a clause, the length of chains in terms of elements, diversity of types of an- tecedents, their parallelism with anaphoras, etc. Further- more, these annotation levels provide the basis for the semi- automatic procedures described in the present paper. 4. Annotation of Cohesion 4.1. Categories to annotate In the following, we provide a more detailed description of the annotation scheme based on the categories introduced in 2. above. Note that this mainly concerns the classications of cohesive devices, which, however, builds the basis for the analysis of cohesive relations (see below). Main cate- gories exist for the main cohesive types reference, conjunc- tion, substitution, ellipsis and conjunction. We distinguish subtypes, which are annotated as 'type' or 'func' feature in the corpus. They reflect general structural groupings of cohesive devices that exist in both languages.These categories, as well as their language realisations (op- erationalisations) are presented in table 3. Each subcategory of reference (type) is further subclassi- ed according to grammatical and semantic features of the cohesive device (func). Personal reference includes per- sonal (head) and possessive (modier) pronouns as well as their morphological variants. For this type, we also anno- tate reference by it/es separately (it-endophoric) due to the ambiguity of their usage in both languages. Demonstrative reference is expressed by means of demonstrative pronouns (head) and determiners (modier), as well as their morpho- logical variants (in German). Moreover, we include local and temporal relations of identity, which are expressed by adverbs (see table 3) as well as pronominal adverbs (pron- adv). These exist in English and German but are employed in German with a higher frequency. Comparative reference is expressed with comparative forms of adjectives, which either trigger a general relation of comparison or a more specic one (particular). Conjunction is classied in terms of main syntactical types: coordinating conjunctions (connects), subordinating con- junctions (subjuncts) and discourse adverbials (adverbials). They may consist of one or multi-word constructions of conjunctions, e.g. that is why , etc. see table 3. For each syntactical subcategory we provide the same semantic sub- classications, according to the main logico-semantic rela- tions that can be established by conjunctive devices. Both in English and in German, substitution is expressed by indenite pronouns or other nominal substitutes (nom- inal), substituting verbs (verbal) and different adverbials, which constructions, such as soin En- glish (clausal). Ellipsis can be triggered by different lexico- grammatical means in both languages, and therefore, au- tomatic detection still remains problematic. Nevertheless groupings can be made in terms of which structural ele- ments are mainly omitted in relation to the preceding full textual structure. Again main categories here are nomi- nal, verbal and clausal. Substitution and ellipsis cannot be categorised in terms of other features since their language- specic features do not allow a common subclassication. For the time being, only two aspects of lexical cohesion are categorised: Textual relations that base on the use of general nouns, for which we use lists of nouns based on those described by (Dipper et al., 2012) and repetitions of lexical bases. We plan to integrate sense relations such as hypernymy and synonymy in the future. 4.2. Annotation of Cohesive Devices Automatic procedures To annotate the categories pre- sented in 4.1., we elaborate a set of semi-automatic proce- dures, which involve an iterative extraction-annotation pro- cess. We use a method derived from the system used for the YAC chunker, see (Kermes and Evert, 2002; Kermes, 2003). The system is based on the option of the CWB tools to incrementally enhance corpus annotations, as query re- sults deliver not only concordances of the searched struc- tures but also information on their corpus positions. The algorithm makes use of the CWB Perl-Modules to access CQP and the encoding functionality using Perl-scripts as wrapper. Additionally, Perl modules are derived from the59device type func realisation so/so/dergleichen ellipsis nouns problem/Problem, situation/Situation, position/Position , etc. Table 3: Annotated categories of cohesion framework of YAC which facilitate the annotation of in- formation gathered using CQP queries. This permits to import the information on queried data back into the cor- pus. In this way, our annotation rules are dened in form of CQP queries that allow regular expressions based on string, parts-of-speech, chunk and further constraints. Each query is applied to the corpus separately. The re- sult is a list of corpus positions indicating the start and the end, and possibly a target position marked within the query. These corpus positions can now be used to extract additional information already encoded in the corpus (e.g., part-of-speech tags, lemma information, sentence position, etc.). If needed, this information can be evaluated against lists in order to classify or exclude them. Finally, the results and possibly additional information can be encoded using the corpus positions as anchors. In table 4, we demonstrate examples of CQP queries to extract and annotate reference. Query 1 is designed to extract textual instances of local demonstrative reference, whereas query 2 delivers occurrences of demonstrative ref- erence with the grammatical function of a modier. The results do not need further processing within the annota- tion process, as the categorisation is encoded in the query itself. The instances are annotated as XML structures with attributes 'type' and 'func', where 'type' is demonstrative and 'func' is either temporal or modier respectively. The tags are then imported back into the corpus and saved as CQP structural attributes. Further two queries are built to extract semantic (query 3) or syntactic (query 4) types of conjunctions. In the nal step,QP query example of Table 4: Examples of queries and tagged structures in XML we combine both annotations (syntactic and semantic) to exclude non-cohesive occurrences of conjunctions, e.g. in case they link phrases and not clauses, as in the sentence in example (6). (6) Renewables 2004 will focus on renewables andaim at strengthening the political momentum. Classication of semantic types proceeds directly within the query which includes a simple lexical search - here, we aim to identify all cohesive instances of 'additive' conjunc- tions (a closed class of lexical items the members of which we know). The same procedure (based on lexical list) is applied to identify and annotate general nouns. Manual procedures As our aim is to produce a corpus with highly precise information on cohesive devices in En-60glish and German, we integrate a step of manual correc- tion into our procedures. To facilitate this, the annotated corpus (with the structures in XML format as shown in table 4 above) is imported into MMAX2, a tool for man- ual annotation (M \u00a8uller and Strube, 2006). Texts are cor- rected by at least two human annotators with linguistic background. The MMAX2 visualisation allows annota- tors to decide whether the candidates tagged by the auto- matic procedures have a cohesive function and belong to the given category. We also add an option to mark the cases as 'problematic' or 'non-problematic' to trace and analyse the reasons for annotators' hesitation in case of a low inter-annotator agreement. This combination of au- tomatic pre-annotation with manual post-correction is less time-consuming for human annotators as annotating raw texts. Moreover, we achieve positive results in the inter- annotator agreement (see below in this section). Correction by human annotators allows us, on the one hand, to improve both annotations and rules for automatic proce- dures (rule-based identication of items can be improved on the basis of human annotators' observations), and on the other hand, to evaluate automatic procedures. Evaluation Our preliminary results show that in the au- tomatic identication of cohesive devices, we are able to achieve a good precision for English (between 76% and 98%) and slightly lower precision for German (between 69% and 73%), shown in table 5. The lower results for German are partially caused by the multi-functionality of the lexico-grammatical means expressing cohesion1. In ad- dition, higher flexibility of ordering clausal constituents in German complicates automatic disambiguation of cohesive and non-cohesive forms on the basis of syntactic rules. In terms of recall, we are also able to achieve satisfactory re- sults for English, e.g. 80% for reference and 73% for con- junction, and lower results for German: 60% for reference and 71% substitution 0.84 0.71 Table 5: Precision of automatic procedures We also calculate the inter-annotator agreement 1) between human annotators (HuHu) and 2) between human annota- tors and automatic procedures (HuAut), see table 6. The best scores are again observed for English in the agreement between humans and the automatic system. For German, the score is lower. However, the agreement between human annotators is slightly higher in the annotation of German. This can be explained, again, by the complexity of German lexico-grammatical means expressing cohesion. Annotation procedures are especially problematic in spo- ken registers, where cohesive devices are much more fre- quent as in written registers, see gure 1 in section 4.3. below. Spoken discourse is characterised by numerous 1See, for example, (Lapshinova-Koltunski and Kunz, in press) for the examples of for reference repairs, ellipses, unclear sentence breaks and therefore. Cohesive and non-cohesive instances cannot be easily di- ambiguated as sentences boundaries do not play a role in spoken discourse. This all poses a real challenge for both semi-automatic and manual annotation. language register precision TOTAL 0.59 0.62 0.60 Table 7: Evaluation of procedures in spoken registers We calculate precision and recall for automatic reference annotation in our spoken data. As seen from table 7, the overall results for English spoken registers are better than for German. Interestingly, the register-specic results differ in both languages. Whereas in English the system performs better on academic speeches (which are mostly monologic), interviews are annotated with less errors than academic speeches in German. 4.3. Annotation of Coreference For the annotation of coreference, we use the output of the semi-automatic procedures described in 4.2. above and manual annotations produced by humans. To our knowl- edge, none of the existing automatic procedures can t our tasks, as most of them operate with a limited set of cat- egories. Moreover, previous works on coreference anno- tation for spoken discourse, have shown that the available systems can achieve around 60% for written and ca. 50% for spoken texts, see, for instance, (Amoia et al., 2012) for the analyses with Stanford CoreNLP (Lee et al., 2011). Therefore, we decide for manual annotation of coreference chains, which includes manual identication of antecedents by human annotators, and their linking to the cohesive de- vices (anaphoras) which were automatically tagged by our system described in 4.2. above, and manually corrected by human annotators. Here again, we use MMAX2 to facili- tate the annotations, as this tool allows visualisation of links between two or more elements. The annotated information is then encoded as an additional attribute of 'mention', which is automatically provided with an identication number (id). Every expression referring to the same antecedent is also assigned with the same id. This information is saved for every text, and then imported into the corpus. The information on the chains can then be extracted with the help of these ids. The information on the type and function of the referring expression is also integrated into this new structure, see gure 1. In the example presented in gure 1, the items indexed with61<mention >received > > went on to the Mathematical Sciences Research Institute to do post-doctoral research. and <mention chain >he</mention > was an assistant professor for two years under the National Science Foundation Fellowship. <mention chain id=\"set 1\" type=\"pers\" func=\"head\" >he</mention >completed the post-doctoral research fellowship at the Institute for Advanced Study in the following year. Figure 1: Annotated coreference chains in the corpus 'set1' belong to a longer chain. Four anaphoras refer to the same antecedent, which is 'Dr Hales'. Lexical chains have not yet been annotated in the corpus. However, we aim to use the annotation of general nouns, as well as repetitions of lexical bases, and integrate semantic relations with the help of available resources, e.g. Word- Net, see Fellbaum (1998). These automatic annotations will then be corrected in terms of cohesiveness by human annotators. 4.4. Annotation Availability The annotated corpus is available in XML format and can be queried with CQP. We also provide a CQP-WEB2ver- sion which is available via CLAIN-D project. 5. Conclusion and future work In the present paper, we have described semi-automatic corpus-based procedures to annotate cohesive types of (co- )reference, substitution, ellipsis, conjunction and lexical cohesion. These procedures allow both automatic identi- cation of cohesive devices and their automatic annotation, which builds the basis for further annotation of semantic relations. Moreover, the integrated procedure of manual correction enables evaluation and improvement of the au- tomatic procedures. Furthermore, they provide a possibil- ity of consistent annotation on the basis of the pre-dened rules, which cannot be ensured if the entire annotation is of manual character. 2cf. (Hardie, 2012)Our procedures concern two Germanic languages only, which have many common or comparable categories. Therefore, it would be interesting to test the proposed ap- proach on another language pair including languages that belong to different language families. However, this is be- yond the scope of the present research project. The enriched corpus facilitates analysis of German vs. En- glish contrasts, providing information on cohesive phenom- ena in both languages. Moreover, the availability of spoken material in our corpus allows the analysis of differences which result from differing conditions of speech, such as strong relation to the communication situation, direct in- teraction of speech participants and constraints on cogni- tive processing. First ndings from our analyses show that mode of production plays an essential role for the group- ing of particular registers in the two languages separately, and also across languages. For instance, the spoken reg- isters in both languages exhibit a tendency towards mark- ing important entities, comparing and evaluating them via cohesive relations. Their lexico-grammatical realisations are partially language-specic. Furthermore, we observe greater variation between written and spoken registers than in English, which may nd further support in the future, when more spoken registers containing speaker turns are integrated in our corpus. Such a resource is valuable not only for contrastive linguis- tics, but also for translation study, including machine trans- lation, as well as further areas of NLP, e.g. automatic coref- erence resolution. The empirical data obtained from these annotations can be interpreted in terms of various linguis- tic aspects on different levels of granularity. It can thus be employed for further investigation and interpretation on se- mantic and conceptual levels of abstraction. 6. References M. Amoia, K. Kunz, and E. Lapshinova-Koltunski. 2012. Coreference in Spoken vs. Written Text: a Corpus-based Analysis. In Proceedings of the the 8th international conference on Language Resources and Evaluation . M. Ariel. 2001. Accessibility theory: An overview. In Ted Sanders, Joost Schliperoord, and Wilbert Spooren, ed- itors, Text representation , Human cognitive processing series, pages 29-87. John Benjamins. J. Bos and J. Spenader. 2011. An annotated corpus for the analysis of VP ellipsis. Language Resources Methoden . Erich Schmidt, Berlin. 6 edition. 2010. The IMS Open Corpus Workbench. http://www.cwb.sourceforge.net. R. De Beaugrande and W. U. Dressler. 1981. Introduction to text linguistics / Robert-Alain de Beaugrande, Wolf- gang Ulrich Dressler . Longman, London New S. Dipper and H. Zinsmeister. 2009. Annotating Discourse Anaphora. In Linguistic Annotation Workshop , pages 166-169. The Association for Computer Linguistics. S. Dipper, M. Seiss, and H. Zinsmeister. 2012. The Use of Parallel and Comparable Data for Analysis of Abstract62Figure 2: Annotated corpus on CQP Web Anaphora in German Association (ELRA). Eckert and M. Strube. 2000. Dialogue Acts, Synchro- nizing Units, and Anaphora Resolution. Journal of Se- mantics , 17:51-89. S. Evert, MIT Press, Cambridge. M.A.K. Halliday and R. Hasan. 1976. Cohesion in En- glish . Longman, London. S. Hansen-Schirra, S. Neumann, and E. Steiner. 2013. Cross-linguistic Corpora for the Study of Translations. Insights from the Language Pair English-German . de Gruyter, Berlin, New York. A. Hardie. 2012. CQPweb - combining power, flexibil- ity and usability in a corpus analysis tool. International Journal of Corpus Linguistics , 17(3):380-409. H. Kermes and S. Evert. 2002. YAC - A Recursive Chun- ker for Unrestricted German Text. In Manuel Gonzalez Rodriguez and Carmen PazSuarez Araujo, editors, Pro- ceedings of the Third International Conference on Lan- guage Resources and Evaluation. , pages 1805-1812. H. Kermes. (and for Computational Lexicography . Ph.D. thesis, Universitt Stuttgart. D. Klein and C. D. Manning. 2003. Accurate Unlexical- ized Parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL '03, pages 423-430, Stroudsburg, PA, USA. As- sociation for E. press. Cohe- sive conjunctions in English and German: Gentens, Caroline Corpora . Rodopi, Amsterdam. K. and E. Steiner. 2012. Cohesive substitution in En- glish and German: a of 201-231. John Benjamins, Amsterdam. K. Kunz and E. Steiner. 2013. Towards a comparison of cohesive reference in English and German: System and text. In Maite sis. Functional and Corpus Perspectives , pages 208-239. Equinox, London. K. Kunz. 2009. Variation in English and German Nominal Coreference . Ph.D. thesis, Saarland University, Conjunc- tions across Languages, Registers and Modes: semi- automatic extraction and annotation. In A. Diaz Negrillo and F. J. Daz Prez, editors, Specialisation and Varia- tion in Language Corpora . Peter Lang. Papers from the CILC2012. Jaen, Spain, March 2012. E. Lapshinova-Koltunski, K. Kunz, and M. Amoia. 2012. , Firenze University Press. H. Lee, . Peirsman, A. Chang, N. Chambers, M. Sur- deanu, and D. Jurafsky. 2011. Stanford's multi-pass M.M. Louwerse and A.C. Graesser. 2005. in discourse. Strazny, editor, Encyclopedia of linguis- tics, pages 216-218. Fitzroy Dearborn, Chicago. C. M \u00a8uller and M. Strube. 2006. Multi-level annotation of63linguistic data with MMAX2. In Sabine Kurt Kohn, Joybrato Mukherjee, editors, Corpus Tech- nology and Language Pedagogy: New Resources, New Tools, New Methods , pages 197-214. Peter Lang, Frank- furt a.M., Germany. R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. K. Joshi, and B. L. Webber. 2008. The Penn Dis- course TreeBank 2.0. In LREC . European Language Re- sources Association. K. Toutanova, D. Klein, C. D. Manning, and Y . Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL '03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology , pages 173-180, Morristown, NJ, USA. Association for Computational Linguistics. R. Weischedel and A. Brunstein. 2005. BBN Pronoun Coreference and Entity Type Corpus.64Spatio-temporal grounding of claims made on the web, in PHEME Leon Derczynski and Kalina Bontcheva University of Shefeld S1 4DP, UK leon,kalina@dcs.shef.ac.uk Abstract Social media presents us with a digitally-accessible sample of all human discourse. This sample is full of claims and assertions. While the state of the art in NLP is adapting to the volume, velocity and variety of this sample and the information in it, the accuracy of claims made in social media remain largely unstudied. P HEME , a 36-month EU project started in January 2014, focuses on this fourth challenge: veracity. As a core part of establishing veracity, we need to identify the spatio-temporal context of assertions made on informal websites. This project note introduces the spatio-temporal challenges and planned semantic annotation activities that are part of the P HEME project. 1. Introduction Social networks are rife with lies and deception, half-truths and facts. Irrespective of an assertion's truthfulness, the rapid spread of such information through social networks and other online media can have rapid and serious conse- quences. In such cases large amounts of user-generated content need to be analysed quickly, yet it is not currently possible to carry out such complex analyses in real time. Social media poses three major computational challenges, dubbed the 3Vs of big data: volume, velocity, and vari- ety (Laney, 2001). Content analysis methods have faced additional difculties, arising from the short, noisy, and strongly contextualised nature of social media. In order to address the 3Vs of social media, new language technolo- gies have emerged, e.g. using locality sensitive hashing to detect breaking news stories from media streams (volume), predicting stock market movements from microblog senti- ment (velocity), and recommending blogs and news articles based on user content (variety). PHEME1focuses on a fourth crucial challenge: veracity. It will model, identify, and verify phemes - internet memes annotated for truthfulness or deception - as they spread across media, languages, and social networks. One of the many challenges in determining veracity is the automatic extraction of a claim's context. As well as under- standing complex social context, it is critical to know when and where each claim was made, or to when and where it was intended to apply. This project note discusses the role of spatio-temporal information extraction and reasoning in solving this challenge. 2. Motivation PHEME addresses the spatio-temporal validity of informa- tion and historical content to assess contradictions, through means of regional and longitudinal models of users, net- works, trust, and influence. The temporal delimitation of any assertion is of great im- portance, because the assertion is true only inside these bounds. Specically, it is possible to extract two truths that seem to contradict (e.g. \"The president of the USA is 1The project is named after the Greek goddess Pheme, who was the personication of fame and renown; her favour being no- tability, her wrath being scandalous rumours.George W Bush\" and \"The president of the USA is Barack Obama\") but are in fact both accurate when the appropri- ate temporal information is added. In other words, there is something like temporal validity of facts, which needs to be taken into account when detecting contradictions. Similarly, assertions have spatial constraints, especially when they discuss underspecied entities. For example, we may say \"The president is Obama\" and \"The president is Hollande\"; these assertions seem to conflict, but are in fact both true simultaneously - just in distinct spatial regions. It may not always be possible to ground assertions using single mentions of relations. Assertions may be spread over multiple documents, each mentioning different constraints. However, failing to determine the bounds of assertions - or assigning incorrect dates and places to claims - poten- tially leads to the rejection of correct information, reducing our overall ability to detect and ground/refute rumours in real-time. Spatio-temporal reasoning and inference offer solutions to these problems, and P HEME seeks to advance spatio-temporal relationship extraction to support measure- ment of veracity on the web. 3. Background Unlike traditional news, a notable proportion of social me- dia content posted online is explicitly geotagged (Sadilek et al., 2012), and studies suggest that it is possible to infer the geo-locations of about half of the remaining such con- tent (Rout et al., 2013). Social media messages also have at least a creation time as temporal context. This implicit spatio-temporal (ST) metadata is not currently heavily ex- ploited by modern NLP methods. Given the constraint that a single entity can only be in any one place at a time, these forms of ST information give a means of determining the truthfulness of statements (Ji and Grishman, 2011; Derczynski and Gaizauskas, 2013). Temporally, current systems are capable of detecting the publication date of documents (Chambers, 2012) and of grounding some of the time expressions contained therein (Str \u00a8otgen and Gertz, 2010). Detecting events and assertions and temporally ordering these with regard to times is critical to ST grounding of facts and rumours; the state of the art in event detection is good (Kolya et al., 2012), but ordering events and times relative to each other65or across documents remains an active area of novel re- search with some progress to be made. Fortunately, linking events to times - the most important type of temporal asso- ciation for P HEME - is the task at which automated systems perform best (Derczynski, 2013). Spatially, the challenge of grounding the locations in docu- ment content is critical to accurate bounding of claims. The state of the art is somewhat less mature than that of tempo- ral context; while many tools can identify a range of named entities, recognition of new families of spatial entities (es- pecially when general nouns are used in a spatial sense) is a subject of active research, e.g. Gaizauskas et al. (2012). Spatio-temporal annotation in P HEME serves as one com- ponent in a complex system, linked together with longitudi- nal user behaviour modelling, information provenance, net- work structure, a-priori knowledge, and cross-media links. 4. Digital journalism Journalists are currently using a plethora of social me- dia applications in order to meet their diverse needs, e.g. Tweetdeck2for monitoring the social web; Storify3for news content ltering sites like Storyful.5The focus of all these tools is on getting the right content to journalists, but not on helping them with inter- pretation and verication of the authenticity and credibility of that content. Methods and tools vary according to the nature of the journalistic task, however. For example, ob- servations of the Guardian newsroom (Procter et al., 2012) revealed that journalists prefer simple Twitter clients rather than more sophisticated tools such as Tweetdeck in activi- ties such as live blogging. For reliability's sake, journalists prefer to rely on sources that their experience suggests they can trust. This solves the problem of reliability but limits their capacity to exploit social media to its full potential. Spatio-temporal knowledge plays also an important role in this use case. A key challenge is to identify the regionality of events (e.g., neighbourhood, city, or country level) (Xu et al., 2012). Regionality is important because different events are relevant at varying scales, which impacts their newsworthiness and interestingness to digital journalists and users interested in local content. 5. Content Annotation The project involves the creation of new language re- sources. These in turn helps create and evaluate general- purpose tools for projecting spatio-temporal annotations across languages, given parallel texts and WikiwarsDE). resources will be used to develop multilingual temporal annotation tools, based on their state-of-the-art techniques, developed for longer texts. The project also addresses the problem of geo-locating events mentioned in documents. We intend to http://www.tweetdeck.com/ 3See http://www.storify.com/ 4See http://www.ushahidi.com/products/swiftriver-platform 5See document, and use disam- biguated URIs (e.g. against GeoNames6) and additional knowledge from the LOD resource (e.g. NUTS subdivi- sions, latitude/longitude, neighbouring locations). schemata, the de standards of ISO-TimeML (Pustejovsky et al., 2010) and ISO- Space (Pustejovsky et al., 2011) will be adopted and exper- imented with. Following Pustejovsky and Stubbs (2011), we intend to use temporal narrative containers for anno- tating events. In addition, recent adaptations of narrative containers to spatial annotation will be tried (Pustejovsky and Yocum, 2013). Narrative containers promise to lighten human annotator load while still capturing expressive rep- resentations of spatio-temporal information. Standoff annotation may be required in some scenarios, as social media data typically has strict licensing constraints. Existing standards provide a framework for annotating the factuality of assertions (e.g. Saur \u00b4 and Pustejovsky (2009)), which can be applied over social media text in order to for- malise the strength of assertions made there. In the scope of rumour detection and analysis, Qazvinian et al. (2011) annotated messages for whether or not they related to a pre-determined rumour. P HEME involves two additional challenges: identifying the rumours in the rst place, and then identifying the type of rumour from one of four classes: misinformation, disinformation, controversy and speculation. For the project, a \"code frame\" system is under development (Procter et al., 2013) for annotating top- ics and actor types. Code frames are specic to a research question that embodies information demand. Rumour mes- sages are subdivided into categories, which may be related to claims, counter-claims or appeals for information; be with or without evidence; or simply rumour-relevant com- ments. Streams of related messages are categorised using code frames and annotated accordingly. 6. Project Contribution PHEME aims to further the state of the art in spatio- temporal annotation and reasoning. In order to spatio- temporally ground asserions, P HEME will adapt existing annotation tools to social media data, through the creation of new training data in this genre. The project will also cover new target languages through lightly-curated annota- tion porting, taking advantage of the language-independent nature of grounded spatial and temporal data. Another important benet of storing and analysing \"tra- ditional\" and social media content over space time is that these archives enable longitudinal analyses (Derczynski et al., 2013). For instance, longitudinal analyses on the online social graphs can reveal the evolution of social relationships and thus build models of trustworthiness and authority. It is also possible to start building user proles over time, in- cluding previously spread rumours and, in general, what users talked about in the past. Focused on specic events, longitudinal analysis reveals discourse around events, aris- ing from both social and traditional media. Similarly, in journalism and brand and reputation management applica- tions, there is also demand for retrospective analyses of 6See http://www.geonames.org/66media content after a signicant incident (e.g. to establish whether social media was used to entice more riots). 6.1. Dataset collection The rst phase is a human pilot annotation, of events, times and places in the target genres and languages. This includes annotation of web and social media text for events and times, in order to later temporally bound assertions. It also includes the annotation of locations (both formal and infor- mal), and identication of document creation locations. Corpora are then extended using cross-linguistic projec- tion. P HEME will develop tools for projecting ST anno- tations across language (Spreyer and Frank, 2008; Costa and Branco, 2012). This allows the creation of new re- sources for English, German, Bulgarian, and possibly also the project's minor languages (French, Italian, Swahili). Following the construction of a dataset, we will build spa- tial and temporal IE systems in multiple languages. These are aimed at ST grounding. As mentioned in Section 5., we intend to follow the narrative containers scheme. This centres on nding spaces and times within which groups of events are collected, before trying to resolve the spe- cic, hard-to-annotate and potentially low-information in- dividual relations. Finally, for grounding, while documents often come with a document creation date, and document creation location is harder to come by. To address this, the project investigates spatial grounding at both document level (creation location) and at assertion/event level. Having found spatial and temporal entities in documents, it becomes possible to reason about bounds of assertions. We will apply and extend temporal reasoning and assertion bounding tools, which brings interesting challenges, partic- ularly in the social media domain where one may be faced with many short documents describing different facets of a claim. In particular, cross-document spatio-temporal rea- soning is a novel and unexplored research area. The output of these reasoning and bound-nding tools will be used as inputs to trustworthiness assessment systems. 6.2. Spatio-Temporal Information Extraction Building upon existing resources is important to the ad- vanced, complex tasks that P HEME addresses. Fully- featured ST information extraction pipelines can be built from state-of-the-art tools. Regarding temporal annotation, we begin with annota- tion primitives: timexes, events and the relations between them. These are reasonably well-researched problems in newswire, but adapting to short messages which are pushed over networks by humans - i.e. social media messages - presents challenges in terms of the large linguistic variety, and interesting opportunities, from extra information and structure in personal proles and network connections. There are existing tools that may provide initial insights into the problem. For timexes, GATE and Heidel- Time (Str \u00a8otgen and Gertz, 2012) excellent entity ex- traction; TIMEN provides an open-source normalisation re- source, and the state of the art leads to flexible parsing tools for handling previously-unseen timex formulations (Angeli et al., 2012; Bethard, 2013). Regarding event extraction, while older systems like EVITA are available, fast newersystems like TIPSem and the outcomes of the ARCOMEM project (Demidova et al., 2013) offer better performance. For linking times to events, systems like TIPSem, ClearTK- TimeML and NavyTime may be helpful. Fewer tools are available for spatial annotation. In terms of locally-accessible location annotation systems, there is ANNIE, LODIE (Damljanovic and Bontcheva, 2012) and tools resulting from SemEval exercises. Developing spatial grounding and annotation systems involves more pioneer- ing work here, beyond adapting existing tools. Importantly, the project involves the creation of new tools for social media. P HEME couples systems like the above with document grounding and temporal relation annota- tion systems which operate on new languages and domains. This will involve the creation of new systems and annota- tions for event co-reference extraction, event-based sum- marisation, and ST grounding of individual messages. For example, the TimeML <TLINK >tag allows expression of intra-document co-reference and full-interval ordering be- tween events, but need to be extended to handle both uncer- tain relations and also cross-document links. Similarly, the ability to create ISO-Space <PATH>s,<QSLINK >s be- tween <LOCATION >s in different documents is required - as well as the ability to dene common frames of refer- ences. We anticipate cross-document co-reference being in- strumental in the grounding and subsequent veracity assess- ment of a signicant proportion of claims and messages. Social media networks present an unconventional kind of discourse, with different uses of reference and anaphora when compared to longer, standalone documents. The in- vestigation of this structure will inform how annotations are used, and then leveraged for reasoning. Cross-document event co-reference is critical in order to group claims to- gether; there is no work on this in social media, but chal- lenges such as TDT generated extensive research on the general topic, e.g. (Bagga and Baldwin, 1999), and general concepts like chains provide a starting point. Construction of timelines from timexes in messages and events mentioned across the network can then help dene temporal bounds for events. Ji and Grishman (2011) excel- lent work on timelines proposes temporal bounding of as- sertions using times mentioned in collections of newswire documents, though this is all at day-level granularity. This granularity is suitable for retrospective analysis involving certain types of assertion (e.g. lifetimes), but not sufcient for realtime ltering of all kinds of events. In addition, ex- tracted temporal bounds are likely to be uncertain, and re- quire e.g. a constraint-satisfaction framework to pin down, as well as probabilistic veracity reasoning. 6.3. Evaluation There are many ways in which P HEME 's ST annotation out- put can be evaluated. Primarily, we can evaluate against a gold standard (ours, or external ones, e.g. from TempEval). A secondary round of pilot annotations, over non-projected data, provides an opportunity for GS-style evaluation, as well as creating new high-quality language resources. Basic P/R/F1 measures work for spatio-temporal entity extraction - but one also needs to account for entity specicity. This may lead to67adopting or creating a new, nuanced evaluation measure. It is difcult to evaluate spatio-temporal reasoning; prior shared tasks in these areas have demonstrated this. In addition, we can perform extrinsic evaluation using un- skilled humans. For assertion grounding, a common-sense check can be applied, asking whether a particular claim (in prose) is intended to apply to certain ST constraints. This could be formulated as dialogue or question answering. A high quality crowdsourcing approach is feasible for this ex- trinsic evaluation (Sabou et al., 2014). 7. Conclusion PHEME involves creating the necessary computational ap- paratus to model, identify, and verify phemes (internet memes with added truthfulness or deception), as they spread across media, languages, and social networks. Do- ing this raises difcult, interesting and important issues in spatio-temporal annotation of text in a wide variety of sit- uations. P HEME investigates these issues in the context of social media, examining digital journalism and healthcare. Furthering spatio-temporal information extraction research promises a better understanding of the ever-present context that the meaning language relies upon so heavily. Acknowledgments This work received funding from the European Unions Sev- enth Framework Programme (grant No. 611233 P HEME ). 8. References G. Angeli, C. D. Manning, and D. Jurafsky. 2012. Parsing time: Learning to interpret time expressions. In Proc. NAACL , pages 446-455. ACL. event coreference: Annotations, experiments, and observa- tions. In Proceedings of the Workshop on Coreference and its Applications , pages 1-8. ACL. S. Bethard. 2013. A synchronous context free grammar for time normalization. In Proc. EMNLP , pages 821-826. N. Chambers. 2012. Labeling documents with timestamps: Learning from their time expressions. In Proc. ACL . F. Costa and A. Branco. 2012. Corpus pages D. Damljanovic and K. Bontcheva. 2012. Named Entity Disambiguation using Linked Data. In Proceedings of the 9th Extended Semantic Web Conference . E. Demidova, D. Maynard, N. Tahmasebi, . Stavrakas, V . Plachouras, J. Hare, D. Dupplaw, and A. Funk. 2013. Extraction and Enrichment. Deliverable D3.3, ARCOMEM. L. Derczynski and R. Gaizauskas. 2013. for temporal bounding. In Proc. ICTIR . L. Derczynski, B. Yang, and C. Jensen. 2013. Towards Context-Aware Search and Analysis on Social Media Data. In Proceedings of the 16th Conference on Extend- ing Database Technology . ACM. L. Derczynski. 2013. Determining the Types of Tempo- ral Relations in Discourse . Ph.D. thesis, University of Shefeld.R. Gaizauskas, E. Barker, C. Chang, L. Derczynski, M. Phiri, and C. Peng. 2012. Applying ISO-Space to Healthcare Facility Design Evaluation Reports. In Proc. ISA, pages 31-38. H. Ji and R. Grishman. 2011. Knowledge base popula- tion: Successful approaches and challenges. In Proc. of ACL'2011 , pages 1148-1158. A. K. Kolya, D. Das, A. Ekbal, and S. Bandyaopadhyay. 2012. Roles of event actors and sentiment holders in identifying event-sentiment association. In Computa- tional Linguistics and Intelligent Text Processing . D. Laney. 2001. 3d data management: Controlling data volume, velocity and variety. META Group Research Note , 6. R. Procter, A. V oss, and P. Brooket. 2012. A study of using social media in journalism. Technical report, University of Warwick. R. Procter, J. Crump, S. Karstedt, A. V oss, and M. Can- tijoch. 2013. Reading the riots: What were the police doing on twitter? Policing and society , 23(4):413-436. J. Pustejovsky and A. 2011. ACL. J. Pustejovsky Z. 2013. Motion ISO-SpaceBank. In Proc. ISA , pages 25-33. J. Pustejovsky, K. Lee, H. Bunt, and L. Romary. 2010. ISO-TimeML: An International Standard for Semantic M. ISO-Space: The annotation of spatial in E. Rosengren, D. R. Radev, and Q. Mei. 2011. Rumor has it: Identifying misinformation mi- croblogs. In Preotiuc-Pietro, Bontcheva, and T. Cohn. 2013. Where's @wally? a classication approach to ge- olocating users based on their social ties. In Proc. Hy- pertext . M. Sabou, 2014. Corpus annotation through To- best In Sadilek, H. Kautz, and V . Silenzio. 2012. Modeling spread of disease from social interactions. In Interna- tional AAAI Conference on Weblogs event factuality. High qual- ity rule-based extraction and normalization of temporal In Proc. SemEval , pages 321-324. J. Nowak, and X. Zhu. 2012. So- cioscope: Spatio-temporal signal recovery from social media. In Machine Learning and Knowledge Discovery in Databases , pages 644-659. Springer.68How to exploit paralinguistic Ada, 34000 Montpellier, France Mathieu.Roche@lirmm.fr Abstract This paper addresses the issue of acronym dictionary building. The rst step of the process identies acronym/denition candidates, the second one selects candidates based on a letter alignment method. This approach has two advantages because it enables (1) to annotate documents, (2) to build specic dictionaries. More precisely, this paper discusses the use of a specic linguistic concept, the gloss , in order to identify candidates. The proposed method based on paralinguistic markers is independent of languages. Keywords: text mining, acronym expansion 1. Introduction Acronyms are numerous in specialized domain, e.g. biomedical and agronomy documents (Chang et al., 2002). An acronym is a set of characters corresponding to the rst letters of a group of words, for instance, the acronym FAO is associated with the denition Food and Agriculture Or- ganization . This paper summarizes a method to identify acronyms and expansions in documents. This automatic recognition enables to annotate these elements in texts. This work deals with the use of paralinguistic features in order to identify acronym/denition couples. After the description of related work in the following sec- tion, Section 3. describes our approach based on 2 steps: Extraction of acronym/expansion candidates (Section 3.1.), Filtering of candidates (Section 3.2.). Finally, before Discussion and Conclusion sections, experiments of our approach are detailed in Section 4. 2. Related work Among the several existing methods for acronym extrac- tion in the literature, some signicant work need to be mentioned. The acronym detection involves recognizing a character chain as an acronym and not as an unknown or misspelled word. Most acronym detecting methods rely on using specic linguistic markers. Yates' method (Yeates, 1999) involves the following steps: First, separating sentences by segments us- ing specic markers (brackets, points) as frontiers. Then the acronym/expansion couples are tested. The acronym/denition candidates are accepted if the acronym characters correspond to the rst letters of the potential denitions words. The last step uses specic heuristics to select the relevant candidates. These heuristics rely on the fact that acronyms length is smaller than their expansionlength, that they appear in upper case, and that long expansions of acronyms tend to use stop-words such as determiners, prepositions, sufxes and so forth. Therefore, the pair \"FAO/Food and Agriculture Organization\" is valid according to these heuristics. Other studies (Chang et al., 2002; Larkey et al., 2000) use similar methods based on the presence of markers associated with linguistic and/or statistical heuristics. In this context (Okazaki and Ananiadou, 2006) propose statistical measurements from the terminology extraction area. Okazaki and Ananiadou apply the C-value measure (Frantzi et al., 2000; Nenadic et al., 2003) initially used to extract terminology. It favors a candidate term that doesn't appear often in a longer term. For instance, in a specialized corpus (i.e. Ophthalmology), the authors discovered that the term \"soft contact\" was irrelevant, while the frequent and longer term \"soft contact lens\" is relevant. An advantage of C-value measure is its independence from characters alignment (actually, a lot of acronyms/denitions are relevant while the letters are in a different order, e.g. \"AW / water activity\"). Other approaches based on supervised learning methods consist of selecting relevant expansions. In (Xu and Huang, 2007), the authors use SVM approaches (Support Vector Machines) with features based on acronym/expansion information (e.g. length, presence of special characters, context, etc). (Torii et al., 2007) present a compara- tive study of the main approaches (supervised learning methods, rules-based approaches) by combining domain- knowledge. Larkey et al. 's method (Larkey et al., 2000) uses a search engine to enhance an initial corpus of Web pages useful for acronym detection. To do so, starting from a list of given69acronyms, queries are built and submitted to the AltaVista1 search engine. Query results are Web pages which URLs are explored, and possibly added to the corpus. 3. Acronym/expansion recognition Our method of construction of acronym dictionaries is based on two steps detailed in the following subsections. 3.1. Step 1: Extraction of candidates First, specic punctuation and character markers are taken into account in order to identify acronym/denition pairs (see Figure 1). In this paper, we investigate the extraction of candidates by exploiting the \"glosses\" of words and paralinguistic markers (i.e. brackets, punctuations, etc.) to detect acronym/denition candidates. Glosses are spontaneous descriptions identiable with specic markers (for example, called ,i.e., and so forth). These ones highlight lexical semantic relationships, e.g. equivalence, specication of the meaning, nomination, hyponomy, hyperonomy. The abstract pattern of glosses is given by the structure Xmarker Y1, Y2...Ynwhere XandYican be acronyms and/or denitions. The identication and selection of glosses are based on the use of patterns and Web-mining approaches (Mela et al., 2012). In this paper, we extract candidates based on the gloss markers \"(\" and \")\": Local Pattern 1 [ X=acronym, Y1=denition]: The rst pattern detects Y1(denition), between \"(\" and \")\" following the acronym ( X). For example, the sentence \"relation empirique entre v\u00b4eg\u00b4etation mesur \u00b4e maximum \" allows to extract X =NDVI andY1=Normalized Difference Vegetation Index . Local Pattern 2 [ X=denition, Y1=acronym]: The second pattern detects Y1(acronym), between \"(\" and \")\" following the denition ( X). The beginning of the denition is recognized with the rst word of the phrase in upper case. For example, the sentence \" ... la mesure Normalized Difference Vegetation Index (NDVI)\" allows to extract X=Normalized Difference Vegetation Index andY1=NDVI . Note that these patterns are independent of languages because the method is based on paralinguistic markers (i.e., brackets in this work). This is very important when languages are mixed, for instance in specialized domains. The example of Figure 1 shows a denition in English (expansion of \"NDVI\") in an abstract written in French. In this situation, we are 4 different cases of results: 1www.altavista.com/Case 1: the relevant denition is returned (like previ- ous examples), Case 2: the extracted phrase contains the relevant def- inition (i.e. partially relevant, but too large), Case 3: the extracted phrase is a part of the relevant denition (i.e. partially relevant, but too specic), Case 4: the extracted phrase is irrelevant. Both proposed patterns will be evaluated in Section 4. of this paper. 3.2. Step 2: Filtering of candidates The second step aims at removing irrelevant acronym/denition pairs and deleting irrelevant word(s) from candidate denitions. For this process, we propose to align the acronym letters with the potential denition words, by mapping each acronym letter with the rst character of each denition word, respecting the order of words. If the rst letter of the candidate denition word can not be aligned with the acronym corresponding character, the following characters (of the word) are taken into account. For instance, this method allows to nd that \"Extraction It \u00b4erative de la Terminologie\" is a possible denition of the French acronym EXIT. 4. Evaluation This paper focuses on the study of a corpus of 2000 paper abstracts provided by Cirad2: French research centre work- ing with developing countries to tackle international agri- cultural and development issues. Table 1 shows that better results are given with the second local pattern. But a lot of cases are partially relevant (i.e. 40%), so we have to improve and enrich this pattern approach. Patterns Local pattern 1 Local pattern 2 Number of extracted 78 64 denitions Case 1 31 28 (relevant) (39.7%) (43.7%) Case 2 3 6 (partially relevant) (3.8%) (9.3%) Case 3 1 18 (partially relevant) (1.3%) (28.1%) Case 4 43 12 (irrelevant) (55.1%) (18.7%) Table 1: Evaluation of extracted denition with patterns. The evaluation of the acronym/expansion extraction method is conducted on a corpus (general domain) having a reasonable size (7465 words). The experiments based on standard evaluation measures of data-mining domain highlight acceptable results (i.e. Precision: 66.7%, Recall: 2http://www.cirad.fr/en/home-page70Figure 1: Recognition of the couple NDVI / Normalized Difference Vegetation Index in A GRITROP database. Examples of extracted with Local pattern 1 NRPS Semi-aride CLF Corynespora Leaf Fall BASIC Br\u00b4esil, Afrique du Sud, Inde, Chine Examples of extracted with Local pattern 2 CIAT Centro international de agricultura tropical BSV Banana streak virus cassava mosaic virus TYLCV Tomato yellow virus Table 2: Examples of acronyms/denitions. 80%, F-measure: 72.7%) (Roche and Prince, 2008). We plan to apply the second step of the process (see Section 3.2.) with the pattern approach described in Section 3.1. on the Cirad corpus. Note that our previous work (Roche and Prince, 2008) uses more global patterns ; then a lot of noise is re- turned. The pattern approach described in this paper is more specic with better results in term of precision (40% in this current work vs. 15% in our previous work). 5. Discussion: Towards a Web-mining approach In this section, we propose to integrate Web-mining measures in order to automatically validate results returned by our approach (Turney, 2001; Mela et al., 2012).For instance, we can query a search engine with the acronym \"BSV\" and its possible denition to check on the Web if this association exists. This query should be a dis- junction (i.e. OR operator) of the acronym and its possi- ble denition returned with our process (i.e. Banana streak virus). This one returns a larger amount of documents. The conjunction of the acronym and the expansion (i.e. AND operator) enables to return a lower number of documents. But the returned documents are more relevant (i.e. the pre- cision is improved). In our case, we choose to consider the \"hits\" given by Google3on the examples of Table 2 (i.e. number of pages returned by the search engine based on conjunction). For instance, we have tested the query \"BSV\" AND \"Banana streak virus\" that returns 7580 pages4. All the results (i.e. hits) are given in Table 3. This table shows that hits have generally very high values, this allows us to automatically validate acronym/denition couples. Note that hits of irrelevant couples return lower values (for in- stance, with the couples \"ETM\"/\"environ 5.000 m3.ha-1\", \"SIPSA\"/\"indicateurs, documents, cartes\", and so on). Moreover, we can integrate this kind of information in clas- sical similarity measures, e.g. Dice measure (Smadja et al., 1996). Dice measure can be used to compute a sort of re- lationship between an acronym (i.e. acro ) and a denition (i.e.def). In our context, Dice measure (formula (1)) is based on the number of Web pages given by search engines (i.e. hits). (Google) cassava TYLCV Tomato yellow leaf curl virus 354000 Table 3: Examples of acronym/denition and hits scores. This measure returns the following result with the previous example: Web Dice (BSV, Banana streak virus ) =2\u00d7hits(\"BSV \"AND \") =2\u00d77580 2840000+15400 = 0.0053 Web Dice can be applied in order to rank couples (see Table 4). This enables to detect relevant acronym/denition pairs (i.e. couples with high Web Dice values). Acronym Possible denition WebDice ATPSM Agricultural Trade Policy Simulation Model MAE Mesures agrienvironnementales 0 Table 4: Acronym/denition couples ranked with Web Dice. 6. Conclusion The process described in this paper is based on the use of specic linguistic markers to detect acronyms. In future work we plan to integrate statistical information and Web- mining approaches in order to improve our methods based on linguistic rules. Our text-mining system allows us to enrich specialized thesaurus (e.g. MeSH5, Agrovoc6). These thesaurus are useful to automatically annotate texts. 5http://www.nlm.nih.gov/mesh/ 6http://aims.fao.org/standards/agrovoc/aboutMoreover we plan to investigate a contrastive analysis of English/French corpora in order to give a new point of view of the phenomenon of spontaneous descriptions. A rst study on aligned English/French texts reveals frequent regularities of glosses in a multilingual context. The alignment enables to improve the multilingual lexical acquisition of new words and their translations. 7. Acknowledgements This work was supported in part by the French National Research Agency under JCJC program, grant ANR-12- JS02-01001, as well as by University Montpellier 2 and CNRS. 8. References Chang, J., Schtze, H., and Altman, R. (2002). Creating an online dictionary of abbreviations from medline. Jour- nal of the American Medical , Frantzi, K., Ananiadou, S., matic recognition of multi-word terms: the C-value/NC- value method. International Journal on Digital Li- braries , 3(2):115-130. Larkey, L. S., Ogilvie, P., Price, M. A., and Tamilio, B. (2000). Acrophile: An automated acronym extractor and server. In Proceedings of the Fifth ACM International Conference on Digital Libraries , pages 205-214. Mela, A., Roche, M., and el Amine Bekhtaoui, M. (2012). Lexical knowledge acquisition using spontaneous de- scriptions in texts. In Proceedings of Natural Lan- guage Processing and Information Systems Conference (NLDB) , pages Nenadic, G., Spasic, I., and Biomedical Literature. Bioinformatics , 19(8):938-943. Okazaki, N. and Ananiadou, S. (2006). Building an ab- breviation dictionary using a term recognition approach. Bioinformatics , 22(24):3089-3095. Roche, M. and Prince, V . (2008). Managing the acronym/expansion identication process for text- mining applications. Int. J. Software and Informatics , 2(2):163-179. Smadja, F., McKeown, K. R., (1996). Translating collocations for bilingual statistical approach. Computational Linguistics , 22(1):1-38. Torii, M., Hu, Z., Song, M., Wu, C., and Liu, H. (2007). A comparison study on algorithms of detecting long forms for short forms in biomedical text. BMC Bioinformatics . Turney, P. (2001). Mining the Web for synonyms: PMI-IR versus LSA on TOEFL. Proceedings of ECML, Lecture Notes in Computer Science , pages 491-502. Xu, J. and Huang, Y . (2007). Using SVM to extract acronyms from text. Soft Comput. , 11(4):369-373. Yeates, S. (1999). Automatic extraction of acronyms from text. In New Zealand Computer Science Research Stu- dents' Conference , pages 117-124.72Lessons Learned from Manual Evalua tion of Named Entity Recognition by Domain Hanmin Jung1, Inga Hannemann2, Mun Technology Information, Daejeon, South Korea 2University of Hildesheim, Hildesheim, Germany 3Korea Advanced Institution of jhm} @kisti.re.kr , inga@outlook.de , munyi@kaist.ac.kr Abstract Recently, NER (Named Entity Recognition) has been adopted in many practical areas. People with smartphones may prefer services that manage their schedules automatically through scheduling applications that contain NER engines for extracting events from messages and emails. Diversifying the application of NER technology to various fields requires the accuracy of the technology . However , there is still a significant difference between NER result s in laboratories and in real fields. For example, t he F-score of our NER system is 0.75 in the laboratory and 0.22 in practice. In order to overcome this issue , NER evaluation should be performed manual ly such that developers and researchers can define the problems that can occur in practical environment s with their current NER engines ; this facilitates improv ements in future versions . This paper addresses the extraction result s of NER engine s. We approach the problem by hiring domain experts to evaluate the extraction result s. Certain problems that are not expect ed to be extracted by machines are presented ; moreover, feedback from the problems is provided in order to improve the NER engine . Keywords: NER, NER Evaluation, Domain Expert, F-measure, Manual evaluation 1. Introduction With the rapid changes in today 's world , it is important to stay current and up -to-date. Large amounts of information that can help with this task may be found on the Internet. However, people often lack sufficient time to read and understand such information. For convenience, m ost individuals prefer using machines to accomplish this task. The system discussed in this paper helps with such a task by extracting named entities from large texts. The extraction result s are fairly acceptable for res earch purpose s. For example, we obtained an F -score of 0.75 in laboratory tests . However, t he goal of our system is to successfully apply the named entities to an intelligent service that we have developed. This means that the extraction result s need to be acceptable for practical environment s as well . In order to estimate such result s in terms of the application , the results must be evaluated manual ly. Through manual evaluation, we can determine the real extraction result s of the system . The F-score we obtained in practical environments is merely 0.22. As can be seen, there is a significant difference between the two F-scores. This means there are certain mistakes in the system that can be discovered only through manual evaluation . When the problems i n the system have been identified, strategies to resolve such problems can be developed. This paper discusses the topic of domain experts evaluating extracted named entities. We demonstrate a few problems that are not expect ed to be extracted through machines and discuss feedback from the problems . Such feedback can contribut e to improve the engine. 2. Related Works A significant number of papers ha ve been published on various approaches to named entity extraction. The evaluation of NER (Name Entity Recog nition) systems is an important step for the improve ment of such systems. Many approaches have been proposed to rank systems based on their annotating capability. MUC (Message Understanding Conference) events cover the correct type and text. The final MUC score is presented as the micro -averaged F-measure . It is a sort of harmonic mean of precision and recall calculated over all entity slots (Nadeau and Sekine, 2007). ACE (Automatic Content Extraction) evaluation is more complex than the F-measure. ACE consider s methods for addressing various evaluation factors ( such as partial match, and wrong type). The final score named EDR (Entity Detection and Recognition V alue) is 100% minus the error rate (penalties). Another approach that evaluates NER is Rizzo and Troncy's NERD (Named Entity Recognition and Disambiguation) . NERD is an application that human evaluators can use to evaluate certain named entity extractors on the web (Rizzo and Troncy, 2011) . A framework is also suggested to evaluate NER s ystems that do not participate in large evaluation conferences for different reasons , but still meet certain demands to qualify as NER systems (Marrero et al., 2009). Marrero et al. compare several systems based on function s, results , and other factors. Another evaluation for NER result s was performed by Santos et al. in 2006 . Santos et al. propose HAREM (HAREM HAREM -Evaluation of NER Systems), which is a contest for Portuguese NER. HAREM consists of three tasks : identification, semantic classification, and morphological classification (Santos et al., 2006) . Typically, p recision , recall , and F-measure are used as evaluation measures , and they obtain similar values for most systems with only a few exceptions. Such measures 73also compare the number of entities a system can recognize. In this paper, we adopt precision, recall, and F-measure for the evaluation of our NER system because these measure s have been used widely . 3. NER System and Its Data Our NER system executes in a distributed and parallel environment and uses dictionaries, rules , and machine learning for data extraction . This combination causes our system to execute much faster than the existing system s (Shin et al., 2014) . Our system takes advantage of the machine learning method, especially the structur ed SVM (Support V ector Machine) that uses a variant of the Pegasos algorithm to recognize the required named entities . The Pegasos a lgorithm is faster and more accurate than the standard SVM training algorithms for structured SVM (Lee and Jang, 2009). There are 7 subtypes of named entities . Institution , University , and Corporation are different types of organizations that have been established to obtain specific roles in a society to achieve certain goals, to educate , or to produce and sell products or technologies; Nation describe s the locations where the organizations operate; Technology in this context describes methods of tool s, material s, and machine development or production processes and required necessary products; Person is the subtype for the persons who work for the organizations and conduct research , or who are otherwise related to the technologies or products; Product indicates the article, such as a model or series, that is produced in corporations using technologies (Shin et al. , 2013 ). The resources that should be addressed for extraction are web articles, papers, and patents. The system extracts named entities from the titles and abstract s of papers and patents. For web articles, only the body is used for extraction . The number of documents is 4 millions for papers, 7 mill ions for patents, and 5 millions for web artic les respectively. 16 million documents are totally processed in extraction. Table 1: Automated Evaluation Result To build the structural SVM model, w e made the tagged corpus which has 25,297 sentences with 687,632 named entities and their subtype . Training and evaluation are performed at a time. We gave -c 1000 as a parameter for the cost of the structur ed SVM and set 200 iterations with the Pegasos algorithm in training. We used the ten -fold validation for cross -validation. The cross -fold validation repeats the entire process multiple times with different random samples and decides on a fixed number of folds. Each fold in turn is used for testing, and the remainder for training . Ten error estimates are averaged. The precision, recall, and F -score are 0.79, 0.70, and 0.7 5, respectively (Table 1) . F1 = 2PR / (P + R) = 2 * 0. 79 * 0.71 / (0.79 + 0.71) = 0. 75 4. Manual Evaluation A total of 10,000 sentences were randomly selected from entire sentences for evaluation . The evaluation was conducted by two domain experts. Each expert was provided with these sentences with instances of named entities that the system had extracted . Based on their availability , one expert evaluate d 6,000 sentences for seven weeks for a total of 16 hours per week . The other expert reviewed 4,000 sentences for seven days for a total of approximately 57 hours. The actual amount per hour varied strongly depend ing on the complexity of the sentences. A total of 690 sentences were excluded because , for various reasons, it could not be determined whether the sentences were correct. We used 9,3 10 sentences for analysis . The number of sentences was 989 from titles, 2,614 from abstract s, and 5,707 from text bodies. 4.1 Process of Evaluation The data that was used for evaluation was given in EXCEL files. The structure of the data that was evaluated contains three units : the s entence where the named entity was found, the named entity , and the subtype of the named entity. If more than one named entity was found in a sentence, it was listed again for each new entity. Each sentence was manually evaluate d. Subsequently , it was determined whether a named entity was extracted correctly or incorrectly. A named entity was deemed to be correct if both the borders of the named entity and the subtype were correct. Finally , all the named entities that should have been extracted but were not extracted , which are false negative, were counted for recall . This process is illustrated in Figure 1 . 4.2 Evaluation Measure Micro-average precision and recall were used as the evaluation measure . In such a measure, the true positives are all the entities that were extracted correctly and the false positives are all the entities that were extracted incorrectly. False negatives are all the entities that should have be en extracted but were not , thus constituting the amount of all recall errors. Additionally , it is important to add that a strict measure was used; extremely small mistakes lead to an extraction being classified as an error , even when such mistakes would no t necessarily compromise the effectiveness of the system. Sub-Type Precision Recall Collect Total Score Collec t Total Score 74 Figure 1 : Process of Manual Evaluation 4.3 Consideration s Although manual evaluation is more thorough and accurate , certain problems became apparent during the process. Evaluators found it difficult to read the data, which was parsed to enhance its accessib ility for the system . Moreover , information that was vital for the understanding of a sentence and for differentiating between possible options was omitted occasionally . In some cases , such an omission poses a problem ; certain sentences appear distorted when devoid of context , because the context might be required to understand the meaning of specific words in the extracted sente nce, such as abbreviations. This limitation is somewhat unavoidable because of random selection. The data for manual evaluation is selected not from documents, but from parsed sentences for equality in number by year and resources. An issue that is linked to content more than format is the domain of the sentences. Such sentences are extract ed from texts in the technology and natural science domain s; therefore , these sentences can sometimes be extremely complicated to understand . In particular, sentences related to natural science ofte n cannot be understood without a significant knowledge of the specific topic. Examples of such sentences are: 'synthesis of Azaheterocycles from Bimetallic Relay Catalysts '. Nevertheless, in some cases it is possible to determine without context that an entity has been incorrectly extracted because common sense dictates that a certain subtype cannot be correct. In cases where the lack of knowledge about a subject makes it unclear whether an entity has been correctly extracted, it is advisable to search for addition al information on the Internet. We eliminated 690 such incomprehensible sentences from evaluation. 5. Manual Evaluation Results 5.1 Overview The outcome of the evaluation shows that 5,695 named entities from a total of 9,310 were extracted incorrectly , and that 3,615 were extracted correctly (Figure 2) . Figure 2: Accuracy of Named Entity Extraction Figure 3 and Table 2 show the distribution of the number of recall errors in terms of correctness and incorrectness. The occurrence of recall errors in sentences with correctly extracted named entities peaks at three recall errors. The amount of correctly extracted named entities in sentences with no recall errors is considerably small b ecause most sentences contain more than one relevant named entity. Figure 3: Distribution of the Number of Recall Errors Number of Recall Errors Number of Sentences among Incorrect Extractions Number of Sentences among Correct Extractions 0 1,205 186 1 687 560 2 826 769 3 1,022 832 4 704 524 5 509 351 6 285 148 7 159 97 8 90 41 9 57 32 10 34 17 11 - 20 105 51 > 20 12 9 Total 5,695 3,615 Table 2: Number of Correctness and Incorrectness by Number of R ecall Errors 75In certain sentences , the number of reca ll errors is exceptionally high : 11 to 20 recall errors , or over 20 recall errors. These cases are sentences that list named entities, for example: 'The bus state controller include s control register s such as wait controller s, and control s the interface of various semicon ductor memory (ROM), burst ROM, SRAM, PSRAM , DRAM , and synchronous RAM , and PC cards (memory and I/O cards in parallel )' or 'System builder and reseller offering Tesla -based '. Considering th e F-measure, t he precision value is 0.38 and the recall value is 0.15 . F-score is calculated as the equation below shows . F1 = 2PR / (P + R) = 2 * 0.38 * 0.15 / (0.38 + 0.15) = 0.22 5.2 Error Analysis When performing the evaluation , different types of errors were observed. The error categories listed in this section either contain information about the subtype or the extracted entity , or a combination thereof. It can be expected that more than the categories listed here can be found . (1) The entity does not belong to any of the subtypes and the subtype does not appear in the sentence . In this category , both the named entity and the subtype are wrong and there is no visible connection between the sentence and the subtype , and between the extra cted entity and the named entities. <Example > Sentence : the following virus which be list in order of the overall abundance wit hin the tested sample be detect : Tobacco streak virus (TSV) , Tomato spotted wilt virus (TSWV), Tobacco etch vir us (TEV) , Tobacco ring spot virus (TRSV), Potato virus Y (PVY) , Cucumber mosaic virus (CMV) and Tobacco mosaic virus (TMV). NE extracted by the system : Tomato Subtype : $Person (2) Wrong part of sentence was extracted. In this case , the extracted entity is not a named entity or part of a named entity , but the subtype is relevant for another named entity in the sentence that has not been extracted. <Example> Sentence : SURFACE COA TED CUTTING TOOL MADE of CEME NT HA VING PROPERTY -MODIFIE D ALPHA TYPE ai 203 layer of HARD COA TING LAYER . NE extracted by the system : HA VING Subtype : $Technology (3) The entity has the wrong subtype. This can be the only error , or it can be combined with the errors explained in (4) and (6). If it is the only error, the named entity has been correctly found, but it has been matched with the wrong subtype. <Example> Sentence : spin around the first offering from IBM since its personal computing division be acquire by China's Lenovo be the ThinkPad X41 Tablet. NE extracted by the system : Lenovo Subtype : $Nation (4) Only parts of the entity have been extracted . There are parts that belong to the named entity , but the parts have not been extracted. <Example> Sentence : subscribe to anti -virus soft ware, such as Norton AntiVirus , McAfee VirusScan or Zone Alarm Security Suite . NE extracted by the system : ZoneAlarm Security Subtype : $Product (5) Wrong subtype and only parts of the entity have been extracted . This error is a combinati on of the errors explained in ( 3) and ( 4). <Example> Sentence : SYSTEM and METHOD for ESTABLISHING PEER TO PEER connection BETWEEN PCS and SMART PHO NES USING network with obstacle . NE extracted by the system : PEER Subtype : $Institution (6) More than what belongs to the entity has been extracted. Parts that do not belong to the entity have been extracted, most often articles or conjunctions. <Example> Sentence : Symantec Corp. form ed the same yea r Skrenta unleashed 'Elk Cloner ', but it dabble d in non -security software before releas ing an anti-virus product for Apple's Macintosh in 1989 . NE extracted by the system : Symantec Corp. form ed Subtype : $Corporation Number of occurrences Percentage (1) Wrong entity and subtype 85 42.5% (2) Wrong part of sentence 11 5.5% (3) Wrong subtype 36 18.0% (4) Entity only partially extracted 25 12.5% (5) Wrong subtype and entity only partially extracted 34 17.0% (6) More than is part of the entity was extracted 9 4.5% Table 2: Rate of E ach Type of Errors 76To compare the importance of each type of error, the rate of each type of error is calculated from a subset of 200 sentences that contain incorrect extractions (Table 2). 6. Lessons Learned When evaluating the extractions, some observations can be made. There are certain specific errors that occur in the same patterns throughout the extracted results. Some of these patterns are domain -specific and some are general phenomena. Furthermore , there are some errors that the NER system generates independently . First, the system has narrowly missed the correct entity. The system extracts a word or phrase that can be, but is not necessarily, a named entity itself and assigns a wrong subtype. This is different from regular subtype errors, because the correctly named entity for the subtype is immediately adjacent to what wa s extracted. However, subtype error s in general are a frequent problem . The number of title -type sentence s is 989. The precision errors are 906 out of 989. The rate is almost 92%. This problem originates from a mistake in the structural analysis of sentences, especially in the titles of papers and patents. The structure of article titles is different from normal sentences. Titles can be regarded as phrases that do not follow the form of sentences: titles typically do not have verbs. To resolve this problem, the system needs to be equipped with a structural analysis module that can address this type of phrases and accept them as sentences. Second, errors are related to the nature of the natural science domain. In the natural science domain, many procedures, scales, instruments, etc. are given the name of the person who invented or discovered them. This leads to the fact that there are many names in the sentences from the natural science domain that do not refer to a specific person. In most c ases, the s entences refer ring to names belong ing to specific people include titles such as Mr, Mrs, or Dr before the name, or the names are followed by the word say or a synonym ; names that do not refer to specific people usually are surrounded by adjectives and nouns , and sometimes have an article in front of the name . In reality , names are extremely important in the natural science domain because they emerge in texts with the inventions such that the names provide information on who invented the t echnology. However, such names are not full name s and are not identified with other similar names. The extraction can be compared to the head and tail of a coin. Therefore, researchers of NER systems need to decide , before extracting , whether a person's na me linked with procedures, scales, and instruments from the natural science domain should be extracted . Third, sentences containing many sequential ly named entities produce many recall errors. These types of sentences are uncommon, though they produce a significant effect on recall errors , as mentioned in Section 5. There were more than 20 named entities in a sentence that should have been extracted . For laboratory evaluation, this means merely one recall error. However, many useful named entities can be missed in practice. To solve this problem, we need to approach with a micro -solution. A NER system that uses machine learning methods is incapable of extract ing these types of named entities unless training data do not include sentences or paragraphs that have these types of named entities. Therefore, we should consider various types of sentences while developing training data. The t ype of sentence is different among sources such as social networking services , journal ar ticles, web article s, patent s, email s, and essay s. The sentences used for training should be selected based on the type of target sentences that are to be extract ed. If the target sentences are from academic papers, the training sentences must include similar types of academic papers . Four th, certain named e ntities were only partially extracted. The cases of (4) and (5) in Section 5.2 match this problem. The error rate is almost 20% , which is significant . In fact, this is not a precision error , but a recall error because the extracted named entity is correct and there is another named entity that should have been extracted. These two named entities overlap and share common tokens in the sentence . The example shown in Section 5.2 explains that the system is correct because ZoneAlarm Security is surely a named entity that we want to extract. The only mistake the system incurred is that it did not extract the other named entity in the senten ce, ZoneAlarm Security Suite. In order to solve th is problem, we need to tag both named entities in the training data. 7. Conclusion The aim of the evaluation discussed in this paper is to determine the ability of the proposed system to manage the extraction of named entit ies, and to determine possible improvement s to the system . Some concerns were found in relation to domain , in addition to other general concerns . First, the proposed system missed some correct entities that were immediately adjacen t to the incorrect extraction; this problem can be solved by developing more capable structural analys es that can manage phrases. Second, a specific guide line for NER needs to be established before extraction because named entities can be included in jargons in a natural science domain . Third, the training sentences should be selected based on the type of target sentences in order to extract many sequential ly named entities within a sentence . Fourth, both named entities should be tagged in the training data to ensure that the problem of partial extraction of some named entities is addressed . 8. References Nadeau, D. and Sekine, S. (2007). A Survey of Named Entity Recognition and Classification. Linguisticae Investigations . 30. pp. 3 --26. 77Rizzo, G . and Troncy, R. (2011). NERD: Evaluating Named Entity Recognition Tools in the Web of Dat. In Proceedings of the Workshop on Web Scale Knowledge Extraction . Morrero, M., S\u00e1nchez -Cuadrado, S., Lara J. M., and Andreadakis, G . (2009) . Evaluation of N amed Entity Extraction System. Research in Computing Science , 41. pp. 47 --58. Santos, D., Seco, N., Cardoso, N., and Vilela, R. (2006). HAREM: An Advanced NER Evaluation Contest for Portuguese. In Proceedings of Language Resources and Evaluation Conference. Shin, S ., Um, J ., Choi, S . P., Jung, H ., Yi, M . Y., and Lee, S. (2013) . Platform to build the Knowledge Base by Combining Sensor Data and Context Data. International Journal of Distributed Sensor Networks (Online published ). Lee, C. and Jang, M . (2009). Large -Margine H., Seo, D ., Choi, S . P., Jung, H . (2013) . Improvement of the Performance in Rule -Based Knowledge Extraction by Modifying Rules. In Proceedings of t he 2nd International Workshop on Semantic Web -based Computer Intelligence with Big-data. 78Annotating Discourse Zones in Medical Interviews Milan Toloski, Evan Zhang, Fred Popowich Simon Fraser University 8888 University Drive Burnaby, BC, Canada V5A 1S6 mta45@sfu.ca, evanz@sfu.ca, popowich@sfu.ca Abstract We improve a visual analytics workflow for analyzing medical interviews by introducing a discourse annotation scheme for creating an effective multi-document visualization that also facilitates inter-document comparisons. We introduce the concept of discourse zones for bringing together the many disparate terms and concepts used in various research areas. The zones are generalized for usage toward any institutional dyad setting (attorney-witness, teacher-student, physician-patient, etc.), including emergency hotlines and switchboards. Our task involves visually identifying the medical problems, their solutions, and contexts in medical encounters (i.e., dialogue-based conversations and interviews). The corpora consists of medical interviews between clinicians and caregivers of children with Fetal Alcohol Spectrum Disorder (FASD). Keywords: medical terminology, discourse, interviews 1. Introduction Medical practitioners and researchers examine document collections to understand patient behaviors, the situations surrounding the behavior, and strategies associated with patient care. However, the creation of interview scripts and the manual processing and analysis of interview con- tent to identify situations, behaviors and strategies can be time consuming. Using a collection of semi-structured in- terviews between researchers and caregivers concerning the care and challenging behaviors of children with Fetal Alco- hol Spectrum Disorder (FASD), we provide an analysis that detects patterns across the study population which also can be used for identifying new relationships. Our system as- sists analysis by automatically labelling the situations, be- haviors, and strategies experienced by caregivers which is then used to provide a high-level, structural visualization of the document collection. Analyzing the text of a conversational dialogue can be cog- nitively demanding due to the difculties in viewing the en- tire conversation all at once (Tat and Carpendale, 2002). One of the objectives involves providing a view of an en- tire conversation in order to reduce cognitive load. A more formidable objective enables the viewing of multiple con- versations simultaneously in order to discover patterns or hidden associations, specically for identifying effective and ineffective caregiver strategies. The goal is to segment the clinical conversations for im- proving an analyst's workflow in identifying patient behav- iors as well as possible strategies associated with patient care. Comparisons between clinical interviews are also im- portant in identifying patterns. 2. Annotation of Medical Interviews The Beginning-Middle-End sequence is a familiar narra- tive structure and can be found in lms, sports, literature, etc. Clinical interviews with caregivers can similarly be segmented according to whether the various interview sec- tions are concerned with discussing Situations, Behaviors,or Strategies used to cope and manage. For an analyst looking into clinical interviews to understand the handling of various scenarios while also being able to learn from said situations, the interview can be structurally characterized into sections such as Problem ,Cause , orSo- lution , with a fourth label None that captures all other con- tent occurring in a medical encounter not of interest to the analyst. The three zones we choose as the foundation for annotating the medical interviews are: theBehavior of the child theSituation or conditions which led to or caused said behavior and the Strategy employed by the caregiver to help the child (i.e., resolving the issue), as well as methods used in coping and managing the situation This structure and its identication forms the basis of the visualization system. A sequence could begin with a Sit- uation that then leads to a Behavior, which then results in a Strategy being employed by the caregiver to remedy said Behavior (assuming difcult behavior was being ex- pressed). Thus, we annotate a medical interview into struc- tural sections for creating a model of the sequences that occur, as well as using such annotations to improve a vi- sual analytics workflow where many interviews are manu- ally pored over. The general template of the interviews between the clini- cian (DR) and caregiver/patient (P) were as (. there any other scenarios? (. . . ) After the discourse zones are applied to the interview, one of three colors is assigned to each zone, which provide a79high-level visual overview consisting of possibly multiple interviews to be compared and viewed simultaneously. The sections annotated with the None label have no color ap- plied. 3. Related Work Much study on discourse zones has been performed across various research areas. Unfortunately, each area has con- structed its own terminology in isolation, making it dif- cult to understand what differences exist (if any) between the various research domains. Some of the terms used to refer to discourse zones are: Phases of Action (Ten Have, 1989), Document Zoning (Varga et al., 2012), Rhetori- cal Zones (Mullen et al., 2005), Evidence Based Medicine such as PICO (Amini et al., 2012), Stages (of lm reviews) (Taboada, 2011), Information Structure (Jindal, 2014; Guo et al., 2010), Argumentative Zoning (Guo et al., 2011; Guo et al., 2012; Teufel et al., 2009), Section Classication (Li et al., 2010), and Conceptualization Zones (Liakata et al., 2012). With this paper's focus being the biomedical do- main (i.e., medical encounters), a comprehensive review of the discourse structure of other domains (e.g., scientic ar- ticles, lm reviews, literature) is outside the current scope, and a generalized discourse zone framework is left to future work. Mullen et al. (2005) perform an experiment to identify the document structure of biomedical texts by developing a supervised approach to classify sentences according to one of the rhetorical zones Introduction ,Method ,Result , andConclusion . For discourse visualization, Guo et al. (2012) create a tool for visually displaying argumentative zones in biomedical articles. Their tool is close to our work both in objective (visual analytics) as well as implementa- tion (use of color to distinguish between discourse zones). The objective of their visualization is to display the infor- mation structure (e.g., Background, the Research Problem, Method, Result, Conclusion, Connection, Difference , and Future-work ) of biomedical articles (i.e., text), whereas our task is to visually identify medical problems, their solu- tions, and contexts in medical encounters (i.e., dialogue- based conversations and interviews). Signicant attention has been placed on the discourse of medical encounters, primarily for the study of doctor- patient power relationships, and how such power manifests (Wilce, 2009; Ainsworth-Vaughn, 2003). The discourse zones, referred to as phases orphases of action in medi- cal discourse research (Byrne and Long, 1976; Ten Have, 1989), segment the dialogue of the medical encounter sim- ilar to the way scientic articles can be structured into a Introduction-Method-Result-Conclusion sequence. Various phase sequences (e.g., 1-2-3-5-3 can be found to be indica- tive of problematic encounters). One early study of medical discourse that has been influential was conducted by Byrne and Long (1976), who segment the medical encounter into six phases of actions: 1. relating to the patient 2. discovering the reason for attendance 3. conducting a verbal or physical examination or both4. consideration of the patients condition 5. detailing treatment or further investigation 6. terminating In terms of the Situation-Behavior-Strategy zones, Byrne and Long's typology can be reduced and mapped to our an- notation scheme by combining phase 1 with 2 (Behavior), phase 3 with 4 (Situation), and phase 5 with 6 (Strategy). 4. Corpora Our corpora consists of medical interviews between clin- icians and caregivers of children with FASD. Some key properties of the corpora to note are: spoken dialogue between two people \"questions & answers\" interview structure dialogue turns are brief fairly unvarying in format entire consultation is fairly brief These properties can be commonly found in medical en- counter discourse. In contrast to conversational discourse, medical encounters in general can be characterized by a more restricted turn-taking system resulting in less inter- ruptions (Ainsworth-Vaughn, 2003). This semi-structured framework allows medical encounters to be processed into discourse zones, unlike normal conversational discourse. The dataset consists of 60 interviews between healthcare workers and caregivers of children with FASD. 34 have been automatically transcribed from speech to text, with 10 of the transcribed documents annotated with our discourse zone scheme consisting of the three discourse zone labels (Situation, Behavior, or Strategy), as well as a fourth label ofNone that captures dialogue not considered of import to the analyst's objective. A speaker's entire turn in a conver- sation was the basic discourse unit. Thus, each turn in the conversation was annotated with one of the four labels. Since only one label is applied to each utterance, the issue of what label to assign in cases where one, two, or even all three of the zones are applicable to a single utterance arises. From our initial test annotations, we decided upon ranking the class labels (where zones are ranked as Behavior, Situ- ation, and Strategy) and assigning the higher ranking zone label as the true label, and leave multi-class evaluation for future work. A subset of the FASD interview document collection is currently in the process of being manually an- notated by two annotators as a pilot in order to further re- ne and tighten the annotation guidelines and handle any unforeseen issues. 5. Discussion We have introduced a general annotation scheme for dis- course zones in medical interviews that label each partic- ipant's turn in the interview as Situation ,Behavior , and Strategy . The generalized scheme is very broad for ap- plication in medical interviews and potentially useful for a80wide range of clinical and counselling contexts as dyad settings etc.). Multi-class evaluation (where a discourse utterance can possibly be labelled with more than one of the Situation, Behavior, and Strategy tags) and its analysis is left for fu- ture work. Also of interest is how well the annotation schema can be applied to other institutional dyads (attorney-witness, teacher-student, physician-patient, etc.). Further, the an- notation scheme will be applied on marital conflict data, where interviews of couples are also to be analyzed in terms of the Situations, Behaviors, and Strategies that arise be- tween them. Other corpora of interest which our annotation scheme can be applied toward are switchboard/911 phone calls. 6. References Ainsworth-Vaughn, N. (2003). 23 the discourse of medical encounters. The handbook of discourse analysis , 18:453. Amini, I., Martinez, D., and Molla, D. (2012). Overview of the ALTA 2012 Shared Task. Population , 7(5.6):7.9. Byrne, P. S. and Long, B. E. (1976). Doctors talking to patients: A study of the verbal behavior of general prac- titioners consulting in their surgeries. London: HSMO, Royal College General Practitioners . Guo, Y ., Korhonen, A., Liakata, M., Silins, I., Sun, L., and Stenius, U. (2010). Identifying the Information Struc- ture of Scientic Abstracts: An Investigation of Three Different Schemes. pages 1-9, June. Guo, Y ., Korhonen, A., and Poibeau, T. (2011). A weakly- supervised approach to argumentative zoning of scien- tic documents. pages 273-283. Guo, Y ., Silins, I., Reichart, R., and Korhonen, A. (2012). CRAB Reader: A Tool for Analysis and Visualization of Argumentative Zones in Scientic Literature. pages 183-190. Jindal, P. (2014). Information extraction for clinical narra- tives. Li, Y ., Lipsky Gorman, S., and Elhadad, N. (2010). Section classication in clinical notes using supervised hidden markov model. pages 744-750. Liakata, M., Saha, S., Dobnik, S., Batchelor, C., and Rebholz-Schuhmann, D. (2012). Automatic recognition of conceptualization zones in scientic articles and two life science applications. Bioinformatics , 28(7):991- 1000, March. Mullen, T., Mizuta, Y ., and Collier, N. (2005). A baseline feature set for learning rhetorical zones using full articles in the biomedical domain. ACM SIGKDD Explorations Newsletter , 7(1):52-58. Taboada, M. (2011). Stages in an online review genre. Text & Talk - An Interdisciplinary Journal of Language, Discourse & Communication Studies , 31(2):247-269, March. Tat, A. and Carpendale, M. S. T. (2002). Visualising hu- man dialog. In Information Visualisation, 2002. Pro- ceedings. Sixth International Conference on , pages 16- 21.Ten Have, P. (1989). The consultation as a genre. Text and talk as social practice , pages 115-135. Teufel, S., Siddharthan, A., Preotiuc-Pietro, D., and (2012). Unsupervised identication using proba- bilistic graphical models. pages 1610-1617. Wilce, J. M. (2009). Medical Discourse. Annual Review of Anthropology , 38(1):199-215, October.81Annotationof Pronouns in a Multilingual YuJieSeah andFrancis Bond LinguisticsandMultilingualStudies NanyangTechnologicalUniversity yjseah1@e.ntu.edu.sg,bond@ieee.org Abstract A qualitative and quantitative approach was used in this stu dy to examine the distribution of pronouns in three language s, that allows them to be easily linked across languages. A single text (The Adventure of the Speckled Band , a short story featuringSherlockHolmes) inthree languages Englishhas number of pronouns, Mandarin Chinese has the hig hest proportion of contentful pronouns in our corpus. Also, English has more translatedcounterparts in Mandarin Chinese as compar ed to Japanese. We attributed this tothe difference in usage of pronouns in the languages. Depronominalisation, surprisingly, was ev en for both corpora. Findings from this study can shed some li ght concerning translationissues onpronoun usage for learners of the lang uages and toimproving machine translation of pronouns. Keywords: pronoun, Chinese, English,Japanese 1. Introduction Pronouns exist in all the world languages, although there is considerable variation in how they are used. In this pa- per, we offer a componential analysis of pronouns that is extended into three language (English, Mandarin Chinese andJapanese)fromthreetotallydifferentlanguagefamili es (Indo-European,Sino-Tibetan and Japonic), The way they are employed in different languages is interesting to many linguists. Furthermore, in such a globalized world like to- day, languages are always translated into other languages. Other than translation of content words, how pronounsare translatedfromlanguagetolanguagecanallowonetolearn a lot about the languageand its translation. English, being the world's most globalized language, has been translated into many different languages. Comparing its translation to MandarinChinese and to Japanese can shed light on the usageofpronounsineachlanguage. Therehavebeenfewcorpusbasedstudiesondifferencesin pronoun use among languages. According to Kim (2009), thereexistqualitativeandquantitativedifferencesinth eus- age of the second person and rst person plural pronouns intextsheexaminedfromEnglishandKoreannewspapers. Ingeneral,English usespronounsmoreoften, with the no- table exception of the rst person plural, which was more common in Korean. Our research is part of a wider study of conceptual differences between the languages (Bond et al., 2013). For this reason, we did no restrict ourselves to personalpronouns,butalsoconsideredindenitepronouns , demonstrativesandinterrogativepronouns. 2. Approach We proceededinfoursteps: 3. Tagthepronounsmonolingualyineachlanguage 4. Analyzethedistributioncrosslingually2.1. Identifythe pronouns We started off by examining words tagged as pronouns in the NTU Multilingual Corpus (NTU-MC) (Tan and Bond, 2012). TheNTU-MCexploitsthelinguisticdiversityavail- able in Singapore for the collection of a vast variety of texts from different languages. The current version is an annotated collection of around 6,000 sentences ( 595,000 words) in 7 languages (Arabic, English, Mandarin Chi- nese, Japanese, Korean, Indonesian and Vietnamese) from 7 language families (Afro-Asiatic, Indo-European, Sino- Tibetan, Japonic, Korean (language isolate), Austronesia n and Austro-Asiatic). Two kinds of annotation are applied in the NTU-MC -monolingual annotation where texts are tagged for parts of speech (POS) and sense; and crosslin- gual annotation where texts are aligned across sentences (Bondet al.,2013;WangandBond,2014). Pronouns from the three languages (English, Mandarin Chinese and Japanese) were extracted from four data sets in the NTU-MC. They are two short stories from Sher- lockHolmes- TheAdventureoftheSpeckledBand andThe Adventure of the Dancing Men (Conan Doyle, 1892; Co- nan Doyle, 1905), an essay named The Cathedral and the Bazaar(Raymond, 1999) and on-line articles about Sin- gapore tourism (Singapore Tourist Board, 2012). In each set, English is the source language while Mandarin Chi- nese and Japanese translation texts are aligned to it at the sentencelevel. Thetextshavebeentokenizedandautomat- icallyPOS tagged. We tookas pronounsanythingmarkedas a pronounby personal pronouns, indenite pronouns and interrogative pronouns. Each language had slightly different part-of-speech tags, with slightly different coverage. We ended up with 60 different Englishpronouns,54Chineseand69Japanese. Thegreater number of Japanese thesamepronouncanbewritteninChinesechar- acters(kare\"he\")orusinghiragana( kare\"he\"). 1PN, WB, WRB, Pronouns was to analyze them componentially. The pronounswereseparatedintoeightcategories: Head,Num- ber, Gender, Case, Type, Formality, Politeness, and Dis- tance from Speaker. The features chosen are in line with other research and reference grammars (Backhouse, 1993; Li and Thompson, 1989; Huddleston, 1988). The purpose of this componential analysis is to code the pronouns so that we can compare and contrast them across languages. This also allows the auto-tagging programme to recognize andlinkthepronounsbytheircode. Thisstagetookaround twoweeksduetothedetailedcomponentialanalysisofev- ery pronounin the four subcorporaand analyzingambigu- ous particularly in Japanese. The different features undereachheadingareshownin Table1. 2.2.1. Head In the rst column - Head, there are altogether nine com- ponents. These are Entity, Time, Manner, Person, Place, Reason, Thing, Personal and Quantier. This feature re- stricts the kind of the referent, or says that the pronoun is a quantier and thus has no restriction. We show them in Figure1. Every pronoun extracted will be tagged with one of these features. For example, demonstrative pronouns such as thisandthatareThing(effectivelywiththesemantics\"this thing\" and \"that thing\") while Entity is used for pronouns that do not have a specic category of referent, as it can refer to both person and object. Such pronounsare all, lia3\"both\" andikutsu\"some\". when,how,why andwhereare examples of pronouns labeled under Time, Manner, Reason and Place respectively. For English pro- nouns,wordsthat endwith -thingare easily groupedunder Thing (something, underPerson. Personalpronounsarefurtherdividedinto1st,2ndand3rd person. 1st person is then divided into exclusive and in- clusive (used by Chinese and Indonesian, which make this distinction). Although strictly speaking not pronouns, determiners and adjectivesthatarecloselyrelatedtopronouns(suchas both andmany)were also analyzed and labeled as Quantiers. For example, we annotate bothin both (1) and (2) of the followingtwosentences. Theysharemanyoftheotherfea- tures, so it makes sense to analyze them together. We didnotattempt to coverall determiners,only those example of Dual, thosefor Plural and zhe4\"this\" for Singular. Many pronounsare notspeciedfornumber. 2.2.3. Gender For the third column - Gender, three features were identi- edaswell-Masculine,FeminineandNeuter. itinEnglish isa neuterpronounwhile : e.g.I, me, . Extending to other languages may require furtherdistinctions. 2.2.5. Type Type differentiates the pronouns by Assertive Existential (somebody ), Elective Existential ( anybody), Negative ( no- body), Reflexive ( myself), Uni- versal (everybody ), Interrogative ( who) or Other (anything else). In a decomposed semantics, we would treat all but Reflexive and Reciprocal as quantiers: anybodythus be- comestheequivalentofthequantier anyandthenoun per- son. 2.2.6. Formality column shows Formality, whether the pronouns areinformalorformal. ThisismainlyfortheJapanesepro- nouns, in Chinese isusedtoreferto highstatuspeople. Note that Formality and Politeness are somewhat differ- ent from the T-V distinctions made in European languages whichtypicallyonlymarksecondperson,andshowthere- lationbetweenspeakerandhearer(historicallyapowerdif - ference,nowmoreoftenadifferenceinfamiliaritybetween the speakers). Japanese pronouns absolute levelofrespectfortheirreferent. 2.2.8. Proximity have a two way distinction (proximal and distal: thisandthat). Japanese has a three way Gender Case Type Formality Politeness Proximit y Quantier Dual Feminine Objective Assertive Formal Polite Distal Entity Plural Masculine Possessive Elective Informal Medi al Time Singular Neuter Subjective Negative Proximal Manner Other Person Reciprocal Place Universal Reason Interrogative Thing Reflexive Personal (1e, 1i,2, 3) Table1: The8typesofpronounfeatures 2.2.9. Summary The features are used to dene a concept, which we treat as a wordnet synset (Fellbaum, 1998). A single synset may have - typesto n:1, Place is location n:1). The other compo- nents were kept as a separate table, linked using the word- net synset IDs. We endedup with 107 differentsynsets for the60English,54Chineseand69Japanesepronouns. 2.3. MonolingualTagging Afteranalyzingthepronounsbytheirdifferentcomponents we added them to our local wordnets' sense inventories (14 were already there, mainly interrogatives and inde- nite pronouns). For English we use the Princeton Word- net(Fellbaum,1998),forChinesetheChineseOpenWord- net (Wang and Bond, 2013) and for Japanese the Japanese Wordnet (Isahara et al., 2008). Treating the pronouns as synsets enabled us to use our existing wordnet tagging tools. We carried out tagging on a single subcorpus: The Adven- ture of the Speckled Band and its Chinese and Japanese translations. We chose it as it had more (reported) speech than the other genres, and was thus had a greater variety of pronouns. We show the numbers of pronouns found in eachlanguageinTable2. Thisincludesalltypes,including quantiers. Themainissueinthemonolingualtaggingwasdistinguish- ing what we shall call contentful pronouns (such as those described above) from purely structural pronouns such as dummyit, existential there,relativepronouns( thedogwho barked) and pronouns Oh God!). We expect contentful pronouns would introduce a quantier into a formal semantic representation, while the structuraloneswouldnot. Inaddition,there were sometokenizationerrors,mainlyin the Chinese and Japanese corpora. These we xed as we carriedouttheannotation. 2.4. Cross-lingualtagging In the initial annotation, each pronoun was linked to the pronounin the correspondingtranslation with the best fea- turematch. Iftherewasatie,theleftmostpronounpairwas linked rst, then the next and so on. The annotator then went through the bilingual corpus and checked each pair. At this stage they checked both whether they are taggedLanguage English Chinese Japanese Contentful 1,370 1,177 463 Other 75 51 the auto-tagging programme and whethertheconceptlinksbetweenthesourcelanguageand target language are accurate. This was done several times to ensure accuracy. This stage took around four weeks to complete both English-Chinese and English-Japanese cor- pora, with a longer time needed for the English-Chinese one due to the greater number of pronouns present there. Onaverage,threetofoursentencescanbedoneeveryhour. An example of matching pronouns is given in (3) where the English is followed by Chinese. The rst two English pronounsmatchtheChinese,thethirdhasnoequivalent. (3) number of contentful and non- contentful (structural or segmentation errors) are shown i n Table2. Differencesinwordandsentencetokenizationgive differentnumbersofwordsandsentencesforthethreelan- guages,eventhoughthecontentisbasicallythesame. Even allowing for these light differences, English has more pro- nounsthanChinesewhichhasfarmorethanJapanese. The non-contentfulpronounsare mainly structural for English , while they are mainly tokenization errors for Chinese and Japanese. The results for the linkage of the pronouns are separated intotwopartsforbetterunderstanding\u2014therstpartbeing theresultsfortheEnglish-Chinesecorpus(Table3)andthe secondpartfortheresultsfoundfromtheEnglish-Japanese corpus(Table4).84LinkedPronouns Non-linkedPronouns # MatchingFeatures Pronoun English Chinese 5 6 7 8 9 toNoun # Pronouns 5 19 54 789 58 134 369 215 Table3: English-Chinesepronountranslation LinkedPronouns Non-linkedPronouns # MatchingFeatures Pronoun English Japanese 5 6 7 8 9 to Noun #Pronouns 15 120 114 37 32 139 943 109 Table4: English-Japanesepronountranslation There are in total 925 English to Chinese pronouns linked toeachother,with0.5%ofthemhavingonly5pronounfea- tures match, 2.1% having 6 pronoun features match, 5.8% having 7 pronounfeatures match, 85.3% having 8 features match and 6.3% having 9 pronounfeatures match where 9 is the maximum match. Most pronouns match everything except Case. Those that matched exactly were mainly in- denitepronouns,whichdon'tshowcase. There are also 134 pronouns that are linked to non- pronouns. 76 of them are English pronouns while 58 of themareChinesepronouns. Thesetypicallylinkedtocom- monnouns. Out of the 1,370 contentful English pronouns, 26.9% of them are not linked. For the Chinese contentful pronouns, only18.2%werenotlinkedtoanything. For English and Japanese, far fewer pronounswere linked. Thereareintotal318linkedEnglishtoJapanesepronouns. Out of these, 4.7% have 5 matched features, 37.7% have 6 matched features, 35.8% have 7 matched features, 11.6% have 8 matched features and 10% have 9 matched fea- tures. The majority of the linked English-Japanese pro- nouns, unlike the English-Chinese corpus, have around 6 to 7 matched features. This is because they typically mis- matchonbothCase inEnglishandPolitenessorProximity inJapanese. Similar to the English-Chinese corpus, there are 139 pro- nouns in the English-Japanese corpus that are linked to non-pronouns. 109 of the pronouns are English pronouns and the other 30 are Japanese pronouns. In contrast to the English-Chinese corpus, most (68.8%) of the English con- includenon-contentfulpro- as dummy it, existential thereand also com- plementizerslike thatandwhich), this becomeseven more pronounced. Also, in English, many pronouns can also double up as determiners (Collins COBUILD, 2005). De- terminers share many common words with pronouns such asthis,thatand indenite ones such as allandsome. In contentful pronouns, andJapanesetendstodroppronounsaltogether.Englishpersonalpronounshavemoredifferentforms: Sub- jective, Accusative and Possessive. English also has other categories of pronouns that both Mandarin Chinese and Japanesedonothave. Forexample,forthecomponentNeg- ative,Englishhas noneandnothingwhichdonothaveiden- tical correspondents in Mandarin Chinese and Japanese: which do not negate inside noun phrases. This is because both languages tend to use verbs to express negativity in- stead of marking it in the pronoun like in (4) where the EnglishisfollowedbyChineseandJapanese. (4) a. jiken dearu 'Thereisnotanyunusualcase.' In addition, Mandarin Chinese and Japanese are topic- prominent languages (Li and Thompson, 1989; Obana, 2000). Once the topic is established, sentences following it omit any pronouns, as there is no need for them to refer back as the readers can infer from contextual knowledge thesubjectofthesentence. Furthermore, out of the three languages, only Japanese markspolitenessandsomeevidentialityontheverb(Back- house, 1993), making the use of pronouns rather unneces- sary and this seems to play an important role in reducing the numbersof pronounsfound in the corpus as compared to the English source text and Chinese translation text, re- sulting in the low rate of links to the English pronouns in the original text. One example can be seen below in (5), withEnglishandJapanese: (5) a. I haveheardofyou,Mr. Holmes b. Anata English- Japanesecorpus,anothermajordifferenceisthenumberof85correspondingfeaturesthatmajorityofthelinkedpronoun s have. For the English-Chinese corpus, majority of the linked pronouns have 8 matching pronoun features while fortheEnglish-Japanesecorpus,majorityofthelinkedpro - nounshave around6 to 7 matching pronounfeatures. This is most likely due to Japanese language having different speech levels (Obana, 2000). The different speech lev- els cause a differentiation between the pronouns, result- ing in Japanese having a few different words for the same pronoun. For example, for the rst person pronoun, in Japanesetherearevariationssuchas washiwhichalso marks for masculine speaker and informal and watashi which marks for formal and politeness. These features do not exist in English but from the perspective of semantics, they should be linked to the rst person pronouns in En- glish. Thisproblemdoesnotexist inMandarinChinese,as thereisnosuchdifferentiationinspeechlevelsinMandari n Chinese. Therefore,morefeaturescanbematched. Also, from the linking of the pronouns, there were many cases where English pronouns were linked to Mandarin Chinese and Japanese pronounsthat are different in mean- ing such as the third person pronoun itin the English text to the demonstrative pronoun sore\"that\" or even to there\"there\" in Japanese. Although this happens in the English-Chinese corpus as well, they are less fre- quent, thus resulting in more of the pronouns linked have morematchedfeaturesascomparedtothoseintheEnglish- Japanese corpus. We give an example of this in (6), with EnglishandChinese,where itislinkedto zhe4\"this\". (6) a noun) occurs almost evenly in both the English-Chinese and English- Japanese corpora As seen in the results, the number of pronounsmatchedto non-pronounsinthe English-Chinese corpus is around the same. This result is not expected as depronominalisationwaspredictedtooccurmuchmorefre- quentlyintheEnglish-JapanesecorpusthanintheEnglish- Chinese corpus. It could be a case of the source lan- guage effecting the translation: although native speakers said the translationswere good, they almost certainly have more pronouns than texts written originally in Chinese or Japanese. From the tagging of the pronouns and their concept links, there were a few interesting cases that were found. In the Englishsourcetext,werealizedthatpronounsoftenexisti n idiomaticphrases. However,thesepronounsdonotactually haveanyparticularantecedenttorefertoastheyarealmost always used in the same way regardlessof its environment andthismeansthattheycannotbelinked. (7) a. MyGod! b. Tian1na 'Heaven!' Nan te kottai 'What theheck' We see in (7) that myis used here as a pronoun in an idiomatic phrase and after translation, no pronouns were seen. In both the Mandarin Chinese and Japanese text, the idiomatictranslationhasnopronouninit. We giveanother examplein (8). (8) c. Kansha shite iru yo '(I)amgrateful.' We give one nal example in (9). The Chinese translation here again choses to take the gurative meaning of I am in your hands and translated it to \"I will obey all your in- structions\". However, in the Japanese text, this is literal ly translated, possibly because the phrase is commonly used intranslatingprayersandisthussomewhatestablished. (9) a. obey all your instruc- tions' c. Anata no te ni subete o o yudane shimasu wa 'Iwill leaveeverythinginyourhands' Another interesting note was that other than pronouns, both Mandarin Chinese and Japanese tend to use classier phrasesanaphorically. Numeralclassiers(likethe headin two head of cattle) are used for most nouns in Japanese. The classier can combine with numerals, interrogatives and in Chinese determiners. The resulting phrase can be used anaphorically: for example na4jian1 \"that room (CLASSIFIER )\" which can mean '\"hat house/room\". With- out the need of the proper noun in Mandarin Chinese, the determiner+classier word can be used to refer to a cer- tain room, thus acting like a pronoun. Although classiers are not as widely used in English as in Mandarin Chinese and Japanese, numerals in English can sometimes take on anaphoricrolesaswell. The annotationscheme we use here has two parts: the lex- icon, which in this case is richly structured with compo- nents,andthe corpus,whichallowsannotationofconcepts and linksbetween them. The two have to be kept synchro- nized.865. Work We wouldliketoextendtheannotationina fewways. One isto tagmoretextsin theNTU-MC.Thepronoundistribu- tions in this paper are solely extracted from one story and thus we cannot generalize the results across genres.2The second is to add more languages to the pronoun analysis: our next language will be Indonesian, again from a differ- ent language family. We also want to extend the compo- nential analysis to related words such as terms of address and numeral classiers. Chinese, Japanese and Indonesian all use kinship terms to refer to non-kin: you may address astrangeras uncleoroldersister . Wewouldalsoliketoexaminefurtherthecasesofpronouns of pronounor demonstrativeor commonnounphrase? We hope that the crosslingual analysis will give some insights into the different strategies employed in the different lan - guages. Ourdistinctionbetweencontentfulandstructuralpronoun s isstillonlyinformallydescribed. Wewouldliketosharpen thisdistinction. Finally,ouranalysisiscompatiblewith(andpartlyinspir ed of pronouns in the En- glishResourceGrammar(ERG),anHPSGimplementation of English (Flickinger,2000). We would like to check that all ourpronounsarein theERG andaddthemtothe corre- sponding grammars of Chinese, clearly between content- fulandstructuralpronouns,andcouldbeusedtohelpinthe monolingualannotation. The annotated corpora and extended from the NTU-MC website: http:// compling.ntu.edu.sg/ntumc . The corpus is li- Creative Commons Attribution Only Li- cense (CC BY)3, and the wordnets under their respective (open)licenses. 6. Conclusions In this paper we introduced an annotation scheme for pro- nouns based on a componential analysis. It was tested on three languages, and used to tag a Chinese, English and Japanese tritext. The results show that pronouns, though universal, are used differently across languages, resulti ng in a difference in distribution among the three languages and a difference in the concept links between the English- Chinese corpusand English-Japanesecorpus. We have be- gantoaccountforthesedifferencesandpresentedexamples ofsomeinterestingcases. With this study, we hope that translation issues regarding pronounusagewouldbeusefulandclearertothosewhoare learningthe languageand that the Japanese as theirsource tocontrol for translationese. 3creativecommons.org/licenses/by/3.0/T2-1-084). Thanks to Shan Wang, David Moeljadi and an anonymousreviewerfortheirhelpfulcomments. 7. References Anthony E. Backhouse. 1993. The Japanese Language: AnIntroduction . OxfordUniversityPress,Oxford. Francis Bond, Shan Wang, Eshley Huini Gao, Hazel Shuwen Mok, and Jeanette Yiwen Tan. 2013. Developing parallel sense-tagged corpora with word- nets. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse (LAW 2013),pages149-158,Soa. Collins COBUILD. 2005. English grammar . Harper Collins, 2edition. Arthur Conan Doyle. 1892. The Adventures of Sherlock Homes. GeorgeNewnes,London. Arthur Conan Doyle. 1905. The Return of Homes. Newnes, London. Project Guten- bergwww.gutenberg.org/files/108/108-h/ Database . MITPress. 2008. Development theJapaneseWordNet. In Language Resources and Evaluation (LREC 2008) , Marrakech. Chul-Kyu Kim. 2009. Personal pronouns in English and Korean texts: A corpus-based study in terms of textual interaction. JournalofPragmatics ,41:2086-2099. Charles N. Li and Sandra A. Thompson. 1989. Mandarin Chinese: A Functional Reference Grammar . University Understanding Japanese: A hand- bookforlearnersandteachers. KurusioPublishers. Eric S. Raymond. 1999. The Cathedral & the Bazaar . O'Reilly. Singapore Tourist Board. 2012. Your Singapore. On- line:http://www.yoursingapore.com . [Ac- cessed 2012]. Liling Tan and Francis an- notating the linguistically diverse NTU-MC (NTU- multilingual corpus). International Journal of Asian LanguageProcessing ,22(4):161-174. Shan Wang and Francis Bond. 2013. Building the chi- nese openwordnet(cow): Startingfromcoresynsets. In Proceedings of the 11th Workshop on Asian Language Resources, a Workshop at IJCNLP-2013 , pages 10-18, Nagoya. Shan Wang and Francis Bond. 2014. Building sense- taggedmultilingualcorpora. In NinthInternationalCon- ference on di Roma \"Tor Vergata\", (4) IBM Center for Advanced Studies, (5) Carnegie Mellon University, (6) ISCT - The paper describes the design and the results of a manual annotation methodology devoted to enrich the Senso Comune resource with semantic role sets for predicates. The main issues encountered in applying the annotation criteria to a corpus of Italian language are discussed together with the choice of anchoring the semantic annotation layer to the underlying dependency syntactic structure. We describe the two experiments we carried to verify the reliability of the annotation methodology and to release the annotation scheme. Finally, we discuss the results of the linguistic analysis of the annotated data and report about ongoing work. 1. Introduction Large-scale linguistic resources that provide relational in- formation about predicates and their arguments are indis- pensable tools for a wide range of NLP applications, where the participants of a certain event expressed by a predicate need to be detected. In particular, hand-annotated corpora combining semantic and syntactic information constitute the backbone for the development of probabilistic mod- els that automatically identify the semantic relationships conveyed by sentential constituents in text, as in the case of Semantic Role Labeling (Gildea and Jurafsky, 2002). In addition, annotated corpora enable the quantitative and qualitative study of various linguistic phenomena at the syntax-semantics interface and the development of data- driven models for lexical semantics. The LIRICS (Linguistic Infrastructure for Interoperable ResourCes and Systems) project has recently evaluated several approaches for semantic role annotation (Prop- Bank, VerbNet, FrameNet, among others) and proposed an ISO (International Organization for Standardization) ratied standard for semantic role representation that en- ables the exchange and reuse of (multilingual) language re- sources. The standard comprises 29 'high 2008; been mapped ( inter alia ) onto VerbNet roles and organized hierarchically (Bonial et al. 2011 a, b). Simi- lar lexicons/annotation efforts include the German SALSA project (Burchardt et al. 2006), the Czech dependency tree- bank and its PDT-Vallex valency lexicon. In this paper we present the design and the results of a man- ual annotation methodology based on the ISO-semantic roles, aiming at enriching the Senso Comune knowledge base of the Italian language (henceforth SC) with seman- tic role sets for predicates, to be used for linguistic research and NLP applications. In SC semantic roles sets are not as- signed to predicates axiomatically but they are induced by the annotation of the usage examples associated with the sensi fondamentali (word meanings which are predominant in terms of use among the most frequent 2000 words in the language, cf. De Mauro, 1999) of the verb lemmas. Themethodology encompasses annotation of the role played by participants in the event described by the predicate (inten- tional agent, affected entity, created entity and so on) as well as annotation of their inherent semantic properties, ex- pressed in the form of ontological categories (person, sub- stance, artifact, and so forth). In the rest of the paper, we rst present an overview of the SC resource, then introduce the annotation scheme and the experimental setting in which the scheme was nalized. Finally, we discuss the results of the annotations in terms of inter-annotator agreements and linguistic generalizations that can be drawn form the analysis of the data. We con- clude by observing how interoperability of lexical data can also be supported formally (in the spirit of SC) in a linked data perspective. 2. Resource overview The SC model features the main structures of standard lexicography (we refer to Vetere et al. 2012 for a gen- eral overview). These consist in lexical entries (lem- mas) with their linguistic characterization and their senses. Each sense is comprised of a denition (glossa), a num- ber of usage marks, specic grammatical constraints, us- age instances, and lexicographic relations. In addition, SC provides substantive senses with ontological annotations, whose labels are taken from a foundational ontology in- spired to DOLCE (Gangemi et al. 2002). The idea at the ba- sis of ontological annotations is that linguistic senses (also referred to as linguistic concepts ) are tangential to real- ity: they are abstract social entities whose relationship with extra-linguistic realities is established in the context of hu- man activities. This idea, which comes from semiotics, calls for a formal distinction between two kinds of inten- sional entities: linguistic concepts (i.e. senses) and onto- logical categories. In fact, the ontological classication of linguistic concepts is not intended as a direct extensional interpretation over some domain of real entities . Instead, we resort on a notion of ontological commitment : a word can be used in a certain sense to refer (even vaguely, evoca- tively, notionally or metaphorically) to entities of some hy- pothetical kind.88Also, we adopt the distinction between type and token which comes form classic semiotics (Peirce); the former being abstract sorts, the latter their situated concrete in- stances. For instance, the Gertrude Stein's verse a rose is a rose is a rose counts three rose word tokens which instanti- ateFLOWER-ROSE , i.e. the (single) specic sense of rose occurring in the sentence, which, in turn, commits to the ex- istence of objects which fall under the NATURAL-OBJECT ontological category. Note that commits is not to be read as logical implication ; on the contrary, senses and ontological categories are logically disjoint, so that lexical relationships (e.g. synonymy) do not imply, nor conflict with, ontologi- cal axioms (e.g. equivalence). 3. Annotation scheme and methodology On approaching the task of providing SC with verbal frames, we decided to start from tokens instead of types. Rather than speculating about predicate structures associ- ated with verbal senses, we focused on annotating usage instances, as registered in the dictionary. The compilation of type-level verbal frames `a la VerbNet is therefore de- ferred to a later process of generalization. To encode the annotation of verbal predicate structures, we opted for a model based on dependencies between shal- low syntactic structures, inspired to eXtended Dependency Graphs (XDG) (Basili and Zanzotto, 2002). Basically, the scheme foresees: the identication of flat constituents (chunks) the identication of the verbal chunk which conveys the exemplied sense the annotation of phrases which hold a thematic rela- tion with the verb. Argumental phrases are annotated according to the follow- ing characterization: each argumental chunk is given -a syntactic role (e.g. SUBJECT ) -a constituent type (e.g. NP) -a semantic role (e.g. AGENT ) -an ontological category tokens of the argumental chunk are -(automatically) assigned a POS tag and a lemma (lemmatisation) -(optionally, and manually) assigned a sense (dis- ambiguation) Both lemmatisation and disambiguation are based on the SC dictionary. The information structure described above is encoded in a specic annotation data model (Fig. 1). This model is specied in OWL, as part of the ontology underly- ing the SC knowledge base1. Also, we provide a Java implementation which is made persistent and accessible 1http://www.sensocomune.org/ontologies/ Figure 1: The Annotation Model on relational databases through an object-relational map- ping. Thus, actual annotation data are integrated in the gen- eral SC database, which allows issuing conjunctive queries where lemmas, senses, grammatical features and argument structures can be joined to extract relevant patterns. The induction of type-level verbal frames from usage an- notation data will require a process of generalization whose study is included in our future plans. To represent typical verbal frames, we plan to adopt a model in which semantics and syntactics are structurally separated, and yet logically connected. This model aims at preserving the generality of semantic structures as distinct from their syntactic real- izations. Our intuition is that, by decoupling semantic and syntactic frames, one could achieve a powerful and con- cise representation of linguistic data, to better handle and investigate their interplay. For instance, action frames in- cluding participants and objects may be rendered in either passive or active forms; still, retrieving the lexical concepts involved in certain actions can abstract from the syntactic unfolding of verbal arguments. In the following sections we describe the component and tags of the scheme in more detail. 3.1. Constituents and Dependency relations We choose a light annotation scheme for syntactic depen- dency relations. Focusing the dency relations, we dened three types of relations: Subject (S), Object (O), and other Complement (C). We avoided the distinction, at the syntactic level, between Complement and Adjunct. This distinction is out of the scope of the syntac- tic phase as it is a target of the overall process of frame annotation. As the model is inspired to the extended dependency graphs XDG) (Basili and Zanzotto, 2002), the syntactic depen- dency relations link constituents. We focus on the con- stituents that may play a role as verb arguments: latter is little tricky as it is dened as a subsentence headed by a verb that is not the target verb. An example for89SC role LIRICS role Agente (AG) Agent, Causa (CAUSE) Reason Strumento (INSTR) Instrument, Means Paziente (PT) Patient Tema (TH) Theme, Pivot Goal (GOAL) Goal Beneciario (BEN) Beneciary Origine (SOURCE) Maniera (MANNER) Manner, Medium Esperiente (EXP) Patient Scopo (PURPOSE) Purpose Frequenza (FREQ) Frequency Attributo (ATTR) Attribute Table 1: Semantic roles set the two levels of annotations is the alla madre) (Luca a book to his mother) where Luca andil libro (the book) are nominal phrases (SN) and alla madre (to his mother) a prepositional phrase (SPred). The three phrases play, respectively, the syntactic role of subject (S), object (O), and other complement (C). 3.2. Semantic Role list The list of SC roles comprises 24 coarse-grained (high- level) semantic roles based on LIRICS (Petukhova and Bunt 2008) and the on-going attempt to create a unied standard set for the International Standard Initiative with the goal of facilitating mappings between semantic re- source of different granularity, including VerbNet (Bonial et al. 2011 a, b). In designing the set, we conflated some LIRICS roles such as Agent and Partner (Co-Agent in Verb- Net), and used some classical semantic roles like Experi- encer rather than LIRICS's ambiguous Pivot. The nal set of categories is given in Table 1, together with the map- pings with the ISO roles of LIRICS. Each roles is dened by a gloss and a set of examples, in the LIRICS style. 3.3. Role Taxonomy To facilitate the understanding of the scheme adopted, in addition to the glosses and the examples, semantic roles are structured into the taxonomic hierarchy of Fig. 2, in a similar way to what is done in (Bonial et al. 2011b) for LIRICS and VerbNet unied roles. A main difference is that we have added intermediate nodes that do not count as role labels, but, with further glosses,help the annotator in understanding the main discriminating elements between roles. This enabled implementing an on- tological distinction between roles that identify event par- ticipants proper, and roles that identify elements of the con- text of the event. As a result, some distinctions that might be difcult to grasp at rst, such as Luogo Iniziale (Initial Location) vs. Origine (Source), are made clearer: in this example the rst is part of the spatial context of the event, while the second is a proper and non-spatial participant to the event. 3.4. Ontological categories and TMEO methodology In the context of Senso Comune we developed a tutoring system to support collaborative ontology population. As the acronym may suggest to philosophers, TMEO (Tutor- ing Methodology for the Enrichment of Ontologies) re- calls Plato's dialectic methodology of discovering knowl- edge through reasoning in dialogues (Reale 1990): in this regard, by distilling the key ontological properties of SC into germane questions targeted at users, TMEO plays the role of a 'digital Socrates' in a basic interaction system. For instance, consider the scenario in which a given user is asked to classify the term shoe, in the sense of \"footwear shaped to t the foot (below the ankle) with a flexible upper of leather or plastic and a sole and heel of heavier material\". TMEO system's interface will submit a series of intuitive conceptual questions to the users in order to disambiguate the intended meaning of the term. The following sequence represents a simplied scenario based on this example: TMEO: Can you touch, see, smell, taste, feel a shoe ? User: Yes TMEO: Would you say that \"a shoe can happen or occur? User: No TMEO: In general, does it make sense to use the word shoe as answer to the question \"when\"? User: No TMEO: does shoe indicate a location? User: No TMEO: Can shoe s act by intention? User: No TMEO: Would you say that shoe s are built by some- one? User: Yes TMEO: shoe in the sense of 'footwear shaped to t the foot (below the ankle) with a flexible upper of leather or plastic and a sole and heel of heavier material' has been classied as ARTIFACT . As the above-mentioned scenario suggests, TMEO method- ology may therefore be adopted not only in the unilateral classication of a given term ('shoe') but also in mak- ing related lexical items explicit. This kind of relatedness between terms actually unwraps the inter-categorial rela- tion(s) holding between the corresponding ontological cat- egories (since a detailed presentation of TMEO is out of scope in the current paper, we remand the reader to a more comprehensive publication (Oltramari et al. 2012).90RoleContextAttributoParticipantSpatialContextTemporalContextQuantitaManieraScopoActorUndergoerDistanzaLuogoPercorsoDurataFrequenzaTempoLuogoInizialeLuogoFinaleTempoInizialeTempoFinaleAgenteCausaAffectedUnaffectedRisultatoPazienteEsperienteTemaOrigineGoalStrumentoBeneciarioFigure 2: The semantic role taxonomy. TMEO has been implemented as a nite state machine (FSM): in general, the elaboration process of a FSM be- gins from one of the states (called a 'start state'), goes through transitions depending on input to different states and must end in any of those available (only the subset of so-called 'accept states' mark a successful flow of the architectural framework of TMEO, the 'start state' is equivalent to the top-most category , the 'transitional states' correspond to disjunctions within onto- logical categories and 'accept states' are played by the most specic categories of the model, i.e. 'leaves' of the relative taxonomical structure. In this context, queries represent the conceptual means to transition: this means that, when the user answers to questions like the ones presented in the above-mentioned example, the FSM shifts from one state to another according to answers driven by boolean logic2). If no more questions are posited to the user, this implies that the system has reached one of the available nal 'accept state', corresponding to the level where ontological cate- gories don't have further specializations. TMEO human language interface is very intuitive and comes in the form of a map where yes/no options are presented together with the step-by-step questions: gure 3 shows the 'shoe' exam- ple in the Italian translation 'scarpa'. In future work we aim at extending the coverage of TMEO's model and improving the scalability of the system towards genuine crowd-based platforms. The ontological categories underlying the TMEO method- ology form a taxonomy as in Fig. 4. The annotation of ontological categories performed in the context of the work reported here differs from the annotations already present in the SC resource and de- scribed in earlier work. Here, instead of a lexical entry with its gloss, annotators were presented a text span in the context of a usage instance. In addition, they were suggested to annotate this text span with multiple categories if this was deemed more adequate than a single one. Such a possibility was introduced to acknowledge the inadequacy of a unique categorization when several 2Uncertainty will be included only in future releases of the TMEO system. Figure 3: Senso Comune's interface for TMEO interpretations co-exist due to systematic polysemy (e.g. \"book\" often refers simultaneously to an artifact and to an information object). Finally, the annotators were pushed to distinguish between singular and collective use of such categories. As a result, a text span like \"Un ufcio\" in the example \"Un ufcio che funziona\" reliability of the annotation scheme by com- paring annotations carried out by multiple annotators inde- pendently. In the following sections we describe the two pilot experiments we carried out, during which the same portion of the corpus was annotated by several participants. 4.1. Annotation experiment We evaluated the annotation procedure in two experimental settings involving multiple annotators and estimated their agreement on the task. We selected 22 target verbs and performed multiple annotation on a set of 66 non disam- biguated examples (3 for each target verb). The annotation91EntitaTangibileNonTangibileOggettoPostoEventoPeriodoEntitaSocialeProprietaSostanzaParteRilevanteAgenteArtefattoOggettoNaturaleInformazioneOrganizzazioneOggettoSocialeGenericoQualitaFunzioneQualitaTemporaleQualitaSpazialeModoQualitaGenerica PersonaAnimaleAutomaCambiamentoStatoProcessoFisicoOperazioneMentaleAzioneStatoFisicoStatoCorporeoStatoPsichicoFigure 4: The ontological category taxonomy. task was split in two subtasks. We rst performed syntactic and semantic role annotation; then, we supplied the anno- tators with the data annotated with the sole syntactic layer, and asked them to annotate the ontological category of the argument llers. Verbs were selected according to variabil- ity in semantic selection (for both roles and ontological cat- egories) and syntactic realization. 4.2. Span detection Detection the span of the verb arguments is one of the most important activity when annotating. The span of the verb argument dene the sentence chunk that has to be syntac- tically and semantically annotated. Each annotator has to work on the same span in order to make annotations compa- rable. Even if the annotators decide for the same syntactic and semantic label for a nearly similar chunk of sentence, annotations cannot be compared. Thus, for comparing the annotations we assessed a gold standard, that is the most voted span for each argument. 5. Results 5.1. Interannotator agreement The two annotation experiments were done by 9 annotators each. Among those annotators, we removed a few outliers, 1 in the rst experiment and 2 in the second, for obvious misunderstanding of the task, resulting in 8 and 7 annota- tors respectively. We chose to use average pairwise Cohen's kappa as a measure of inter-annotator agreement, data be- ing particularly skewed (Artstein and Poesio 2008). For the rst experiment, the inter-annotator agreement among the 8 annotators is 0.86 for the subtask on syntactic dependency relations (4 labels: 3 relations + no annotation) and 0.66 for the subtask on semantic roles (25 labels: 24 roles + no annotation). Such values are usually considered respectively as very good and fair, the latter especially so since semantic tasks are notoriously difcult. Subgroups of annotator apparently achieved a deepest ex- pertise, with pair agreement respectively reaching maxi- mums of 0.91 and 0.88 on each sub-task. In the second experiment, since we gave annotators the pos- sibility to annotate multiple categories, there were in to- tal 60 different labels (including no annotation). The raw agreement among the 7 annotators is quite low at 0.41. Taking into account partial agreement in the relatively fewcases in which annotators used multiple categories (27 oc- currences) and/or used the collective tag (36 occurrences), the agreement slightly rises to 0.46, with a pairwise maxi- mum of 0.57. However, taking advantage of the hierarchi- cal organization of the categories into a taxonomy, mean- ingful aggregation of categories can be proposed. For in- stance, one can reduce the 30 base-category labels in Fig. 4 actually used (only the coloured nodes have been used in the experiment), a rather large gure, into 9 labels corre- sponding to the orange-coloured ones on this gure. This forms a more shallow ontology, but still a meaningful dis- criminating one, and yields 17 different labels (with mul- tiple categories and collectives). With such a reduction of the labels, the overall agreement clearly increases at a rea- sonable 0.60, with a pairwise peak at 0.79. Further anal- ysis of the data may show where exactly annotators tend to diverge, enabling focusing on specic merges only and keeping a more ne-grained taxonomy. 6. analysis of annotations Besides conrming well-known difculties in semantic role annotation, such as confusion between PT and TH due to uncertainties in the interpretation of the notions of \"modi- cation\", the specicity of the annotation scheme allows us to make interesting observations regarding the role played by the semantic context, particularly the ontological cate- gory associated with the argument ller, in semantic roles annotation. This can be illustrated by focusing on the an- notation of the semantic role of the subject for the 24 cases in our corpus in which there is complete agreement about the inanimate nature of referent of the ller. The rst obser- vation is that in these cases there is much more confusion between roles than average (average of kappa = 0,51). In our view this is related to the following aspects (as a refer- ence theoretical framework cf. Pustejovsky 1995): there is metonymy between verb and argument in the context the noun is inherently polysemous the verb exhibits a shift in meaning the annotator confuses the inherent properties of the argument ller with its role.92Consider for example the case of disagreement between AG and TH (the most frequent in this set of data), that can be found in examples such as \"il treno corre nella pianura a 100 all'ora\" in the plains at 100 Km/h' 3AG / 5TH). In these cases, the annotator is confused by the fact that the verb in its basic meaning reports an intentional eventuality, whereas the ller in the instance is inanimate. It appears that two solutions are taken in annotation: ei- ther the ller is somewhat interpreted metonymically and assigned the AG role, or the verb is interpreted as carrying a meaning which is not the basic agentive meaning, and the subject is tagged TH. The additional case of \"Un ufcio che funziona\" ('An of- ce that works well' 5 AG / 3 TH) appears to be more com- plex, due to the inherent polysemy in the noun. In fact, in this case, we register high disagreement not only at the level of roles but also at the level of ontological categories, where case, one can argue that two phenomena are at play simultaneously, which confuse the annotators: the verb dis- ambiguates the polysemous noun in context but at the same time its meaning is redened by it (from 'to work properly' to 'to perform a task well'). Among our 24 cases, other signicant cases of disagree- ment can be found with nouns denoting instruments. Con- sider the examples \"la penna cut well', have INSTR by 3/8 and 4/8 respectively ( penwas further tagged as TH by 5/8, while scissors as TH by 3/8 and AG by 1/8). These subjects (called Instrument subjects in literature, see e.g. Alexiadou et Sch \u00a8afer 2006) refer to entities frequently used as facilitat- ing instruments in everyday life (as expressed in sentences like \"I wrote the letter with a fountain pen\", \"I used the scissors to open the package\"), but in the examples above they are not presented as instruments, but rather as the en- tity about which the verb predicates something (that is, they have the characteristic of writing and cutting). Nobody uses them to perform an action; hence, they are THs because they are the participants in the condition described by the verb and are not modied by the event. We argue that in these cases annotators who tag them INSTR confuse the ontological type of the entity denoted by the ller with the semantic role the participant plays in the event. 7. Interoperability of Semantic Roles on the Semantic Web SC has been formally represented in OWL, and this of- fers an opportunity to make it interoperable at both synset level (through an ongoing alignment to the Italian version of MultiWordNet, which will be part of the Lexical Linked Data Cloud), and at semantic role level, by aligning it to the VerbNet and FrameNet RDF datasets. Recently, the problem of interoperability between differ- ent linguistic ontologies (schemas for representing linguis- tic data) has entered the Semantic Web and Linked Open Data radar, since there are mutual advantages in creatinglinguistic data expressed in RDF (the basic language for the Semantic Web): the Web as an integration platform for heterogeneous linguistic data, as well as easier support for lexicalizing ontologies. In that context, several initiatives are boosting the adoption of good practices for sharing linguistic data, and make them interoperable at a formal level. NLP Interchange Format (NIF) is an RDF/OWL-based format that allows to com- bine and chain several NLP tools in a flexible, light-weight way. The Linguistic Linked Open Data initiative is link- ing many linguistic datasets, but it is still missing a tight integration of lexical resources including semantic roles. FrameNet and VerbNet have been ported to RDF and OWL (cf. Nuzzolese et al. 2011 for FrameNet-OWL), including the mapping between FrameNet frames and VerbNet pred- icates, but this is not yet extended to the respective role structures. The OntoLex W3C Community Group is going to publish a proposal for a standard to describe lexical re- sources jointly with ontologies and linked datasets (where the basic innovation is to allow for a sense layer distin- guished from lexical expressions and ontological entities, which enables intensional semantics of lexical resources to be used in the mostly extensional formal semantics as- sumed in the Semantic Web). The potential of the Semantic Web for semantic role label- ing (and vice versa) is exemplied by the FRED architec- ture (Presutti et al. 2012), where VerbNet roles are used to automatically annotate RDF graphs that are extracted from text by means of multiple NLP algorithms (semantic role labeling, frame detection, relation extraction, sense disam- biguation, named entity recognition). FRED allows to link those graphs to linked data resources; it aligns named entities to linked data resources, as well as named concepts (typically derived from disambiguated terms) to WordNet or DBpedia resources. Since RDF re- sources are usually typed, FRED graphs can be used for investigating the actual coverage of VerbNet roles, with their associated types ( `a la selectional restrictions). In fact, FRED complements partial coverage of VerbNet with other roles, e.g. directly expressed by prepositions, which can be further investigated. 8. Conclusions In this paper, we described the design of a manual annota- tion methodology devoted to enrich the SC resource with semantic role sets for predicates. We discussed the results of the two experiments performed to verify the reliability of the annotation methodology, in terms of inter-annotator agreement and linguistic generalizations that can be drawn form the analysis of the data. For the future, we plan to per- form automatic chunking of the data to be annotated and check it manually before annotation; to annotate the on- tological category of the argument llers out of context; to develop a methodology for extraction of semantic roles sets for predicates from the annotated data; to link SC seman- tic roles sets to other lexical resources for Italian such as T-PAS structures (Jezek et al. 2014).93Acknowledgments We thank three anonymous reviewers for their useful com- ments. References A. Alexiadou and F. Sch \u00a8afer. 2006. Instrument Subjects Are Agents or Causers. In D. Baumer, D. Montero, and M. Scanlon (eds.), Proceedings of the 25th West Coast Confer- Formal Linguistics corpus: a german corpus resource for lexical semantics. In Proceedings of LREC 2006. R. Artstein and M. Poesio. 2008. Inter-coder agreement for computational linguistics. In Computational 34, 4, 555-596. R. Basili and F.M. Zanzotto. 2002. Parsing engineering and empirical robustness. Natural Language Engineering , 8/2-3. C. Bonial, S.W. Brown, W. Corvey, V . Petukhova, M. Palmer, H. Bunt. 2011a. An Exploratory Comparison of Thematic Roles in VerbNet and LIRICS. In Proceedings of the Sixth Joint ISO - ACL SIGSEM Workshop on Interoper- able Semantic A hierarchical unication of LIRICS and VerbNet semantic roles. In Proceedings of the 2011 IEEE Fifth International Conference on Semantic Computing , IEEE Computer Society Washington, DC, USA, 483-489. T. De Mauro. 1999. Introduzione. In De Mauro Se- A. Gangemi, Oltramari, L. Schneider. 2002. Sweetening Ontologies with DOLCE. Proceedings of the 13th International Conference on Knowledge Engineering and Knowledge Management (EKAW 02) . D. Gildea and D. Jurafsky. 2002. Automatic roles. Computational Linguistics , V ing Semantic Roles and Modelling the Syntax-Semantic Interface. Proceedings of the International Confer- on Semantics (IWCS E. Jezek, B. A. Feltracco, A. Bianchini and O. Popescu. 2014. T-PAS: A resource of corpus-derived Types Predicate-Argument Structures for linguistic analysis and semantic processing. in Proceedings of LREC 2014 (to ap- 2011. Data Knowledge Patterns from FrameNet. In O. Corcho, M. Musen (eds.) Proceedings of K-CAP 2011 The Sixth International Conference on Knowl- edge Capture , ACM. A. Oltramari, A. H. L\u00a8ungen, A. Storrer, A. Witt. 2012. An Introduction to Hybrid Semantics: The Role of Cognition in Seman- tic Resources. In Modeling, Learning, and Processing of Text Technological Data Structures , Berlin / Heidelberg, Springer, vol. 370, 97-109. V . Petukhova, H. Bunt. 2008. LIRICS semantic role anno- tation: Design and evaluation of a set of data categories. In Proceedings of the Sixth International Conference on Lan- guage Resources and Evaluation (LREC . Presutti, F. Draicchio and A Gangemi. 2012. Knowledge Extraction based on Discourse Representation Theory and Linguistic Frames. In A. ten Teije and J. Vlker (eds.) Pro- ceedings of the Conference on Knowledge Engineering and Knowledge Management (EKAW2012), LNCS, Springer. J. Pustejovsky. 1995. The Generative Lexicon . Cambridge MA: MIT Press. G. Reale. 1990. A History of Ancient Philosophy: Plato and Aristotle . SUNY Press. G. 2012. Comune': In Revue TAL (Traitement Automatique des Langues) , Journal Special Issue Free Language Re- sources 52.3, 217-43.94Automatic Tagging of Modality: Department of Informatics, University of \u00c9vora, Po rtugal 2 Center for Linguistics at the University of Lisbon, Portugal 3 Center for Language Studies, Radboud University Nij megen, The Netherlands 4 L2F - Spoken Language Systems Laboratory, INESC-ID, Portugal Abstract We present an experiment in the automatic tagging o f modality in Portuguese. As we are currently lacki ng a suitable resource with detailed modal information for Portuguese, we exper iment with small sample of 160.000 tokens, manually annotated according to the modality scheme that we previously developed for Eu ropean Portuguese (Hendrickx et al., 2012). We cons ider modality as the expression of the speaker (or subject)'s attitude t owards the proposition and our modality scheme acco unts for seven major modal values, and nine sub values. This experiment focuse s to', which may all have more than one modal value. We first report on the task of correctly detecting the modal uses of poder and dever , since these two verbs may have non modal meanings . For the identification of the modal value of each occurrence of those three verbs, we applied a machine learning approach that takes into considera tion all the features available from a syntactic parser's output. We obtained the b est performance using SVM with a string kernel and the system improved the baseline for all three verbs, with a maximum F-scor e of 76.2. Keywords: modality, annotation scheme, automatic tagging 1. Introduction As the vast amount of digitally available data keep s growing, so does the demand to automatically extrac t relevant information. A clear problem for automatic extraction tools is to recognize the factual or non -factual nature of events, and the subjective perspective underlying the texts. In this paper we focus on mod ality: an important indicator of subjectivity and factuali ty in text. Modality is usually defined as the expression of the speaker's opinion and of his attitude towards the proposition (Palmer, 1986). It traditionally covers epistemic modality, which is related to the degree of commitment of the speaker to the truth of the propo sition (whether the event is perceived as possible, probab le or certain), but also deontic modality (obligation or permission), capacity and volition. Modality detect ion is therefore also clearly linked to the current trend in NLP on sentiment analysis and opinion mining. This paper presents an experiment in the automatic tagging of modality in Portuguese. Not much related work has been done in this area, certainly not for langu ages other than English. A prerequisite for building an automatic modality tagger is to have a corpus with labeled examples to train and evaluate such tool. As we are currently lacking a large and suitable corpus, one of the main aims of the study presented here is to create a tagger on a small corpus sample in order to (semi) automat ically tag a larger corpus with modality information. For this purpose, we use a corpus of 158.553 tokens, manuall y annotated with a modality scheme for Portuguese (Hendrickx et al., 2012b). In this paper, we restri to'. These three verbs are high frequent words in Portuguese and have different modal meanings, what makes them an excellent study object for our experi ments. The automatic modality tagger that we devised has two objectives: the identification of modal verbs ( which we call the modal trigger) and the attribution of a modal value to this trigger. All three verbs have two or more modal meanings: for example, poder may be Epistemic, stating that something is possible, as in example ( 1); Deontic, denoting a permission, as in (2); or it ma y express an Internal capacity, the fact that someone is able to do something, as in (3). And frequently, a singl e context may be ambiguous between one and more of these readings. (1) E \u00e9 evidente que um jogador que arrisque pode vir a arriscar mais. 'It is obvious that a player that takes risks migh t be caught but, without the certainty that there will b e a control, in my opinion he will tend to take more risks.' (2) Segundo C\u00e2ndida to C\u00e2ndida Almeida, \"the journalists ca n not use means that the law itself forbids to the po lice and to prosecutors in the name of the citizen's rig hts, liberties and warranties. (3) Portuguese representatives to the European Parliament, to be heard and to have influence, need to be able to communicate easily with their 95colleagues, what implies, in a genuinely multilingu al environment, the mastery of several foreign languages.' This polysemy increases the level of difficulty of the automatic annotation task. To create the modality t agger, we first automatically assign POS and syntactic tag s, we then automatically identify modal triggers and appl y a machine learning approach to attribute a modal valu e to the triggers, comparing the results with our gold d ataset of 158.553 tokens. The paper is structured as follows: we first revise related work in section 2, before briefly presentin g our modality scheme and golden dataset in 3. Our automa tic annotation system is described in section 4, the re sults of trigger identification are presented in 5.1 and the results of automatic attribution of modal value in 5.2, follow ed by a conclusion in 6. 2. Related work Several annotation schemes of modality have been proposed in recent years, such as Baker et al. (201 0), Matsuyoshi et al. (2010); Saur\u00ed et al. (2006), Nire nburg and McShane (2008) and, for Brazilian Portuguese, \u00c1 vila and Melo (2012). We will not discuss here in detail the differences between those annotation schemes (see Hendrickx et al. (2012b) and Nissim et al. (2013)) but rather focus on some experiments in the automatic annotation of modality that have been reported, mai nly for English. Baker et al. (2010) tested two rule-based modality taggers to identify the modal trigger and its target and report results of 86% precision for tagg ing of a standard LDC data set. Also, Saur\u00ed et al. (2006) re port on the automatic identification of events in text, and their characterization with modality features, achieving accuracy values of 97.04 with the EviTA tool. Batti stelli and Damiani (2012) segments have enunciative and modal (E_M) features. They use semantic clues to identify modal triggers and a syn tactic parser to calculate the length of the E_M segment. However, the implementation of the system is an upcoming work. A specific system for the annotation of belief is reported by Diab et al. (2009). The autho rs mention that they treat all auxiliary verbs as epis temic, although they are aware of the fact that they may b e deontic, and consider that this might be a source o f noise in their system (an aspect that we also have to dea l with). An extension of this experiment is reported in Prabhakaran et al. (2012), testing the tagging of d ifferent modality values (Ability, Effort, Intention, Succes s and Want). The authors report experiments on MTurk annotations (using only those examples for which at least two Turkers agreed on the modality and the target o f the modality) and on a gold dataset, with respectively an overall 79.1 and 41.9 F-measure. It is important to mention that the corpora for both experiments diffe r greatly: MTurk data is entirely from email threads, whereas Gold data contains sentences from newswire, letters and blogs in addition to emails. The work of Ruppenhofer and Rehbein (2012) is close to our own objectives in this paper. The auth ors report an experience to automatically identify five English modal verbs ( ) in texts and predict their modal value, by training a maximum entropy classifier on features extracted from the training set. The authors manage to improve the baseline for all verbs but must , and achieve accuracy numbers between 68.7 and 93.5. The detection of uncertainty and its linguistic sco pe was the subject of a shared task at CoNLL2010 (Fark as et al., 2010) focusing on hedging clues, which include s a broader set of lexical and syntactic clues than mod ality as we contemplate it in this paper. The area of BioNLP includes modality and factuality in the annotation of events: the dimension \"level of certainty\" is part of the system of meta-knowledge assignment to pre-recognis ed events described in Miwa et al. (2012), which attai ns F- measures of 74,9 for \"low confidence\" and 66,5 for \"high but not complete confidence\". 3. Annotation Scheme and Corpus The annotation scheme for Portuguese presented in Hendrickx et al. (2012a) is not restricted to modal verbs and also covers nouns, adjectives and adverbs. Moda lity is understood as the expression of the speaker's attit ude towards the proposition. So, the concept of factual ity is not included, contrary to approaches such as Nissim et al. (2013), who accounts for both values but in differe nt layers of the annotation scheme. Furthermore, our annotation scheme does not account for verb tense a nd mood, although this category is related to modality . The approach is very similar to the OntoSem (Mcshane et al., 2005) annotation scheme for modality (Nirenburg and McShane, 2008). We include several modal values, based on the modality literature, but also on studies focused on annotation and information extraction (e.g. Palmer (1986); van der Auwera and Plungian (1998); Baker et al. (2 010)). Seven main modal values are considered (Epistemic, Deontic, Participant-internal, Volition, Evaluation , Effort and Success), and There are fiv e sub- values for epistemic modality: Knowledge, Belief, D oubt, Possibility and Interrogative. Contexts traditional ly considered of the modal type \"evidentials\" (i.e., s upported by evidence) are annotated as Epistemic belief. Two sub values are identified for deontic modality: Deontic obligation and Deontic permission (this includes w hat is sometimes considered Participant-external modality, as in van der Auwera and Plungian (1998)). Participant-in ternal modality is subdivided into Necessity and Capacity. Four other values are included: Evaluation, Volition and , following Baker et al. (2010), Effort and Success. We present the list of values and sub values in Table 1, together with their frequency in our golden set. 96 Figure 1: Screenshot of MMAX2 annotation tool Main modal values Sub values Freq % Epistemic knowledge 183 7,1 belief 161 6,3 doubt 29 122 4,8 Evaluation 159 6,2 Volition 396 15,4 Effort 110 4,3 Success 119 4,6 Table 1: Modal values and frequencies in our golden set The annotation scheme comprises several components: (a) the trigger, which is the lexical e lement conveying the modal value; (b) the target; (c) the source of the event mention (speaker or writer) and (d) th e source of the modality (agent or experiencer). The trigger receives an attribute modal value , while both trigger and target are marked for polarity. An example with the verb dever is given in (4)1. In fact, the example sentence in (4) contains three other triggers as well. In this part icular context, the trigger esque\u00e7o 'I forget' expresses the Epistemic knowledge, the expresses and the trigger capaz 'be able' expresses Participant-internal capacity. In example (4) however we focus on the annotation of the trigger dever in more detail. (1) Nunca me esque\u00e7o da ironia arrasadora de Church ill, que o pol\u00edtico devia ser capaz de amanh\u00e3, e que previu n\u00e3o 'I never forget the devastating irony of Churchill , who argued that a politician should be able of predicting what is going to happen tomorrow, next 1 Notice that the discontinuity of the target is mark ed with the symbol @ in the example, but is encoded in XML in o ur data set. month and next year and then explain why what he had predicted se i passar amanh\u00e3, no pr\u00f3ximo m\u00eas e no pr\u00f3ximo \u00e9 que previu n\u00e3o Source of the modality: Churchill Source of the event: writer Ambiguity: none This annotation scheme was applied to a corpus sample extracted from the written subpart of the Reference Corpus of Contemporary Portuguese (CRPC) (G\u00e9n\u00e9reux et al, 2012). Details about the selection of the sample are provided in Hendrickx et al (2012b). We used the MMAX2 annotation software tool (M\u00fcller and Stru be, 2006) for our manual annotation task. The MMAX2 software is platform-independent, written in java a nd can freely be downloaded from http://mmax2.sourceforge.net/ . The elements of our annotation consist of markables that are linked to the same modal event, which we call a \"set\". We present a screenshot of the results in Figure 1. The trigger devia and related markables are connected under a single set and are highlighted. Full details on our annotation scheme and on the results of an inter-annotator experiment are provid ed in Hendrickx et al. (2012b). An enriched version with the interaction between Focus and Modality, specificall y the case of exclusive adverbs, is presented in Mendes e t al. (2013). In the experiments that we present here, we focus o n the Trigger component and its attribute modal value , and specifically on three semi-auxiliary modal verbs. T he frequency of the modal verbs in our data set and th eir values are presented in Table 2. The verb dever has two modal values in our golden set: Deontic obligation and Epistemic possibility. The value Participant-internal capacity is also possibl e with this verb but was never selected in our data as the primary meaning, although manual annotators have marked it in the 'Ambiguity' field of our annotation system in s everal cases. For this experiment, we didn't take into consideration cases marked as ambiguous but this is certainly an important aspect to tackle in future r esearch. Our experiments will therefore focus on five modal values: Deontic obligation, Deontic ess. 97Main values Sub values Freq. dever 113 Deontic obligation 74 Epistemic possibility 39 poder 244 Deontic permission 43 Epistemic possibility 158 Participant-internal capacity 44 conseguir 84 Participant-internal capacity 41 Success 43 Table 2: Frequency of dever , poder and conseguir in our gold dataset. 4. Modality tagging Our automatic modality tagger is composed by three modules: Syntactic analysis verb with the appropriate modal value in its specific context. The syntactic analysis was performed by the PALA VRAS parser (Bick, 1999), and the results were transformed into XML and logical terms (Prolog form at) using the tool Xtractor (Gasperin et al., 2003). We then selected the set of parsed sentences that included the modal verbs and distinguished the modal uses of the verbs from the non-modal ones. As we aim to use this tagg er to create a larger corpus, this first step of finding the modal triggers needs to be performed with very high accur acy. We then used SVM, Support Vector Machines (Vapnik, 1998), to classify the modal value of each verb. We evaluated several machine learning algorithms an d SVM kernel types with Weka (Hall et al., 2009), and obtained the best performance using SVM with a stri ng kernel (Lodhi et al., 2002). We report the results obtained in two experiments: one using just the original sen tences and another using the POS tags and functional and syntactic information extracted from the sentence's parse tree, in a window of 70 characters around the verb. For the evaluation we used a 10-fold stratified cross-valid ation procedure. Note that this is a challenging task as we only have a few hundred examples to train and test the automatic tagger. We analyze the results in the nex t section. 5. Results 5.1 Modal verb detection Here we first discuss to what extent we were able t o correctly detect the modal verbs based on the outpu t of the automatic syntactic parser. The verbs poder and dever may occur with non-modal uses, therefore the task involves the correct identification of contexts tha t are indeed modal. The case of the verb conseguir is different because it always involves one of the modal values contemplated in our annotation system. For this spe cific verb, the system has to correctly identify sentence s containing the lemma in the results of the parser, a much simpler task. Taking this into consideration, we wi ll only discuss the results obtained for the verbs poder and dever , and compare our system's output with the manually tagged information. This is summarized in Table 3. poder dever total verb occurrences 258 120 modal occurrences 244 113 automatic identification 236 108 false positives 0 0 error rate 3.1 4.2 precision 100 100 recall 96.7 95.6 F-measure 98.3 97.7 Table 3: Results of modal verb detection Data from Table 3 show that the error rate in the identification of the modal occurrences is quite lo w: 3.1 for poder and 4.2 for dever . Precision receives the maximum value and Recall is above 95 for the two ve rbs. Errors are due to complex Portuguese sentences caus ing parsing problems, especially contexts where the sem i- auxiliary modal verbs and the main verb are distant in the sentence. Another difficulty of the parser is to de al with cases where the semi-auxiliary modal is followed by a pronominal clitic. These issues could be partially dealt with in an additional post-processing step and woul d possibly result in an improvement of our performanc e in the future. However, syntactic complexity will rema in a difficult challenge for semi-auxiliary detection. 5.2 Attribution of modal value To identify the modal value, we applied a machine learning approach to the sentences detected by the previous module. Our system takes into consideratio n all the features available from the PALAVRAS output: lemma and POS of the trigger, left and right syntac tic context, and semantic features: predicate argument structure, [\u00b1human] nature of arguments. We also computed scores for a baseline system that always a ssigns the most frequent modal value for each verb. The results for both experiments (using the sentenc es and a text linearized format of the parse tree with in a window around the verb) are presented in Table 4 (f or dever ), Table 5 (for poder ) and Table 6 (for conseguir ). We give results for a baseline and for both experim ents (sentences and window parse tree), computing Precis ion (P), Recall (R) and F-value (F) and the macro-avera ge over the different modal values. 98 dever baseline sentences window parse tree count P R F P R F P R F Total/macro-average 108 32.9 50.0 39.7 65.6 63.8 64.3 65.7 74.4 81.7 77.9 75.0 80.3 77.6 epistemic possibility 37 0 0 0 56.7 45.9 50.7 56.3 48.6 52.2 Table 4: Results of the automatic modal value attri bution for dever poder baseline sentences window parse tree count P R F P R F P R F total/macro-average 236 21.8 33.3 26.3 34.6 33.4 32.2 34.3 34.0 33.7 deontic permission 42 0 0 0 23.1 7.1 10.9 18.8 14.3 16.2 epistemic possibility 70.1 participant internal capacity 40 0 0 0 16.1 12.5 14.1 18.5 12.5 14.9 Table 5: Results of the automatic modal value attri bution for poder conseguir baseline sentences window parse tree count P R F P R F P R F total/macro-average 84 25.6 50.0 33.9 57.1 57.0 56.8 76.3 0,762 76.2 participant 0 57.1 48.8 52.6 76.9 73.2 75.0 success 43 51.2 100 67.7 57.1 65.1 60.9 75.6 79.1 77.3 Table 6: Results of the automatic modal value attri bution for conseguir The results in Tables 4-6 show that our system was able to improve the baseline for all three verbs: f or dever it improves the baseline from 39.7 to 64.7 macro-av erage F-value, for poder from for conseguir from 33.9 to 76.2. The higher values attained for conseguir are tied to the fact that its two modal values have similar frequencies in our gold dataset, makin g it easier to improve the baseline. With these experiments we obtained macro-average F-values between 33.7 and 76.2. We obtain better performance measures for conseguir and dever than for poder , possibly because poder has three modal values. Obviously, the automatic tagger obtains the best re sults for the most frequent values. Comparing the experiments using the sentences and the window parse tree, the results show no signific ant differences, although the window parse tree experim ent generally presents higher results, especially with conseguir (F-value 76.2 vs. 56.5). 6. Conclusion We have presented a system for the automatic taggin g of modality in Portuguese, using a manually annotated corpus as training data. The identification of the modal instances of the three auxiliary verbs receives hig h recall and precision values and could be further improved at the parsing level. The results of the attribution of th e modal value reach macro-average F-measures between 33 and 76 % F-value depending on the modal verb and on the mo dal value. The results are promising, considering that we trained our system on a tiny data set, and suggest that our aim: creating a larger corpus with modal informatio n by a (semi) automatic tagging process based on a small s ample seems to be a feasible next step. In future work we plan to provide a detailed study identifying the individual role of the syntactic an d semantic features that play a role in the automatic attribution of the modal value in our system. Anoth er goal is to apply the modality tagger to a larger set of verbs to see whether we can keep a reasonable performance fo r a more diverse set of verbal triggers. We also aim to compute a learning curve to estimate the amount of manually annotated examples that are needed to get a good performance from the modality tagger. As we are currently applying a 'word expert' approach and training separate classifiers for diff erent verbal triggers, it is clear that this approach wil l not be able to handle modal triggers that it has not seen before. As a next step we will study this problem and for e xample try to train a general modal trigger classifier tha t is not dependent on the verb itself. Acknowledgements This work was partially supported by national funds 99through FCT - Funda\u00e7\u00e3o para a project Pest-OE/EEI/LA0021/2013 and project PEst-OE/LIN/UI0214/2013. We would like to thank the anonymous reviewers for their helpful comments and suggestions. References Luciana \u00c1vila and Heliana Melo (2013) Challenges in modality annotation in a Brazilian Portuguese Spontaneous Speech Corpus, Proceedings of IWCS 2013 WAMM Workshop on the Annotation of Modal Meaning in Natural Language , March 19-20, Postdam, Kathrin Baker, Michael Bloodgood, Bonnie Dorr, Nathaniel W. Filardo, Lori Levin, and Christine Pia tko. 2010. A modality lexicon and its use in automatic tagging. In LREC'10 parser analysis. In Proceedings of WAMM-IWCS2013 , Potsdam, Germany. Eckhard Bick. 1999. The parsing system PALA VRAS. Aarhus University Press. Mona Diab, Owen Rambow, Vinodkumar Prabhakaran, and Weiwei Guo. 2009. Committed belief annotation and tagging. In Proceedings of the Third Linguistic Annotation Workshop , Suntec, Singapore, task: Learning to detect hedges and their sc ope in natural language text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning , ACL, 1-12. Caroline Gasperin, Renata Vieira, Rodrigo Goulart, and Paulo Quaresma. 2003. Extracting XML syntactic chunks from portuguese corpora. In TALN'2003 - Workshop on Natural Language Processing of Minority Languages and Small Languages of the Conference on \"Traitement Automatique des Langues Naturelles\" , Iris Hendrickx, and Am\u00e1lia Mendes. 2012. Introducing the Reference Corpus of Contemporary Portuguese On-Line\". In Proceedings of the Eighth International Conference on Language Resources and Evaluation - LREC 2012, Istanbul, May 21-27, 2012, 2237-2244. Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 200 Data Mining Software: An Update; a Proposal for Corpus Annotation. In Proceedings of the Eighth International Conference on Language Resources and Evaluation - LREC 2012, Istanbul, May 21-27, 2012, 1805-1812. Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nell o Cristianini, and Christopher J. C. H. Watkins. 2002 . Text Classification using String Kernels. Journal of Annotating event mentions in text with modality, fo cus, and source information. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC'10) , Valletta, Malta. ELRA. Marjorie McShane, Sergei Thomas O'Hara. 2005. Semantically rich human-aided machine annotation. In Proceedings of the Workshop on Frontiers in Corpus Annotations II : Pie in the Sky. Interaction between Modality and Focus: the case of exclusive particles. In Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discour se (LAW VII) . Association for Computational Linguistics, Sofia, Bulgaria, August 8-9 2013, 228-237. Makoto Miwa, Paul Thompson, John McNaught, Douglas B Kell and Sophia Ananiadou. 2012. Extracting semantically and Michael annotation of linguistic data with MMAX2. In Corpus Technology and Language Pedagogy: New Resources, New Tools, New Methods , 197-214. modality: a data-driven hierarchical model. In Harr y Bunt (ed.) Proceedings of the 9th Joint ISO - ACL SIGSEM Workshop on McShane. 2008. Annotating modality. Technical report, University o f Maryland, Baltimore County, March 19, 2008. Frank R. Palmer. 1986. Mood and Modality . Cambridge textbooks in linguistics. Cambridge University Pres s. Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen Rambow, and Benjamin Durme. from rule-based annota tions and crowdsourcing. In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM '12) Rehbein. 2012. Yes n!? Annotating the senses of English modal verbs. In Proceedings of the 8th International Conference on 100Language Resources and Evaluation (LREC) , May 24-26, 2012, and James Pustejovsky. 2006. Annotating and recognizing event modality in text. In Proceedings of the 19th International FLAIRS Conference . Johan Van der Auwera and Vladimir do Algarve, Faro, Portugal rcorreia@cs.cmu.edu, Nuno.Mamede@inesc-id.pt, jbaptis@ualg.pt, max@cs.cmu.edu Abstract This paper addresses issues relating to the denition and non-expert understanding of metadiscursive acts. We present existing theory on spoken metadiscourse, focusing on one taxonomy that denes metadiscursive concepts in a functional manner, rather than formally. A crowdsourcing annotation task is set up with two main goals: (a) build a corpus of metadiscourse, and (b) assess the understanding of metadiscursive concepts by non-experts. This initial annotation effort focus on ve categories of metadiscourse: I NTRODUCING The crowdsourcing task is described in detail, including instructions and quality insurance mechanisms. We report results in terms of time-on-task, self-reported condence, requests for additional context, quantity of occurrences and inter-annotator agreement. Results show the crowd is capable of annotating metadiscourse and give insights on the complexity of the different concepts in the taxonomy. Keywords: Metadiscourse, Crowdsourcing, Non-experts 1. Introduction Metadiscourse is one of the basic functions of language. Commonly referred to as discourse about discourse , it is composed of rhetorical acts and patterns used to make the discourse structure explicit, acting as a way to guide the audience. Crismore et al. (1993) dene metadiscourse as \"linguistic material in texts, written or spoken, which does not add anything to the propositional content but that is intended to help the listener or reader organize, interpret and evaluate the information given\". Some examples of metadiscursive acts include introductions (\"I'm going to talk about...\"; \"In this paper we present ...\"), conclusions (\"In sum,...\"), or emphasis (\"The take home message ...\"; \"Please note that ...\"). This study focuses on the function of metadiscourse in spo- ken communication. The functional analysis of such phe- nomena in discourse can contribute to tasks such as simpli- cation or language understanding, and can be used for lan- guage learning purposes, such as presentation skill instruc- tion. We describe the task of building a corpus of metadis- cursive acts using crowdsourcing to annotate transcripts of presentations. By using non-experts, we expect not only to obtain the annotations of some metadiscursive acts, but also to get feedback on how those acts are perceived. In this paper we start with background on metadiscursive theory, addressing how existing taxonomies and resources represent it (Section 2). Section 3 focuses on the choice of the material that the crowd will annotate with metadiscur- sive acts. Section 4 describes a preliminary annotation task aimed at testing the presence of some of the acts taken from our adopted metadiscourse taxonomy. Section 5 focuses on the setup of the crowdsourcing task, considerations regard- ing instructions, and quality control. The results obtained using the crowd and an ensuing discussion are presented in Sections 6 and 7. In Section 8, we conclude and present future directions.2. Background In the literature on discourse analysis we nd studies that address function in discourse. For example, the contribu- tion of Miltsakaki et al. (2008) to the Penn Discourse Tree- bank (PDTB) (Marcus et al., 1993) organized discourse connectives according to their function, considering cate- gories such as giving examples (I NSTANTIATION ), making clarications (R ESTATEMENT ONTRAST ), or showing cause ( REASON ). An- other example is the RST Discourse Treebank (Marcu, 2000), a semantics-free theoretical framework of discourse relations based on Rhetorical Structure Theory (Mann and Thompson, 1988), which includes categories such as E X- AMPLE , DEFINITION , or S UMMARY . Even though these projects explore function in discourse, they focus on written language and do not address the meta aspect of language. The lack of work on the explicit nature of discourse moti- vated our decision to build a corpus targeting the function of metadiscourse in spoken communication. To accomplish that, we looked for denitions of metadiscourse. Luuka (1992) developed a taxonomy for use in both written and spoken academic discourse. This taxonomy is com- posed of three main categories: TEXTUAL (strategies re- lated to the structuring of discourse), INTERPERSONAL (re- lated to the interaction with the different stakeholders in- volved in the communication) and CONTEXTUAL (covering references of audiovisual materials). Mauranen (2001), on the other hand, focused only on spoken discourse. This author's taxonomy is also composed of three categories with no further division: MONOLOGIC (similar to TEX- TUAL in Lukka's to INTER - PERSONAL in Lukka's taxonomy) and INTERACTIVE (re- lated to question answering and other interactions with the speaker). Luuka's and Mauranen's taxonomies organize metadis- course in similar ways. However, both studies focus on102METALINGUISTIC COMMENTS Repairing Reformulating Commenting on Linguistic Form/Meaning Clarifying Manage Terminology DISCOURSE ORGANIZATION Managing Topic Introducing Topic Delimiting Topic Adding to Topic Concluding Topic Marking Asides Enumerating Managing Phorics Endophoric Marking Previewing Reviewing Contextualizing SPEECH ACT LABELS Arguing Exemplifying Other REFERENCES TO THE AUDIENCE Managing Comprehension Managing Discipline Anticipating Response Managing the Message Imagining Scenarios Figure 1: \u00a8Adel's taxonomy of metadiscourse. theform of metadiscourse (i.e. number in- volved), not addressing its function . Afunctional approach to metadiscourse can be found in the work of \u00a8Adel (2010) who unies existing taxonomies un- der a framework that encompasses both spoken and written discourse. This framework was built using two academic- related corpora: MICUSP (R \u00a8omer and Swales, 2009) - comprised of academic papers - and MICASE (Simpson et al., 2002) - a corpus of university lectures. The categories and organization of \u00a8Adel's taxonomy of metadiscourse (Figure 1) reflect the author's concern about the unication of theories for both written and spoken dis- course and the desire to describe metadiscourse in a func- tional manner. For these reasons, we have decided to adopt this taxonomy as a source of categories of metadiscourse. This taxonomy will be discussed further in Section 4.3. Corpora Having adopted a set of metadiscursive acts to annotate, we then needed to select a source of data where these strategies could be found. Two main sources of data were considered: classroom recordings and TED talks1. Analysis of the contents of these two sources led us to choose TED talks over classroom recordings. TED talks are consistently good quality presentations from good pre- senters. Each talk is carefully rehearsed beforehand, con- veying one message in a short span of time (from 5 to 20 minutes). This contrasts with classroom recordings which are typically longer and where there is an order in which the classes should be listened too. Even if only self-contained classes are considered, they are targeted at a very specic audience and the topics are advanced and require a signif- icant amount of previous knowledge. Secondly, TED talks are uniform in content. They contain high-quality audio and video material and are available in several languages. They are also updated daily and subtitled, providing a good source of transcribed material. Classroom recordings, on the other hand, are a more heterogeneous resource as far as source and recording conditions are concerned, making them harder to be automatically processed with the least amount of human intervention possible. Even though they are not further addressed in this paper, classroom record- ings would be a good resource set to extend our TED nd- ings at a later time. At the time of the preparation of this annotation task there were 730 TED talks available in English with subtitles, synced at sentence level (a total of 180 hours, approxi- mately). 4. Preliminary Annotation Task A small preliminary annotation task was carried out to test the suitability of the combination of \u00a8Adel's taxonomy and the TED talks. The goal of this annotation was to nd which metadiscursive categories are present in the TED talks. Ten TED talks were annotated with the tags from the chosen taxonomy (see Figure 1). The ten talks were randomly chosen, spanning a variety of topics and years. This annotation task was performed by the rst author. The following paragraphs, each named after the 4 main cat- egories of the taxonomy, present the taxonomy itself and, at the same time, describe how each type of metadiscourse is distributed over the sample. Metalinguistic Comments are composed of 5 metadis- cursive acts: REPAIRING ,REFORMULATING ,COMMENT - ING ON LINGUISTIC FORM /MEANING ,CLARIFYING and MANAGING TERMINOLOGY . Most of these categories are exclusive to spoken discourse. From this set, only CLAR - IFYING and MANAGING TERMINOLOGY (dening of con- cepts) were found consistently in the sample. We believe that the fact that the other tags were not found is due to the high degree of preparation of each talk (when compared to academic lectures). 1http://www.ted.com/103Contextualizing Anticipating Response Arguing Managing Terminology Previewing Commenting on Form Emphasize Imagining Scenarios Reviewing Introducing Topic Clarifying Marking Asides Concluding Topic Exemplifying 0 5 10 15 20 25Figure 2: Occurrences of the most frequent tags. Discourse Organization is divided in two other cate- gories: Manage Topic and Manage Phorics . In Man- Topic , there are 5 metadiscursive acts: I NTRODUCING TOPIC ,DELIMITING TOPIC ,ADDING TO CON- CLUDING TOPIC and M ARKING ASIDES . These structures were found consistently throughout the sample. The au- dience comes from a broad set of areas, and the speakers must wisely structure their discourse to convey their mes- sage. Additionally, the short time frame that is allotted for each talk demands an efcient use of language. The excep- tions in this group were the tags DELIMITING TOPIC and ADDING TO TOPIC . The reason behind this may be the fact that TED talks have well-dened topics. The speakers tend to focus on what they want to talk about, going straight to the relevant points. Manage Phorics , the other subcategory under Discourse Organization , has four tags. P REVIEW - ING, R EVIEWING and C ONTEXTUALIZING are related to pointing to other locations in the current discourse. E N- DOPHORIC MARKING contains references to physical ele- ments (such as an image in the presentation), and was not considered in this preliminary task since it involved the in- tegration of elements outside the discourse. The rst three categories were well-represented in our sample. Speech Acts contains 3 metadiscursive acts: ARGUING , EXEMPLIFYING , and OTHER (where the author included acts that were not frequent enough to generate a new tag). The rst two tags were found frequently in the sample, and the category other was ignored since it did not represent a single concept. References to the Audience is related to contact with the audience. Unlike in academic lectures, in TED talks the speaker typically does not interact with the audience. The message has to be conveyed without direct interaction, such as questions and checks for understanding. For these rea- sons, the tags MANAGE COMPREHENSION (check if the au- dience is in synch with the content of the presentation) and MANAGE DISCIPLINE (adjusting the channel asking for less noise, for example), were not found and therefore were not considered. The remaining 3 tags in this category ( AN- TICIPATING RESPONSE ,MANAGING THE MESSAGE and IMAGINING SCENARIOS ), on the other hand, were found frequently throughout the sample.Figure 2 shows the distribution of the most frequent tags found in the ten talk sample. From the resulting fourteen categories, a small subset was chosen for the initial annota- tion effort in which we tested the suitability of using non- experts to identify occurrences of metadiscourse. Three cri- teria dictated the set of tags used in this annotation task. We considered (a) the most frequent concepts in the literature on presentation skills, (b) the concepts that could be best explained to non-experts, and (c) the input from Carnegie Mellon's International Communications Center (entity that holds presentation skills workshops and is responsible for administering tests for non-native speakers applying for teaching assistant positions). The resulting set of ve tags are: I NTRODUCING TOPIC , CONCLUDING TOPIC , MARK- EXEMPLIFYING and MANAGING THE MES - SAGE . Additionally, under the category E XEMPLIFYING we decided to collapse both E XEMPLIFYING and IMAGIN - ING SCENARIOS (since they both consist of illustrating an idea). For simplication, MANAGING THE MESSAGE (in \u00a8Adel's work, \"typically used to emphasize the core mes- sage in what is being conveyed\") will be referred to as E M- PHASIZING . 5. Crowdsourcing It has been shown that the quality of the crowdsourcing re- sults can approach that of an expert labeler, while requir- ing less monetary- and R\u00a8uger, 2010; Zaidan and Callison-Burch, 2011; Eskenazi et al., 2013). However, this advantage comes at a cost. Unlike experts, using the crowd requires setting up training and quality assurance mechanisms to eliminate noise in the an- swers. Additionally, it is necessary to approach problems in a different way, such as dividing complex jobs in subtasks to reduce cognitive load (Le et al., 2010; Eskenazi et al., 2013). In our case, the reasons behind using crowdsourcing go be- yond time and money. It allows the assessment of the crowd understanding. By designing a task requiring the annota- tion of metadiscourse, we are building a corpus of the phe- nomenon and understanding how non-experts comprehend metadiscursive concepts. In the remainder of this section, we will describe the setup of a crowdsourcing annotation task (run on Amazon Me- chanical Turk2). The rst decision concerned the amount of text that workers would annotate in each HIT (Human Intelligence Task - the smallest unit of work someone has to complete in order to be paid). Each HIT had to be simple and to allow workers to do it in the fastest way possible. However, metadiscur- sive phenomena are not local, requiring understanding the context. With this in mind, we decided to use segments of approximately 300 words. This limit was influenced by the design of the interface of the annotation task, taking into consideration that all the text should be visible on the screen without having to scroll down (scrolling increases time-on- task, influencing the answer rate). To make it monetarily worthwhile for a worker to chose our task, we included four segments per HIT, shown in a 2 by 2 matrix. Figure 3 shows 2https://www.mturk.com/mturk/welcome104Figure 3: One of the four segments in a HIT the interface for one of the segments in a HIT. This con- guration generated 2,461 HITs (or 9,844 segments) per category. It is also important to notice the presence of the button See more context in Figure 3. This feature allowed the workers to see the surrounding text of the segment in the talk (before and after), in case they needed additional context to support their decision. The second consideration concerns the design of the in- structions. Knowing that a metadiscursive act is a complex notion which workers may have never heard of, we decided that each HIT would target only one category, instead of re- quiring the identication of all ve categories in each seg- ment in one single passage. This decision lessens the cog- nitive load for the workers at each point. The instructions, for the emphasis task read as follows: When making a presentation, to guide the audi- ence, we often use strategies that make the struc- ture of our talk explicit. Some strategies are used to announce the topic of the talk (\"I'm going to talk about...\"; \"The topic today will be ...\"), to conclude a topic or the talk (\"In sum, ...\"; \"To conclude,...\"), to emphasize (\"The take home message is...\"; \"Please note that ...\"), etc. We believe that by explaining and explicitly teaching each of these strategies we can help students im- prove their presentation skills. In this task, we ask you to focus on the strategies that the speaker uses to EMPHASIZE A POINT. Your job is to identify the words that the speaker uses to give special importance to a given point, to make it stand out, such as \"more important\", \"especially\", or \"I want to stress that ...\". The passages you mark will be used on a presentationskills virtual tutor, showing students how profes- sional speakers EMPHASIZE a point. Since the idea is to do one pass over all the segments for each one of the tags, we designed different sets of instruc- tions for each one of the ve metadiscourse categories. It is important to notice that the rst paragraph of the instruc- tions above was only included after some preliminary trials. Its inclusion was intended to reveal a concrete example of the applicability of our work, as a way of motivating work- ers. The inclusion of this paragraph increased the response rate. After the instructions there is a section with examples and counterexamples derived from the preliminary anno- tation task. Finally, at the bottom of the page, before the presentation segments, there is a succinct set of steps that explain the interface and how to use it to annotate the pas- sages: STEP 1: For each of the extracts below, click on EV- ERY word that the speaker uses to EMPHASIZE A POINT. There may be zero, one ore more instances in each extract. STEP 2: The words you click on will display a light blue background. If you change your mind, you can click on the word again to deselect it. STEP 3: If you need more information to support your decision, you can click \" See more context \" be- low the segment to see the its surrounding context in the talk. STEP 4: If the speaker does not emphasize any point in the extract, select the \" No occurrences in this text \" checkbox below the text. STEP 5: Click the SUBMIT button once you are n- ished. The last set of considerations had to do with quality con- trol. We took advantage of the AMT prerequisites feature to lter out workers who were not native-speakers of En- glish and nd those who had a high reliability rate ( 95%). Workers who satised the prerequisites and accepted the HIT were then guided through a four-segment training ses- sion. The training tested if the worker read the instructions and examples carefully, and was capable of performing this task. Only upon successful completion of the four training segments were the workers allowed to access real HITs in the category they were just certied on. This training strategy is effective in ltering out bots, how- ever it does not prevent malicious workers from giving ran- dom answers to the real HITs. For that reason, and in line with what is done in much of the crowdsourcing commu- nity, we dened a gold standard for each of the ve metadis- cursive tags. In every four HITs, at least one segment was compared to an expert annotation. The gold standard seg- ments were very similar to the examples provided, and fail- ing one of them raised a flag for the worker. This infor- mation was then checked before accepting or rejecting that annotator's work. Workers also noted their condence level for each segment on a 5-point Likert scale (see Figure 3).105Category time (m)Condence Context Requests (%) ASD 10 3.60 5.52 INT 3.7 3.95 1.32 CONC 3.94 4.81 EMPH Results in terms of time-on-task, self-reported condence score and percentage of context expansion requests for M ARKING ASIDES (ASD) I NTRODUCING TOPIC (INT), C ONCLUDING XEMPLI - FYING (EXMPL) and E MPHASIZING (EMPH). A nal mechanism to assure quality consisted of submitting the same HIT to 3 different workers, using a majority vote scheme. Prior to publishing all the HITs in each category, we up- loaded a small sample of 100 HITs to test the suitability of the instructions and interface. This trial phase allowed us to modify the instructions and examples for each category if necessary, and to test if the workers were able to understand and identify the metadiscourse act. 6. Results This section presents the results of the annotation for each of the ve metadiscursive acts. In Table 1 we report the results in terms of average time-on-task in minutes; self- reported condence score on a 5-point Likert scale; and percentage of segments in which workers expanded con- text (by clicking on the See more context button). Table 2 indicates the number of occurrences of the metadiscourse tag; and inter-annotator agreement ( ). We used the Fleiss' kappa (Fleiss, 1971) as a measure of annotator agreement. Complete agreement corresponds to = 1, and no agree- ment (other than chance) corresponds to 0. Herein, annotators agree if the intersection of the words selected by each of them is not empty. For example, two workers agree when one selects \"Today, I would like to say that\" and the other misses some of the words, selecting \"I would like to say\". It is important to notice the absence of the tag M ARKING ASIDES in Table 2. All the categories with the exception of MARKING ASIDES produced satisfying results in the trial sample of 100 HITs uploaded prior to submitting the entire set of talks. This fact lead us to discard the asides-related category. This will be discussed in detail in Section 6.1. 6.1. Marking Asides As mentioned, the annotation of M ARKING ASIDES was discontinued due to the inconclusive results obtained dur- ing the AMT trial phase. The rst indicator of unsuccess- ful annotations was the slow response rate. The 100 HITs were up for one week during which less than 50% were completed. In the remaining categories, the sample was fully completed in less than two days. This slow response rate could be due to the small amount of HITs that were uploaded (workers tend to focus on tasks that have a signif- icant amount of HITs online, in order to minimize trainingCategory # occurrences INT 1,159 0.64 CONC 628 0.72 Number of occurrences and inter-annotator agree- ment (Fleiss' kappa) for the completed categories: I TRODUCING TOPIC (INT), C ONCLUDING TOPIC (CONC), EXEMPLIFYING (EXMPL) and (EMPH). time and maximize payment). However, the four other cat- egories were also rst presented with 100 HITs and com- pleted much faster. We looked for other indicators and decided that the crowd could give us some insight on the understanding of the con- cept M ARKING ASIDES . Workers were spending 10 min- utes on average for each HIT, contrasting with the 4 to 6 minutes the other tasks took. Self-reported condence scores were also the lowest of the ve categories: 3.60, as opposed to 4.00 for the category C ONCLUDING TOPIC . Fi- nally, the workers wrote comments that clearly showed the task was hard, justifying the slow response rate and lack of condence. Workers wrote: \"I am nervous that I am not doing these correctly *at all*\" ;\"I hope that this is what you are looking for\" ; and \"a little difcult\" . 6.2. Introducing a Topic The task of annotating introductions resulted in an inter- annotator agreement of 0.64. Workers took on average 3.7 minutes to complete each HIT and identied over 1,000 in- stances of I NTRODUCING TOPIC in our set of talks. It is important to note that speakers sometimes introduce sev- eral topics throughout a single talk, and therefore there can be more occurrences of I NTRODUCING TOPIC than the to- tal number of talks in the set (in this case 730). A nal interesting point was the low number of times that workers asked for more context: only in 1.32% of the segments. 6.3. Concluding a Topic The annotation of conclusions provided results that resem- bled the previous category: a slightly lower inter-annotator agreement (= 0.60), and similar average time-on-task and self-reported condence. An important difference comes from the percentage of segments for which annota- tors asked to see the surrounding context: 37% of the seg- ments. This might be an indication that conclusions are less local, needing a wider context to be identied. It is also important to notice that the number of occurrences of conclusions (628) is lower than the number of talks. This aligns with what we encountered in the preliminary anno- tation task (7 conclusions over 10 talks) and is related to the fact that the speakers do not always explicitly conclude (particularly true for shorter talks). 6.4. Exemplifying In this category, workers spent on average two more min- utes per HIT than while annotating instances of I NTRO - DUCING TOPIC and C ONCLUDING TOPIC . This results from the greater quantity of occurrences detected (1,327).106The more occurrences a category has, the more time work- ers will spend clicking on them. As previously described, this category collapses two metadiscursive acts as dened in\u00a8Adel's taxonomy: E XEMPLIFYING and IMAGINING SCENARIOS . Despite the collapse of tags, in this cate- gory annotators reached the highest agreement ( = 0.72), which corroborates our decision to combine the two tags. 6.5. Emphasizing While annotating occurrences of E MPHASIZING , the rela- tionship between average time-on-task and number of in- stances was similar to the one found for the previous cat- egory. Workers spent on average 6.3 minutes per HIT and identied over 2,500 occurrences. While identifying em- phasis, workers asked for the lowest amount of additional context amongst the ve categories (1.14). E MPHASIZING was also the category where workers achieved the lowest inter-annotator agreement (0.58). This result may be due to the fact that this category is the only one in which there is a scale of intensity related to the concept, i.e., different work- ers might have different thresholds for considering that the speaker is emphasizing. 7. Discussion The results obtained in this annotation task show that, once trained, non-experts can understand concepts of metadis- course and identify them on TED presentations. However, this is not true for all of the categories we proposed to anno- tate. The category M ARKING ASIDES was discarded dur- ing the trial phase on AMT since workers manifested signs of not understanding the task. After the experiment took place, we looked into the instruc- tions for this category to understand why workers were not able to annotate it. One of the counterexamples stressed the difference between M ARKING ASIDES (where the speaker digresses to a topic sidetrack, such as in \"Just a little side note here...\") and A DDING TO TOPIC (where the speaker explicitly adds to the current topic, such as in \"Let me add that...\"). This distinction may have added to the worker's cognitive load. They were not only asked to be aware of an- other category in the taxonomy, but also required to focus on a subtle difference. The solution to this problem may be the division of the category in two. This can be done with a rst pass collapsing both concepts under a more general no- tion, such as adding information , and a second pass where workers now only see instances that were detected in the rst pass and decide if the addition of information is on or off-topic. Another interesting result from this experiment is the need for additional context in different metadiscursive acts. Workers were able to identify occurrences of I NTRODUC - INGTOPIC and E MPHASIZING in a window of 300 words without requesting for additional context. On the other hand, identifying conclusions was the task where more con- text was needed. The fact that workers expanded context in 37% of the segments might result from the necessity to rst understand which topic is being presented, before deciding on the occurrence of its conclusion.8. Conclusion and Future Work In this paper, we have described an annotation task that took place on Amazon Mechanical Turk, where workers focused on a predened set of metadiscourse categories to anno- tate text extracted from TED talks. We started from a set of 730 presentations and a taxonomy of metadiscourse and described the considerations for setting up a crowdsourcing annotation task aimed at nding metadiscursive concepts in the talks. The task was successful for four of the ve categories that were submitted. In future work, we plan to continue this annotation effort, extending it to the remaining categories of \u00a8Adel's taxon- omy, and rening unsuccessful attempts (i.e. M ARKING ASIDES ) to meet the workers' cognitive load. We plan to extend this analysis to other languages, more precisely to European Portuguese, comparing the use of metadiscourse between the two languages. Finally, we aim at using the re- sulting annotation as training data for an automatic metadis- course classier. Acknowledgments This work was supported by national funds through FCT Fundac \u00b8 ao para a Ci encia e project PEst-OE/EEI/LA0021/2013 by FCT project CMU- PT/HuMach/0053/2008. 9. References \u00a8Adel, Annelie. (2010). Just to give you kind of a map of where we are going: A Taxonomy of Metadiscourse in Spoken and Written Academic English. Nordic Journal of English Studies , 9(2):69-97. Crismore, , 10(1):39. Eskenazi, Maxine, Levow, Gina-Anne, Meng, Helen, and Parent, (2013). Crowdsourcing for Speech Pro- cessing . John Wiley & Sons. Fleiss, Joseph L. (1971). Measuring nominal scale agreement among many raters. Psychological Bulletin , 76(5):378. Le, John, Edmonds, Andy, and (2010). Ensuring quality in crowdsourced search relevance evaluation: The effects of training question distribution. In SIGIR 2010 Workshop on Crowdsourc- ing for Search Evaluation , pages 21-26. Luukka, Minna-Riitta. (1992). Metadiscourse in academic texts. In Conference on Discourse and the Professions. Uppsala, Sweden , volume 28. Mann, William C and Thompson, Sandra A. (1988). Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243-281. Marcu, Daniel. (2000). The theory and practice of dis- course parsing and summarization . The MIT press. Marcus, Mitchell P, Marcinkiewicz, Mary Ann, and San- torini, Beatrice. (1993). Building a large annotated from MICASE. In Corpus linguistics in North107America: 1999 symposium , pages 165-178. Miltsakaki, Eleni, Robaldo, Livio, Lee, Alan, annotation in the Penn Dis- course Treebank. In Computational Linguistics and In- telligent Text Processing , pages 275-286. Springer. Nowak, Stefanie and R \u00a8uger, Stefan. (2010). How reliable are annotations via crowdsourcing: a study about inter- annotator agreement for multi-label image annotation. In Proceedings of the International Conference on Multi- media Information Retrieval , pages 557-566. ACM. R\u00a8omer, Ute and Swales, John M. (2009). The Michigan Corpus of Upper-level Student Papers (MICUSP). Jour- nal of English for Academic Purposes , April. Simpson, Rita C., Briggs, Sarah L., Ovens, Janine, and Swales, John M. (2002). The Michigan Corpus of Aca- demic Spoken English. Zaidan, Omar F. and Callison-Burch, Chris. (2011). Crowdsourcing translation: Professional quality from non-professionals. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguis- tics: Human Language "}