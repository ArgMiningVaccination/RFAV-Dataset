{"title": "Reddit", "author": null, "url": null, "hostname": null, "description": null, "sitename": null, "date": "2021-07-21", "cleaned_text": "Share your burning hot takes and unpopular opinions! I'm really not sure why people in America think that Western Medicine is the only true form of medicine and/or healing. We've been conditioned as a society to believe that it is the only way to receive proper care. It's been beaten into us so much that any person who has success in helping themselves with their health is shamed and ridiculed. It's a real shame that people are attacked for taking care of their health rather than congratulated or supported. The truth is here but people don't want to believe it because they've been lied to. The American people have such egotistical views about their own country that they fail to see any other way of life. Not every country practices the same forms of medicine. We're on the top of the list of countries for diagnosing health problems yet we're not even on the top 30 for curing them. How many other people here are awake to the truth? Sometimes it's feels Iike a really lonely road. EDIT: I'm not interested in debating over which systems of medicine/healing \"work\" and \"don't work\". If that is what you comment, you have missed the point of my post and I'm not going to answer. Thank you to all of you who posted respectful comments. Those of you who have shamed and ridiculed me have validated my opinion further for me. It's a win win. Peace and Love. "}