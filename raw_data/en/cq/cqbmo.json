{"title": "PDF", "author": "PDF", "url": "http://eprints.lse.ac.uk/110866/2/rapid_assessment_on_online_misinformation_and_media_literacy_puvblished.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "Rapid Evidence Assessment on Online Misinformation and Media Literacy Final report Professo r Lee Edwards Dr Mariya Stoilova Dr Nick Anstead Andra Fry Gail El- Halaby Matthew Smith Final Report for Ofcom 9 June 2021 How to cite this report: Edwards, L. Stoilova, M., Anstead, N., Fry, A., El -Halaby, G. and Smith M. (2021) Rapid Evidence Assessment on Online Misinformation and Media Literacy : Final Report for Ofcom . Available at: www.ofcom.org.uk . This report was c ommissioned by Ofcom, although the views expressed in the report are those of the authors and do not necessarily reflect Ofcom's views. 2 Table of Contents EXECUTIVE SUMMARY ........................................................................................................................................................... 4 BACKGROUND .......................................................................................................................................................................... 7 METHODOLOGY ....................................................................................................................................................................... 9 AIMS................................................................................................................................................................................................................... 9 METHODS .......................................................................................................................................................................................................... 9 PHASE 1: SCOPING ............................................................................................................................................................... 11 FINDINGS FROM THE GREY LITERATURE ................................................................................................................................................... 11 School Curriculum Development ........................................................................................................................................................ 11 Plat form interventions ........................................................................................................................................................................... 12 The need for robust evaluation ........................................................................................................................................................... 13 FINDINGS FROM THE EXPERT INTERVIEWS .............................................................................................................................................. 14 RESULTS OF THE RAPID EVIDENCE ASSESSMENT ..................................................................................................... 16 KEY TERMS , MISINFORMATION TOPICS AND THEORETICAL UNDERPINNINGS .................................................................................... 16 Literacy ......................................................................................................................................................................................................... 16 Misinformation .......................................................................................................................................................................................... 17 The digital context ................................................................................................................................................................................... 17 Misinformation topics ............................................................................................................................................................................. 17 Theoretical underpinnings ................................................................................................................................................................... 18 MEDIA LI TERACY PRACTICES ...................................................................................................................................................................... 19 MEDIA LITERACY INTERVENTIONS ADDRESSING MISINFORMATION .................................................................................................... 21 Positive effects of literacy ...................................................................................................................................................................... 21 Sear ch skills ................................................................................................................................................................................................. 22 Games and gamification ........................................................................................................................................................................ 22 System 1 v s System 2 thinking ............................................................................................................................................................. 23 Normative messaging ............................................................................................................................................................................. 23 Persistence of misinformation ............................................................................................................................................................. 24 Mediating factors ...................................................................................................................................................................................... 24 TECHNICAL INTERV ENTIONS ADDRESSING MISINFORMATION .............................................................................................................. 26 The effectiveness of fact -checking, correction and flagging of misinformation ............................................................ 26 Refutations and ratings ......................................................................................................................................................................... 27 Knock -on effects and limitations ........................................................................................................................................................ 27 LIMITATIONS OF THE STUDIES .................................................................................................................................................................... 29 Methods ......................................................................................................................................................................................................... 29 Geography .................................................................................................................................................................................................... 29 Sampling demographics ........................................................................................................................................................................ 29 CONCLUDING DISCUSSION ................................................................................................................................................ 30 APPENDIX 1: DETAILED METHODOLOGY .................................................................................................................... 34 Grey literature search ............................................................................................................................................................................. 34 Expert interviews ...................................................................................................................................................................................... 34 Main search protocol .............................................................................................................................................................................. 35 Results ............................................................................................................................................................................................................ 40 APPENDIX 2: REA ARTICLE DATABASE ......................................................................................................................... 43 APPENDIX 3: MEDIA LITERACY PRACTICES: DETAILED REPORT ......................................................................... 44 Media literacy approaches and techniques ................................................................................................................................... 44 3 Challenges to media literacy ................................................................................................................................................................ 48 APPENDIX 4: STUDIES ON MEDIA LITERACY TECHNIQUES AND ON AUDIENCE RESPONSE ......................... 50 Media literacy practices (n=62) ......................................................................................................................................................... 50 Audience response (n=43) ..................................................................................................................................................................... 53 APPENDIX 5: EXPERT INTERVIEWS ............................................................................................................................... 56 BIBLIOGRAPHY ..................................................................................................................................................................... 57 4 Executive summary This report summarises the results of the Rapid Evidence Assessment (REA) on Online Misinformation and Media Literacy (REA) , conducted from November 2020 to April 2021 and commissioned by Ofcom. The review is focused on studies that measure the effectiveness of interventions designed to tackle misinformation, both within the media literacy curriculum and in relation to technological interventions that draw on literacy principles (such as critical thinking, information evaluation and active engagement), even if they are not conducted in an educational setting. The results show ed that robust evaluation of media literacy curriculum interventions is not very common. M ore evaluation has been done on the effectiveness of non -curricular intervent ions. Nonetheless, findings from both types of research provide important insights into how evidence -based, targeted approaches to dealing with misinformation by improving media literacy might be further developed, building on existing policy and industry initiatives and fostering audience empowerment and agency. Key findings Findings on interventions in existing research 1. Research shows that three specific types of media literacy skills - particularly critical thinking , which may involve asking questions where information comes from or using information to construct evidence based arguments; evaluation strategies, including a refle ctive approach to one's own status as an audience member ; and knowledge of the operation of news and media industries - have consistently been found to have positive effects on the ability to critically engage with misinformation . 2. Research consistently identifies that interventions based on system 2 thinking are more effective than those based on automatic, instinctive responses . System 2 thinking is defined as slow, critical-rational thinking ( as distinct from type 1 thinking, which is rapid and intu itive). 3. The limited research on games and gamification show that these tools may help improve digital media and information literacy , prompting more critical engagement with misinformation online. Online games can expose participants to different types of misinformation and guide them through the skills required to make informed judgements about information. 4. A number of studies consistently identified perceptions of source credibility (trustworthiness and believeability) and the ability to critically evaluat e the quality of sources, as important factor s that underpin effective media literacy skills and in fluence attitudes towards misinformation. Methodological limitations of existing research 1. Published work in this area is not very methodologically varied . There is a strong emphasis on experimental methods where the relationships between different variables are tested in controlled conditions. Studies that test experimental results in the field, under 'real- world' conditions, and longitudinal studies, carried out over an extended period in order to track changes over time and the longevity of effects, are both rare. 2. The majority of research defines the potential impact of media literacy interventions in terms of their effects on attitude, knowledge or understanding of misinformation. Analysis of actual beha viour change is less common. 5 3. There are a number of sampling limitations in the research: a. Facebook and Twitter are the primary sites for investigating misinformation on social media. The ways in which media literacy interventions might affect engagement w ith misinformation on other platforms have not been investigated in any depth. Given the fast -changing nature of the digital environment, this is an important gap in knowledge. b. Research emphasises the US context . While the authors of such work do not explicitly make the claim of universal applicability, much of this research lacks any real consideration of the specific socio -cultural , political and institutional context of the United States , or of the extent to which findings might be applicable in oth er contexts. c. While some research uses representative sample s, a large proportion uses non -representative sampling methods , gathered using volunteers from school or university student cohorts, or via services like MTurk1. This raises questions about the a pplicability of results to the wider population. d. Most studies are carried out with adult populations, with limited differentiation of responses within these populations. This means t here is a limited understanding of variability in the effectiveness of media literacy as a tool to tackle misinformation across different populations , including younger children and diverse ethnic groups . 4. There is a lack of interdisciplinarity across studies , so that mutually beneficial insights - for example, the value of different media literacy frameworks for engaging with mis information, or the ways in which changing modes of misinformation may require new forms of media literacy training - are being overlooked. Recommendations In light of these findings, we make the following general recommendations for researchers , media literacy practi tioners 2 and for collaboration between multiple parties . They reflect the need to adapt practice to take into account the current findings, and to deliver new research that can extend the findings . Research ers to broaden the ways in which media literacy is applied in research on strategies to counter misinformation, so that the full range of benefits from media literacy education can be identified. It should include elements of self -reflection, knowledge of the media industries and how they work, the social and cultural context for media, and critical analyses of representatio n. to improve the range of samples employed in studies , and particularly to include younger populations , more diverse populations, and a wider range of platforms . to work towards a unified framework for media literacy evaluation, so that the impact it has on capacities for dealing with misinformation can be more easily compared across contexts and a reliable body of comparable results can be built. 1 Owned by Amazon, this platform pays small sums of money to users for undertaking tasks, such as filling in surveys and participating in online experiments 2 'Practitioners' is defined here as the range of parties that provide media literacy initiatives and training, including educators, third sector organisations, news organisations, platform operators, and other industy actors. 6 Media Literacy Practitioners to explore how system 2 thinking, and particularly games and ga mification, might be consistently integrated and evaluated in technical interventions and media literacy education . to work towards overcoming the challenges posed by integrat ing evaluation into media literacy curricula and into technical innovations by pl atforms , in order to clearly identify the impact they have on audience knowledge, attitudes, understanding and behaviours dealing with misinformation. Collaboration between multiple parties to continue to facilitate regular and consistent dialogue between platform organisations and media literacy practitioners, so that media literacy curricula can keep up to date with the fast -changing digital environment. to explore how proprietary data may be made available for research , so that a wider range of methods and research questions can be deployed. A note on method The REA was conducted in two phases. Phase 1 involved scoping the main search by identifying what is already being done in media literacy communities of practice to address misinformation and identifying sectors that might have useful/transferable lessons for media literacy initiatives to deal with misinformation. It involved two methods: a search focused on finding non -academic literature and a series of expert i nterviews that helped us map practice s relating misinformation, media literacy and other fields that could provide insights for the REA. Based on the results of Phase 1, a search protocol was developed for the main search. Following agreement on the searc h protocol with Ofcom, the main search was conducted. The search was limited to English language publication s, although the evidence gathered covered a variety of countries. The search revealed a wide range of literature relevant to the topic, and 201 peer -reviewed studies were included in the final analysis. 96 of these addressed the core focus of the REA, the intersection of media literacy and misinformation , including the effectiveness of technical interventions to tackle misinformation. In addition, articles focused on media literacy practices were included in the analysis, because they provided important contextual information for the main findings and recommendations . 7 Background The scale of misinformation and the breadth of its effects , ranging from the current Covid -19 pandemic to electoral campaigning, climate change and migration, is cause for significant concern . In politics, polarisation has increased the tendency to use lies as part of a political spectacle, and false information is circulated by users less concerned by the quality of claims than by their function in ongoing political and ideological struggles (Allard-Huver, 2017; Giusti & Piras, 2020) . Misinformation about Covid -19 has had a significant effect on critical public h ealth behaviours, including reported willingness to be vaccinated, and adherence to public health guidance (Roozenbeek, Schneider, et al., 2020) , while systematic efforts to use misinformation to challenge the reality of climate change have, for many years, undermined calls for action to address the climate crisis to the extent that scientists themselves may adjust their claims to accommodate a 'climate of denial' (Lewandowsky, Oreskes, Risbey, New ell, & Smithson, 2015) . Given these wide -ranging effects, there is a n urgent need for more of what Guoping (2019) , in the context of STEM education, calls 'epistemic education', that can foster engagement in rational and critical thinking. Addressing the challenge of misinformation involves action from audiences, platforms and organisations of all kinds, but the slow pace of regulatory and educational change and the varied approaches - from regulation to co-regulation and self -regulation (Durach, Bargaoan u, & Nastasiu, 2020) - means that the effectiveness of many current interventions is unclear. In particular, efforts to educate audiences and change the ways they engage with information are fragmented across educational contexts, platform-specific initiat ives, media -related initiatives and fact -checking services. As a result , their impact on audience knowledge, attitudes and behaviour towards misinformation is unclear. Research on media literacy programmes and related curricula (for example, in English or PSHE syllabi) show s that the se are important vehicles for educating audiences on how to deal with misinformation. A recent review for the European Commission (McDougall, Zezulkova, van Driel, & Sternadel, 2018) shows that EU countries are starti ng to address these issue s, with media literacy initiatives specifically targeting misinformation already taking place in the UK, Belgium, Germany, Finland and France . Some journalistic organisations are also engaged in outreach to schools through programmes teaching media and news literacy ; Kaniaj and Lechpammer (2019) show that in the EU, many professional associations for journalism recognise their potential role in c ombating misinformation by educating people about how the media works and communicat ing the values of good journalism - although they do not always actively fulfil this role. In the UK, the 'NewsWise' programme is run by the National Literacy Trust in collaboration with Guardian Newspapers, while the BBC's Young Reporter scheme is a curriculum offering for schools to engage students in reporting and build their understanding of media organisations. Beyond the curriculum, many civil society organsations hav e developed media literacy interventions and initiatives to support children and young people in particular, as they learn to navigate the online world. Internet Matters, Parent Zone, Get Safe Online, and Childnet , among others, have all developed advice, tools and resources specifically focused on media literacy and tackling misinformation (ChildNet International, 2021; Get Safe Online, 2021; Internet Matters, 2021; ParentZone, 2021) . Platforms companies are also active in this area. For example, Google and YouTube have supported the twin programmes 'Be Internet Legends' and 'Be Internet Citizens', working with civil society organisations ParentZone and BeatFreaks to support media literacy in primary and secondary school age chi ldren (Phillips, Gatewood, & Parker, 2020) . McDougall et al (2021) provide 8 an international 'top ten' of English language media literacy resources for tackling mis information, as rated by students, teachers, librarians and journalists. Digital literacy is at the cutting edge of media literacy interventions , because misinformation is most widespread and has the greatest potential influence via online media. The complex online context means that tackling misinformation constitutes a significant challenge for educators . Not only do teachers themselves need the skills and knowledge to identify misinformation before they can be taught to students, but the 'pollution' of information online undermines the value of evidence itself as a n educational too l (McDougall et al., 2018) . Moreover, and as Mehta and Guzm\u00e1n (2018, p. 119) note, the multimodal techniques used in online news media mean that meaning can be manipulated using visuals rather than text, so that digital literacy needs to incorporate the ab ility to 'read' complex forms of communication and their 'symbol systems', both separately from and in combination with text. These difficulties notwithstanding, multiple authors have argued that media literacy is essential for addressing misinformation. For example, Rubin (2019) proposes a conceptual model to address the fake news epidemic, where interventions are targeted at the three causal factors of fake news - the pathogen (falsifications, clickbait and other forms of fake news); the host (audiences and their limitations when engaging with information); and the environment. Automation can address the pathogen and regulation the environment, but education is essential for reducing the influence of factors that make audiences susceptible to believing a nd spreading fake news. In a similar fashion, Eysenbach (2020) proposes a model for addressing the infodemic surrounding Covid - 19, where eHealth literacy and science literacy are essential for building audience capacity to accurately select and assess health information. Other methods for tacking misinformation include regulation and automated processes for detecting and deleting misinformation . Like media literacy, these measures also face challenges in practice. Regulation, for example, must balance limit ing the spread of misinformation with preserving freedom of speech; self -regulation faces the same challenge and also raises the question of where authority over the definition of 'truth' lies. Automated filters, flags, blocking and debunking techniques run the risk of false positives, backfire effects and perpetua ting human bias (Kertysova, 2018; Mortimer, 2017) . In this context, a s Kertysova (2018, p. 20) notes , 'increasing media and digital literacy may be one of the most efficient and powerful tools to restore a healthy relationship to information and increase t he resilience of our democracies to online disinformation.' 9 Methodology Aims The Rapid Evidence Assessment has the following aims: 1. To summarise the work already being done in the field of media literacy to address online misinformation ; 2. To identify which of these initiatives are impactful in helping the public recognise, assess and/or avoid misinformation ; 3. To identify differences in impact for different population segments ; 4. To identify learning from other fields where interventions to change public behaviour have been successful (e.g. health, climate change) ; 5. To consider how these learnings can be applied to the field of media literacy ; 6. To identify gaps in current knowledge that still need to be addressed. Methods The evidence assessment used three different methods3. 1. Scoping : we carried out scoping of grey literature4 in order to identify what is already being done in the field of media literacy to address misinformation, as well as to identify sectors that might have useful/transferable lessons for media literacy initiatives to deal with misinformation. 2. Expert interviews: w e interviewed 11 experts (Appendix 5) who work in a range of sectors and have experience of addressing misinformation through policy and practice interventions. The interviews identified current trends in misinformation production and circulation, initiatives being undertaken to address these, technological innovations, and specific case studies that could be investigated in more detail during the main literature search . 3. Main search : we carried out a rapid evidence review and assessment following the Preferred Reporting Items for Systematic Rev iew and Meta -Analysis Protocol (PRISMA -P) guidelines (Moher et al., 2015) . We searched 16 multidisciplinary and subject -specific databases, covering a range of subject areas, including media and communications, sociology, psychology, health, environment, politics, education, library science, and business. The search identified 1 ,767 unique results. A detailed screening process, drawing on agreed exclusion criteria, generated a final sample of 201 for full review. The sample covered the following themes: 1. Media literacy interventions addressing misinformatio n (n=35) 2. Technical i nterventions addressing misinformation (n=61) 3. Media literacy practices (n=62) 4. Audience responses to interventions (n=43) 3 The full methodology description can be seen in Appendix 1 4 The grey literature covered non-academic, non- peer-reviewed literature in the area of interest, e.g. industry, policy or think tank reports. 10 Groups 1 and 2 were analysed in detail using a framework developed especially for this review (see Appendi ces 1 and 2 ). The framework captures definitions and measurements of media literacy and misinformation, evidence on their relationship, information about interventions (description, findings, measures of effectiveness), and study details (location, methods, sample size and age). The studies in groups 3 and 4 are less relevant to the core focus of the review. However, articles in group 3 are summarised in this report (see also Appendix 3 ) because they provide useful context for the main findings . Articles in group 4 are not summarised, because they did not add any new insights to the main findings , but t he references are included in the bibliography . 11 Phase 1: Scoping Findings from the grey literature The majority of grey literature we reviewed originated from in the UK or USA, with a smaller number from Australia and elsewhere in Europe. Authorship was mainly institutional, including non-governmental organisations, research institutes, civil society organisations and think tanks. The reports focus on reviews of practice and research as well as policy advice . Content includes detailed, descripti ons of practice , quantitative overviews of existing media literacy and misinformation trends and practices, and areas for development offering a range of proposals for stakeholders to implement . Proposed i nterventions are heterogeneous, with reco mmendations including top-down (wide-scale and systematic action from institutions and technology companies ) and bottom-up (direct interventions with media users) approaches. Two main themes were noted: a focus on comprehensive interventions in school curriculum development, and an assessment of initiatives implemented by technology platforms. School Curriculum Development Reports addressing the media literacy curriculum begin with a recognition that media illiteracy in the current climate is both common and problematic. Reports emphasise the need for literacy interventions to combat the rising levels of misinformation in the evolving political and digital landscape. The surge i n misinformation surrounding the COVID -19 pandemic is framed as a catalyst for change, highlighting the need for wide -scale measures to combat the problem (see Brennen, Simon, Howard, & Nielsen, 2020; OECD, 2020 for reviews) . The complex and challenging nature of teaching media literacy is noted, particular ly in relation to differ ent levels of education, different types of misinformation and the information -saturated online environment, which has triggered a cultural 'infodemic' (Nielsen, Fletcher, Newman, Brennen, & Howard, 2020) . This is particular ly evident in reports that discuss literacy initiatives in global contexts. For example, a report by Fraillon, Ainley, Schulz, Friedman, and Gebhardt (2014) found significant variation in information literacy s kills across countries and socioeconomic backgrounds, and corresponding variations in understanding of the term 'media and information literacy'. In this context, the literature places a particular focus on children's media literacy. Children are consi stently reported to be particularly vulnerable to online harms and misinformation, and so increasing critical thinking skills in school -age children is regarded as an important solution to the problem. Two typical examples of proposed interventions in the report can be seen in the case studies in Box 1. 12 A large number of reports focus on children's and young adults' media literacy , both in terms of levels of literacy as well as comparisons of educational initiatives in different countries (e.g. Full Fact et al., 2020; McDougall et al., 2018; Nettlefold & Williams, 2019; Phillips, King, Boyer, & Augeri, 2019) . This might be expected since research has shown that this demographic is also more vulnerable to the effects of misinformation than other age groups (Bontcheva & Posetti, 2020) . Reports on adult -level initiatives are much less common, and tend to be embedded in general discussions of media literacy skills at the population level. One set of report s did consider media literacy skills for journalists, rather than the general population (Ireton & Posetti, 2018; Posetti, Simon, & Shabbir, 2 019) . Platform interventions Discussion of interventions made by platforms themselves tend to focus on user -centred methods addressing 'front -end friction'. Friction is defined as 'represent[ing] anything that slows down a process or function' (Digital, Culture, Media and Sport Committee, 2019, p. 86) . Applied to misinformation, friction is introduced through methods that delay sharing content. Such measures theoretically trigger a 'pause for thought' obstacle before sharing, to decrease the spread and spe ed of misinformation. The grey literature discusse s these kinds of interventions and their deployment by some platforms, particularly Twitter and Facebook, as a response to misinformation associated with the COVID-19 pandemic. Measures include warning us ers that the content they are sharing is similar to previously reported content, giving individuals the chance to revise their posting, or displaying cues such as 'Are you sure you're not sharing misinformation about COVID -19?', which may also provide an e xternal link to information sites (Simpson & Conner, 2020) . Corporations such as Apple, Facebook, Google, Twitter and others have all responded to the risk posed by COVID -19 misinformation in a variety of ways (Ofcom, 2021b) . However, measurements of efficacy and results from their interventions are not widely reported, with only Instagram publising results (Instagram, 2020) . Box 1: Case studies of children's media literacy interventions In 2018-2019 the Stanford History Education Group (Wineburg et al, 2019) enrolled 3,446 high school students in 14 USA states and found that high school students were unable to evaluate basic digital information. In response, and supported by Google, they produced a Civic Online Reasoning curriculum unit. A pilot study was implemented in a Midwestern school district and fo cused on providing students with the skills to evaluate online sources, encouraging lateral thinking and developing critical thinking when faced with ambiguity (e.g. who took this photo? Where did it originate?). 464 juniors and seniors across 6 high schools took a pre -intervention assessment and a post -test at the end of the semester, comprising two visual 'evaluating evidence tasks' using different stimuli. Students who initially showed acceptance of the image reported significant evidence of being more d iscerning, critical and engaged in critical questions regarding the nature of the image, following completion of the course. In the United Kingdom , the Guardian Foundation, National Literacy Trust, and PSHE Association delivered workshops to over 2, 400 c hildren across 42 schools in disadvantaged areas, encouraging 9- 11-year -old students to critically engage with news and media content (Full Fact, Africa Check, & Chequado, 2020) . In a simulat ed newsroom environment , children were taught to critically think about the types of information they trusted to share with wider audiences. The simulation increased their confidence to recognise misinformation and engage in fact checking, and the participants reported increased confidence in identifying misinformation, from 33% to 49% following their participation. 13 The need for robust evaluation Both the top-down and bottom-up approaches recommended in the grey literature ap pear consistent with the goal of empowering users with ways to increase their media literacy and engage with misinformation both critically and safely. However, evidence of effectiveness is limited, which means that the value of different strategies, and t heir (in)consistency across contexts, remains unclear. In the majority of discussions of media literacy interventions, success was evaluated in similar ways to the cases above, using self -reported measures but without a robust measure of effectiveness or external validity (e.g. testing the effectiveness of interventions against a non-exposed control group) . Case studies with robust measures of impact were scarce and measures also differed across cases. These diverse measures mean results from any particul ar intervention strategy are difficult to generalise. Moreover, given that self -reported and in- context evaluations could be subject to the 'experimenter effect' (a tendency for researchers to favourably influence participant responses), they have to be vi ewed with caution. The lack of common and robust evaluation frameworks is exacerbated by the variety of approaches to defining media literacy and its components. This has a number of consequences. First, it makes effectiveness difficult to measure becaus e there is no empirically agreed framework for measuring levels of media literacy. Second, it prevents replication, so that results from one study cannot be tested for their applicability or robustness in other contexts. Finally, it makes monitoring sustai nable behaviour change extremely difficult - a crucial concern if media literacy is to be a long-term solution for the challenges posed by misinformation (Polizzi & Taylor, 2019) . Some reports encourag e a collaborative approach to implementing literacy initiatives. For example, in a National Literacy Trust report, teachers and parents highlight the need for a partnership between media organisations and educational facilities (National Literacy Trust, 2018) . Such proposals are based on the recognition that substantial and societal change in media literacy skills can only occur when rolled out consistently and at scale. Overall, the grey literature focus es predominantly on summaries of practice, recommendations for future policy and research , and descriptions of possible interventions rather than empirically focussed cases. In order to resolve the misinformation infodemic, standardised, collaborative and pragmatic approaches to media literacy are recommended . While the reasons for the production and circulation of misinformation are relatively well understood, the literature suggests that evidence of exemplary and efficient interventions that can produce long-term behaviour change remains elusive. 14 Findings from the expert interviews We carried out 11 interviews with experts working in tech nology industry, advertising, social marketing, policy and media research/academia (for the list of intervewees, see Appendix 5) . The interviews lasted around an hour and covered : misinformation production and circulation, causes and consequences, audience perceptions and behaviour, techniques and initiatives used to tackle misinformation and/ or to educate and persuade audiences, successful initiatives, and lessons applicable to media literacy education. The misinformation topics and examples discussed covered a wide range of issues, from climate change to public health, radicalisation and extremism, and politics. Overall, the interviews confirmed that extensive knowledge exists about the technical mechanisms and human psychology underpinning the distribution and circulation of misinformation. They also confirmed a wide range of initiatives across different sectors, addressing the circulation of misinformation; the ways that audiences engage with (online) news and information; and the ways algorithms and AI treat misinformation and disinformation. However, almost all interviewees noted that considerable gaps remain in knowledge about the effective ness of initiatives , and in the data available for measuring effectiveness. Many interviewees commented on the complexity of audience behaviours and the difficulty of en couraging change when actions were dependent on multiple factors. Interviewees from the advertising and social marketing industries, for example, emphasised the importance of demographics, psychographics and individual motivations to information take -up an d use. To create effective messaging that would prompt change, communication needed to 'land' with audiences, appealing to them for emotional, not only rational, reasons, and prompting more in -depth cognitive processing activity. Targeting audiences could be effective, allowing for tailored messaging to high -risk groups, for example, and so audience segmentation is particularly important. A part of the population (e.g. conspiracy theorists) will always remain resistant to messages, while others are more open to change (and therefore a more valuable target) because they are ambivalent or even supportive towards a wider range of perspectives. However, targeting also raised the potential problem of communication becoming too transactional, focused on informatio n exchange rather than relationship -building, so that trust between an organisation and its audiences is lost. In relation to the issue of misinformation, this is reflected in the reality that levels of trust in different information sources may vary consi derably, and established actors may be some of the least trusted. Communicative style and tone were also seen to be crucial, including prioritising short, memorable messages; messages with 'talkability'; and using credible intermediaries, including online or offline communities that audiences belong to, as a means of additional influence. Nudging strategies can be powerful prompts for behaviour change ( for the academic evidence on nudging, see Thaler & Sunstein, 2008) . They have been used to address misi nformation, for example when information characteristics (e.g. false or unverified content) are flagged to audiences in order to prompt a specific response , such as higher levels of critical thinking or sceptism . However, our interviewees pointed out that to be effective , nudging needs to be context -driven. Simply giving people more information may not change behaviour; there need to be additional reasons for making the required change. For example, s ocial media companies use nudging by highlighting information quality, type, sharing statistics, and other features. However, because these 15 strategies are universal rather than tailored to specific contexts for audiences , they constitute little more than additional knowledge and their effectiveness is not guaranteed. Improving critical thinking is also a key objective in media literacy curricula, but some interviewees pointed out that media literacy is underpinned by assumptions that moving participants to a 'higher quality' med ia format, or a higher quality engagement with media, is the desired outcome. This does not necessarily speak to participants' interests or motivations, or address how they engage with the information they are reading, and so long -term behaviour change may be elusive. Moreover, interviewees specialising in the journalism industry noted that the level of interest in news affects the breadth of sources that audiences draw on, as well as the importance of social media as a source of news. Audiences look for re levance more often than credibility, and media organisations orient to these preferences by targeting them as news consumers, delivering what they will find interesting or emotionally appeal ing. In the process, journalists and traditional media organisatio ns use the same techniques as disinformation actors , and this confuses the information landscape for audiences, making disinformation more difficult to detect when it is mixed in with genuine news feeds. In the technology and digital arena, interviewees noted that content-focused solutions are complicated by the danger that censorship presents for human rights, and particularly freedom of speech. Interviewees, particularly those from technology companies, raised a number of issues with content moderation, including its inaccuracy if dependent on AI; its dependence on users flagging up content; its labour -intensive and difficult nature if humans are employed; and the fact that it doesn't actually change audience behaviour or address the circulation of disinformation. Fact -checking and better information for audiences (for example, flagging up problematic aspects of information) were seen as important tools that could educate audiences about what they were reading, but their effect on behaviour change was unc lear. Technology companies were also faced with a very fast-moving information landscape, with tools for creating 'news' available to all, and extending far beyond formal media organisations. As a result, any measures they take can never be definitive. All interviewees noted that evaluation of existing initiatives to address mis/ disinformation was relatively weak, regardless of the type of initiative being assessed. While some organisations were making significant investments in media literacy programmes , the techniques they used were evaluated on a case -by-case basis, if at all. I nterviewees working in the policy / think tank arena noted that social media companies could be more open about their data and operations, while those in social marketing and advertising noted that measuring impact and/or causality in the context of daily life was extremely complex. In summary, the interviews confirmed the difficulty of tackling disinformation for a number of reasons relating to the complexity of m isinformation production and circulation, including: the multiplicity of actors involved, including 'established' source s such as politicians and the media ; the range of ways that m isinformation circulates, and the rate of change, so that finding a way of cutting misinformation off or removing it from the online sphere altogether was almost impossible ; the need for multi -faceted solutions, using different strategies to address different aspects of the misinformation problem, and involving a range of stakeholders (e.g. technology companies, audiences, journalists, communications specialists, regulators) ; the need for audienc e-focused interventions to address the motivations and interests of audiences, rather than being based only on rationality and information provision. 16 Results of the Rapid Evidence Assessment This section reports the core findings of the REA, summarising the available evidence about the effectiveness of media literacy interventions designed to counter misinformation, and conducted by both media literacy practitioners and digital services. As noted in the methodology summary , relevant studies fe ll into four categories, three of which are summarised in this report . The categories summarised below are: media literacy interventions addressing misinformation; technical interventions addressing misinformation; and m edia literacy practices . We first provide a summary of key terms, misinformation topics, and theoretical underpinnings for the articles , followed by a brief overview of the research on media literacy practices . These practices provide context for the discussion of the main result s, analysing the effectiveness of media literacy and technical interventions designed to counter misinformation. Key terms , misinformation topics and theoretical underpinnings Literacy Media literacy has been defined in a wide range of ways5, but the majority of studies in this review adopted a functional approach, with a strong focus on the ability to use a range of techniques for evaluating online information in order to combat misinformation. These included search capability (Donovan & Rap p, 2020) , source evaluation (Leeder, 2019; Yang et al., & Tornatora, 2014; Buijzen, 2021; Yang et al., 2021) . They align with competence and skills dimensions of literacy definitions, and reflect a focus on 'information' as identified by Buckingham (2015) . The functional approach in the studies may be explained by the fact that many were computer science -based , experimental, or analysed the effectiveness of short -term interventions in response to specific pieces of misinformation ( e.g. fact -checking) . Their objectives focused on identifying relat ionships between specific variables, rather than operationalising a broader understanding of literacy. That said , some studies focused on critical evaluation skills and the emotional and symbolic aspects of online communication, because of the role these skills play in the production, consumption and prevention of misinformation. These studies included a 5 Concepts of media literacy incorporate a range of different underst andings of both 'media' (e.g. as a cultural and social form, a channel for delivering information, or a specific set of technologies) and 'literacy' (e.g. critical thinking, self -reflection, or skills for gathering and analysing information). Buckingham (2 015, p. 223) , for example, argues that the notion of literacy implies 'a broader form of education about media, that is not restricted to mechanical skills or narrow forms of functional competence'. He argues that media literacy critically engages with four areas of m edia: representation, language, production, and audience; digital literacy engages with the way the specific technologies, economics and actors of the online environment affect these four areas. News literacy, in turn, is defined by Craft, Ashley, and Maks l (2016) in line with Potter's (2004, p. 146) model of media literacy, as knowledge of news production, content and structures; personal motivations for engaging with news; and competences and skills for engaging with information processing tasks related t o news. Ofcom defines media literacy in terms of audience capacity, as 'the ability to use, understand and create media and communications in a variety of contexts' (Ofcom, 2021a) . Finally, the combination of m edia and information literacy (MIL) has been d efined as '\"a combination of knowledge, attitudes, skills, and practices required to access, analyse, evaluate, use, produce, and communicate information and knowledge in creative, legal and ethical ways that respect human rights\" (UNESCO, IFAP, & IFLA, 20 12, p. 2) . 17 focus on understanding news production, capacity for analytical thinking, evaluation of and reflection on news and information. They resonate with more complex typologies of media literacy advocated by Buckingham (2015) , Potter (2004) and Craft et al . (2016) . However, they do not engage with the relationship between the critical analysis of misinformation and broader knowledge of production and consumptio n normally involved in media and news literacy skills development (Ardi, 2019; Tseng, 2018) . Only three studies used a more elaborated understanding of media and news literacies in their research, including an understanding of news processes and operations (Amazeen & Bucy, 2019) , comparing different types of literacy (Jones -Jang, Mortensen, & Liu, 2021) and combining news literacy with source, message and media credibility (uminas & Jastramskis, 2020) . Misinformation In the studies we analysed, misinformation was often left undefined, or defined in very general terms, such as 'false and misleading online information' (Yang et al., 2021) , or being linked to lying (Tsipursky, Votta, & Roose, 2018) . Some studies elaborate - for example, Amazeen and Bucy (2019) use Tandoc et al .'s (2018) definition of fabricated news stories, defining them as stories that are 'intentionally deceptive, contain little or no facticity, and may be driven by political motivations as well as financial incentives' (p. 416). Friesem and Gutsche (2019) , on the other hand, use the term 'information disorder' to describe the misinformation environment, including fabricated content, imposter content, misleading content, satire/parody, false connection, false context and manipulated content. In empirical studies, misinf ormation is operationalised most often as information characterised by simple falsity, poor source credibility, or a poor evidence base. Other studies focus on a specific type of misinformation - rumour, myth, or native advertising (Pal et al ., 2019) , or define misinformation in a specific context. The latter approach is most common in relation to health misinformation. Scharrer, Stadtler, and Bromme (2019) use the term 'false medical information', Tseng (2018) focuses on medical misinformation about vaccin es, and Paynter et al. (2019) focus on myths about autism, defining the latter as non -evidence - based treatments. Many of the more recent studies testing interventions focused on specific measures to tackle misinformation, such as fact -checking or warning f lags, rather than on the specific nature of misinformation itself. A full list of definitions of misinformation by article can be seen in Appendix 2. The digital context The digital context is recognised throughout as a challenge for media literacy education and a location where misinformation easily proliferates, but can also be refuted using various digital resources. The majority of studies explore the impact of media literacy and technical interventions (e.g. fac t-checking, flagging content) on specific sites for misinformation, such as social media posts and ( to a lesser extent ) websites (e.g. Clayton et al., 2020; Ecker, Lewandowsky, Chang, & Pillai, 2014; . More m edia literacy -oriented studies investigate how well media literacy skills translate to the online environment (e.g. Addy, 2020; Tseng, 2018) . A few studies focus on the multimodal nature of the digital space, examining the role played by images and text in the production, circulation and perpetuation of misinformation (e.g. Hameleers, Powell, Van Der Meer, & Bos, 2020; Shen et al., 2019) . Misinformation topics Most studies used stimuli that related to common areas of misinformation (e.g. climate change, vaccination misinformation, GM crops, political misinformation). Media literacy -related studies were focused on curriculum interventions, either enhancing critical and informatio nal skills in non-media courses, or addressing changes in 18 media literacy curricula themselves6. Four studies evaluated the effectiveness of games as a means of teaching skills to combat misinformation. A small number of studies were focused specifically o n political misinformation (these were most common in the USA) or health misinformation (e.g. vaccine- related misinformation, autism misinformation, misinformation about C OVID -19) (e.g. S. C. Kim, Vraga, & Cook, 2020; Paynter et al., 2019; Roozenbeek, Schneider, et al., 2020) .7 A full breakdown of topics by article can be seen in Appendix 2. Theoretical underpinnings The studies draw primarily on psychological and behavioural science theories for their justification and design. Inoculation theory , which argues that exposure to counter -arguments prior to encountering misinformation can provide resistance to its persuasive power (McGuire, 1961) , features heavily. T esting the effectiveness of interventions based on system 1 vs system 2 thinking is also a focus for a number of studies, and several studies incorporate tests of backfire effects8, confirmation bias, and emotional responses to fake news. Media literacy theory in its comprehensive form (see below ) is incorporated into very few studies , but information and news literacy principles are the basis of many studies (e.g. in studies designed to assess the effect of improved critical thinking and analytical capabi lity on engagement with misinformation ). A small number of studies use linguistic analyses to identify characteristics of misinformation and a few use computational or mathematical modelling to test interventions. 6 The quality of evaluation in these studies was often less rigorous than experimental studies, but we have included them because of their specific focus on media literacy and misinformation. 7 More studies in specialist areas were identified in the main search, but the majority had to be excluded because of their poor quality, or because they were too far removed from the focus of the REA. 8 Backfire effects occur when attempts to correct misinformation actually lead to the incorrect beliefs being more widespread or held more strongly than was previously the case. 19 Media literacy practices The studies on media literacy practices describe a range of approaches to media literacy , and include a variety of settings, actors, formats and rationales (Bulger & Davison, 2018) . We identified seven distinct approaches: 1. Critical thinking : Includes specific techniques such as developing evidence -based arguments and question ing online information (Wells, 2018) , as well as more comprehensive capacities, such as learning awareness , reflection, or critical app lication of information (Bryan, 2018) . 2. Credibility verification : Verifying the reliability and credibility of resources and checking the accuracy of information on the internet (Al-Abdullatif & Gameil, 2020) . Examples include detecting misleading media information, being able to judge the credibility of research studies (Jones, 2018), using conspiracy theory/rumour debunking techniques (Dyrendal & Jolley, 2020) , or applying information verification tools (Conrado, Nevi lle, Woodworth, & O'Riordan, 2016) . 3. Media competence development: D eveloping competence about the media ecology, different media formats, and information navigation (Conrado et al., 2016; Frolova, Ryabova, & Rogach, 2018) . One strand on news literacy (Fash, 2017; Sivek, 2018; including reappraising plausibility of information and knowledge claims (Sinatra and Lombardi, 2020) or evaluating bias (Sperry, 2018) . 4. Integrating media and digital literacy : Awareness of the digital ecology a s integral to understanding the media ecosystem (Valtonen, Tedre, M\u00e4kitalo, & Vartiainen, 2019) . Techniques focus on awareness of algorithm-driven automation for media production and consumption, capabilities in navigating digital spaces, or interventions boosting competencies of reasoning and resilience to manipulation (Kozyreva, Lewandowsky, & Hertwig, 2020) . 5. Cross-context skills cross -cutting academic/scientific and non- academic/scientif ic environments. Techniques point to the relationship between research/academic and scientific competencies (or \"scientific literacy\", Bonney, 2018) and media literacy in relation to finding, evaluating and discerning relevant and reliable scientific infor mation (Delellis & Rubin, 2018; Frisch, Jackson, & Murray, 2013) . 6. Systemic approach to media literacy: Connects media literacy social and historical processes in a society and/ or the power relations behind the media economy (Manfra & Holmes, 2020) . 7. Empowerment : A more recent movement of media literacy \"away from protection or inoculation and toward empowerment\" (Bulger & Davison, 2018) , discussing the crucial role of media literacy for civic participation , citizenship and wellbeing (Azlan, 2019; Jain & Bickham, 2014; O'Sullivan, 2011) . Some techniques refer to the ethical and responsibl e sharing of information for advocacy and participatory opportunities (Middaugh, 2018) . The studies we reviewed suggest several challenges to media literacy, e ither in relation to reducing the positive outcomes of existing interventions, creating unfavourable learning conditions (for example, due to the complexity of the information environment), or by facilitating the spread of misinformation. The main challenges are: Information is created, distributed and consumed in a complex and dynamic environment (digital ecology, media ecosystem, and political climate) that can mask misinformation. Some changes in the online environment have made it difficult to understa nd the flow of information and its origins due to functionalities such as persuasive and manipulative choice architectures (Kozyreva et al., 2020; Valtonen et al., 2019; Walker & Gutsche, 2019) . Media and information ecosystems are fragmented and dynamic 20 (Baildon & Damico, 2011; Walker & Gutsche, 2019) , accompanied by a growing distrust in democratic institutions (Bonney, 2018; Hodgin & Kahne, 2018) and the media (Walker & Gutsche, 2019) . The removal of misinformation is virtually impossible (Schmitt, Rieger, Ernst, & Roth, 2018) . The dynamic media environment dictates the need for new and engaging media information strategies. Media literacy needs to \"keep up\" to maintain the interest of learners, especially younger generations. There is a need for a broad range of formats of media literacy delivery to include new and more dynamic elements and blended offline/online learning. So me examples include gamified activities, memes, use of social media (Encheva, 2018; Kheak Hui & Liew, 2018) . The present media and digital climate also relies on emotion analytics but news and media literacy education tr aditionally tends to focus on the significance of facts, sourcing, and verifiability while the role of emotion in news consumption remains marginal (Sivek, 2018) . Translating media literacy knowledge to appropriate behaviours is a challenging task, especia lly for children , which leaves a gap between competence and conduct (Jain & Bickham, 2014) . Existing media literacy approaches tend to be biased towards critical thinking and not behaviour (Bulger & Davison, 2018; Jeong, Cho, & Hwang, 2012) , and effects on behaviour change are much less documented (Bulger & Davison, 2018) . Overall, there is a lack of comprehensive evaluation of media literacy initiatives . This is mainly due to the difficulty of collecting such data and capturing tangible and long-term effec ts (Encheva et al., 2020) . For example, randomized control trials for curricular testing are hard to do, most of the studies measure single courses and use one-time measures (Bulger & Davison, 2018) , very few studies measure the relationship between misinformation and media literacy, and studies interpret media literacy in different ways . The diversity of the approaches discussed in this section demonstrates the substantial effort dedicated to the development of media literacy techniques and the great potential of this work. However, the lack of evaluation, inconsisten t definitions, and the specificity of many interventions limits the lessons that can be learned about the relationship between media literacy and misinformation. 21 Media literacy interventions addressing misinformation This section covers the findings from the 'core' articles in group 1. The focus is on the extent to which intervent ions in various forms of media literacy are linked to improvements in critical engagement with misinformation online. Positive effects of literacy A number of studies confirm that news literacy and /or information literacy in particular are correlated with the ability to identify misinformation , or engage critically wi th information online. For example, Amazeen and Bucy (2019) conducted an online national survey with adults in the USA to test their ability to identify native advertising and fake news headlines, and showed that both recognition of and critical engagement with native advertising was positively correlated with news consumption and knowledge of news media operations. Tseng's (2018) small qualitative study of fourteen 16-18 year old students found that more critical students showed a higher level of engagement with and critique of science in blog entries . Leeder (2019) administered a practical test to 63 adults aged 19 -24, and showed that critical evaluation behaviours were positively correlated to the correct identification of fake news stories. McGrew (2020) investigate d the effect of an eight -lesson online reasoning course about information literacy using techniques adopted by fact -checkers . The results of pre- and post -testing showed that the 68 US high school students (aged 16 -17), who followed the course as part of their history curriculum, achieved significantly better scores in lateral reading, evidence analysis and researching a claim , as compared to the control group. Jones -Jang et al. (2021) investigated fake news and showed that information literacy was the only type of literacy ( among media literacy, news literacy, and online, new media and digital literacy ) that had a positive association with fake news identification . Mason et al. ( 2014) also focused on high school students : 69 of 134 Italian students aged 14 -15, were given instructions about how to evaluate the reliability and truthfulness of a website. S tudents exposed to the intervention demonstrated more extensive navigation and verification activity, and spent more time examining the sites themselves , than those who were not. These effect s lasted over a week, to the point at which the second stage was completed. Addy (2020) also found that when information literacy instruction w as combined with teaching students a four -step fact -checking process tailored to the digital context (Stop; Investigate the source; Find better coverage; and Trace claims), first year university students were able to evaluate digital information more accurately. Key findings Elements of media literacy skills - particularly critical thinking, evaluation strategies and knowledge of the operation of news and media industries - are consistently found to have positive effects on the ability to critically engage with misinformation. Interventions that prompt more cognitively demanding, 'system 2' thinking are more effective than those relying on automatic, instinctive responses from audiences. The limited research on games and gamification shows that these tools may help improve d igital media and information literacy, prompting more critical engagement with misinformation online. Perceptions of source credibility and the ability to critically evaluate source quality, are important factors that underpin effective media literacy ski lls and influence attitudes towards misinformation. 22 More complex effects of training were identified by Calvo, Cano -Oron, and Abengozar (2020) , in a study where 55 Spanish students participated in a workshop to help them identify the characteristics of bots on T witter. The y were first taught th e context for this type of automated manipulation, then experiment ed with bot detection tools, and finally reflect ed on their experience. A comparison of pre- and post, self -completed questionnaires showed that t he students' ability to identify bots actual ly decreased following the workshop, suggesting that a more complex understanding of bot characteristics increased the levels of doubt in their judgement. However, they did use a wider range of criteria to identify bots following the workshop, suggesting t hey had developed a more comprehensive understanding of this particular misinformation technique. In a professional context, LaCaille, LaCaille, Damsgard, and Maslowski (2019) , show ed that a group of North American university students who completed a semester -long course in psychological myth -debunking demonstrated more accurate psychological knowledge than those in a control group, who had followed a study skills course. An experiment conducted by Payn ter et al. (2019) exposed Australian professionals being trained in autism treatment to a course in 'optimal debunking' (incorporating a debunking message, an explanation of the reason for debunking, and evidence for debunking). Pre- and post -measurements showed that professionals who participated in the course were less likely to say they would recommend or use non-evidence -based treatments following the course. However, when tested again 6 weeks later , the effects had not lasted. Search skills A small number of studies address the impact of improved search capabilities on misinformation outcomes, and have largely consistent results. For example, an experiment conducted by Donovan and Rapp (2020) showed that the opportunity to search online when answering q uestions was negatively correlated with the propensity to share fake news , and positively correlated with accurate information reproduction. Searches tended to be focused on finding new information rather than confirming or checking information already known. In other studies, search techniques were integrated into curriculum enhancements and their impact measured as part of a broader range of curricular interventions. For example, Maitz et al. (2020) describe a three -day workshop delivered to 14 Austrian schoolchildren aged 12 -14, designed to improve information literacy throu gh coaching on search strategies, reading and evaluation techniques, and source credibility. The workshops improved children's understanding of their own levels of health literacy, reducing particularly high self -estimates, and their understanding of how to find helpful online sources for health information. However, the results also showed that students tended to over -estimate their health literacy and did not always visit high-quality websites, despite the training. Games and gamification Four articles f ocused on the effectiveness of games as a means of teaching media literacy and / or information literacy skills. The game 'Bad News' was evaluated in two studies. In the game, players learn about misinformation techniques (impersonation, emotional language, polarisation, conspiracy theories, discrediting opponents, and trolling). Defining Bad News as 'a broad spectrum vaccine against misinformation' , Roozenbeek and van der Linden (2020, p. 3) argue that active inoculation , where participants are 'trained to be more attuned to specific deception strategies' (p. 7) , enabled players to more accurately detect fake news tweets deploying such strategies. Their findings also suggest that the impact of playing the game is most beneficial for players who are more sus ceptible to fake news. Also evaluating Bad News, Basol, Roozenbeek, and van der Linden (2020) show ed that the decrease in perceived accuracy of fake news tweets was greater in individuals who played the game than in their control group (who played Tetris), while their confidence in their own judgements increased 23 more than the control group. Katsaounidou, Vrysis, Kotsakis, Dimoulas, and Veglis (2019) reviewed user feedback on the game MAthE, designed to enhance detection of misinformation and improve news verification techniques in the Greek context. The results of their randomised online field study showed that in self - evaluations, users of the game agreed that it supported their news verification skills and their ability to apply such skills, enhancing thei r digital literacy. Finally, Yang et al. (2021) evaluate Trustme!, an online educational quiz, and show that the gamification of the quiz (adding scenarios, points and game rules) enhanced players' ability to critically analyse information, but did not change levels of scepticism towards online information. Taken together, and although there are only a small number of studies, the findings show that games and gamification techniques may help improve digital media and information literacy in ways that pro mpt more critical engagement with misinformation online. System 1 vs System 2 thinking The game studies show that active engagement with different literac y skills , rather than passive instruction, is important for prompting changes in beliefs, attitudes a nd / or behaviour. Other studies have also confirmed this in a range of contexts. Most simply , Tsipursky et al. (2018) , aske d 21 US citizens, politicians and journalists to sign a pro -truth pledge that committed them to 12 actions related to sharing, honor ing, and encouraging truth. Signing the pledge meant participants were less likely to share poor quality information on Facebook. Also focused on Facebook posts, Kirchner and Reuter (2020) compared four strategies for countering fake news o n social media. They found that three warning-based approaches (a warning with no elaboration, a warning accompanied by an indication of how many friends believe the article is fake, and a warning accompanied by an explanation of why the article has been flagged) were all effective in reducing perceive d accuracy of false headlines, but the condition with the warning plus explanation had the most significant effect 9. Ecker et al . (2020) in their study of th e 2016 US election also found that the effect of more detailed fact checking labels lasted longer than simply flagging inaccura te content. Tseng's (2018) study suggest s that the critical perspective can be \"activated\" (p.262) : students were asked to reflect critically prior to reading a blog, and the results suggested that encouraging active engagement is a possible route to more robust individual strategies for tackling misinformation. Further evidence of the potential to activate critical engagement comes from Murrock, Amulya, Druckman, and Liubyva (2018) , who examined the effect on Ukrainian adults of participation in a 'Learn to Discern' media training programme, comp rising training on information and media literacy, fake news and manipulation techniques, and debunking tools. As well as finding that participation increased scepticism in relation to both fake and real news, participants' new behaviours persisted for at least a year following the intervention. Normative messaging A few studies test ed the effect of reinforcing norms about news literacy on participants, but without clear outcomes. R esearch by Tully, Vraga, and Bode (2020) and Vraga, Bode, and Tully (2020) suggests that attempts to reinforce the importance of news literacy may not affect perceptions of information credibility, and that news literacy messages have to be tailored to the context in order to attract attention from readers in an information - saturated env ironment. In an online experiment conducted with 3 ,024 participants in the USA, they found mixed results regarding the effect of tweets promoting the importance of news literacy as an individual and social 9 In contrast, neither of the approaches already used by Facebook (reducing the size of the post and showing fact -checking articles alongside) were effective. 24 responsibility on participants' perceptions of inf ormation credibility and their news literacy beliefs. Using a slightly different approach to normative persuasive effects, Pal, Chua, and Hoe -Lian Goh (2019) explored the degree to which integrating three salient beliefs into rumour denials affected intent ion to share. The three beliefs were behavioural, normative and control-related: that sharing denials helps spread the truth; that friends and the online community encourage sharing; and that source credibility encourages sharing (p. 115). The results sho wed that when all three beliefs were integrated into denial messages, they were significantly correlated with intention to share. Integrating any single belief, on the other hand, had no effect. These results, from a large and robust study, suggest that si mply telling people about the importance of news literacy in the context of misinformation is less powerful than either implicit messaging integrated into a text, or than activating critical cognition and behaviours when citizens engage with information. Persistence of misinformation A number of studies point to t he persistent persuasive power of misinformation . Leeder (2019) , for example, found that although critical evaluation was associated with correctly identifying fake news, such stories were also rated as more believable and more trustworthy by the study participants, although they were less willing to share such storie s as compared to real news stories. Scharrer et al. (2019) focused on the 'easiness' effect when complex topics are simplified to make them more accessible, as is the case in popular science news. Their investigation focused on whether the easiness effect would persist even when readers had rich and accessible source information with which to evaluate the article. The findings showed that source credibility was valued by participants as a means of evaluation, but the easiness effect, evidenced by participa nts' propensity to evaluate based on article content rather than source, remained even when the opportunity to conduct robust independent judgement was available. Finally, Banas and Miller (2013) investigated the operation of inoculation effects in an experiment with 312 US college students. They found that fact -based inoculation messages were most effective, but the effect was lower if the participants were previously exposed to beliefs that made them resistant to the correction (meta -inoculation). Simila rly, in the research conducted by Jones -Jang et al. (2021) , prior exposure to fake news in fact reduced fake news identification. Mediating factors Across the studies in this group, a number of different mediating factors were consistently identified as having an effect on the relationship between media literacy and related skills, and engagement with misinformation. Forms of literacy and critical skills A number of studies identified pre- existing media literacy as a factor that increased th e accuracy of recognising fake news. Vraga and Tully (2019) conducted an online survey of 788 US adults and found that both news literacy and media literacy negatively correlate with posting news and political content on social media, and are positively co rrelated with scepticism towards information on social media. Amazeen and Bucy (2019, p. 429) found that procedural news knowledge acted as an 'implicit forewarning mechanism' against misinformation and a survey by Craft, Ashley, and Maksl (2017) found that knowledge of the news media also lowered the probability of conspiracy theory endorsement. Ku et al. (2019, p. 10) similarly found that knowledge of news media and the news industry facilitated awareness of the need to use critical thinking skills when e ngaging with news. uminas and Jastramskis (2020) showed that journalism students with some training in news creation were better able to recognise fake and trustworthy news tha n students studying publishing and advertising, paying more attention to news credibility indicators such as sources and captions. 25 More generally, critical thinking ability has been found to have a positive effect on the ability to evaluate fake news (Lutzke, Drummond, Slovic, & \u00c1rvai, 2019; Tseng, 2018) . Yang et al. (2021) found that students with higher levels of engagement in political and civic issues also had higher levels of scepticism about online information, and Amazeen, Vargo and Hopp (2019) also found that individuals actively seeking out political information were more likely to post fact -checks online. Finally, Allcott and Gentzkow (2017) found that total media consumption positively correlated with more accurate beliefs about the veracity of headlines. Other factors Partisanship has also been found to affect accuracy in relation to misinformation in some studies: Jones -Jang et al. (2021) and van Stekelenburg et al. (2021) both show that liberal -leaning participants hold more accurate beliefs and are able to more accurately identify the veracity of new s than other political groupings. Amazeen, Vargo, and Hopp (2019) also find that more liberal-leaning citizens are more likely to shar e fact- checks online. In a different context, Ardi's (2019) experiment with 71 Indonesian 18 -23 year olds shows that supporters of the political opposition are more likely to trust and spread news that damages their opponents. However, Berinsky (2017) studied rumour and refutation in relation to the topic of 'death panels' and the Affordable Care Act, and found that while partisanship of the recipient affects how information is processed, rumour rejection is most effective when the source of the rumour is speaking across partisan boundaries. The departure from an expected political position appears to make statements more credible. Some studies have identified age as a factor in the ability to identify misinformation. Amazeen and Bucy (2019) found that yo unger participants in their survey are more susceptible to persuasion by fake news and native advertising, while Jones -Jiang et al . (2021) found that older voters more accurately identify fake and authentic news stories and Ku et al. (2019) found that older school students were better at critical thinking than younger students. Amazeen et al. (2019) also found that older people were m ore likely to share fact -checks online. Similarly, Lucassen, Muilwijk, Noordzij, and Schraagen (2013) found that high school students used a more limited range of strategies for evaluating credibility than undergraduate and postgraduate students, both of whom included critical evaluation of sources and objectivity in the tactics they used. Allcott and Gentzkow (2017) also found that age correlated positively with accurate beliefs about headline veracity. Prior knowledge about a subject also affects attitu des and behaviours towards misinformation. Mason et al .'s (2014) study of high school students' engagement with information on web sites showed that the effectiveness of critical evaluat ion skills was highly correlated with prior knowledge of topics. Lutzke et al. (2019) found that subject -specific knowledge helped to prevent sharing of fake news regardless of whether the participants were doubters or believers in the news story itself, and Chua an d Banerjee (2017) study of university students in Southeast Asia found that pre-existing medical knowledge made it less likely that rumours would be shared. Roozenbeek and van der Linden (2020) note that the order of stimulus presentation can affect the im pact of the inoculation techniques emb edded Thorson, and Tham (2020) in their study of the 2016 US election find that the initial level of belief in a headline conditioned the extent to which an audience membe r was likely to engage in verification activities. 26 Technical interventions addressing misinformation In this section, we summarise the studies from the 'core' articles in group 2, focused on technical interventions relating to misinformation, but not engaging with the kinds of literacy skills featured in the previous section10. The effectiveness of fact -checking, correction and flaggi ng of misinformation A number of studies address the value of fact -checking in various contexts, and many reveal the complexities of its effects. For example, positive results for fact -check interventions are found in experimental studies by Clayton et al. (2020) , who found that tags labelling headlines as 'disputed or 'rated false' both reduced perceived accuracy, and by Mena (2020) , who found that a warning label on Facebook posts reduced perceived credibility and intention to share. Nekmat (2020) also fo und that fact checks reduce the likelihood of sharing, and that the effect is enhanced when the tagged news comes from an unfamiliar source. However, the scale of the reduction in the likelihood of sharing is greater for tagged news coming from a mainstrea m source. Some studies explored the impact of different formats on fact -checking effectiveness. For example, Ecker, O'Reilly, Reid, and Chang (2020) compared brief 'tags' with a 140 character fact check label, and found that while both labels were effe ctive in reducing the perceived accuracy of false posts, the more detailed refutation improved the quality of reasoning and evidence-based conclusions among participants, and also led to longer retention. In a slightly different approach, Hameleers et al. (2020) compared multimodal and text -based fact check formats for misinformation about refugees and school shootings. Findings showed that multimodal communication made the refugee-focused misinformation more credible, but not the school shooting information, but both forms of fact -checking were effective in reducing the credibility of both types of disinformation. Also focused on the style of fact -checking information, S. C. Kim et al. (2020) used eye-tracking to test message attention and credibility for corrections to vaccine misinformation based on logical fallacies or parallel argumentation, but also incorporated humour into the analysis by adding a cartoon or an infographic into the correction s. They f ound that non -humourous corrections were associated with higher credibility, but the effect 10 A small n umber of studies in this category model the behaviour of misinformation and rumour online, but these are not reported in this summary (though we include them in the bibliography) because the insights they provide for media literacy work are extremely limited. Key findings The effectiveness of technical interventions such as fact -checking, flagging and corrective information, is highly contextual. Over all, fact -checking interventions tend to have positive effects on audience perceptions of misinformation, but their impact is mediated by a number of factors, including prior knowledge, source, topic and format. Interventions that engage with more detail ed explanations that could prompt more cognitively demanding, 'system 2' thinking from audiences seem to be more effective. Interventions can be less effective when backfire effects, confirmation bias or partisanship affect audience responses, but research has not yet identified consistent conditions for these results. 27 of humourous correction was mediated by attention, where increased attention to the cartoon lowered the original tweet credibility and thereby reduced vacc ine misconceptions. The complexity of correction effects is echoed in a study by Huang and Wang (2020) exploring the effectiveness of narrative (story -based) and non - narrative (factual) corrections . Narrative attempts to correct misinformation are more effective in changing attitude and behavioural intentions if corrections are algorithmically driven (that is, the correction is presented via an algorithmic mechanism), but when corrections come from members of the social media network, then factual informati on may be more effective. Refutations and ratings A number of studies explore the effectiveness of refutations, rather than only fact check interventions. Featherstone and Zhang (2020) , for example, in an experimental s tudy testing the effect of refutations on vaccination attitude, found that refutation s increased support for vaccinations, while two -sided refutations, where one argument is countered by a second argument accompanied by evidence and reasoning, enhanced positive attitudes to vaccinat ion by reducing the anger prompted by revelations of misinformation. Elaborated refutations were also found to be more effective by Moravec, Kim and Dennis (2020) who investigated the effect of simple warning flags (a stop sign) that appeal to s ystem 1 ty pe thinking, and elaborated warnings that demand system 2 cognitive engagement. They found that a combination of both approaches was most effective in influencing the believability of a headline, and the effect was even stronger when users had received awa reness training about the warning flags. Similarly, an experiment by van der Meer and Jin (2020) also found that while corrective information (in this case, about a virus outbreak) influenced awareness, attitude and emotional state, more elaborate corrections additionally improved the potential for behavioural change. Vraga, Kim, Cook, and Bode (2020) analyse the effectiveness of correction s to climate -related misinformation on Instagram , and found that corrections focus ed on the flawed logic of fake news effective ly reduc ed the credibility of the misinformation, whether placed before or after exposure. Fact-based corrections were only effective post -exposure. In a different study, Vraga et al . (2019) also found that logic -based corrections out - performed humour -based corrections for vaccination misinformation. A few studies explored the impact of ratings as a correction strategy. Amazeen, Thorson, Muddiman, and Graves (2018) found that rating scales were effective when used to correct non-politic al misinformation, but had no effect on political misinformation. Pennycook and Rand (2019) asked their participants to rate different websites in order to explore the effect of source on ratings, and found that mainstream sources were perceived as more trustworthy than partisan or fake news sources. Quality assessments were better among liberal participants, and those who demonstrated higher levels of cognitive reflection. Knock- on effects and limitations Perhaps the most commonly identified knock -on effect of correction interventions is the backfire effect , when attempts to correct misinformation actually lead to the incorrect beliefs being more widespread or held more strongly than was previously the case. The existence of a backfire effect is controversial and the dataset included evidence both for and against its existence. Nyhan, Reifler, and Ubel (2013) , for example, found some evidence of a backfire effect in a study of the effect of a fact -checking intervention for political advertising by Sarah Palin. While participants with low knowledge of Palin and low political knowledge responded to the fact -check intervention, a backfire effect was evident for those with higher political knowledge and who supported Palin. Vraga, Kim, and Cook (201 9) also show that logic -based corrections can further increase the credibility of 28 misinformation among those people already convinced of its veracity. The danger of prior exposure is reinforced in a study by Pennycook, Cannon, and Rand (2018) , which shows that even minimal prior exposure to fake news can increase perceptions of its accuracy, and repetition over time can compound the problem. Peter and Koch (2016) claim strong evidence for a backfire effect based on the way audience members remember fact -checking , confusing claims and counterclaims over time . An alternative version of the backfire effect is when media literacy education actually decreases belief in real stories. Guess et al (2020) found such an effect, although it was smaller than occurred for \"genuine\" fake stories . In contrast , Zhang, Featherstone, Calabrese, and Wojcieszak (2021) found no backfire effect in their study of the flagging of inaccurate content on vaccination, and Roozenbeek et al .'s (2019) study of the impact of playing the Bad News game showed no backfire effect. In a slightly more unusual set of results, Jang, Lee, and Shin (2019) conducted an experiment with Korean participants, and found that corrections did not affect the perceived validity of the original news, but did increase negativity towards both the source of the misinformation and towards social media as a whole. Conversely, Pennycook, Bear, Collins, and Rand (2020) explore the implied truth effect of fact -checkers. In an online experiment comparin g tagged Facebook posts with a non -tagged control, they found that participants in the tagged study perceived all non -tagged headlines in their sample as more accurate, regardless of whether they were actually true or false. Overall, the studies did not pr ovide consistent evidence as to when and how backfire effects occur. Confirmation bias was observed by Moravec, Minas, K., and Dennis (2019) , who showed that users tended to believe headlines that were in tune with their political opinions, and even when flagged as fake news, these beliefs were not dislodged - the only effect that the flag had was to prompt users to spend more time considering the headline. A. Kim, Moravec, and Dennis (2019) also found evidence of confirmation bias - participants in their experiment were more likely to believe and share articles corresponding to their beliefs. Attitudinal intransigence was also noted by Porter, Wood, and Bahador (2019) , who show that debunking can improve factual knowledge, but may still not alter political beliefs or support for candidates and their policies. Radechovsky, Berger, and Wolling (2019) conducted an online experiment with 607 German respondents to t est the effectiveness of fact -checking. They found that, while corrections to information thought to be false are successful in changing people's beliefs, corrections are much less effective when applied to misinformation that is believed to be true. Several studies explore the limitations of fact -checking, particularly in political contexts. For example, in a randomised online experiment al study of the French 2017 presidential election, Barrera, Guriev, Henry, and Zhuravskaya (2020) found that fact- checking did not dilute the potential for misinformation to generate support for candidates, specifically Marine Le Pen. They theorised this is because the support was not gained because of facts but instead because of the sentiment in the story which coul d still appeal to voters. Amazeen et al. (2019) found that intent to share fact -checking information on social media wa s higher when it supports an individual's political views. The type of content may also have an effect on fact -checking effectiveness. Mo st studies are focused on news -related content, but Oeldorf -Hirsch, Schmierbach, Appelman, and Boyle (2020) experimented with fact check labels associated with memes, and found them less effective than other studies suggest. They enjoyed low levels of reca ll and had no impact on the item's perceived credibility, nor on the intent to share or to explore the topic in more detail. 29 Limitations of the studies Methods While the studies included a range of methods, including traditional surveys, focus groups, c ontent analysis, social network analysis, and qualitative/ thematic analysis, the dataset was dominated by experiments. These were either organised face -to-face or through online surveys. Experimental methods are powerful because they can isolate causation effectively. They do this by exposing a group of participants to a particular stimulus and then comparing their behaviour with a non -exposed control group. This approach offers a significant advantage over purely correlative studies (i. e. surveys). Howev er, results from experimental research need to be tested in the field to establish their validity in the complex 'real world' of media consumption and misinformation . Without such tests there is no guarantee that the results will hold. Geography The majority of articles in the dataset featured the United States either as their sole focus or as a comparative example, raising questions about the applicability of findings in other national contexts. Certainly , some of the comparative studies suggested th at the US experience was not universal. For example , Hameleers (2019) found that American and Dutch citizens reacted differently to exposure to fact -checking. Other countries did feature in the dataset including Australia, the Czech Republic, France, Germa ny, Mexico, the Netherlands, Singapore, South Korea and Spain. However, in all these cases the dataset contains at most two studies, so knowledge of these other populations remains very limited. Sampling demographics The samples in the dataset took a var iety of forms. Some survey researchers had access to professional panel surveys provided by companies such as YouGov or were able to bolt their studies onto pre -existing research instruments (such as the National Congressional Survey in the US), and were a ble to use nationally representative samples. Others recruited samples that were less representative (such as those obtained via MTurk) occasionally attempting to use statistical techniques to make them representative. For those doing experiments, particul arly in a face -to-face setting, convenience samples were often used, including recruiting students to participate. The result of these sampling strategies is that participants in most studies were adult, and often young adults. Fewer studies (and none at all in the research on technical interventions) sampled participants under -18 (i.e. school age) . Moreover, within t he adult population, results were rarely disaggregated into findings for different demographic groups (e.g. by ethnicity, disability or age), so the effects of interventions on populations who may be differently affected by misinformation are significantly under -explored. 30 Conclu ding discuss ion The REA indicate s the range of research being conduc ted in t he area of media literacy and misinformation. Overall, the results confirm that a significant amount of research has been conducted on ways to tackle the production, circulation and consumption of misinformation through media literacy and technical interventions , and on the development of media literacy skills among children and young people. The results should be reviewed in light of the fact that all three methods in the REA (scoping grey literature, interviews, and article analysis) produced insights t hat underline the difficulty of resolving the challenges that misinformation presents, because of the complex context for applying both media literacy and technical interventions. Media literacy curricula struggle to keep up with rapidly evolving media env ironments and technologies, and research tends to focus on only a few digital contexts (mainly Facebook and Twitter), neglecting newer, and very popular platforms and technologies, such as WhatsApp, Instagram, Tumblr, SnapChat or Pinterest. Rapidly evolving algorithmic technologies, digital infrastructures and user interfaces also alter the interactions between media literacy and misinformation, but these changing dynamics are very difficult to capture in research. Perhaps reflecting these difficulties, re search on the interactions between media literacy and misinformation, and on media literacy more broadly, shows that media literacy tends to be operationalised in partial terms and in different ways. For example, i n studies of misinformation, information and news literacy are more commonly investigated than broad forms of media literacy. This finding is echoed in the grey literature and expert interviews, where the lack of a unified framework for defining, teaching and evaluating media literacy was identified. Broader understandings of the social, cultural, economic and political dynamics of news and media production and circulation are not often used in the interventions featured in academic research, even though some studies have found to them to be predictive of more critical engagement with misinformation. There is clearly scope for integrating a more comprehensive understanding of media literacy, but this may also raise challenges in terms of research complexity and feasibility. Finally, one problem limiting the scope and scale of research may be difficulties associated with acce ssing data about online behaviour. Much of this data is proprietary, and as some of our expert interviewees identified, there is no ob ligation to share it with researchers or policymakers. While platforms such as Facebook and Twitter do facilitate some access, it is incomplete. This makes it harder to employ methods such as network analysis or observational methods. Even where researcher s can obtain some real -world data, it is difficult to judge how representative that data is of wider populations . Platforms may produce their own evaluations of interventions, but these are not necessarily independent and may not address broader skill sets , such as those fostered by media literacy, in their design and execution. Below we summarise the key findings and limitations of research from the REA, before concluding with specific recommendations for research, practice and collaboration. Findings on interventions in existing research 1. Research shows that elements of media literacy skills - particularly critical thinking , which may involve asking questions where information comes from or using information to construct evidence based arguments; evaluation strategies, including a reflective approach to one's own status as an audience 31 member ; and knowledge of the operation of news and media industries - have consistently been found to have positive effects on the ability to critically engage with misinformation. Developing media literacy skills may be regarded as a powerful 'inoculation' option in the struggle to limit the influence and spread of misinformation. 2. Research consistently identifies that interventions based on system 2 thinking are more effective than those based on automatic, instinctive responses . System 2 thinking is defined as slow, critical rational thinking (as distinct from system 1 thinking, which is rapid and intuitive). System 2 interventions demand greater cognitive engagement with subject matter (whether warnings, fact checks, guidelines for evaluation or other media literacy or technical interventions), and produce more effective, and sometimes longer lasting, effects on the ability to critically engage with misinformation. 3. The limited research on games and gamification shows that the se tools may help improve digital media and information literacy . Online games can expose participants to different types of misinformation and guide them through the skills required to make informed judgements about information. Both games and gamification techniques that enable players to develop skills in reflexivity, critical thinking, identifying misinformation techniques and information evaluation (aspects of the 'system 2' thinking noted above), appear to have a positive effect on the abilty to assess and evaluate the credibility of misinformation. 4. A number of studies consistently identified that perceptions of source credibility (trustworthiness and believeability) and the ability to critically evaluat e the quality of sources, are important factor s that underpin effective media literacy skills and in fluence attitudes towards misinformation . Results are somewhat varied, and as the expert interviews identified, a decrease in trust in traditional institutions complicates the attribution of credibility to any particular type of sources. Nonetheless, eva luating sources may be understood to be a key factor in effective media literacy that could make a significant difference to engagement with misinformation. Methodological limitations of existing research 1. Published work in this area is not very methodolo gically varied . There is a strong emphasis on experimental methods, where the relationships between different variables are tested in controlled conditions. Studies that test experimental results in the field, under 'real- world' conditions, and longitudina l studies, carried out over an extended period in order to track changes over time and the longevity of effects, are both rare. Alternative methods, including qualitative methods that can provide insights into more nuanced audience rationales for dealing with misinformation, are much less common than quantitative methods. 2. The majority of research defines the potential impact of media literacy interventions in terms of their effect s on attitude, knowledge or understanding of misinformation. Analysis of actual behaviour change is less common. Behavioural change most often appear s as an 'intent' (for example, to share misinformation or rebuttals). Thus, it is not clear from most research whether media literacy interventions will affect actual sharing behaviour on social media platforms. 3. There are a number of sampling limitations in the research: a. Facebook and Twitter are the primary sites for investigating misinformation on social medi a. The ways in which media literacy interventions might affect engagement with misinformation on other platforms have not been investigated in any depth. Given the fast -changing nature of the digital environment, this is an important gap in knowledge. 32 b. Research emphasises the US context . While the authors of such work do not explicitly make the claim of universal applicability, much of this research lacks any real consideration of the specific socio -cultural , political and institutional context of the United States, or of the extent to which findings might be applicable in other contexts. c. While some research uses representative samples, a large proportion uses non -representative sampling methods, gathered using volunteers from school or universi ty student cohorts, or via services like MTurk. This raises questions about the applicability of results to the wider population. d. Most studies are carried out with adult populations, with limited differentiation of responses within these populations. Liber al-leaning individuals and older populations are generally found to be more critical of misinformation and more accurately assess credibility. However, the evidence base for these conclusions is relatively small. No research was identified that engaged in depth with gender, ethnicity, sexuality, disability or other identity categories . In addition, younger, school -age populations are largely overlooked even though studies show that younger age groups have more limited capabilities for engaging with misinfor mation . Both media literacy research and the grey literature suggest that media literacy interventions are most effective when delivered to younger , school -age children . The demographic limitations of much research mean there is a limited understanding of variability in the effectiveness of media literacy as a tool to tackle misinformation across different populations, some of whom may be more vulnerable to misinformation . 4. There is a lack of interdisciplinarity across studies. The majority of studies condu cted at the intersection of media literacy, technical interventions and misinfor mation are underpinned by principles of psychology and behavioural science and rarely reference theories about media literacy . Media literacy research, while inherently interdisciplinary, does not connect to the work being done on misinformation. As a result, mutually beneficial insights - for example, the value of different media literacy frameworks for engaging with mis informatio n, or the ways in which changing modes of misinformation may require new forms of media literacy training - are being overlooked. Recommendations We propose the following recommendations for researchers, media literacy practitioners, and for collaboration between multiple parties, as a way of building on the findings and addressing the limitations revealed in the REA. Research ers To broaden the ways in which media literacy is applied in research on strategies to counter misinformation, so that the full range of benefits from media literacy education can be identified. It should include elements of self -reflexivity, knowledge of the media industries and how they work, the social and cultural context for media, and critical analyses of representatio n. To improve the range of samples employed in studies , and particularly to include younger populations , more diverse populations, and a wider range of platforms . To work towards a unified framework for media literacy evaluation, so that the impact it has on capacities for dealing with misinformation can be more easily compared across contexts and a reliable body of comparable results can be built. 33 Media Literacy Practi tioners To explore how system 2 thinking, and particularly games and gamification, might be consistently integrated and evaluated in technical interventions and media literacy education . To work towards overcoming the challenges posed by integrat ing evaluation into media literacy curricula and into technical innovations by platforms , in order to clearly identify the impact they have on audience knowledge, attitudes, understanding and behaviours dealing with misinformation . Collaboration between multiple parties To continue to facilitate regular dialogue between platform organisations , researchers and media literacy practitioners, so that media literacy curricula can keep up to date with the fast -changing digital environment. To explore how proprietary data may be made available for research, so that a wider range of methods and research questions can be deployed. 34 Appendix 1: Detailed methodology The first stage of the project involved two methods: a grey literature search and a series of expert interviews. Grey literature search The grey literature search focused on finding non -academic literature (e.g. produced by industry organisations, civil society organisations, governmental advisory groups) that addressed misinformation and/ or media literacy issues. The search was broad and inclusive, to ensure that no potentially useful terms were missed. The search was conducted on the following databases: UK government and public sector sources: DCMS report on d isinformation; The House of Lords report on political polling and digital media; the NHS evidence site; the HMIC database; data reports International library; APO Australian effectiveness\" The searches were concluded once the saturation point wa s reached (i.e. no new documents were appearing in the search results) or when the search produced no relevant documents. The searches generated 75 relevant documents, including 9 evidence reviews, which were documented using the Zotero reference database. They covered a range of topics within the scope of the study, including general documents reporting on initiatives to educate the public in critical thinking; more specific reports on educating the public about disinformation; anti - disinformation strategi es or policies; and analyses of specific cases such as disinformation relating to Covid - 19, or elections in different countries. Where relevant academic articles appeared in the searches, they were set aside for inclusion in the main search. Once the search was concluded, the documents were read by the research team to extract specific search terms for the test and main search protocol. Expert interviews Alongside the grey literature search, we carried out 1 1 expert interviews with experts working in the tech industry, advertising, social marketing, policy and media research/ academia. The interviews lasted around an hour and were focused on the area of expertise of each participant. Each interview was recorded and reviewed for key insights. Broadly , the discussions covered disinformation production and circulation, causes and 35 consequences, audience perceptions and behaviour, techniques and initiatives used to tackle misinformation or to educate and persuade audiences, successful initiatives, and les sons applicable to media literacy education. The disinformation topics and examples covered a wide range of issues, from climate change to public health, radicalisation and extremism, and politics. Main search protocol The database search followed the Pr eferred Reporting Items for Systematic Review and Meta -Analysis Protocol (PRISMA -P) guidelines (Moher et al., 2015) . Search strategy On 26 January 2021, we searched the following databases covering a range of subject areas: Multidisciplinary: Scopus, Web of Science Core Collection, International Bibliography of the Social Sciences (ProQuest); Communications: Communication and Mass M Abstracts (via EBSCO), the ProQuest Politics Collection (PAIS Index, Political Science Database, Worldwide Political Science Abstracts); education: British Education Index (via EBSCO), Education Resources Information Center (via EBSCO), Education Abstracts (via EBSCO); Library science: Library, Information Science & Technology Abstracts (via EBSCO); Sociology: SocINDEX (via EBSCO); Business: ABI/INFORM (via ProQuest). The databases were selected so that they give a cross -disciplinary breadth of the evidence review. A comprehensive search strategy was developed for the concepts: misinformation, media and related literacies, and platform/online. Search terms were identified by the research team, which were further developed with the help of a systematic evidence review specialist (Andra Fry, LSE Library). These were used to run keyword searches in the title, abstract and author -supplied keyword fields or equivalent. Search operators such as truncation (finding different endings) and proximity (finding words within a cer tain distance of each other) were used for a comprehensive search. The Boolean operator \"OR\" was used between similar terms, whilst \"AND\" was used between concept groups. Searches were limited to the English language, studies published from 2011 until 2021, and peer -reviewed publication types. An initial search strategy was developed for Scopus, which was then adapted for the other databases. 36 Table 1: Scopus search strategy Misinformation TITLE -ABS-KEY((misinform* OR disinform* OR OR ((fake OR false OR misle* OR manipulat* OR harmful OR hoax* OR fabricate* OR inaccurac* OR information OR content* OR narrative* OR image* OR story OR stories OR media OR fact O R facts OR belief*))) Media literacy (((media or digital or information or news or comput* or technolog* or critical* W/2 (news OR information OR content* OR narrative* OR story OR stories OR media OR source* OR fact OR facts OR belief*))) Platform (online OR \"on -line\" OR internet Facebook* Instagram* Pinterest* Reddit* Notes: * is used for truncation. \" \" are used for phrase searches. W/x is used for proximity searches. TITLE -ABS-KEY is used for title, abstract and author supplied keyword searches. Keyword searches were also developed for the three case studies of interest: vaccination/pandemic (\"anti vax*\" OR vaccin* OR immunis* OR jabs OR OR OR O OR ecolog* OR sustainab*). These searches were used to check if sufficient relevant results were captures by the main search strategy in Scopus. For this, we combined the initial search with AND with each additional concept, for example misinformation AND media literacy AND platform AND vaccination . Out of 1 ,231 results found in Scopus, 113 were identified for vaccinations, 266 for elections, and 163 for global warming. As the numbers were satisfactory for each case study, these specific keywords were n ot included in the final search strategy. Results from each database were exported to EndNote and deduplicated following the Falconer (2018) method first, and then the Bramer, Giustini, de Jonge, Holland, and Bekhuis (2016) method. 37 Databases and res ults Table 2: Databases and search results Database Interface Date of search Hits Description Scopus - 26/01/20 21 1231 Among the largest abstract and citation databases of peer - reviewed literature including scientific journals, books and conference proceedings. It includes research outputs from across the world in the fields of science, technology, medicine, social sciences, and arts and humanities. Web of Science Core Collection - 26/01/20 21 1049 Rich collection of citation indexes repre senting the citation connections between scholarly research articles found in the most globally significant journals, books, and proceedings in the sciences, social sciences and art & humanities. Includes content on life sciences, biomedical sciences, engi neering, social sciences, arts & humanities. International Bibliography of the Social Sciences (IBSS) ProQuest 26/01/20 21 134 Leading online research tool for the Social Sciences and related interdisciplinary subjects. It is a detailed index of over 2 million journal articles, book reviews and selected chapters published since 1951 and it is updated weekly. Communication and Mass Media Complete (CMMC) EBSCO 26/01/20 21 127 Robust communication studies database. It provides full -text, indexing and abstracts for many top communication journals covering all related disciplines, including media studies, linguistics, rhetoric and discourse. PsycINFO OvidSP 26/01/20 21 155 World's largest resource devoted to peer -reviewed literature in behavio ural science and mental health. Produced by the American Psychological Association, it is an indispensable tool for the discovery of global scholarly research. Medline OvidSP 26/01/20 21 429 World's leading bibliographic source for biomedical scholarly literature and research. Created by the United States National Library of Medicine, MEDLINE is an authoritative bibliographic database containing citations and abstracts for biomedical and health journals used by health care professionals, nurses, clinicians and researchers engaged in clinical care, public health and health policy development. GreenFILE EBSCO 26/01/20 21 11 Research database covering all aspects of human impact to the environment. Its collection of scholarly, government and general- interest titles includes content on global warming, green building, pollution, sustainable agriculture, renewable energy, recycling, and more. International Political Science Abstracts (IPSA) EBSCO 26/01/20 21 7 An indispensable tool for work in the fields of political science, political sociology, political psychology, political communications, international relations, international law, human rights, conflict studies, ethnic s tudies and related fields. Politics Collection (PAIS Index, Political Science Database, Worldwide Political Science Abstracts) ProQuest 26/01/20 21 94 Covering political science and public policy. This collection provides access to renowned databases such as PAIS and WPSA, covering the international literature in political science and public administration/policy, along with related fields. British Education Index EBSCO 26/01/20 21 29 Covers all aspects of educational policy and administration, evaluation and assessment, technology and special educational needs. 38 Database Interface Date of search Hits Description The Education Resources Information Centre (ERIC) EBSCO 26/01/20 21 138 Authoritative database of indexed and full -text education literature and resources. Sponsored by the Institute of Education Sciences of the US Department of Education. Coverage dating back to 1966. Education Abstracts EBSCO 26/01/20 21 142 Education research database providing high -quality indexing and abstracts for hundreds of journals. Coverage spans all levels of education and includes adult education, multicultural education and teaching methods. Library, Information Science & Technology Abstracts (LISTA) EBSCO 26/01/20 21 126 Covers library science, information science and related fields. SocINDEX EBSCO 26/01/20 21 59 Covers a broad range of studies, including gender studies, criminal justice, social psychology, religion, racial studies and social work. Business Source Complete EBSCO 26/01/20 21 141 Essential tool for business researchers. It covers all disciplines of business, including marketing, management, accounting, banking, finance and more. ABI/INFORM ProQuest 26/01/20 21 156 Connects business researchers with more of the scholarly information that they need. ABI/INFORM Global contains the full text of thousands of journals, including essential scholarly journals and the most important trade journals. Total results combined : 4028 Duplicates: 2261 Results for screening: 1767 Study selection We created detailed screening criteria including a seven-step process of decision -making. The exclusion criteria were identified based on the scope of the review. A screening tool was produced which clarifies the decision- making and how the criteria should be operationalised (see Table 3: Screening tool). The seven exclusion criteria were ranked from the easiest to the hardest to apply and were used in hierarchical order - selecting the highest - raking criteri on (reason for exclusion) that applies. The reasons for exclusion were recorded for each study that is removed from the sample, which enabled us to report on the number of exclusions per reason at the stage of abstract screening and full -text screening. To allow the thorough recording and monitoring of the screening process, we used a specialised software - Rayyan, which also facilitated reliability checks and reporting. All team members received training on how to use Rayyan. All studies were imported in Rayyan for coding. A separate library with a sub -sample of 20 studies was created for testing the screening process. The (in/ex)clusion criteria and the screening tool were tested on a sample of 20 studies. The studies were purposefully selected to cove r examples likely to fall under \"inclusion\", \"exclusion\" and borderline (\"maybe\") categories. A team of 5 researchers test -coded the 20 studies, each researcher coding all studies blindly. While screening, each researcher also made notes of changes and improvements to the coding tool, the (in/ex)clusion criteria, and overall issues with making the decisions about inclusion or exclusion. 39 Table 3: Screening tool No Inclusion criteria Actions Rayyan exclusion reason More details No Yes Maybe Don't know 1 Is this paper in English? Exclude Include Discuss/ Include Maybe 1. Foreign language Exclude anything that is not in English. 2 Is this paper a peer -reviewed article, a book, or a peer -reviewed chapter? Exclude Include Discuss/ Include Maybe 2. Wrong publication type Exclude anything that is not a peer -reviewed article, a book, or a chapter (electronic): e.g. no PhD theses, no book reviews, no conference papers or proceedings. 3 Is this paper about misinformation or disinformation? Exclude Include Discuss/ Include Maybe 3. Not about misinformation/ disinformation Exclude anything that is not about the intentional or unintentional spread of incorrect information. 4 Is this paper about information or media-related literacy (e.g. learning, skills, competencies, awareness, attitudes)? Exclude Include Discuss/ Include Maybe 4. Not about media literacy Exclude anything that does not discuss some information - and/or media-related literacy. Literacy should be applied in the broad sense: learning, skills (like when to trust info or not; check ing facts), awareness, attitudes, critical thinking etc. 5 Is this paper about an intervention (e.g. a programme, a website, an experiment, an information campaign, a change in content (e.g. flagging by platforms)? Exclude Include Discuss/ Include Include 5. No intervention Exclude anything that does not have an element of \"change\"; something happening that helps people learn from, get informed or tries to alter the flow of mis/disinformation for users. Intervention should be applied in the broad sense: a programme, a website, an experiment, an information campaign, a change in content (e.g. flagging by platforms) etc 6 Does this paper address effectiveness of the intervention? (number of people with improved outcomes, what elements are successful/i mpactful) Exclude Include Discuss/ Include Include 6. No effectiveness Exclude anything that does not measure how effective the intervention is. Again, measuring effectiveness is understood in a broad sense - showing how many people are doing better follow ing an intervention; showing what types of elements (of an intervention, website, social media campaign) help fight misinformation, etc. 7 Does this study use a robust and suitable for our review methodology? (e.g. clear research questions, appropriate and high-quality methods, good sampling, ethical recruitment and research, justified conclusions, clear definition of misinformation and clear measurements) Exclude Include Discuss/ Include Include 7. Poor or unsuitable methodology Exclude thin gs that you find methodologically week or unfit for the purpose of the study. Your judgments will vary based on the type of study but could include: unclear findings or methods of analysis, unclear sample, poor or vague recruitment strategy, poor or unclear measures, unclear definition of misinformation or literacy, unjustified conclusions. All coders discussed their screening suggestions and any challenges experienced during the coding process. Based on this, the screening tool was refined and clarified further. This was mostly related to a clearer description of what might be considered an intervention (used in a broad sense to incorporate studies that discuss case studies or examples) and the minimum criteria regarding media literacy and misinformation (both used in an inclusive way to retain studies which offer some discussion of the relat ionship between media literacy and misinformation). Any differences between the decisions made by the coders were discussed and further guidance on coding was produced to maximise inter -coder reliability. The screening was carried out in two stages - based on abstract and then on full text. A total of N 1=1,767 studies were screened based on the abstract. This was carried out by three researchers who each blind - screened approximately a third of the studies allocating them to one of three groups: included, excluded and maybe. All included and maybe studies were blindly screened by a second person and any inconsistencies were discussed and reconciled. A randomly selected sample from the excluded studies (10% of the scanned by each researcher) was also screene d by a second person. This resulted in three studies being added back to the inclusions. At the abstract stage , a total of 1 ,492 studies were excluded (See Figure: PRISMA flow diagram for a breakdown by exclusion criteria). The exclusions mainly related to articles on health and online information, information security, app or technology -related solutions (e.g. fact -checking), t he role of libraries, or political propaganda, which did not discuss sufficiently misinformation or media literacy. Other exclusions include publications that look at knowledge/ awareness/ literacy related to particular issues (vaccination, health, environ ment) but were descriptive and did not have a sufficient literacy or intervention angle. The search also captured a number of philosophical articles about the nature of \"truth\", which were deemed irrelevant to the review. The exclusions can also be explained by the substantial number of publications that discuss misinformation in general terms while setting the context of the study or conclude with a call for more media literacy education but do not cover substantially either misinformation and/or media lit eracy. This resulted in 275 studies being included in the second screening stage - based on full text. After removing the studies with no access to full text (n=11), a total of 264 publications were screened based on full text, applying the same exclusio n criteria. The full text allowed more precise decisions about relevance and methodological robustness. A further 63 studies were removed for not meeting the inclusion criteria resulting in a final sample of N=201 studies that were retained for analysis. Results We identified the relevant results following the PRISMA method ( Moher, et al., 2015). 41 Figure 1: PRISMA flow diagram 42 The final sample of studies w as coded into the following themes based on the main focus of the discussion: 1. Media literacy interventions addressing misinformation (n=35) 2. Technical interventions addressing misinformation (n=61) 3. Media literacy practices (n=62) 4. Audience response (n=43) Groups 1 and 2 (n=96) are directly relevant to the scope of the rapid evidence review and were analysed in detail using an analytical framework developed especially for this review (see Table 4: Analytical Framework). See Appendix 2 ( a separate document) f or the coded studies. Table 4: Analytical framework Reference Add reference Theme Code the main theme of the publication Research methods Describe the research method used Country of data collection If the study uses primary research with participants enter the country/ies where the data was collected. Age of sample Age range of participants, enter mean age if range not available Sample size Enter the size of the sample o n which the analysis is based Overall findings Summarise the key findings Definition and measurement of misinformation Briefly explain how misinformation if measured/ defined Definition and measurement of media literacy Briefly explain how media literacy is measured Findings on t he relationship between misinformation & literacy Discuss the findings which explain the relationship between misinformation and media literacy Intervention (type, target audience, description) Describe what the intervention involved Findings on the int ervention outcomes Discuss what the intervention found and how effective it was How evaluated Describe the evaluation method 43 Appendix 2: REA article database The database of articles included in the REA is issued as a separate document, in conjunction with this report. 44 Appendix 3: Media literacy practices: detailed report As definitions of media literacy remain \"fluid and contested\" (Bulger & Davison, 2018, p. 4) , it is not surprising that the studies on media literacy techniques and strat egies operationalise media literacy differently. Some studies include narrower or more specific definitions, for example news literacy, civic media literacy (Middaugh, 2018) , while others use more comprehensive approaches, such as critical thinking. A revi ew of media literacy program mes found that they also include a variety of settings, actors, and rationales for media literacy (Bulger & Davison, 2018) , which is supported by our findings. Overall, the studies show that there is a lack of comprehensive ev aluation data on media literacy efforts. This is mainly due to the difficulty of collecting such data and capturing tangible and long- term effects. For example, randomized control trials for curricular testing are hard to do, most of the studies measure single courses a nd use one-time measures (Bulger & Davison, 2018) , very few studies measure the relationship between misinformation and media literacy, and many efforts operationalise media literacy differently. In addition, capturing the effects of media literacy activit ies is difficult (Encheva et al., 2020) . Some research shows that media literacy efforts can have little or no impact, or even produce negative effects related to overconfidence or backfire effects (Bulger & Davison, 2018) . Correspondingly, the studies in this section mainly represent a \"proof of concept\" approach through small -scale case studies and recommendations based on personal experience, rather fully developed interventions with outcome evaluations. Therefore, they should be considered mostly as off ering promising insights and creative solutions and further research should aim to establish the effectiveness of such techniques . The studies on techniques and strategies in our sample relate mainly to the experiences of children and young people. However, most of this literature focuses on further education settings (Al-Abdullatif & Gameil, 2020; Delellis & Rush, 2018; Wade & Hornick, 2018) and less often on children of secondary school age (Bonney, 2018; Fash, 2020) . Primary school studies are very rare in our sample. A substantial number of studies focuses on the role of librarians and/or library -based education (Bluemle, 2018; Dahri & Richard, 2018; Gibson & Jacobson, 2018; Musgrove, Powers, Rebar, & Mu sgrove, 2018) . Smaller numbers discuss other professionals, such as teachers (Baildon & Damico, 2011; Baxa & Christ, 2018) and examine fact- checks (Conrado et al., 2016; Wells, 2018) . Media literacy approaches and techniques By reviewing the studies on media literacy approaches, we identified distinct approaches based on the main learning techniques : critical thinking, credibility verification, media competence development, integrating media and digital literacy, cross -context literacy, systemic approaches , and empowerment. While these approaches have distinctive features, they are often used in a combination in media literacy programmes and should not be seen as exclusive. Alternative ways of classifying the content of media literacy initiative s exist. For example, in an overview of 11 massive open online courses (MOOCs) on information literacy, Dreisiebner (2019) found that the most common themes covered by the content include: (1) determining the nature and extent of needed 45 information; (2) accessing needed information effectively; (3) evaluating information and its sources critically; (4) using information effectively; (5) understanding the economic, legal and social issues surrounding information use (e.g. ethical issues, commerce). Dreisiebner (2019 ) also identified a range of \"additional topics\", amongst which featured fake news and media consumption. Our classification of media literacy approaches bears some resemblance to the features identified by Dreisiebner (2019) but reflect s the broader focus of media literacy in comparison to Dreisiebner's (2019) information lite racy. Below we discuss each of the media literacy approaches and techniques we identified. Critical thinking Media literacy is \"traditionally conceived as a process or set of skills based on critical thinking\" (Bulger & Davison, 2018, p. 1) , therefore, it is not surprising that one of the most prominent approaches to media literacy involves some form of critical thinking development (Cerf, 2019; Franco, Marques Vieira, & 2017) . Critical thinking is discussed by the studies in more narrow terms, for example in relation to asking questions about information gathered online or offline (Cerf, 2019) or developing evidence -based arguments and using them to question online information (Wells, 2018) , as well as broader definitions related to learning awareness and reflection (Schmitt et al., 2018) or critically applying information, technology, and media to learning (Bryan, 2018) . Indicative findings from one study suggest the usefulness of \"scenario theory and practice\" for developing critical thinking (Gramigna & Marling, 2018) . Scenario analysis revolves around 'what if' questions and an invest igation of different possible, probable and hypothetical options. These tools foster critical thinking by teaching self -reflection to assess the present (Gramigna & Marling, 2018) . Another study , carried out with high school students in Finland, compared t wo models of teaching critical literacy - one where critical thinking skills are taught as a separate course and also embedded into other subjects , and a second where critical thinking is only delivered as an embedded topic in other subjects (Horn & Veermans, 2019) . Pre- and post -survey results showed that the first method was more effective in enhancing students' ability to evaluate sources and assess the quality of evidence in materials they read. Credibility verification Another prominent approach relates to verifying the reliability and credibility of resources and checking the accuracy of information on the internet (Al -Abdullatif & Gameil, This usually involves some analytical skills (Dyrendal & Jolley, 2020) or information and source evaluation skills (Weber & Hagan, 2020; Wineburg & McGrew, 2019) , as well as the ability to make judgements about accuracy and credibility (Hodgin & Kahne, 2018) . Examples of this approach include detecting misleading media information, being able to judge the credibility of research studies (Jones, 2018), using conspiracy theory/rumour debunking techniques (Dyrendal & Jolley, 2020) , or applying information verification tools (Conrado et al., 2016) . Hodgin and Kahne (2018) describe a pedagogical approach designed to develop the capacity to judge the accuracy and credibility of online information. Steps include developing nuanced skills and strategies (mov ing beyond hard and fast rules or rote checklists, develop ing critical inquiry questions), reflecting on 46 one's own thought process es and opinions , and practicing across settings and contexts, integrat ing within the core curriculum) . Media competence development This approach relates to developing competence about the media ecology, different media formats, and information navigation (Conrado et al., 2016; Frolova et al., 2018) . Some of the examples here relate to skills required to properly seek, access, understand and apply information found online (Azlan, 2019; Fash, 2017) or using \"journalistic questions\" (of what, who, where, when, why, and how) when evaluating online sources (Elmwood, 2020) . Part of this ap proach is related to news literacy (Fash, 2017; Sivek, 2018; Sperry, 2018; Wade which can refer to reappraising plausibility judgments when evaluating the connections between sources of information and knowledge claims (Sinatra and Lombar di, 2020), understanding emotional responses to news (e.g. via mindfulness techniques or psychological approaches to thinking processes (Sivek, 2018), or evaluating bias (Sperry, 2018) . In some cases, this approach refers to the wider media ecology and iss ues like responsible and ethical consumption of information (popular, news, social media) (Grombly & Anderson, 2020) or the development of knowledge about information resources and systems or developing awareness of the media space and its components (Frol ova et al., 2018) . Integrating media and digital literacy A number of studies pointed to the interconnectedness of media literacy and digital literacy - suggesting that skills related to one of these spheres impact the other (Jun & Pow, 2011) but also that awareness of the digital ecology is integral to understanding the media ecosystem (Valtonen et al., 2019) . Lee and Soep (2016) use the term \"critical computational literacy\" and argue that this is a new pedagogical and conceptual framework th at combines critical literacy and computational thinking. Valtonen et al. (2019) point out how automation takes over media processes such as production, content generation, curation, delivery, recommendation, and filtering of information and argue that. Th ey suggest that media literacy education needs understanding of algorithm- driven media and \"re-think[ing] the connections between media literacy education and computing education\" (Valtonen et al., 2019, p. 20) . Similarly, Azlan (2019) argues that literacy related to e -health incorporates the skills of traditional health literacy (ability to access, understand, process and apply health information), as well as the domains of access and capabilities in navigating digital spaces. More details on the relations hip between media and digital literacy is offered by Kozyreva and colleagues (2020) who review behavioural and cognitive interventions to help users navigate the challenges of the digital environment. In particular, they note the value of 'boost ' intervent ions, which enhance both individual agency (e.g., self -nudging, deliberate ignorance) and motivational competencies to act (e.g. decision aids, inoculation) in digital environments (Kozyreva et al., 2020) . Boosts, they argue, are promising techniques becau se they are easily used to enhance user cognition (e.g. by providing additional information for decision -making), and can support individual empowerment online, particularly when used in conjunction with other techniques such as nudges and regulation. Moreover, boosts are transparent and so are not imposed on users - there is no obligation to use the information provided . Cross -context literacy Following the principle of integrating media and digital literacy, another group of studies looks at the connections between academic/scientific and non -academic/scientific environments. T his body of 47 literature explores media literacy in relation to scient ific knowledge (Bonney, 2018; Crist, Duncan, & Bianchi, 2017; Frisch et al., 2013; Sinatra & Lombardi, 2020) or academic and research competencies (Delellis & Rubin, 2018; Grombly & Anderson, 2020; Kaufman, 2020) . These studies point to the relationship between research/ academic/ scientific competencies (or \"scientific literacy\", Bonney, 2018) and media literacy in relation to finding information , close -reading, critical disposition, evaluating and using appropriate sources and information, bias awareness, and appropriate dissemination (e.g. in scientific language or format) (Crist et al., 2017; Delellis & Rubin, 2018; Frisch et al., 2013; Sinatra & Lombardi, 2020) . The studies make the case for a broader understanding of information use, evaluation and pro duction that connects media consumption and academia (Kaufman, 2020) , given that the skills needed to engage with academic and non -academic sources are transferable (Grombly & Anderson, 2020) . A specific approach to teaching \"scientific literacy\" is descr ibed by Bonney (2018) . When teaching climate change, vaccination and evolution to students, he integrates cultural cognition theories that help students understand how social cues, religion, and political ideologies shape perception of science and promot enthusiastic debate (Bonney, 2018) . Systemic approaches A number of studies conect media literacy to social and historical processes in a society or the power relations behind the media economy. For example, a US-based case study of media literacy suggests that teaching competencies related to fake news need to consider the wider socio -historic context in which misinformation occurs (Manfra & Holmes, 2020) . Alongside efficacy in using tools for detecting fake news and misinformation, this model includes expl oring the history of fake news in the country's history, looking at contemporary examples, tracing the history of the field of journalism and journalistic ethics, and connecting media literacy with the purposes of social studies education (Manfra & Holmes, 2020) . Other studies argue that media literacy needs to include a complex understanding of media in society as a whole, including the the broader structures, actors, and social and cultural context and values underpinning information systems, and awareness of the roles and power of different actors in the information cycle (Bryan, 2018; Frolova et al., 2018; Pennell & Fede, 2018; Rush, 2018) . Empowerment Some studies reflect a movement of media literacy \"away from protection or inoculation and toward empowerment\" (Bulger & Davison, 2018) , discussing the crucial role of media literacy for civic participation and citizenship (Al -Abdullatif & the term \"civic media literacy\" to refer to the process of searching, credibility analysis and circulating information for the purposes of advocacy, which also involves ethically and responsibly sharing information (Middaugh, 2018) . In this sense media literacy includes civic elements and opens up participatory opportunities (Middaugh, 2018) . Another study referred to the need for culturally-specific media literacy in order to empower students. This can be achieved through allowing them to explore alternat ive explanations to their own, address ing their questions, and foster ing sensitivity to the specific way they consume and evaluate news and information (Kaufman, 2020) . 48 Challenges to media literacy The studies we reviewed suggest several challenges to media literacy, ether in relation to outcomes of existing interventions, unfavourable learning conditions (for example, due to the complexity of the information environment), or the spread of misinformation. The challenges can be grouped around several themes . Environmental c hallenges (digital ecology, media ecosystem, political climate) Online functionalities such as persuasive and manipulative choice architectures, AI-assisted information architectures , deepfakes and dynamic information presentation (e.g. non-linear hypertext, multimedia, and interactive text features) , have all made it difficult to understand the flow of information and its origins (Baildon & Damico, 2011; Kozyreva et al., 2019; Walker & Gutsche, 2019) . The ease of sharing information on numerous channels and audiences makes the removal of misinformation virtually impossible (Schmitt et al., 2018) . Similarly, there are media -related challenges linked to the post -truth environment, where the notion of tr uth has itself become a politicised concept and a focus for 'truth games' (Harsin, 2015) rather than an absolute reality (Bluemle, 2018; Rosenzweig, 2017; Sinatra & Lombardi, 2020) and to the accelerating fragmentation of media and information ecosystems (Gibson & Jacobson, 2018) . Further difficulties relate to polarisation in political life and the growing distrust in democratic institutions (Bonney, 2018; Hodgin & Kahne, 2018) and the media (Walker & Gutsche, 2019) . Dated media information strategies Traditional approaches such as strategies for sourcing, contextualizing, and corroborating texts do not work well in the dynamic digital environment (Baildon & Damico, 2011) . Digital infrastructures also rely on emotion analytics (e.g. sentiment analysis) but news and media literacy education traditionally tends to focus on the significance of facts, sourcing, and verifiability while the role of emotion in news consumption remains marginal (Sivek, 2018) . Application of knowledge to practice Translating media literacy knowledge to appropriate behaviours is a challenging task, especially for children (Jain & Bickham, 2014) . Existing media literacy approaches tend to be biased towards critical thinking and not behaviour (Bulger & Davis on, 2018; Jeong et al., 2012) , and effects on behaviour change are much less documented (Bulger & Davison, 2018) . Need for new and diverse formats In a rapidly changing digital environment, media literacy needs to \"keep up\" to maintain the interest of learners, especially younger generations. Several studies reported on the need to widen the formats of media literacy delivery to include new and more dynamic elements , user -centred design, and blended offline/online learning. Some examples described include gamified activities, memes, online comments, adapting evaluation techniques, links posted on social media (Encheva et al., 2020; Johnson, 2018; Kheak Hui & Liew, 2018) . In summary, t he diversity of the approaches summarised here demonstrates the substantial effort dedicated to the development of media literacy techniques and the great potential of this work. However, the lack of evaluation evidence, the inconsistency of definitions and outcomes, and the specificity of many 49 of the se interventions limits significantly the lessons learned about the relationship between media literacy and misinformation. As Bulger and Davison (2018, p. 2) argue, \"Media literacy, however, cannot be treated as a panacea. Media literacy is just one frame in a complex media and information environment\". Therefore, media literacy solutions need to be part of more comprehensive literacy education and supplemented by regulatory and practical solutions. 50 Appendix 4: Studies on media literacy techniques and on audience response Media literacy practices (n=62) Al-Abdullatif, A. M., & Gameil, A. A. (2020). Exploring students' knowledge and practice of digital citizenship in higher education. International Journal of Emerging Technologies in Learning, 15(19), 122-142. Azlan, A. A. (2019). Measures of ehealth literacy: Options for the Malaysian population. Jurnal Komunikasi: Malaysian Journal of Communication, 35 (4), 211-228. Baildon, M., & Damico, J. (2011). Judging the credibility of internet sources: Developing critical and reflexive readers of complex digital texts. Social Education, 75 (5), 269-273. Baxa, J., & Christ, T. (2018). The digilit framework. Reading Teacher, 71 (6), 703- 714. Bluemle, S. R. (2018). Post -facts: Information literacy and authority after the 2016 election. Portal: Libraries & the Academy, 18 (2), 265-282. Bonney, K. M. (2018). Fake news with real consequences: The effect of cultural identity on the perception of science. American Biology Teacher (University of California Press), 80 (9), 686-688. Bryan, L. (2018). Media literacy & the AASL standards. Knowledge Quest, 47 (1), 39-44. Bulger, M., & Davison, P. (2018). The promises, challenges and futures o f media literacy. Journal of Media Literacy Education, 10 (1), 1 -21. Caviglia, F., & Delfino, M. (2016). Foundational skills and dispositions for learning: An experience with information problem solving on the web. Technology, Pedagogy & Education, 25 (4), 487-512. Cerf, V. G. (2019). Hazards of the information superhighway. Association for Computing Machinery. Communications of the ACM, 62(11), 5. Conrado, S. P., Neville, K., Woodworth, S., & O'Riordan, S. (2016). Managing social media uncertainty to supp ort the decision making process during emergencies. Journal of Decision Systems, 25 , 171- 181. Cooper, T. (2019). Calling out 'alternative facts': Curriculum to develop students' capacity to engage critically with contradictory sources. Teaching in Higher Education, 24 (3), 444- 459. Dahri, R. B. M., & Richard, H. C. Y. (2018). Librarians joining the fight against fake news: A NUS case study. Singapore Journal of Library & Information Management, 47 , 15-24. De Paor, S., & Heravi, B. (2020). Information lite racy and fake news: How the field of librarianship can help combat the epidemic of fake news. Journal of Academic Librarianship, 46 (5), N.PAG -N.PAG. Delellis, N. S., & Rubin, V. L. (2018). Educators' perceptions of information literacy and skills required to spot 'fake news'. Proceedings of the Association for Information Science and Technology, 55 (1), 785-787. Dreisiebner, S. (2019). Content and inst ructional design of MOOCS on information literacy : A comprehensive analysis of 11 xMOOCS . Information and Learning Sciences, 120 (3), 173-189. Dyrendal, A., & Jolley, D. (2020). Conspiracy theories in the classroom: Problems and potential solutions. Religi ons, 11(10). 51 Elmwood, V. (2020). The journalistic approach: Evaluating web sources in an age of mass disinformation. Communications in Information Literacy, 14 (2), 269-286. Encheva, M., Tammaro, A. M., Kumanova, A., & ra. (2020). Games to improve student s information literacy skills. International Information & Library Review, 52 (2), 130-138. Fash, L. G. (2017). Information literacy in the A merican Literature classroom. Transformations: The Journal of Inclusive Scholarship & Pedagogy, 27 (2), 195-201. Franco, A., Marques Vieira, R., & Tenreiro -Vieira, C. (2018). Educating for critical thinking in university: The criticality of critical thinking in education and everyday life. ESSACHESS - Journal for Communication Studies, 11 (2), 131-144. Friesem, Y., & G utsche, R. E. (2019). Teaching truth, lies, and accuracy in the digital age: Media literacy as project -based learning. Journalism & Mass Communication Educator, 74 (2), 185-198. Frisch, J. K., Jackson, P. C., & Murray, M. C. (2013). Research and teaching: Wikied --using web 2.0 tools to teach content and critical thinking. Journal of College Science Teaching, 43 (1), 70-80. Frolova, E., Ryabova, T., & Rogach, O. (2018). Electronic educational environment as the tool of manager student media competence develo pment. Mediaobrazovanie -Media Education (1), 68-76. Gibson, C., & Jacobson, T. E. (2018). Habits of mind in an uncertain information world. Reference & User Services Quarterly, 57 (3), 183-192. Gramigna, R., & Marling, R. (2018). Scenario as a tool for cri tical thinking: Climate change awareness and denial as a case study. ESSACHESS - Journal for Communication Studies, 11 (2), 67-84. Grombly, A., & Anderson, A. (2020). Information and media literacy: Integrating literacies into library instruction. Media Li teracy and Academic Research, 3 (1), 6 -17. Hobbs, R., Seyferth -Zapf, C., & Grafe, S. (2018). Using virtual exchange to advance media literacy competencies through analysis of contemporary propaganda. Journal of Media Literacy Education, 10 (2), 152-168. Hodgin, E., & Kahne, J. (2018). Misinformation in the information age: What teachers can do to support students. Social Education, 82 (4), 208-212. Horn, S., & Veermans, K. (2019). Critical thinking efficacy and transfer skills defend against 'fake news' at an international school in Finland. Journal of Research in International Education, 18 (1), 23-41. Johnson, M. (2018). Fighting \"fake news\": How we overhauled our website evaluation lessons. Knowledge Quest, 47 (1), 32-36. Jones, E. K. (2018). \"All lies ma tter!\": Revealing misleading information in media stories about police brutality. Multicultural Education, 25 (3), 41-46. Jun, F., & Pow, J. (2011). Fostering digital literacy through web -based collaborative inquiry learning --a case study. Journal of Infor mation Technology Education, 10 . Kaufman, C. (2020). Civic education in a fake news era: Lessons for the methods classroom. Journal of Political Science Education. Kheak Hui, H., & Liew, A. (2018). Transforming information literacy programmes - a design thinking approach. Singapore Journal of Library & Information Management, 47 , 25-34. Kiili, C., Coiro, J., & Raikkonen, E. (2019). Students' evaluation information during online inquiry: Working individually or in pairs. Australian Journal of Language and Literacy, 42 (3), 167-183. Kozyreva, A., Lew andowsky, S., & Hertwig, R. (2020). Citizens versus the internet: Confronting digital challenges with cognitive tools. Psychological Science in the Public Interest, 21 (3), 103-156. 52 Ku, K. Y. L., Kong, Q., Song, Y., Deng, L., Kang, Y., & Hu, A. (2019). What predicts adolescents' critical thinking about real -life news? The roles of social media news consumption and news media literacy. Thinking Skills and Creativity, 33 . Lee, C. H., & Soep, E. (2016). None but ourselves can free our minds: Critical computational literacy as a pedagogy of resistance. Equity & Excellence in Education, 49 (4), 480-492. Leland , C., Ociepka, A., Kuonen, K., & Bangert, S. (2018). Learning to talk back to tex ts. Journal of Adolescent & Adult Literacy, 61 (6), 643-652. Manfra, M., & Holmes, C. (2020). Integrating media literacy in social studies teacher education. Contemporary Issues in Technology and Teacher Education (CITE Journal), 20 (1). Middaugh, E. (2018 ). Civic media literacy in a transmedia world: Balancing personal experience, factual accuracy and emotional appeal as media consumers and circulators. Journal of Media Literacy Education, 10 (2), 33-52. Musgrove, A. T., Powers, J. R., Rebar, L. C., & Musgrove, G. J. (2018). Real or fake? Resources for teaching college students how to identify fake news. College & Undergraduate Libraries, 25 (3), 243-260. O'Sullivan, M. (2011). TRUTHFUL : A method to assist patients with evaluating health information on the internet. Online Journal of Nursing Informatics, 15 (3). Pek, S., & Damien, W. (2018). National Library Board's public education on information literacy: Teaching citizens to fight fake news. Singapore Jo urnal of Library & Information Management, 47, 2 -14. Piedade, F., Malafaia, C., Neves, T., Loff, M., & Menezes, I. (2020). Educating critical citizens? Portuguese teachers and students' visions of critical thinking at school. Thinking Skills and Creativit y, 37 . Piro, J. S., & Anderson, G. (2018). Intentional online discussions in teacher education. Teacher Educator, 53(2), 167-189. Ranieri, M., Nardi, A., & Fabbro, F. (2019). Teachers' professional development on media and intercultural education. Result s from some participatory research in europe. Research on Education and Media, 11 (1), 109-120. Reed, K., Hiles, S. S., Tipton, P., & Gutsche, R. E. (2019). Sense and nonsense: Teaching journalism and science students to be advocates for science and inform ation literacy. Journalism & Mass Communication Educator, 74 (2), 212-226. Rinne, N. A. (2017). The new framework: A truth -less construction just waiting to be scrapped? Reference Services Review, 45 (1), 54-66. Rosenzweig, A. (2017). Understanding and und ermining fake news from the classroom. Berkeley Review of Education, 7 (1), 105-112. Rush, L. (2018). Examining student perceptions of their knowledge, roles, and power in the information cycle: Findings from a 'fake news' event. Journal of Information Lit eracy, 12(2), 121-130. Schmitt, J. B., Rieger, D., Ernst, J., & Roth, H. J. (2018). Critical media literacy and I slamist online propaganda: The feasibility, applicability and impact of three learning arrangements. International Journal of Conflict and Vio lence, 12 . Sinatra, G. M., & Lombardi, D. (2020). Evaluating sources of scientific evidence and claims in the post - truth era may require reappraising plausibility judgments. Educational Psychologist, 55 (3), 120- 131. Sivek, S. C. (2018). Both facts and fe elings: Emotion and news literacy. Journal of Media Literacy Education, 10 (2), 123-138. 53 Sperry, S. (2018). News literacy lesson #1: There's nothing new about \"fake news\". Social Education, Valtonen, T., Tedre, M., M\u00e4kitalo, K., & Vartiain en, H. (2019). Media literacy education in the age of machine learning. Journal of Media Literacy Education, 11 (2), 20-36. Wade, S., & Hornick, J. (2018). Stop! Don't share that story!: Designing a pop -up undergraduate workshop on fake news. Reference Librarian, 59 (4), 188-194. Walker, A. S., & Gutsche, R. E. (2019). Preparing students for the fight against false information with visual verification and open source reporting. Journalism & Mass Communication Educator, 74(2), 227-239. Weber, C. A., & Hagan , H. N. (2020). Is the \"right to clean water\" fake news? An inquiry in media literacy and human rights. Social Studies and the Young Learner, 33 (1), 3 -9. Wells, D. D. (2018). You all made dank memes: Using internet memes to promote critical thinking. Journal of Political Science Education, 14 (2), 240-248. Wineburg, S., & McGrew, S. (2019). Lateral reading and the nature of expertise: Reading less and learning more when evaluating digital information. Teachers College Record, 121 (11) Audience response (n=43) Baxter, G., Marcella, R., & Walicka, A. (2019). Scottish citizens' perceptions of the credibility of online political \"facts\" in the \"fake news\" era: An exploratory study. Journal of Documentation, 75 (5), 1100-1123. Buchanan, T. (2020). Why do people s pread false information online? The effects of message and viewer characteristics on self -reported likelihood of sharing social media disinformation. PloS one, 15(10). Chen, X., Sin, S. -C. J., Theng, Y.-L., & Lee, C. S. (2015). Why students share misinfor mation on social media: Motivation, gender, and study -level differences. Journal of Academic Librarianship, 41(5), 583-592. Chu, S. K. W., Lau, W. W. F., Chu, D. S. C., Lee, C. W. Y., & Chan, L. L. H. (2016). Media awareness among Hong Kong primary studen ts. Journal of Librarianship & Information Science, 48 (1), 90-104. Colli, & er, J. (2019). \"This is fake news\": Investigating the role of conformity to other users' views when commenting on and spreading disinformation in social media. Computers in Human Behavior, 97, 202-215. Ecker, U. K. H., Lew andowsky, S., Ee Pin, C., & Pillai, R. (2014). The effects of subtle misinformation in news headlines. Journal of Experimental Psychology. Applied, 20 (4), 323-335. Einav, S., Levey, A., Patel, P., & Westwood, A. (2020). Epistemic vigilance online: Textual inaccuracy and children's selective trust in webpages. British Journal of Developmental Psychology, 38 (4), 566- 579. Evanson, C., & Sponsel, J. (2019). From syndication to misinformation: How undergraduate students engage with and evaluate digital news. Communications in Information Literacy, 13 (2), 228-250. Freiling, I. (2019). Detecting misinformation in online social networks: A think -aloud study on user strategies. SCM Studies in Communication and Media, 8 (4), 471-496. Garrett, R. K., Nisbet, E. C., & L ynch, E. K. (2013). Undermining the corrective effects of media -based political fact checking? The role of contextual cues and na\u00efve theory. Journal of Communication, 63(4), 617-637. 54 Garrett, R. K., & Poulsen, S. (2019). Flagging Facebook falsehoods: Self -identified humor warnings outperform fact checker and peer warnings. Journal of Computer -Mediated Communication, 24(5), 240-258. Gordon, A., Quadflieg, S., Brooks, J. C. W., Ecker, U. K. H., Lew andowsky, S. (2019). Keeping track of 'alternative facts': T he neural correlates of processing misinformation corrections. NeuroImage, 193, 46-56. Gunther, A. C., McLaughlin, B., Gotlieb, M. R., & Wise, D. (2017). Who says what to whom: Content versus source in the hostile media effect. International Journal of Public Opinion Research, 29 (3), 363-383. Head, A. J., DeFrain, E., Fister, B., & MacMillan, M. (2019). Across the great divide: How today's college students engage with news. First Monday, 24 (8), 1 -1. Herrero -Diz, P., Conde -Jimenez, J., Tapia -Frade, A., Varona -Aramburu, D. (2019). The credibility of online news: An evaluation of the information by university students. Cultura Y Educacion, 31 (2), 407-435. Jang, S. M., & Kim, J. K. (2018). Third person effects of fake news: Fake news regulation and media literacy interventions. Computers in Human Behavior, 80 , 295-302. Kahne, J., & Bowyer, B. (2017). Educating for democracy in a partisan age: Confronting the challenges of motivated reasoning and misinformation. American Educational Research Journal, 54 (1), 3-34. Kauttonen, J., Hannukainen, J., Tikka, P., & Suomala, J. (2020). Predictive modeling for trustworthiness and other subjective text properties in online nutrition and health communication. PloS one, 15(8), e0237144. Khaldarova, I., & Pantti, M. (20 16). Fake news the narrative battle over the ukrainian conflict. Journalism Practice, 10 (7), 891-901. Khan, M. L., & Idris, I. K. (2019). Recognise misinformation and verify before sharing: A reasoned action and information literacy perspective. Behaviour & Information Technology, 38 (12), 1194-1212. Kim, M. (2017). How consumer of news do social networking activity through confirmation bias, value relevant involvement and issue relevant involvement: Exploring the implication of news literacy. Turkish Onli ne Journal of Educational Technology, 2017 , 376-382. Laato, S., Islam, A. K. M. N., Islam, M. N., & Whelan, E. (2020). What drives unverified information sharing and cyberchondria during the covid -19 pandemic? European Journal of Information Systems, 29(3 ), 288-305. S., Ecker, U. K. H., Seifert, C. M., Schwarz, N., & Cook, J. (2012). Misinformation and its correction: Continued influence and successful debiasing. Psychological Science in the Public Interest, 13 (3), 106-131. Loos, E., Ivan, L ., & Leu, D. (2018). \"Save the pacific northwest tree octopus\": A hoax revisited. Or how vulnerable are school children to fake news? Information and Learning Sciences, 119 (9), 514- 528. Meirick, P. C. (2013). Motivated misperception? Party, education, par tisan news, and belief in \"death panels\". Journalism and Mass Communication Quarterly, 90 (1), 39-57. Nee, R. C., & Gutsche, R. E. (2019). Youthquakes in a post -truth era: Exploring social media news use and information verification actions among global te ens and young adults. Journalism & Mass Communication Educator, 74 (2), 171-184. Notley, T., & Dezuanni, M. (2019). Advancing children's news media literacy: Learning from the practices and experiences of young Australians. Media, Culture & Society, 41 (5), 689-707. 55 Nygren, T., & Guath, M. (2018). Mixed digital messages: The ability to determine news credibility among swedish teenagers. Proceedings of the 15th International Conference on Cognition and Exploratory Learning in the Digital Age . Budapest, Hungary: International Association for Development of the Information Society. Nygren, T., & Guath, M. (2019). Swedish teenagers' difficulties and abilities to determine digital news credibility. NORDICOM Review, 40(1), 23-42. Paglinawan, W. M . C. (2020). University students engagement with and disengagement from fake news. Media Literacy and Academic Research, 3 (2), 77-87. Ping Yu, R. (2020). How types of Facebook users approach news verification in the mobile media age: Insights from the dual -information -processing model. Mass Communication and Society , 24(2) , 233-258. Scheufele, D. A., & Krause, N. M. (2019). Science audiences, misinformation, and fake new s. Proceedings of the National Academy of Sciences of the United States of America, 116(16), 7662-7669. Shen, C., Kasra, M., Pan, W., Bassett, G. A., Malloch, Y., & O'Brien, J. F. (2019). Fake images: The effects of source, intermediary, and digital media literacy on contextual assessment of image credibility online. New Media & Society, 21 (2), 438-463. Smith, L. N., & McMenemy, D. (2017). Young people's conceptions of political information. Journal of Documentation, 73(5), 877-902. Stover, S., & Mabry, M. (2020). Evaluating information: The impact of major, class standing, and experience with primary literature. Journal of College Science Teaching, 49 (3), 16-21. Szabo, A. (2020). Immediate and persisting effects of controversial media information on you ng people's judgement of health issues. Europes Journal of Psychology, 16 (2), 249-261. Vlasceanu, M., Morais, M. J., Duker, A., & Coman, A. (2020). The synchronization of collective beliefs: From dyadic interactions to network convergence. Journal of Expe rimental Psychology. Applied, 26(3), 453-464. Wagner, M. C., & Boczkowski, P. J. (2019). The reception of fake news: The interpretations and practices that shape the consumption of perceived misinformation. Digital Journalism, 7 (7), 870-885. Wang, L., & Fussell, S. R. (2020). More than a click: Exploring college students' decision-making processes in online news sharing. Proceedings of the ACM on Human-Computer Interaction, 4 . Weeks, B. E. (2015). Emotions, partisanship, and misperceptions: How anger and anxiety moderate the effect of partisan bias on susceptibility to political misinformation. Journal of Communication, 65(4), 699-719. Wenzel, A. (2019). To verify or to disengage: Coping with \"fake news\" and ambiguity. International Journal of Communicat ion (19328036), 13, 1977-1995. Zerback, T., T\u00f6pfl, F., & Kn\u00f6pfle, M. (2020). The disconcerting potential of online disinformation: Persuasive effects of astroturfing comments and three strategies for inoculation against them. New Media and Society , 23(5), 1080-1098. Zollo, F. (2019). Dealing with digital polarised context of narratives and tribes. EFSA Journal: European Food Safety Authority, 17. 56 Appendix 5: Expert interviews Paul Bainsfair , Director General of the Institute of Practitioners in Advertising Professor Charlie Beckett , London School of Economics and Political Science. Director of Polis, the LSE Media Policy Project, the LSE/Polis Journalism/AI project, and Lead Commissioner for the T3 Commission. Specialist in journalism research and practice, AI and misinformation Damian Collins MP, former Chair of the DCMS Sub -Committee on Disinformation, Chair of the DCMS Select Committee Inquiry into Disinformation and Fake News Alina Dimoft e, Public Policy and Government Relations Manager, Google Renee DiResta, Technical Research Manager, St anford Internet Observatory Richard Earle y, Public Policy Manager UK, Facebook Richard Fletcher, Reuters Institute for the Study of Journalism Jennie King , Senior Policy Manager, Institute for Strategic Dialogue Tom Knox , Executive Partner, MullenLowe Group UK and former President, Institute of Practitioners in Advertising Dr David McElroy , Board Member, European Social Marketing Association and Senior Policy Manager, Marine Stewardship Council. Specialist in sustainable behaviour change and experience across t he energy, marine and environmental sectors . Konrad Shek , Director of Policy Research, UK Advertising Association 57 Bibliography Addy, J. M. (2020). The art of the real: Fact checking as information literacy instruction. Reference Services Review, 48 (1), 19-31. Al-Abdullatif, A. M., & Gameil, A. A. (2020). Exploring students' knowledge and practice of digital citizenship in higher education. International Journal of Emerging Technologies in Learning, 15(19), 122-142. Allard-Huver, F. (2017). Between disinformation tactics and deciphering strategies, towards a semio - political M. (2017). Social media and fake news in the 2016 election. Journal of Economic Perspectives, 31 (2), 211-236. Amazeen, M. A., & Bucy, E. P. (2019). Conferring resistance to digital disinformation: The inoculatin g influence of procedural news knowledge. Journal of Broadcasting & Electronic Media, 63 (3), 415-432. Amazeen, M. A., Thorson, E., Muddiman, A., & Graves, L. (2018). Correcting political and consumer misperceptions: The effectiveness and effects of rating scale versus contextual correction formats. Journalism & Mass Communication Quarterly, 95 (1), 28-48. Amazeen, M. A., Vargo, C. J., & Hopp, T. (2019). Reinforcing attitudes in a gatewatching news era: Individual -level antecedents to sharing fact-checks on social media. Communication Monographs, 86 (1), 112-132. Ardi, R. (2019). Partisan selective exposure to fake news content. Makara Hubs -Asia, 23 (1), 6 -16. Azlan, A. A. (2019). Measures of ehealth literacy: Options for the Malaysian population. Jurnal Kom unikasi: Malaysian Journal of Communication, 35 (4), 211-228. Baildon, M., & Damico, J. (2011). Judging the credibility of internet sources: Developing critical and reflexive readers of complex digital texts. Social Education, 75 (5), 269-273. Banas, J. A. , & Miller, G. (2013). Inducing resistance to conspiracy theory propaganda: Testing inoculation and metainoculation strategies. Human Communication Research, 39 (2), 184-207. Barrera, O., Guriev, S., Henry, E., & Zhuravskaya, E. (2020). Facts, alternative facts, and fact checking in times of post -truth politics. Journal of Public Economics, 182 , N.PAG -N.PAG. Basol, M., Roozenbeek, J., & van der Linden, S. (2020). Good news about B ad News: Gamified inoculation boosts confidence and cognitive immunity agains t fake news. Journal of cognition, 3(1). Baxa, J., & Christ, T. (2018). The digilit framework. Reading Teacher, 71 (6), 703- 714. Berinsky, A. J. (2017). Rumors and health care reform: Experiments in political misinformation. British Journal of Political S cience, 47 (2), 241-262. Bluemle, S. R. (2018). Post -facts: Information literacy and authority after the 2016 election. Portal: Libraries & the Academy, 18 (2), 265-282. Bonney, K. M. (2018). Fake news with real consequences: The effect of cultural identity on the perception of science. American Biology Teacher, 80 (9), 686-688. Bontcheva, K., & Posetti, J. (2020). Balancing act: Countering digital disinformation while res pecting freedom of expression . Geneva / Paris: UNESCO Bramer, W. M., Giustini, D., de Jonge, G. B., Holland, L., & Bekhuis, T. (2016). De-duplication of database search results for systematic reviews in Endnote. Journal of the Medical Library Association : JMLA, 104(3), 240-243. Brennen, J. S., Simon, F. M., Howard, P. N., & Nielsen, R. K. (2020). Types, sources, and claims of C ovid-19 misinformation . Oxford: Reuters International Institute for the Study of Journalism. Bryan, L. (2018). Media literacy & the AASL standards. Knowledge Quest, 47 (1), 39-44. Buckingham, D. (2015). Defining digital literacy - what do young people need to know about digital media? Nordic Journal of Digital Literacy, 10 , 21-35. Bulger, M., & Davison, P. (2018). The promises, challenges and futures of media literacy. Journal of Media Literacy Education, 10 (1), 1 -21. 58 Calvo, D., Cano -Oron, L., & Abengozar, A. E. (2020). Materials and assessment of literacy level for the recognition of social bots in political misinformation contexts. Revista Icono Comunicacion Y Tecnologias, 18 (2), 111-136. Cerf, V. G. (2019). Hazards of the information superhighway. Association for Computing Machinery: Communications of the ACM, 62(11), 5. ChildNet International. (2021). Childnet. Retrieved from , A. Y. K., & Banerjee, S. (2017). To share or not to share: The role of epistemic belief in online health rumors. International Journal of Medical Informatics, 108, 36-41. Clayton, K., Blair, S., Busam, J. A., Forstner, S., Glance, J., Green, G., . . . N yhan, B. (2020). Real solutions for fake news? Measuring the effectiveness of general warnings and fact -check tags in reducing belief in false stories on social media. Political Behavior, 42 (4), 1073-1095. Conrado, S. P., Neville, K., Woodworth, S., & O'R iordan, S. (2016). Managing social media uncertainty to support the decision making process during emergencies. Journal of Decision Systems, 25 , 171- 181. Craft, S., Ashley, S., & Maksl, A. (2016). Elements of news literacy: A focus group study of how teen agers define news and why they consume it. Electronic News, 10 (3), 143-160. Craft, S., Ashley, S., & Maksl, A. (2017). News media literacy and conspiracy theory endorsement. Communication and the Public, 2 (4), 388-401. Crist, C. A., Duncan, S. E., & Bianchi, L. M. (2017). Incorporation of cross -disciplinary teaching and a wiki research project to engage undergraduate students' to develop information literacy, critical thinking, and communication skills. Journal of Food S cience Education, 16 (3), 81-91. Dahri, R. B. M., & Richard, H. C. Y. (2018). Librarians joining the fight against fake news: A NUS case study. Singapore Journal of Library & Information Management, 47 , 15-24. Delellis, N. S., & Rubin, V. L. (2018). Educa tors' perceptions of information literacy and skills required to spot 'fake news'. Proceedings of the Association for Information Science and Technology, 55 (1), 785-787. Digital, C ulture, Media and Sport Committee,. (2019). Disinformation and 'fake news': Final report . London: House of Commons. Donovan, A. M., & Rapp, D. N. (2020). Look it up: Online search reduces the problematic effects of exposures to inaccuracies. Memory & Cognition, 48(7), 1128-1145. Dreisiebner, S. (2019). Content and instructional design of MOOCS on information literacy: A comprehensive analysis of 11 xMOOCS . Information and Learning Sciences, 120 (3), 173-189. Durach, F., Bargaoanu, A., & Nastasiu, C. (2020). Tackling disinformation: E U regulation of the digital space. Romanian Jo urnal of European Affairs, 20 (1), 5 -20. Dyrendal, A., & Jolley, D. (2020). Conspiracy theories in the classroom: Problems and potential solutions. Religions, 11 (10). Ecker, U. K. H., Lewandowsky, S., Chang, E. P., & Pillai, R. (2014). The effects of subt le misinformation in news headlines. Journal of Experimental Psychology Applied, 20 (4), 323-335. Ecker, U. K. H., O'Reilly, Z., Reid, J. S., & Chang, E. P. (2020). The effectiveness of short -format refutational fact -checks. British Journal of Psychology, 111(1), S., Mour\u00e3o, R. R., Thorson, E., & Tham, S. M. (2020). When do audiences verify? How perceptions about message and source influence audience verification of news headlines. Journalism & Mass Communication Quarterly, 97 (1), 52-71. Elmwood, V. (2020). The journalistic approach: Evaluating web sources in an age of mass disinformation. Communications in Information Literacy, 14 (2), 269-286. Encheva, M., Tammaro, A. M., & Kumanova, A. (2020). Games to improve students information litera cy skills. International Information & Library Review, 52 (2), 130-138. Eysenbach, G. (2020). How to fight an infodemic: The four pillars of infodemic management. Journal of Medical Internet Research, 22 (6). 59 Falconer, J. (2018). Removing duplicates from an endnote library. Available at: https://blogs.lshtm.ac.uk/library/2018/12/07/removing-duplicates -from -an-endnote-library/ May 2021. L. G. (2017). Information literacy in the A merican Literature classroom. Transformations: The Journal of Inclusive Scholarship & Pedagogy, 27 (2), 195-201. Featherstone, J. D., & Zhang, J. (2020). Feeling angry: The effects of vaccine misinformation and refutational messages on negative emotions and vaccination attitude. Journal of Health Communication, 25 (9), 692-702. Fraillon, J., Ainley, J., Schulz, W., F riedman, T., & Gebhardt, E. (2014). Preparing for life in a digital age: The IEA international computer and information literacy study international report . Retrieved from Melbourne: https://research.acer.edu.au/cgi/viewcontent.cgi?article=1009&context=ict _literacy Franco, A., Marques Vieira, R., & Tenreiro -Vieira, C. (2018). Educating for critical thinking in university: The criticality of critical thinking in education and everyday life. ESSACHESS - Journal for Communication Studies, 11 (2), 131-144. Friesem, Y., & Gutsche, R. E. (2019). Teaching truth, lies, and accuracy in the digital age: Media literacy as project -based learning. Journalism & Mass Communication Educator, 74 (2), 185-198. Frisch, J. K., Jackson, P. C., & Murray, M. C. (2013). Research an d teaching: Wikied --using web 2.0 tools to teach content and critical thinking. Journal of College Science Teaching, 43 (1), 70-80. Frolova, E., Ryabova, T., & Rogach, O. (2018). Electronic educational environment as the tool of manager student media compe tence development. Mediaobrazovanie -Media Education (1), 68-76. Full Fact, Africa Check, & Chequado. (2020). Media and information literacy: Lessons from interventions around the world. London: Full Fact. Get Safe Online. (2021). Fake news. Retrieved from https://www.getsafeonline.org/protecting- yourself/fake -news/ Gibson, C., & Jacobson, T. E. (2018). Habits of mind in an uncertain information world. Reference & User Services Quarterly, 57 (3), 183-192. Giusti, S., & Piras, E. (Eds.). (2020). Democracy and fake news: Information and post -truth politics . Abingdon, Oxon: Routledge. Gramigna, R., & Marling, R. (2018). Scenario as a tool for critical thinking: Climate change awareness and denial as a case study. ESSACHESS - Journal for Communication Studies, 11 (2), 67-84. Grombly, A., & Anderson, A. (2020). Information and media literacy: Integrating literacies into library instruction. Media Literacy and Academic Research, 3 (1), 6 -17. Guoping, Z. (2019). STEM education in the age of \" fake news\": A J ohn Stuart Mill perspective. Philosophy of Education Yearbook , 393-406. Hameleers, M. (2019). Susceptibility to mis - and disinformation and the effectiveness of fact - checkers: Can misinformation be effectively combated? S CM Studies in Comm unication and Media, 8 (4), 523-546. Hameleers, M., Powell, T. E., Van Der Meer, T. G. L. A., & Bos, L. (2020). A picture paints a thousand lies? The effects and mechanisms of multimodal disinformation and rebuttals disseminated via social media. Political Communication, 37 (2), 281-301. Harsin, J. (2015). Regimes of posttruth, postpolitics, and attention economies. Communication, Culture & Critique, 8 (2), 327-333. doi:10.1111/cccr.12097 Hodgin, E., & Kahne, J. (2018). Misinformation in the information age: What teachers can do to support students. Social Education, 82 (4), 208-212. Horn, S., & Veermans, K. (2019). Critical thinking efficacy and transfer skills defend against 'fake news' at an international school in Finland. Journal of Research in International Education, 18 (1), 23-41. Huang, Y., & Wang, W. (2020). When a story contradicts: Correcting health misinformation on social media through different message formats and mechanisms. Information Communication and Society . 60 Instagram. (2020). Our progres s on leading the fight against online bullying. Retrieved from https://about.instagram.com/blog/announcements/our -progress -on-leading -the-fight -against - online-bullying Internet Matters. Journalism, fake news & disinformation: Handbook for journalism education and training (1 ed. Vol. 1). Paris: UNESCO. Jain, A. V., & Bickham, D. (201 4). Adolescent health literacy and the internet: Challenges and opportunities. Current Opinion in P ediatrics, 26 (4), 435-439. Jang, J. -w., Lee, E. -J., & Shin, S. Y. (2019). What debunking of misinformation does and doesn't. CyberPsychology, Behavior & Social Networking, 22(6), 423-427. Jeong, S. H., Cho, H., & Hwang, Y. (2012). Media literacy interventions: A meta -analytic review. Journal of Communication, 62, 454- 472. Johnson, M. (2018). Fighting \"fake news\": How we overha uled our website evaluation lessons. Knowledge Quest, 47 (1), 32-36. Jones, E. K. (2018). \"All lies matter!\": Revealing misleading information in media stories about police brutality. Multicultural Education, 25 (3), 41-46. Jones -Jang, S. M., Mortensen, T., & Liu, J. (2021). Does media literacy help identification of fake news? Information literacy helps, but other literacies don't. The American Behavioral Scientist, 65 (2), 371-388. Jun, F., & Pow, J. (2011). Fostering digit al literacy through web -based collaborative inquiry learning --a case study. Journal of Information Technology Education, 10 , 57-71. Kaniaj, I., & Lechpammer, S. (2019). The role of organisations of journalists in promoting media literacy - building credi bility and trust. Media Literacy and Academic Research, 2 (1), 24-37. Katsaounidou, A., Vrysis, L., Kotsakis, R., Dimoulas, C., & Veglis, A. (2019). M AthE the game: A serious game for education and training in news verification. Education Sciences, 9 (2), 1 55-170. Kaufman, C. (2020). Civic education in a fake news era: Lessons for the methods classroom. Journal of Political Science Education, 17 (2), 326-331. Kertysova, K. (2018). Artificial intelligence and disinformation: How AI changes the way disinforma tion is produced disseminated, and can be countered. Security and Human Rights, 29 (1), 55-81. Kheak Hui, H., & Liew, A. (2018). Transforming information literacy programmes - a design thinking approach. Singapore Journal of Library & Information Management, 47 , 25-34. Kim, A., Moravec, P. L., & Dennis, A. R. (2019). Combating fake news on social media with source ratings: The effects of user and expert reputation ratings. Journal of Management Information Systems, 36(3), 931-968. Kim, S. C., Vraga, E. K. , & Cook, J. (2020). An eye tracking approach to understanding misinformation and correction strategies on social media: The mediating role of attention and credibility to reduce HPV vaccine misperceptions. Health Communication (Online First) . Kirchner, J., & Reuter, C. (2020). Countering fake news: A comparison of possible solutions regarding user acceptance and effectiveness. Proceedings of the ACM on Human-Computer Interaction, 4 . Kozyreva, A., Lewandowsky, S., & Hertwig, R. (2020). Citizens versus the internet: Confronting digital challenges with cognitive tools. Psychological Science in the Public Interest, 21 (3), 103-156. Ku, K. Y. L., Kong, Q., Song, Y., Deng, L., Kang, Y., & Hu, A. (2019). What predicts adolescents' critical thinking ab out real -life news? The roles of social media news consumption and news media literacy. Thinking Skills and Creativity, 33 . LaCaille, R. A., LaCaille, L. J., Damsgard, E., & Maslowski, A. K. (2019). Refuting mental health misconceptions: A quasi -experiment with abnormal psychology courses. Psychology Learning and Teaching, 18 (3), 275-289. Lee, C. H., & Soep, E. (2016). None but ourselves can free our minds: Critical computational literacy as a pedagogy of resistance. Equity & Excellence in Education, 49 (4), 480-492. 61 Leeder, C. (2019). How college students evaluate and share \"fake news\" stories. Library & Information Science Research (07408188), Leland, C., Ociepka, A., Kuonen, K., & Bangert, S. (2018). Learning to talk back to texts . Journal of Adolescent & Adult Literacy, 61 (6), 643-652. Lewandowsky, S., Oreskes, N., Risbey, J., Newell, B., & Smithson, M. (2015). Seepage: Climate change denial and its effect on the scientific community. Global Environmental Change, 33 , 1-13. doi:10 .1016/j.gloenvcha.2015.02.013 Lucassen, T., Muilwijk, R., Noordzij, M. L., & Schraagen, J. M. (2013). Topic familiarity and information skills in online credibility evaluation. Journal of the American Society for Information Science & Technology, 64(2), 254 -264. Lutzke, L., Drummond, C., Slovic, P., & \u00c1rvai, J. (2019). Priming critical thinking: Simple interventions limit the influence of fake news about climate change on facebook. Global Environmental Change Part A: Human & Policy Dimensions, 58 . Maitz, E., Maitz, K., Sendlhofer, G., Kamolz, L. -P., & Gasteiger -Klicpera, B. (2020). Internet -based health information -seeking behavior of students aged 12 to 14 years: Mixed methods study. Journal of Medical Internet Research, 22 (5). Manfra, M., & Holmes, C. (2020). Integrating media literacy in social studies teacher education. Contemporary Issues in Technology and Teacher Education (CITE Journal), 20 (1), 121-141. Mason, L., Junyent, A. A., & Tornatora, M. C. (2014). Epistemic ev aluation and comprehension of web - source information on controversial science -related topics: Effects of a short-term instructional intervention. Computers & Education, 76 , 143-157. McDougall, J., Buckingham, D., Bulger, M., Mihailidis, A., Gerodimos, R., & Fowler -Watt, K. (2021). Media literacy versus fake news: Critical thinking, resilience and civic engagement. Medijske Studije/Media Studies, 10(19). McDougall, J., Zezulkova, M., van Driel, B., & Sternadel, D. (2018). Teaching media literacy in Europe: Evidence of effective school practices in primary and secondary education . Luxembourg: NESET II. McGrew, S. (2020). Learning to evaluate: An interventio n in civic online reasoning. Computers & Education, 145 . McGuire, W. J. (1961). The effectiveness of supportive and refutational defenses in immunizing and restoring beliefs against persuasion. Sociometry, 24, 184-197. Mehta, R., & Guzm\u00e1n, L. D. (2018). Fake or visual trickery? Understanding the quantitative visual rhetoric in the news. Journal of Media Literacy Education, 10 (2), 104-122. Mena, P. (2020). Cleaning up social media: The effect of warning labels on likelihood of sharing false news on Facebo ok. Policy and Internet, 12 (2), 165-183. Middaugh, E. (2018). Civic media literacy in a transmedia world: Balancing personal experience, factual accuracy and emotional appeal as media consumers and circulators. Journal of Media Literacy Education, 10 (2), 33-52. Moher, D., Shamseer, L., Clarke, Petticrew, M., . . . Group, P. -P. (2015). Preferred reporting items for systematic review and meta -analysis protocols ( PRISMA -P) 2015 statement. Systematic Reviews, 4 (1). Moravec, P. L., Minas, R., K., a., & Dennis, A. R. (2019 ). Fake news on social media: People believe what they want to believe when it makes no sense at all. MIS Quarterly, 43 (4), 1343-1360. Mortimer, K. (2017). Understanding conspiracy online: Social media and the spread of suspicious thinking. Dalhousie Jour nal of Interdisciplinary Management, 13 . Murrock, E., Amulya, J., Druckman, M., & Liubyva, T. (2018). Winning the war on state-sponsored propaganda: Results from an impact study of a Ukrainian news media and information literacy program. Journal of Media Literacy Education, 10 (2), 53-85. Musgrove, A. T., Powers, J. R., Rebar, L. C., & Musgrove, G. J. (2018). Real or fake? Resources for teaching college students how to identify fake news. College & Undergraduate Libraries, 25 (3), 243-260. 62 National Literac y Trust. (2018). Fake news and critical literacy. London: National Literacy Trust. Nekmat, E. (2020). Nudge effect of fact -check alerts: Source influence and media skepticism on sharing of news misinformation in social media. Social Media + Society, 6(1). Nettlefold, J., & Williams, K. (2019). Insight five: A snapshot of media literacy in Australian schools . Hobart, Tasmania: University of Tasmania Institute for Social Change. Nielsen, R. K., Fletcher, R., Newman, N., Brennen, J. S., & Howar d, P. N. (2020). Navigating the 'infodemic': How people in six countries access and rate news and information about coronavirus . Oxford, UK: Reuters International Institute for the Study of Journalism. Nyhan, B., Reifler, J., & Ubel, P. A. (2013). The haza rds of correcting myths about health care reform. Medical care, 51 (2), 127-132. O'Sullivan, M. (2011). Truthful: A method to assist patients with evaluating health information on the internet. Online Journal of Nursing Informatics, 15 (3). OECD. (2020). C ombatting Covid-19 disinformation on online platforms . Paris: OECD. Oeldorf -Hirsch, A., Schmierbach, M., Appelman, A., & Boyle, M. P. (2020). The ineffectiveness of fact - checking labels on news memes and articles. Mass Communication and Society, 23(5), 682-704. Ofcom. Chua, A. Y. K., & Hoe-Lian Goh, D. (2019). Debunking rumors on social media: The use of denials. Computers in Human Behavior, 96 , 110-122. Parent Zone. (2021). Parent zone: T he experts in digital family life. Retrieved from https://parentzone.org.uk/home Paynter, J., Luskin -Saxby, S., Keen, Imms, C., . . . Ecker, U. (2019). Evaluation of a template for countering misinformation -real-world autism tre atment myth debunking. PloS one, 14(1). Pennell, S., & Fede, B. (2018). Fighting fake news: Interdisciplinary online literacies for social justice. Voices from the Middle, 25 (4), 48-53. Pennycook, G., Bear, A., Collins, E. T., & Rand, D. G. (2020). The i mplied truth effect: Attaching warnings to a subset of fake news headlines increases perceived accuracy of headlines without warnings. Management Science, 66 (11), 4944-4957. Pennycook, G., Cannon, T. D., & Rand, D. G. (2018). Prior exposure increases perc eived accuracy of fake news. Journal of Experimental Psychology, 147(12), 1865-1880. Pennycook, G., & Rand, D. G. (2019). Fighting misinformation on social media using crowdsourced judgments of news source quality. Proceedings of the National Academy of S ciences of the United States of America, 116 (7), 2521- 2526. Phillips, J., Gatewood, C., & Parker, L. (2020). Be internet legends and be internet citizens: Impact report. London: Institute for Strategic Dialogue. Phillips, J., King, J., Boyer, I., & Augeri, A. (2019). Young digital leaders 2019: From safety to citizenship online . London: Institute for Strategic Dialogue. Piedade, F., Malafaia, C., Neves, T., Loff, M., & Menezes, I. (2020). Educating critical citize ns? Portuguese teachers and students' visions of critical thinking at school. Thinking Skills and Creativity, 37 . Piro, J. S., & Anderson, G. (2018). Intentional online discussions in teacher education. Teacher Educator, 53(2), 167-189. Polizzi, G., & Ta ylor, R. (2019). Misinformation, digital literacy and the school curriculum . Media Policy Briefs 22. Media Policy Project , London School of Economics and Political Science. Porter, E., Wood, T. J., & Bahador, B. (2019). Can presidential misinformation on climate change be corrected? Evidence from internet and phone experiments. Research & Politics, 6 (3). Posetti, J., Simon, F., & Shabbir, N. (2019). Lessons in innovation: Ho w international news organisations combat disinformation through mission -driven journalism . Retrieved from Oxford, UK: Potter, J. (2004). Theory of media literacy: A cognitive approach . Thousand Oaks, CA: Sage. 63 Radechovsky, J., Berger, P., & Wolling, J. ( 2019). Nothing's gonna change my world - or do journalistic clarifications help against rumors? S CM Studies in Communication and Media, 8 (4), 497-522. Ranieri, M., Nardi, A., & Fabbro, F. (2019). Teachers' professional development on media and intercultur al education: Results from some participatory research in europe. Research on Education and Media, 11 (1), 109-120. Rinne, N. A. (2017). The new framework: A truth -less construction just waiting to be scrapped? Reference Services Review, 45 (1), 54-66. Roozenbeek, J., Maertens, R., McClanahan, W., & van der Linden, S. (2020). Disentangling item and testing effects in inoculation research on online misinformation: Solomon revisited. Educational and Psychological Measurement, 81 (2), 340-362. Roozenbeek, J., Schneider, C. R., Dryhurst, S., Kerr, J., Freeman, A. L. J., Recchia, G., . . . van der Linden, S. (2020). Susceptibility to misinformation about C ovid- 19 around the world. Royal Society O pen Science, 7 . Roozenbeek, J., & van der Linden, S. (2019). Fake news game confers psychological resistance against online misinformation. Palgrave Communications, 5 . Roozenbeek, J., & van der Linden, S. (2020). Breaking Harmony Square: A game that \"inoculates\" against political misinformation. Harvard Kennedy School Misinformation Review . doi:10.37016/mr- 2020-47. Rosenzweig, A. (2017). Understanding and undermining fake news from the classroom. Berkeley Review of Education, 7 (1), 105-112. Rubin, V. L. (2019). Disinformation and misinformation triangle: A conceptual model for \"fake news\" epidemic, causal factors and interventions. Journal of Documentation, 75 (5), 1013-1034. Rush, L. (2018). Examining student perceptions of their knowledge, roles, and power in the information cycle: Findings from a 'fake news' event. Journal of Information Literacy, 12 (2), 121-130. Scharrer, L., Stadtler, M., & Bromme, R. (2019). Judging scientific information: Does source evaluation prevent the seductive effect of text easiness? Learnin g and Instruction, 63 . Schmitt, J. B., Rieger, D., Ernst, J., & Roth, H. J. (2018). Critical media literacy and I slamist online propaganda: The feasibility, applicability and impact of three learning arrangements. International Journal of Conflict and Vio lence, 12 . Shen, C., Kasra, M., Pan, W., Bassett, G. A., Malloch, Y., & O'Brien, J. F. (2019). Fake images: The effects of source, intermediary, and digital media literacy on contextual assessment of image credibility online. New Media & Society, 21 (2), 4 38-463. Simpson, E., & Conner, A. (2020). Fighting coronavirus misinformation and disinformation: Preventive product recommendations for social media https://www.americanprogress.org/issues/technology - May 2021. Sinatra, G. M., & Lo mbardi, D. (2020). Evaluating sources of scientific evidence and claims in the post - truth era may require reappraising plausibility judgments. Educational Psychologist, 55 (3), 120- 131. Sivek, S. C. (2018). Both facts and feelings: Emotion and news literacy. Journal of Media Literacy Education, 10 (2), 123-138. Sperry, S. (2018). News literacy lesson #1: There's nothing new about \"fake news\". Social Education, 82(4), 222-227. uminas, A., & Jastramskis, D. (2020). The importance of media literacy education: How lithuanian students evaluate online news content credibility. Central European Journal of Communication, 13(2), 230-248. Tandoc, E., Lim, Z., & Ling, R. (2018). Defining fake news . Digital Journalism, 6 (2), 137-153. Thaler, R., & Sunstein, C. (2008). Nudge: Improving decisions aboout health, wealth and happiness . Newhaven, CT: Yale University Press. 64 Tseng, A. S. (2018). Students and evaluation of web -based misinformation about vac cination: Critical reading or passive acceptance of claims? International Journal of Science Education, Part B: Communication and Public Engagement, 8 (3), 250-265. Tsipursky, G., Votta, F., & Roose, K. M. (2018). Fighting fake news and post -truth politics with behavioral science: The pro -truth pledge. Behavior & Social Issues, 27 (1), 47-70. Tully, M., Vraga, E. K., & Bode, L. (2020). Designing and testing news literacy messages for social media. Mass Communication & Society, 23 (1), 22 -46. UNESCO, IFAP, & IFLA. (2012). The Moscow declaration on media and information literacy . Retrieved from https://www.ifla.org/files/assets/information -literacy/publications/moscow M\u00e4kitalo, K., & Vartiainen, H. (2019). Me dia literacy education in the age of machine learning. Journal of Media Literacy Education, 11 (2), 20-36. van der Meer, T. G. L. A., & Jin, Y. (2020). Seeking formula for misinformation treatment in public health crises: The effects of corrective information type and source. Health Communication, 35 (5), 560- 575. van Stekelenburg, A., Schaap, G., Veling, H., & Buijzen, M. (2021). Investigating and improving the accuracy of US citizens' beliefs about the Covid-19 pandemic: Longitudinal survey study. Journal of Medical Internet Research, 23 (1). Vraga, E. K., Bode, L., & Tully, M. (2020). Creating news literacy messages to enhance expert corrections of misinformation on Twitter. Communication Research. Vraga, E. K., Kim, S. C., & Cook, J. (2019). Testing logic -based and humor -based corrections for science, health, and political misinformation on social media. Journal of Broadcasting & Electronic Media, 63(3), 393-414. Vraga, E. K., Kim, S. C., Cook, J., & Bode, L. (2020). Testing the effectiveness of correction placement and type on I nstagram. International Journal of Press/Politics, 25 (4), 632-652. Vraga, E. K., & Tully, M. (2019). News literacy, social media behaviors, and skepticism toward information on social media. Information Communication & Society. Wade, S., & Hornick, J. (2018). Stop! Don't share that story!: Designing a pop -up undergraduat e workshop on fake news. Reference Librarian, 59 (4), 188-194. Walker, A. S., & Gutsche, R. E. (2019). Preparing students for the fight against false information with visual verification and open source reporting. Journalism & Mass Communication Educator, 74(2), 227-239. Weber, C. A., & Hagan, H. N. (2020). Is the \"right to clean water\" fake news? An inquiry in media literacy and human rights. Social Studies and the Young Learner, 33 (1), 3 -9. Wells, D. D. (2018). You all made dank memes: Using internet me mes to promote critical thinking. Journal of Political Science Education, 14 (2), 240-248. Wineburg, S. Breakstone, J., Smith, M. McGrew, S. & Ortega, T. (2019). Civic online reasoning: Curriculum evaluation. Stanford, CA: Stanford Historiy Education Group . Wineburg, S., & McGrew, S. (2019). Lateral reading and the nature of expertise: Reading less and learning more when evaluating digital information. Teachers College Record, 121 (11). Yang, S., Lee, J. W., Kim, H. -J., Kang, M., Chong, E., & Kim, E.-M. (20 21). Can an online educational game contribute to developing information literate citizens? Computers & Education, 161 . Zhang, J., Featherstone, J. D., Calabrese, C., & Wojcieszak, M. (2021). Effects of fact -checking social media vaccine misinformation on attitudes toward vaccines. Preventive "}