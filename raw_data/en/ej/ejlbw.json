{"title": "PDF", "author": "PDF", "url": "https://www.zora.uzh.ch/id/eprint/190307/1/Ros_et_al_2020_Consensus_on_the_reporting_and_experimental.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "Zurich Open Repository and Archive University of Zurich University Library Strickhofstrasse 39 CH-8057 Zurich www.zora.uzh.ch Year: 2020 Consensus on the reporting and experimental design of clinical and cognitive-behavioural neurofeedback studies (CRED-nf checklist) Ros, Tomas ; Vadim ; et al ; Nicholson, Andrew A ; Drechsler, Renate Abstract: Neurofeedback has begun to attract the attention and scrutiny of the scientific and medical mainstream. Here, neurofeedback researchers present a consensus-derived checklist that aims to improve the reporting and experimental design standards in the field. DOI: https://doi.org/10.1093/brain/awaa009 Posted at the Zurich Open Repository and Archive, University of Zurich ZORA URL: https://doi.org/10.5167/uzh-190307 Journal Article Published Version The following work is licensed under a Creative Commons: Attribution-NonCommercial 4.0 International (2020). Consensus on the reporting and experimental design of clinical and cognitive- behavioural neurofeedback DOI: https://doi.org/10.1093/brain/awaa009UPDATE Consensus on the reporting and experimental design of clinical and cognitive-behavioural neurofeedback /C3 /C3These authors contributed equally to this work. All other authors are listed in reverse alphabetical order. Neurofeedback has begun to attract the attention and scrutiny of the scientic and medical mainstream. Here, neurofeedback researchers present a consensus-derived checklist that aims to improve the reporting and experimental design standards in the eld. 1 Departments of Neuroscience and Psychiatry, University of Geneva; Campus Biotech, Geneva, Switzerland 2 Department of Clinical Neuropsychology, University of Groningen, Groningen, The Netherlands 3 Department of Biomedical Sciences of Cells & Systems, University Medical Center Groningen, Groningen, The Netherlands 4 Laureate Institute for Brain Research, Tulsa, Oklahoma, USA 5 University of Pittsburgh School of Medicine, Pittsburgh, PA, USA 6 Institute of Psychology, University of Graz, Graz, Austria 7 Massachusetts Institute of Technology, Cambridge, MA, USAdoi:10.1093/brain/awaa009 BRAIN 2020: 143; 1674-1685 |1674 Received April 18, 2019. Revised October 10, 2019. Accepted October 28, 2020. Advance Access publication March 16, 2020 /C223The Author(s) (2020). Published by Oxford University Press on behalf of the Guarantors of Brain. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.comDownloaded from https://academic.oup.com/brain/article/143/6/1674/5807912 by University of Zurich user on 24 September 20208 Northeastern University, Boston, MA, USA 9 Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Macau, Macau, China 10 Campus Biotech, University of Geneva, Switzerland 11 Institut PiPsy, Draveil, France 12 Institute of Bioengineering, Center for Neuroprosthetics, E \u00b4cole Polytechnique Fe \u00b4de\u00b4rale de Lausanne (EPFL); Campus Biotech, Geneva, Switzerland 13 Faculty of Health, Ben-Gurion University of the Negev, Beer-Sheva, Israel 14 Beer-Sheva Mental Health Center, Israel Ministry of Health, Beer-Sheva, Israel 15 Living Health Center for Research and Education, Istanbul, Turkey 16 Department of Mechanical Engineering, University of Texas at Austin, Austin, TX, USA 17 Institute for Medical Psychology and Behavioral Neurobiology, University of Tu \u00a8bingen, Tu \u00a8bingen, Germany 18 Neurobiology and Biobehavioral Psychiatry, David Geffen School of Medicine, University of California Los Angeles, Los Angeles, CA, USA 19 Boston University School of Medicine, Department of Pediatrics, Boston, MA, USA 20 Department of Cognitive Neuroscience, Maastricht University, Maastricht, The Netherlands 21 Clinical Neurotechnology Laboratory, Neuroscience Research Center (NWFZ), Department of Psychiatry and Psychotherapy (CCM), Charite \u00b4- University Medicine Berlin, Berlin, Germany 22 Institute of Biological and Medical Engineering, Ponticia Universidad Cato \u00b4lica de Chile, Macul, Santiago, Chile 23 Ottawa University, Surprise, Arizona, USA 24 Department Clinical Psychology, University of Tu \u00a8bingen, Tu \u00a8bingen, Germany 25 Department of Basic Psychological Research and Research Methods, Faculty of Psychology, University of Vienna, Vienna, Austria 26 Department of Psychiatry, Psychotherapy and Psychosomatics, Psychiatric Hospital, University of Zu \u00a8rich, Zu \u00a8rich, Switzerland 27 University of Salzburg, Centre for Cognitive Neuroscience and Department of Psychology, Salzburg, Austria 28 Department of Child and Adolescent Psychiatry, Institute of Psychiatry, Psychology and Neuroscience, King's College London, London, UK 29 Laseeb-ISR-IST Universidade de Lisboa, Portugal 30 Technion, Israel Institute of Technology, Haifa, Israel 31 Cognitive Science Department, University of California, San Diego, CA, USA 32 Department of Psychosomatic Medicine and Psychotherapy, Central Institute of Mental Health Mannheim, Medical Faculty Mannheim/Heidelberg University, Germany 33 National Research University Higher School of Economics, Moscow, Russia 34 Department of Psychology, Shanghai Normal University, Shanghai, China 35 Bitbrain/C213, Zaragoza, Spain 36 SANPSY, Bordeaux, Place Amelie Raba Leon, Bordeaux, France 37 Department of Psychiatry, University of Mu \u00a8nster, Mu \u00a8nster, Germany 38 Department of Psychology, University of Tennessee, Knoxville, USA 39 Inria Bordeaux Sud-Ouest/LaBRI University of Bordeaux - CNRS-Bordeaux INP, Bordeaux, France 40 Department of Psychiatry and Neuropsychology, School for Mental Health and Neuroscience, Faculty of Health, Medicine and Life Sciences, Maastricht University, Maastricht, The Netherlands 41 Department of Psychology, University of Texas at Austin, Austin, TX, USA 42 Center for Bioelectric Interfaces of the Institute for Cognitive Neuroscience, National Research University Higher School of Economics, Moscow, Russia 43 Department of Information and Internet Technologies of Digital Health Institute; I.M. Sechenov First Moscow State Medical University, Moscow, Russia 44 Duke Center for Neuroengineering, Duke University, Durham, NC, USA 45 Department of Psychiatry, Western University, London, Ontario, Canada 46 Department of Psychology I, Psychological Intervention, Behavior Analysis and Regulation of Behavior, University of Wu \u00a8rzburg, 47 Neuropsychology Lab, Department of Psychology, University of Oldenburg, Oldenberg, Germany 48 Magnetic Resonance Research Center (MRRC), Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT, USA 49 Medical University of Vienna, Department of Child and Adolescent Psychiatry, Vienna, Austria 50 JARA-Institute Molecular neuroscience and neuroimaging (INM-11), Ju \u00a8lich Research Centre, Ju \u00a8lich, Germany 51 Department of Psychology, The University of Shefeld International Faculty, City College, Thessaloniki, Greece Universite \u00b4Toulouse Jean and Neuroscience, Maastricht University, Maastricht, The Netherlands 54 Multimodal imaging and Cognitive Control Lab, Department of Psychology, University of Olso, Norway 55 Alpert Medical School, Brown University, Providence, RI, USA 56 Electrical Engineering Department, Eindhoven University of Technology, The Netherlands 57 Sagol Brain Institute, Wohl Institute for Advanced Imaging, Sourasky Medical Center, Tel Aviv, Israel 58 Department of Radiology and Biomedical Imaging, Yale University School of Medicine, New Haven, CT, USA 59 Division of Neurorehabilitation, Department of Clinical Neurosciences, University Hospital Geneva, Geneva, SwitzerlandCRED-nf Checklist BRAIN 2020: 143; 1674-1685 |1675Downloaded from https://academic.oup.com/brain/article/143/6/1674/5807912 by University of Zurich user on 24 September 202060 Division of Functional and Restorative Neurosurgery, University of Tu \u00a8bingen, Tu \u00a8bingen, Germany 61 Department of Psychology, Goldsmiths, University of London, London, UK 62 Univ. CHU Lille, France 63 UNAM Institute of Neurobiology, National Autonomous University of Mexico, Juriquilla, Mexico 64 Psychophysiology and Optical Imaging, Department of Psychiatry and Psychotherapy, University of Tu\u00a8bingen, Tu \u00a8bingen, Germany 65 Department of Child and Adolescent, Psychiatry and Psychotherapy, Psychiatric Hospital, University of Zu \u00a8rich, Zu \u00a8rich, Switzerland 66 Omneuron, Inc., Menlo Park, CA, USA 67 Department of Surgery, Section of Neurosurgery, University of Otago, Dunedin, New Zealand 68 Department of Psychological Sciences Birkbeck, University of London, Bloomsbury, London, UK 69 GIPSA-lab, CNRS, University Grenoble Alpes, Grenoble-INP, Grenoble, France 70 School of Computing and Mathematical Sciences, University of Greenwich, London, UK 71 Radboud University Nijmegen, Department of Clinical Psychology, Nijmegen, The Netherlands 72 Department of Child and Adolescent Psychiatry and Psychotherapy, Central Institute of Mental Health, Medical Faculty Mannheim/Heidelberg University, Mannheim, Germany 73 Laureate Institute for Brain Research, Tulsa, OK, USA 74 Institute for Medical Psychology and Behavioural Neurobiology, University of Tu \u00a8bingen, Tu \u00a8bingen, Germany 75 State Research Institute of Physiology and Basic Medicine, Novosibirsk, Russia 76 School of Medicine, Aristotle University of Thessaloniki, Thessaloniki, Greece 77 School of Psychology, Faculty of Health and Medical Sciences, University of Surrey, Guildford, UK 78 Brainclinics Foundation, Research Institute Brainclinics, Nijmegen, The Netherlands 79 School of Psychological Science, University of Bristol, Bristol, UK 80 MRC Integrative Epidemiology Unit at the University of Bristol, Bristol, UK Correspondence to: Robert T. Thibault School of Psychological Science University of Bristol, 12a Priory Road, Bristol, BS8 1TU, UK also be addressed to: Stefanie Enriquez-Geppert Department of Clinical and Developmental Neuropsychology, Faculty of Behavioural and Social Neuroscience Center, Department of Neuroscience, Consensus on the reporting and experimental design of clinical and cognitive-behavioural neurofeed- back studies; MCID = minimal clinically important difference Introduction After a protracted history, neurofeedback has begun to attract the attention and scrutiny of th e scientic and medical main- stream (Kamiya, 2011; Linden, 2014; Sitaram et al. , 2017). A debate now centres on the extent to which neurofeedback alters brain function and behaviour, and the mechanisms through which neurofeedback operates (e.g. neurofeedback- specic versus non-specic). A series in Lancet (Micoulaud-Franchi and Fovet, 2016; Thibault and Raz, 2016 a,b;P i o t t et al. , 2017; Scho \u00a8nenberg et al., Witte et al. , 2018) discuss the theoretical arguments and empirical data backing the involvemen to ft h e s et w om e c h a n i s m s .The apparent controversy that the correspondence letters present stems from a well-known phenomenon in neuro- psychology: that multiple components can drive the benets of a treatment (Enriquez-Geppert et al. , 2013; Campbell and Stanley, 2015). We depict this hypothesized multi-compo- nent model for the context of neurofeedback in Fig. 1. We divide the mechanisms driving experimental outcomes into ve bins: neurofeedback-specic (related to training a target neurophysiological variable), neurofeedback non-specic (de- pendent on the neurofeedback context, but independent from the act of controlling a particular brain signal), general non-specic (including the common benets of cognitive training as well as psychosocial influences, such as placebo responding), repetition from https://academic.oup.com/brain/article/143/6/1674/5807912 by University of Zurich user on 24 September 2020and natural (e.g. spontaneous remission, cognitive develop- ment) (Micoulaud-Franchi and Fovet, 2018). Although a framework based on these terms and con- cepts is only beginning to concretize in the neurofeedback literature, most scientists involved in neurofeedback agree on their general usage and interpretation. The greater points of contention centre on (i) whether previous experi- ments provide sufcient evidence to identify specic factors as a key driver of neurofeedback outcomes; and (ii) how to best design an experiment to clearly dissociate the various mechanisms driving neurofeedback outcomes. If neurofeed- back outcomes occur independently of the information provided by the neural feedback signal (i.e. come from non-specic mechanisms), then neurofeedback does not rely on the main criteria that set it apart from other inter- ventions, such as cognitive training and meditation. An ideal demonstration of neurofeedback-specic effects would include evidence of online (i.e. intra-session) and offline (i.e. inter -session or post-treatment) changes in tar- geted brain activity, as well as a control group or condition to rule out non-specic effects (e.g. sensory stimulation, placebo). Individual neurofeedback studies, however, con- tain varying proportions of each of these criteria and have led to a diversity of opinions regarding the specicity of mechanisms involved in neurofeedback. The present check- list provides the structure to develop a more comprehensive and rigorous evidence base. Evidence for putatively causal, neurofeedback-specic mechanisms relies on our knowledge of the physiological basis of neural activity and its relevance to cognition (for a review of neurofeedback mechanisms, see Ros et al. , 2014; Sitaram et al. , 2017). For example, the association between neural activity and cognition in animals (Cao et al. , Babapoor-Farrokhran et al. , 2017) suggests that self-regu- lation of brain circuits can alter behaviour and cognition. A number of neurofeedback experiments in animals (Sterman et al. , 1970; Schafer and Moore, 2011), and humans (Watanabe et al. , 2017; Young et al. , 2017 b) further sup- port this view. Evidence suggesting that mechanisms other than neurofeedback-specic factors account for the effects of neurofeedback come from a number of recent studies and reviews that nd comparable benets between partici- pants who receive veritable neurofeedback from their own brain and those who observe a sham-neurofeedback signal unrelated to their neural activity of interest (Schabus To advance the eld of neurofeedback, scientists can benet from designing future studies with the methodo- logical rigour capable of disentangling the various mechan- isms driving the effects of neurofeedback. As authors of the correspondence, alongside other researchers active in the eld, we propose a standardized checklist outlining best practices in the experimental design and reporting of neu- rofeedback studies. We believe that widespread adoption of this checklist will help advance our scientic understanding of how neurofeedback affects brain function and behaviour.Objectives of the checklist This checklist is intended to encourage robust experimental design and clear reporting for clinical and cognitive-behav- ioural neurofeedback experiments (for a methodological review see al. , 2014; Enriquez-Geppert et al. , 2017). Because all neurofeedback aims to train brain activ- ity, these guidelines generalize across EEG, magnetoencepha- lography (MEG), functional MRI, functional and other neurofeedback modalities. The checklist focuses mainly on aspects unique to the neu- rofeedback context (as general standards for each imaging modality already exist; Gross et al. , 2013; Nichols et al. , 2017; Pernet et al. , 2018). It serves as a complement, rather than alternative, to the Consolidated Standards of Reporting Trials (CONSORT) guidelines (Schulz et al. , 2010) (http://www.consort-statement.org/checklists). When submitting neurofeedback results for publication, we encou- rage researchers to include the checklist (Fig. 2), ideally using the application available at www.rtn.org/CREDnf. Alternatively, the checklist can be downloaded from the Supplementary material and the nal column can be lled with the relevant text from your manuscript, or the page number identifying where in the manuscript each item is addressed. This checklist does not aim to inhibit the explor- ation of novel directions in neurofeedback research. On the contrary, it advocates robust designs and clear reporting to promote informed research decisions that can effectively build upon previous work. These guidelines are a rst iter- ation. As neurofeedback research progresses, we invite the community to provide comments for improving this check- list (see rtn.org/CREDnf for a link to the commenting plat- form). We hope these guidelines will help disentangle the relative contribution of the mechanisms outlined in Fig. 1. Description of checklist items Below, we include a short description of each checklist item followed by examples from published neurofeedback articles. Pre-experiment Item 1a. Preregister experimental protocol and planned analyses This item is essential for clinical and replication studies, and is encouraged for others. Preregister, for example, on a platform such as www.o sf.io, as a randomized controlled trial (RCT) on ClinicalTrials.gov or the European Union Clinical Trials Register (EUCTR), or by submitting a registered report (see www.cos.io/rr for information concerning registered reports). Clearly label primary and secondary outcome variables. Indicate the number, frequency, and durationCRED-nf Checklist BRAIN 2020: 143; 1674-1685 |1677Downloaded from https://academic.oup.com/brain/article/143/6/1674/5807912 of Zurich user on 24 September 2020of neurofeedback sessions. In the publication, report which analyses were preregistered, which were explora- tory, and disclose any potential deviations from the pre- registered protocol. Examples: (i) See The Collaborative Neurofeedback Group (2013) for a pre-published protocol of a double-blind multisite RCT, and https://clinicaltrials.gov/ct2/show/NCT02251743 for the pre- registration document. (ii) See Holtmann et al. (2014) for a pre-published protocol of the study by Strehl et al. (2017) with trial registry number ISRCTN 76187185. Item 1b. Justify sample size This item is essential. Describe the sampling plan and how it was determined. Ideally, justify the sample size with a power analysis based on the smallest effect size of interest [e.g. minimal clinically important differences (MCIDs), see Item 6a] or another method (e.g. Bayesian sequential sampling). Otherwise,label the experiment as a pilot, proof-of-concept, or feasi- bility study. If the preregistered sample size is not met, state so. Whereas smallest effect sizes of interest may be derived from previous literature, we do not recommend selecting a sample size based solely on an 'expected' effect size derived from previous published results. Because of publication bias, which remains common across research elds, this practice can leave experiments underpowered (Albers Algermissen and Mehler, 2018). Examples: (i) 'Estimates of a clinically relevant effect size were derived from the Go \u00a8ttingen pilot-study using the same primary out- come measures [18]. It is expected that in the neurofeedback group the mean FBB-ADHS score at Post-Test 2 is 1.20 and in the control group 1.50 with a common standard devi- ation of 0.55. The expected outcome requires a sample size of 72 subjects per group ( a= 0.05, two sample t-test, two-sided) to achieve a power of 90%.' (Holtmann et al. , 2014). (ii) 'Owing to feasibility and proof of principle, we intend fol- lowing a Bayesian sampling strategy with a minimum of N=5 patients and continue recruiting either until the Bayes factor for both hypotheses (A and B) is conclusive - i.e. either for the alternative with BF10 410 (indicating strong evidence for a positive effect) or for the null with BF01410 (indicating strong evidence for a null effect) - or until the end of the data collection period (September 30, 2017) is reached.' (Mehler et al. , 2017). Control groups Item 2a. Employ control group(s) or control condition(s) This item is essential. Use a control group (between subjects) or control condi- tion (within subjects). This could include a placebo-control (e.g. sham-neurofeedback, neurofeedback from a largely unrelated brain signal, or inversing the neurofeedback reward contingency) or another active non-neurofeedback control (e.g. a similar type of computerized cognitive train- ing, biofeedback, or medication). See Sorger et al. (2019) for an in-depth review of control groups in neurofeedback research. Consider the potential for, and report any adverse effects in both the experimental and control groups. Examples: (i) 'Four separate healthy subject control groups were trained and tested using similar or identical procedures but in the absence of valid rACC rtfMRI information ...Group III (n = 8) received identical training to the experimental group, but using rtfMRI information derived from a different brain region in posterior cingulate cortex that is not believed to be involved in pain processing to examine spatial and physio- logical specicity. Group IV (n = 4) received identical train- ing to the experimental group, but, unknown to the subjects, the rtfMRI displays that they viewed corresponded to acti- vation from a previously tested experimental subject's rACC, Figure 1 Multiple mechanisms drive the effects of neuro- feedback training. Neurofeedback participants may benet from: (i) the specic neurophysiological process of training a particular brain signal (green). Non-specic factors, including (ii) those unique to the neurofeedback environment (e.g. trainer-participant inter- action in a neurotechnology context) (dark blue); and (iii) those that are common across interventions (e.g. all other benets from engaging in a form of cognitive training as well as the psychosocial and placebo mechanisms related to participating in an experiment) (light blue). (iv) Repetition-related effects (purple). (v) Natural ef- fects, which can be positive (e.g. cognitive development in child- hood) or negative (e.g. cognitive decline in older age) (orange). These mechanisms may interact synergistically to create a greater overall effect, interact antagonistically to lessen the total benet, or combine additively (for a discussion of this topic, see Rothman, 1974; Finnerup et al. , 2010). By including control groups, carefully designing experiments, and measuring both brain activity and be- haviour, researchers can better estimate the contribution from each of these mechanisms.1678 |BRAIN 2020: 143; 1674-1685 T . Ros et al.Downloaded from https://academic.oup.com/brain/article/143/6/1674/5807912 by University of Zurich user on 24 September 2020rather than their own rACC brain activation.' (deCharms et al. , 2005). (ii) semi-active condition EMG feedback of coord- ination in the supraspinatus muscles was chosen. Participants were instructed either to contract or to relax the left relative to the right supraspinatus muscle. This protocol was chosen to induce differential EMG control cor- responding to the \"polarities\" comparable to the NF condi- tion, without requiring simple relaxation or tension. This allowed us to use the same device and the same representa- tion of the feedback signal on the screen. We did not choose a standard EMG feedback protocol because the control con- dition should be as unspecic as possible but include the possibility to learn self-regulation, i.e. the unspecic variable of any biofeedback treatment.' (Strehl et al. , 2017). Item 2b. When leveraging experimental designs where a double-blind is possible, use a double-blind This item is essential. For example, in experiments with a placebo-neurofeed- back control group or within participant control conditions. Example: 'To blind staff to treatment condition, The SmartBox interface de- vices were independently preprogrammed by an off-site consultant who had no interaction with participants or data (analogous to prepackaged randomized medication).' (Arnold et al. , 2013). Comment: Currently, few neurofeedback software packages are designed for blinding the treatment staff. Item 2c. Blind those who rate the outcomes, and when possible, the statisticians involved For this item, see Dutilh et al. (2019); this item is encouraged. Indicate which individuals were blinded and how blind- ing was achieved. Example: 'The Behavioral Observation of Students in Schools [BOSS] ... is a systematic interval recording observation system for coding classroom behavior and reports on engagement ...and off-task behaviors ...Data output from observations are objective quan- titative assessments, which can help reduce observer bias ...The BOSS was completed ...for all study participants by trained RA [research assistants] who were unaware of the participants' ran- domization conditions. The participants were unaware that they were being observed.' (Steiner et al. , 2014). Item 2d. Examine to what extent participants and experimenters remain blinded This item is encouraged. For an overview on reporting whether blinding was suc- cessful, see Kolahi et al. (2009). Example: 'The CSQ [consumer satisfaction questionnaire], administered at Treatments 24 and 40, also included questions to examineblindness to treatment assignment ...Of 34 participants at Treatment 40, 35% of children and 29% of parents said that they did not know which treatment they had been assigned to and declined to guess. Only 32% of children and 24% of par- ents guessed correctly, with 32% and 47%, respectively, gues- sing incorrectly.' (Arnold et al. , 2013). Item 2e. In clinical efcacy studies, employ a standard-of-care intervention group as a benchmark for improvement This item is encouraged. This design helps establish whether neurofeedback is su- perior to, or at least non-inferior to, standard treatments. Example: 'Potential participants are screened for eligibility, and those who are eligible are randomly assigned to the treatment group (receiving rtfMRI NFT in addition to treatment as usual) or the control group (receiving only treatment as usual).' (Cox et al. , 2016). Control measures Item 3a. Collect data on psychosocial factors This item is encouraged. For example, participant motivation, treatment expect- ation, effort exerted, and subjective sense of success. Examples: (i) 'To compare the NFT and the pseudo NFT group concern- ing the plausibility of the intervention, a subject self-report was utilized. Subjects reported on motivation to participate in the study, commitment to the study (before each session), and difculty of the session (right after each session) using a seven-point Likert-scale (1 = not at all to 7 = very strong).' (Enriquez-Geppert et al. , 2014). (ii) 'In the present study, the effects of sex of participant, sex of experimenter, as well as the role of locus of control in deal- ing with technology will be investigated ...Although the purpose of the present study is not to investigate further the effects of mindfulness and SMR baseline power on neu- rofeedback training outcomes, their impact will be measured and controlled statistically in the experimental design.' (Wood and Kober, 2018). Item 3b. Report whether participants were provided with a strategy This item is essential. If strategies were provided, report the details of the strategies. Examples: (i) 'Importantly, the experimenter did not provide any explicit instruction to the participant regarding strategies; rather par- ticipants were told to increase the number of counts and bell rings by any mental means they could.' (Davelaar et al. , 2018). (ii) 'Subjects were instructed to execute or imagine the kines- thetic experience of a sequential nger from https://academic.oup.com/brain/article/143/6/1674/5807912 by University of Zurich user on 24 September 2020person perspective with either the right or left hand (20 trials per hand in randomized order).' (Zich et al. , 2015). Comment: Currently there is no standard regarding the provision of strategies, nor is there systematic research on which strategies are the most effective (see section 'provi- sion of strategies' from Enriquez-Geppert et al. , brain-computer interface (BCI) is the exception. Item 3c. Report the strategies participants used This item is encouraged. Examples: (i) 'The reported mental strategies and the subsequent categor- ization process are described in Table A1 of the Appendix in more detail.' (Kober et al. , 2013) (ii) 'Among them, the most efcient strategies were friends (1.625), love (1.4) and family (1.1) while the worst were anger (-2.0) and calculation (-0.15). The effects of some positive strategy subtypes like love (lover (1.67)), nature (hometown (1.5)) and family (brothers (2.0)) stood out.' (Nan et al. , 2012). Item 3d. Report methods used for online data pro- cessing and artefact correction This item is essential. For example, detection and rejection/correction of ocular and muscular artefacts (EEG, MEG), and of tory movement artefacts Examples: (i) 'Before the start-baseline measurement, an EOG calibration method (3 min) was implemented that calculates the subject- specic, artifact-associated frequency band. This was used for all following measurements for eye blink detection and rejection during further measurements (for details see Huster et al. , 2014) ...Thus, the subject-specic artifact-associated frequency band that was calculated in the EOG calibration measure was monitored. Whenever the mean amplitudes of a 2 s segment was higher than the subject-specic artifact- associated frequency band (minus one standard deviation), the segment was rejected and not used for feedback.' (Enriquez-Geppert et al. , 2014). (ii) 'Pre-processing of single-subject using AFNI implementa- tion of the RETROICOR method. The cardiac and respiratory waveforms recorded simultaneously during each fMRI run were used to generate the cardiac and respiratory phase time series for the RETROICOR.' (Young et al. , 2014). Item 3e. Report condition and group effects for artefacts This item is encouraged. Report condition and group effects for the artefacts de- tailed for Item 3d (to test whether artefacts are more preva- lent in certain participants and conditions). Examples:(i) 'We observed an intra-subject effect of regulation condition on HR [heart rate] (F(2,52) = 6.092; p = 0.004), which was driven by an increased HR during the active (''UP'' and ''DOWN'') regulation conditions (Figure 6A). The relative difference between ''UP'' and ''DOWN'' conditions was not correlated with regulation capacity (2-tailed Pearson R = 0.038, p = 0.853, Figure 6C). For RVT [respiration volume per time], there was a trend for an intra-subject effect of regulation condition (F(2,52) = 3.148; p = 0.051, Figure 6B). Additionally, we found a correlation between the relative RVT-difference between the ''UP'' and ''DOWN'' conditions and regulation capacity (2-tailed Pearson R = -0.450, p = 0.018, Figure 6D).' (Marxen et al. , 2016). (ii) 'In Fig. 6, mean heart and breathing rates obtained during the different feedback conditions are plotted jointly for P02- P05 and P09 (with all values being in the normal range). While observed differences in heart rate across target-level conditions were extremely weak, slightly augmented breath- ing frequencies were detected for higher target-level condi- tions on a descriptive level.' (Sorger et al. , 2018). Feedback specications Item 4a. Report how the online feature extraction was dened This item is essential. For example, a frequency band, frequency band ratio, single region of interest, or functional connectivity measure. Was it individualized or xed across all participants? How was it extracted (e.g. number and location of electrodes)? Examples: (i) 'In each session, the IAF [individual alpha frequency] was calculated as the peak frequency of the alpha band during the rst base rate and UA [upper alpha] was dened as the frequency band from IAF to IAF + 2 Hz.' (Zoefel et al. , 2011). (ii) 'For the localizer scan, real-time statistical analyses were carried out via an incremental general linear model (GLM) using Turbo-BrainVoyager (TBV) ...Target ROIs in the respective groups were identied during a localizer scan based on the t-statistic of the contrasts of interest, which were dened as positive vs. neutral pictures in the NFE group and scene vs. face pictures in the NFS group. Target ROIs in the NFE group were limited to limbic and frontal portions of the anterior cerebrum based on models of emotion processing in the human brain [19].' (Mehler et al. , 2018). Item 4b. Report and justify the reinforcement schedule This item is essential. For example, justify the reinforcement schedule, or the feedback threshold criteria, in relation to existing neuro- feedback literature and practice. Report how the feedback was given (e.g. continuous or periodic, proportional or binary). Report the amount of reward (e.g. percentage) per subject and across subjects. Example:1680 |BRAIN 2020: 143; 1674-1685 T . Ros et al.Downloaded from https://academic.oup.com/brain/article/143/6/1674/5807912 by University of Zurich user on 24 September 2020'Thus the patient actually controlled the quality of the picture on the screen by his/her brainwaves: when the biofeedback par- ameter was higher than threshold, the picture on the screen was clear, otherwise the TV picture was blurred by the noise. The threshold for the biofeedback parameter was dened by the prefeedback baseline mean measure taken during a 2.5-min feedback-free period with eyes opened at the beginning of the rst session in a way to grant that the biofeedback parameter exceeds the threshold about 50% of the time.' (Kropotov et al. , 2005). Item 4c. Report the feedback modality and content This item is essential. Identify the feedback modality (e.g. visual, auditory, tact- ile, proprioceptive), and the feedback format (e.g. video clip, simple graphic, melody, tone). Example: 'Children from one group received the NFB treatment using as reinforcement an auditory stimulus (Auditory Group, AG), and children of the other group received a NFB treatment using as reinforcement a visual stimulus (Visual Group, VG) ...The auditory stimulus was a tone of 500 Hz at 60 dB, and the visual stimulus was a white square of 20 cm2over a black background of a computer monitor.' (Ferna \u00b4ndez et al. , 2016) Item 4d. Collect and report all brain activity variable(s) and/or contrasts used for feedback, as displayed to experimental participants This item is essential for points (ii) and (iii); and we en- courage researchers to include points (i) and (iv-vi). Time points may include: (i) a pre-training baseline; (ii) rest blocks; (iii) training (iv) a post-training baseline; (v) transfer run(s) without neurofeedback; and (vi) long-term follow-up. Report the relevant units. Example: 'Thus the aim of this study was to focus on alpha neurofeed- back and examine changes in three different measures: ampli- tude, percent time, and integrated alpha, across four methods: within sessions, across sessions, within sessions compared to baseline, and across sessions compared to baseline.' (Dempster and Vernon, 2009). Item 4e. Report the hardware and software used This item is essential. Include the versions. Outcome measures (brain) Item 5a. Report neurofeedback regulation success based on the feedback signal This item is essential. Identify the baseline or contrast used (e.g. subject-specic data from a previous session, reference data based on aver- aged data from a normative group). Identify the compara- tor run (e.g. training run or transfer run). Report both statistically signicant and non-statistically signicant ndings.Comment: We raise this point because some experiments report only the changes in a subset of brain activity that was not used for the neurofeedback signal. Item 5b. Plot within-session and between-session regulation blocks of feedback variable(s), as well as pre-to-post resting baselines or contrasts This item is essential. Plotting the session course by comparing the session be- ginning, middle, and end (for instance, by arbitrarily divid- ing sessions into segments or using session blocks) allows the assessment of within-session dynamics. Between-session comparisons allow the assessment of the whole training course on a temporally more abstract level. Example: 'Thus, relative to the VC group, the VTA feedback group showed enhanced activation over the duration of the ACTIVATE trial ...Relative to baseline, the VTA Feedback group increased activation in the rst half of the trial (t(18) = 4.74, p 50.0005) ...In addition to group differences, VTA Feedback group activation at Post-test was signicantly greater than Pre-test (t(18) = 2.36, p 50.05) and overall: t(18) p 50.005).' Also, see Fig. 3 in MacInnes et al. (2016). Item 5c. Statistically compare the experimental condition/group to the control condition(s)/group(s) (not only each group to baseline measures) This item is essential. Comparing experimental and control groups/conditions to their respective baselines, but not to each other fails to test whether the experimental intervention outperforms the control intervention(s) (Nieuwenhuis et al. , 2011). Example: 'Figure 2 ...Amygdalar hemodynamic response was assessed using fMRI during exposure to (A) masked sad face presentations (SN-NN condition) and (B) masked happy face presentations (HN-NN condition). Error bars indicate /C61S E M ./C3indicates a signicant difference from the corresponding baseline at p corrected 5.05. # indicates a signicant difference from the experimental group at p corrected 5.05.' (Young et al. ,2 0 1 7 a). Outcome measures (behaviour) Item 6a. Include measures of clinical or behavioural signicance, dened a priori , and describe whether they were reached This item is essential. For example, by using MCIDs to establish the magni- tude of an effect to interpret as clinically meaningful (see Engel et al. , 2018; Lakens et al. , 2018 for an overview on establishing MCID values and smallest effect sizes of inter- est). Many of these values remain open to discussion\u2014 explain the reasoning behind the value used. Moreover, collect data on acceptability, safety, and adverse effects.CRED-nf Checklist BRAIN 2020: 143; 1674-1685 |1681Downloaded from https://academic.oup.com/brain/article/143/6/1674/5807912 by University of Zurich user on 24 September 2020In this paper, we are using the term 'behaviour' in the broad sense to encompass all non-physiological measures, including self-reports. Examples: (i) 'Minimal clinically important differences (MCIDs) were dened as \"the smallest differences in scores in the domainof interest, which patients perceive as benecial, and which would mandate, in the absence of troublesome side effects and excessive costs, a change in the patient's management\" ...The MCID value for the 10-m walk test was 0.19 m/s;45 3.5 s for TUG; 46 and 5 points each for the UPDRS-Brad and UPDRS-III.47The MCID values of 5 points and 2 points were adopted for BBS and PDQ-39 (mobility), respect- ively.45,48' (Costa-Ribeiro et al. , 2017).CRED-nf best practices checklist 2020 Domain Item # Checklist item Reported on page # Pre-experiment 1a Pre-register experimental protocol and planned analyses 1b Justify sample size Control groups 2a Employ control group(s) or control condition(s) 2b When leveraging experimental designs where a double-blind is possible, use a double-blind 2c Blind those who rate the outcomes, and when possible, the statisticians involved 2d Examine to what extent participants and experimenters remain blinded 2e In clinical efficacy studies, employ a standard-of-care intervention group as a benchmark for improvement Control measures 3a Collect data on psychosocial factors 3b Report whether participants were provided with a strategy 3c Report the strategies participants used 3d Report methods used for online-data processing and artefact correction 3e Report condition and group effects for artefacts Feedback specifications 4a Report how the online-feature extraction was defined 4b Report and justify the reinforcement schedule 4c Report the feedback modality and content 4d Collect and report all brain activity variable(s) and/or contrasts used for feedback, as displayed to experimental participants 4e Report the hardware and software used Outcome measures Brain 5a Report neurofeedback regulation success based on the feedba ck signal 5b Plot within-session and between-session regulation blocks of feedback variable(s), as well as pre-to-post resting baselines or contrasts 5c Statistically compare the experimental condition/group to the control condition(s)/group(s) (not only each group to baseline measures) Behaviour 6a Include measures of clinical or behavioural significance, defined a priori , and describe whether they were reached 6b Run correlational analyses between regulation success and behavioural outcomes Data storage 7a Upload all materials, analysis scripts, code, and raw data used for analyses, as well as final values, to an open access data repository, when feasible Figure 2 Consensus on the reporting and experimental design of clinical and cognitive-behavioural neurofeedback studies (CRED-nf) best practices checklist 2020. An online tool to complete this checklist is available at rtn.org/CREDnf. Darker shaded boxes represent 'essential' checklist items; lightly shaded boxes represent 'encouraged' checklist items. We recommend using this checklist in con- junction with the standardized CRED-nf online tool (rtn.org/CREDnf) and the CRED-nf article, which explains the motivation behind this checklist and provides details regarding many of the checklist from https://academic.oup.com/brain/article/143/6/1674/5807912 by University of Zurich user on 24 September 2020(ii) 'The primary outcome measure was the arm section of the Fugl-Meyer Assessment (FMA). A minimal clinically import- ant difference (MCID) for this scale was set to 7 point.' (Pichiorri et al. , 2015). Item 6b. Run correlational analyses between regulation success and behavioural outcomes This item is essential. Examples: (i) 'For the mean alpha amplitude at P4 (the NFB controlled parameter), we found no signicant correlations with any neglect severity measures (i.e. omissions on the left, center, or right parts of the cancellation test, deviation on line bi- section). However, as shown in Table 2, for the alpha vari- ability and its left-right parietal asymmetry, we observed signicant correlations with performance on the cancellation test.' (Ros et al. , 2017). (ii) 'The exploratory robust regression analysis suggested that changes in self-efcacy predicted residualized depression scores at the primary endpoint (R2 = 0.18, adjusted R2 = 0.15, b= -0.187 /C60.073, Fig. 2c), such that increase in self-efcacy was associated with less depression severity (t30 = -2.551, p = 0.016).' (Mehler et al. , 2018). Data storage Item 7a. Upload all materials, analysis scripts, code, and raw data used for analyses, as well as nal values, to an open access data repository, when feasible This item is encouraged. Description of consensus process The authors T.R., S.E-G., and R.T.T. developed the idea for a checklist of this type. They worked together, in the form of an adversarial collaboration, to produce an initial outline of the present checklist. They then requested input from re- searchers involved in recent correspondences on neurofeed- back, particularly those published in Brain and Lancet Psychiatry . These R.T.T. then worked together to implement the comments from the researcher listed above and produce a rst complete draft. This rst complete draft was then sent to neurofeed- back researchers involved in relevant discussions at recent conferences [e.g. Society for Applied Neuroscience (SAN) le well as the rst-round contributors, to ask: (i) whether they agreed with the contents of the checklist; (ii) whether they would like to add, modify, or remove any material; and (iii) to invite researchers they believe may be interested in joining or commenting on the consensus. Together, T.R., S.E-G., and R.T.T. discussed each of the second-round comments and implemented those theybelieved appropriate for this checklist. Not all comments were addressed; in particular, specic comments relevant to only a subset of neurofeedback research, as well as a few points where contributors disagreed, were excluded from the present checklist. This second draft was then shared with all contributors before submitting for publication. Funding No funding was received towards this work. R.T.T. is sup- ported by a postdoctoral fellowship from the de recherche en Que \u00b4bec. A.O. ported by the Center for Bioelectric Interfaces National Research University Higher School of Economics, Russian Federation Government grant, ag. No.14.641.31.0003. Competing interests U.S. has been paid for public speaking by Novartis, Medice, NeuroCare, the German Society for Biofeedback, the German Society for Psychotherapy and Psychiatry and the Akademie Ko\u00a8nig und Mu \u00a8ller. K.R. has received a grant from Takeda for another project. M.H. has a patent application for fNIRS neu- rofeedback, titled 'Methods and systems for treating a subject using NIRS neurofeedback' (PCT/US2017/036532, led June 8, 2017) as well as a contract with Elsevier to edit a book titled 'FMRI Neurofeedback'. D.B. serves as an unpaid scien- tic advisor for an EU-funded neurofeedback trial unrelated to the present work. B.B. was paid for public speaking by the neuroCare Group (Mu \u00a8nchen, Germany). M.A. is unpaid chair- man of the Brainclinics Foundation, a minority shareholder in neuroCare Group (Munich, Germany), and a co-inventor on four patent applications related to EEG, neuromodulation and psychophysiology, but receives no royalties related to these patents; Research Institute Brainclinics received research fund- ing from Brain Resource (Sydney, Australia), Urgotech (France) and neuroCare Group (Mu \u00a8nchen, Germany), and equipment from Deymed, neuroConn, Brainsway and Magventure. R.T.T. has consult with neurofeedback start-up companies. R.C.D. holds patents related to rtfMRI and rtfMRI-based feedback, and is CEO and a shareholder in Omneuron, a company that has de- veloped technology related rtfMRI-based feedback. All other authors report no competing interests. Supplementary material Supplementary material is available at Brain online. References Albers C, Lakens D. When power analyses based on pilot data are biased: inaccurate effect size estimators and follow-up bias. of Zurich user on 24 September 2020Algermissen J, Mehler DM. May the power be with you: are there highly powered studies in neuroscience, and how can we get more of them? J Neurophysiol 2018; 119: 2114-7. Arnold LE, Lofthouse N, Hersch S, Pan X, Hurt E, Bates B, al. EEG neurofeedback for ADHD: Theta and beta synchrony coordinate frontal eye elds and anterior cingu- late cortex during sensorimotor mapping. Nat Commun 2017; 8: 1- 4. Campbell DT, Stanley JC. Experimental and quasi-experimental designs for research. Ravenio Books; 2015. Cao B, Wang J, Zhang X, Yang X, Poon DC, Jelfs B, et al. Impairment of decision making and disruption of synchrony between basolateral amygdala and anterior cingulate Y, Castro R, Baltar A, et al. Transcranial direct current stimulation associated with gait training in Parkinson's disease: a pilot randomized Cox WM, L, Linden DE, \u00a8hrs M, McNamara R, Playle R, et al. Neurofeedback training for alcohol dependence versus treatment as usual: study protocol for a randomized controlled learners and non-learners in frontal alpha neurofeed- back: JM, Soneji D, et al. Control over brain activation and pain learned by using real-time functional MRI. Proc Natl Acad Sci U S A 2005; 102: 18626-31. Dempster T, Vernon D. Identifying indices rofeedback 1-28. doi: 10.1007/s11229-019-02456-7. Engel Minimal clinically important differ- ence: a review of outcome measure score interpretation. Rheum tions: Improving executive functions with behavioral training, neuro- tool to modulate cognition and behavior: a review tutorial. Front D \u00b4az-Comas L, Gala \u00b4n L, et al. Neurofeedback in learning S, Barnes GR, Henson RN, Hillebrand A, Jensen O, et al. Good practice for conducting and reporting MEG research. NeuroImage 2013; children with attention-decit/hyperactivity disor- der (ADHD)-a controlled multicenter 2014; 91: 36-45.Kamiya J. The rst communications about EEG. J Neurother M, Neuper C, Wood G. Learning to modulate one's own brain activity: the effect of spontaneous mental strategies. Front Hum Neurosci 2013; 7: 695. Kolahi J, Bang H, Park J. Towards a proposal for assessment of blinding success in clinical trials: up-to-date review. correlates of EEG 55: 23-34. research: tutorial. Adv Methods Pract Psychol Sci 2018; 1: 259-69. Linden D. Brain control: developments in therapy and implications for society. . Basingstoke, Hampshire: Palgrave Macmillan; 2014. MacInnes RA. Cognitive neurostimulation: learning to volitionally Mu \u00a8ller S, Ackley E, Hellrung L, et al. Amygdala regulation following fMRI-neurofeedback instructed Front Range M, et al. Targeting the affective brain?a randomized controlled trial of Neuropsychopharmacology DMA, Williams AN, Whittaker JR, Krause F, Lu \u00a8hrs M, Wise RG, et al. Study pre-registration: Gradual real-time fMRI neurofeed- back training of motor imagery in middle cerebral artery stroke patients [Internet]. 2017. Available from: osf.io/qnsv7. Micoulaud-Franchi for method. Lancet Psychiatry 2016; 3: e16. Micoulaud-Franchi J-A, Fovet T. A framework the truth of neurofeedback: comment Psychol 2018; 73: 933-5. Nan W, Rodrigues JP, Ma J, Qu X, Wan F, Mak PI, et al. Individual alpha neurofeedback training effect on short term memory. Int J Psychophysiol 2012; 86: 83-7. Nichols TE, Das S, Eickhoff SB, Evans AC, Glatard T, Hanke M, et al. Best practices in data analysis and sharing in neuroimaging using MRI. Nat Neurosci 2017; 20: 299-303. Nieuwenhuis S, BU, Wagenmakers E. analyses of interactions in neuroscience: problem signicance. A, Maurits N, Michel C, Pang E, et al. Best practices in data analysis and sharing in neuroimaging using MEEG. PsyArXiv 2018 Pichiorri Pisotta I, Molinari M, et al. Brain-computer interface boosts motor imagery practice during stroke recovery. Ann Neurol 2015; 77: 851-65. Pigott HE, Trullinger M, Harbin H, Cammack J, Harbin F, Cannon R. Confusion regarding operant conditioning of the EEG. Lancet Psychiatry 2017; 4: 897. Ros T, brain Saj A, Vuilleumier P. Increased alpha-rhythm dynamic range from https://academic.oup.com/brain/article/143/6/1674/5807912 by University of Zurich user on 24 September 2020Schabus M, Griessenberger H, Gnjezda MT, Heib study primary insomnia. Brain 2017; 140: 1041- 52. Schafer RJ, Moore T. Selective attention from voluntary control of neurons in prefrontal cortex. Science 2011; 332: PM, et al. Neurofeedback, sham neurofeedback, and cogni- tive-behavioural group therapy attention-decit hyper- activity disorder: a triple-blind, randomised, controlled trial. 4: Scho\u00a8nenberg Wiedemann E, Schneidt A, Scheeff J, Logemann A, Keune PM, et al. Confusion regarding operant conditioning of the EEG-authors' reply. Lancet Psychiatry 2017b; 4: Moher D. statement: updated guidelines for reporting parallel group randomized trials of TO. Ann Intern Med 2010; 152: 726-32. Sitaram R, Ros T, Stoeckel LE, F, Lewis- Peacock J, et al. Closed-loop brain training: the science neuro- feedback. Nat 2017; 18: 86-100. Sorger B, Kamp T, Weiskopf N, When the brain takes 'BOLD' steps: real-time fMRI neurofeedback can further enhance Young KD. Control freaks: Towards optimal selection KM, Brennan RT, Perrin EC. Neurofeedback and cognitive attention training for children with attention-decit hyperactivity disorder in schools. J of spindle-burst sleep by conditioning of electroencephalographic activity while awake. Science 167: Arana M, et al. Neurofeedback of slow cortical potentials in children with attention-decit/hyperactivity disorder: a multicenter randomized trial of neurofeedback for 140: 862-4. Thibault The climate of neurofeedback: scien- tic rigour and the perils of ideology. Brain 2018; 141: e11. Thibault RT, Raz A. peutics. Lancet Psychiatry 2016; 3: e18. Thibault RT, A. When can neurofeedback join the clinical arma- mentarium? Lancet Psychiatry 2016; 3: 497-8. Thibault RT, Raz A. The psychology of neurofeedback: clinical inter- vention even if applied placebo. Am Psychol 2017; 72: 679-88. Watanabe T, Sasaki Wood G. Noisy but not placebo: dening metrics for effects of neurofeedback. Brain 2018: 1-3. Wood G, Kober SE. EEG Zotev V, Phillips R, et al. Real-time functional magnetic resonance imaging amygdala neuro- feedback changes positive information processing in major Misaki M, Yuan al. Randomized clinical trial of real-time fMRI amygdala neurofeed- back for major depressive disorder: effects on symptoms and auto- biographical memory recall. Am J Psychiatry 2017b; 174: 748-55. Young KD, Zotev V, Phillips et al. Real-time FMRI neurofeedback training of amygdala activity in patients disorder. PLoS ONE 2014; 9: e88785. Zich C, S, Maurer S, Kranczioch C. Lateralization patterns of covert but not overt movements change with an of the upper alpha by University of Zurich user on 24 September 2020 "}