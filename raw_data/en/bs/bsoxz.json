{"title": "Analyzing sentiment towards the Covid vaccines Using pretrained Huggingface models", "author": "Alidu Abubakari", "url": "https://medium.com/@alidu143/analyzing-sentiment-towards-the-covid-vaccines-using-pretrained-huggingface-models-8222d2e3610d", "hostname": "medium.com", "description": "Case Study:", "sitename": "Medium", "date": "2023-05-14", "cleaned_text": "Analyzing sentiment towards the Covid vaccines Using pretrained Huggingface models Case Study: The COVID-19 pandemic has had a significant impact on the world, affecting economies, societies, and the lives of individuals around the globe. One of the most significant efforts to combat the pandemic has been the development and distribution of COVID-19 vaccines. While the vaccines have been proven to be highly effective in preventing severe illness and death, there has also been a lot of misinformation and controversy surrounding their safety, efficacy, and distribution. One way to understand public opinion and attitudes towards COVID-19 vaccines is through sentiment analysis of social media data, particularly Twitter. Twitter is a popular platform for discussing current events and has been used extensively to discuss the COVID-19 pandemic and vaccines. Sentiment analysis of COVID-19 vaccine tweets can provide insights into the attitudes and emotions of people towards the vaccines. To conduct a sentiment analysis of COVID-19 vaccine tweets, we can use pre-trained models from Huggingface, which have been trained on large amounts of text data and can be fine-tuned for various NLP tasks, including sentiment analysis. We can collect a dataset of tweets related to COVID-19 vaccines and use these pre-trained models to classify the tweets into positive, negative, or neutral sentiments. The results of the sentiment analysis can be used to gain insights into public opinion and attitudes towards COVID-19 vaccines. For example, we can identify common themes and concerns in the negative tweets, such as safety, side effects, or distribution. These insights can be used by public health officials and policymakers to address concerns and improve public trust and confidence in the COVID-19 vaccine. Sentiment analysis of COVID-19 vaccine tweets using pre-trained models from Huggingface therefore can provide valuable insights into public opinion and attitudes towards the vaccines. These insights can be used to address concerns and improve public trust and confidence in the vaccines, ultimately contributing to the global efforts to combat the COVID-19 pandemic. In this article I will be presenting on how to develop a complete notebook for analyzing sentiment in text. STEP1: Import libraries # Import libraries import os import uuid import pandas as pd import numpy as np from scipy.special import softmax import gradio as gr from google.colab import drive from datasets import Analyzing social media sentiment towards Covid vaccines is a crucial task in understanding public opinion about the vaccines. In this article, we will explore how to use pretrained Huggingface models to analyze social media sentiment towards Covid vaccines. This first step describes importing the necessary libraries that will be used in analyzing sentiment towards Covid vaccines using pre-trained Huggingface models. Python will be used for the analysis, and popular libraries like pandas, NumPy, and SciPy will be utilized. Additionally, the Huggingface transformers library will be used for pre-trained models for natural language processing. Other important libraries include the os library for interacting with the operating system, the uuid library for generating unique identifiers, and the gradio library for creating web-based user interfaces for machine learning models. The datasets library will be used for manipulating datasets, and several classes and functions from the transformers library will be used for fine-tuning and training models. These include the AutoTokenizer class for tokenizing input data, the AutoConfig class for configuring the model, the AutoModelForSequenceClassification and TFAutoModelForSequenceClassification classes for loading pre-trained sentiment analysis models, the IntervalStrategy class for setting the interval for saving checkpoints, the TrainingArguments class for specifying training arguments, the Trainer class for training the model, and the EarlyStoppingCallback class for stopping training if the validation loss does not improve after a certain number of epochs. STEP 2: Load the dataset # Load the CSV file into a DataFrame url = \"https://github.com/Azubi-Africa/Career_Accelerator_P5-NLP/raw/master/zindi_challenge/data/Train.csv\" df = pd.read_csv(url) To begin the sentiment analysis of Covid vaccines, the first step is to load the CSV file containing the data into a Pandas DataFrame. This is done using the \"read_csv\" function from the Pandas library. In the code above, we have assigned a URL link that points to the location of the CSV file on the internet. The \"pd.read_csv\" function reads the data from this URL and loads it into the DataFrame \"df\". The CSV file contains the data that will be analyzed, and the DataFrame is a useful tool for managing and processing large amounts of data. Once the data has been loaded into the DataFrame, we can start exploring and manipulating it. This includes checking for missing or null values, filtering, and sorting the data to extract meaningful insights. It is important to note that the URL link used in the code above is specific to the dataset being analyzed. In practice, the URL link would be replaced with a link pointing to the location of the data file to be analyzed. Additionally, the \"read_csv\" function can be customized with various parameters to adjust how the data is read and processed, depending on the specific requirements of the analysis. df.info() STEP 3: Data Quality checks Before analyzing the sentiment of the text-based data, it is important to perform data quality checks to ensure the accuracy and reliability of the results. The following are some data quality checks that can be performed: Data cleaning: This involves removing irrelevant or duplicate data, correcting misspellings, and removing any special characters or symbols that may interfere with the analysis. This step ensures that the text data is consistent and ready for analysis. Data sampling: It is important to ensure that the data is representative of the entire population. A random sample can be taken to ensure that the data is not biased. Data preprocessing: This involves converting the text data into a format that can be analyzed. Text preprocessing techniques such as tokenization, stemming, and stop word removal can be applied to reduce the dimensionality of the data and improve the efficiency of the analysis. Labeling: For a sentiment analysis project, the text data needs to be labeled with the corresponding sentiment category (positive, negative, or neutral). This can be done manually or by using pre-existing labeling tools. Data validation: It is important to validate the labeled data to ensure that it is accurate and consistent. This can be done by comparing the results of multiple annotators or by using machine learning models to validate the labels. Performing these data quality checks ensures that the analysis is based on accurate and reliable data, which improves the accuracy and effectiveness of the sentiment analysis results. For the purpose of this work, I had to: First check the missing values # Select rows with missing values df.isnull().sum() # Select rows with missing values df[df.isnull().any(axis=1)] # Select row by index and assign values to columns df.loc[4798, 'label'] = 0 df.loc[4798, 'agreement'] = 0.666667 # Use .iat[] and update safe_text column df.iloc[4798, df.columns.get_loc('safe_text')] = complete_text # Generate random UUID string for tweet_id '''UUIDs are often used in software applications for various purposes such as generating unique IDs for entities, tracking unique user sessions, or creating unique file names''' rand_tweet_id = str(uuid.uuid4()) # Select row by index and assign values to columns row_index = 4799 .iat[] update safe_text column df.iloc[row_index, df.columns.get_loc('safe_text')] = df.iloc[row_index, 1] Secondly ; We split the DataFrame df into two subsets, train and eval, for training and evaluation respectively. The split is performed using train_test_split function from scikit-learn library, which randomly divides the data into a training set and an evaluation set, with a specified ratio of the test size (in this case, 0.2 or 20% of the original data). # Split the train data => {train, eval} train, eval = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label']) Check the first few records of the dataset STEP 4: load the data for use by Huggingface. # Save splitted subsets # Define load_dataset('csv', data_files={ 'train': '/content/drive/MyDrive/NLP-Sentiment-Classification/train_subset.csv', 'eval': '/content/drive/MyDrive/NLP-Sentiment-Classification/eval_subset.csv' }, encoding='ISO-8859-1') To achieve that, we save the train and evaluation subsets generated from the previous code snippet as CSV files in Google Drive. The file path is defined as '/content/drive/MyDrive/NLP-Sentiment-Classification'. Here, the os.path.join() method is used to concatenate the file path with the name of the CSV files to create the full file path. The to_csv() method is then used to save the train and eval subsets as CSV files in Google Drive. The index=False argument is passed to exclude the index column in the CSV files. Lastly, the load_dataset() function from the Hugging Face datasets library is used to load the CSV files into a dataset. The data files are specified using a dictionary where the keys are the names of the subsets ( train and eval) and the values are the file paths to the corresponding CSV files. The encoding parameter is set to ISO-8859-1 to handle any encoding issues that may arise during loading of the CSV files. STEP 5: Define your model parameters The first parameter is to define is the Tokenizer. AutoTokenizer is used to instantiate a tokenizer. AutoTokenizer is a class in the Transformers library that provides a convenient way to automatically select the appropriate tokenizer for a given pre-trained model. The AutoTokenizer class uses heuristics to determine the type of tokenizer that should be used based on the architecture and configuration of the pre-trained model. This can be useful when working with a variety of pre-trained models, because it allows you to use the appropriate tokenizer without having to manually select one for each model. tokenizer = AutoTokenizer.from_pretrained('bert-base-cased') This code instantiates a tokenizer for the BERT (Bidirectional Encoder Representations from Transformers) pre-trained model with the bert-base-cased configuration. The bert-base-cased configuration refers to a version of the BERT model that has a cased vocabulary, meaning that it distinguishes between uppercase and lowercase letters. This can be useful in tasks where the case of words is important, such as named entity recognition or sentiment analysis. The same idea is used to load Tokenizer for any other model like Distillbert etc. By instantiating a tokenizer for the bert-base-cased model using AutoTokenizer.from_pretrained(), you can tokenize text according to the same scheme used during pre-training of the BERT model. This can be useful when fine-tuning the pre-trained model on a specific task, because it ensures that the input data is pre-processed in the same way as the data used to train the original model. STEP 6: Data and Label transform # Define a function to transform the label values def transform_labels(label): # Extract the label value label = label['label'] # Map the label value to an integer value num = 0 if label == 1: #'Positive' num = 2 # Return a dictionary with a single key-value pair return {'labels': num} # Define a function to tokenize the text data def tokenize_data(example): # Extract the 'safe_text' value from the input example and tokenize it return tokenizer(example['safe_text'], padding='max_length') # Apply the transformation functions to the dataset using the 'map' method # This transforms the label values and tokenizes the text data dataset_out = dataset.map(transform_labels) dataset_base = dataset_out.map(tokenize_data, batched=True) # Define a list of column names to remove from the dataset remove_columns = ['tweet_id', 'label', 'safe_text', 'agreement'] # Apply the 'transform_labels' function to the dataset to transform the label values # Also remove the columns specified in 'remove_columns' dataset_base = dataset_base.map(transform_labels, remove_columns=remove_columns) We need to preprocess text data for sentiment analysis using Hugging Face's transformers library. To do this we define the following. The transform_labels() function is used to transform the label values in the dataset to integer values. The tokenize_data() function is then used to tokenize the text data using the tokenizer provided by the transformers library. The map() method is used to apply the transformation functions to the dataset. The batched parameter is set to True to enable processing the data in batches, which can be more efficient for large datasets. The remove_columns parameter is used to specify a list of columns to be removed from the dataset. The columns specified in remove_columns are removed from the dataset because they are not needed for the subsequent analysis or model training. tweet_id: This column contains unique identifiers for each tweet, which are not relevant for the analysis or modeling. label: This column contains the original label values, which have already been transformed into numerical values using the transform_labels function. safe_text: This column contains the preprocessed text data that has already been tokenized and encoded, so it is not needed for subsequent analysis or modeling. agreement: This column indicates the level of agreement among the annotators for each tweet. While this information might be useful for some analyses, it is not necessary for the sentiment analysis task at hand. By removing these columns, the resulting dataset is more compact and easier to work with, while retaining all the relevant information for the sentiment analysis task. STEP 7: Define the training arguments We use the standard training argument definition as shown below. training_args = TrainingArguments( output_dir='./results', # Directory where the model checkpoints and evaluation results will be stored evaluation_strategy=IntervalStrategy.STEPS, # Interval for evaluating the model during training (every specified number of steps) save_strategy=IntervalStrategy.STEPS, # Interval for saving the model during training (every specified number of steps) save_steps=500, # Number of steps between two saves load_best_model_at_end=True, # Whether to load the best model at the end of training num_train_epochs=10, # Number of training epochs per_device_train_batch_size=2, # Batch size per GPU for training per_device_eval_batch_size=2, # Batch size per GPU for evaluation learning_rate=3e-5, # Learning rate weight_decay=0.01, # Weight decay warmup_steps=500, # Number of warmup steps logging_steps=500, # Number of steps between two logs fp16=True, # Whether to use 16-bit precision gradient_accumulation_steps=16, # Number of steps to accumulate gradients before performing an optimizer step dataloader_num_workers=2, # Number of workers to use for loading data push_to_hub=True, # Whether to push the model checkpoints to the Hugging Face hub hub_model_id=\"Abubakari/finetuned-Sentiment-classfication-BERT-model\", # Model ID to use when pushing the model to the Hugging Face hub ) #use hub_model_id=\"finetuned-Sentiment-classfication-ROBERTA-model #use EarlyStoppingCallback( early_stopping_patience=3, # Number of epochs with no improvement before stopping training early_stopping_threshold=0.01, # Minimum improvement in the metric for considering an improvement ) # Combine the training arguments and the early stopping callback training_args.callbacks = [early_stopping] The function above defines the training arguments and the early stopping callback for fine-tuning a pre-trained transformer model for sentiment classification using Hugging Face's Transformers library. The TrainingArguments class defines various hyperparameters for training, such as the number of training epochs, batch size, learning rate, and optimization settings. The early stopping callback is used to monitor the validation loss and stop the training process early if the model's performance does not improve over a certain number of epochs. The code also specifies the model ID for pushing the trained model checkpoints to the Hugging Face model hub. STEP 8: Load a pretrain model Loading a pretrain model while specifying the number of labels in our dataset for fine-tuning. AutoModelForSequenceClassification is a class in the Transformers library that is used for sequence classification tasks, where the input is a sequence of text and the output is a label or category assigned to that sequence. The benefit of using AutoModelForSequenceClassification is that it automatically selects the appropriate pre-trained model architecture based on the specified configuration and dataset. This makes it easy to fine-tune pre-trained models for various sequence classification tasks without having to manually select the appropriate model architecture. model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=3) STEP 9: Define Evaluation Metrics eval_dataset_base = dataset_base['eval'].shuffle(seed=10) The next step is to define a function compute_metrics that computes the root mean squared error (RMSE) metric for evaluating the model's performance during training. The function takes in a tuple of logits and labels from the evaluation dataset predictions and computes the RMSE by comparing the predicted labels to the actual labels. The train_dataset_base and eval_dataset_base variables are then created by shuffling the training and evaluation datasets, respectively, with a seed value of 10. This ensures that the data is randomly sampled during training and evaluation. STEP 10: Train! Train !! Train !!! First is to define the Trainer object: trainer_base the Trainer object from the transformers library with the specified training arguments, model, training and evaluation datasets, and the compute_metrics function to calculate evaluation metrics. The compute_metrics function takes in the evaluation predictions and returns the calculated evaluation metric. This is a crucial step in training the sentiment classification model using BERT. Train The model Training process on the specified train_dataset. Training the sentiment classification model using BERT or any of the loaded pretrained Models. trainer_base.train() The Trainer object is a high-level interface provided by the transformers library to facilitate training a neural network model. It allows us to easily specify the training and evaluation datasets, as well as the training parameters and the optimization algorithm. During training, the Trainer object performs the forward and backward passes through the neural network and updates the model's parameters using the optimization algorithm. In addition to tracking the training progress, the Trainer object also provides various other functionalities, such as logging the training metrics, saving and loading the model checkpoints, and pushing the final model to the Hugging Face model hub. The compute_metrics function is a callback function that is called after each evaluation step during training. It takes in the predicted outputs and the actual labels for the evaluation dataset and computes any desired evaluation metrics. In this case, the function computes the root mean squared error (RMSE) between the predicted and actual labels, which is a commonly used metric for regression problems. STEP 11: Evaluation Evaluating the Performance of BERT-base on the Sentiment Analysis Task # Evaluate the model eval_results = trainer_base.evaluate() # Create a dictionary of the evaluation DataFrame from the dictionary results_df = pd.DataFrame([results_dict]) The trainer_base.evaluate() method is used to evaluate the performance of the trained model on an evaluation dataset. When called, the method will run the evaluation dataset through the model and compute the loss and any specified metrics (in our case, RMSE) on the predicted outputs compared to the actual outputs. The method returns a dictionary containing various evaluation metrics, including the evaluation loss, the RMSE (in our case), the total evaluation runtime, the number of samples processed per second, the number of steps processed per second, and the epoch number. These evaluation metrics can be used to compare the performance of different models or to track the performance of the same model over different epochs. # Print the results print(results_df) Model Loss RMSE Runtime Per Second \\ 0 Bert_base 0.603271 61.019 Steps Per Second Epoch 0 30.525 10.0 Based on the results, the model achieved a loss of 0.603271 and an RMSE of 0.675109 on the evaluation dataset. The loss value indicates how well the model is able to predict the target variable, with lower values indicating better performance. The RMSE value measures the average difference between the predicted and actual values, with lower values indicating better performance. The evaluation was performed after the model was trained for 10 epochs, with an average runtime of 32.7931 seconds per evaluation. The model was able to process an average of 61.019 samples per second and 30.525 steps per second during evaluation. Overall, these results suggest that the model was able to achieve reasonable performance on the sentiment classification task. However, further analysis may be needed to fully understand the model's strengths and weaknesses and how it compares to other models or approaches. STEP 12: PUSH TO HUB Push the final fine-tuned model to the Hugging Face model hub The trainer_base.push_to_hub() method uploads the trained model to the model hub, making it available for others to use or fine-tune for their own applications. Similarly, the tokenizer.push_to_hub() method uploads the tokenizer used to preprocess the input data, which is required for using the model to make predictions. # trainer_base.push_to_hub(\"Abubakari/finetuned-Sentiment-classfication-BERT-model\") tokenizer.push_to_hub(\"Abubakari/finetuned-Sentiment-classfication-BERT-model\") By sharing the model and tokenizer on the Hugging Face model hub, others can easily download and use the trained model for sentiment analysis tasks without having to go through the entire training process themselves. It also makes it easier for the community to collaborate and improve upon existing models, as they can access and fine-tune each other's work. STEP 13: Test! Test!! Test!!! Load your model from anywhere using from_pretrained! # Load the tokenizer tokenizer = tokenizer.from_pretrained(\"Abubakari/finetuned-Sentiment-classfication-BERT-model\") # 1: \"neutral\", 2: \"positive\"} # Make predictions on some example text result = model(\"I love these covid vaccines.\") # Map the numerical label to the corresponding class name result[0][\"label\"] = label_map[int(result[0][\"label\"].split(\"_\")[1])] # Print the predicted label and score print(result) In this block, we load the previously fine-tuned BERT model and its associated tokenizer from the Hugging Face model hub using their unique identifiers. We then define a label map to map the numerical labels to their corresponding class names (negative, neutral, positive) for easy interpretation of the model's predictions. Finally, we use the loaded model to make predictions on some example text and print the predicted label and score. This snippet showcases how to use the fine-tuned BERT model for sentiment classification tasks after it has been trained and saved. Please check out the actual notebook on "}