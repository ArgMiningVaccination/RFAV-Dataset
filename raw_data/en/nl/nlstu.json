{"title": "PDF", "author": "PDF", "url": "https://www.massgeneral.org/assets/MGH/pdf/Research/dcr/AbstractBook2019.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "Day 17th Annual Celebration of Clinical Research October 3, 201 9 8 am - 1 pm O'Keeffe Auditorium & Bulfinch Tents i Welcome to Clinical Research Day 2019 Clinical Research Day is an annual celebration of clinical and translational investigators and their accomplishments over the past twelve months at the Massachusetts General Hospital (MGH) . Every year the clinical research community at MGH becomes more vibrant and robust with this year being no exception. This year, we had a record breaking 3 56 abstract submissions reflecting the exciting work being conducted by clinical research teams here at MGH. Clinical Research Day is a great opportunity for interaction between clinical researchers and for learning more about the different types of research being conducted at our Institution. It is also an opportunity for sharing ideas that may contribute to strategic thinking and future planning. The keynote speaker for Clinical Research Day 2019 is Hal Dietz , M.D. Dr. Dietz is the Victor A. McKusick Professor of Medicine and Genetics, Investigator, Howard, Hughes Medical Institute, Departments of Pediatrics, Medicine, and Molecular Biology & Genetics, Johns Hopkins University School of Medicine. Dr. Dietz will present on \"Leveraging Nature's Success: Lessons from Modifiers of Cardiovascular Disease\" . Following the Keynote Address by Dr. Dietz , there will be two poster s essions and a panel discussion: 9 to 11 am Poster Session I Bulfinch Tents 11 to noon Panel Discussion O'Keeffe Can Genetic Studies Lead to New Treatment? Auditorium Moderator: Maurizio Fava, M.D., Director, Division of Clinical MGH Research Institute Panel Members: Hal Dietz, M.D ., Victor A. McKusick Professor of Medicine and Genetics, Investigator, Howard, Hughes Medical Institute, Departments of Pediatrics, Medicine, and Molecular Biology & Genetics, Johns Hopkins University School of Medicine Heidi Rehm, Ph.D , Chief Genomics Officer, Department of Medicine and Center for Genomic Medicine, MGH; Medical Director, Genomics Platform, Broad Institute of MIT and Harvard Roy Perlis, M.D., MSc, Professor of Psychiatry, Harvard Medical School; Director, Center for Quantitative Health, Massachusetts General Hospital Luk H. Vandenberghe, Ph.D , Grousbeck Associate Professor in Therapy Center; Associate Director, Ocular Genomics Institute; Broad Institute, Mass Eye and Ear, Harvard Kali Stasi, MD, Ph.D., Director Translational Medicine Ophthalmology, Novartis Institutes for BioMedical Research 11:30am to 1pm Poster Session II Bulfinch Tents ii Acknowledgements Clinical Research Day is a collaborative effort amongst the clinical research community. We would like to acknowledge investigators who submitted abstracts, mentors who encouraged them and departmental leaders who continuously nurtur e clinical investigation at MGH. They deserve our collective gratitude for doing this important work. We would also like to thank the chiefs for their sponsorship of departmental awards : Jeanine P. Wiener -Kronish, M.D., Anesthesia David E. Fisher, M.D., Ph.D., Dermatology David F .M. Brown , M.D., Emergency Medicine James A. Gordon, M.D ., MPA, Chief Learning Officer/Education Research Katrina A. Armstrong , M.D., Medicine Merit E. Cudkowicz , M.D., Neurology Jeffrey L. Ecker, M.D., Ob/Gyn David N. Louis , M.D., Pathology Ronald E. Kleinman, M.D., Pediatrics Jerrold F. Rosenbaum , M.D. , Psychiatry James A. Brink, M.D., Radiology Keith D. Lillemoe , M.D., Surgery Faculty and staff of the MGH Division of Clinical Research contributed valuable ideas, time and energy to m ake this day possible. We thank you all for your help and support. Special thanks are due to: Andrew Nierenberg, M.D., and Karen Miller, M.D., Co- Director s, Training and Education, MGH Research Institute Teresa La, Stacey Grabert, Tatiana Koretskaia A very special thank you to Tiereny Morrison -Rohlfs for her leadership and hard work to make Clinical Research Day a success. iii Keynote Speaker Hal Dietz, MD, Victor A. McKusick Professor of Medicine and Genetics Investigator, Howard Hughes Medical Institute Departments of Pediatrics, Medicine, and Molecular Biology & Genetics Johns Hopkins University School of Medicine. Hospital- Wide Awards $2,500 Team Science Awards (2) $500 Translational Science Award (3) The sulfur microbial diet, sulfur -metabolizing bacterial communities, and risk of colorectal cancer 244 Nguyen, Long H; Song, Mingyang; Medicine - Gastroenterology Effectiveness of the ESM -UBT Package for Management of Uncontrolled Post - partum Hemorrhage among Referral Facilities in Maharashtra and Madhya Pradesh , India: A Retrospective Difference -in-Difference Analysis 254 Poster ID # One-year effectiveness and cost -effectiveness of lifestyle intervention for type 2 diabetes in primary care: The REAL HEALTH -Diabetes Randomized Clinical Trial 71 Delahanty, Linda M .; Chang, Amy; Wexler, Deborah J Medicine - Endocrine - Diabetes Implementing a Community Health Worker Intervention at Hospital Discharge 130 Carter, Jocelyn Alexandria ; Walton, Anne; Hassan, Susan; Tho rndike, Anne; Donelan, Guha, Moytrayee; Garg, Lorraine F.; Burke, Thomas F. Emergency Medicine Development of a machine learning model to predict neonatal follow -up bilirubin leve ls and comparison with clinician performance 50 Chou, Joseph H. Pediatrics $500 Clinical and Outcome Research Award (3) A higher -sensitivity LAM assay for TB testing in hospitalized patients with HIV: cost Medicine Minimally Behavioral Changes in Smokers from the PCOR I Pragmatic Trial \"Integrated Smoking Cessation Treatment for Smokers with Serious Mental Klebsiella Pneumo niae and Quinolones on Survival of patients Treated with Gemcitabine for Pancreatic S.; Carlos Department of Surgery vi Poster number Anesthesia Genetic basis of daytime napping and consequences on cardiometabolic health Hassan Dashti $1,000 68 A Novel Application of a Systems Theory -Based Prospective Safety Model to Medication Use in the Operating Room Environment Aubrey Samost -Williams $500 279 Dermatology Novel Strategy to based upon Genomic Features: A Potential Anti -Tumor Mechanism Ugoji 250 Emergency Medicine Prevalence of Health -Related Social Needs in the Emergency Department Melanie Molina $750 229 Health Professions Education Research Clinicians and trainees performing outside scope of training (POST) during international global health activities Brett Nelson $1,000 242 Multicenter Study of Influence of Gender on Resident Assessment in Internal Medicine Residency Training Programs Nneka Ufere Honorable Mention 328 Medicine Hypertensive Disorders of Pregnancy and Long -Term Cardiovascular Risk Michael Honigberg $1,000 135 Inherited Causes and Clinical Consequences of Clonal Hematopoiesis from 100,002 Whole Genomes Alexander Bick $500 31 Neurology ICU-SLEEP: Investigation of Sleep in the Intensive Care Unit 314 Ryan Tesh $500 Clinical and Electrographic Predictors of Medication Responsiveness in Acute Brain Injury 275 Daniel Rubin $500 Ob/Gyn Clinical Trial Participation and Measures of Aggressive Care at the End of Life in Patients with Ovarian Cancer Alexandra Bercow $500 27 Assessment and Selection of Human Embryos Destined to Implant using Deep Convolutional Neural Networks (CNN) Stylianos Vagios $500 331 vii Pathology Whole genome sequencing analysis of E. faecium clinical isolates reveals novel sequence types and high accuracy prediction of antimicrobial resistance Melis Anahtar $250 12 Clinically Integrated Molecular Diagnostics in Adenoid Cystic Carcinoma Julie Thierauf $250 317 Pediatrics Development of a machine learning model to predict neonatal follow -up bilirubin levels and comparison with clinical performance Joseph Chou $500 50 Psychiatry A phenome -wide approach to identifying modifiable resilience factors for depression Karmel Choi $200 49 Behavioral Changes in Smokers from the PCORI Pragmatic Trial \"Integrated Smoking Cessation Treatment for Smokers with Serious Mental Illness Rachel Plummer $200 195 Association between automotive assembly plant closures and opioid overdose mortality in the United States Alexander Tsai $200 324 Association between cannabis use and delusional beliefs in a college student sample Samantha Hines $100 126 Connectivity of a peripersonal space -monitoring network predicts personal space and social motivation in schizophrenia Sarah Zapetis $100 352 A Randomized Controlled Trial of a Campus -based Resilience Training Intervention for At -Risk College Student Wisteria Deng $100 72 JUUL in school: Pervasive, persistent, but not clearly tied to smoking Jenny Zhang Honorable Mention 354 Psychotropic medication effects on repetitive transcranial magnetic stimulation (rTMS) treatment response Sofia Uribe Honorable Mention 330 Assessment of a Novel Health and Fitness Program for Veterans with PTSD and/or TBI Megan McCarthy Honorable Mention 211 NASA Integrated One -Year Mission Protocol: Evaluating Physiological Correlations with Impacted Operational Proficiency using NINscan -SE during spaceflight Katie Harris Honorable Mention 144 From Ecology to Psychology: Critical Slowing Down as a Predictor of Panic Attack Vulnerability Olivia Losiewicz Honorable Mention 194 viii Radiology Portal Venous Pulsatility 18 Cost-effectiveness analysis of non -invasive FFRCTfor stable chest pain evaluation - A comparison to coronary CTA and functional testing based on the PROMISE Trial Julia Karady $500 209 Surgery Impact of Klebsiella Pneumoniae and Quinolones on Survival of patients Treated With Pancreatic Cancer Maximilian Weniger $250 342 The Autotaxin -Lysophosphatidic Acid Signaling Axis Stimulates Aggressive Tumor Biology and Desmoplastic Response in Pancreatic Cancer Mozhdeh Sojoodi $250 296 Division of Clinical Research (DCR) Mission: Increase quantity, quality and efficiency of translating basic science advances into improved care for our patients . Consultations with DCR faculty: Bioinformatics Biostatistics Biostatistics for K Awardees Community Access, Recruitment, and Engagement Drug Discovery Rounds Global Health Research Imaging Biomarkers K Grant Writing Mentoring Corner Nursing Research Omics Research Patient -Centered Outcomes Research Philanthropy Education Project Management Qualitative and Mixed Methods Research Quantitative Health Research Research Ethics Consultation Study Hypothesis and Design Survey Research Translational Medicine Trial Innovation To learn more about DCR and request Consultations and Se rvices, please, go to: www.massgeneral.org/dcr 11 Jonathan R. Abraham, MPH, Performance Analysis and Improvement/Practice Improvement Enhancing Provider Engagement in Patient Experience J.R. Abraham Office of Patient Experience, Massachusetts General Hospital, Boston, MA, USA Introduction: While many providers in health care will acknowledge patient experience as an important aspect of their care, it is often challenging to achieve a high level of engagement in this domain with multiple priorities at hand. Yet, with a greater emphasis on patient experience results across health care institutions, there remains a case to find innovative ways to overcome these barriers. At Massachusetts General Hospital (MGH), nearly 2000 clinically active physicians in the Massachusetts General Hospital Physicians Organization (MGPO) participate in a Quality Incentive Program (QI program), which is administered by the MGPO. MGH senior leadership sought to leverage the QI program to further engage physicians in improving patient experience with specific tactics and goals. The primary aim of this initiative was to determine whether leveraging the QI program between January 2018 and March 2019 would be effective in engaging providers on a broad scale to improve patient experience results, and ultimately, the experience of patients and families.Methods: Following approval from MGPO senior staff and with buy-in from Clinical Department Quality Leaders representing 21 departments at MGH, consensus was reached that each eligible physician would be engaged in the following key components of this initiative between January 2018 and March 2019: 1) review his/her patient experience data and comments at select time points, 2) attend an educational session around a patient experience topic of their choice, and 3) contribute to the design of and implement their respective department's project addressing patient experience. Hospital- aggregate level patient experience data between January 2017 and May 2019 were analyzed to assess the degree to which the QI Patient Experience initiative improved scores from baseline.Results: Data suggest that the initiative with its various components resulted in an increase in the hospital-aggregate CG-CAHPS 'Rating of Provider' composite score (84.5%) compared to the baseline average (83.7%). Moreover, a sustained positive shift (8 consecutive months of scores above average) occurred during the periods of the educational sessions and the planning/implementation of departmental projects. Conclusion: While acknowledging the many limitations, this initiative highlights how awareness of patient experience data, educational trainings, and the provision of practical interventions can culmulatively enable a culture of improving patient experience. To sustain our improvement results, a symposium will be held to facilitate peer learning and discussion on ways to replicate department projects to other areas of the hospital. In addition, patient experience data and comments will continue to be distributed every six months. Finally, individual departmental consultation meetings will be held to discuss ways to incorporate real-time surveying tools to solicit patient feedback on the projects that have already been started and drive further improvement efforts. 22 Paul F. Abraham, BS, Orthopedics Perioperative Opioid Usage, NSAID Usage, and Pain Levels with the Potentially Modulating Effect of Bone Marrow Aspirate Concentrate Therapy during Arthroscopic Acetabular Labral Repair Nazal, J. Gibbs and S.D. Martin Orthopaedic Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: As the surgical field continues to progress towards minimally invasive procedures with the hopes of reducing postoperative pain and recovery time, hip arthroscopy is a rapidly growing area of Orthopaedic Surgery. With the nation in the midst of an opioid epidemic, understanding patients' usage of opioid and non-steroid anti-inflammatory drug (NSAID) medication after surgery is critical to balancing adequate pain management and judicious medical resource utilization. Within the technical limitations of the hip joint, bone marrow aspirate concentration (BMAC) therapy is a viable treatment for addressing cartilage wear and may have beneficial anti-inflammatory effects due to the numerous growth factors and anti-inflammatory proteins present in BMAC. We sought to evaluate the effect of BMAC on pain levels and pain medication utilization after hip arthroscopy.Methods: A prospective cohort of 34 patients underwent hip arthroscopy for labral repair between July 2018 and March 2019. Inclusion criteria were hip arthroscopy for labral tear and above the age of 18 years old. Upon gaining arthroscopic visualization, the hip joint was assessed for degeneration of the cartilage and chondrolabral junction. BMAC was used in hip joints with appreciable cartilage defects, but was not used if the hip joint had minimal or too extensive cartilage degeneration. Postoperatively, patients were prescribed 5 mg oxycodone for breakthrough pain of moderate intensity and instructed on proper use of both opioid and NSAID medication utilization. Before discharge from the PACU (post-anesthesia care unit), patients were given a perioperative VAS pain score and medication tracker. They were instructed to fill out the tracker over the next two weeks and to bring it to their suture removal appointment 12-14 days later.Results: The cohort consisted of 34 patients with 26 (76.4%) patients in the BMAC group, and 8 (23.6%) in the No BMAC group. The mean patient age was 27.6 years (SD, 5.97, range, 18 to 38), 19 (55.9%) of the patient were male, and 15 (44.1%) were female. Surgery was performed on the left hip in 20 (58.8%) patients, and on the right hip in 14 (41.2%). The mean VAS pain score at Postoperative Day #1 (POD#1) was 4.00 in the BMAC group, and 7.25 in the No BMAC group (p-value = .009). On POD#5, the mean pain score was 2.88 (BMAC) vs. 4.50 (No BMAC) (p-value=0.097). The mean pain level POD#10 was 1.63 (BMAC) vs 4.00 (No BMAC) (p-value = Finally, pain level at suture removal was 1.00 (BMAC) vs 2.50 (No BMAC) (p-value = .025). (Figure 1) The mean number of days that opioid medication was used was 3.13 days (BMAC) vs 5.75 days (No BMAC), which was borderline statistically significant with a p-value of 0.053. The mean total number of opioid pills was 6.61 pills (BMAC) vs 17.00 pills (No BMAC) (p-value = .007). Finally, when patients were asked about what was done with the leftover opioid pills, 30 (88.2%) patients reported the pills were still in their home/medicine cabinet, 1 (2.9%) flushed down sink or toilet, and 3 (8.8%) dropped off at a collection area. The mean number of days that NSAID medication was used was 6.75 days (BMAC) vs 8.00 days (No BMAC), with a p-value of 0.023. The difference between the mean total number of NSAID pills used and mean maximum number of NSAID pills used in a single day were not significant (p-value= 0.912 and 1.000, repectively).Conclusion: BMAC treatment appears to decrease postoperative pain levels, opioid usage, and NSAID usage. First, there was a statistically significant decrease in pain at POD#1, POD#10, and at suture removal. (Table 1) Second, there was a significant decrease in the total number of opioid pills used. Finally, there was a significant decrease in the number of days NSAID pills were used. Although this cohort size is small, data collection is ongoing, and further clinical and basic science study is needed to substantiate the anti-inflammatory effects of BMAC. Perioperative pain questions divided by BMAC group and No BMAC group, with mean average = Bone Marrow Aspirate Concentrate. POD = Postoperative Day. Figure 1. Postoperative pain level on a 10-point scale comparing BMAC group to the No BMAC group at POD#1, POD#5, POD#10, and Suture Removal (roughly POD#14). A p-value less than 0.05 indicated a statistical Significance Notation: *: Marrow Aspirate Concentrate. POD = Postoperative Day.33 Paul F. Abraham, BS, Orthopedics Arthroscopic Repair Compared to Physical Therapy for Symptomatic Acetabular Labral Tears: A Randomized Controlled Trial (RCT) in Over Abraham1, Martin1 Surgery, Massachusetts General Hospital, Boston, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Hip arthroscopy and physical therapy (PT) have both been shown to have good functional and pain outcomes in patients with symptomatic labral tears. Studies have found that arthroscopic labral repair has variable outcomes in older patients compared to the dependable positive outcomes in younger patients. Age older than 40 and presence of grade 4 Outerbridge changes at time of arthroscopy were found to be independently associated with high rates of progression to total hip arthroplasty (THA) or repeat hip arthroscopy, suggesting that higher rates of osteoarthritis (OA) explains some, but not all, of these differential outcomes between age groups. The purpose of this study is to compare the efficacy of arthroscopic repair to physical therapy alone in patients 40 years or older with a symptomatic labral tear and without severe OA.Methods: The inclusion criteria were: 40 years or older; diagnosis of acetabular labral tear; failure of a minimum of two months of physical therapy, and willingness to undergo randomization. The exclusion criteria were: significant osteoarthritis, as defined by less than 2mm of joint space or Tonnis Grade 3 changes; or previous hip surgery. Patients were randomized to either surgery and physical therapy (SPT group) or physical therapy alone (PTA group). Patients in the PTA cohort who had undergone at least 6 weeks of PT but felt that their PT results were unsatisfactory, despite their physical therapist determining that they had achieved the maximal improvement from PT, were allowed to cross over to undergo hip arthroscopy (CO group). Clinical outcome data consisted of patient reported outcome measures (PROMs); VAS pain score; and patient satisfaction. Preoperative outcomes were compared with follow-up at 12 and 24 months.Results: The study consists of 99 enrolled patients, with 51 (51.5%) patients in the SPT group and 48 (48.5%) patients in the PTA group. At the time of analysis, 32 (32.3%) patients failed to achieve adequate progression after a minimum of 6 weeks of PT, crossing over (CO group) to get hip arthroscopy. A total of 82 (82.8%) patients had completed at least 6-month follow-up, with an average follow-up of 19.0 months (range, 6-24 months). The mean patient age was 49.3 years (SD, 5.35, range, 40-67). There were 44 (44.4%) males and 55 (55.6%) females. Mean Tonnis grade was 1.26 (SD, 0.49). The SPT group and CO group experienced significant improvement in all six PROMs, even after Tukey adjustment, while the PTA group significantly improved in only the modified Harris Hip Score. Interestingly, comparison of PROM improvement between the three groups was non-significant (Table 1-2). The mean VAS pain score decreased significantly for the SPT group (24 month p<0.001) and CO group (24 month p=0.005) but not the PTA group (12 months p=0.61; 24 months p=0.27). Finally, patient satisfaction was evaluated at 12 months. 96.3% of patients reported that they were satisfied with the treatment received and 90.7% of patients would choose the same treatment in the future if given the option. Conclusion: Patients managed by PT alone had significant improvement in only one PROM, while SPT and CO had signif- icant improvement in all six. Although there appears to be a higher improvement in PROMs in both the SPT and CO groups compared to the PTA group, this difference was non-significant. This may be due to moderate sample size and large standard deviations in each group. We believe these results indicate that surgical intervention may be preferred over PT for patients over the age of 40 with symptomatic acetabular labral tears and limited radiographic arthritis. However, additional patients and longer follow-up are necessary to further delineate these findings. Table 1. Comparison of Improvement from Enrollment to 12 months F/U by Group All median differences depicted as median and interquartile range (IQR); PT = Physical Therapy. mHHS = modified Harris Hip Score. HOS-ADL = Hip Outcome Score-Activity of Daily Living. HOS-SSS = Hip Outcome Score- Sports Sub-scale. NAHS = Non-arthritic Hip Scale. iHOT-33 = International Hip Outcome Tool. LEFS = Lower Extremity Functional Scale (LEFS).Table 2. Comparison of Improvement from Enrollment to 24 months F/U by Group All median differences depicted as median and interquartile range (IQR); PT = Physical Therapy. mHHS = modified Harris Hip Score. HOS-ADL = Hip Outcome Score-Activity of Daily Living. HOS-SSS = Hip Outcome Score- Sports Sub-scale. NAHS = Non-arthritic Hip Scale. iHOT-33 = International Hip Outcome Tool. LEFS = Lower Extremity Functional Scale (LEFS).44 Paul F. Abraham, BS, Orthopedics Endoscopic Repair of Full and Partial Thickness Gluteus Medius and Minimus Tears--Prospective Study with Nazal, Gibbs, S. Gillinov and S.D. Martin General Hospital, Boston, MA, USA Introduction: Abductor tendon tears, or tears of the gluteus medius minimus, are a common etiology of greater trochan- teric pain syndrome (GTPS), especially in females between the fourth and sixth decade. Abductor tendon tears present with lateral hip pain, tenderness to palpation of the greater trochanter, weakened hip abduction, and a positive Trendelenburg sign. Excellent outcomes have been demonstrated using open repair with an increasing interest in endoscopic management. Regard - less of technique, there is a paucity of literature on abductor tendon repairs. The purpose of this study is to add to the growing body of evidence by prospectively evaluating the short-term outcomes of endoscopic repair of full abductor tendon tears.Methods: All patients who underwent endoscopic abductor tendon repair by a single surgeon between December 2013 and August 2017 were prospectively evaluated. Inclusion criteria were primary full-thickness gluteal tendon tears, endoscopic gluteal tendon repair, age greater than 18 years, and completion of a minimum of 1-year follow-up. Exclusion criteria were re-tear of gluteal tendons, less 1-year of follow-up, and incomplete patient-reported outcome measures (PROMs). Clinical outcome data consisted of VAS pain score, hip abduction strength, Trendelenburg gait, complications, and patient reported outcome measures (PROMs): modified Harris Hip Score, Hip Outcome Score, Non-arthritic Hip Scale, International Hip Outcome Tool, and Lower Extremity Functional Scale. Preoperative measurements were compared with follow-up at 12 and 24 months. Results: A total of 15 hips (13 patients) met inclusion criteria, with no patients being excluded. The cohort consisted of 12 (80%) females and 3 males (20%), with mean patient age was 66.9 years (SD of 9). (Table 1) All repairs were performed using a transosseous equivalent double-row technique. The minimum follow-up was 12 months, with an average follow-up of 20 months (SD of 5.86). On physical exam, there was a significant increase in hip abduction strength, with 8 (53.3%) hips improving by at least 1 point (p= .02056). Furthermore, all 15 hips had a Trendelenburg sign preoperatively, which had resolved in 14 hips (93.3%) at 1-year (p<0.001) and in all hips at 2-years (p= .0019). The mean VAS score decreased signifi- cantly from a mean preoperative score of 5.36 (SD of 1.80) to 1.57 (SD of 1.76) (p= .0260). The mean difference for all six PROMs was statistically significant, even after Bonferroni adjustment, with an average improvement of 19 points. (Table 2) There were no complications. All 13 patients (100%) reported patient satisfaction with their treatment(s). Conclusion: In this prospective study of 15 hips with full thickness gluteal tendon tears managed endoscopically, we found excellent outcomes at a minimum of one-year follow-up. Results showed significant improvement in 6 PROM scores, hip abduction strength, resolution of Trendelenburg sign and gait, and VAS pain scores, with no complications (including re-tears). Relative to open repair, endoscopic repair allows for less violation of the surrounding tissue and musculature, more extensive lysis of adhesions, less postoperative pain, and fewer complications. This study has similar favorable results as other prospective endoscopic repair studies. There are numerous strengths of this study: 100% patient follow-up, prospectively collected PROMs, use of six PROM tools, quantification of the size of the tear, retraction, and fatty infiltration. Reporting on the presence and resolution of a Trendelenburg sign, onset of the gluteal tears as spontaneous or traumatic, and patient comorbidities. Finally, intraarticular procedures were not performed, allowing for measurement of hip improvement based on the gluteal tendon repair alone. Table 1. Patient Demographics Continuous variables depicted as Mean (SD). Categorical variables depicted as n (%). Abbreviations: BMI=Body Mass IndexTable 2. Patient Reported Outcome Measures (PROMs) Over Time All PROM values depicted as Mean (SD); modified Harris Hip Score. HOS-ADL = Hip Outcome Score-Activity of Daily Living. HOS-SSS = Hip Outcome Score-Sports Sub-scale. NAHS = Non-arthritic Hip Scale. iHOT-33 = International Hip Outcome Tool. LEFS = Lower Extremity Functional Scale (LEFS).55 Meagan Adamsick, PharmD, Pharmacy Impact of Infectious Diseases Pharmacists as Part of an Interdisciplinary OPAT Team Managing Vancomycin M. Adamsick1, R. Gandhi1, S. Bidell1, S. Nelson2, K. Ard2 and R. Elshaboury1 1Pharmacy, Massachusetts General Hospital, Boston, MA, USA and 2Division of Infectious Diseases, Massachusetts General Hospital, Boston, MA, USA Introduction: Outpatient Parenteral Antimicrobial Therapy (OPAT) is a growing area of Infectious Diseases (ID) that allows for the treatment of severe infections in the ambulatory setting. Massachusetts General Hospital (MGH) incorporated inpatient ID pharmacists into the OPAT team in June 2017 to assist with vancomycin monitoring and dosing. Laboratory results were received and documented by the OPAT nurse and forwarded to the pharmacists for assessment via the electronic medical record (EMR). Pharmacists then sent clinical recommendations to the physician. This study aimed to determine the impact of pharmacists' involvement in OPAT vancomycin management.Methods: An EMR-generated report identified patients in the OPAT program from June 2016 through May 2017 as the control group and June 2017 through May 2018 as the intervention group. One hundred patients were randomly selected during each period. Patients were excluded from the intervention group if no pharmacist documentation was present. The primary outcome was to evaluate the proportion of vancomycin levels within the patient-specific goal range and secondary outcomes included the proportion of 1) pharmacists' recommendations accepted by the ID physician and 2) patients who experienced adverse drug events. Results: A total of 200 patients were evaluated. The most common indication for enrollment was osteomyelitis (46%). No differences in baseline characteristics were noted, and the median age was 67 years. The percentage of vancomycin levels within goal was significantly higher in the pharmacist-managed group compared to the control group (66.8% vs. 54.2%; p < 0.0001). The number of patients who experienced adverse drug events was similar between the two groups (39% vs. 43%; p = 0.66); however, fewer patients in the pharmacist group experienced acute kidney injury (5% vs. 13%; p = 0.08). Finally, 100% of pharmacist recommendations were accepted by ID physicians.Conclusion: Leveraging inpatient ID pharmacists at MGH in the management of OPAT vancomycin provided improved percentage of vancomycin in therapeutic range and high acceptance rate of interventions. Further evaluation is necessary to assess the inpatient ID pharmacists' workflow for implementation into other OPAT Sepideh Afshar, PhD, Radiology Assessment of Glutamatergic Neurosystem in Fragile X Knock Out Mouse Model for Targeted Therapy S. Afshar1, X. Qu1, G. Yuan1, S. Lule3, A. Brownell1 and M. Mody2 1Radiology, Gordon Center for Medical Imaging, Boston, MA, USA, 2Radiology, Athinoula A. Martinos Center for Biomedical Imaging, Boston, MA, USA and 3Pediatrics, Massachusetts General Hospital, Boston, MA, USA Introduction: Fragile X syndrome (FXS) is a monogenic developmental disorder caused by mutations of the Fragile X Mental Retardation 1 (FMR1) gene. Patients with FXS are characterized by learning disabilities and cognitive impairments including visuospatial processing deficits, speech and language delays and attention problems. Finding an efficient treatment requires a robust biomarker to assess the progression of the disease and its response to intervention. Currently there are no specific medications for FXS; it draws on treatments designed for other disorders that share similar symptoms. The focus of the present study is on PET imaging of FMR1 knockout mice and their performance on the Morris Water Maze (MWM) task, a measure of spatial learning and memory, towards developing a biobehavioral marker specific to FXS. We build on findings from studies of genetic reduction of mGluR5 expression in the FmR1 knockout mouse and pharmacological studies using short-acting mGluR5 inhibitors which confirm the role of overactive mGlu5 receptors in FXS disorder.6Methods: 67 FXS mice and 70 age- and gender-matched healthy control mice underwent PET imaging using our previously developed allosteric modulator compound [18F]FPEB (3-[18F]flouro-5-(2-pyridinylethynyl) benzonitrile) to examine mGluR5 expression in combination with behavioral performance on the MWM task at three time points: when mice were 1 month old or less (T1), between 1-6 months (T2) and between 6-12 months (T3). For PET imaging studies, animals were anesthe - tized with isoflurane/nitrous oxide/oxygen, adjusted into the scanner (Triumph II, Trifoil Imaging, Inc) and administered with [18F]FPEB. Dynamic volumetric imaging data were acquired for 45 min followed by CT imaging to obtain data for attenuation correction and anatomical borderlines. PET images were reconstructed using maximum likelihood iteration method (MLEM) with 30 iterations. The regions of interest were drawn on all coronal and axial levels, visualized in the fused CT-PET images. The data were submitted to statistical testing (t-tests, ANOVA, linear regression analysis) to evaluate the effects of independent variables (age, disease status, gender) on the dependent variable (FPEB binding potential, BP) in multiple brain regions.Results: Mutivariate ANOVA (2 x 2 x 3: disease status, gender, age) of the PET data (BP) revealed main effects for age[MM1] (p <0.0007) and disease status (p<0.009) and a significant age x disease status interaction effect (p<0.05). The effects were most evident in areas including the cortex, hippocampus, thalamus and olfactory bulb. Additionally, the effects appear to be driven by the FXS mice, regardless of gender, observable at a younger age (viz., < 1 month) in males but persisting later (viz., 6-12 months) in the females. Results of the linear regression analysis point to the hippocampus and cortex as playing important roles contributing to differences in mGluR5 activity in the brain between FXS and control mice. In the FXS male mice, the cortex appears to step up its role relative to the hippocampus in the older age groups compared to the control mice. In contrast, in FXS female mice, the cortex appears to give way to the hippocampus and thalamus. Behaviorally[MM2], in both male and female groups, the control and FXS mice did not differ in their performance on the MWM task (latency, swim speed) at baseline. At later ages both male and female FXS mice showed significantly slower MWM latency and swim speeds than the control mice, mediated by multiple brain regions. Conclusion: The hippocampus is a key region in the brain for spatial navigation which we assessed with the MWM task. The PET imaging showed differences between the FXS and control mice in the hippocampus corresponding to differences between them in latency and swim speed on the MWM task. Furthermore, the PET results appear to support differences between male and female mice with FXS with regard to mGluR5 expression in hippocampus, cortex, and thalamus. These findings are in keeping with the vulnerabilities predicted on the basis of gender and disease status and hold exciting potential as targets for pharmacological intervention. 7 Sung M. Ahn, B.A. candidate, Radiology Gaussian Process Regression and 3D Molecular Fingerprints for Biodistribution Prediction: A Bayesian approach for the prediction of biodistribution with 3D molecular structures S.M. Ahn 1, B. Sanchez-Lengeling2, Y. Baek1 and H.S. Choi1 1Gordon Center for Medical Imaging, Mass General Hospital, Canton, MA, USA and 2Chemistry and Chemical Biology, Harvard Graduate School of Arts and Sciences, Cambridge, MA, USA Introduction: The accurate prediction of biodistribution can lead to discovering new potential drugs and improving cancer treatments. Beyond physical modelling of these phenomena, we can use experimental data to build predictive machine learning models. A large component in this effort is maintaining and curating a high-quality dataset. We consider several types of molecular descriptors, from physical properties to 3D structure and use these with a gaussian process framework. Gaussian processes are Bayesian models that besides prediction can also provide uncertainty estimates to prediction. We find that the most important feature towards accurately predicting biodistribution lies with 3D structural information, in the form of molecular 3D fingerprints. Methods: On the data side, we used the Biodistribution database of various dyes we have collected and have maintained. Each dye was equipped with several physical properties, such as LogD and TPSA 2, as well as a SMILES string that contained 7information about the dye's molecular topology. Each dye also came with biodistribution data taken from analyzing pictures of various organs in post-injection biopsies. Using the SMILES, we created 3D molecular fingerprints encoded via E3FP. On the modelling side, we leverage these different data sources to build a predictive model using Gaussian Processes (GPs). GPs are flexible Bayesian models that provide uncertainty estimates on prediction. We chose the best combination of features and access their regression performance via cross-validation.Results: The results, as shown in Table 1, from running GP regression on each organ does not show significant accuracy in the algorithm's predictions. However, compared to the other methods, it was found that GP had a significant advantage when predicting the organs, even on this very raw, unclean dataset. Though the average R 2 value was only about 0.42, it was on average 0.09 better than the next best approach, and consistently had the highest r value while maintaining the smallest Mean Absolute Error (MAE), AE and Root mean squared error (RMSE). As a comparison, the Support Vector Regression approach, which was the worst algorithm among the ones tested, had an average R2 value of 0.01. Conclusion: Much of using machine learning algorithms requires acquiring a clean database, combined with finding the right algorithm. Though both parts must be carefully done and perfectly balanced with each other, we found that even with a preliminary, less-than-clean dataset, we were able to choose the best algorithm for predicting biodistribution. However, GP Regression still did not perform very well objectively. We believe that acquiring a much cleaner and larger dataset and fine-tuning the algorithm will vastly improve the predictive power of GP. With more data, a more complex model could be used. Table 1. A comparison between the different machine learning methods used to predict biodistribution. 8 Christopher Alba, Surgery - Cardiac Perioperative predictors of survival after VA-ECMO cannulation Sundt1 Surgery, Massachusetts General Hospital, Boston, MA, USA and 2Anesthesiology, Critical Care, & Pain Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Venoarterial extracorporeal membrane oxygenation (VA-ECMO) is an accepted form of cardiac and respira- tory support for patients in cardiogenic shock. Despite improving technology and practices, morbidity and mortality remain high. We investigated predictors of survival in patients with cardiogenic shock who were treated with VA-ECMO as a bridge to myocardial recovery after cardiac surgery.8Methods: A retrospective cohort analysis was conducted on 110 adult post-cardiotomy patients who were supported by VA-ECMO between February 2009 and May 2019. Baseline characteristics and post-ECMO outcomes were compared between non-survivors and patients who were weaned from ECMO as well as those who survived to hospital discharge. Logistic regression was performed to identify predictors of hospital discharge. Results: Of the 110 patients, 55 (50%) were weaned from ECMO and 33 (30%) survived to discharge. In an unadjusted analysis, patients with preoperative chronic kidney disease (p=0.03), those who had longer cardiopulmonary bypass (p<0.01) and longer cross clamp times (p<0.01) during surgery were less likely to survive to discharge. Duration of ECMO support after surgery was similar between those who survived to discharge and those who did not (4.9 vs. 5.8 days respectively, p=0.30). After controlling for relevant confounders, clearance of lactic acid to <2mg/dl in under 24 hours after cannulation was the most important predictor of hospital discharge (OR 6.70 [1.63-27.60], p<0.01).Conclusion: History of chronic kidney disease, duration of bypass, and time to normalize lactic acid under 24 hours were significant predictors of short-term survival in patients on VA-ECMO with cardiogenic shock after cardiac surgery. These results may aid in identifying post-cardiotomy patients with low chances of survival who are treated with VA-ECMO and inform treatment decisions to convert from VA-ECMO to more durable bridging modalities such as CentriMag implantation or early listing for heart transplant. Better informed clinical decision making between treatment modalities may ultimately improve outcomes for adult post-cardiotomy cardiogenic shock patients. Multivariable Predictors of Hospital Discharge A multivariable regression model was built to identify predictors of hospital discharge. All variables studied were considered for inclusion in the model and the final model was determined was using the purposeful selection method and clinical relevance of the covariates. OR = odds ratio. BMI = Body Mass Index. CKD = Chronic Kidney Disease. EF = Left ventricular ejection fraction. ACS = Acute Coronary Syndrome. 9 Jehan Alladina, MD, Medicine - Pulmonary Airway Dendritic Cells Distinguish Allergic Asthmatics from Allergic Controls J. Alladina1,2, D. Hamilos2, A. Luster2, B. Medoff1,2 and J. Cho3 1Pulmonary and Critical Care, Massachusetts General Hospital, Boston, MA, USA, 2Massachusetts General Hospital, Center for Immunology and Inflammatory Diseases, Boston, MA, USA and 3Carver College of Medicine, University of Iowa, Iowa City, IA, USA Introduction: Despite systemic sensitization to allergen, only a subset of people with allergies develop asthma. This suggests important tissue-level regulation in the pathogenesis of allergic asthma. Our prior work has shown that while both allergic asthmatics (AA) and allergic controls (AC) develop type 2 inflammation in the airway in response to allergen, AA recruit more antigen-specific CD4+ T cells polarized to a type 2 phenotype. Dendritic cells (DC) play an important role in orches- trating effector immune responses in the tissue, however the role of airway DC in human asthma remains unknown. We hypothesize that airway conventional DC (cDC) are critical in regulating the effector immune response in allergic asthma and may provide a novel target for inducing disease remission. Methods: Segmental allergen challenge (SAC) was performed in AA, AC, and healthy controls (HC). Bronchoalveolar lavage (BAL) was obtained at baseline and 24 hours after diluent and allergen challenge. We characterized airway DC with flow cytometry. We FACS sorted CD11c +DC and a HiSeq2500 platform (Illumina) at a sequencing depth of 30 million reads (AA, n=4; AC, n=4; HC, n=2). Reads were mapped using the STAR aligner. Statistical analysis was performed using R/DESeq2. An interaction term between group and condition was created and a likelihood ratio test identified genes with condition:group interaction (FDR<10%). Pathway analysis was performed using Ingenuity Pathway Analysis (Qiagen).Results: Airway cDC in AA uniquely regulated their gene expression after SAC compared to AC and HC, with downregu-lation of many genes involved in DC tolerance. Pathway analysis identified IL-13 as the most activated upstream transcrip - tional regulator of observed gene expression changes (Figure 1). Flow cytometry analysis of airway cDC subsets showed that AA and AC had similar numbers of airway CD141 +DC at baseline. While airway CD141+DC only increased in AC upon allergen exposure, airway CD141+DC in AA had increased MHCII expression after SAC. AA also had higher airway 9CCL19 and CCL20 levels after SAC, and endobronchial brushings suggested mucosal accumulation of CD141+DC after SAC in AA compared to AC (Figure 2). AA had higher airway levels of Th2 cytokines and total and allergen-specific IgE after SAC compared to AC. Conclusion: Airway cDC in AA downregulate tolerogenic gene expression and the CD141+DC subset increases MHCII expression after SAC, suggesting that airway cDC in AA are less tolerogenic in response to allergen compared to AC. AA also display differences in cDC subset trafficking and an increased effector response in the airway after SAC, compared to AC. CD141+DC may accumulate in the lung mucosa after SAC, suggesting that the lung mucosal microenvironment changes in response to allergen. Airway cDC may promote the development of asthma in allergic individuals and could be a novel target for disease-modifying therapies. Functional studies of airway DC subsets and interrogation of key pathways in DC activation and tolerance may clarify the role of airway DC in regulating the local effector response to allergen. 10 Jeremy S. Altman, B.A., Anesthesia, Critical Care and Pain Medicine A Multicenter, Randomized, Double-blind, Active-controlled Study to Evaluate the Safety and Efficacy of EXPAREL when Administered via Infiltration into the Transversus Abdominis Plane (TAP) Versus Bupivacaine Alone in Subjects Undergoing Elective Cesarean Section J.S. Bao2 1Anesthesiology, Perioperative and Pain Medicine, Brigham & Women's Hospital, Boston, MA, USA and 2Anesthesiology, Critical Care and Pain Management, Massachusetts General Hospital, Boston, MA, USA Introduction: In spite of recent initiatives that provide improved multi-modal analgesia, some women who undergo C-section experience moderate to severe pain postoperatively and require supplemental dosing with opioids. Women undergoing Cesarean deliveries are commonly provided with a multimodal approach for postoperative analgesia that includes intrathecal morphine and frequent NSAIDs/acetaminophen. The use of transversus abdominus plane (TAP) block is a new regional anesthetic technique that has led to better pain control in various surgical models. For women undergoing C-section, however, administration of immediate-acting bupivacaine through TAP block may not improve analgesia, as bupivacaine HCL has a short half life compared to C-section pain that persists 24-72 hours after surgery. Liposomal bupivacaine (Exparel) is a new long-acting formulation of local anesthetic. A retrospective study reported that TAP block with liposomal bupivacaine (LB) reduced opioid consumption and improved analgesia following cesarean deliver (Baker et al). Therefore, we conducted a prospective Phase 4 multicenter, randomized, double-blind, active-controlled study to compare total postsurgical opioid consumption in women after C-section who received a TAP block with LB plus Bupi compared to women who received only Bupi. We hypothesized that subjects receiving LB with Bupi would consume less opioids than subjects receiving Bupi alone. Secondary outcomes include time of first postsurgical opioid rescue medication, pain intensity rating by Visual Analog Scale (VAS), and ratio of opioid-free subjects. Methods: Women (n=26 at MGH; n=186 across all sites) with term pregnancies of 37-42 weeks of gestation undergoing elective cesarean delivery using spinal anesthesia were randomized (1:1) to TAP HCL or bupivacaine HCL alone. Subjects were administered a TAP block within 90 minutes of procedure end, with ultrasound guidance to confirm needle placement and accurate drug administration. During the ensuing 72 hours, research staff evaluated subjects every 6 hours to assess pain levels, vital signs and adverse events. Data was collected on opiate consumption, time of first ambulation, discharge readiness, and questionnaires were administered on subject satisfac-tion, overall benefit of analgesia, and quality of recovery. At the time of wound closure, subjects in both groups were given IV-acetaminophen (1000mg) and IV-ketorolac (15mg). Subjects then received a regimen of oral acetaminophen (650mg) and ibuprofen (600mg) every 6 hours until 72 hours. Subjects were permitted to request oxycodone (5-10mg) for breakthrough pain but were asked to document a pain intensity VAS score prior to opiate consumption. At the time of discharge, subjects were given a diary to record daily VAS scores and analgesic consumption up to 14 days 10postoperatively. Phone calls were made at 14 days postoperatively to record any adverse events, and to document unscheduled phone calls/appointments/hospital admissions related to pain.Results: Of the 26 enrolled subjects at Massachusetts General Hospital, 16 subjects completed the study and were included in statistical analysis (7 dosed with LB plus Bupi; 9 dosed with Bupi alone), and 10 subjects were excluded due to failure to comply with pre-specified protocol requirements (compliance to multimodal analgesia dosage, correct placement of TAP block, or accurate dosing of local anesthetic). Across all sites, 186 subjects were enrolled. Of these, 97 subjects received LB 266mg with 71 subjects meeting analysis criteria, and 89 subjects received bupivacaine alone with 65 subjects meeting analysis criteria. At MGH, subjects receiving LB had a 63% reduction in opiate consumption through 72 hours after C-section compared to subjects receiving bupivacaine alone. Across all sites, there was a 51.6% reduction in opioid utilization in the LB+Bupi group compared to the Bupi group at 72 hours. Through 72 hours after surgery, 73.3% of subjects at MGH who received LB+Bupi group were opioid-spared (taking 10mg of oxycodone or equivalent) compared to 21.0% of subjects who received Bupi alone. Across all sites, 53.5% of the LB+Bupi group were opioid-spared compared to 24.7% of the Bupi group at 72 hours. Across all sites, total postsurgical opioid consumption was reduced for subjects receiving LB+Bupi through 24 hours, 48 hours, 7 days, and 14 days, with results being statistically significant at 24 hours and 14 days. The median time to first opioid rescue was longer in the LB group compared to those receiving only Bupi, but this was not statistically significant. The safety profile for the two groups were similar. Treatment emergent adverse events (TEAE's) included pruritus, nausea, vomiting and headache. There were no serious TEAE's considered to be related to the study drug. Conclusion: TAP block administration using liposomal bupivacaine plus bupivacaine HCL as a part of multimodal analgesia regimen after C-section resulted in reduced opioid consumption and more opioid-spared patients compared to TAP block with bupivacaine alone. TAP block using LB plus Bupi was also associated with reduced pain intensity compared with Bupi alone. Using LB through TAP block administration was found to be a safe method of postoperative pain management, as there were no drug related serious adverse events noted, and the frequency and type of TEAE's were comparable between both groups. At MGH, site-specific factors may have led to more favorable results compared to other study sites. Further analysis of potential variation in procedural techniques, patient-specific traits across sites, and institutional approaches towards opioid administration may elucidate reasons for different outcomes between sites. 11 Selen Amado, BA, Psychiatry Suicide Risk in Bipolar Patients with Comorbid Posttraumatic Stress Disorder S. Amado1, D. Katz1, and N. George1 1Dauten Family Center for Bipolar Treatment Innovation, Massachusetts General Hospital, Boston, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Bipolar disorder (BD) and Posttraumatic Stress Disorder (PTSD) are common comorbidities, which individ - ually confer a heightened suicide risk as compared to the general population. While suicidality has been studied in a variety of BD comorbidities, there has been little research examining the impact of comorbid PTSD on suicidal ideation in those with BD.Methods: Data was collected from the Clinical and Health Outcomes Initiative in Comparative Effectiveness for Bipolar Disorder study (Bipolar CHOICE), a 6-month, parallel group, multi-site, randomized controlled trial (RCT) of individuals with BD. Bipolar CHOICE enrolled 482 individuals over 10 different clinical sites. A multiple regression analysis assessed whether comorbid PTSD was associated with suicide risk when accounting for common correlates of suicidality in this population, such as the presence of comorbid anxiety disorders, substance use disorders, gender, education, and bipolar subtype.Results: Consistent with the hypothesis, comorbid PTSD was a significant predictor of suicidal risk as assessed by Concise Health Risk Tracking Scale (CHRT) score ( = 2.71, p = .038) after accounting for other baseline variables. All participants with comorbid PTSD (N=58) endorsed current suicidal ideation (p=.005) and were more likely to have had a previous suicide attempt (p<.001).Conclusion: The current findings point to the conclusion that when bipolar disorder and PTSD, conditions which themselves carry substantial suicide risk, co-occur, the probability of suicidal behavior rises even further. This study highlights the need for additional research on the intersection of BD and PTSD and treatments for this population.1112 Melis N. Anahtar, MD, PhD, Pathology Whole genome sequencing analysis of E. faecium clinical isolates reveals novel sequence types and high accuracy prediction of antimicrobial resistance M.N. Anahtar1,2, J. Bramante2, Pierce1,3 and D.S. Kwon2,3 1Pathology, Massachusetts General Hospital, Cambridge, MA, USA, 2Ragon Institute of MGH, MIT, and Harvard, MGH, Cambridge, MA, USA and 3Infectious Disease, Massachusetts General Hospital, Boston, MA, USA Introduction: Vancomycin-resistant Enterococcus (VRE) is recognized as a major cause of hospital-acquired infection. The majority of VREs belong to the species E. faecium, yet much of what is known about E. faecium is extrapolated from E. faecalis, a species that is far less antibiotic-resistant. Whole genome sequencing (WGS) provides a powerful tool to learn more about E. faecium transmission patterns and antimicrobial resistance (AMR) mechanisms. Most E. faecium genomic studies published to date include isolates from single intra-hospital transmission events rather than routine sampling. Additionally, the use of WGS to predict E. faecium AMR has not been tested systematically. Here we use WGS to charac - terize nearly 200 E. faecium clinical isolates from MGH to assess their strain diversity and AMR mechanisms. Methods: Clinical isolates were collected from the MGH Microbiology Laboratory from 1/2016-12/2017. Isolates were collected directly into lysis solution with a subset also stored in LB-glycerol. Species identification was performed using the bioM\u00e9rieux VITEK MS instrument and confirmed using kraken and MetaPhLan. Antimicrobial susceptibility testing for routine clinical care was performed using the AST-GP75 card on the bioM\u00e9rieux VITEK 2 instrument. Bacterial DNA was extracted by bead-beating. Libraries were created using the Nextera DNA Library Preparation kit with a modified protocol and sequenced with Illumina NextSeq. Reads were quality filtered using Trimmomatic. Only isolates with greater than 10x average genome coverage were analyzed. Of these 309 sequenced isolates, the median sequencing depth was 25 (IQR 16.9, 52). Multilocus sequence typing (MLST) was determined using SRST2 with an updated allele table retrieved from PubMLST.org. Resistance genes were determined in 193 samples with over 20x genome coverage using SRST2 manually with AliView. Results: Of the 193 E. faecium isolates with adequate WGS coverage, 101 (52%) originated from rectal swabs or stool, likely representing colonization. The remaining isolates originated from urine (12%), blood (11%), abdominal sites (9%), abscesses (4%), and other tissues (12%). MLST analysis demonstrated strikingly high sequence type diversity, with the three most frequent MLST types (ST412, ST18, ST736) comprising fewer than half of samples. This contrasts with previous E. faecium genomic studies performed in hospitals in New York City and Australia in which the top three MLST types were found in over 90% of isolates examined. Surprisingly, the fourth most common MLST type in the current study was novel, consisting of a combination of alleles not previously reported in the central MLST repository of over 1500 MLST profiles. Further examination revealed two novel MLST profiles seen fairly frequently at MGH and four singleton novel MLST profiles. Cultivatable isolates were available for three of these novel MLST profiles and repeat identification by mass spectrometry confirmed E. faecium identity. Next, we examined whether sequence analysis of known AMR genes could provide high accuracy predictions of phenotypic susceptibility. At MGH, E. faecium isolates are routinely tested for susceptibility to ten antibiotics. Vancomycin resistance was predicted by the presence of vanA, found in 95% of resistant isolates, and vanB , found in the remaining 5% of resistant isolates, with an overall sensitivity of 100% and specificity of 92% (Table 1). High-level gentamicin resistance was predicted by the presence of an intact aac(6)-Ie-aph(2 )-Ia gene, with a sensitivity of 90% and specificity of 98.2% (Table 1). Erythromycin resistance was predicted by the presence of either the ermB or ermC genes, with a sensitivity of 97% and specificity of 95%. Tetracycline resistance was predicted by the presence of either the tetL, tetM, or tetS genes, with a sensitivity of 99% and specificity of 88% (Table 1). Ampicillin resistance was mediated by a SNP at amino acid 485 of the prp5 gene, with a sensitivity of 100% and specificity of 87.5% (Table 1). Finally, ciprofloxacin and levofloxacin resistance were associated with classic SNPs in gyrA and parC, resulting in 100% sensitivity and specificity for predicting fluoroquinolone resistance (Table 1). However, it was not possible to distinguish strains that were fully susceptible to fluoroquinolones from strains that had intermediate resistance based on examination of gyrA, gyrB, parB, or parC alone. Additionally, doxycycline, linezolid, and nitrofurantoin resistances were difficult to ascertain based on known AMR genes, but MGH isolates were almost uniformly susceptible to linezolid (no resistant strains were seen) and nitrofurantoin is only applicable to urinary tract isolates of E. faecium, which comprise only about 11% of all E. faecium isolates. Conclusion: By analyzing WGS of clinical E. faecium isolates collected over two years at MGH, we found that the patient population harbors high strain diversity and locally common yet novel sequence types. Additionally, we found that known AMR genes can be used to predict phenotypic resistance with high accuracy for 7 of the 10 routinely tested antibiotics. Further testing will be performed to resolve phenotypic-genotypic discrepancies and determine the true sensitivity and specificity of WGS for prediction of phenotypic resistance by applying these criteria to a validation set of E. faecium samples.12Genotypic-phenotypic correlations * Grouping intermediate with susceptible 13 Miranda Arakelian, BA, Psychiatry Massachusetts General Hospital Postpartum Psychosis Project (MGHP3): A Clinical and Genetic Study General Hospital, Boston, MA, USA and 2Neuroscience, Mass. General Hospital, Boston, MA, USA Introduction: Introduction: Postpartum psychosis (PPP) is a severe and relatively rare disorder, occurring in 1-2 per 1000 women after delivery according to multiple estimates. While previous research has consistently demonstrated a strong link between PPP and bipolar disorder, about half of women who present with PP have no prior psychiatric history, making identification of women at greatest risk particularly challenging and of considerable clinical significance. Studies to identify risk factors for PPP reveal disparate findings, which may reflect differences in methodology and underlying heterogeneity in this population. The aims of the Massachusetts General Hospital Postpartum Psychosis Project (www.mghp3.org) (MGHP3) are 1) to rigorously describe the phenomenology of PPP with respect to timing of onset, symptomology, and comorbidities, and 2) to identify clinical and genomic predictors of this disorder. Methods: Methods: DNA is collected from women ages 18 and older who have experienced a psychotic episode within 6 months of a live birth, stillbirth, or intrauterine fetal demise occurring within the past 10 years. Participants are mailed saliva collection kits and are rigorously phenotyped by phone using a structured questionnaire and the DSM-5 Mini International Neuropsychiatric Interview for Psychotic Disorders (MINI). Clinical information is abstracted from subject interviews. DNA samples are processed for genome-wide analysis and genotype quality control using the OGR-DISCOVER Oragene 600 saliva collection kit from DNA Genentech and the MEGA SNP chip processed at the Massachusetts General Hospital Core Laboratory.Results: Results: Since the start of this initiative in October 2018, participants have been enrolled with increasing recruitment pace, and a system of DNA sample procurement has been established. A hybrid model of recruitment has been successful with respect to enrollment of subjects with histories of PPP and has included novel platforms such as digital partnerships with advocacy groups and parenthood resource centers, provider networks, mental health forums, targeted social media outreach, and online communities. The ultimate goal is establishment of a global PPP consortium which will use consistent methods of phenotyping and genetic testing.Conclusion: Conclusion: Modeled after existing psychiatric genetics initiatives, MGHP3 represents, to our knowledge, the first American initiative in reproductive neuroscience with this level of rigor and specificity aimed at a better understanding of postpartum psychosis. The broad range of MGHP3 recruitment platforms allows accession of a diverse population of participants, permitting delineation of phenotypes in understudied populations using established, validated tools. Through the expansion of participant population and the pairing of genomic and clinical data collection, we will determine the degree to which PPP risk reflects higher polygenic loading for schizophrenia or bipolar disorder versus a novel genetic signature. MGHP3 will examine genetic underpinning for this serious disorder and address critical questions about the clinical presen- tation, attendant morbidity, and potential mortality of PPP which have eluded researchers. 14 Henry C. Ashworth, MPH, Emergency Inconsistencies in assessing health-related social needs by question type H.C. Ashworth1, G. Ciccolo2, C.A. Camargo2 and M. Samuels-Kalow2 1Harvard Medical School, Ventura, CA, USA and 2Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Health-related social needs (HRSN), such as food or housing insecurity, have significant effects on health and health system utilization. There has been increasing emphasis on the importance of screening for HRSN, leading to the incorporation of standardized questionnaires into health systems and accountable care organizations (ACOs). However, the 13best method of screening is not clear. The goal of this study was to compare the ascertainment of HRSN between previous- ly-reported screening questions and direct inquiry regarding whether a patient wanted assistance with a particular area.Methods: This was a planned secondary analysis of data from a prospective cohort study of emergency department (ED) patients visiting an academic center in Boston. Patients were enrolled over a non-consecutive 48-hour sample of week days and 48 hours of weekend in each treatment area of the ED. All patients entering the ED who could communicate in either English or Spanish with capacity to consent were approached. For patients under age 18 years, a parental guardian was recruited. Participants completed a basic demographic survey and an HRSN screen involving a standardized screening questionnaire from the literature. They also were directly asked if they wanted help with any needs, including housing, food, transportation, utilities, and safety.Results: Overall, 614 patients or parents of patients were screened, of whom 483 (79%) were eligible. Of those, 276 (57%) consented and completed the survey. Overall, 103 (37%) of patients screened positive for an HRSN by the screening questions and 87 (32%) screened positive by the direct questions. The overlap in answers for screening modalities varied across each HRSN area. For example, for housing, 64 respondents (23%) screened positive when asked the screening questions versus 40 (15%) when asked for assistance by the direct question. However, for utility challenges, 11 respondents (4%) screened positive when asked the screening questions versus 49 (18%) when asked by direct question; only 6 patients (2%) screened positive by both methods. The complete comparison between both screening modalities can be seen in Table 1. Conclusion: Our data suggest that the format of the question meaningfully affects ascertainment of HRSN and, thereby, challenges the current paradigm of screening question use. The differences in responses could be related to a number of factors, including cultural background and health literacy. Additional research is needed to identify the best strategies for ascertaining HRSN in the ED to enable effective interventions. Table 1: Positive response to HRSN by question type 15 Steven J. Atlas, MD, MPH, Medicine - General Internal Medicine A pragmatic trial of abnormal cancer screening result follow-up among patients followed in primary care practice networks: The mFOCUS Trial S.J. Atlas1, Haas1 1Division of General Internal Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Division of General Internal Medicine and Primary Care, Brigham and Women's Hospital, Boston, MA, USA, 3Department of Biomedical Informatics, Vanderbilt University Medical Center, Nashville, TN, USA and 4The Dartmouth Institute for Health Policy & Clinical Practice, Dartmouth College, Lebanon, NH, USA Introduction: The benefits of cancer screening are only realized through timely follow-up of abnormal breast, cervical, colorectal and lung screening results. Unfortunately, many common abnormal cancer screening test results do not receive timely follow-up and there is substantial variation in follow-up rates. Incomplete follow-up remains an ongoing problem that undermines efforts to promote cancer screening and represents a liability risk for physicians. In the United States, most screening is initiated by primary care providers (PCPs) but requires care transitions with other providers who either perform the test or evaluate the result. The lack of processes to systematically identify abnormal results, notify patients and providers, address barriers, and track completion of the appropriate diagnostic work-up stem from the complex interactions among patients, PCPs, and specialists associated with the specific cancer. The mFOCUS (multilevel FOllowup of Cancer Screening) study seeks to develop, test and disseminate a health information technology (IT)-enabled, multilevel, stepped care intervention grounded in primary care for abnormal breast, cervical, colorectal and lung screens. We hypothesize that a comprehensive intervention to promote follow-up should be based in primary care as PCPs holistically approach each patient's needs and coordinate care among specialists. Methods: mFOCUS is being conducted in three primary care networks (Massachusetts General Hospital, Brigham and Women's Hospital, and Dartmouth Hitchcock Health). A four-arm trial will be performed with randomization at the practice-level. Key mFOCUS components include: system design to promote identification/tracking using an electronic health record (EHR)-integrated population management platform with education to promote culture change around the management of abnormal screens; use of individual patient and PCP reminders and tools; and a stepped-care team-level enhancement with increasing intensity of contact (i.e., administrative support and patient navigation). Approximately 40 practices will be randomly assigned to system, individual and team components of mFOCUS vs. standard care. Since practices vary by patient 14characteristics, we will use historical data to provide balance between intervention and control groups in terms of patient factors, specific cancer, and sample size. Patients are eligible for the study if they have an abnormal breast, cervical, colorectal or lung screening result and are overdue for follow-up based upon data from the network's electronic records. Since the mFOCUS intervention is designed to function as a \"fail-safe\" system that does not attempt to replace existing reminder efforts, the study will obtain a waiver of written informed consent. The primary outcome will be whether an eligible patient receives follow-up, defined based on the type of screening abnormality and organ type, within 120 days of becoming overdue. Outcomes documenting completion of diagnostic tests or therapeutic treatments will be obtained using electronic data. The study design will permit assessing the marginal effectiveness of system, individual and team-level enhancements, and exploratory analyses will address relevant subgroups. Power calculations require enrolling 3324 patients across the 3 primary care networks.Results: Preliminary data from participating networks show generally poor rates of follow-up. While timely follow-up of high-risk mammogram abnormalities are high (87-94%), rates of follow-up of lower risk abnormalities are poorer (54-83%). Follow-up of positive stool cards for colon cancer screening are 33-48%, and follow-up of adenomas detected on colonos- copy are only 30-50% regardless of the number and size of polyps. Follow-up of cervical cancer screening results ranges between 17-43% for lower risk abnormalities (e.g., ASCUS, HPV negative) to 50-76% for high risk abnormalities (e.g., LSIL, HSIL). Follow-up of abnormal lung cancer screening tests are similar to those described for breast cancer screening. We have worked with specialists to obtain agreement and map appropriate follow-up intervals, diagnostic tests and treatments for each abnormal screening test result. In collaboration with IT experts, we are identifying eligible patients within primary care networks and redesigning health maintenance reminders within the EHR to differentiate routine cancer screening recommen-dations from overdue abnormal results in a way that permits practice-level randomization. Engagement of practice leaders and physicians will occur prior to study initiation early in 2020.Conclusion: mFOCUS will develop, implement and assess whether a health IT-enabled intervention with system, individual and team-based components can increase completion of abnormal and overdue breast, cervical, colorectal and lung cancer screening results. Unique aspects of the trial include a primary care population management perspective, creating a standard system for four cancers and a range of abnormal results, implementing it as a as a \"fail-safe\" system that complements rather than replaces existing reminder efforts, collaborating with specialists, and assessing the relative impact of system, individual and team-based components. Outcome assessment will determine whether it is an effective new standard of care. 16 Chinenye Azoba, B.A., Cancer Center Feasibility of a Supportive Oncology Care Hospital-in-the-Home Intervention for Patients with Pancreatic Cancer Receiving Preoperative Chemotherapy C. Azoba1,2, C. Qian1,2, A. El-Jawahri1 and R. Nipp1 1Cancer Center, Massachusetts General Hospital, Boston, MA, USA, 2Psychiatry, Massachusetts General Hospital, Boston, MA, USA and 3Medically Home Group, Inc., Boston, MA, USA Introduction: Patients receiving preoperative chemotherapy for pancreatic cancer often experience substantial side effects and require frequent hospital admissions for uncontrolled symptoms. Previous studies have demonstrated that symptom monitoring interventions can improve symptom management and quality of life in patients with cancer. Additional studies have investigated the efficacy of hospital-in-the-home interventions to prevent hospitalizations and minimize the use of health care resources. However, such interventions have not been studied among patients with pancreatic cancer receiving preoperative chemotherapy. Patients with pancreatic cancer receiving preoperative chemotherapy have unique supportive care needs, and thus novel supportive care interventions, such as symptom monitoring and/or hospital-in-the-home care models, have great potential to benefit patients and prevent excess health care utilization. Methods: We are conducting a pilot study to assess the feasibility of a hospital-in-the-home intervention for patients with pancreatic cancer, which we call \"Supportive Oncology Care at Home.\" We enroll in-state patients prior to receiving their first cycle of preoperative FOLFIRINOX at Massachusetts General Hospital. Supportive Oncology Care at Home entails: (1) remote patient monitoring (e.g. patient-reported symptoms, home-monitored vital signs and body weight); (2) a \"hospital in the home\" care model for symptom assessment, evaluation, and management (e.g. triggers for phone calls and visits to patients' homes to address concerning issues identified and hydration at home on days 3, 5, and optional day 7 each cycle); and (3) structured communication with the oncology team (including daily phone calls between the hospital-in-the-home clinicians and a gastrointestinal oncologist). We will define the intervention as feasible if we enroll 60% of potentially eligible patients and they complete 60% of daily assessments within the first two weeks of enrollment. Additionally, we are monitoring the number of triggered phone calls, emails, and home visits, the proportion of patients requesting optional day 7 hydration at home, and the length of daily phone calls.Results: From 1/31/19 to 5/3/19, we have enrolled 6 of 7 patients approached (85.7% enrollment rate). One person had to withdraw following consent due to living out-of-state. For the 5 individuals participating, the median age is 65 (range 56-72) 15years and 40.0% are female. Within the first two weeks of enrollment, participants have reported 100% of daily symptoms, 100% of daily vital signs, and 90.0% of weekly body weights. Each participant has triggered an average of 1.41 phone calls, 1.30 emails, and 0.18 home visits each week. All participants have requested the optional day 7 hydration at least once, and most (60.0%) have requested it every chemotherapy cycle. The average length of the daily phone call between hospital- in-the-home clinicians and a gastrointestinal oncologist has been 9.23 minutes (2.57 minutes per patient). Conclusion: We have successfully enrolled patients with pancreatic cancer receiving preoperative chemotherapy in a single-arm pilot study to assess the feasibility of Supportive Oncology Care at Home. Ongoing work will continue to evaluate the feasibility and acceptability of this intervention. This work will inform future studies of Supportive Oncology Care at Home, and we will use study results to develop a randomized trial in the future to determine the efficacy of the intervention in reducing health care utilization and improving patient outcomes. 17 Manisha Bahl, MD, Radiology Digital 2D versus Tomosynthesis Screening Mammography among Women Aged 65 and Older in the United States M. Bahl, N. Pinnamaneni, S. Mercaldo, A. McCarthy and C. Lehman Massachusetts General Hospital, Boston, MA, USA Introduction: Although breast cancer incidence and mortality rates increase with advancing age, there are limited data on the benefits and risks of screening mammography in older women and on the performance of two-dimensional digital mammography (DM) and digital breast tomosynthesis (DBT) in older women. The purpose of this study was to compare performance metrics of DM and DBT among women aged 65 years and older.Methods: For this retrospective study, consecutive screening mammograms in patients aged 65 years and older from March 2008 to February 2011 (DM group) and from January 2013 to December 2015 (DBT group) were reviewed. Cancer detection rate, abnormal interpretation rate, positive predictive values, sensitivity, and specificity were calculated. Multivariable logistic regression models were fit to compare performance metrics in the DM versus DBT groups.Results: The DM group had 15,019 women (mean age \u00b1 standard deviation, 72.7 years \u00b1 6.3), and the DBT group had 20,646 women (mean age, 72.1 years \u00b1 5.9). After adjusting for multiple variables, there was no difference in cancer detection rate between the DM and DBT groups (6.9 vs 8.2 per 1000 examinations; adjusted odds ratio [AOR], 1.13; P = 0.23). Compared with the DM group, the DBT group had a lower abnormal interpretation rate (5.7% vs 5.8%; AOR, 0.88; P < 0.001), value 1 (14.5% vs 11.9%; AOR, 1.26; P = 0.03), and 94.8%; AOR, 1.18; P < 0.001). The DBT group had a higher proportion of invasive cancers relative to in situ cancers (81.1% vs 74.4%; P = 0.06) and fewer node-positive cancers (10.2% vs 16.6%; P = 0.054) than did the DM group. Conclusion: In women aged 65 years and older, integration of digital breast tomosynthesis led to improved performance metrics, with a lower abnormal interpretation rate, higher positive predictive value 1, and higher specificity.16Comparison of screening performance metrics with DM versus DBT DM = digital 2D mammogram. DBT = digital breast tomosynthesis. aOR (95% CI) = adjusted odds ratio with 95% confidence interval. Odds ratio adjusted for age, race, breast density, history of breast cancer, presence of prior screening mammogram, and reader. CDR = cancer detection rate. AIR = abnormal interpretation rate. PPV = positive predictive value. A 72-year-old female with a screening-detected invasive cancer. Tomosynthesis slice from the left craniocaudal view demonstrates architectural distortion in the lateral aspect of the left breast at anterior depth. The finding is not as well seen on the two-dimensional left craniocaudal view. The patient underwent image-guided core biopsy and subsequent lumpectomy, which revealed grade 2 invasive lobular cancer. 18 Masoud Baikpour, MD, Radiology Portal Venous Pulsatility Steatohepatitis M. Massachusetts General Hospital, Boston, MA, USA, 2Center for Ultrasound Research & Translation, Radiology, Massachusetts General Hospital, Boston, MA, USA, 3Department of Radiology, Lahey Hospital & Medical Center, Burlington, MA, USA and 4Department of Radiology, University of Florida, Gainesville, FL, USA Introduction: Adequate management of non-alcoholic fatty liver disease (NAFLD) requires identification of high risk nonalcoholic steatohepatitis (hrNASH), defined as F2 fibrosis according to the NASH Clinical Research Network Scoring System. The purpose of this study was to assess the value of portal vein pulsatility for non-invasive diagnosis of hrNASH. Methods: This IRB-approved HIPAA-compliant study included 123 consecutive subjects (54 males, 69 females, average age 50.3\u00b112.2 years) with a biopsy proven diagnosis of NAFLD who underwent Duplex Doppler ultrasound assessment of their main portal vein within 1 year of liver biopsy (January 2014 to February 2018). Doppler ultrasound images were reviewed. The spectral waveform was used to measure the maximum (V max) and minimum (Vmin) velocity of blood in their portal veins. Venous Pulsatility Index (VPI) defined as (Vmax-Vmin)/Vmax was calculated. Regression analyses were performed to determine the value of this index for diagnosis of hrNASH and compared with that of four clinical decision aids including NAFLD-FS, FIB-4, BARD and APRI. The value of adding VPI to these indices was also investigated. Results: Of 123 study subjects, 33 (26.8%) had hrNASH and were found to have a lower VPI than the other 90 subjects (0.19\u00b10.08 vs. 0.32\u00b10.11; p<0.001). VPI, NAFLD-FS, FIB-4 and APRI significant diagnostic values for hrNASH, with the VPI having the highest optimism-corrected area under the curve value. Addition of VPI to any of the four scoring systems significantly improved their diagnostic values for hrNASH.17Conclusion: VPI may be a useful non-invasive biomarker for hrNASH diagnosis. Logistic regression analysis quantifying diagnostic performance of risk score(s) for predicting hrNASH. hrNASH: high risk non-alcoholic steatohepatitis; to platelet ratio index; Odds ratio estimates are associated with a 1-unit change in either NAFLD-FS, FIB-R, BARD, or APRI and a 0.1 unit change in VPI.Risk score comparisons and estimates of the incremental effect of VPI AUCc: Optimism-corrected area under the receiver operator curve; * A negative value in risk score comparison indicates superior diagnostic performance of VPI versus the given clinical risk prediction model. 19 Audrey C. Bangs, BA, Medicine Missed opportunities for measles-mumps-rubella (MMR) vaccination among US pediatric international travelers 1Medical Practice Evaluation Center, Massachusetts General Hospital, Boston, MA, USA, 2Division of Infectious Diseases, Massachusetts General Hospital, Boston, MA, USA, 3Harvard Medical School, Boston, MA, USA, 4MGH Biostatistics Center, Massachusetts General Hospital, Boston, MA, USA, 5Department of Global Health, Boston University School of Public Health, Boston, MA, USA, 6Division of Viral Diseases, Centers for Diseases Control and Prevention, Atlanta, GA, USA, 7Immunization Services Division, National Center for Immunization and Respiratory Diseases, Centers for Diseases Control and Prevention, Atlanta, GA, USA, 8Division of Pediatric Infectious Diseases, Steven and Alexandra Cohen Children's Medical Center of New York, New Hyde Park, NY, USA, 9Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Hempstead, NY, USA, 10Division of Global Migration and Quarantine, Centers for Diseases Control and Prevention, Atlanta, GA, USA and 11Travelers' Advice and Immunization Center, Massachusetts General Hospital, Boston, MA, USA Introduction: The United States is experiencing a resurgence of measles, with more than 1,000 cases in the first six months of 2019. Imported measles cases among returning international travelers are the source of most US measles outbreaks, and such importations can be reduced with pretravel measles-mumps-rubella (MMR) vaccination. Although children account for less than 10% of US international travelers, pediatric travelers account for almost half of all measles importations. We sought to characterize clinical practice among experienced providers regarding MMR vaccination of pediatric travelers seen for pretravel consultation and describe reasons for nonvaccination of those identified as MMR-eligible.Methods: We performed a prospective, observational study at 29 sites associated with the Global TravEpiNet (GTEN), a Centers for Disease Control and Prevention (CDC)-supported consortium of clinical sites that provide pretravel consulta - tion. Travelers were eligible for inclusion if they were <18 years when they attended a GTEN site from January 1, 2009, through December 31, 2018. We divided pediatric travelers eligible for inclusion into three age groups given age-stratified guidelines for MMR vaccination from the Advisory Committee on Immunization Practices (ACIP) (Table). At the pretravel consultation, providers used a structured questionnaire to confirm details entered by the traveler and/or guardian regarding demographics, medical conditions, travel itinerary, immunization history, and recommended vaccinations. Providers assessed travelers' past MMR vaccination status and administered MMR vaccine according to their clinical practice. We reviewed the data that providers entered in the GTEN structured questionnaire to classify travelers as MMR-eligible if their vaccination history did not meet the age-stratified ACIP recommendations. We grouped reasons for nonvaccination into three categories: provider decision; guardian refusal; or referral to another provider. We obtained odds ratios with 95% confidence intervals from multivariable logistic regression to assess the relationship of vaccination of MMR-eligible pediatric travelers with traveler sex, age group, region, purpose, and duration of travel, time to departure, and type or region of GTEN site.Results: Of 14,602 pretravel consultations for pediatric international travelers, we identified 2,864 travelers (20%) as eligible to receive pretravel MMR vaccination at the time of the consultation (Figure): 365 of 398 (92%) infants (6 to <12 of 3,623 (60%) preschool-aged travelers years), 338 10,581 (3%) school-aged travelers (6 to <18 years). MMR-eligible frequently not vaccinated (1,682 of travelers, and 299 of 338 (88%) school-aged travelers. We observed a diversity of clinical practice at different GTEN sites. In multivariable analysis, MMR-eligible pediatric travelers were less likely to be vaccinated at the pretravel consultation if they were school-aged [OR The most common reasons for nonvaccination were provider decision not to administer MMR vaccination (37%) and guardian refusal (36%).Conclusion: Although most infant and preschool-aged travelers evaluated at GTEN sites were eligible for pretravel MMR vaccination, fewer than half were vaccinated during pretravel consultation, mostly due to provider decision or guardian refusal. Strategies are needed to improve MMR vaccination among pediatric travelers that will reduce measles importations and resultant outbreaks in the United States. Differences between the routine MMR vaccination schedule and the MMR vaccination recommendations for US pediatric international travelers aA total of 3 lifetime MMR doses is recommended for children who receive a dose of MMR before 12 months of age. bThe second dose of MMR vaccination should be given 28 days after the first dose. Abbreviation: MMR: Measles-mumps-rubella Reasons for nonvaccination among MMR-eligible pediatric travelers. All travelers (gray) included all MMR-eligible pediatric travelers, regardless of age. Infants (red) included travelers aged 6 to <12 months, preschool (light blue) included travelers aged 1 to <6 years, and school-aged (navy) were travelers aged 6 to <18 years. Abbreviations: MMR, measles-mumps-rubella; GTEN, Global TravEpiNet a From 2009 through 2012, providers did not collect reasons for guardian refusal of MMR vaccination. b From 2012 through 2018, providers were prompted to ask guardians to specify one of three reasons for MMR refusal. 20 Kai Bao, PhD, Radiology Multimodal PET-NIR imaging probes for pancreatic neuroendocrine tumor imaging K. Bao, G. Park, H. Yuan, E. McDonald and H. Choi Radiology, MGH, Boston, MA, USA Introduction: Pancreatic neuroendocrine tumors (PNET) are notoriously difficult for early detection and treat surgically due to their small size and ectopic locations. Therefore, there are no available screening tools to lower the risk of dying from PNETs. Positron emission tomography (PET) has gained increasing importance for diagnosis of various diseases and thus experiences an increasing dissemination. Beyond the development of agents suitable for PET alone, recent tendencies aim at the synthesis of bimodal imaging probes applicable in PET as well as NIR imaging, as this combination of modali- ties can provide clinical advantages. Previously, we reported a series of hydrophilic, small (<300 Da), positively charged phenoxazine derivatives with high targetability and retention to pancreatic neuroendocrine tumor. The representative molecule, OX61, can highlight the tumor throughout their progression, including hyperplasia, dysplasia, early malignancy, mature tumors, and necrotic stages of cancer. In this study, OX61 was selected as the lead compound and the F18 labeled OX61(OX61-F18) was design and developed as the dual-function NIR/PET probe for pancreatic neuroendocrine tumor imaging.19Methods: The target compound, OX61-F18 was designed by a systematically analysis and comparison with the physico- chemical parameters of OX61. The synthesis of F18-OX61 was finished by a multistep F18 chemistry, including a one-pot reaction of deprotection of the MOM group and the condensation to generate the target compound OX61-F18. NIR fluores- cence and the PET images were taken over 1 h (n = 3, mean \u00b1 s.d.) for signal accumulation at tumor site was observed over 60 min post-injection.Results: OX61-F18 and OX61-F19 showed comparable targeting ability to the PNET as the lead compound OX61. These bioengineered probes permits sensitive detection of ultrasmall (<0.2 mm) ectopic tumors within a few seconds after a single bolus injection, highlighting every tumor in the pancreas from the surrounding healthy tissues with reasonable half-life.Conclusion: In conclusion, NIR fluorescent/PET imaging using Ox61-F18 greatly facilitated the localization of small occult PNET and has the potential to be employed as a novel imaging contrast agent for the early diagnosis and intraoperative PNET Protein/Albumin Post-operative Complications in the Treatment of Periprosthetic Joint Infection? M. Fury 2, A. Barghi1,3, V. Tirumala3, C. Klemt3, Xiong3, W. Boonyanuwat3, and Kwon3 1Medicine, Harvard Medical School, Somerville, MA, USA, 2Orthopaedics, Harvard Combined Orthopaedic Residency Program, Boston, MA, USA and 3Orthopaedics, MGH, Boston, MA, USA Introduction: Periprosthetic Joint Infection (PJI) is a devastating complication after Total Joint Arthroplasty (TJA). The economic and clinical burden of treating this complication is likely to increase as the rate of TJA increases. The C-Reactive Protein/Albumin ratio (CAR) is a marker of inflammation that has been associated with negative outcomes in the general surgery, critical care, oncology, and rheumatology literature, however there is a paucity of data on its utility in the orthopedic literature. This study investigates the potential association of preoperative CAR with postoperative complications, specif-ically reinfection, re-revision, amputation, death, and 30, 60, and 90-day readmissions, in the treatment of periprosthetic joint infection.Methods: A retrospective chart review was performed for all patients who underwent revision total knee or total hip arthro- plasty at a tertiary academic center with the preoperative diagnosis of PJI. Patients who had undergone either single-stage or 2-stage revision with preoperative CRP and Albumin values within 30 days of reimplantation surgery were included in the study cohort. Patients with incomplete data or a history of I&D or prior revision were excluded. In patients undergoing single-stage revision, preoperative CRP and Albumin values were recorded. In patients undergoing 2-stage revision, CRP and Albumin values before re-implantation Postoperative complications including re-revision; 20amputation; death; and 30, 60, and 90-day readmissions were recorded for all patients. The area under receiver operating characteristic (ROC) curves was calculated to evaluate CAR as a predictive value for the above complications.Results: 177 patients with both preoperative CRP and Albumin values were identified including 112 patients undergoing single-stage revision and 65 patients undergoing 2-stage revision. In the single-stage revision cohort, significant differences between the mean CAR were found for 30 and 60-day readmissions (p=0.04 and 0.01) but significance was not reached for 90-day readmissions (p=0.054). Differences in CRP were only significant for 60-day readmission (p = 0.01), while Albumin differences were not significant. For reinfection, there was a significantly higher mean CAR (p = 0.03) only. For re-revision, amputation, and death, there were no significant differences in the mean CAR, CRP, or Albumin values. Receiver operating characteristic (ROC) curves were generated to comparatively assess the diagnostic values of the parameters for each compli - cation. For 30-day readmissions, the area under curve (AUC) values for CAR, CRP, and Albumin were 0.65, 0.67, and 0.50, respectively. For 60-day readmissions the AUC values for CAR, CRP, and 0.69, and 0.51, respectively. For readmissions, AUC values CAR, CRP, and Albumin 0.66, 0.65, and 0.52, respectively. For reinfec - tion, the values for CAR, CRP, and Albumin were 0.68, 0.67, and 0.56, respectively. Threshold points for CAR were 28.7, 20.5, and 20.5 for 30, 60, and 90-day readmissions, respectively. The threshold CAR for reinfection was 37.7. Unlike the single-stage revision cohort, differences in CAR, CRP, or Albumin were not statistically significant for any postoperative complication in the two-stage revision cohort. Conclusion: High preoperative CRP/Albumin ratio was associated with increased risk of re-infection as well as 30, 60, and 90-day readmission in patients undergoing single-stage revision surgery for periprosthetic joint infection, and this ratio may help identify higher risk patients while guiding prognosis and treatment selection in these patients. Further study on the clinical utility of the CRP/Albumin ratio is recommended. 22 Megan E. Barra, PharmD, Pharmacy Changes in Process and Cost Associated with Introducing Andexanet Alfa Therapy for Reversal of Hospital, Medford, MA, USA, 2Department of Neurology, Massachusetts General Hospital, Boston, MA, USA, 3Department of Hematology, Massachusetts General Hospital, Boston, MA, USA, 4Department of Neurosurgery, Massachusetts General Hospital, Boston, MA, USA and 5Department of Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Andexanet alfa (andexanet) is the first antidote approved for treating life-threatening intracranial hemorrhage (ICH) associated with factor-Xa inhibition. Before its 2018 approval, patients frequently received off-label treatment with 4-factor prothrombin complex concentrate (4F-PCC). We evaluated the operational processes and outcomes of patients with rivaroxaban or apixaban-associated ICH treated with 4F-PCC or andexanet.Methods: We performed a retrospective, single-center cohort study of adults receiving 4F-PCC or andexanet to reverse rivaroxaban or apixaban for ICH over 3 years. Fisher's exact, Wilcoxon Rank-sum, and students t-test statistics were calculated, respectively, to compare proportions and nonparametric data. Results: Thirty patients met the inclusion criteria; 11 received 4F-PCC and 19 received andexanet. Median [IQR] initial Glasgow Coma Scale (GCS) was 10 [6-13] in 4F-PCC-treated patients versus 15 [14-15] in those receiving andexanet (p=0.002). Three patients in each group underwent emergency neurosurgery. The median 4F-PCC dose was 26.9 units/kg (2500 units); all andexanet-treated patients received an 880-mg, low-dose regimen. Median door-to-order time was 2.4 hours in both groups. Median order-to-administration time was 0.5 hours [0.1-0.8] vs. 1.1 hours [0.8-1.4] in the 4F-PCC and andexanet group, respectively (p=0.0004). Direct drug costs by average wholesale price were $6,925/patient vs. $29,970/patient in the 4F-PCC and andexanet group, respectively; p<0.0001. Intensive Care Unit and hospital length of stay were similar between groups. In-hospital mortality was higher among patients treated with 4F-PCC versus (63.6% vs. 26.3%; p=0.06). Discharge GCS score was [12-15] the 4F-PCC and therapy is associated with a longer time to treatment, potentially attributable to increased compounding complexity, and a 433% increase in direct drug costs. Future studies should evaluate if these limitations are balanced by differential clinical outcomes.2123 Sara V. Bates, MD, Pediatrics Maternal Infant NeuroDevelopment Study (MINDS) - A Pilot Neuroimaging Study S.V. Bates1, Weiss1, 2OBGYN, 3Radiology, MGH, Boston, MA, USA, 4Radiology, BCH, Boston, MA, USA and 5Center for Addiction Medicine, MGH, Boston, MA, USA Introduction: The rates of opioid use disorder (OUD), particularly among pregnant women, are unprecedented. Fetal opioid exposure commonly leads to neonatal abstinence syndrome (NAS), or neonatal opioid withdrawal syndrome (NOWS), if withdrawal is specifically due to in-utero opioid exposure. In-utero drug exposure(s) has been associated with adverse outcomes in childhood and adolescence (e.g. motor, language, cognitive, and visual impairments and decreased processing speed, self-regulation, and executive functioning skills). Despite the growing epidemic of OUD in pregnant women and potential adverse consequences for offspring, little is known about the potential impact of in-utero exposures (including drugs, nutrition, stress, etc.) on the structure and function of the developing human brain and neurocog-nitive outcomes. The Maternal Infant NeuroDevelopment Study was developed to implement an advanced imaging pipeline for neonates that integrates our patient-centered and trauma-informed care in often marginalized patient cohorts. Our goal is to maintain our practices of reducing stigma, partnering with families, and maximizing the brain health of our smallest patients. Our specific aims are to: 1. collect multi-modal MR imaging in opioid-exposed neonates with NAS (n=30) and age- and sex-matched healthy control infants (n=20) to characterize the prefrontal cortex, and 2. correlate structural and diffusional white matter alterations with neurodevelopmental outcomes assessed with the Bayley Scales of Infant and Toddler Development, 3 rd ed (BSID-III) in opioid-exposed/NAS neonates between 12 and 24 months of age. Methods: With IRB approval, we have started to enroll infants with known in-utero drug exposure(s) as well as normative control subjects. Both cohorts consist of newborns born at 35 weeks gestational age, with no known genetic, neurological, or congenital conditions. Study procedures for enrolled infants include a 45-minute un-sedated MRI (structural, functional, diffusion tensor, and MRS images) within the first 3 weeks of life, and a BSID-III assessment at 1-2 years of age for the \"exposed\" cohort. Other clinical information from the maternal and infant charts is collected and stored in a REDCap database. To answer our research questions, we will analyze quantitative measures from the neuroimaging data in the two groups -- such as structural and functional connectivity/FA, volumetric data, and cortical thickness. In addition, we will explore whether those quantitative measurements can provide insight into long-term developmental outcomes with the ultimate goal of providing more comprehensive, patient-targeted care to infants and their families.Results: Over the past 14 months, we have been building a safe imaging pipeline that includes exposed and normative cohorts. More than 40 people have contributed to the implementation of this pipeline including specialists in: newborn medicine, psychiatry/addiction medicine, neuroimaging, MR operations, social work, and OBGYN. The consent rate has been 75% (8 families approached, 6 consented). All imaging requests have been acknowledged and carried out in a timely manner by the MGH Department of Radiology. To date, six infants have been enrolled in the study and have successfully completed multi-modal MRI imaging. Our goal is to enroll a total of 50 infants over the next 1-2 years. Conclusion: Thus far, the study has proved feasible in its early stages. The protocol we have developed to screen, consent, and image this patient population has required complex coordination across several MGH departments. We anticipate preliminary data will be available in the next 6-12 months. 24 Clare Beatty, Psychology, Psychiatry Self perception in individuals with body dysmorphic disorder and social anxiety disorder C. Beatty2,3, S. Wilhelm1,3, D.S. Manoach1,3,4 and A. Fang1,3 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 3Harvard Medical School, Boston, MA, USA and 4Athinoula A. Martinos Center for Biomedical Imaging, Boston, MA, USA Introduction: Self-focused attention (SFA) is defined as an awareness of self-referent, internally-focused thoughts, feelings, beliefs, and physical states, and is a fundamental cognitive bias in several psychiatric disorders, including body dysmorphic disorder (BDD) and social anxiety disorder (SAD). Research suggests that BDD shares some similarities with SAD1; for example, both BDD and SAD symptoms are triggered by social situations, which activate heightened SFA and subsequently trigger negative self-referent cognitions2,3,4. Cognitive behavioral therapy for BDD and SAD emphasizes the identification of maladaptive patterns of thinking, including distorted self-beliefs5. Cognitive models of BDD and SAD propose that 22effective therapy may lead to decreased negative self-focused thoughts6. Despite this, reliable measures of SFA are largely lacking. The present study uses a self-referential processing task to assess SFA reflected in the endorsement of positive and negative self-views in patients with SAD, BDD, and healthy controls before starting cognitive behavioral therapy (CBT), and after completing 12 sessions of CBT. We hypothesized that clinical patients, who are highly self-focused, would endorse greater negative self-referent cognitions, compared to healthy controls, at pre-treatment. We also hypothesized that clinical patients would endorse fewer negative self-referent cognitions after CBT as compared to pre-treatment.Methods: To date, 32 participants have completed the study: 20 patients (13 SAD, 7 BDD) and 12 healthy controls. Eligibility was determined by scoring greater than +/- 1SD of the Self-Consciousness Scale normative mean, respectively for each group ( 18 for clinical patients and 9 for healthy controls). Patients completed a self-referential processingtask, which required participants to make three types of judgments: SELF (\"Does this adjective describe you?\"), OTHER (\"Does this adjective describe former U.S. President Barack Obama\"), and CASE (\"Is this adjective printed in UPPERCASE letters?\"). SELF trials included 20 positive and 20 negative personality trait adjectives. We analyzed the endorsement of positive and negative self-views before and after treatment. Results: At pre-treatment, clinical patients endorsed significantly fewer positive self-views, t (26.07) = 6.39, p < .001, Cohen' s d = 2.50, and more negative self-views, t(24.28) =-7.07, p< .001, Cohen's d = 2.77, compared to healthy controls. Repeated measured ANOVAs indicated a significant 3-way interaction of group by time by valence, F(1, 17.35, p <.001, 2= .35. Follow-up paired-samples t-tests indicated that clinical patients showed a significant increase in positive self-views, t(20) =-4.725, p < .001, Cohen's d = .67 and decrease in negative self-views from pre to post treatment, t(20) = 3.417, p =.003, Cohen's d =.57. Healthy controls did not show significant changes in positive or negative self-views from pre to post treatment. Conclusion: Our findings illustrate a difference in the endorsement of self-referent positive and negative adjectives between healthy controls and clinical patients, as individuals with maladaptive self-focus endorsed more negative self-views and less positive self-views compared to healthy controls. These results suggest that maladaptive self-focus may be an important therapeutic target in clinical patients with BDD and SAD. One limitation of the study was the use of a binary yes/no response format. Future studies could include participant-generated stimuli to examine whether personal salience has an effect on self-views, compared to experimenter selected stimuli and employ a continuous scale to obtain more fine-grain responses to negative/positive adjectives. Further research is also needed to determine whether CBT impacts maladaptive self-focus in patients with BDD and SAD by modulating self-views. 25 Margaret V. Becker, Pathology The effect of transport temperature and time on the recovery of antimicrobial resistant Enterobacteriaceae 1Department of Pathology, Massachusetts General Hospital, Boston, MA, USA and 2Department of Medicine, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA Introduction: Antibiotic resistant (AR) Enterobacteriaceae are an increasing problem prompting the need for enhanced surveillance. The gut is the primary reservoir for transmission and the ability to screen for colonization improves our understanding of this problem. The feasibility of screening healthy subjects for gut colonization is hindered by the need to process samples quickly to prevent suboptimal transport times and conditions that could affect organism recovery. This study evaluates the survival of AR Enterobacteriaceae in stool over a range of transport times and conditions. Methods: In part 1, human stool spiked with one of six well characterized Enterobacteriaceae containing extended spectrum beta lactamases (ESBL), carbapenemases (CP), and/or mcr-1-genes at concentrations of 10 1 to 104 CFU/ml were inoculated into modified Cary-Blair and stored for 15 days at 3\u00b0C, 20\u00b0C, and 37\u00b0C. Cultures using ESBL-, CP-, and mcr-1-screening protocols were performed on days 3, 8, and 15. All isolated colonies were identified and underwent antimicrobial susceptibility testing according to laboratory validated protocols. In part 2, stool from healthy, asymptom-atic subjects was screened for ESBL-, CP-, and mcr-1-containing Enterobacteriaceae two weeks prior to and fourteen days after international travel over a one-year period as part of a study evaluating the acquisition of AR organisms in U.S. international travelers. To assess the effect of transport temperature on AR Enterobacteriaceae recovery rates, only samples from subjects from New England were included in the analysis. The recovery rates of AR Enterobacteriaceae were compared over transport times and temperature conditions to determine if these variables influenced organism recovery.Results: In part 1, at least two-thirds of the isolates were recovered at approximately the LOD or approximately one 10-fold concentration above the LOD of each medium at all temperature storage conditions at three days with more than half of them recovered at 8 days (Figure 1). There were small decreases in recovery rates across all storage conditions by day 15 with the most pronounced effect seen at the colder storage conditions (3\u00b0C), (Figure 1). Higher recovery rates were seen from stool 23stored in ambient and warm conditions. In part 2, AR Enterobacteriaceae were isolated in 20.9% (114/545) of stool samples submitted for processing over a one-year period. Mean transport time was 4 days (\u00b1 4) with 76% (416/545) and 90% (488/545) of samples arriving within 4 and 7 days of collection. Transport time ranged from 0 to 34 days. AR Enterobacteriaceae were recovered in 21.9% (91/416) of samples collected within 4 days and 17.8% (23/129) of samples collected after 4 days of processing (Figure 2A). The effect of transport temperature on AR Enterobacteriaceae recovery rates is shown in Figure 2B. AR Enterobacteriaceae were recovered in 20.8% (104/499) of stool samples collected from subjects residing in New England during this one-year time period. Recovery rates were 20.3% (62/306) and 21.8% (42/193) for samples collected during cold and warm temperature months. Conclusion: AR Enterobacteriaceae can successfully be recovered from stool over variable transport times and tempera - ture conditions, providing a more practical time period with which to process samples without significantly compromising detection rates. This could improve our ability to screen subjects for gut colonization with AR organisms. Figure 1. Percentage of isolates recovered at approximately the limit of detection (LOD) of each medium or approximately one 10-fold concentration above the LOD stratified by storage duration (days) and storage conditions (\u00b0C). Figure 2. Antimicrobial resistant (AR) bacteria recovery rates (%) from preserved stool of asymptomatic patients stratified by transport duration (A) and time of year (B). Cold temperature=November through April; Warm temperatures=May through October 26 Christopher L. Bennett, MD, Emergency Gender Differences in Faculty Rank Among Academic Emergency Physicians in the United States C.L. Bennett Emergency Medicine, Harvard Medical School, Cambridge, MA, USA Introduction: The purpose of this study was to complete a comprehensive analysis of gender differences in faculty rank among U.S. emergency physicians that reflected all academic emergency physicians. Methods: We assembled a comprehensive list of academic emergency medicine (EM) physicians with U.S. medical school faculty appointments from Doximity.com linked to detailed information on physician gender, age, years since residency completion, scientific authorship, National Institutes of Health (NIH) research funding, and participation in clinical trials. To estimate gender differences in faculty rank, multivariable logistic regression models were used that adjusted for these factors. Results: Our study included 3,600 academic physicians (28%, or 1,016, female). Female emergency physicians were younger than their male colleagues (mean [\u00b1SD] age was 43.8 [\u00b18.7] years for females and 47.4 [\u00b19.9] years for males [p < 0.001]), had fewer years since residency completion (12.4 years vs. 15.6 years, p < 0.001), had fewer total and first/last author publications (4.7 vs. 8.6 total publications, p < 0.001; 4.3 vs. 7.1 first or last author publications, p < 0.001), and were less likely to be principal investigators on NIH grants (1.2% vs. 2.9%, p = 0.002) or clinical trials (1.8% vs. 4.4%, p < 0.001). In unadjusted analysis, male physicians were more likely than female physicians to hold the rank of associate or full professor versus assistant professor (13.7 percentage point difference, p < 0.001), a relationship that persisted after multivariable adjustment (5.5 percentage point difference, p = 0.001).Conclusion: Female academic EM physicians are less likely to hold the rank of associate or full professor compared to male physicians even after detailed adjustment for other factors that may influence faculty rank.24 27 Alexandra S. Bercow, MD, OB/GYN Clinical Trial Participation and Measures of Aggressive Care at the End of Life in Patients with Ovarian Cancer A.S. Bercow1,2,3, R. Nitecki1,2,3, A. Gockley4,3 and W. Growdon5,3 1Obstetrics & Gynecology, Massachusetts General Hospital, Boston, MA, USA, 2Obstetrics & Gynecology, Brigham and Women's Hospital, Boston, MA, USA, 3Harvard Medical School, Boston, MA, USA, 4Gynecologic Oncology, Brigham and Women's Hospital, Boston, MA, USA and 5Gynecologic Oncology, Massachusetts General Hospital, Boston, MA, USA Introduction: Many patients with advanced ovarian cancer seek investigational therapy as a therapeutic option. In non- gynecologic cancers, clinical trial participation has been associated with aggressive care at the end of life (EOL). Coping with Cancer, a prospective cohort study of patients with cancer found that clinical trial participation was significantly associated with late hospice enrollment, hospital death, and death in an ICU. However, studies have also shown that participation in a clinical trial is associated with increased overall survival independent of other prognostic factors. The objective of this investigation was to examine the association between participation in clinical trials and measures of aggressive care as well as hospice care at the end of life among patients with ovarian cancer.Methods: With institutional approval, we conducted a retrospective review of all women treated for ovarian cancer at our institution from 2010 through 2016. We examined several clinical variables which have been identified by the National Quality Forum as measures of aggressive end of life (EOL) care including chemotherapy in the last 14 days of life, ICU admission in the last 30 days of life, and death in the acute care setting. Data were analyzed with univariable and multivariable parametric and non-parametric testing, and survivals were calculated using the Kaplan-Meier method and cox-proportional hazard models. Results: We identified 175 women treated for ovarian cancer that died of disease, 19% of whom were enrolled in at least 1 clinical trial. Patients who enrolled in clinical trials experienced a higher overall survival and were found to live a median of one year after a trial. Interestingly, patients who enrolled in clinical trials were more likely to have undergone primary debulking surgery (PDS, OR .42, p<0.009). A cox proportional hazard model incorporating PDS and clinical trial enroll - ment found both to be independently associated with improved survival estimates (HR 0.63 and HR 0.53 respectively, both p < 0.002). While clinical trial participants were more likely to start a new line of chemotherapy within 30 days of death (p < 0.03), no association with other metrics of aggressive care at the end of life including administration of chemotherapy in the last 14 days of life, ICU admissions and death in an acute care setting was observed.Conclusion: Women with ovarian cancer who are enrolled in clinical trials appear to present with an increased overall survival. While they also have a higher rate of starting a new line of chemotherapy within 30 days of death, they do not undergo more measures of aggressive care at the end of life when compared with ovarian cancer patients who are not enrolled. Further study to understand drivers of patient selection and quality of life will be crucial to understanding how clinical trial participation interacts with outcome for women with ovarian cancer.25Measures of aggressive EOL care for deceased patients in the cohort Measures abstracted from multiple prior studies on EOL care, including Earle et al, J Clin Oncol, 2003. 28 Yosef Berlyand, MD, Emergency Patient experiences with transfer for community hospital inpatient admission from an academic emergency Harvard Medical School, Boston, MA, USA and 2Adult Inpatient Medicine, Newton Wellesley Hospital, Newton, MA, USA Introduction: Emergency department (ED) crowding continues to be a major challenge facing healthcare providers and administrators in the United States, with important ramifications for patient care quality. As ED volume continues to grow while hospital capacity remains mostly stagnant, the number of patients who have been admitted to an inpatient service but have no inpatient bed available has grown significantly in recent years. One strategy to decrease ED crowding has been to leverage alternative pathways within a healthcare system to care for patients who otherwise may have warranted hospital admission. One such pathway for patients not requiring the subspecialty services limited to a large, academic medical center (AMC) is transfer from the academic hospital's ED to a smaller, affiliated community hospital (CH) for inpatient care. Through a survey-based retrospective cohort study, we attempted to determine whether patients who agreed to transfer and admission to an affiliated CH were satisfied with the transfer process and quality of care provided.Methods: This is a retrospective, qualitative cohort study performed at a 995-bed quaternary care AMC and Level 1 trauma center. The secondary study site is a CH with approximately 330 beds, located 11 miles from the primary study site. Criteria for inclusion included all patients requiring general medicine admission in the study period who presented to the AMC's ED and were subsequently transferred to the affiliated CH for their inpatient care. The decision to admit patients from the ED was made per usual protocol by ED providers. As part of the admitting process, a prompt was built into the electronic medical record (EMR) asking each provider to indicate for every admission whether an admission to the CH was appropriate and whether this was offered to the patient. Exclusion criteria included: ED Observation Unit admission; ED Boarder patients, requiring plasma exchange; history of organ transplant; requiring specialized care not available at the CH; and ophthalmologic or oncologic chief complaints. After retrospective chart review, an IRB-approved recruitment letter was mailed to the primary addresses listed in the EMR for the 92 eligible patients. If no response was received, patients were contacted by telephone.Results: A total of 42/92 (46%) of eligible participants responded to the survey either by mail or phone. Some questions were left blank by study participants, resulting in a total of 39 entirely completed surveys. Eighty-five percent of participants rated their overall experience as either great or good, 92% did not find it hard to make the decision to be transferred rather than stay to be admitted at the AMC, and 95% found the transfer itself to be easy. Of the overall cohort of patients transferred to the CH for inpatient admission during the study period, one death occurred within 30 days, 19 (17%) were readmitted within 30 days, and 3 (3%) required an upgrade in level of care. Average ED LOS was 11.25 hours and average CH LOS was 3.85 days. The most common hospital discharge diagnoses were cellulitis, pneumonia, and congestive heart failure. There were no significant differences in demographics or clinical outcomes among the initial cohort and those who ultimately completed the survey either by mail or phone with the exception of whether or not patients had a PCP listed in the EMR; all of those who ultimately completed the survey had a listed PCP.Conclusion: In conclusion, this study demonstrates that admitting patients directly to an affiliated CH from a large, urban, academic hospital's ED resulted in generally positive patient experiences. Given that academic hospitals often suffer from crowding, leading to long waits for admitted patients and deleterious effects on care, a reduction in patient admissions is one potential tactic to alleviate this longstanding and growing problem. More thoughtful matching of patient-specific needs 26to available resources may also serve to decrease waste and lower overall healthcare cost burden, while simultaneously improving patient experience and providing equivalent care quality. While these results are promising, larger, prospective studies are needed to further assess the feasibility of and patient experience with alternative pathways to hospital admission. Survey results. Percentages are given as a fraction of the total number of responses for each given question. Representative examples of patient survey comments. If applicable, square brackets indicate patient rating for the question. Where possible, sample responses are provided from the entire represented spectrum of survey ratings. 29 Jean Bernhardt, PhD CNP, Medicine - Primary Care Nurse Sensitive Indicators in the Care of Individuals with Opioid Use Disorder J. Bernhardt1,2 1MGH Charlestown Healthcare Center, Massachusetts General Hospital, Boston, MA, USA and 2School of Nursing, MGH Institute of Health Professions, Charlestown, MA, USA Introduction: Opioid use disorder is a chronic disease affecting over 2 million people in the United States (National Institute on Drug Abuse [NIDA], 2018). Individuals with opioid use disorder (OUD) seek care in primary care settings where nurses support the provision of medication assisted treatment. With the growing prevalence of chronic disease, nurses have become important members of the primary care team as the number of hospital outpatient visits increases. The profession of nursing has struggled to quantify its unique contribution to care. While working in teams, nurses assume a role and perform functions that contribute to health outcomes. Understanding the role of the specific contribution of the nurse in team-based outcomes is important to defining nursing quality in primary care settings. Understanding the contribution of the nurse provides the potential to evaluate the quality of nursing care as well as to design interventions and delivery systems to improve outcomes for people with OUD in primary care settings. Nurse sensitive indicators are activities demonstrated to make a difference in health outcomes. They have been shown to be valid and reliable measures of established inpatient care structures, processes, and outcomes; however, they require more application in ambulatory settings. Nurse sensitive indicators provide benchmarks for the quality of nursing care provided. Care coordination and transition management were two ambulatory nurse indicators identified as consistent with the role of the nurse. The Nursing Effectiveness Role Model seeks to explain the relationships between outcomes and changes in individuals' health when they can be linked empirically to a nursing activity. The indepen - dent contribution of nurses to changes in individuals' health status can be expressed as activities and functions of the nurse in relation to a nursing diagnosis and are not part of the medical care plan which requires a physician order. The purpose of this initiative was to identify if the NSIs, care coordination and transition management were present in the nursing documentation of people with OUD cared for by OBOT nurses.Methods: Linking specific nursing care activities to outcomes has the potential to improve nursing care and delivery. To determine if nursing activities impact outcomes in people with OUD, documentation that they were provided must first be present and distinguishable as nurse-sensitive. A retrospective chart review categorizing statements based on the dimensions of care coordination and transition management identified by the AAACN Nurse-Sensitive Indicator Task Force was conducted. Content analysis was used to identify themes from nursing visit documentation of people enrolled in an OBOT program in an urban hospital-based primary care clinic. Descriptive analyses of demographics and frequency determi-nation of statements in each NSI and subcategory were conducted. Results: Of the 100 persons who met the inclusion criteria, 60 were males and 40 were females ranging in age from 22 to 67 years old. There were 368 nursing notes that included statements indicative of nurse activities categorized as either care coordination (n=321) or transition management (n=288). Sixty percent of notes indicated that care coordination and transition management were provided in combination at the visit. There were 1547 statements categorized as care coordination and 835 27categorized as transition management. Care coordination (87.2%) was documented more often than transition management (78.2%). Seven subthemes in care coordination were identified: 1) discussed harm reduction techniques; 2) medication management; 3) managing symptoms; 4) facilitating communication to and between providers; 5) promoting adherence to a therapeutic care plan; 6) self-management goals setting; and 7) general health education. Transition management (78.2%) was present in the form of four subthemes: 1) supporting connections to social supports/community resources; 2) referral facilitation; 3) medication reconciliation; and 4) directing pre- and post-hospitalization needs. Counseling was identified as a subtheme in the statements from notes classified as other. Conclusion: This deductive approach to content analysis demonstrated the presence of nurse sensitive activities. This method provided a rich understanding of the variability in nursing documentation that exists in providing nurse sensitive care. This content analysis confirmed the presence of two NSIs, care coordination and transition management in the activities of the OBOT nurse. Additionally, it confirmed multiple subthemes which supported the determination that care coordination and transition management were present in the practice the OBOT nurse in an ambulatory setting. The framework of using NSI to guide OBOT nursing practice in people with OUD has the potential to contribute to nursing quality and make a difference in health outcomes in primary care. Expanding this initiative to examine the documentation of other OBOT nurses will allow the findings of this initiative to be tested further and advance the understanding of the OBOT nurse's contribution to health outcomes. Frequencies and occurrence of statements/subthemes related to Care Coordination (n=1547) and Transition Instability in the Sagittal Plane R. Bhimani1, B. Lubberts1, J. Massri-Puggin1, Wolf3, G. Waryasz1,2 and C. DiGiovanni1,2 1Orthopedic Surgery, Massachusetts General Hospital, Boston, MA, USA, 2Orthopedic Surgery, Newton-Wellesley Hospital, Boston, MA, USA and 3Orthopedic Surgery, West Valley Medical Center, Caldwell, ID, USA Introduction: Syndesmotic instability is multi-directional, occurring in the coronal, sagittal, and rotational planes. Despite the multitude of studies examining such instability in the coronal plane, other studies have highlighted that syndesmotic instability may instead be more evident in the sagittal plane. The primary aim of this study was to arthroscopically assess the degree of syndesmotic ligamentous injury necessary to precipitate syndesmotic instability in the sagittal plane. Secondarily, we aimed to determine the optimal cut-off measurement of fibular translation in the sagittal plane arthroscopically distinguishes stable from unstable injuries.Methods: Twenty-one above-knee cadaveric specimens underwent arthroscopic evaluation of the syndesmosis, first with all syndesmotic and ankle ligaments intact and subsequently with sequential sectioning of the anterior inferior tibiofibular ligament (AITFL), (IOL), the posterior inferior tibiofibular ligament (PITFL), and deltoid ligaments (DL). In all scenarios, an anterior to posterior (AP) and a posterior to anterior (PA) fibular translation tests were performed under a 100N-applied force. AP and PA sagittal plane translation of the distal fibula relative to the fixed tibial incisura was arthroscopically measured using carefully machined and calibrated ball-tipped probes in 0.1-mm increments (from 0.1 to 6.0 mm, 60 consecutive probes total). Results: Compared with the intact ligamentous state, there was no difference in sagittal fibular translation when only one or two ligaments were transected. After transection of all the syndesmotic ligaments (AITFL, IOL, and PITFL), after partial transection the syndesmotic ligaments (AITFL, IOL) alongside the DL, fibular translation in the sagittal plane signifi-cantly increased as compared with the intact state (p-values ranging from p=0.041 to p<0.001). The optimal cut-off point to distinguish stable from unstable injuries was equal to 2mm of fibular translation for the total sum of AP and PA translation (sensitivity 77.5%; specificity 88.9%).28Conclusion: Syndesmotic instability appears in the sagittal plane after an injury to all three syndesmotic ligaments or after partial syndesmotic injury with a concomitant deltoid ligament injury. The optimal cut-off point to arthroscopically distin-guish stable from unstable injuries was 2mm of total fibular translation. Clinical Relevance This data can help surgeons arthroscopically distinguish between stable syndesmotic injuries and unstable ones that require syndesmotic stabilization. Figure 1: Arthroscopic evaluation of syndesmotic instability at 100N. 31 Alexander Bick, MD PhD, Medicine Inherited Causes and Clinical Consequences of Clonal Hematopoiesis from 100,002 Whole Genomes A. Bick1 and P. Natarajan2 1Medicine, Massachusetts General Hospital, Boston, MA, USA and 2Cardiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Clonal Hematopoiesis of Indeterminate Potential (CHIP) is a clonal expansion of blood cells arising from a leukemogenic somatic mutation in hematopoietic stem cells. CHIP increases with age and has been associated with hemato - logic malignancy and coronary artery disease. Simultaneous somatic and germline whole genome sequence analysis now provides the opportunity to identify root causes of CHIP. Here, we analyze genomes from 100,002 participants of diverse ancestries in the NHLBI TOPMed program to identify inherited variation associated with CHIP and its clinical consequences.Methods: We identified CHIP through somatic variant calling with MuTect2 followed by filtering on known leukemogenic CHIP driver mutations. We analyzed associations between CHIP and clinical phenotypes including blood traits, inflamma - tory markers and stroke. We performed a genome-wide association study to identify common inherited germline variation associated with the development of CHIP. We applied burden tests to rare loss-of-function (LOF) variation in coding genes, and rare regulatory variation in hematopoietic stem cell enhancers.Results: We identified 4,587 individuals with CHIP, 90% of whom had a single driver mutation. >75% of the driver mutations were in one of three CHIP driver genes (DNMT3A, TET2, and ASXL1). CHIP prevalence was strongly associ- ated with age (p<10-300). CHIP associated with blood cell trait RDW (p=1.3 x 10-4), and inflammatory and Lp-PLA2 (p=1.8 x 10-7). We found an increased risk for the first nononcologic incident event conferred 29by CHIP - ischemic stroke (Adjusted HR: 1.1, p=0.047), with larger CHIP clones (VAF>20%) conferring increased risk (HR: 1.2, p=0.008). CHIP driver gene specific phenotypic profiles were also observed. Four genome-wide significant risk loci were associated with CHIP, including one locus at TET2 that was African ancestry specific. Computational analyses of the TET2 locus implicated rs79901204 as the putative causal variant which disrupts a hematopoietic stem cell specific enhancer element. Rare LOF variants in CHEK2, a DNA damage repair gene, and rare regulatory variation in HAPLN1 enhancers, a bone marrow stromal cell protein, were the top associations with CHIP in rare variant analyses (p=2.1 x 10-5 and p=2.0 x 10-5 respectively). Conclusion: Heritable variation altering hematopoietic stem cell function and the fidelity of DNA-damage repair increased the likelihood of developing CHIP. 32 Sarah Bick, MD, Neurosurgery Caudate Stimulation Enhances Learning S. Bick1, S.R. Patel1, S. Cash2 and E. Eskandar1 1Neurosurgery, MGH, and 2Neurology, MGH, Boston, MA, USA Introduction: Neuromodulation is a promising treatment modality for disorders of learning and memory, offering the possibility of precise alteration of disordered neural circuits. Studies to date have failed to identify an optimal target and stimulation paradigm. Research in humans and primates supports an important role for the caudate in learning. Our objective was to determine whether caudate stimulation could modulate learning in humans and to examine the neural circuitry involved in this process.Methods: Six epilepsy patients with depth electrodes implanted for seizure localization participated in our study. We recorded local field potentials from implanted electrodes while subjects participated in an associative learning task requiring them to learn an association between presented images and a button press. Three subjects participated in stimulation sessions during which caudate or putamen stimulation was delivered for half of the presented images during feedback after correct responses. We calculated the learning curve for stimulated and non-stimulated images using a state space model. We additionally calculated beta power during the feedback epochs of the task and examined its relationship to learning.Results: Caudate stimulation during the feedback period following correct responses significantly enhanced learning. Both caudate and dorsolateral prefrontal cortex demonstrated a beta power increase during the feedback period of the learning task that was greater following correct than incorrect trials. In dorsolateral prefrontal cortex, this difference increased with learning and persisted beyond the end of the feedback period. Caudate stimulation was associated with increased dorsolateral prefrontal following feedback.Conclusion: Caudate and dorsolateral prefrontal cortex demonstrate feedback related beta power changes, and in dorsolateral prefrontal cortex these changes are related to learning. Temporally specific caudate stimulation enhances learning and is associated with dorsolateral prefrontal cortex beta power changes. These findings suggest that temporally specific caudate stimulation is a promising neuromodulation strategy to improve learning in disorders of learning and memory.30 Striatal stimulation trajectory passing through anterior striatum or anterior internal capsule. E Schematic of the associative learning task. F-H Individual subject learning curves for the stimulated and non-stimulated images for the 3 subjects who completed stimulated sessions. F,G Bilateral caudate stimulation was performed in 2 subjects and significantly improved learning. H Subject 3 received right putamen and left caudate stimulation. While this subject displayed learning of non-stimulated images in the stimulation session, the subject did not display learning of stimulated images. The combination of right putamen and left caudate stimulation significantly impaired learning in this subject Stimulation related power changes. A Timecourse of average DLPFC z-score beta power for correct stimulated versus correct non-stimulated trials in stimulated sessions, aligned to feedback offset. B Box and whisker summary of DLPFC z-score beta power 250-750ms following feedback offset for correct stimulated versus correct non-stimulated trials in stimulated sessions 33 Patricia P. Bloom, MD, Medicine - Gastroenterology Oral Fecal Microbiota Transplant Capsules Improve Cognition in Patients with Recurrent Hepatic Encephalopathy in a Donor-Specific P.P. Xavier1, E. Hohmann2 and R. Chung1 1Gastroenterology, Massachusetts General Hospital, Boston, MA, USA and 2Infectious Disease, Massachusetts General Hospital, Boston, MA, USA Introduction: The established relationship between intestinal microbial dysbiosis and hepatic encephalopathy (HE) provides rationale for manipulation of the microbiome to treat HE. Two trials have confirmed the safety of fecal microbiota transplant (FMT) in patients with recurrent HE; each administered a single dose of FMT from a single donor. Results from these trials have been mixed. It is known that patients with cirrhosis require higher doses of oral FMT capsules to treat recurrent C. difficile infection (Pringle P et al, CGH, 2019, 17(4):791-793), and may need more than a single dose to improve cognition in HE. Furthermore, the ideal FMT donor for HE remains unknown. In this study, we build on prior work by: (1) assessing the efficacy and safety of increased FMT volume to treat ongoing cognitive deficits in patients with a history of overt HE; and (2) identifying donor characteristics that are associated with positive recipient outcomes after FMT (NCT03420482). Methods: We are performing an open-label 10-patient pilot study of FMT in patients with a history of overt HE and ongoing cognitive dysfunction, despite lactulose and Rifaximin therapy. After extensive screening, five healthy donors each provide stool for FMT to 2 patients. Fifteen oral FMT capsules are administered to patients 5 times over 3 weeks. The primary outcomes are (1) serious adverse events and (2) change in cognitive function, as assessed by the Psychometric HE Score (PHES). Stool is collected at regular intervals for metagenome sequencing.Results: Seven patients of mean age 62.4 years (range 57-72) have completed FMT administration and follow-up to 4 weeks. MELD score did not change from before FMT (13.9\u00b12.8) to one week after (14.3\u00b13.4, p=0.63). Six minor adverse events were reported, including bloating and nausea. One serious adverse event occurred, and was deemed possibly related to FMT: extended-spectrum beta-lactamase E. coli bacteremia. After 3 of 5 FMT treatments, there was a trend toward improved PHES (+2.4 points, p=0.08). One week after the 5 th FMT treatment, there was a trend towards improved PHES (+2.6 points, p=0.06). Four weeks after the final FMT treatment, there was a significant improvement in PHES (+3.9 points, p=0.01). One week after FMT, there was a trend towards lower Stroop test scores (56 sec improved, p<0.10) and again 4 weeks after FMT (22 sec improved, p=0.08). Mean change in PHES score by the 4 donors were +6.0, +3.0, +2.3, and -1.0. There was no significant difference in change in PHES between those with and without prior TIPS (+2.5 vs. +2.6, p=0.95). There were one week after. Microbiome sequencing is pending at the time of submission and will be reported.Conclusion: At this interim analysis, oral FMT capsules given in multiple doses appear to improve cognitive function in cirrhotic patients with persistent HE, with effect varying by donor. 34 Leonardo Boiocchi, MD, Pathology Integrative Genomic Testing Improves Clinical Management of Hematological Neoplasms: A Focus on Structural Variations L. Boiocchi1, P. Dal Cin2, J. Lennerz1, L. Le1, Iafrate1, A. Dubuc2 and V. Nardi1 1Pathology, Massachusetts General hospital, Boston, MA, USA and 2Pathology, Brigham and Women's Hospital, Boston, MA, USA Introduction: New understanding of the genomic landscape of hematologic malignancies has led to revisions in their classification often requiring assessment of driver structural variants. RNAbased Anchored multiplex PCR (AMP) assays can identify gene fusion transcripts without prior knowledge of the expected findings, but their clinical utility has not been comprehensively characterized. We describe an inter-institutional analysis of the clinical utility of a molecular fusion assay on an unselected cohort of 137 cases of myeloid and lymphoid neoplasms and hypereosinophilia.Methods: We developed an AMP-based fusion assay testing 82 genes frequently affected in hematologic disorders. 137 unselected cases suspicious for leukemia or hypereosinophilia were evaluated between 2017-2019 by fusion studies and conventional cytogenetics (karyotype and FISH). We studied 71 acute myeloid leukemias 42 acute lymphoblastic of cases, fusion studies were concordant with cytogenetics, with both detecting a fusion (48/108; 44%) or excluding a structural variant (SV) (60/108; 56%). In 21% (29/137) of cases results were discordant. Each discor-dant SV was classified into a three-tier system based on published evidence indicating clinical utility (Tier 1: strong; Tier 2: potential; Tier 3: unknown). In 5/29 discordant cases, cytogenetics identified a Tier 1 or 2 SV that was not detected by fusion studies; three SVs were associated with enhancer hijacking (IGH rearrangement) and thus undetectable by RNA-based studies. Notably, in the majority of discordant cases (20/29; 69%) fusion studies identified a Tier 1 or 2 SV that was not detected by cytogenetics, with strong ALL (8/42; 19%) clarified cytogenetic results (n=8) or subclonal alterations below the sensitivity of cytogenetics techniques (n=2). Importantly, clinically relevant findings missed by cytoge-netics analyses and uncovered by fusion studies were strongly associated with absence of a cytogenetic driver event (3/55 versus 21/84; P<0.01). Conclusion: This is a comprehensive and unbiased assessment of the clinical utility of AMP-based fusion assays. Fusion studies seem to have a strong potential to improve diagnostic accuracy and influence clinical management identifying action-able alterations. A step-wise approach to fusion studies could maximize clinical utility and balance costs.3235 Meg Bor, Bachelors of Business Administration, Marketing, Medicine All of Us Research Program Data Browser - a resource for MGH and beyond M. Bor All of Us Research Program, Partners Personalized Medicine, MGH, Cambridge, MA, USA Introduction: The All of Us Research Program (AoURP) is a historic effort to gather data from one million or more people living in the United States to accelerate research and improve health. By taking into account individual differences in lifestyle, environment, and biology, researchers will uncover paths toward delivering precision medicine. The mission of the AoURP is to accelerate health research and medical breakthroughs, enabling individualized prevention, treatment, and care for all of us. By enrolling one million or more volunteers, the AoURP will have the scale and scope to enable research for a wide range of diseases, both common and rare, as well as increase our understanding of healthy states. A critical component of the All of Us Research Program is the access to the data and research that is collected. Varying levels of access to the data (depending on individual credentials and clearances) will be granted to doctors, researches, scientists and citizen scientists alike, around the world, via the All of Us Data Browser. This interactive tool will allow researchers to see de-identified participant demographics, health histories, and search for any health condition within the data set. There is a significant opportunity for researchers to utilize this data to inform on-going and future research initiatives right here at Massachusetts General Hospital.Methods: Massachusetts General Hospital, Brigham and Women's Hospital, and Boston Medical Center are collaborating as All of Us New England (AoUNE). AoUNE will enroll 93,000 participants, with a target of 46% participants coming from communities underrepresented in biomedical research (UBR). We use the following methods to build trust, understanding and provide bidirectional benefit to UBR participants: prioritizing the participant voice in roles as co-leaders; soliciting feedback on which program and data sharing elements provide value; training staff continuously to ensure a culturally sensitive workforce; employing a geographic strategy of in-reach and outreach.Results: Entering into year 2, enrollment data for AoUNE shows 75.83% of AoUNE participants identifying in the program's UBR categories, with many participants qualifying in more than one category. 40.17% of participants identify race/ethnicity as American Indian/Alaska Native, Asian, Black or African American, Hispanic or Latino, Native Hawaiian or Other Pacific Islander, Multi-Ancestry, or other; 28.14% identify income as below the Federal Poverty Level; 27.82% identify age at consent as 65 years or older; 11.81% identify sexual orientation as bisexual, gay, lesbian, or \"none of these describe me\"; 10.41% identify education level as less than a high school degree; 1.77% identify geography as living in a rural zip code; and 0.11% identify as intersex, non-binary, transgender, gender identity that is different than sex at birth, or other.Conclusion: A focus on relationship building and ensuring bidirectional benefit in research are important values to advance in research that successfully engages diverse groups, along multiple lines of social identity. The data being collected through the AoURP focuses on individual differences in lifestyle, environment, and biology, helping researchers to speed up breakthroughs in precision medicine. The interactive All of Us Data Browser tool will allow researchers to see de-identified participant demographics, health histories, and search for any health condition within the data set. There is a significant opportunity for researchers to utilize this data to inform on-going and future research initiatives right here at MGH. 3336 Anderson Borovac-Pinheiro, PhD, Emergency Uterine Balloon Tamponade for the Treatment of Postpartum Hemorrhage: a Systematic Review and Meta-Analysis A. Borovac-Pinheiro1,5, A. Conde-Agudelo3, Burke1,6 1Emergency, MGH, Boston, MA, USA, 2Boston Medical Center, Boston, MA, USA, 3Eunice Kennedy Shriver National Institute of Child Health and Human Development/National Institutes of Health/Department of Health and Human Services, Bethesda, Detroit, MI, USA, 4University of Stellenbosch, Capetown, South Africa, 5University of Campinas, Campinas, Brazil and Medical School, Boston, MA, USA Introduction: Postpartum hemorrhage (PPH) remains the most common cause of maternal death worldwide. Uterine balloon tamponade (UBT) has become widely used to treat women with refractory hemorrhage. However, controversy has arisen around effectiveness. This Systematic Review was conducted to assess the success rate and safety of UBT placement in women with PPH. Methods: Literature searches of PubMed, Ovid MEDLINE, Embase, POPLINE, Web of Science, African Index Medicus, LILACS/BIREME, Cochrane Library, and Google Scholar were conducted in August 2018 and on May 2019 by Harvard library services. The search strategy included a combination of PPH and UBT terms and relevant references from included studies were also reviewed. Randomized controlled trials (RCTs), non-randomized studies of interventions, and case series studies that reported on the success rate and safety of UBT placement in women with PPH were included. UBT success was defined as cases where the bleeding was arrested without death and/or additional invasive surgical interventions. UBT failure was defined as cases where hemorrhage persisted, and operative intervention and/or death occurred. Safety was defined as any description of short and/or long-term complications potentially related to the UBT insertion, likewise fever, uterine perfora- tion, infertility, etc. A meta-analysis of RCTs and non-randomized studies of interventions was additionally performed. Results: 3,653 studies met initial screening criteria, of which 470 were potentially eligible. Ninety studies, including 4,665 women, met inclusion criteria, of which six were RCTs, 15 were non-randomized studies of interventions, and 69 were case series studies. Indications to use a UBT device for the treatment of PPH included atonic uterus in 22 (25%) studies, placenta previa eight (9%) studies, and placenta accreta in two (2%) studies. All other 46 (52%) studies reported use of UBT for management of multiple causes of PPH. Among the 89 studies that reported success rate, the sample size was 4,665 and the overall pooled UBT success rate was 85.9%, 95% CI [83.8-87.9]. The pooled UBT success rate for RCTs was 89.1%, whereas the UBT success rates for non-randomized studies of interventions and case series studies were 85.2% and 85.7%, respectively. Moreover, in studies reporting the use of UBT for vaginal births the pooled UBT success rates were 87.0%, 95% CI [84.0-89.8]. The pooled UBT success rate for studies reporting the use of UBT for PPH after cesarean delivery was 81.7%, 95% CI [78.0-85.1]. The pooled UBT success rate was lowest in women with placenta accreta spectrum (66.7%) and retained products of conception (76.8%). Among the 23 studies published as abstracts and the 21 unobtainable articles that provided data on UBT success rates, 36 reported data to perform a sensitivity analysis from included articles, studies published in abstract form, and abstracts of unobtainable articles. All 36 studies were case series studies. The total sample size was 6,425 and the overall pooled UBT success rate was 85.8%, 95% CI [84.0-87.5], which was almost identical to the original analysis. A sensitivity analysis of UBT success rate among case series studies did not show any significant differ-ences between studies classified as having a high risk of bias (86.0%) vs. low risk of bias (85.6%). The meta-analysis of RCTs of uterine balloon tamponade plus standard care vs. standard care alone in PPH due to uterine atony after vaginal delivery showed better performance for the UBT group for blood loss >1000 ml, mean stay in ICU (days), and, mean hemoglobin and hematocrit at discharge (g/dl). For surgical procedure(s) or maternal death, there were no statistical differences. A meta-analysis of non-randomized studies of interventions of uterine balloon tamponade vs. no uterine balloon tamponade in postpartum hemorrhage due to placenta previa during cesarean delivery showed better performance for the UBT group for the necessity of the additional surgical procedure, mean blood loss (ml) and, mean hospital stay (days). Safety: Among the 4,665 women, there were 45 (1%) short-term complications that were either potentially attributed to the use of UBT. Seven studies reported 29 cases of fever or infection (6.5%) after the placement of a UBT device among 445 women. Three studies reported seven cases of endometritis (2.2%) attributed to the use of UBT among 308 women.Conclusion: Uterine Balloon Tamponade presented highly success rate among the studies with few descriptions of short- and long-term complications related to the UBT insertion.3437 Suzanne Brodney, PhD, Medicine - General Internal Medicine Validation of a Measure of Trust in the Surgeon S. Brodney1, F.J. Fowler2, K. Sepucha1, K. Valentine1 MGH, Boston, MA, USA and 2Center for Survey Research, University of Massachusetts, Boston, Boston, MA, USA Introduction: There is potential for shared decision making (SDM) supported by patient decision aids (pDAs) to improve the informed consent process and mitigate malpractice risk. A Cochrane review of more than 100 randomized trials in over 30,000 participants has shown that SDM supported by pDAs leads to better quality decisions; however, the impact of SDM and pDAs on factors that may drive medical-legal risk, including trust and regret, have received little attention. Malpractice suits are often the result of poor communication, mistrust, and unexpected, unfavorable outcomes leading to regret. Improving trust and reducing regret should mitigate malpractice risk. The importance of trust in health care has received recent attention as evidenced by a 2019 series on trust in health care published as JAMA Viewpoints. Having reliable and valid measures of trust is a necessary component of documenting the level and importance of trust in the patient-provider relationship. There are few instruments that measure a patient's trust in the clinician, and a lack of measures that focus on trust in the surgeon, specifically. The aim of this project is to develop and validate a short measure of trust in the surgeon.Methods: A series of focus groups and cognitive interviews were conducted at Massachusetts General Hospital in 2018 to learn how people who had a hip or knee replacement or back surgery in the past 2 years understood a set of questions about what it means to have trust in your surgeon. The survey that was tested was adapted from a published survey from the 1990s that had been used before, but not specifically with people who had a hip or knee replacement or back surgery. Based on the collective feedback, a revised set of questions was compiled. In December 2018-January 2019, a 14-item version of the trust measure was tested using a web-based platform to achieve a national sample of 300 respondents aged 30 and older who had a hip or knee replacement or back surgery in the past 2 years. To further reduce the number of survey items, we evaluated 14-item and 5-item versions and we reduced items based on qualitative input and patterns of correlations and then compared them. To evaluate construct validity, we hypothesized that higher trust scores should be significantly related to higher Shared Decision Making Process (SDMP) scores and lower Decision Regret Scale scores. Both these scales have been previously validated. We correlated participants' scores on the 14-item and 5-item versions of the trust scale with their SDMP and Decision Regret Scale scores. To evaluate for a potential ceiling effect, we additionally dichotomized the trust data into a top achievable score versus all other scores and compared the 14-item and 5-item versions.Results: Of the 300 participants in the web survey, 32% had hip surgery, 33% had knee surgery and 34% back surgery. The mean age was 53 years, 45% female, 80% white, 36% had a high school degree or less, and 19% often/always need help with reading health instructions. We examined the 14-item version by condition (hip, knee and back) but combined the data, which were similar across conditions. To explore the correlation between the items in each version, we looked at the item intercor - relations for the 14- and 5-item measures (14-item: 5-item: 0.58-0.71). The and 14-item trust scores was 0.96 (p<0.01). For construct validity, as hypothesized, the scores of the 14-item and 5-item versions were positively correlated with participants' SDMP (0.42 and 0.41, respectively, both p=0.01), and negatively correlated with their Decision Regret scores (-0.51 and -0.48, respectively, both p=0.01). We dichotomized the data by those receiving a top score versus the others. The 14-item version included 25% with a top score and the 5-item version included 35%, suggesting some ceiling effect with both scales. Given the very high correlation of the two versions and similar construct validity and ceiling effects, the 5-item version was selected. The response options for each question are: completely, mostly, somewhat, a little, not at all. 5-item Trust Measure: How much did you trust your surgeon to provide you with information on all potential medical options? How much did you trust your surgeon to present you with information on the benefits of the potential medical options? How much did you trust your surgeon to provide you with information on the downsides of the potential medical options? How much did you trust your surgeon to consider your personal goals and concerns when making medical decisions? How much did you trust that your surgeon cared about what you wanted? Conclusion: Our project led to a parsimonious and valid measure of trust in the surgeon. The 5-item scale is being used in an ongoing study funded by CRICO. The goal is to survey 600 patients at Partners institutions six months after hip or knee arthroplasty or back surgery for herniated disc or spinal stenosis. Many patients receive pDAs prior to these surgeries, but many do not. This situation creates a \"natural experiment\" to study whether prescription of a pDA prior to these operations increases trust and reduces regret about the surgical decision, which should result in lower malpractice risk.3538 Mackenzie L. Brown, BA, Psychiatry Ecological momentary assessment provides insight into individual symptom changes after cognitive-behavioral therapy for panic Massachusetts General Hospital, Boston, MA, USA, 2Psychology, Suffolk University, Boston, MA, USA, 3Psychology, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA and 4Psychiatry, New York University School of Medicine, New York, NY, USA Introduction: Panic disorder is prevalent cross-nationally, with 1.7% of the population having lifetime prevalence of panic disorder (Jonge et al., 2016). The predominance of panic disorder cross-nationally suggests a need to better understand the symptomatology of patients with panic disorder and how symptoms change over the course of treatment. Most studies examining the efficacy of cognitive-behavioral therapy for panic disorder have two noteworthy limitations. First, they rely on retrospective recall of symptoms over weeks or months. Second, they use aggregate sum scores of panic disorder to assess treatment efficacy. These methods limit our understanding of how cognitive-behavioral therapy has its effect. Specif-ically, retrospective recall is prone to bias and sum scores mask potential differences in how individual symptoms respond to treatment. In this study, we address these limitations by using ecological momentary assessments (EMA) to examine the effects of treatment on individual symptoms of panic disorder.Methods: In this study, patients (n=20) received seven sessions of cognitive-behavioral therapy for panic disorder. Patients completed EMA assessments on their smartphones using the application LifeData for two weeks at pre- and post-treatment. Patients were prompted to complete the EMA questionnaire five times per day. In each questionnaire, patients rated the presence and severity of core panic symptoms in the present moment on a 10-point scale ranging from Not At All to Very Much. We used this questionnaire to calculate scores for five symptoms: cognitions, emotions (i.e., feeling anxious or panicky), arousal-related bodily sensations, escape behavior, and avoidance behavior. We assessed the within-subject mean and standard deviation for each symptom for each individual patient at pre- and post-treatment. Results: We used paired-samples t-tests to compare within-subject means and within-subject standard deviations for each symptom score at pre- vs. post-treatment. All symptom scores exhibited significant reductions in both within-person mean (mean differences 0.55-0.90, all ps < 0.04) and within-person standard deviation (mean differences 0.46-0.71, all ps < 0.01), suggesting that symptoms reduced both in severity and in variability. The largest reductions in both mean severity and variability were seen for the emotion symptom (d = 0.80 and d = 1.01, respectively). Conclusion: Patients exhibited a significant reduction in both the within-person mean and the within-person standard deviation of each symptom, suggesting treatment lowered both the severity and variability of panic disorder symptoms. Notably, information about the within-person variance of individual symptoms is only available with an EMA approach, illustrating its importance in understanding a patient's improvement. Similarly, we found significant differences between symptoms in their response to treatment, illustrating the value of a symptom-level approach to evaluating treatment outcomes. Moreover, EMA findings readily generalize to the naturalistic context in which symptoms occur. EMA measures should be used more widely in clinical research to allow us to develop a better understanding of precisely how treatments have their effect on panic disorder. Future directions for research will be discussed in the context of implications for using EMA and technology to enhance the delivery and efficiency of cognitive-behavioral therapy. 39 Clement BULEON, MD, Anesthesia, Critical Care and Pain Medicine Building a rubric for peer guidance on feedback, debriefing and conflict management: Delphi method in an expert community of practice C. BULEON1,2,4, D. SZYLD3,2,4, R. SIMON1,2,4, M. FEY1,2,4, J. PALAGANAS1,2,4 and J. RUDOLPH1,2,4 1Department of Anesthesia, Critical Care and Pain Medicine, Massachusetts General Hospital, Cambridge, MA, USA, 2Center for Medical Simulation, Boston, MA, USA, 3Emergency Department, Brigham and Women's Hospital, Boston, MA, USA and 4Harvard Medical School, Boston, MA, USA Introduction: Feedback conversations, debriefing, and conflict management in healthcare share some commonalities: sharing one's own perspective and goals, eliciting those of others' and finding a way to collaborate, teach, or learn across differences. Conducting these conversations poorly can slow or degrade patient care and clinical learning. Evidence-based scripts and mnemonics can be used to build these skills, but scarce expert time for feedback, ineffective peer feedback, and lack of precision on how to assess or improve \"bite-sized\" chunks of conversation inhibit rapid skill-building of these crucial 36skills. Breaking conversations down into constituent building blocks can support quick and easy skill-building toward a standard. We propose a multi-use conversational \"molecule\" for feedback, debriefing, or conflict management conversations and a rubric to assess it. Such a rubric can facilitate rigorous peer guidance for in-person or distributed on-line communities of practice. We use the metaphor of a \"molecule\" to indicate that any complex conversation might be made up of numerous molecules for easy observation, assessment or guidance. We drew on existing research to develop a \"molecule\" for conver-sations that benefit from explicit signposting of topics, sharing one's own perspective, learning other people's, and bridging those perspectives to support collaboration or learning. It includes 5 successive components: Preview, Observation, Point of View, Inquiry and Listening. Abbreviated description of the elements presented to the experts in Delphi Study: Preview: Signposts the topic of the conversation. Observation: Describes concrete actions that were seen or statements that were heard. Point of View compares the feedback giver's observation to a given standard and describes the implications. Inquiry: Is an open-ended question and explores the receivers' perspectives. Listening: Hearing, acknowledging, and inviting receivers' to share their perspectives. Methods: This Delphi method study built a consensus behaviorally anchored rating scale (BARS) to assess a core part of feedback, debriefing, and conflict management conversations. The BARS supports teaching, learning and assessing conversa - tional skills based on the 5 elements of a \"conversational molecule\" (CM): Preview, Observation, Point of View, Inquiry and Listen. Conducted in 4 anonymous survey rounds (R1-4) with research ethics approval, the study canvassed 39 experts from 14 countries who had been involved in simulation and taught feedback, debriefing, or conflict management for respectively 11 (5-24) and 7 (3-22) years (median, min-max). They were asked to: describe \"good\" and \"poor\" behaviors for each element of the molecule (R1); rate behaviors clumped into thematically alike descriptors by authors CB & JR according to their importance (R2); select the 6 most important (R3); and classify them according to degree of difficulty to master from beginner to advanced (R4). The Delphi study was designed and conducted by authors who were also co-participants in the study. This \"insider/outsider\" research approach leveraged the authors' insider understanding of teaching, learning, and assessing the conversational molecule while also leveraging the outsider experts' anonymous survey data on behavioral descriptors which were weighted according to their frequencies. Every conversational molecule element is rated with a seven-point scale from \"Extremely Ineffective/Detrimental\"(1) to \"Extremely effective/Outstanding\"(7). Results: In Round 1, investigators CB and JR coded the 871 raw descriptors from experts into 118 like terms to provide the first set of good and poor descriptors for the 5 CM elements. There was variance in raw number of descriptors for good and poor behaviors. Round 2 ranked the descriptors and Round 3 selected 6 descriptors for each CM element. The bottom ranked items were discarded. Agreement on the descriptors to keep was high (87 to 100% per descriptor), agreement on ones to discard lower: (56 to 91% per descriptor). In round 4, descriptors described were categorized from beginner to advanced (higher score) (Figure 1). We used the final descriptors and their ranking order to create the BARS. An example for the element \"Preview\" is present in Figure 2.Conclusion: Potent, caring, and efficient conversations to bridge within-profession, cross-profession, and cross-hierarchy perspectives are crucial to crafting clinical management plans and to learning in clinical contexts. The rubric we built through 4 rounds of Delphi study could lower barriers to mastering key conversation skills. It can be used to spur peers to practice and provide each other feedback on defined \"bite-sized\" chunks of conversation. Figure 1: Final Delphi results with ranking of expertise (higher score = advanced): Figure 2: Example of BARS for the FM element \"Preview\".3740 Neel Butala, MD, MBA, Massachusetts General Physicians Organization (MGPO) Variation in Outpatient Visit Frequency after Acute Myocardial Infarction N. Butala1, A. Oseran1, Wasfy1,2 1Medicine, MGH, Boston, MA, USA, 2MGPO, Boston, MA, USA and 3Partners Healthcare, Somerville, MA, USA Introduction: As alternative payment models become more prevalent, there is greater need to evaluate variation in outpatient physician visits. Acute myocardial infarction (AMI) represents a discrete, prevalent, and costly condition where post- discharge care is important for outcomes. There exists wide variation in timing of first visit after AMI, but little is known about the variation in subsequent outpatient visits. Methods: We identified all patients with a primary discharge diagnosis of AMI between 1/1/2016-12/31/17 with at least 1 outpatient cardiology visit in the year before or after admission at a large academic medical center using clinical and adminis-trative data. The primary outcome was the number of outpatient visits to a cardiologist in the year after AMI admission. Covariates included age deciles, gender, race, whether primary language was English, Charlson comorbidity index, Higashi score, affiliated cardiology practice, number outpatient cardiology visits in the 12 months pre-AMI, whether a patient had an affiliated primary care physician (PCP), and number of PCP visits in 12 months pre-AMI and post-AMI. We used negative binomial regression to evaluate the association of covariates with number of post-AMI outpatient cardiology visits.Results: A total 893 patients had a mean of 2.19 post-AMI cardiology 13.37). Post-AMI cardiology visit with (p=0.018), comorbidity scores (p<0.001), primary affiliated cardiology practice (p<0.001), and post-AMI PCP visits (p=0.005), but was not associated with gender, race, language preference, affiliated PCP, pre-AMI PCP visits, or pre-AMI cardiology visits.Conclusion: There is substantial variation in the number of post-AMI outpatient visits. Although there are no disparities across most demographics, there is significant variation by age, patient complexity, and PCP visit frequency, and among affiliated cardiology practices. Further research into determinants of post-MI visit frequency and associated outcomes is needed to determine whether this variation may represent an opportunity for improved patient care and cost savings. 41 Elizabeth T. Cafiero-Fonseca, SM, Performance Analysis and Improvement/Practice Improvement Integrating a Population Health Navigator in the Emergency Department of a Large Academic Medical Center E.T. Cafiero-Fonseca1, K. Risley1, 1Performance and Improvement, Massachusetts General Hospital, Boston, MA, USA, 2Department of Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 3Department of Social Service, Massachusetts General Hospital, Boston, MA, USA, 4Massachusetts General Physicians Organization, Boston, MA, USA, 5Cardiology Division, Department of Medicine, Massachusetts General Hospital, Boston, MA, USA and 6Massachusetts General Hospital for Children, Boston, MA, USA Introduction: In 2018, Massachusetts restructured its Medicaid delivery system to create accountable care models (ACOs) that cover 850,000 people across the state. ACOs are financially accountable for the cost, quality, and experience of care for their members. The Massachusetts General Physicians Organization (MGPO) participates in the Partners HealthCare Choice ACO. Approximately 31,000 Partners HealthCare Choice members receive primary care from MGPO providers. Our ACO strategy centers on identifying and addressing unmet medical and non-medical needs impacting patients' health. Emergency Medicine offers a unique opportunity to improve care coordination while patients are on-site as well as after they transition out of the Emergency Department (ED) and return to the community. We aimed to integrate a non-clinical navigator in the Massachusetts General Hospital (MGH) ED to support patients in connecting to primary and specialty care, population health programs, and community-based resources. Navigators are \"trained, lay health care workers who guide patients in overcoming barriers to health care access and utilization\"(1). The use of navigators began over 2 decades ago in cancer care (2) and has expanded since. As part of the Partners HealthCare Choice ACO, Brigham and Women's Hospital and North Shore Medical Center also developed navigator programs. Methods: To drive forward our ACO strategy, a multidisciplinary group undertook a gap analysis comparing the ideal state (i.e., consistently identifying and addressing the unmet clinical and social needs of ACO patients who visit the MGH ED) to the current state. The analysis revealed opportunities to better serve patients, particularly those with high acute utilization but low ambulatory engagement or unmet social needs. The strategies identified were: 1. Identification: Ability to identify and rapidly engage with ACO patients in the ED 2. Downstream interventions: Supporting safe dispositions by connecting patients to longitudinal care, programs, and community-based services 3. Upstream interventions for patients who frequently 38visit the ED: collaborating with Primary Care to develop Acute Care Plans To facilitate the implementation of these strate- gies, a navigator role was designed and integrated into the MGH ED. To achieve integration, we undertook several steps: Role design: we engaged a multidisciplinary group of ED physicians, nurses, case managers, social workers, and population health administrators to develop the job responsibilities so that they did not duplicate existing efforts and used patient arrival curve data to identify the optimal schedule. Oversight: to ensure the new role would effectively interface with key disciplines, we created a multidisciplinary team to oversee and guide the navigator, rather than sit within one discipline/department. Patient selection : we conducted chart reviews to determine patient selection criteria. After an initial soft launch, we refined our criteria to better target the navigator's outreach to apppropriate patients. Workflow development : we tested workflows for patient identification, assessment, navigation, and coordination with other disciplines. Patient identification required an enhancement to our electronic health record that Partners HealthCare supported; a new kind of documentation encounter enabled the collection of performance data. Since this is an episodic intervention; most cases should be closed or handed off to a longitudinal team after 1-2 weeks. Performance management : Partners HealthCare provided performance targets for the navigator, which we closely monitor via weekly data reports. Optimization: we undertake continuous quality improvement to ensure steady volume and improve quality of the referral workflows. Daily operations are supplemented by a twice-monthly multidisciplinary team meetings to review specific complex patients with high ED utilization.Results: The navigator regularly exceeds the target number of weekly patient encounters. From January-June 2019, there were 530 initial patient outreach encounters and close to 900 referrals and connections made (Graph 1). Initial results demonstrated opportunity to reach patients who visit the ED during nights/weekends, when the navigator is not present. We leveraged an MGH notification system to identify patients for post-ED follow-up calls.Conclusion: We have integrated a new role into the MGH ED to facilitate the connection of patients to longitudinal care, popula-tion health programs, and community-based resources. We employed several strategies to embed this role into operations, including using data to inform the role design, creating a multidisciplinary team to advise the role and refine the program, and establishing criteria for patient identification. Our initial experience has demonstrated that it is feasible for a non-clinical ED team member to provide a bridge for patients to longer term clinical and social supports. Future work will aim to provide in-person coverage 6 days a week, optimize workflows, and evaluate the impact of the program. References: 1. Wang ML, Gallivan L, Lemon SC, Borg A, Ramirez J, Figueroa B, McGuire A, Rosal MC. Navigating to health: evaluation of a community health center patient navigation program. Prev Med Rep. 2015;2:664-8. 2. Freeman HP, Rodriguez RL. The History and Principles of Patient Navigation. Cancer. 2011; 117:3539-3542. 42 Jocelyn A. Carter, MD, MPH, Medicine - General Internal Medicine Implementing a Community Health Worker Intervention at Hospital Discharge J.A. Carter1, A. Walton1, S. Hassan1, A. Thorndike1 and K. Donelan2,1 1Medicine, Massachusetts General Hospital, Boston, MA, USA and 2Medicine, Mongan Institute Health Policy Center, Boston, MA, MA, USA Introduction: In 2011, ~3.3 million adult 30-day US hospital readmissions generated $41.3 billion in hospital costs. $8.26 billion (15-20%) of this was considered preventable. Numerous studies demonstrate relationships between hospital readmis - sions and social determinants of health (SDoH). Lack of education, socioeconomic status, and lack of social support have all been cited as core contributors to readmission rates. The MGH Department of Medicine (DOM) cares for over 10,000 inpatients per year presenting with diagnoses identified by CMS as most commonly associated with 30-day hospital readmis-sions including CHF, septicemia and pneumonia. Data from an MGH DOM readmissions outcomes study showed that less than 45% of these patients have reading skills expected for basic interpretation of prescription labels and 22% are within 250% of the federal poverty level. The MGH DOM leadership is passionate about reducing disparities, however, despite 39extensive clinical/safety-net programming and best practices, MGH 30- day readmission rates remain sub-optimal at ~17.1% in 2017. The initiative described focuses on a novel and narrowly studied process of implementing community health workers (CHWs) as a clinical and social determinants of health-inclusive innovation and involves extensive collaboration with members of inpatient and outpatient care teams. Despite strong evidence demonstrating the efficacy of hospital-based CHWs in reducing readmissions, this is the first MGH pilot of an intervention of this kind.Methods: Inpatient adults meeting criteria (MGH ACO or Partners Healthcare Management patients, with a PCP, not on hospice care, with at least 1 hospitalization in the prior 3 months or 2 hospitalizations in the prior 12 months) and hospital - ized on one of six MGH internal medicine inpatient study units (4/2017-3/2019) were randomized (block randomization) to usual care with or without the 30-day CHW intervention. Patients were then approached by enrollment coordinators and those interested in participating were consented and enrolled. All participants completed an enrollment questionnaire which functioned as a needs assessment. Questionnaire domains included patient-perceived confidence in taking care of themselves after discharge, understanding of the care plan, health care challenges, and barriers related to social determinants of health (e.g. health care access, housing, economic stability, educational status, and social support). CHWs reviewed patient responses to the enrollment questionnaire prior to meeting intervention patients while they were still hospitalized to establish goals and care plans for the 30-day interval. CHWs also attended daily multi-disciplinary rounds on the six medicine study units which fostered collaboration with and guidance from bedside attending nurses, physician teams, case managers, social workers, physical and occupational therapists and pharmacists. This integration with the inpatient team helped ensure that the appropriate clinical and non-clinical community resources were provided to enrolled patients at discharge. In addition, CHWs placed an enrollment note in the electronic health record (EPIC) for each participant notifying all patient care team members (including primary care providers) of enrollment via staff messages. During the 30-day intervals, CHWs documented all patient communication and encounters in EPIC and engaged in communication with outpatient providers and programs (e.g., primary care providers/clinic staff, the MGH integrated complex management (iCMP) case managers/teams, Partners Health at Home nurses, MGH DOM Stay Connected staff, visiting nursing associations and care teams at skilled nursing and rehabilitation facilities). For the 30-day CHW intervention, CHWs employed a specific brand of health care coaching focused on motivational interviewing, execution of behavioral change plans and psychosocial support for patients. This approach was honed by both the patient enrollment questionnaire as well as valued input from inpatient and outpatient clinical care team members. CHW care itself was delivered by performing home visits, showing up outside a patient home in a hired car to accompany them to a PCP or specialty visits, or even sitting elbow- to- elbow with patients to assist with the completion of forms (e.g. insurance applications). CHWs also connected patients to low or no-cost services and community resources (e.g., transportation, food access, housing or other domains driving gaps in care) while fostering patient connections to care teams. Performance measures were tracked during the 30-day post-discharge period and demonstrated 45% lower readmission rates, 51% lower emergency room visits, and 38% fewer missed post-discharge appointments as compared to controls. Finalized analysis for 547 participants is pending and expected 5/1/2019.Conclusion: The application of a peri-discharge CHW intervention demonstrated clear reduction in 30-day hospital readmis-sions for a vulnerable ACO population at high risk for readmission. Based on these results, this type of intervention should be considered for future collaborations with established safety-net programming to enhance the standard of care/best practice for high-risk populations in our health care system. 43 Douglas Cassidy, MD, Health Professionals Education Research Feasibility and Benefits of a Peer-Led ABSITE Review Course D. Cassidy1, S. McKinley1, I. R. Phitayakorn1 and D. Gee1 1Surgery, Massachusetts General Hospital, Boston, MA, USA and 2Department of Epidemiology and Biostatistics, Memorial Sloan Kettering Cancer Center, New York, NY, USA Introduction: The American Board of Surgery In-Training Examination (ABSITE) is a yearly multiple-choice examination designed to measure the progress attained by general surgery residents in their knowledge of applied science and manage - ment of clinical problems related to surgery. The examination has become a key factor in evaluating resident knowledge and playing an increasing role in fellowship applications. While many program directors agree that there is value in exam review courses, there is a paucity of information on their benefit. This study sought to investigate the effects of utilizing a supplemental ABSITE review course taught by residents on examination performance.40Methods: An institutional needs assessment was performed by analyzing 5-years (2014-2018) of resident score reports. Score reports were deidentified and a database was constructed of all \"missed questions>. An institutional ABSITE review course was constructed using this needs assessment to identify the most commonly missed questions for the program and at each postgraduate level. These report cards were used to guide teaching sessions, which utilized a small group format and the principles of peer teaching. Participation in the sessions were voluntary. Following each session, learners completed surveys about the quality of both the session and the teaching. Following completion of the course and the 2019 ABSITE, a residen-cy-wide survey was conducted of participants and non-participants, assessing demographics, study habits, and perceptions of the ABSITE. Free response text regarding positives and negative qualities of each teaching session was inductively analyzed for prominent themes. Score reports from the 2019 ABSITE were deidentified and coded, and participant performance was analyzed. A multivariate linear regression model was used to determine the association of review session participation with performance on the 2019 exam.Results: The ABSITE review course consisted of 22 sessions led by 13 categorical general surgery residents. There were 205 total attendees across the course, averaging 9.3 residents per session with a range of 4-14 residents. A total of 34 categorical residents (54.8%) attended at least 1 session. Of these 34 residents, 16 (25.8%) attended at least 5 sessions and 7 (11.3%) attended at least 10 sessions. In general, residents viewed the review course favorably. Across all post-session surveys, an overwhelming majority of residents indicated that the sessions were useful for their ABSITE preparation (100%) and relevant to their current clinical responsibilities (97.5%). In all 163 completed surveys, resident learners indicated that they would attend the session again (100%). Out of 163 completed surveys, resident participants generated a total of 116 free-text responses on the positive features of the sessions (71.2% of completed surveys) and 53 free-text responses on the negative features of the sessions (32.5% of completed surveys). Thematic analysis of open-ended questions regarding the sessions demonstrated 9 distinct positive themes and 6 distinct negative themes (Table 1). Following the 2019 ABSITE, a survey was administered to all residents who took the exam with 46 total responses (76.7% response rate). Demographics, study habits, and opinions of the ABSITE were compared between course participants and non-participants. Participants were more likely to be research residents, to start dedicated ABSITE preparation earlier than non-participants, and were more likely to agree that the department valued ABSITE results. A total of 25 (73.5%) residents who attended at least one session responded to the post-ABSITE survey with consistent favorable opinions of the review. The majority indicated that the sessions were useful for the ABSITE preparation (100%) and that the peer teachers were effective in preparation for the exam (100%). Eleven residents (44%) indicated that the course had a substantial impact on their preparation whereas 10 residents (40%) indicated the course had a modest effect. Twenty-two residents (88%) would attend most or all the time if the course was offered again. Commonly cited barriers to participation for non-participants included time (42.8%), preferring other study methods (28.6%), the ABSITE not being a priority (19.0%), and conflicts with clinical duties (9.5%). None of the non-participants indicated that the review course \"would not be useful\" as a barrier. With regard to overall ABSITE performance, there was no significant effect on review session attendance and standard score performance on the 2019 exam. In a multivariate model, the variables predictive of 2019 exam performance were prior ABSITE performance (p=0.039), USMLE2 score (p=0.046), and number of question bank questions completed (p=0.018). Number of sessions attended (p=0.123) was not a significant predictor. Conclusion: While the implementation of an institution-specific ABSITE Review Course did not show a significant improve - ment in ABSITE performance, there was a subjective improvement in the educational experience and the ABSITE prepara-tion of our residents. An institutional-specific review course requires a significant time commitment from residents but is also a low cost to the institution by utilization of peer teachers as opposed to faculty. The principles of this course can be easily adaptable to other institutions and can be tailored to a program based on this needs assessment. Resident positive and negative feedback on ABSITE review sessions 4144 Douglas Cassidy, MD, Health Professionals Education Research Utilization of an ABSITE Review Course in the Development of Resident Teachers D. Cassidy1, S. McKinley1, S. Chakraborty2, and D. Gee1 1Surgery, Massachusetts General Hospital, Boston, MA, USA and 2Department of Epidemiology and Biostatistics, Memorial Sloan Kettering Cancer Center, New York, NY, USA Introduction: Peer and near-peer teaching are educational strategies utilized in graduate medical education with benefits to teachers, learners, and the institution alike. Benefits include alleviating faculty pressures, offering educational content at a learner's cognitive level, and preparing physicians for their future roles as educators. In resident education, peer teaching and near-peer teaching is feasible with high levels of resident participant satisfaction in both simulation and physical examina - tion didactic programs. However, there is limited information on either the benefits or usage of peer-teaching in general surgery residency as well as in resident didactics and curriculum. The American Board of Surgery In-Training Examination (ABSITE) has become a key factor in evaluating resident knowledge. Given the dual need of developing resident teachers and improving performance on the ABSITE, we sought to investigate the effects of utilizing a supplemental ABSITE review course taught by residents on examination performance and resident teacher development.Methods: A 22-session review course for the 2019 ABSITE was offered to residents at our institution. Volunteer resident teachers were recruited to lead sessions and were given report cards for each subtopic providing performance metrics of the residency to guide content. Following each teaching session, learners completed surveys about the quality of the session and the teacher performance, and report cards were provided to teachers with a performance rating and comments based on learner feedback. Following completion of the course and the 2019 ABSITE, resident teachers were surveyed about their experience leading peer teaching sessions. Teacher free response transcripts regarding the benefits of being a peer teacher as well as learner responses to the positive and negative attributes of each teacher were inductively analyzed for prominent themes. Responses to each open-ended question were openly coded by a single author without a predefined framework. These codes were then organized into broader themes, with a definition of each theme generated based on the associated codes. This theme book was iteratively refined with input from a multidisciplinary team. Finally, score reports from the 2019 ABSITE were deidentified and coded, and teacher examination performance was also analyzed. Teacher performance was compared across years for both overall and subject-specific teaching. Results: The ABSITE review course consisted of 22 review sessions led by 13 categorical general surgery residents. Residents taught between 1 and 5 sessions with 6 residents teaching multiple sessions. Resident teachers were highly rated following each teaching sessions, with an overwhelming majority of residents indicating that teachers had excellent knowledge of the subject matter (88.9%), demonstrated enthusiasm and encouraged participation (91.4%), and created a comfortable teaching environment (93.2%). The quality of the sessions was given a high rating (3 or 4 on 4-point Likert scale) 99.4% of the time. Out of 163 completed surveys, resident participants generated a total of 142 free-text responses on the positive attributes of the teachers (87.1% of completed surveys) and 29 free-text responses on the negative attributes of the teacher performance (17.8% of completed surveys). The absence (negative) and presence (positive) of the following themes were described: engaging and encouraging participation, utilization of practice questions to engage learners, teacher organization and presentation of material, and teacher preparation and knowledge. Eleven of thirteen resident teachers (84.6%) completed surveys after the ABSITE about their teaching experience. The majority of teachers believed that leading teaching sessions was useful to their general ABSITE preparation (81.8%) and their subject-specific ABSITE preparation (100%). With regard to their development as a teachers, residents believed that leading these sessions contributed to their development (90.9%) and that the feedback they received was useful (81.8%). All 11 responders indicated that they would participate as a teacher again. Four main themes were identified by resident teachers when asked about the benefits of participation in these sessions and how the sessions impact their future practice (Table 1). First, residents reported improved surgical knowledge and benefits in ABSITE preparation, most notably by reviewing teaching topics more in-depth to promote mastery for teaching. Second, teachers identified benefits to the utilization of a peer teaching model, such as fostering a group discussion, having a greater responsibility as a peer teacher, and providing content at a similar education level to their peers. Third, teachers referenced improvement to their teaching practice and ability, such as become more comfortable with teaching and outlining and designing a lecture or session. Fourth, teachers noted future changes that they will make to their teaching methods and the quality of peer feedback provided to them. Resident teachers did not demonstrate a significant improvement in their subtopic-specific ABSITE performance from 2018 to 2019 (73.1% vs. 82.3%, p=0.163). Conclusion: Utilization of peer teachers for resident didactics is feasible and provides residents with opportunities to improve their teaching performance and receive quality peer feedback. Structured teaching opportunities provide both personal benefits to resident teachers, specifically through knowledge gains and improved perceived teaching ability, and benefits to the program as a cost-effective means of instruction.42Themes and representative quotes from residents who participate as a peer teacher 45 Janine Cerutti, Psychiatry The association between socioeconomic position and DNA methylation: A systematic review J. Cerutti1, Medicine, Massachusetts General Hospital, Cambridge, MA, USA, 2Department of Epidemiology, Harvard T.H. Chan School of Public Health, Boston, MA, USA, 3Department of Psychiatry, Harvard Medical School, Boston, MA, USA and 4Stanley Center for Psychiatric Research, Broad Institute of MIT and Harvard, Boston, MA, USA Introduction: Decades of observational studies have shown that socioeconomic position (SEP) is associated with a host of behavioral and health outcomes in childhood and adulthood, spanning cognitive development to academic achievement, as well as physical and mental health risks. Increasingly, evidence from experimental studies suggests that SEP may in fact play a causal role in driving these outcomes, especially when low SEP is experienced early in development or chronically throughout childhood. Although both observational and experimental studies have established SEP as a strong determinant of health and well-being, the biological mechanisms explaining this relationship, or how SEP \"gets under the skin,\" are not yet well understood. One widely pursued hypothesis is that the socioeconomic environment programs the epigenome through DNA methylation (DNAm) marks, which act as a kind of biological memory that does not alter the sequence of the genome, but instead influences the functioning of genes and therefore risk for future health problems and disease. Numerous studies have examined the impact of SEP on DNAm. However, these studies have not yet been summarized in a comprehensive systematic review, which is needed for the field to better understand SEP's potential biological consequences and how best to analyze SEP-related measures in epigenetic studies. Thus, we performed a systematic review of the relationship between SEP--inclusive of diverse measures taken across the lifecourse--and DNAm. Methods: We systematically searched for articles published from inception through February 15, 2019 on PubMed and PsycINFO. To capture diverse measures of SEP, we included search terms representing all of the following socioeconomic domains: income, financial difficulties, education, occupation/employment, and neighborhood characteristics. The search returned 428 total articles across these two databases. From these results, we reviewed abstracts and applied the following eligibility criteria to identify studies for inclusion in our review: 1) included at least one measure of an SEP-related construct as an exposure (or independent variable), and 2) included DNAm as an outcome (or dependent variable). There were 39 articles that met the eligibility criteria. Full-texts and reference lists of eligible articles were reviewed in-depth for possible inclusion. Results: In total, we included 31 articles in our review that met our inclusion criteria (see Figure 1 for detailed search and selection strategy). The mean sample size of these studies was 807 (40 - 10,767). These studies covered populations from the United States, Canada, Europe, Israel, the Dominican Republic, and the Philippines. With respect to the research design, most studies were cross-sectional (n=16; 51%) or prospective (n=12; 39%), with (n=3; <1%) being longitu - dinal, meaning with repeated measures assessed over time. Most of these studies were also based on candidate genes (n=15; 48%), with 9 being epigenome-wide association studies (29%), and 8 being global DNA (26%) analyses. In total, roughly 94% of studies found at least one significant association between SEP and DNAm. However, the heterogeneous nature of the studies made it difficult to fully compare results across studies. In particular, the articles varied greatly by approaches to assessing SEP and DNAm analyses. For example, 22 (71%) studies used an individual measure of SEP (e.g., occupation) or assessed multiple measures independently, while 9 (29%) utilized a broad, composite measure of SEP. Future analyses will assess the general overlap in significant DNAm profiles between the 31 studies and will annotate the gene ontology of any consistent findings to infer potential gene-related risks.43Conclusion: The existing literature on the link between SEP and DNAm varies greatly both conceptually and method- ologically, which made it challenging to compare across studies and assess the strength of findings. However, at least two important trends did emerge from the current review to date. First, the four studies that assessed both child and adult SEP reported a stronger association between child SEP and DNAm in adulthood compared to adult SEP, suggesting a potential sensitive period during the lifecourse when SEP may be more impactful on the epigenome. Second, although all epigenome-wide analyses reported at least one significant result, the two studies that analyzed more than one measure of SEP independently (vs. one broad, composite measure) found little to no overlap in DNAm profiles between measures. This suggests that the construct of SEP is not singular, but rather that sub-domains within the construct of SEP (e.g., education vs. weekly income) may influence DNAm differently. Practically, results from this review highlight the need to identify best-practices to define this multifactorial construct and measure it. Findings from this review also emphasize the need for future research to independently analyze different indicators of SEP (when possible) to assess if and how different measures of SEP vary with regards to their biological consequences. Figure 1. Search and study selection process. 46 Reena Chabria, B.A., Cancer Center Effects of A Cognitive-Behavioral Therapy Mobile Application on Anxiety and Depression Symptoms in Patients with Advanced Cancer: The Moderating Role of Time Since Cancer Diagnosis R. Chabria, J.M. Jacobs, J.S. Temel and J.A. Greer Cancer Center, Massachusetts General Hospital, Weymouth, MA, USA Introduction: Approximately 25%-30% of patients with advanced cancer experience marked anxiety and depression symptoms that are associated with significant distress, worse functioning, poor treatment adherence, and reduced quality of life. Although cognitive-behavioral therapy (CBT) is an evidence-based, first-line treatment for anxiety and depression symptoms, patients have limited access to such mental health care, especially among those with high symptom burden receiving cancer treatment. We therefore adapted a brief CBT intervention for patients with advanced cancer to a mobile application (app) platform for self-administration via tablet computer. A recent randomized controlled trial of this CBT mobile app intervention versus a health education control program revealed that patients in both study groups reported signif-icant improvements in anxiety and depression symptoms over a three-month period. The goal of this follow-up study was to explore whether the effects of the intervention on patient-reported outcomes varied by time since diagnosis of advanced cancer. Methods: From 2/15 to 8/16, 145 patients with incurable solid malignancies who screened positive for at least mild anxiety symptoms (i.e., Hospital Anxiety & Depression Scale-Anxiety subscale, HADS-A>7) were randomized 1:1 at two cancer centers to receive either the CBT mobile app or a mobile health education program (control), delivered via tablet computers. The CBT app consisted of 7 modules teaching skills to relax the body, reduce worry, stay present-focused, improve communi-cation, and plan/pace activities, which patients completed over 12 weeks. The health education program was matched for the number of sessions and included general information about side effects of cancer treatment, exercise, nutrition, memory and cognition, sexual health, and quality of life. To assess anxiety and depression symptoms, we administered the Hamilton Anxiety Rating Scale (HAM-A), Hospital Anxiety & Depression Scale (HADS), and Patient-Health Questionnaire-9 (PHQ-9) at baseline and 12 weeks. Linear regression models were used to test interaction effects between group assignment and time since cancer diagnosis on patient-reported outcomes, controlling for baseline scores. We dichotomized time since cancer diagnosis at the median to explore whether intervention effects differed within each subgroup.44Results: Enrolled patients (Mean Age=56.45 years, SD=11.30) predominantly self-identified as white (91.0%), female (73.8%), and married/partnered (75.9%). The most common cancer types were advanced gastrointestinal, gynecological, lung, and breast malignancies, with a median time from diagnosis of 7.13 months (IQR 1.92-23.46). Time since cancer diagnosis moderated the effects of the CBT mobile app on the HADS-Anxiety Subscale (Group Assignment X Time Since Diagnosis B=3.24, SE=1.21, p=.008), HADS-Depression Subscale (Group B=2.69, SE=1.07, p=.013), and PHQ-9 (Group Assignment X Time Since Diagnosis B=3.21, SE=1.58, p=.044) but not the HAM-A scale (Group Assignment X Time Since Diagnosis B=3.04, SE=2.74, p=.270). Specifically, among the subgroup of patients who were closer to diagnosis (i.e., median of 7.13 months), those assigned to the CBT mobile app reported greater improve- ments in anxiety and depression symptoms compared to those assigned to the health education program. Among patients who had been diagnosed greater than 7.13 months, the two study groups did not differ significantly in their reported improvements in anxiety and depression symptoms from baseline to post assessment.Conclusion: Patients who received the CBT mobile app and were closer in time to diagnosis of advanced cancer reported greater improvements in psychological outcomes than those in the health education program. These findings suggest that providing an evidence-based CBT intervention may yield salient benefits in alleviating depression and anxiety symptoms closer to cancer diagnosis in this population. Moreover, such mobile technologies possess great promise for improving access to essential mental health care in a timely, convenient, and low-burden manner. 47 Hope Chang, Cancer Center Virtual Patient-Centered Intervention for Women on Adjuvant Endocrine Therapy after Breast Cancer: An Open MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Dana-Farber Cancer Institute, Boston, MA, USA and 4University of Miami, Coral Gables, FL, USA Introduction: Approximately 75% of breast cancers are hormone sensitive and treated with adjuvant endocrine therapy (AET) for up to 10 years, reducing risk of recurrence by 40-50%. Despite clinical benefit, adherence to AET is suboptimal; up to 59% of patients do not take AET as prescribed. Previous literature has led to a robust understanding of risk factors for poor AET adherence, including side effects (e.g., hot flashes, joint pain), low self-efficacy in managing side effects, low perceived need for medication, doubts about therapeutic efficacy, minimal social support, and distress (e.g., depression, anxiety). While several factors are modifiable, evidence-based interventions to improve adherence to AET are lacking. Therefore, we developed a patient-centered, cognitive-behavioral, virtual, small group-based intervention targeting adherence to AET, symptom management, and distress for patients after breast cancer treatment. We created the five-session intervention (Symptom-T argeted Randomized Intervention for D istress and Adherence to Adjuvant E ndocrine Therapy [STRIDE]) using qualitative data from semi-structured interviews with patients on AET (n=30) and experts in breast oncology and behavioral intervention science. Here, we report the findings from a small open pilot study (N=5) to a) examine preliminary accept - ability and feasibility of the intervention, b) review study procedures, and c) refine the intervention for a pilot randomized controlled trial (RCT). Methods: From 3/2019-5/2019, we recruited female patients prescribed AET after completing treatment for early-stage (0-IIIb), hormone-receptor positive breast cancer. Patients who screened in for high distress ( 4/10) related to AET, adherence difficulties, or side effects signed informed consent, were enrolled, and completed baseline self-report psycho-social assessments. They were also instructed to store their AET in a Medication Event Monitoring System (MEMS) Cap that records bottle-use to monitor adherence. We compiled three cohorts: two groups of two participants each, and one single participant cohort due to a scheduling conflict. All participants completed the five one-hour, weekly, videoconfer - ence STRIDE sessions with a licensed clinical psychologist. The intervention included assessment and problem-solving of barriers to adherence, psychoeducation, cognitive-behavioral skills, relaxation training, and coping strategies for side effects. Following each session, participants rated five acceptability constructs on a 1-10 scale (higher scores=greater acceptability): enjoyableness, convenience, helpfulness, usefulness, and satisfaction. At 3-months post-baseline, participants completed a post-intervention assessment that included a 5-item questionnaire to assess overall acceptability and feasibility on a scale of 1 (quite dissatisfied) to 4 (very satisfied). Finally, participants completed semi-structured interviews with trained study staff to solicit feedback about intervention content, length of assessments, and other preferences. We descriptively examined the mean session ratings, mean construct ratings across sessions, and mean overall acceptability items. We collated qualitative responses to inform intervention changes.Results: Patients (n=5) were an average of 56 years of age (SD=13.78), were non-Hispanic white, and had been on AET for 8.4 months (SD=4.83), on average. All participants completed 100% of sessions, with excellent acceptability ratings (1-10) 45averaging 9.55 (SD=0.36) across sessions. Ratings for sessions four (M =9.90, SD=0.14) and five (M=9.88, SD=0.11), which focused on coping skills for individual side effects, were the highest, reflecting interview statements that patients found these sessions to be most helpful. Across sessions, patients rated the intervention as highly enjoyable (M=9.59, SD=0.46), convenient (M =9.34, SD=0.46), helpful (M =9.39, SD=0.64), and satisfactory (M =9.69, SD=0.28). On the overall acceptability and feasibility measure, participants were highly satisfied with the virtual delivery (M =3.8, SD=0.45), length of assessments (M=3.8, SD=0.45), and the group setting (M=4.0, SD=0.0). Satisfaction with the duration of each session (M =3.6, SD=0.55) and number of sessions (M =3.2, SD=0.84) reflected qualitative feedback that patients wanted more sessions and time spent on addressing personal side effects. Additionally, participants expressed that the relaxation exercises, ability to receive support from others on AET, and the tailoring of coping strategies to be extremely helpful. They described enthusiasm and would strongly recommend the program to others on AET.Conclusion: Patients on AET after breast cancer who participated in a patient-centered, small group-based, virtual interven - tion for adherence, symptom management, and distress reported high intervention acceptability, enjoyableness, and useful- ness. We demonstrated feasibility with retention and high scores related to the virtual delivery, group setting, convenience, and assessment length. Patients found the material to be beneficial and tailored to their needs, desiring more time and sessions to address their personal side effects. Subsequently, we incorporated this feedback into the final intervention by adding a sixth session that focuses on side effect management and will begin formal testing of acceptability and feasibility in a pilot RCT 48 Anil Chekuri, Ph.D, Neurology Characterization of the retinal in a humanized FD mouse model with defective ELP1 splicing A. Chekuri1, Morini1, X. Wang2, L. Vandenberghe2 and S. Slaugenhaupt1 1Nuerology, Mass General Hospital, Boston, MA, USA, 2Mass Eye and Ear Institute, Boston, MA, USA and 3Department of Physiology, The University of Tennessee, Health Science Center, Memphis, TN, USAIntroduction: Familial dysautonomia (FD) by a splice mutation in the gene encoding Elongator complex protein 1 (ELP1, also known as IKBKAP ). This mutation results in skipping of exon 20 and tissue specific reduction of ELP1 protein levels, predominantly in the central and peripheral nervous system. Although FD patients exhibit a complex neurological phenotype due to the degeneration of sensory and autonomic neurons, progressive retinal degeneration severely impacts quality of life. Two different mouse models have been previously generated to study the retinal phenotype that results from complete loss of ELP1. However, neither of these models accurately recapitulates the tissue specific defective splicing observed in FD patients. Our recently developed FD mouse model, TgFD9; Ikbkap 20/flox, was generated by introducing the human ELP1 transgene with the FD mutation into a mouse hypomorphic ELP1 allele and a knock-out allele. Detailed characterization of the retinal phenotype in this FD mouse model was performed to investigate the pathology associated with the splice mutation in ELP1. Retinal thickness was measured using Optical Coherence Tomography (OCT) and retinal morphology was analyzed using Haematoxylin and Eosin staining of retinal sections from FD and control mice. Retinal whole mounts were used to analyze the number of retinal ganglion cells (RGC). Results: Optical Coherence Tomography (OCT) analysis to investigate retinal thickness revealed significant reduction in the thickness of the retinal nerve fiber layer (RFNL) and Ganglion cell layer (GCL) when compared to control littermates. Analysis of retinal morphology revealed progressive loss of (RGC's) and significant reduction in outer nuclear layer (ONL) thickness, indicating loss of photoreceptors. RGC cell counting using whole mount retina was also performed to substantiate the loss of RGC's. Conclusion: Our recently developed FD mouse model exhibits most of clinical features of the disease and, importantly, accurately models the tissue specific splicing defects observed in FD patients. Our findings suggest that our novel FD mouse model will provide a platform in which to test the in vivo efficacy of ELP1 splicing modulation to increase functional ELP1 in the retina.4649 Karmel Choi, PhD, Psychiatry A phenome-wide approach to identifying modifiable resilience factors for depression K. Choi2,1, M. Stein3, K. Nishimi2, T. Ge1, K. Koenen2 and Smoller1 1Massachusetts General Hospital, Boston, MA, USA, 2Harvard T.H. Chan School of Public Health, Boston, MA, USA, 3UCSD, La Jolla, CA, USA and 4King's College London, London, United Kingdom Introduction: Genetic vulnerability and traumatic life events represent two of the most robust risk factors for depression, the leading cause of disability worldwide. Although many individuals with these risk factors do not develop depression\u2014suggesting resilience\u2014the breadth of actionable prevention targets remains relatively unknown. Using data from the UK Biobank (N=112,587), we took advantage of a unique opportunity to screen a wide range of potentially modifiable factors that could offset known risk factors for depression.Methods: We curated baseline data on over 100 factors in participants' lives, including behavioral (e.g., exercise, sleep, media use, diet), social (e.g., support, activities), and environmental (e.g., greenspace, pollution) variables. In a follow-up survey, participants also reported on their traumatic life experiences and mental health, including depression. Polygenic risk scores for depression were generated based on large-scale genome-wide association results. Excluding those meeting criteria for depression at baseline, we identified at-risk individuals at high predicted probability (> 90th percentile) for clinically significant depression at follow-up based on their (i) polygenic risk, and (ii) reported history of traumatic life events. Using a phenome-wide design corrected for multiple testing and adjusted for potential confounders, we examined prospective associations between each baseline factor and subsequent depression in the full sample and among at-risk individuals. We then used two-sample Mendelian random- ization (MR)\u2014a genetically informed method for causal inference\u2014to validate the relationship between significant factors and depression.Results: Social support, exercise, and sleep factors were most strongly associated with absence of depression among at-risk individuals, while environmental and dietary factors were additionally observed in the full sample. MR evidence supported a potential causal role of social support, media use, and other factors in reducing depression risk. Conclusion: Here, we present a novel approach to screen broadly for potentially modifiable factors that contribute to resilience to depression even among those at high genetic and environmental risk, which could be further validated in mechanistic studies of depression and clinical prevention trials. 50 Joseph H. Chou, MD PhD, Pediatrics Development of a machine learning model to predict neonatal follow-up bilirubin levels and comparison with clinician performance J.H. Chou MGH, Boston, MA, USA Introduction: Hyperbilirubinemia affects many newborns and if not appropriately treated can result in irreversible brain injury. Unrecognized hyperbilirubinemia also has a major economic impact with ~80% of newborn readmissions related to jaundice and dehydration. Bilirubin is a byproduct of red blood cell metabolism, processed by the liver, and excreted in the urine and stool. Phototherapy converts bilirubin into a more easily excretable form. Previous work in neonatal hyperbil - irubinemia has largely been limited to term and late preterm newborns, without risk factors, and prior to the initiation of phototherapy. We sought to develop a predictive model of follow-up bilirubin measurement that could be utilized regardless of gestation or prior treatment, and to compare performance with clinician predictions.Methods: Subjects of this retrospective study were patients born between June 2015 and June 2019 at four Massachusetts birthing hospitals with available electronic health records. Patients of all gestations were included. Data was restricted to measurements obtained in the first 10 days of life. The prediction target was a follow-up bilirubin measurement obtained < 72 hours after a prior measurement. Candidate predictive features abstracted from the electronic health record included: birth gestation, gender, birth weight, prior bilirubin results, age in hours, time to next result, phototherapy, enteral intake, stools and urine output, change in weight, initial hematocrit, maternal and baby blood type and Rh, direct Coombs status, maternal age, grava / para, race and ethnicity. Birth before versus after February 2019 was used to generate a training set (9,723 patients, 27,428 target bilirubin measurements) and held-out test set (1,224 patients, 3,320 measurements) respec-tively. A number of prediction models were trained, including via linear models, linear models with interaction terms selected via LASSO, random forest, simple neural network, long short-term memory neural network, and Xgboost. To assess model 47performance, predictions on the held-out test set were also compared with a corresponding 210 predictions from clinicians, including attending physicians, advanced practitioners, pediatric residents, and nurses.Results: The best predictive performance on the held-out test set was obtained with the simple neural network (mean absolute error, MAE, 1.04 mg/dL), and Xgboost (MAE 1.03 mg/dL). A limited number of predictors was sufficient for best performance and avoiding overfitting. For the neural network, the predictors used included age in hours, proportion of time under phototherapy, previous result, time to next result, weight change, gestational age, and previous rate-of-rise and time under phototherapy. For Xgboost, the predictors identified as most important were very similar. Predictive performance in preterm newborns was similar or better than in term newborns. Clinicians made a total of 210 prospective predictions after February 2019 (which would all be in the held-out test set). The simple neural network model performance on this subset of 210 predictions had an MAE of 1.05 mg/dL, better than all clinicians combined with MAE 1.36 mg/dL (23% lower error, MAE difference = 0.31, p < 0.0001). However, clinician performance may have differed by role, with advanced practitioners MAE = 1.16 mg/dL (n = 74), attending physicians 1.32 mg/dL (n = 60), pediatric residents 1.53 mg/dL (n = 45), and nurses 1.65 mg/dL (n = 31). When limited to comparing predictions only against advanced practitioners and attending physicians, the neural network still had superior performance (23% lower error, MAE difference = 0.29, 0.95 versus 1.24 mg/dL, p = 0.0003, n = 134).Conclusion: We have developed a model for follow-up bilirubin level prediction in newborns less than 10 days old which outperforms clinicians. This may also be the first report of a bilirubin predictive model that is not limited to term and late preterm gestation patients and which takes into account the effect of phototherapy and additional risk factors such as dehydra - tion and isoimmune hemolytic risk. 51 Anita Chung, BSN, Dermatology Gastrointestinal Screening in the Primary Care Setting Using Tethered Capsule Endomicroscopy A. Chung1, C. Grant1, N. Bhat1, Photomedicine, MGH, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Assembly Row Primary Care, Massachusetts General Hospital, Somerville, MA, USA Introduction: Upper endoscopies are currently used to diagnose GI diseases such as Barrett's Esophagus. These procedures are expensive, time consuming, uncomfortable, inconvenient to patients, and require sedation. Tethered Capsule Endomi - croscopy (TCE), developed by the Tearney Lab, is a well-tolerated and convenient method of examining the esophagus in an unsedated population. After successfully enrolling 60 subjects in a primary care pilot study, the Tearney Lab is now conducting a larger study in a primary care practice to determine the feasibility of using TCE as a screening method for Barrett's esophagus in this setting. This study will also help to determine the prevalence of Barrett's esophagus in a primary care practice cohort at MGH. Methods: TCE involves swallowing a tethered capsule that circumferentially scans an OCT beam inside the body as it traverses the organ. Throughout the procedure, microscopic images of the esophagus are acquired in real time in an unsedated subject. Flyers containing information about the study and how to participate in the study are available to patients in the waiting room of Mass General primary Care Assembly Row. Patients are approached by research study staff in the waiting room. If patients are interested in participating in the study, study staff will determine their eligibility and will consent the patient if they are eligible. The imaging procedure is either conducted after their doctor's appointment or at a later time convenient to the patient, as to not disrupt provider workflow. The procedure is conducted by trained operators. Subjects are asked to take sips of water to swallow the capsule. They are also given the option of using an over the counter throat numbing spray and an over the counter lubricant spray. The imaging procedure starts after the capsule operator confirms that the capsule has been swallowed. The capsule operator then moves the capsule down and up the esophagus up to 4 times. At the end of the imaging procedure, the capsule is removed from the esophagus through the mouth. A feedback questionnaire is provided at the end of the procedure.Results: The goal of the study is to enroll 200 subjects. To date, 78 subjects have been enrolled at Mass General Primary Care Assembly Row. 55 subjects (71%) successfully swallowed the tethered capsule. 51 subjects were female, and 27 subjects were male. Of the 51 female subjects, 34 subjects (67%) successfully swallowed the tethered capsule. Of the 27 male subjects, 21 subjects (78%) successfully swallowed the tethered capsule. On average, a successful imaging procedure took 4.5 minutes to complete. The overall procedure discomfort rating was 2.8 (average) on a 0-10 scale (0-no discomfort, 10-most discomfort). All subjects who were able to successfully swallow the tethered capsule said they would recommend the procedure over an endoscopy.48Conclusion: Tethered capsule endomicroscopy is tolerated well in this primary care setting and appears to be an effective method for examining the esophagus in an unsedated population. Future work will include data analysis to determine the prevalence of Barrett's in MGH primary care practices. 52 Timothy W. Churchill, MD, Medicine - Cardiology Exercise Intensity and the Human Plasma Massachusetts General Hospital, Boston, MA, USA, 2Cardiovascular Performance Program, Massachusetts General Hospital, Boston, MA, USA and 3Cardiovascular Research Center, Massachusetts General Hospital, Boston, MA, USA Introduction: Aerobic exercise has well-described benefits to human health and holds a central place in guideline-directed care for the promotion of cardiovascular and neurological health. Despite this, the mechanisms by which exercise transduces its effects and the precise influence of exercise intensity remain poorly understood. Plasma proteins play fundamental roles in numerous biological processes including growth, repair, and signaling, but characterization of their changes with exercise have been limited to date. We hypothesized that the human plasma proteome would demonstrate distinct, intensity-dependent responses to single sessions of exercise and that these acute changes, when integrated over time, might explain in part the beneficial and adverse effects of chronic moderate and vigorous intensity exercise. Methods: We conducted a prospective study in 12 healthy adult males participating in running sessions at discrete, cardio- pulmonary exercise test-defined intensities to examine the impact of aerobic exercise intensity on the circulating plasma proteome. Plasma concentrations of ~1,300 proteins were assayed using a well-validated aptamer-based proteomics platform (SOMAscan platform, SomaLogic, Boulder, Colorado) before and immediately after bouts of treadmill running at moderate and high intensity. We developed a model of transcriptional inference to assess likely tissue sources of proteins increasing in plasma with exercise. Gene ontology analyses were employed to define the functional biologic pathways perturbed at each exercise intensity. Lastly, we integrated existing epidemiologic findings and genomic data to link protein levels and associated genetic loci to recognized exercise-modifiable traits.Results: Baseline cardiopulmonary testing defined moderate and high intensity exercise with the inflection point between the two at the ventilatory threshold (Figure 1a). A total of 623 proteins (48% of measured proteome) were dynamically regulated by acute exercise. Of these, 25 and 439 proteins were uniquely responsive to moderate and high intensity exercise, respectively, while 159 changed at both intensities (Figure 1b). Of this latter group, all were perturbed concordantly and changed in the same direction, although we observed a wide range of intensity dependence with the magnitude of change dictated by exercise intensity (Figure 1b). Proteomic contribution to the plasma involved nearly all organ systems, with predominant inferred contributions from the nervous, cardiovascular, and gastrointestinal systems; skeletal muscle was notably a relatively minor contributor (Figure 1c). Functional analyses showed both shared and distinct functional pathways at the two exercise intensities, revealing enrichment at moderate intensity for pathways involving leukocyte chemotaxis and chylomicron metabolism and at high intensity for pathways associated with Wnt signaling, neuronal axonal sprouting, and nitric oxide biosynthesis. Figure 2 highlights exercise regulation of 15 proteins tied to genomic loci associated with relevant exercise-associated cardiovascular and neurologic phenotypes and reveals directionally-congruent changes in protein levels consistent with observed population-level effects. Finally, we examined exercise-regulation of a set of proteins recently noted to be associated with incident atrial fibrillation and found that nearly all (7 of 8) changed in a direction associated with increased risk of atrial fibrillation (2 at moderate intensity, all 7 at high intensity). Conclusion: This study provides the first comprehensive characterization of how acute aerobic exercise at varying intensi - ties differentially modulates the human plasma proteome and uncovers both common and distinct responses to moderate and high intensity exercise. Functional analyses suggest that these distinct proteomic responses may translate into distinct biologic impacts that underlie exercise-associated traits, with high intensity exercise appearing to hold particular implications for neurological health and unwanted cardiovascular effects such as atrial fibrillation. These data support the concept that exercise may transduce some of its effects by influencing non-cell autonomous protein signaling and set the stage for future work to further deconstruct the systemic benefits of exercise.49 53 Gia Ciccolo, BA Biology/Spanish, MPH, Emergency Improving understanding of health-related social needs screening questions using cognitive interviews G. Ciccolo, A. Curt, C. Camargo and M. Samuels-Kalow Emergency Medicine Research, MGH, Boston, MA, USA Introduction: Emergency departments (EDs) serve as safety net providers, but face significant challenges in addressing the non-medical, but health-relevant, needs of their patient population - i.e., health-related social needs (HRSN). Identification of HRSN is challenging due to the wide variety of screening tools in use and the lack of a 'gold standard' for validating such tools. The goal of this study was to develop an easily understandable screening tool for HRSN for ED patients based on questions that were used in prior HRSN studies. Methods: We conducted a systematic review of existing HRSN screening tools using web-based searches and PubMed and developed a list of questions in the public domain corresponding to the 5 categories of the Accountable Health Communities Screening Tool. We then performed iterative cycles of cognitive interviews with patients and family members. Cognitive interviewing is a qualitative technique that allows a researcher to examine a participant's experience with a survey. We used a hybrid model of (1) 'think-aloud' where the participant is asked to describe their thought process, and (2) 'verbal probing' where the interviewer asks specific questions about the participant's experience with the tool, focused around clarity and ease of use. After thematic saturation was reached in one cycle, the tool was changed in response to participant input; cycles were completed until we received no new feedback about the tool.Results: A total of 16 patients completed cognitive interviews over the course of 3 cycles. Of the 16 participants, 4 (25%) were primarily Spanish speaking and 5 (31%) were categorized as having low health literacy. Based on participant feedback, the survey was reduced and simplified from 16 questions, concerning 5 domains of HRSN (homelessness, food insecu-rity, transportation, inability to pay for utilities and neighborhood safety) to 10 questions concerning 4 of the 5 domains, excluding that of neighborhood safety. Questions concerning neighborhood safety were removed as participants did not have consensus on the meaning of safety. Overall, questions were reordered for clarification, or removed for discomfort in responding or similarity to other questions. Additionally, responses were simplified to binary options for improved partici - pant understanding and ease in taking the survey. Patients with low health literacy provided less suggestions in the cognitive interviews, suggesting that additional techniques may be needed to optimize screening for this population.Conclusion: An iterative cognitive interviewing process was used to develop a short form questionnaire for HRSN screening in the ED. The amount and variety of patient informed edits made to the tool in this process demonstrates the importance of patient input to refine HRSN questionnaires prior to use in surveillance or future research.50HRSN Survey Tool changes through each Round of Cognitive Interviews 54 Debora Ciprani, MD, Surgery - General and Gastrointestinal An elevated serum CA 19-9 is associated with invasive cancer in IPMN and worse survival D. Ciprani, V. Morales-Oyarvide, M. Qadan, T. Hank, M. Weniger, C. Ferrone, K. Lillemoe, A. Warshaw and C. Fernandez-Del Castillo Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Current guidelines for the management of IPMN include serum carbohydrate antigen (CA) 19-9 among the worrisome features. However, the correlation of CA 19-9 with histological malignant features and survival is unclear. The aim of the current study was to assess the utility of serum CA 19-9 in the management of IPMN.Methods: Patients with resected IPMN between 1990-2018 from the Massachusetts General Hospital were analyzed. Clinical, pathological, and survival data were collected and compared to preoperative levels of CA 19-9. Univariate analysis and Cox regression were performed comparing patients with CA 19.9 levels lower and equal or higher than 37 U/mL.Results: 600 patients who underwent resection for IPMN and had preoperative CA 19-9 measured were identified. A total of 137 patients had elevated levels of CA 19-9 greater than 37 U/mL. Median follow up was 68 months (0-302). Patients with high CA 19.9 were more likely to have main duct IPMN than the ones with normal levels (35.2% vs. 21.7%, p=0.007), and elevated CA 19.9 was twice as likely in patients who harbored invasive carcinoma within their cyst (56.9% vs. 28.4%, p < 0.001). There were no differences in CA 19-9 levels in patients with high-grade dysplasia compared to those without (28.81 % vs 22.36%, p=0.28). Patients with elevated serum levels of CA 19-9 had worse overall and disease-free (HR CA 19-9 greater than 37 U/mL is associated with invasive cancer in patients with IPMN and with worse overall and disease-free survival. The recent inclusion among the worrisome features appears to be justified. 55 Mark Clapp, OB/GYN Severe unexpected complications in term newborns as an obstetric quality metric M. Clapp1, K. James1, S. Bates2 and A. Kaimal1 1Obstetrics and Gynecology, MGH, Boston, MA, USA and 2Pediatrics, MGH, Boston, MA, USA Introduction: Unexpected complications in term newborns have been recently adopted by the Joint Commission as a marker of obstetric care quality. It is intended as a balancing measure to more maternal-focused metrics but is largely understudied. The objective of this study was to understand the variation and patient and hospital factors associated with severe unexpected newborn complications among hospitals in the US. Methods: This is a population-based study of all births in counties with one obstetric hospital in the United States using 2015-2017 county-identified birth certificate data and American Hospital Association Annual Survey data. The study popula-tion included liveborn, term, singleton infants weighing at least 2,500 grams. Severe unexpected complications in term newborns were defined as neonatal death, 5-minute Apgar 3, seizure, assisted ventilation 6 hours, or transfer to another 51facility. Multi-level models using patient-level data were used to quantify the hospital-level variation and predictors associ- ated with unexpected newborn complications.Results: There was a wide range of hospital complication rates (0.6 to 89.9 per 1,000 births) with measurable between-hos- pital variation. In the adjusted models, there was little effect of case mix to explain the observed between-county variation (pre-adjustment 11.7% 10.0-12.6%)). Neonatal transfer was the primary driver of hospital complication rates, especially among hospitals with the highest rates (66% of all complications). A hospital's level of neonatal care was related to its severe unexpected neonatal complication rate. The risk for unexpected neonatal complication was increased by over 50% for those neonates born at hospitals without a neonatal intensive care unit (NICU) compared to those with a NICU (adjusted odds ratio 1.55 (95% CI 1.38-1.75)).Conclusion: There is wide variation in severe unexpected complication rates among term newborns, which highlights the potential for a metric of this type to meaningfully reflect quality of care. However, when using the current definition, neonatal transfer is the primary driver of complications, especially among counties with the highest rates. Transfers are more likely to be necessary when infants are born in places with lower levels of neonatal care. Thus, if this metric is to be used in its current form, accreditors, regulatory bodies, and payers should consider adjusting for or stratifying by a hospital's level of neonatal care in order to avoid disincentivizing against appropriate transfers. Comparison of severe unexpected newborn complications among hospitals with low (1st decile), mid-range (2nd-9th decile), and high (10th decile) unexpected newborn complication rates All complication data are presented as n (% of deliveries). Figure 1A shows the distribution of hospital rates of severe unexpected newborn complications, including neonatal transfers per the Joint Commission metric, among hospitals without NICU beds (shaded pink bars) and with NICU beds (white with red outline bars). Figure 1B shows the same distribution when neonatal transfers are excluded from the complication metric. NICU, neonatal intensive care unit. 56 Emily C. Cleveland Manchanda, MD, MPH, Emergency Interprofessional Gender Bias During Emergency Medicine Residency Training E.C. Cleveland Manchanda1,2, A. Chary1,2, N. Konstantopoulos1 and V. Dobiesz2 1Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Emergency Medicine, Brigham and Women's Hospital, Boston, MA, USA and 3Global Health and Social Medicine, Harvard Medical School, Boston, MA, USA Introduction: Gender disparities continue to persist within the medical field. The adverse effects of gender bias have been well documented, including among trainees in Emergency Medicine (EM). Recent studies have demonstrated significant differences in the evaluation of female and male trainees with respect to milestone achievement during residency, 1,2 which may be attributable to unconscious gender bias among educators. The extent to which gender-based discrimination occurs in the context of interprofessional interactions is not well understood. Of particular interest is extent to which this occurs in interactions between resident physicians and nurses. The aim of this study is to explore and understand perceptions and experiences of bias in the context of inter-professional relationships between EM residents and emergency department (ED) nurses. Methods: We undertook a mixed-methods study exploring perceptions of gender bias in the workplace. Using a triangulation model, we explored the way gender shapes interprofessional interactions in our EDs through key informant interviews, focus groups, and an anonymous survey administered to all EM trainees and nurses working in the EDs at Brigham and Women's and Massachusetts General Hospitals. Results: Female and male nurses and residents identified gender as an important factor in interprofessional working relation- ships in the ED. Among female interviewees and focus group participants, several themes emerged: negative perceptions of females' professional capability, particularly between professional groups; lower confidence and trust between female nurses and resident physicians; and frequent negative personal or personality-related comments in interprofessional interactions. 52Among male physicians and nurses, recognition of gender privilege was mixed. However, males noted that gender bias adversely affects their female colleagues on a frequent basis. [Note survey data collection and analysis is not yet complete; this will be completed prior to poster presentation (September 2019).] Conclusion: Gender continues to play a significant role in shaping interprofessional interactions, including between trainees in EM and nursing staff. Gender bias contributes to dissatisfaction in the workplace, the effects of which are felt by both male and female nurses and resident physicians. 57 Alison Cloutier, Masters Degree, Neurology Visualization of Multivariate Stroke Recovery Data Hochberg1 and Massachusetts General Hospital, Boston, MA, USA, 2Physical Therapy, Massachusetts General Hospital, Boston, MA, USA and 3Occupational Therapy, Massachusetts General Hospital, Boston, MA, USA Introduction: Stroke affects approximately 800,000 people per year in the United States alone and is the leading cause of acquired adult disability. The World Health Organization's International Classification of Function, Disability, and Health (WHO ICF) provides a framework for understanding how stroke comprehensively affects people. Longitudinal assessments spanning the WHO ICF framework can be used to build a multidimensional database of outcomes that capture recovery after stroke. Here we investigated the use of radar plots to visualize recovery over the course of one year post stroke.Methods: One hundred participants were enrolled in an ongoing single-center, prospective, observational cohort study (Stroke Motor Rehabilitation and Recovery Study). Participants ranged from 21 to 88 years of age, could follow simple commands, and presented with arm weakness due to ischemic stroke. Assessments spanning the WHO ICF domains for body function and structure (NIHSS, Fugl-Meyer, Dynamometer), activity (9 Hole Peg, Box and Blocks, Barthel Index, Rankin Scale), and participation (Stroke Impact Scale, PROMIS-10) were performed during the acute stroke hospitalization, at 6 weeks, 3 months, 6 months, and one year post stroke. Twenty-three participants with complete data at all time points were included in this study. Results: A radar plot was constructed for each individual participant where each ray projecting from the central point of the plot represents one clinical outcome scale. Data from each assessment was normalized and plotted on a scale from zero to one, with one representing full recovery. Solid lines connect the data for each scale across a single assessment time point creating a closed polygon. As a participant recovers, the goal is for the polygon to grow towards the outer most point on each ray. On an individual basis, these plots provide a way to give rapid visual feedback of recovery over time to patients and providers. When visualized as a collective group, these radar plots illustrate the dynamic nature of recovery and the heterogeneity of this population. For example, two participants who score a 9 on the NIHSS, a 4 on the Fugl-Meyer, and have no hand function during the acute stroke hospitalization, display dramatically different recovery trajectories across the ICF domains: one patient regained very little hand function by one year and the other had the ability to complete the 9 Hole Peg test, a measure of hand dexterity. Conclusion: Radar plots when used to present data longitudinally over time, illustrate the dynamic and multidimensional nature of stroke recovery. 5358 Marya Cohen, MD MPH, Medicine - Primary Care Evaluation of an Interprofessionalism Curriculum Within a Network of Student-Faculty Collaborative Clinics M. Cohen Medicine, MGH, Chelsea, MA, USA Introduction: Interprofessional education (IPE) allows trainees across health professions to learn with each other and about each other's roles, preparing them to effectively serve on the interprofessional teams that are critical to modern healthcare practice. The Crimson Care Collaborative (CCC) is a network of seven student-faculty collaborative practices that provides primary care at sites across the greater Boston area and includes dental, medical, nursing, physician assistant, social work, and undergraduate student volunteers from 7 Boston-area health professions training programs. Here we describe the develop- ment and evaluation of a didactic IPE curriculum developed to complement learners' clinical IPE experiences. Methods: An IPE committee was formed that included both student and faculty input from a diverse array of health profes-sions involved in CCC. The committee identified clinical scenarios encountered by volunteers at MGH CCC sites that support development of competency in IPE, including the importance of various healthcare roles, communication, team -based care and the social determinants of health. Student-faculty dyads, including student volunteers engaged in clinical care at CCC developed interactive case-based curricula utilizing these clinical scenarios to explicitly focus on the IPEC competencies in monthly small group discussions. Facilitator guides were also developed to promote high-quality didactic education and standardization of teaching across clinical sites. A standardized evaluation tool was implemented to assess the students' educational experience, using 5-point Likert scales. Descriptive statistics were calculated. Results: A total of 104 trainees responded to post-session surveys after participating in at least one of 7 developed cases. Most respondents were medical students (47%), followed by nurse practitioner students (19%) and physician assistant students (8%). Participants reported significantly higher knowledge of IPE after participation (3.6\u00b10.9 vs. 2.9\u00b11.1, p<0.004). Year of training was not significantly associated with increased IPE knowledge either before or after the session. Students with previous exposure to IPE curriculum had higher pre-knowledge scores than those without prior curricular exposure. Qualita - tive analysis of open-ended feedback provided by learners indicated that they enjoyed the interactivity and \"real-world\" feel of the cases, and would like additional time to discuss the cases in subsequent sessions.Conclusion: The cases developed by the IPE team have been well-received by learners, who perceive them to be beneficial and increase confidence working in teams with other health professionals. The sessions also serve to standardize IPE experi- ences across sites. Our results indicate the importance of supportive didactic training structures to reinforce the interprofes- sional training and practice that takes place in clinical learning environments. 59 Austin K. Collins, BA, Orthopedics Femoral Offset has Minimal Impact on Vitamin-E diffused Polyethylene Liner Wear at 5-Years A.K. Collins, N. Dupaguntla, V. Galea, C. Bragdon and H. Malchau Orthopaedics, Massachusetts General Hospital, Boston, MA, USA Introduction: Total Hip Arthroplasty (THA) performed with Metal-on-Polyethylene (MoP) implants has proven to be an effective treatment for end-stage hip osteoarthritis. The current wide-spread success of this procedure is due in part to the introduction of crosslinked polyethylene (XLPE), which has improved implant durability by minimizing liner wear over time. Liner wear, leading to osteolysis and aseptic loosening, was previously the most common cause of THA failure when THA implants utilized non-crosslinked ultra-high molecular weight polyethylene (UHMWPE). While XLPE has demonstrated negligible in vivo wear through 12 years, in vitro studies have highlighted its potential for long-term oxidation and embrit - tlement. To prevent this potential issue, a new generation of XLPE, infused with the antioxidant vitamin-E (VEPE), was introduced to the market in 2007. While longer patient follow-up will elucidate the anti-oxidative benefits of VEPE in the coming decade, current VEPE research is focused on determining the patterns of mid-term VEPE wear. One factor thought to influence liner wear is femoral offset (FO), defined as the perpendicular distance from the center of the femoral head to a line through the center of the femoral canal. FO has been shown to influence UHMWPE wear, but, to our knowledge, no studies have investigated its effects on VEPE. Therefore, the aim of this study was to measure the impact of FO on VEPE wear through 5 years following THA as assessed by radiostereometric analysis (RSA), the gold-standard method of measuring in vivo implant wear. Methods: A prospective, international, multicenter RSA study, with the primary purpose of investigating the clinical and radiographic outcomes of VEPE liners in THA, was conducted. Of a total of 208 participants (221 THAs), 85 hips (38%) were excluded from the present analysis as they were treated with non-VEPE liners. Furthermore, to be included in this 54analysis the participant must have had a postoperative anterior-posterior hip (APH) radiograph as well as 1-, 2-, 3-, or 5-year RSA radiographs. FO was measured using the postoperative APH radiograph and mdesk software. Offset quartile was calculated based on the distribution of offset values. Femoral head penetration into the VEPE in the proximodistal plane was calculated with umRSA software. VEPE creep was defined as penetration within the first year and VEPE wear was defined as penetration after one-year. Femoral head penetration into the VEPE within the first year has been shown to be due to liner deformation rather than liner material loss. As a result, the period up to 1-year was excluded from our analysis of total wear and wear rate. Linear mixed model analysis was used to determine the effects of FO on VEPE wear over time. The final model included the interaction of FO and time as fixed effects and was selected with the Bayesian information criterion. The fixed effect of the interaction between FO and time was considered to represent the effect of FO on liner wear. The observations were clustered by patient as a random effect. The wear rates (mm/year) were calculated by plotting a line of best fit to the fixed predicted values stratified by offset quartile. Statistical significance was set at p < 0.05. SPSS statistical software was used.Results: The interaction of femoral offset and time was found to significantly effect total wear at each interval (p= 0.018, 0.022, and 0.030, Table 1). The mixed model also computed fixed predicted total wear values for each participant at each interval (Figure 1). Other factors correlated with wear of previous generations of liners were not found to improve the model. The first quartile (lowest FO) had a wear rate of 0.00301 mm/year, the second had at wear rate of 0.00359 mm/year, the third had a wear rate of 0.00399 mm/year, and the fourth (highest FO) had a wear rate of 0.00447 mm/year. Conclusion: To our knowledge, this would be the first analysis focused on FO and total wear that uses the highest standard of wear measurement tools, RSA. Additionally, the present analysis has the largest sample size, 136 hips, and is the first to focus on VEPE liners. Previous studies found a negative relationship between FO and liner wear, while we have found a positive relationship. Ultimately however, wear rates were very low and well below previously reported osteolysis thresholds. These results are promising for the longevity of VEPE liners and should give surgeons confidence that variations in FO will not impact the wear rate in a clinically significant way. 60 David Combs, MD PhD, Anesthesia, Critical Care and Pain Medicine Factors Related to Abnormal Coagulation Testing in Preeclampsia D. Combs1 and B.T. Bateman2 1Anesthesia Critical Care and Pain Medicine, MGH, Boston, MA, USA and 2Anesthesiology, BWH, Boston, MA, USA Introduction: Along with thrombocytopenia, women with preeclampsia are at heightened risk for coagulopathy, potential contraindications to neuraxial procedures. Limited evidence informs laboratory evaluation of coagulopathy in preeclampsia and whether completing such testing should delay neuraxial techniques. Methods: We performed a cross sectional study of women with preeclampsiaof deliveries with preeclampsia who underwent received coagulation testing and were hospitalized for delivery at two one of two academic hospitals between 1995 and 2018. Patients who receivedreceiving anticoagulant medications were excluded. The frequency of abnormal coagulation tests were compared across different patients grouped by admission platelet count or peak transaminase level. The rate of blood product transfusions was also compared. Results: Of 3,359 women with preeclampsia who met inclusion criteria, 124 (3.7%) had abnormal coagulation testing. The risk of abnormal testing (elevated aPTT or INR) increased with decreasing platelet counts. For an initial platelet count 150,000/L the risk was 2.7% (95% CI: 2.1 - 3.4), for between 150 and 100 x 103 / L was 5.1% (95% CI: 3.6 - 7.3), for between 99 and 70 x 103 / L was 6.5% (95% CI: 3.8 - 10.8), and for less than 70 x 103 / L was 14% (95% CI: 8.7 - 21.8). Patterns of abnormal coagulation testing were similar when redefined by more extreme test values or after excluding women 55with antepartum hemorrhage or known congenital coagulopathy. Abnormal coagulation testing was also associated with elevated transaminases. The combination of either any thrombocytopenia or transaminase elevation identified abnormal coagulation testing with a sensitivity of 74% and a negative predictive value of 98%. Blood product transfusion occurred in 33.1% of deliveries with abnormal coagulation testing compared to 7.3% with normal testing. Conclusion: In patients with preeclampsia, the frequency of abnormal coagulation testing is higher more frequent in those with a lower initial platelet count or elevated transaminases. In such patients, anesthesiologists will need to weigh the risks and benefits of proceeding with erforming neuraxial techniques without coagulation testing. 61 Grace Crotty, MD, Neurology Metabolomics analysis identifies caffeine and its metabolites as plasma markers of resistance to Parkinson's disease among LRRK2 mutation carriers in the LRRK2 Cohort Consortium (LCC) G. Crotty 1, J. Wang2, MGH, Cambridge, MA, USA, 2Denali Therapeutics, South San Francisco, CA, USA, 3Epidemiology and Nutrition, Harvard Chan School of Public Health, Boston, MA, USA and 4Biostatistics, MGH, Boston, MA, USA Introduction: LRRK2 gene mutations have incomplete penetrance for PD suggesting that genetic and/or environmental modifiers determine whether or at what age PD will develop. Metabolomic analysis provides a powerful approach for identifying such modifiers. Objective: To identify novel metabolic pathways at the intersection of LRRK2 gene biology and Parkinson's disease, and markers of resistance to Parkinson's disease among LRRK2 mutation carriers (LRRK2+). Methods: Plasma LC-MS. We quantified 307 analytes, focusing on purines and potentially related markers of lysosomal function or PD risk. For each analyte, we assessed differences among the four groups, specifically the presence of a LRRK2-PD status interaction, using ANCOVA models adjusted by age and sex, with Benjamini-Hochberg p-value corrections to control the false discovery rate. Data and biospecimens used in the analyses presented in this abstract were obtained from the MJFF-sponsored LRRK2 Cohort Consortium (LCC). For up-to-date information on the study, visit www.michaeljfox.org/lcc.Results: Caffeine concentration (mean; adjusted for age and sex) was lower in PD subjects vs. unaffected controls (p<0.0001), more so among LRRK2+ carriers (76%) than among LRRK2- subjects (31%), with significant LRRK2-PD status interaction (p=0.006). Similarly, adjusted concentrations of caffeine metabolites (paraxanthine, theophylline, 1-methylxanthine) and a marker of coffee consumption (trigonelline) were lower in PD subjects vs. unaffected controls (p<0.0001), more so among LRRK2+ carriers (by 61-67% for each) than among LRRK2- subjects (by 17-23%), with significant LRRK2-PD status interaction (p<0.01 for each).Conclusion: Metabolomic analysis of the LCC identified caffeine, its demethylated metabolites, and trigonelline as markers of reduced PD penetrance of pathogenic LRRK2 mutations, with stronger associations among LRRK2+ carriers than non- carriers. As these analytes are known both as correlates of coffee consumption and as neuroprotectants in animal models of PD, their links to LRRK2+ PD may reflect a pathophysiological to PD among LRRK2+ caffeine avoiders or direct neuroprotective effects of therapeutic relevance to LRRK2 mutation carriers. The LRRK2 Cohort Consortium is coordinated and funded by The Michael J. Fox Foundation for Parkinson's Research.5662 Kristin DSilva, MD, Medicine - Allergy/Immunology/Rheumatology Rituximab Therapy for Interstitial Pneumonia with Autoimmune Features (IPAF): A Case Series of Nineteen Patients K. D'Silva1, M.B. Bolster1, F.V. Castelino1, and Choi1 1Rheumatology, Massachusetts General Hospital, Boston, MA, USA, 2Radiology, Massachusetts General Hospital, Boston, MA, USA and 3Pulmonary and Critical Care Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Interstitial lung disease (ILD) is a major cause of morbidity and mortality in patients with connective tissue diseases (CTD). Approximately one-third of patients with ILD have autoimmune manifestations not classifiable as a CTD, designated as interstitial pneumonia with autoimmune features (IPAF). To our knowledge, there are no studies examining the efficacy of rituximab (RTX) in IPAF. Here, we describe a case series of 19 patients with IPAF treated with RTX. Methods: An institution-wide registry of patients seen consecutively at a large academic medical center from 2000-2018 was queried for patients aged 18 years with diagnostic codes for ILD, treatment with RTX, and positive met the 2015 classification criteria for IPAF (N=19) (Fischer A, et al., Eur Respir J 2015; 46:976-87). Patients were excluded if they had received RTX for a known autoimmune disease or malignancy. Clinical data and pulmonary function tests (PFTs) were collected by medical record review. Chest computed tomography (CT) scans were reviewed independently by 2 radiologists. Clinical improvement was based on a composite of clinician assessment, oxygen requirements, hospitalization, and survival. Only those subjects with PFTs within 3 months prior to or 1 month after RTX initiation and 6-18 months after RTX initiation were included in the PFT analysis (N=10); PFT improvement was defined as 10% improvement in forced vital capacity (FVC).Results: To date, we have identified 19 patients with IPAF treated with RTX, with median follow up time of 24 months (Table 1). The average number of RTX doses received was 7, over a median follow up time of 24 months (range 8-120 months). After treatment with RTX, sixteen patients (84%) were improved or stable based on clinical parameters (Table 2). Among the ten patients with PFTs within the designated timeframe, the absolute changes in FVC percent predicted was +8% (SD 13%) and diffusion capacity of carbon monoxide (DLCO) was +3% (SD 13%). No patients stopped therapy due to adverse events. One patient died due to respiratory failure. Six patients were tapered off glucocorticoids completely after 1 year of treatment with RTX. Conclusion: In this case series of 19 patients with IPAF treated with RTX, most patients appear to have had improvement or stability. These findings call for prospective studies, including potential randomized controlled trials, to further determine the risks and benefits of RTX use in IPAF. Table 1. Baseline characteristics of 19 patients with interstitial pneumonia with autoimmune features (IPAF) treated with rituximab. Table 2. Outcomes after treatment with rituximab in patients with interstitial pneumonia with autoimmune features (IPAF). *Definition based on composite of clinician assessment, change in oxygen requirements, need for hospitalization, and survival. **Improvement defined as 10% improvement in forced vital capacity. Stable defined as forced vital capacity within +/- 10% of prior measurement. Worsening defined as 10% decline in forced vital capacity. 5763 Iyas Daghlas, Center for Genomic Medicine (CGM) Bidirectional between sleep health and migraine: a study I. and R. Saxena1,2,5 1Broad Institute of MIT and Harvard, Cambridge, MA, USA, 2Center for Genomic Medicine, Boston, MA, USA, 3Division of Preventive Medicine, Boston, MA, USA, 4Neurology, Brigham and Women's Hospital, Boston, MA, USA and 5Anesthesia, Critical Care and Pain Medicine, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA Introduction: Migraine is a debilitating, highly prevalent contributor to disability worldwide, and is understudied relative to its clinical and economic impact. By the time of clinical presentation, those with migraine are more likely to report several comorbidities, including several sleep disturbances and disorders. Whether these represent causal relationships remains unknown, as prior epidemiologic studies were not designed for causal inference, and are potentially biased by residual confounding and reverse causality. Here, we aimed to assess and orient causality between sleep phenotypes and migraine using two-sample Mendelian randomization data from genome-wide studies (GWAS) of sleep conducted in UK Biobank (UKB; n >= 237,627), and of migraine conducted by the International Headache Genetics Consortium (IHGC; 59,674 cases / 316,078 controls). Bidirectional, two-sample MR was used to estimate causal effects between sleep phenotypes and migraine. Instrumental effects observed in two-sample MR were replicated in UKB (N cases=10,647 / Ncontrols =350,494), and we conducted sensitivity analyses to determine whether results were driven by horizontal pleiotropy, confounding, or reverse causality. Results: Difficulty getting up in the morning (odds ratio 1.37, 95% CI 1.12-1.68, p=0.002) and liability to insomnia (1.09, 1.02-1.16, p=0.015) increased migraine risk, and had independent effects in multivariable MR. The effects of both difficulty getting up (1.73, 1.67-1.80, p=8.77e-05) and insomnia (1.59, 1.29-1.97, p=0.04) on migraine replicated in UKB. Sensitivity analyses indicated minimal bias by horizontal pleiotropy, confounding, or reverse causality. In contrast, MR in the opposite direction did not support a consistent influence of migraine on sleep phenotypes, with significance only for a modest effect of increased napping (Beta for increased napping on ordinal scale = 0.01, 0.00-0.02, p=0.024).Conclusion: We provide support for causal effects of sleep disturbances on migraine, with minimal evidence of migraine influencing sleep phenotypes. These findings highlight the relevance of sleep health to migraine, and suggest that clinical intervention of sleep disorders may ameliorate migraine. Figure 1. Forest plot of two-sample inverse-variance weighted Mendelian randomization estimates for effect of sleep exposures on odds of migraine in IHGC GWAS (59,674 cases / 316,078 controls) 5864 Adam Dalia, MD, MBA, Anesthesia, Critical Care and Pain Medicine Outcomes in Outside Transfer versus In House Venoarterial-ECMO Patients in a 266 Patient Cohort at a Quaternary Referral Center A. Dalia, D. Lu, J. Ortoleva and G. Cudemus Anesthesiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Patients requiring Veno-Arterial Extracorporeal Membrane Oxygenation (VA-ECMO) are critically ill and require highly trained specialists and resources to be safely cared for. While the concept of an ECMO referral center is not new, there is a lack of data on the safety and clinical outcome of patients who are cannulated for VA-ECMO at outlying hospital and subsequently transferred and managed on a large referral ECMO center. If it could be deemed safe to cannulate these patients at outlying hospitals and transfer them to larger referral centers, it would allow this therapeutic modality to be utilized on a much wider scale in those presenting in critical conditionMethods: Retrospective chart review-based study in a single quaternary care center. All patients undergoing VA ECMO that are at least 18 years of age. One group of patients was cannulated at other institutions and sent to the quaternary referral center as transfers (transferred group), the other group comprised of patients already at the institution that were then cannulated in house (in-house group) Results: 266 patients were included in the retrospective study, 215 patients (81%) were cannulated in house and 51 patients (19%) were cannulated at an outside hospital and subsequently transferred to the quaternary referral center. 18 different outside hospitals transferred patient to the quaternary care center. The majority of patients were transferred to the institution within 0 or 1 days of cannulation (average of 0.5 days), only 2 patients were transferred after 3 days and 1 patient transferred after 2 days of ECMO care at an outside hospital. There was no difference in age between the patients transferred on ECMO and those cannulated in house (in house: 53.8 \u00b1 13.5 vs outside transfer: 53 \u00b1 17.4 years P = 0.731). The average VA ECMO run for in house patients was 5.6 \u00b1 5.5 days versus 6 \u00b1 7.1 days for outside hospital transfers (P= 0.747). Survival of the ECMO run for in house patients was 56.7% (122/215 patients) versus 60.8% (31/51 patients) for transferred group (p=0.58). Survival to discharge in patients cannulated in house was 38.1% (82/215 patients) versus 47.1% (24/51 patients) for transferred group (P = 0.23). Subsequent subset analysis within extracorporeal cardiopulmonary resuscitation (eCPR) group and non-eCPR group did not show statistical significance between in-house group and transferred group.Conclusion: Retrospective chart review of 266 patients suggests that there is no difference in survival of the ECMO run or survival to discharge in patients cannulated at other institutions and transferred versus those that were already in house and cannulated in house. This analysis has extensively been performed in patients with respiratory failure requiring VV ECMO but minimal investigation has been performed in patients requiring VA ECMO. These results suggest that having a quaternary care center that serves as a referral center for VA ECMO patients is a feasible patient care model. 5965 Adam Dalia, MD, MBA, Anesthesia, Critical Care and Pain Medicine Off Site Transesophageal Echocardiography requiring Cardiac Anesthesiology Care A. Dalia and K. Miller Anesthesiology, Massachusetts General Hospital, Boston, MA, USA Introduction: For transesophageal echocardiograms (TEE) requiring cardiac anesthesia support for moderate sedation and ventilatory support, coordination between anesthesia and cardiology is suboptimal leading to prolonged patient wait times, inefficient use of human resources, and potentially cancellation or delay of outpatient TEEs leading to patient dissatisfaction. On average, 9 TEEs occur in the echo lab on a weekly basis (350 a year), with approximately 30-50% of these requiring cardiac anesthesia due to complex medical needs of the patient (Sleep Apnea, substance abuse, difficult airway, inability to lay flat, severe pulmonary hypertension, etc.). The goal of our study was to decrease patient wait times and improve communi - cation between cardiology and cardiac anesthesiology to reduce inefficiencies and waste. Methods: The measures collected included: delay, in minutes, of patient receiving TEE with cardiac anesthesia, number of incorrectly ordered TEEs, number of cancelled or rescheduled patients. The echocardiography lab nurses recorded time of patient rooming to time of procedure start for each scheduled TEE case requiring cardiac anesthesia. Our intervention to improve these measures included daily screening of outpatient TEE orders to determine anesthetic needs. We designated certain days for Anesthesia cases: Monday, Thursday, and Friday. Lastly, we clearly identify who the daily cardiac anesthe-siologists would be on a calendar sent out to the echocardiography lab in order to improve communication between the echocardiography lab and the department of anesthesia. Results: After analyzing the data we identified 33% of TEE orders were ordered incorrectly; meaning were ordered with anesthesia when in fact the patient did not require anesthesia. There was a reduction in patient wait time by 20% down to an average wait time of 28 minutes. There was also an improvement in communication in advance with the cardiac anesthesi - ology division. Additionally, after the intervention there were 0 patients who required rescheduling or cancellation of their TEE. Conclusion: After developing a structured system for scheduling TEE's that require cardiac anesthesiology the delay in minutes for patients was reduced by 20%. The average wait time from patient arrival to induction of anesthesia was 28 minutes, reduced from pre-intervention of 35 minutes Importantly 0 patients were rescheduled or cancelled after the interven - tion. The communication between the division of cardiology and cardiac anesthesia improved around this structured system which in turn improves provider satisfaction. All these measures help improve patient safety, satisfaction, and quality of care. Screening out patients who do not need to be subjected to the risks of anesthesia, decreasing the wait time for patients who may require other procedures improves both the safety and quality of care these patients are receiving. The next steps include continued designation of days for TEE's requiring anesthesia and developing a centralized \"smart\" calendar for scheduling TEE cases with anesthesia. 66 Shu Dan, Orthopedics Contrary to Popular Belief, the Destination is More Important Than the Journey: Comparing the KOOS PASS and MCII 1 Year After Total Knee Arthroplasty S. Dan, N. Sauder, I. Florissi, Y. Colon Iban, A. Collins, D. Shin, A. Dupaguntla, V. Galea, H. Malchau and C. Bragdon Harris Orthopaedics Lab, Massachusettes General Hospital, Boston, MA, USA Introduction: The Knee Injury and Osteoarthritis Outcome Score (KOOS) is a Patient Reported Outcome Measure (PROM) used to evaluate the success of orthopedic procedures including Total Knee Arthroplasty (TKA). The KOOS measures different dimensions and sub scales, such as Pain and Function. To facilitate the interpretation of PROMs, the Patient 60Acceptable Symptom State (PASS), defined as the PROM score beyond which patients consider themselves well, and the Minimum Clinically Important Improvement (MCII), defined as the smallest change in measurement that signifies a meaningful improvement to patients, were developed. Although the PASS for the KOOS has been established, no studies have yet defined the MCII for this measure after TKA. The primary aim of this study was to derive the 1-year MCII thresholds for the KOOS Function and Pain sub scores after TKA. The study would also determine the proportion of patients achieving 1) both the PASS and the MCII; 2) only the PASS; 3) only the MCII; 4) neither. The final aim was to determine the distribu-tion of the preoperative KOOS scores in each group. Methods: To derive the MCII for KOOS Function and Pain, we used an anchor based method that entailed dichotomizing a 10-point satisfaction scale. Patients with scores 2.5 were considered satisfied and all others were considered unsatisfied. Receiver operating characteristic (ROC) curve analysis was used to determine the ability of 1-year improvement in KOOS Function or Pain to predict patient satisfaction. The MCII threshold was determined using the 80% specificity rule. This 1-year improvement ROC curve was plotted against the 1-year final score ROC curve, which had previously been used to define the KOOS PASS, and the Areas Under the Curve (AUC) were compared. This would assess whether 1-year KOOS improvement or 1-year KOOS absolute score may better predict satisfaction. We determined the proportion of patients achieved both the PASS and the MCII (Group 1), only the PASS (Group 2), only the MCII (Group3), neither (Group 4). For the final aim, Kruskall-Wallis H tests were used to determine how preoperative scores for KOOS Pain and Function varied among the four groups. Results: The MCII for KOOS Function was 38.0 and for KOOS Pain was 43.0. The AUC for the ROC curves were 0.754 and 0.787 respectively. In comparison, the AUC for the 1-year absolute score ROC curves that used to derive the PASS were 0.864 for Function and 0.858 for Pain (Figure 1). When analyzing the KOOS Pain score, 174 (44.3%) patients reached both the MCII and PASS (Group 1), 76 (19.3%) patients reached the PASS only (Group 2), 28 (7.1%) patients reached the MCII only (Group 3), and 115 (29.3%) patients reached neither the MCII nor the PASS (Group 4). When analyzing the KOOS Function score, 171 (43.5%) patients were in Group 1, 83 (21.1%) patients were in Group 2, 30 (7.6%) patients were in Group 3, and 109 (27.7%) patients were in Group 4 (Table 1). The mean preoperative KOOS Function score was 39.0 for Group 1, 68.2 for Group 2, 25.8 for Group 3, and 29.3 for Group 4. The mean preoperative KOOS Pain score was 35.2 for Group 1, 62.3 for Group 2, 26.2 for Group 3, and 41.7 for Group 4 (Figure 2). The preoperative KOOS Pain and Function scores differed significantly among the four groups (p < 0.001).Conclusion: We found that both the PASS and MCII are meaningful outcomes to patients and are predictive of overall satisfaction after TKA. The AUC comparisons suggested that, regardless of the cutoff used, 1-year absolute score and PASS achievement may be more meaningful to patients than 1-year improvement and MCII achievement. Less than half of the patients achieved an ideal outcome of reaching both the PASS and MCII. A similar study that analyzed the PASS and MCII interaction for THA patients found that a higher proportion of patients achieved both. Therefore, despite continuing surgical innovation, TKA remains a more challenging procedure with less ideal subjective patient outcomes. Patients who achieved the PASS only had the highest preoperative KOOS scores. They would have needed a smaller improvement to reach the PASS threshold and may not be able to improve by enough to achieve the MCII. Similarly, those who achieved MCII only started with the lowest preoperative KOOS scores, which may have predisposed them to achieving the MCII, but it may be unlikely for them to have improved enough to reach the PASS. In conclusion, MCII achievement may be less meaningful to TKA patients than 1-year absolute score or PASS achievement. Additionally, a high proportion of patients in contemporary TKA studies still reported unideal PROM outcomes. Finally, we analyzed how PASS and MCII achievement varied with patients' preoperative scores, which will be useful to clinicians when interpreting preoperative PROMs while undertaking TKA patient selection.61 67 Sudeshna Das, PhD, Neurology Metformin Reduces Risk of Dementia and Death in Diabetics: Novel Anti-Aging Neuronal Mechanism S. Das1, General Hospital, Cambridge, MA, USA, 2Biostatistics, HSPH, Boston, MA, USA, 3HMS, Boston, MA, USA and 4Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: Alzheimer's disease (AD)\u2014the most common type of dementia\u2014affects 5.7 million people in the US and costs about $250 billion annually. Since there are no disease modifying therapies to date, repurposing FDA-approved drugs that reduce the risk of developing dementia offers an expedited path to reduce the impact of this epidemic. Aging is the principal risk factor for AD, and metformin, an anti-diabetic drug, has demonstrated anti-aging properties in many experimental model systems. Here, we tested the hypothesis that metformin reduces the risk of AD in diabetic patients by conducting an \"in-silico drug trial\" of metformin vs another anti-diabetic drug class, the sulphonylureas, in a real-world patient population of diabetics selected from the RPDR, Partners electronic health records (EHR). Methods: In an intention-to-treat analysis, we followed patients aged 50+ from their first drug anti-diabetic drug exposure in the EHR. Anti-diabetic drug initiation was limited between 2007-2017 and follow-up times ranged from 1 to 12 years (median: 5). Dementia incidence was defined by the presence of either International Classification of Diseases (ICD) code for dementia or initiation of drugs predominantly used in AD (acetylcholinesterase inhibitors and memantine). After exclusion of cases with a dementia diagnosis prior to their first diabetes drug prescription, with polytherapy at baseline or prior use of hypoglycemics, the study cohort resulted in 11832 and 2437 patients on metformin and sulfonylureas monotherapy, respectively. A total of 1523 AD cases were observed in the sample (9.7% of metformin- vs. 15.6 % of sulfonylureas-users), with an onset age ranging from 56 to 113 years old (median: 76). We estimated the effect of metformin use, as compared to sulphonylurea, on dementia risk, with death as a competing event. We used a propensity scores model to emulate baseline randomization and to estimate the average treatment effects of metformin and sulphonylurea on the cumulative incidence of dementia and competing death. We computed the cumulative incidence using both a non-parametric survival and a Cox proportional hazards (PH) model and constructed confidence intervals using bootstrap resampling.62Results: Both a non-parametric survival model and a Cox proportional hazards (PH) model obtained similar results. We observed 5-years dementia risk of 9% (CI: 11% (CI: risk difference of 2% (CI: 0-4%) (Figure 1A). The risk of death by 5 years of use of Metformin (3%, CI: 2.5-3.5%) compared to 5 years of use of Sulphonylurea (5%, CI: 4-6%) was even more significant (Figure 1B) - this finding is consistent with the anti-aging effects of metformin, discovered previously in other studies. The analysis using the Cox PH model yielded similar results and showed a protective effect of metformin on the rate of dementia onset (HR=0.8 CI 0.7-0.88) and on rate of death (HR=0.57, CI 0.53-0.68). To explore actions of metformin and sulphonylureas directly on human neural cells, we measured gene-expression changes in differentiated humanReNcells, an immortalized human neural progenitor cell line, exposed to metformin, sulphonylurea or vehicle (DMSO). Relative to sulphonylurea and vehicle, metformin significantly reduced the expression of ribosomal protein and RNA genes in this mixed culture of neurons, glia, and oligodendrocytes. We hypothesize that reduced translation, which has been implicated as an anti-aging mechanism, is a potential mechanism of action for metformin in the human brain to reduce the incidence of AD. Conclusion: Our in-silico study presents clinically compelling effects of Metformin on dementia and the in-vitro results suggest novel mechanisms of the drug in the human brain. The observed effect on dementia risk, however, may be biased due to unobserved latent variables or other factors and the generalizability of the result to other population is still unknown. Thus, future work including validation in other EHR cohorts and assessment of this novel neuronal mechanism of action for metformin through CSF biomarkers and autopsied brains, is required. Our multi-disciplinary approach using data-science, statistics, epidemiology, neuroscience, neurology and system-pharmacology presents a novel approach towards identifying candidate drugs for repurposing to prevent dementia. 68 Hassan S. Dashti, Anesthesia, Critical Care and Pain Medicine Genetic basis of daytime napping and consequence on cardiometabolic health H.S. Dashti1,4,2, and R. Saxena1,4,2 1Center for Genomic Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Broad Institute, Cambridge, MA, USA, 3University of Murcia, Murcia, Spain and 4Department of Anesthesia, Critical Care and Pain Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Although daytime napping is a common, evolutionarily conserved behavior, its genetic basis is unknown. Moreover, napping has been related to cardiometabolic health, however causality is unclear. Elucidating the genetic basis of daytime napping may clarify relevant underlying biological pathways and determine causal links with disease. Methods: We performed a genome-wide association study of self-reported daytime 38.3% sometimes and 56.7% rarely/never nappers) using linear regression in participants of European ancestry in the UK Biobank and assessed robustness of our signals with wrist accelerometer-derived daytime inactivity duration (n 84,671). To assess genetic overlaps and possible clinical implications of napping, we conducted a phenome-wide association study in the Partners Biobank (n =30,683 with genetic data) using a genome-wide polygenic score (GPS) for napping, and Mendelian randomization (MR) cardiometabolic traits. Lastly, to deconstruct the napping genetic variants into possibly unique subgroups based on associations with other sleep traits, we applied a novel \"soft clustering\" Bayesian and tested their associ- with cardiometabolic health outcomes in the UK Biobank. Results: We identified 121 distinct genome-wide significant (P<5x10 -8) loci for daytime napping, with lead signals at or near genes KSR2 (kinase suppressor of ras 2), HCRTR1/HCRTR2 (hypocretin receptor 1/2), family transcrip - tional and MAPT (microtubule-associated protein tau), among others. SNP-based heritability for naps was 11.9%. The 121 loci further associated with accelerometer-derived daytime inactivity duration, where more frequent napping 63associated with ~20 minutes increase in daytime inactivity duration per unit increase in napping category (P =1.62e-11). Gene enrichment analyses pointed to pathways involved in neurogenesis, particularly in the frontal cortex, and others including nervous system development and opioid signaling. Genetic overlaps were evident in the Partners Biobank where highest decile of napping GPS associated with 1.3, 1.4, and 1.5 higher odds for essential hypertension, obesity, and nonalcoholic liver disease, respectively, compared to the lowest decile (all P <0.0001). In MR, potential causal links were identified between more frequent napping and higher diastolic blood pressure (2.67 mmHg per in systolic blood pressure (3.65mmHg, =6.40e-05), waist (0.28 SD units, 0.11-0.45, P=0.0015). The clustering of variants identified 3 robust clusters primarily driven by non-overlapping variants and sleep-related traits, suggesting possibly distinct naps-promoting mechanisms (cluster 1: \"higher sleep propen-sity\"; cluster 2: \"more fragmented/inefficient night sleep\"; cluster 3: \"early chronotype\"). Only clusters 2 and 3 PRSs were significantly associated with worse cardiometabolic health outcomes, including higher BMI, waist circumference, CRP, and triglycerides (all P<0.05). Conclusion: The present findings expand our understanding of the genetic architecture of naps implicating multiple biolog-ical pathways, indicating possible genetic overlap and causal links to cardiometabolic traits, and suggesting potentially 3 distinct nap-promoting mechanisms with differential associations with health outcomes. 69 Kathryn A. Davis, MA, Psychiatry Teeth as potential new tools to measure early life adversity and subsequent mental health risk: An interdisciplinary review and conceptual model Genomic General Hospital, Boston, MA, USA, 2Department of Orofacial Sciences, School of Dentistry, University of California, San Francisco, CA, USA, 3Center for Children's Oral Health Research, School of Dentistry, University of California, San Francisco, CA, USA, 4Forsyth Institute, Cambridge, MA, USA, 5Department of Developmental Biology, Harvard School of Dental Medicine, Boston, MA, USA, 6Department of Psychiatry, Harvard Medical School and the Massachusetts General Hospital, Boston, MA, USA and 7Henry and Allison McCance Center for Brain Health, Massachusetts General Hospital, Boston, MA, USA Introduction: Early life adversity affects nearly half of all youth in the United States, and is a known risk factor for psychi- atric disorders across the life course. One strategy to prevent mental illness may be to target interventions towards children who are exposed to adversity, particularly during \"sensitive periods\" when these adversities may have even more enduring effects. However, a major obstacle impeding progress in this area is the lack of tools to reliably and validly measure the existence and timing of early life adversity. In this review, we summarize empirical work across dentistry, anthropology, and archeology on human tooth development and discuss how teeth preserve a time-resolved record of our life experiences. Specifically, we articulate how teeth have been examined in these fields as biological fossils, in which the history of an individual's early life experiences is permanently imprinted; this area of research is related to but distinct from studies of oral health. We then integrate these insights with knowledge about the role of psychosocial adversity in shaping psychopathology risk to present a working conceptual model (Figure 1), which proposes that teeth may be an understudied yet suggestive new tool to identify individuals at risk for mental health problems following early life psychosocial stress exposure. We end by presenting a research agenda and discussion of future directions for rigorously testing this possibility, and with a call to action for interdisciplinary research to meet the urgent need for new biomarkers of adversity and psychiatric outcomes.Methods: See Introduction for full abstract.Results: See Introduction for full abstract.Conclusion: See Introduction for full abstract.64 70 Samantha DeAndrade, MD, MPH, OB/GYN Perineal breakdown after vaginal delivery: What next? S. Hudson1,3, J. Lee3,1, K. James1 and L. Berkowitz1,3 1Massachusetts General Hospital, Boston, MA, USA, 2Brigham & Women's Hosptial, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: Obstetric anal sphincter injuries (OASIS) are common and cause significant distress and morbidity to affected patients. At MGH, The objective of this study was to characterize the sequela, management and follow-up of patients who experience either OASIS or perineal breakdown in order to inform efforts to improve care. Methods: This was a retrospective cohort of women who delivered at one academic medical center between 2015 and 2018 and sustained a third or fourth degree perineal laceration after vaginal delivery and/or perineal wound breakdown.Results: Of the 7,230 lacerations sustained during the study period, there were 74 (1.02%) perineal breakdowns diagnosed, of which 69.9% (n = 51) were superficial and 31.1% (n = 23) demonstrated full thickness dehiscence. Forty-two percent (n = 31) were second degree lacerations, 48.6% (n = 36) were third degree lacerations and 9.5% (n=7) were fourth degree lacerations. The majority of cases were managed by either Generalist Ob/Gyn or MFM providers (77%, n = 57), while 15% (n = 11) were managed by Midwives and only 5.4% (n = 4) by FPMRS subspecialists. Fifty-one percent (n = 38) of perineal breakdowns were managed expectantly, 4.1% (n = 3) were able to be repaired in an outpatient setting, 18.9% (n = 14) required debridement and 39.2% (n = 29) required repair in the OR. Of those who were expectantly managed, 97.4% (n = 37) had breakdown of vaginal epithelium only. At a six-week follow-up visit, it was noted that 4.1% (n = 3) of patients had underwent pelvic floor physical therapy. The prevalence of pelvic pain was initially 8.1% (n = 6) at 6 weeks postpartum, decreasing to 2.7% (n = 2) after one year postpartum. The prevalence of urinary incontinence was 9.5% (n = 7), decreasing to 4.1% (n = 3) after one year postpartum. No patients developed a fistula. See Table 1 for details. Conclusion: Perineal laceration breakdown is an important complication of vaginal delivery and may predispose to multiple sequelae of pelvic floor dysfunction. There was a relatively low number of pelvic floor physical therapy referrals within the cohort and several patients continued to struggle with pelvic pain and urinary incontinence after one year postpartum. Even at a large academic medical center, FPMRS subspecialists represent a small portion of providers managing these complica - tions. This underscores the importance of ongoing training for all Ob/Gyn providers in the management of these lacerations. Future prospective studies could examine whether interventions such as pelvic floor physical therapy may help to improve outcomes after perineal breakdown.65Characteristics of Perineal Breakdowns 71 Linda M. Delahanty, MS, RD, Medicine - Endocrine-Diabetes One-year effectiveness and cost-effectiveness of lifestyle intervention for type 2 diabetes in primary care: The REAL HEALTH-Diabetes Randomized 1Massachusetts General Hospital (MGH) Diabetes Research Center, Diabetes Unit, and Department of Medicine, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Mongan Institute Health Policy Research Center, MGH, Boston, MA, USA, 4Division of General Internal Medicine, Massachusetts General Hospital, Boston, MA, USA, 5MGH Chelsea Health Center, Boston, MA, USA, 6MGH Charlestown Health Center, Boston, MA, USA and 7MGH Revere Health Center, Boston, MA, USA Introduction: Lifestyle intervention (LI)\u2014dietary modification and increased physical activity, coupled with behavior change\u2014improve medical outcomes in obesity and type 2 diabetes and are USPSTF grade B, implying that they should be widely available, yet are not currently available in usual care due to challenges with implementation in usual care settings. The REAL HEALTH-Diabetes trial aimed to adapt, implement, and compare the effectiveness and costs of two group LI programs, in-person (in-person LI) and telephone conference call (telephone LI), to medical nutrition therapy (MNT) on weight loss at 1 year.Methods: Randomized, assessor-blinded, practice-based clinical trial conducted in three community health centers and one hospital-based practice. Eligible participants were 211 patients with type 2 diabetes, HbA1c 6.5-<11.5, BMI >25 kg/m 2 (>23 kg/m2 in Asians) receiving primary care in a single health system. The interventions were practice-based, dietitian- delivered in-person or telephone group LI programs with medication management (25 sessions in the first year adapted from Look AHEAD's LI program) or facilitated referral to medical nutrition therapy with a registered dietitian, the current standard of care. The primary outcome was mean percent weight change at 6 months. Secondary outcomes included weight change at 12 months; 5% and 10% weight loss, change in HbA1c, and medication changes at 6 and 12 months; and cost per kilogram lost from the health system perspective. Results: At one year, 208 of 211 participants were retained (97%): 70 assigned to in-person LI, 72 to telephone LI, and 66 to MNT. Characteristics were: mean age 62 [SD 7.7 [SD 1.2]). Mean percent weight change at 6 and 12 months was -5.6% (SD 4.9) and -4.6% (SD 6.1) for in-person LI, -4.6% (SD 5.8) and -4.8% (SD 6.1) in telephone LI, and -1.1% (SD 3.7) and -2.0% (SD 4.1) in MNT, with statistically significant differences between each LI arm and MNT (P<0.001) but not between LI arms (P=0.63). HbA1c improved in all participants. Compared to MNT, the incremental cost per kilogram lost was $321 for in-person and $484 for telephone LI; this was sensitive to the degree of reduction in medication cost, with greater medication reduction being associated with a lower cost per kilogram. Conclusion: Lifestyle intervention can be successfully implemented in usual care settings and can achieve one-year weight loss outcomes in type 2 diabetes comparable to those achieved in clinical trials and greater than those achieved with costly medications used to promote weight loss. Outcomes were equivalent for in-person and telephone conference call arms, suggesting potential for scalability. REAL HEALTH-Diabetes has adapted a high-intensity evidence-based intervention 66for delivery in usual care, demonstrating feasibility, effectiveness, cost-effectiveness, and potential for scalability, thereby completing a crucial step in the implementation research cycle. 72 Wisteria Deng, BA, Psychiatry A Randomized Controlled Trial of a Campus-based Resilience Training Intervention for At-Risk College Students W. Deng1, A. Burke1, C. Cather1, C. M. Nyer1 and D. Holt1 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Emerson College, Boston, MA, USA, 3Boston University, Boston, MA, USA and 4University of California, Los Angeles, Los Angeles, CA, USA Introduction: Symptoms of serious mental illnesses typically begin during late adolescence and early adulthood, which overlaps the period of time many youth are attending college. A variety of stress-reduction and self-care curricula have been previously developed for the college student population, including mindfulness-based training. However, there is little empirical evidence (i.e., randomized controlled trials) supporting the efficacy of such treatments for bolstering resilience and reducing risk for illness. Thus we have conducted an RCT to test the efficacy of a 4-session resilience training curric-ulum for college students with subthreshold symptoms of psychopathology. In our previous, single-arm pilot study of this intervention (n = 60), significant decreases in symptoms of depression, anxiety and psychosis and significant increases in levels of self-compassion and self-efficacy occurred following the intervention. Thus, in the current study, we predicted we would observe similar effects in the active, compared to the control, participants. Methods: On-campus mental health screenings, involving the completion of self-report measures, were held at two local universities. Students endorsing mildly elevated levels of depressive and/or psychotic experiences, who were not receiving mental health treatment, were eligible to participate in the resilience training intervention and were randomly assigned to either begin the 4-week intervention immediately (Active group; n = 46) or in four weeks (Waitlist group; n = 50). The group intervention consists of both didactic and experiential exercises that teach mindfulness, mentalization and self-compassion skills. Self-reported symptom levels and resilience-promoting factors were measured in both groups at baseline and after four weeks.Results: Participants in the Active group demonstrated significantly greater improvements in measures of self- compassion and positive affect, as well as significantly greater reductions in depression, anxiety and psychotic experiences (all ps < 0.05), compared to those in the Waitlist group (significant group x time interactions, all ps < 0.05). Moreover, in a secondary analysis that included all participants who completed the resilience training intervention (n = 96), a reduction in subclinical paranoia was found to be significantly correlated with increases in self-efficacy and resilience following the intervention (ps < 0.05).Conclusion: These findings of an RCT of a campus-based resilience training intervention for at-risk college students provides initial evidence for the program's efficacy. Longitudinal follow-up assessments will determine whether these immediate benefits are maintained.6773 Libby A. DesRuisseaux, BS, Neurology Protocol for a Pilot Multiple Crossover, Randomized Block Sequence, Double-Blind, Placebo-Controlled Trial for Use of Methylphenidate for Cognitive and Behavioral Symptoms in Mild Cognitive Impairment and Dementia L.A. DesRuisseaux1, Arnold1 Research, Massachusetts General Hospit, Cambridge, MA, USA and 2Biostatistics, College of Public Global Health, New York University, New York, NY, USA Introduction: The pervading clinical trial design in the field of Alzheimer's Disease (AD) is the randomized, placebo- controlled parallel-group trial (PGT). However, these designs are costly to implement and often require exceedingly large and highly selective cohorts of patients to detect a meaningful effect. As an alternative, the \"N-of-1\" multi-crossover, randomized control trial (MCRCT) specifies that each patient serves as their own control across alternating blocks of active treatment and placebo and overcomes limitations of PGT trials by yielding an unbiased and personalized assessment of treatment efficacy - regardless of unique patient characteristics.Methods: Here we lay out a novel implementation platform for a \"N-of-1\" MCRCT to assess the efficacy of methylphenidate (MPH) to treat cognitive, behavioral, and functional symptoms of Alzheimer's Disease. Approximately 10 subjects with MCI or early AD or related disorders will undergo three treatment blocks in which they will receive two weeks of treatment with MPH and two weeks of placebo. Outcome measures will include standard, office-based clinical assessment, as well as unconventional, daily home-based measures that we hypothesize will represent a more sensitive and reliable measure of treatment efficacy. Results: This pilot study is currently being implemented in MGH Alzehimer's Clinical and Translational Research Unit, with an estimated enrollment end date of October 2019 and study completeion date of March 2020.Conclusion: Although the \"N-of-1\" MCRCTs are susceptible to disease progression during the trial, carryover effects of the intervention, and increased subject burden, we expect this trial design to efficiently evaluate treatment efficacy for symptom- atic interventions in AD and will importantly accelerate early drug discovery and repurposing without the associated burden of large PGTs. 74 Laura E. Dichtel, MD, MHS, Medicine - Endocrine-Neuroendocrine Effects of Ganaxolone on Depression Postmenopausal 1Internal Medicine / Neuroendocrine Unit, Massachusetts General Hospital, Boston, MA, USA and 2Depression Clinical and Research Program, Department of Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: The neuroactive steroid metabolite of progesterone, allopregnanolone, is a positive allosteric modulator of GABAA receptors and a putative treatment for mood disorders. We performed a pilot study to determine whether an oral allopregnanolone analog (ganaxolone) may be effective for treatment-resistant depression in postmenopausal women.Methods: Ten post-menopausal women (mean age 6.3 years, range 53-69) with treatment-resistant depression [current DSM-IV major depressive episode per the Structured Clinical Interview for DSM-IV (SCID), Montgomery-Asberg Depres-sion Rating Scale (MADRS), and treated with an adequately dosed antidepressant for 6 weeks] were administered open-label ganaxolone (225 mg BID, increased to 450 mg BID if tolerated) for 8 weeks, followed by a 2-week taper.68Results: Mean total MADRS score (primary endpoint) decreased by 8 weeks [24.4\u00b11.6 (SEM) to 12.8\u00b12.9, p=0.015] and persisted over the two-week taper (p=0.019) (Figure 1); 44% of subjects experienced response (score decrease 50%) and remission (final score <10), which persisted in 100% and 50% of subjects at 10 weeks, respectively. Secondary endpoints showed significant improvement, including the Inventory of Depressive Symptomatology-Self-Report (IDS-SR; p=0.003), MADRS Reduced Sleep subscale (p <0.001), Symptoms of Depression Questionnaire (SDQ) total score (p =0.012) (Figure 2), SDQ subscales for disruptions in sleep quality (p=0.003) (Figure 2), and changes in appetite and weight (p=0.009) over 8 weeks. No significant effects were observed on quality-of-life or sexual function. All subjects experienced sleepiness and fatigue; 60% experienced dizziness. Conclusion: In this open-label, uncontrolled pilot study, ganaxolone appears to exert antidepressant effects but produces sedation with twice-daily dosing. Ganaxolone may also improve sleep, which may be useful in patients with depression and insomnia. Given the sedation experienced by a majority of participants and promising durability of antidepressant effects over the two-week period, bedtime dosing only should be considered for future randomized, placebo-controlled studies. Should rigorous studies confirm an antidepressant effect, it will be important to identify subsets of women who respond - for example, women with neuroactive steroid dysregulation - and to identify specific mechanisms of action, including regional brain targets. Figure 1. A) There was a reduction in depression symptom severity, as measured by mean Total MADRS score, that remained reduced through a 2-week taper period. B) There was an improvement in sleep quality as demonstrated by a reduction in the mean MADRS Reduced Sleep Subscale score, that remained reduced through a 2-week taper period. *Indicates significant change compared to baseline with p-values noted. Error bars indicate SEM. Figure 2. There were improvements in the SDQ Total Score (Panel A) and multiple SDQ Factors (Panels B-F) during the treatment period (Baseline-Week 8), some of which showed durable improvement after a 2-week drug taper (Week 10) and at three months off of drug treatment (Week 22). *Indicates significant difference and indicates trend in difference compared to baseline value. Error bars indicate SEM.6975 Laura E. Dichtel, MD, MHS, Medicine Unit, Endocrine Division, Department of Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Butler Hospital and the Warren Alpert Medical School of Brown University, Providence, RI, USA, 3Depression Clinical and Research Program, Department of Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 4Department of Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 5Department of Medicine, Harvard Medical School, Boston, MA, USA, 6Department of Biostatistics, Harvard School of Public Health, Boston, MA, USA, 7Department of Psychiatry, Beth Israel Deaconess Medical Center, Boston, MA, USA and 8Mayo Clinic Endocrine Laboratory, Rochester, MN, USA Introduction: Low-dose testosterone has been shown to improve depression symptom severity, fatigue and sexual function in small studies in women not formally diagnosed with major depressive disorder (MDD). We sought to determine whether adjunctive low-dose transdermal testosterone improves depression symptom severity, fatigue, and sexual function in women with treatment-resistant MDD. A functional MRI (fMRI) substudy examined effects on activity in the anterior cingulate cortex (ACC), a brain region important in in 101 women, ages 21-70, with treatment-resistant MDD. Primary outcome measure was depression symptom severity by Montgomery-Asberg Depression Rating Scale (MADRS). Secondary endpoints included fatigue, sexual function, and safety measures. fMRI substudy (n=20) primary outcome was change in ACC activity. Results: Mean age was 47\u00b114 (SD) years and mean baseline MADRS score was 26.6\u00b15.9. Eighty-seven (86%) participants completed 8 weeks of treatment. Mean MADRS scores decreased in both arms [testosterone: 26.8\u00b16.3 to 15.3\u00b19.6; placebo: 26.3\u00b15.4 to 14.4\u00b19.3 (baseline to 8 weeks, respectively)], with no difference between groups (p=0.91) (Figure 1). Improve- ment in fatigue and sexual function did not differ between groups, nor did side effects. fMRI showed a negative relationship between ACC activation and androgen levels pretreatment, but no difference in ACC activation with testosterone vs placebo.Conclusion: Adjunctive transdermal testosterone, though well tolerated, was not more effective than placebo in improving depression, fatigue or sexual dysfunction symptom severity. Imaging in a subset of participants demonstrated that testosterone did not result in greater activation of the ACC. Figure 1: Depression symptom severity over time represented by mean MADRS score in the testosterone (solid black circle, solid black line) and placebo (open diamond, dotted line) groups by study visit. There was no significant difference between the testosterone and placebo groups at any visit.7076 John Donlan, BS, Medicine - Gastroenterology Knowledge and Perceptions of Palliative Care for Patients with End-Stage Liver Disease: A Qualitative Interview Study of Patients and Caregivers J. Donlan1, E. Walsh3, R. Chung1, A. El-Jawahri2 and N. Ufere1 1Gastroenterology, Massachusetts General Hospital, Boston, MA, USA, 2Medical Oncology, Massachusetts General Hospital, Boston, MA, USA and 3Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: Despite evidence demonstrating the benefit of specialty palliative care (PC) for patients with chronic illnesses, patients with end-stage liver disease (ESLD) rarely receive PC referrals. Data suggest that negative patient and caregiver perceptions of PC are a barrier to referral. We sought to explore knowledge and perceptions of PC by patients with ESLD and their caregivers and explore their supportive care needs. Methods: We are conducting an ongoing qualitative study and used purposeful sampling to identify and recruit transplant- eligible (n = 5) and transplant-ineligible (n = 5) patient-caregiver dyads from a single liver transplant center. We conducted semi-structured interviews with patients (n = 10) and caregivers (n = 10) independently. We explored participants' knowledge and perceptions of PC as well as their supportive care needs. Data analysis, including open coding and categorization, was done by two independent raters for inductive thematic analysis. Results: Patients had a mean age of 58 (range: 29-66) and 30% were male with a mean MELD-Na score of 16.3 (range: 9-24). 50% had alcoholic cirrhosis, 30% had NASH cirrhosis, 100% had a history of ascites, and 90% had a history of hepatic encephalopathy. Caregivers (n = 10) were 70% male and 80% were spouses. Three main themes emerged: 1) Many patients and caregivers had minimal knowledge of PC and perceived PC as synonymous with end-of-life care; 2) Most patients and caregivers had a more positive attitude regarding PC after being informed of the role of PC; and 3) All patients and caregivers believed patients with ESLD should learn about PC early after their diagnosis. All three themes were consistent among patients and caregivers and regardless of transplant eligibility. Patients and caregivers identified multiple supportive care needs including the need for emotional support, financial support, and desire for a better understanding of the illness and prognosis. Conclusion: Participants had minimal knowledge of PC and often perceived it as end-of-life care. Most patients and caregivers had a more positive attitude regarding PC after learning about its role, and all participants, regardless of transplant eligibility, believed patients with ESLD should learn about PC early in the course of their illness. These data underscore the need to better educate patients and caregivers about the potential role of PC for improving their quality of life and care. 7177 Stephen C. Dorner, MD, Emergency Perceptions of Urgent Care Experiences of Mobile Integrated Health/Community Paramedic Patients Compared with Emergency Department Patients S.C. Dorner1,3,4, A.J. Wint2 and L. Iezzoni2 1Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Mongan Institute, Massachusetts General Hospital, Boston, MA, USA, 3Emergency Medicine, Brigham and Women's Hospital, Boston, MA, USA and 4Emergency Medicine, Harvard Medical School, Boston, MA, USA Introduction: The Acute Community Care Program (ACCP) is a pilot program in which specially-trained community paramedics, in constant communication with physicians, provide in-home urgent care with the goal of preventing emergency department (ED) visits and improving patients' healthcare experiences. We surveyed patients receiving ACCP care and patients with urgent medical needs who received standard ED services. Our objective was to describe and compare percep-tions of ACCP care with ED care for urgent health problems.Methods: ACCP is a collaboration between a non-profit managed care program serving socioeconomically disadvantaged populations in Massachusetts and a local ambulance provider employing specially trained paramedics. All individuals surveyed were dually-eligible for both Medicare and Medicaid and at least 21 years old. We received survey responses from 206 ACCP visits and 718 ED visits. We conducted individual, open-ended interviews with ACCP patients to inform design of our survey instrument. The survey underwent cognitive interviews and pilot testing. To limit respondent burden by minimizing skip patterns, we created separate surveys for both the ACCP and ED experiences. For the ACCP, we sought a census sample surveying all persons receiving ACCP care. To reduce respondent burden, we interviewed individual patients about no more than two ACCP visits. We used the commercial PatientPing software to identify plan members who visited the ED and subsequently screened those visits to identify only urgent and exclude emergent visits. We aimed to survey 600 ED patients.Results: ACCP patients were significantly older than patients seen in the ED, with mean (SD) age of 66 (17) versus 58 (14) years (p < 0.0001). ACCP patients were also significantly more likely to be African-American (29.1% versus 12.0%, p < 0.0001). ACCP patients were significantly more likely to report that decisions made about their care were \"definitely right\" (66.1% versus 55.6%, p = 0.02). Significantly more (67.2%) ACCP patients would prefer future in-home urgent care by ACCP paramedics than patients receiving ED care (35.9%, p < 0.0001). Conclusion: Socioeconomically disadvantaged patients with complex health needs appear satisfied receiving paramedic- delivered urgent care treatments in their homes rather than going to EDs. Nearly 70% of ACCP patients reported they would prefer future urgent care in the home with community-based paramedics, should the need arise. Interestingly, 36% of patients who had never experienced ACCP care expressed a similar preference. These findings suggest a promising urgent care alternative that would be acceptable to complex, high-cost patients, although future research must determine whether longer-term clinical outcomes between an ACCP-type program and standard ED care are comparable.72 Table 1: Demographic Characteristics of Survey Respondents by ACCP or ED Treatment Table 2: Patient Follow Up and Preferences 78 Konstantinos A. Douglas, MD, DVM, MBA, Massachusetts Eye and Ear (MEEI) - Ophthalmology Scleral Buckling with J.B. 1Ophthalmology, Mass Eye and Ear, Boston, MA, USA, 2Joslin Diabetes Center, Boston, MA, USA and 3Florida Retina Institute, FL, USA Introduction: Scleral buckling (SB) with an exoplant was pioneered separately and with variations by Custodis and Dr. Schepens teams in late '40s and 50s. Prior to pars plana vitrectomy (PPV), SB was the gold standard for of rhegmatog - enous retinal detachment (RRD). While PPV is now the surgical treatment of choice for RRD in patients with a posterior vitreous detachment (PVD), SB remains the primary approach in young patients without a PVD. Primary SB procedures can be difficult to teach compared to PPV in teaching fellowship programs with parts of the surgery (such as indirect ophthal - moscopy during cryoretinopexy) requiring viewing by a single examiner at a time. Here we present the use of the Alcon NGENUITY\u00ae 3D Visualization System to perform SB, which facilitates teaching the SB technique to trainees. Methods: We reviewed 272 consecutive cases performed on the NGENUITY\u00ae 3D Visualization System (Alcon Forth Worth, TX) by two vitreoretinal surgeons and identified 12 cases that had undergone SB for RRD. We then excluded any cases that underwent a concurrent PPV to focus on primary scleral buckling.Results: Seven cases of primary SB were performed on the Alcon NGENUITY\u00ae 3D Visualization System. Cryoretinopexy was successfully performed under direct visualization with endoillumination of all retinal breaks and lattice degeneration. External drainage of subretinal fluid was performed with either a scleral cut down or aspiration with a subretinal 30g needle under direct visualization on the NGENUITY\u00ae 3D Visualization System. Successful retinal reattachment was achieved in all cases without complication.Conclusion: NGENUITY\u00ae 3D Visualization System offers a valuable teaching platform for primary scleral buckling with cryoretinopexy and subretinal drainage. With wide screen 3D digital system, the entire surgical team including the surgeon, trainees, and nursing staff can observe the surgical techniques and effectiveness. This is particularly important for cryoreti - nopexy which currently cannot be simultaneously be observed by trainee and expert surgeon. The 3D digital viewing also eliminates the need for indirect ophthalmoscopy, reducing total surgical time and the potential to improve patient outcomes.7379 Vivian Paraskevi Douglas, MD, DVM, MBA, Massachusetts Eye and Ear Infirmary (MEEI) - Ophthalmology Success with a Variety of Surgical Maneuvers with Heads-Up Display (NGENUITY\u00ae 3D Visualization System) V. Douglas1, K.A. Miller1, A. Marmalidou1 and S. Houston III2 1Ophthalmology, Mass Eye and Ear, Boston, MA, USA and 2Florida Retina Institute, Florida, FL, USA Introduction: Over the past few years, there has been a significant increase in the medical application of three-dimensional imaging system technology (3D) and heads-up adjustable output display in many medical disciplines including neurosurgery, urology, gastroenterology, gynecology, and ophthalmology. While there are limited publications regarding its use in vitreo - retinal surgery, the generally accepted main benefits include improved ergonomics, enhanced surgical team communication, reduced retinal phototoxicity, increased depth of field, and display image manipulation. Despite these possible benefits, many retina specialists have questioned its universal applicability to a wide variety of vitreoretinal surgeries. Herein, we review the early surgical experience using the Alcon NGENUITY\u00ae 3D Visualization System at two busy vitreoretinal practices in both the academic and community settings. We hope to illustrate the diverse capabilities of this powerful surgical visualization tool in a variety of surgical retina scenarios. Methods: A retrospective review was conducted of consecutive surgical cases performed on NGENUITY\u00ae 3D Visualiza - tion System at Massachusetts Eye and Ear Infirmary and Florida Retina Institute from June 2017 to November 2018. Age, presenting diagnosis, surgical procedure, and intraoperative details were recorded. Results: We performed 272 vitreoretinal surgeries on Ngenuity Heads Up 3D Surgical Display, including 71 rhegmatog - enous retinal detachments membrane peels (MP), and 24 intraocular lens (IOL) maneuvers, as well as 23 oil removals and 22 vitreous floaters, 3 removals (RLF), 1 scleral buckle (SB) removal, 1 endophthalmitis and 1 acute retinal necrosis case. We found no limitations in the type of cases that could be completed with the system. One intraoperative iatrogenic small peripheral retinal break was noted during peripheral shaving of the supero-nasal quadrant during an RRD repair. Familiarity with the viewing system may have contributed, but focal laser pexy was performed and ultimately the patient did very well without recurrent detachment or other negative outcome. Conclusion: Despite general concerns about visualization with the Ngenuity system, we report the diverse application of this viewing system to a variety of surgical retina cases without limitation allowing for safer manipulations and increasing the surgeon's comfort and freedom of movements. 80 David M. Dudzinski, MD, Medicine - Cardiology Combination Of Right Ventricular Function And Pulmonary Pressure Improves Prediction Of Adverse Outcome In Acute Pulmonary Embolism M.D. Lyhne2,1,3, C. Kabrhel1, and Dudzinski2 Medicine, Massachusetts General Hospital, Boston, Denmark, 2Department of Cardiology, Massachusetts General Hospital, Boston, MA, USA and 3Department of Cardiology, Aarhus University Hospital, Aarhus N, Denmark Introduction: Right ventricular (RV) failure causes death in acute pulmonary embolism (PE); thus, RV function is central to risk stratification in PE. RV failure is a consequence of mismatch between RV function and afterload. We derived an echocardiographic index of this mismatch: systolic function by tricuspid by pulmonary arterial systolic pressure (PASP). We assessed the hypothesis, that TAPSE/PASP in PE patients would predict outcome better than the two measurements separately.Methods: Patients were from the pulmonary embolism response team (PERT) registry at Massachusetts General Hospitak from 2012-2019 with confirmed PE and a formal echocardiogram performed within 3 days. Primary endpoint was a composite of 7-day mortality or clinical deterioration (systemic hypotension or need for systemic thrombolysis, catheter- directed intervention, surgical embolectomy, ECMO, mechanical ventilation, or vasopressor). Secondary outcomes were 7- and 30-day all-cause mortality.Results: A total of 606 patients were included; 129 met the primary endpoint. TAPSE/PASP predicted outcome significantly better than TAPSE or PASP individually (Figure). TAPSE/PASP independently predicted the primary composite outcome with OR 0.026 did.74Conclusion: In conclusion, TAPSE/PASP is a stronger predictor of clinical deterioration and mortality in acute PE compared to TAPSE and PASP alone. This easily obtainable measurement may be useful in optimizing risk stratification of patients with acute PE. 81 Michelle Dundek, Bachelor of Engineering, Emergency Ultra-low-cost, high quality bubble CPAP for low resource settings M. Dundek1,2, K. Mollazadeh-Moghaddam1,2, A. Bellare2, T. Burke1,3,2, R. Sharma2, N. Houston2 and S. Rushforth2 1Division of Global Health Innovation, Department of Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Vayu Global Health Innovations, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: Bubble Continuous Positive Airway Pressure (bCPAP) is a life-saving intervention for children and newborns with respiratory distress. High cost, use of pressurized air, and need for uninterrupted electricity are barriers to scale in low- resource settings. We aimed to create an ultra-low-cost, high quality bCPAP system that overcomes these barriers. Methods: When possible, we sourced existing, low cost components such as tubing and binasal prongs. The critical component, an air/oxygen blender, uses a high velocity, low pressure oxygen jet to entrain ambient air and output a mixture with a safe oxygen concentration. It was prototyped and optimized with high resolution 3D printing.Results: The complete bCPAP device provides pressures and imposed work of breathing comparable to the gold standard device. The simple plastic blender eliminates need for electricity or pressurized air and reduces the need for active humidi - fication. It outputs an adjustable mixture of 30- 100% oxygen with minimal waste.Conclusion: We present a bCPAP device at < 1% the cost of the gold standard device that breaks down barriers to scale without sacrificing quality. With thoughtful implementation, this device can bring lifesaving therapy to children and newborns all over the world. 82 Samantha Dunn, BA, Radiation Oncology Combination Radiation Therapy and Immune Checkpoint Blockade Therapy for Breast Cancer: The Current and Future Clinical Trial Landscape S. Dunn, S.Y. Tabrizi and A.Y. Ho Radiation Oncology, Massachusetts General Hospital, Boston, MA, USA Introduction: In the United States, approximately 295,290 new breast cancer cases are estimated in 2019 (Siegel et al., 2019). High risk patients, including those with metastatic breast cancer (MBC), triple negative breast cancer (TNBC), and high-risk HR+/HER2- BC, have a poor prognosis and limited therapeutic options. Immune checkpoint blockade (ICB) is an emerging therapy in BC. Early trials investigating ICB monotherapy demonstrate a modest response, limited by low baseline immunogenicity of breast cancers(Emens et al., 2019; Nanda et al., 2016; Rugo et al., 2018; Sobral-Leite et al., 2018). Due to immunostimulatory effects, radiation therapy (RT) has been combined with ICB and demonstrates encouraging results 75(Demaria et al., 2005; Dewan et al., 2014; 2019; et al., 2015). RT's optimal integration with ICB for BC remains to be established (i.e. dose, fractionation, and timing). This review aims to examine current clinical trials combining RT and ICB therapy and discuss future directions for the use of this treatment combination. Methods: Clinical trials involving RT and ICB (targeting PD-1, PD-L1, and/or CTLA-4) in breast cancer were queried using ClinicalTrials.gov. Details on the type of immunotherapy, radiation regimen, breast cancer subtype, sponsoring centers, study design, target accruals and recruiting status were tabulated Results: Twenty-nine clinical trials, evaluating 3933 patients, have been registered on ClinicalTrials.gov since August 2011. Four are completed, 15 are currently recruiting and 10 are not yet recruiting. Prior to 2017, 11 trials investigated RT and ICB combination therapy primarily in TNBC and MBC patient populations, with only 2 trials in the non-metastatic setting. As of July 2019, there are 11 (38%) in the non-metastatic setting and 18 (62%) in the metastatic setting. The first trial investi - gating HR+/HER2- patients opened January 2017. There are now 6 (21%) HR+/HER2- BC trials. In the metastatic setting, 13 include only BC patients, while 5 include patients with other tumor types. Characteristics of the clinical trials are further described in Figure 1.Conclusion: The number of clinical trials investigating RT and ICB combination therapy in BC tripled in the last 3 years, with the largest increase in non-metastatic trials. TNBC and MBC continue to be target populations. Within the last 3 years, there has been a dramatic increase in trials including HR+/HER2- BC patients as well as BC patients with brain metastases. These trends highlight combination RT/ICB therapy as a promising strategy in select subtypes of breast cancer at high risk for relapse. The current registered clinical trials aim to evaluate efficacy and optimal integration of RT and ICB combination therapy. 83 Anup Dupaguntla, B.A., Orthopedics Use of the Harris Hip Score PASS and MCII in Conjunction to Analyze Postoperative Outcomes of Patients Undergoing Total Hip Arthroplasty A. Bragdon Orthopaedics Laboratory, Massachusetts General Hospital, Boston, MA, USA Introduction: The Harris Hip Score (HHS) is a Patient-reported Outcome Measure (PROM) that quantifies a patient's subjective level of hip pain and function. Metrics such as the Patient Acceptable Symptom State (PASS) and the Minimal Clinically Important Improvement (MCII) have been defined to interpret PROM scores. The PASS is the value on a PROM scale beyond which the average patient considers themselves to be satisfied, while the MCII is the magnitude of improvement between pre- and post-operative PROM values that the average patient considers satisfactory. In simple terms, the PASS refers to \"feeling good\", while the MCII refers to \"feeling better\". Though both the MCII and the PASS have been defined for the HHS at several postoperative timepoints following total hip arthroplasty (THA), to our knowledge, no study has analyzed both interpretability metrics in conjunction. The primary aim of this analysis was to determine the proportion of patients that achieved the PASS, MCII, both, or neither 3 years after THA. The secondary aim of this study was to analyze how preoperative HHS values affect the chances of achieving the PASS, MCII, both, or neither. The final aim of this analysis was to identify demographic variables that might influence achieving at least one of these thresholds.Methods: Data used for this retrospective analysis was derived from a prospective, international study analyzing the outcomes of Vitamin E Polyethylene liners in THA. Demographic variables were collected preoperatively. The HHS was 76collected preoperatively and at 1, and 3 years postoperatively. Satisfaction was assessed by a 21-point numerical rating scale for satisfaction for each postoperative timepoint. 977 patients who had a preoperative HHS were enrolled in this IRB- approved study. 726 patients from 14 centers had preoperative and 1-year postoperative PROMs, and 592 patients also had 3-year postoperative PROMs. Existing literature has defined values for 2-year MCII to be 18.0 points for the HHS. Given that postoperative PROMs were collected at the 1- and 3-year in the present study, the 2-year MCII was applied to evaluate 3-year PROM scores preferentially, and 1-year values for patients with missing 3-year PROMs. Previously defined 1- and 3-year PASS values of 89 and 93 points, respectively were used. Patients were classified into 4 subgroups: (1) achieving both the MCII and the PASS, (2) achieving the MCII but not the PASS, (3) achieving the PASS but not the MCII, (4) achieving neither the PASS nor the MCII. To evaluate aim 3, patients in subgroups 1, 2, and 3 were aggregated and evaluated against patients in Group 4 to identify any demographic variables contributing to achieving either the PASS, MCII, or both metrics. A forward conditional binary logistic regression was performed to determine an association between preoperative demographic variables and achieving at least one of the PASS or MCII. Demographic variables included age, sex, body mass index, joint space width, preoperative EuroQol-5 Dimension, and the Mental Component Score and Physical Component Score of the SF-36 survey.Results: 430 patients (59.2%) reached both the PASS and the MCII (Group 1). 208 (28.7%) patients reached the MCII but not the PASS (Group 2). 24 (3.3%) patients reached the PASS but not the MCII (Group 3). 64 (8.82%) patients reached neither the PASS nor the MCII (Group 4). Figure 1 displays the distribution of these scores in each group. The average preoperative HHS was 52.34 (95% CI: 51.03-53.66) (39.30-42.90) 86.29 (84.34-88.24) for Group 3, and 61.38 (56.91-65.84) for Group 4. There was no significant difference in any of the measured demographic variables between the patients who achieved neither the PASS nor the MCII and those who achieved at least one of these thresholds (p > 0.116).Conclusion: The PASS and the MCII have great potential to facilitate PROM interpretation by providing an understanding of what scores constitute a successful operation, as determined by patients themselves. To our knowledge, this study is the first to evaluate these metrics in conjunction. We found that patients starting with a high preoperative HHS have less room for improve-ment and are less likely to achieve the MCII, despite achieving the PASS. In addition, patients starting with lower preoperative HHS are more likely to achieve just the MCII rather than both the MCII and the PASS. Our finding that none of the tested demographic variables were associated with achieving the PASS, MCII, or both suggests that analyses using the MCII and PASS in conjunction may not need to control for these demographic variables. This differs from previous studies concluding that the PASS and the MCII, when analyzed individually, are significantly affected by demographic variables. These data highlight that the individual limitations of the PASS and the MCII can be mitigated by using these interpretability metrics together. Figure 1. Distribution of preoperative HHS scores by patient subcohort.7784 Sienna M. Durbin, Cancer Center Clinical outcomes of hospitalized patients with stage IV cancer receiving immune checkpoint inhibitors S.M. Durbin1, L. Zubiri1,2, Reynolds1,2 1Harvard Medical School, Boston, MA, USA, 2Cancer Center, Massachusetts General Hospital, Boston, MA, USA, 3Division of Palliative Care and Geriatric Medicine, Massachusetts General Hospital, Boston, MA, USA and 4Pharmacy Department, Massachusetts General Hospital, Boston, MA, USA Introduction: Immune checkpoint inhibitors (ICI) represent a major leap in the treatment of many cancers. Use has rapidly expanded in recent years, yet it is unknown whether hospitalized patients, who often have lower performance status than those who were studied in clinical trials, derive benefit and incur the same risks. The primary objectives of this study were to characterize the clinical features and outcomes of inpatients receiving ICI at a single institution, and to identify predictors of post-discharge survival after receipt of ICI.Methods: After IRB approval, we conducted a retrospective chart review of patients with Stage IV solid tumors receiving ICI while admitted to Massachusetts General Hospital between January 2015 - October 2018. Patients included those receiving ICI monotherapy, combination therapy, and those receiving ICI with chemotherapy. Patients receiving ICI as part of a clinical trial were excluded. We examined performance status (PS) at the time of admission, Charlson Comorbidity Index (CCI) score, cancer type, and outcomes of length of stay, intensive care unit (ICU) use, readmission within 30 days, and documented immune-related adverse event (irAE). We estimated progression-free and overall survival by the Kaplan-Meier method and used Cox multivariable regression to identify predictors of post-discharge survival. Results: A total of 103 patients with Stage IV solid tumors were treated with ICI as inpatients between January 2015 - October 2018. Median age was 58 years; 57% were male; 27% had ECOG performance status Charlson Index score was 8.3. Melanoma (35%) and lung (22%) were most common cancer types. Seventy-six percent began ICI as an inpatient and 24% received ICI as continuation of outpatient therapy. Eighteen percent experienced an irAE at some point following drug administration, most commonly colitis (21%) and pneumonitis (16%). Average length of stay was 17.2 days and 15% of patients had an ICU stay during the admission. The 30 day readmission rate was 41%. The median post-discharge survival was 31 days; 11% survived more than 6 months; 23% died during the admission. Factors predictive of shorter post-discharge survival were PS of 3-4 relative to PS 0-2 (HR 2.0, p < 0.004), lung cancer (HR 2.0, p = 0.02) and other tumor types, including head and neck cancer and GI malignancies, (HR 2.1, p = 0.004) relative to melanoma. Conclusion: While the majority of inpatients receiving ICI died during admission or within 30 days of discharge, a subset of patients with stage IV disease were alive at 6 months. Rates of irAEs are comparable to those seen in the clinical trial population. Tumor type and ECOG PS predict post-discharge survival and may be used to identify inpatients more likely to benefit from ICI. These novel findings from a single institution require additional validation. 85 Kristina Dzara, Ph.D., Health Professionals Education Research Medical Education Journal Club for the Millennial Resident: An Interactive, No Prep Approach K. Dzara1,2,3 and A. Frey-Vogel1,3 1Pediatrics, Massachusetts General Hospital, Boston, MA, USA, 2Obstetrics and Gynecology, Massachusetts General Hospital, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: The traditional journal club format consisting of participants reviewing an article followed by group discussion may be misaligned with millennial learners' needs and may not rely on best principles of adult learning. Our objective was to deliver an interactive medical education research journal club allowing pediatric residents to critically engage with medical education research without prior preparation and measure the perceived impact of our educational intervention. Methods: We conducted 4 one-hour 'Interactive, No Prep' medical education journal clubs for residents in the MGH pediatrics residency program in 2018. Without prior reading, participants developed methods to answer the study question, compared that to actual methods, analyzed the results, and extrapolated the findings. Each journal club explored a purpose-fully selected article with quantitative, qualitative, or mixed methodology chosen by the session leaders fitting fit four criteria aligning with adult learning theory: 1) relevant to pediatrics residents' real-world training experience (fits with learners' prior experience); 2) focused on graduate medical education (aligns with learners' self-concept); 3) well-described research methodologies (encourages learners' need to know about research methods for research projects) and 4) published within the past three months (supports learners' motivation to review new literature). We developed a simple, anonymous evaluation tool to determine perceived educational impact. Mixed-methods analysis was conducted. Quantitative data were 78analyzed for descriptive statistics including mean, mode, and interquartile range (IQR) and open-ended, qualitative data were analyzed using the 'five stages to qualitative research framework.' The educational evaluation was designated 'clinical quality improvement/measurement' by the Partners Human Research Committee, requiring no additional institutional board review. Results: 52/59 participants (88% response rate) indicated on a 7-point scale that the journal club helped them think about how to analyze a paper (mean=5.32; SD=1.11), use a paper to inform further study questions (mean=5.42; SD=1.24), and understand medical education research (mean=6.00; SD=1.01). Four qualitative themes indicated the following: 1) The journal club offered a strong interactive learning experience; 2) Respondents valued that no advanced preparation was required; 3) The journal club could be improved; and 4) The journal club spurred a desire for further learning.Conclusion: Our journal club approach utilizing active learning principles and requiring no advance preparation is proof of concept that faculty needs to teach critical literature evaluation and millennial needs for engagement can be simultaneously met. Findings are published in Academic Pediatrics. We offer a practical solution to a challenge which residency programs throughout Massachusetts General Hospital may experience. Implementation in other specialties is encouraged. We will pilot this journal club in the Obstetrics and Gynecology residency program and evaluation data will be collected to offer a comparison between disciplines. Quantitative and Qualitative Educational Evaluation Results Journal Club Implementation Flowchart7986 Carly Eiduson, B.A., Anesthesia, Critical Care and Pain Medicine A Multicenter, Randomized, Active- Controlled Study to Evaluate the Efficacy and Safety of Liposomal Bupivacaine When Administered via Infiltration into the Transversus Abdominis Plane versus Standard of Care in Subjects Undergoing Elective Cesarean Section C. Eiduson 1,2, X. Bao1, A. Chalupka1, and S. Nedljkovic2 1Department of Anesthesia, Critical Care, and Pain Medicine, Massachusetts General Hospital, Boston, MA, USA and 2Department of Anesthesiology, Perioperative and Pain Medicine, Brigham and Women's Hospital, Boston, MA, USA Introduction: Cesarean delivery (CD) accounts for almost one third of births in the United States. Neuraxial blockade is the preferred anesthetic for surgery. For spinal anesthesia, the standard of care (SOC) includes intrathecal (IT) injection of a local anesthetic (LA) and opioid. A common regimen includes intrathecal this small dose of intrathecal morphine may provide up to 24 hours of postoperative analgesia, patients often rely on additional analgesics, including opioid pills (oxycodone) to control incisional pain after surgery. Opioids have well-characterized adverse effects such as nausea, vomiting, pruritus, and possible respiratory depression. Other concerns include the potential for misuse and addiction and the transfer of opioids into the breast milk. The primary aim of this study is to compare opioid consumption in the 72-hour postoperative period following the infiltration of liposomal bupivacaine (LB) into the transversus abdominis plane (TAP) vs SOC management with IT opioids and no LB TAP block in patients undergoing elective CD. Additional aims of the study are to assess efficacy and safety parameters, and participant satisfaction.Methods: Enrollment: This clinical trial is a Phase 4, multicenter, randomized, non-blinded, active-controlled study that will include 182 patients presenting for elective CD at a gestational age of 37-42 weeks who meet the following inclusion criteria: Age 18, ASA 3, and ability to provide informed consent and follow the study schedule via an electronic device (ePro). Randomization : Subjects will be randomized in a 1:1:1 ratio into one of three groups: Group 1 (SOC arm): Subject receives 150 mcg IT morphine and no LB TAP infiltration. Group 2 (reduced IT morphine): Subject receives 50 mcg IT morphine + LB TAP infiltration. Group 3 (no IT morphine): Subject receives LB TAP infiltration and no IT morphine. All participants will receive spinal anesthesia consisting of 0.75% hyperbaric bupivacaine (1.4-1.6 mL) and fentanyl (15 mcg). At the time of skin closure, all subjects will receive IV ketorolac (30 mg) and IV acetaminophen (1000mg). Postoperative pain management: All subjects will receive the same multimodal pain regimen of IV ketorolac 30mg q6h and IV acetaminophen 1000mg q6hr during the first 24 hrs after skin incision closure. Afterwards, all subjects will receive scheduled PO acetaminophen 975 mg q6h and scheduled PO ibuprofen 600 mg q6h for up to 72 hours or until discharge. Rescue medications: When breakthrough pain occurs for a subject, rescue medications are available and will include both IV and PO opioid medication. Outcome Assessments: Assessments at 12, 24, 48 and 72 hours after closure of the skin incision will be documented. These assessments include the Visual Analog Scale (VAS) for pain intensity, itching intensity, opioid consumption, Opioid Related Symptom Distress Scale (ORSDS), Recovery from Cesarean Sections (RCSS), and vital signs. Following discharge, subjects report VAS pain intensity and any pain medications taken through an ePro device. Research staff will call subjects on postoperative days 14 and 30 to ask about any adverse events and opioid consumption since hospitalization.Results: We anticipate that the administration of LB via TAP infiltration will provide comparable analgesia to SOC with a more prolonged effect. In addition, we expect to see decreased postoperative opioid consumption in participants receiving LB vs SOC, with a concomitant decreased incidence of pruritus and other side effects of IT opioids. An interim analysis will be conducted after 20 patients have been enrolled in each arm of the study (n=60). At that time, we may find that there is no additional analgesic benefit seen in the group that received low dose IT morphine along with TAP block with LB (Group 2) compared to the group that received SOC (Group 1) or no IT morphine with TAP block with LB (Group 3). If that is the case, we plan to stop enrolling patients into Group 2 and continue enrollment of Groups 1 and 3 until the completion of the study. Conclusion: We hypothesize that the data will show a significant decrease in opioid consumption with better or equal pain control in patients who receive a TAP block with LB compared to those who receive standard of care IT morphine and no TAP block. In addition, we expect to see a reduction in opioid-related side effects such as pruritus and nausea in patients who receive a TAP block with LB compared to those who receive IT morphine. We hope to show that excellent postoperative analgesia with high levels of patient satisfaction is possible after major surgery without the use of opioids and associated adverse events. Funding Statement: This study is being supported by funding from Pacira Pharmaceuticals, Inc. Clinical Trials identifier: NCT038536948087 Ramy Elshaboury, PharmD, Pharmacy Clinical Experience with Ceftolozane/tazobactam and Ceftazidime/avibactam at an Academic Medical Center R. Elshaboury1, M. Adamsick1, R. Gandhi1, A. Letourneau2 and M. Bidell1 1Pharmacy, Massachusetts General Hospital, Boston, MA, USA and 2Infectious Diseases, Massachusetts General Hospital, Boston, MA, USA Introduction: Ceftolozane-tazobactam (CTZ) and ceftazidime-avibactam (CZA) are two of the newest broad-spectrum antibiotics, approved by the FDA in 2014 and 2015, respectively. Both agents are suitable for treating infections due to multidrug-resistant Gram-negative pathogens, including Pseudomonas aeruginosa and Enterobacteriaceae spp. The role of these agents for treatment of severe infections caused by highly resistant Gram-negative pathogens, including carbapen - em-resistant strains, remains an area of ongoing research.Methods: This was a retrospective analysis of the clinical experience with CTZ and CZA at Massachusetts General Hospital (MGH). The aim was to inform internal practice and add to the limited literature on these agents. An electronic health record report was generated of all patients who received at least one dose of CZA or CTZ at MGH between April 2016 and September 2018. Retrospective chart review was conducted to collect necessary endpoints. Baseline demographics, including immunocompro-mised status, source of infection, pathogen, co-morbidities, INCREMENT Mortality Index, McCabe Comorbidity Classification and renal function were collected. Patients who received less than 72-hours of CTZ or CZA therapy were excluded. The primary objective was to evaluate clinical cure, defined as no escalation of antibiotics, no additional source control needed, and no death within 14 days. Secondary objectives included assessing outcomes with mono- and combination-therapy, a sub-group analysis of Cystic Fibrosis patients, and the surveillance of susceptibility patterns to analyze the development of resistance. Patients who received CZA and CTZ as continuous, extended, or standard infusion were assessed for possible effects on the primary endpoint.Results: Forty-three cases were included in the final (1 [2.3%]), all of which were carbapenem-resistant. Combination therapy was utilized in 26 cases (60.4%), with aminoglycosides being the most common agents (12 [46%]). Patients were treated for pneumonia (24 [55.8%]), intra-abdominal infection (7 [16.3%]), and skin and soft tissue infection with or without osteomyelitis (7 [16.3%]). Seven patients (16%) were found to have bloodstream infections. Clinical cure was achieved in 69.7% (n=30) patients. Patients with rapidly fatal comorbidities, as determined by the McCabe Comorbidity Classification, were less likely to achieve clinical cure (50%). Clinical cure in patients treated with CTZ or CZA monotherapy was 77%, compared to 65% in patients treated with combination therapy. Of the 21 patients with more than one isolate available for susceptibility testing, 7 (33%) developed resistance after the initial treament course.Conclusion: CTZ and CZA are appropriate options to treat severe infections due to multi-drug resistant Enterobacteriaceae spp. and Pseudomonas aeruginosa. Clinical cure rates in this cohort were comparable to those observed in other studies. Additional analyses are needed to provide further insight into predictors for clinical failure and the development of resistance while on therapy. Analysis for Clinical Cure 88 Amira M. Eltony, PhD, Dermatology S. Yun1,2,4 1Wellman Center for Photomedicine, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Massachusetts Eye and Ear, Boston, MA, USA and 4Harvard-MIT Division of Health Sciences and Technology, Cambridge, MA, USA Introduction: Although corneal water content is altered in many eye diseases such as Fuchs' Endothelial Corneal Dystrophy (FECD), clinical tools to quantify the distribution of water content in the cornea are limited. A sensitive method for mapping corneal edema in vivo could potentially enhance our understanding regarding the development and progression of FECD and improve diagnosis and treatment monitoring of the disease. Brillouin microscopy is an emerging technique developed to characterize the biomechanical properties of tissues, and has shown potential in corneal disease diagnosis. Recent studies have established that Brillouin frequency shifts are modulated by water concentration. This study investigates whether 81Brillouin microscopy has sufficient sensitivity to detect corneal edema in patients with FECD and to visualize quantitative, spatial patterns of water concentration in FECD patients.Methods: Participants were scanned using a custom-built Brillouin imaging system with a 780 nm, 5 mW laser focused to a spot size of 2 x 2 x 30 m in the corneal stroma. Data from 30 axial scans were combined to create a color-coded map of the mean Brillouin shift laterally across the corneal stroma. Volunteer sample included 7 patients diagnosed as having FECD, and 8 normal subjects. Exclusion criteria for participating in the study included prior refractive or intraocular surgery, dry eye syndrome, glaucoma, and diagnosis of any corneal dystrophies other than FECD. Results: Brillouin maps of normal subjects (n = 8) were relatively homogeneous, while FECD patients' (n = 7) maps exhibited significantly reduced Brillouin shifts in the central region. The mean Brillouin shift in the central region (within 1 mm of the corneal apex) was 5731 \u00b1 15 MHz (mean \u00b1 SD) subjects, and 5648 \u00b1 42 MHz for Fuchs' patients, a significant difference (unpaired t test, P < .001). The mean difference of 83 MHz corresponds to approximately 3.9% higher water content (percentage difference in volume fraction) in the central corneas of the FECD group relative to normal subjects. The Brillouin scan of a FECD patient 1-month after Descemet membrane endothelial keratoplasty (DMEK) measured an increase in Brillouin shift by 62 MHz relative to pre-operative level, indicating normalization of corneal hydration (2.9% reduction in water content).Conclusion: All FECD patients scanned in this pilot study exhibited a centralized reduction in Brillouin shift, distinct from the normal subjects measured, and consistent with centralized edema characterized by pachymetry. Brillouin scans revealed substantially reduced water content after DMEK. These results suggest that Brillouin microscopy could aid treatment planning and assessment of FECD. Moreover, corneal hydration mapping may be useful in understanding fluid pump function dynamics of the cornea and developing early interventions for FECD. Representative examples of Brillouin maps (top row; radius of outer circle is 4 mm) and corneal topographies (bottom row) for normal subjects (A, B, C) and FECD patients (D, E, F) scanned in this study. Normal subjects A and B both had a central-to-peripheral thickness ratio (CPTR) of 0.86. Subject C had a CPTR of 0.89. FECD subject D had the lowest CPTR of the FECD patients measured (0.89) and presented with central guttae with areas of pigmented confluence. Subject E had a CPTR of 0.94 and presented with central guttae with patchy confluence. Subject F had the largest CPTR of the FECD patients measured (0.99) and presented with centrally confluent guttae and haziness. Case study of an FECD patient scanned just prior to DMEK surgery and again one month post-operatively. Top row shows Brillouin shift map and bottom row shows corneal thickness. Before surgery, the patient had a CPTR of 0.96 and presented with confluent guttae. One month after surgery their CPTR was 0.87 (with the graft). 89 Hamdi Eryilmaz, PhD, Psychiatry Decoding a cognitive phenotype via functional connectivity and machine learning H. Eryilmaz, K. Dowling, D. Hughes, A. Rodriguez-Thompson, F. Huntington, W. Coon and J. Roffman Psychiatry, Massachusetts General Hospital, Somerville, MA, USA Introduction: Working memory (WM) is a key building block of human cognition and its dysfunction is associated with cognitive impairment in neuropsychiatric disorders. While precise mechanisms of WM processing are unclear, short-term maintenance of information is thought to be supported via interactions between brain networks involved in memory represen-tations and attention processes. Functional connectivity is a powerful tool used to characterize brain networks at various brain states. Machine learning (ML) has the unique advantage to assess covariance in connectivity patterns and detect subtle changes. In this study, we took advantage of ML to determine the characteristic connectivity features that separate high and low WM load conditions in healthy adults.Methods: Participants The study cohort included 182 healthy adult participants (age 24.96\u00b13.60), who underwent fMRI during resting state and working memory performance. 5 participants were excluded from the analyses due to suprathreshold head motion during MRI. Image acquisition MRI acquisition included a T1-weighted image and a multislice resting-state functional scan, during which participants were instructed to keep their eyes open and remain still. Participants performed the 82Sternberg Item Recognition Paradigm (SIRP) while undergoing functional MRI. Task The task comprised encoding, delay, and probe epochs. During encoding, a set of 1, 3, 5, or 7 consonants were presented (6 s). After a brief delay screen with a fixation cross (2 s), 14 consecutive probes were presented (1.1 s each), separated by a varying intertrial interval. Partici - pants indicated whether the probe was a target (presented during encoding) or foil (not presented during encoding) using a keypad. Preprocessing of imaging data and functional connectivity Functional images underwent standard and connectivity- specific preprocessing. Time courses were extracted from 611 seed regions covering the entire cortex to compute a 611x611 correlation matrix for each subject. Each seed was assigned a cortical network based on a validated cortical parcellation. Functional connectivity was computed during retrieval for a given load (e.g., 1, 3, 5, or 7). For each load, a task correlation matrix (611x611) was generated by computing the Pearson's correlation among all pairs of seeds. Machine Learning Analysis Correlation matrices generated for each WM load were analyzed using a linear Support Vector Machines (SVM) algorithm (fitcsvm ) in Matlab. SVM is a supervised ML method, which uses support vector machines to define a hyperplane separating two classes with a maximum margin. We performed binary classification for all possible task condition pairs and computed classication accuracy using 5-fold cross validation. In order to determine which network features contribute most to the 1T-7T classification, we used Neighborhood Components Analysis (NCA) via fscnca in Matlab, which provided us with feature weights (as a measure of feature importance) for 1T-7T classification. Once the feature weights were computed by the NCA, we computed classification accuracy iteratively using increasing number of \"top\" features. The accuracy was maximum at top 718 features. We then obtained the distribution of brain networks among these 718 features, assessing within-network (e.g., visual-visual) and between-network (e.g., visual-frontoparietal) features separately.Results: Classification SVM successfully decoded different task conditions resulting in classification accuracies above chance level for each classification (see Table 1). Task conditions that substantially differed in difficulty depicted higher accuracy rates than the ones with similar levels of difficulty. Discriminative connections For the 1T-7T classification, NCA determined the weights for the most informative connectivity features, which allowed us to categorize them based on their network assignment. Distribution of top 718 connections is displayed in Figure 1. Among within-network features, connections within-visual and within-ventral attention network contributed most to the classification, whereas connections between visual and somatomotor as well as between visual and frontoparietal networks were the top contributors to the 1T-7T classification. Conclusion: Here, we successfully employed machine learning to decode WM load from connectivity matrices and determine the most informative features separating high and low WM. Our findings highlight the importance of between- network connectivity (visual-frontoparietal and visual-somatomotor) in temporary storage and maintenance of information and provide an opportunity to examine these features in neuropsychiatric disorders, in which working memory is impaired. Classification accuracy based on 5-fold cross-validation using all 186,355 features Connectivity features that best distinguish high and low working memory load 90 Parastou Eslami, PhD, Radiology Endothelial Shear Stress Calculation Based on Invasive versus Noninvasive Coronary Artery Imaging P. Eslami1, J. Karady1, M. Albaghdadi2, Z. Jin1, MGH, 2Cardiology, MGH, Boston, MA, 3Cardiology, BWH, Boston, MA, USA Introduction: Low endothelial shear stress (ESS) is associated with coronary plaque progression on invasive intravascular ultrasound (IVUS) and optical coherent tomography (OCT). Recent advances allow ESS to be calculated noninvasively based on coronary computed tomography angiography (CTA) using computational fluid dynamics (CFD). In a pilot study, we performed an unblinded comparison of ESS calculated from IVUS/OCT versus coronary CTA.83Methods: In four patients who had ECG-gated CCTA less than 90 days before IVUS/OCT imaging segmentation of the lumen of the coronary tree and aortic root (Medis QAngioCT) was performed. Hemodynamics were calculated using a CFD solver customized for vascular flow (Simvascular) based on physiological information (cardiac output, heart rate and blood pressure), first in a full coronary network including all branches. For the comparisons to IVUS/OCT iterative and optimized CFD calculations were performed only for the targeted vessel with IVUS/OCT in imaging both modalities. Cross-sectional time averaged ESS (TAESS) was calculated for consecutive 0.3 cm subsegments along the vessel's centerline. Pearson correlation were used to test agreement between IVUS/OCT and coronary CTA models. High and low TAESS values were categorized based on the IVUS/CTA values in each artery. Results: In a total of 4 coronary arteries (in 4 patients), we found similar patterns of TAESS from invasive vs. noninvasive imaging modalities (Figure). High local TAESS was observed at locations of luminal narrowing whereas low ESS was found distal and proximal to luminal narrowing. In total of 328 subsegments, the Pearson correlation between TAESS in IVUS/CTA and coronary CTA model was r=0.82(p<0.01). Conclusion: This pilot study suggests that ESS calculated noninvasively from coronary CTA in excellent correlation with IVUS/OCT. 8491 Breanna Ethridge, B.A., Anesthesia, Critical Care and The Psychomimetic Properties of Ketamine Anesthesia are Distinguishable from its Analgesic Effects B. Ethridge1, Mekonnen1, K. Colon1, J. Gitlin1, J. Qu1 and O. Akeju1,2 1DACCPM, Massachusetts General Hospital, Boston, MA, USA and 2McCance Center for Brain Health, Massachusetts General Hospital, Boston, MA, USA Introduction: Opioid misuse for the treatment of pain is a public health concern. Non-opioid based analgesic medications and neural circuits are being sought to mitigate the ongoing opioid epidemic. Ketamine is associated with psychomimetic proper- ties that are mediated by the N-methyl-D-aspartate (NMDA) receptor. Ketamine is also associated with analgesia. Empirical evidence suggests that the analgesic properties of ketamine are associated with its psychomimetic effects. However, this postulate remains unproven.Methods: We calibrated a painful stimulus (8/10 pain) using a pain cuff on the calf of healthy volunteers and assessed pain scores longitudinally for 120 minutes post administration of intravenous ketamine (2 mg/kg). We also assessed psycho- mimetic effects longitudinally using the Clinician Administered Dissociative States Scale (CADSS). We administered midazolam (2mg) to manipulate the psychomimetic effects of ketamine 60 minutes post ketamine. We analyzed the data using a backward elimination mixed effects longitudinal analysis.Results: There was a significant (p<0.0001) quadratic relationship between time and pain intensity, with the minimum pain intensity scores occurring 70 minutes after ketamine was administered. When the model for pain intensity was implemented with an additional main effect for CADSS as a predictor term, the effect of CADSS was not significant suggesting that the effect of ketamine in reducing pain intensity is not mediated through psychomimetic properties.Conclusion: The NMDA mediated psychomimetic properties of ketamine are separable from its analgesic properties. This finding suggests future studies of ketamine and ketamine metabolites may lead to non-NMDA testable neural hypoth- eses and therapeutic targets for pain. 92 Mark Evans, MD, Pathology Routine Clinical RNA-Sequencing in Castration-Resistant Prostate Cancer M. Evans1,2, M. Jan2, V. A. Iafrate2 and J. Lennerz2 1Department of Pathology and Laboratory Medicine, University of California, Irvine, Orange, CA, USA and 2Department of Pathology, Center for Integrated Diagnostics, Massachusetts General Hospital, Boston, MA, USA Introduction: Abiraterone and enzalutamide are approved for the treatment of castration-resistant metastatic prostate cancer. Certain splice-variants of the androgen receptor (AR-v7) and presence of certain intergenic fusions (e.g. involving the oncogenic transcription factor ERG) have been associated with reduced response to enzalutamide, abiraterone, and/or taxane- based chemotherapies. As a result, there is a demand for a reliable and highly sensitive method for detecting these alterations when selecting appropriate treatment. Here we demonstrate how routine clinical RNA-sequencing of solid tumor samples at MGH reliably identifies several intergenic and intragenic aberrations in prostate cancer.Methods: Cases of castrate-resistant prostate carcinoma were identified from the medical record, which had previously been submitted for routine next-generation sequencing at MGH from 2010 to 2019. Castrate resistance was designated by evidence of disease progression during the course of ADT. All histologic tumor types were included, as well as consultation cases that were evaluated at our institution. The results of the RNA-sequencing were reviewed retrospectively to determine the presence of intergenic fusions, gene mutations, and specifically expression of AR-v7. Results: Data was available for thirty-three castrate-resistant prostate cancer samples analyzed at MGH. The median age was 70 years (range, 49-89 years). The histologic pattern in all cases was typical acinar and/or intraductal carcinoma, with the exception of one tumor that demonstrated small cell morphology. Each of the patients had been treated with ADT in combination with several additional treatment modalities, including abiraterone and enzalutamide. Twenty-one had developed extra-prostatic metastases at presentation, and at the time of the study, eight had died from their disease. Among the thirty-three patients, we successfully identi - fied intergenic fusions in sixteen (nine of which were the novel ERG-TMPRSS2 fusion), and AR-v7 was detected in thirteen cases.Conclusion: In a specific castrate-resistant cohort of prostate cancers, our study demonstrates that routine RNA-sequencing practices at MGH reliably detects a fusion rate of approximately 50%, with ERG-TMPRSS2 present in the majority of these cases. Moreover, AR-v7 was identified in 40% of patients, which is concerning for poor response abiraterone and enzalut - amide. We believe that these results represent a relevant subset of patients with prostate cancer tested at MGH, and that our current RNA-sequencing practices have utility in detecting both intragenic and intergenic alterations in prostate cancer, which can ultimately influence important treatment decisions.8593 Abigail Farrell, BS, Psychiatry N-Acetylcysteine in the Treatment of Pediatric Bipolar Spectrum Disorders A. Farrell1, M. DiSalvo1, S.V. Faraone3, A. J. Wozniak1,2 1Clinical and Research Program in Pediatric Psychopharmacology and Adult ADHD, Massachusetts General Hospital, Boston, MA, USA, 2Department of Psychiatry, Harvard Medical School, Boston, MA, USA and 3Departments of Psychiatry and Neuroscience & Physiology, SUNY Upstate Medical University, Syracuse, NY, USA Introduction: Pediatric bipolar disorder affects a considerable portion of children and adolescents and is considered a prevalent public health concern. The Food and Drug Administration has approved several medications for the treatment of bipolar disorder in a pediatric population, although these medications often pose the risk of unpleasant side effects. Therefore, research must be done to investigate both the efficacy and safety of potential alternative and adjunctive treatment options. The objective of this study was to assess the efficacy and tolerability of N-acetylcysteine (NAC), a natural dietary supplement, in children and adolescents with bipolar spectrum disorders.Methods: We conducted a 12-week open-label clinical trial. Participants were male and female children 5-17 years of age meeting DSM-V diagnostic criteria for bipolar spectrum disorders (type I, II, or not otherwise specified) and displaying mixed, manic, or hypomanic symptoms without psychotic features. All participants received NAC (BioAdvantex brand) on an open-label treatment schedule adjunctive to previously established and stable treatment regimens. Symptoms of mania and depression were assessed weekly using the Young Mania Rating Scale (YMRS), Hamilton Depression Rating Scale (HDRS), Children's Depression Rating Scale (CDRS), and Clinical Global Impression (CGI) Severity (CGI-S) and Improvement (CGI-I) scales for mania and depression.Results: Overall, there was a significant reduction in YMRS, HDRS, and CDRS mean scores from baseline to endpoint (all p<0.001). Of the 24 exposed participants, 54% had an antimanic response measured by a reduction in YMRS 30% and 46% had a CGI-I mania score 2 at endpoint. Additionally, 62% of participants had an antidepressive response measured by a reduction in HDRS 30%, 31% had an antidepressive response measured by a reduction in CDRS 30%, and 38% had a CGI-I depression score 2 at endpoint. Conclusion: NAC reduced manic and depressive symptoms in a majority of children with bipolar spectrum disorders. NAC may be an effective and safe alternative and natural treatment for pediatric bipolar spectrum disorders. 94 Robyn Farrell, Medicine - Cardiology Post-Exercise Oxygen Uptake Recovery Kinetics Are Associated with Cardiac Performance and Outcomes in Suspected Heart Failure with Preserved Ejection Fraction R. Farrell, M. Malhotra and Lewis Cardiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Exercise intolerance in patients with heart failure with preserved ejection fraction (HFpEF) reflects multi-organ dysfunction characteristic of a systemic disorder. We sought to determine if easily-obtainable non-invasive oxygen uptake (VO 2) kinetic patterns derived during cardiopulmonary exercise testing may aid in resolution of cardiac-specific impairment during exercise. Delayed kinetics of VO2 recovery will predict cardiac performance and heart failure (HF) outcomes in patients presenting for evaluation of dyspnea on exertion.Methods: We performed upright maximum incremental ramp cycle ergometry cardiopulmonary exercise testing with invasive hemodynamic monitoring in patients referred to a single center for evaluation of exertional dyspnea. Pulmonary capillary wedge pressure (PCWP) and direct Fick cardiac output (CO) were measured at rest and during each minute of exercise to derive PCWP/CO slopes. We assessed recovery O 2 kinetics by measuring VO2 recovery delay (VO2RD), defined as the time it took for recovery VO2 to fall below peak VO2 (defined by the highest 30 second median within the final minute of exercise). VO2RD >25s was evaluated as a prognostic cut point. Heart failure (HF) hospitalization and HF event-free survival were assessed by Cox regression models.Results: Among 342 patients with measured VO2 kinetics (27%). across quartiles of PCWP/ CO slopes (Figure, ANOVA p<0.0001). After adjustment for age, sex, and BMI, VO2RD >25s was associated with a higher hazard for HF hospitalization (HR 3.35, 95% CI 1.28-8.76, p=0.014) and combined HF or death (HR 2.1, 95% CI 1.04-4.18, p=0.039).86Conclusion: In patients undergoing evaluation of dyspnea on exertion, abnormal VO2 recovery kinetics that are easily discernable during non-invasive cardiopulmonary exercise testing are associated with a steep increment in invasively-derived left ventricular filling pressures in response to exercise. Lower VO2RD also predicts HF-free survival, making it an attractive non-invasive parameter to measure during exercise-based evaluation of suspected HFpEF. Figure 1. A) VO2RD is calculated as the time required for recovery VO2 to fall below peak VO2. Normal individuals (red) have minimal VO2RD (0-10 seconds), while people who are impaired (teal) are seen to have a more prolonged VO2RD B) Relationship between non-invasively derived VO2RD across quartiles of an invasively derived, PCWP/CO slopes. 95 Emily H. Feig, PhD, Psychiatry Do mid-life adults respond differently to a positive psychology intervention compared to older adults? A secondary analysis across three study phases of the Positive Emotions After Cardiac Events (PEACE) Study E.H. Boston, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: During mid-life, defined as age 45-64, adults commonly experience a variety of life stressors that place them at higher risk for developing chronic medical conditions. Interventions that target improved psychological well-being and health behavior adherence are critical to mitigate this risk. Positive psychology interventions use structured activities such as engaging in kind acts, expressing gratitude to others, and identifying personal strengths. They are straightforward to deliver and have been shown to improve psychological well-being and health behavior engagement. These interventions show potential to improve coping with the stressors facing mid-life adults. However, individual in mid-life may be less likely to engage in and benefit from interventions that are perceived as too burdensome in terms of time and effort due to competing demands. The present study is a secondary analysis of the Positive Emotions After Cardiac Events (PEACE) Study, which used a stepwise approach to develop and test a positive psychology intervention for patients who experienced an acute coronary syndrome. The aim of the present study is to compare mid-life to older participants, across three phases of the PEACE study, in their response to the intervention. We hypothesized that mid-life participants would show smaller improvements during the intervention in optimism, positive affect, depression and anxiety symptoms, and physical activity compared to older participants. Methods: Participants were a subset (N=200) of adults who enrolled in an intervention study after being hospitalized for an acute coronary syndrome at an academic medical center. To be eligible for this analysis they had to be at least 45 years old and randomized to the positive psychology study condition. Sixty percent (n=120) of participants were in mid-life and the remainder were older; 53% of participants were male. The intervention was similar across study phases, consisting of weekly individual telephone calls with a trained interventionist (MD, PhD, or LCSW) plus a written manual that taught skills to improve psychological well-being. Depending on study phase and condition, some participants (n=118) also received strategies based in motivational interviewing to increase physical activity. Intervention length ranged from 8-12 weeks depending on study phase. Assessments were completed at baseline and end of treatment. Psychological well-being measures included the Life Orientation Test - Revised which assesses dispositional optimism, the Positive and Negative Affect Scale which assesses positive affect, and the Hospital Anxiety and Depression Scale, which assesses symptoms of depression and anxiety. These measures were collected in all three study phases. Accelerometer-measured physical activity was assessed in two study phases (n=170). The effect of age group (mid-life vs. older) on pre-post change in psychological and physical activity outcomes will be tested using a random intercept linear mixed effects model with main effects for time, age group, age-by-time interaction, and a categorical effect of study phase.Results: Results are forthcoming.Conclusion: If results are in line with our hypothesis, the positive psychology intervention may require modifications to meet the unique needs of mid-life participants and improve efficacy for this group. For example, mid-life participants with high demands on their time may benefit more from a mobile intervention, delivered via text message and/or an app compared to a telephone intervention due to the increased flexibility allowed from these modalities. Customizing the intervention content to address the stressors most common in mid-life that often serve as barriers to physical activity (e.g., financial stress, time management difficulties, caregiving stress) may also be beneficial.8796 Alyssa L. Fenech, BA, Psychiatry Post-Traumatic Stress Symptoms in Hematopoietic Stem Cell Transplant (HCT) Recipients A.L. Fenech1, O. Van Benschoten1 and A. El-Jawahri2 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA and 2Cancer Center, Massachusetts General Hospital, Boston, MA, USA Introduction: HCT is an intensive and potentially curative therapy for patients with hematologic malignancies. Patients admitted for HCT experience a prolonged, isolating hospitalization and endure substantial physical and psychological symptoms. However, the impact of HCT on long-term post-traumatic stress (PTSD) symptoms in transplant survivors is currently unknown. Furthermore, predictors of PTSD in this population have not been explored.Methods: This secondary analysis included longitudinal data from 250 patients who underwent autologous and allogeneic HCT at Massachusetts General Hospital between 7/2013-1/2016. We used the Post-Traumatic Stress Checklist (PTSD-CL) to assess for PTSD symptoms at six months post-HCT. We used the Functional Assessment of Cancer Therapy - Bone Marrow Transplant, and the Hospital Anxiety and Depression Scale to assess quality of life (QOL) and depression and anxiety symptoms at the time of admission for HCT, at week-2 during HCT hospitalization, and at six months post-HCT. We used multivariate regression models to assess predictors of PTSD symptoms. Given collinearity between QOL, depression, and anxiety symptoms, we modeled these separately. Results: The mean age for study participants was 56.3 years (SD = 13.3). The majority were White (88.0%, 220/250), and female (51.2% 128/250). The most allogeneic HCT. The of clinically PTSD symptoms at six months post-HCT was 18.9% (39/206). Participants with clinically significant PTSD symptoms experienced hypervigilance (92.3%), avoidance (92.3%), and intrusion (76.9%) symptoms. Among patients without clinically significant PTSD symptoms, 24.5% had clinically significant hypervigilance symptoms and 13.7% had clinically significant avoidance symptoms. Younger age (B= -0.14, P = 0.025), male sex (B=3.41, P = 0.038), and lower QOL at time of HCT admission (B = -0.19, P <0.001) were associated with higher PTSD symptoms at six months post-HCT. In a separate model incorporating anxiety, only baseline anxiety at HCT admission (B = 1.26, P < 0.001), and change in anxiety during HCT hospitalization (B = 0.55, P = 0.033) were associated with PTSD symptoms at six months post-HCT. Notably, patients' baseline depression symptoms was also associated with PTSD symptoms at six months post-HCT (B = 0.92, P < 0.001).Conclusion: Approximately one fifth of patients undergoing HCT experience clinically significant PTSD symptoms at six months post-transplant. The prevalence of hypervigilance and avoidance symptoms are notable even among patients who do not have clinically significant PTSD symptoms. Patients' baseline QOL and psychological symptoms emerge as important predictors of their risk for PTSD at six months post-HCT. Interventions to prevent and treat PTSD symptoms in HCT recipi - ents are clearly warranted. 97 Neil D. Fernandes, MD, Pediatrics Positive end-expiratory pressure generated by RAM cannula using a Lung Simulator N.D. Fernandes1, M. Salt1, E. Chung2, B. Ejiofor2, R. Carroll1 and R. Kacmarek2 1Pediatric Critical Care, Massachusetts General Hospital for Children, Boston, MA, USA and 2Respiratory Care Services, Massachusetts General Hospital, Boston, MA, USA Introduction: Bronchiolitis is the most common cause of lower respiratory tract infection during a child's first year of life and the most common cause of admission for children under 2 years of age in the United States, leading to $1.73-billion USD in healthcare expenditure. The current standard of care involves supportive measures, including continuous positive airway pressure (CPAP). CPAP or positive end-expiratory pressure (PEEP) enhances functional residual capacity and reduces the work of breathing, leading to improved outcomes and reduced use of invasive ventilatory support. CPAP is traditionally delivered through a well-fitting nasal cannula, pillows, or a full facemask. Recently, however, pediatric intensive care units have been adopting a simple, easy-to-use, CPAP patient interface, the RAM cannula. It is hypothesized that nCPAP delivered via RAM cannula is non-inferior to traditional methods. If nCPAP delivered via RAM cannula is considered non-inferior it may lead to greater adoption, increased use of non-invasive respiratory support, reduced invasive ventilatory support, reduced need for sedation, and reduced length of hospital stay.88Methods: To test this hypothesis we used 3D-printed models of various sizes of pediatric upper airways connected to the ASL 5000 Lung simulator. We applied each model of RAM Cannula to an age-appropriate printed airway, delivering pressures of 5, 7, and 10 cm H2O using a Medtronic 980 ventilator in the CPAP mode. Leaks of 0, 20, 40, and 60% were generated to emulate poor fit and/or open-mouth breathing. Pressures from the artificial trachea and alveoli were measured. Results: We found that CPAP of 5 through 10 cm H2O generated PEEPs ranging from 1.5 to 12 cm H2O. Overall, we found that for each specific cannula-airway combination, increasing the nCPAP and decreasing the air leak resulted in higher levels of PEEP delivered. None of the delivered PEEP levels exceeded that set on the ventilator by more than 2 cm H2O. Conclusion: The use of a RAM cannula to deliver CPAP is a safe and efficient method of delivering noninvasive CPAP. 98 Stephanie Fiedler, BA, Radiology The Investigation of Epigenetic Mechanism in vivo in Alcohol Use Disorder Patients quantified by Non-Invasive PET Imaging using a PET/MR S. Fiedler1, N. Taghian2, R. Striar1, H. Wey1, R. Weiss2 and C. Wang1 1Radiology, Massachusetts General Hospital, Cambridge, MA, USA and 2McLean Hospital, Belmont, MA, USA Introduction: The purpose of this study is to evaluate altered regional histone deacetylase (HDAC) expression (density and distribution) in Alcohol Use Disorder (AUD) patients and age- and sex-matched healthy controls, using [11C]Martinostat as a PET radiotracer. We aim to measure HDAC density and distribution in the brain of AUD patients. We will test the hypothesis that HDAC enzyme expression is altered in AUD brains. We will take advantage of a combined PET/MRI system to collect simultaneous PET and structural MRI to obtain high-resolution spatial maps. This will address fundamental questions about chromatin modifying enzymes in vivo in a way that has not been possible until now. HDACs are a family of epigenetic enzymes that regulate gene expression in the human brain by chemically modifying chromatin, a fundamental network of proteins and DNA in chromosomal structure. HDACs drive chromatin changes in response to life experience and the environment. Research from diverse disciplines supports the hypothesis that HDAC dysfunction can lead or contribute to brain disease, including AUD. HDAC expression in the brain has been evaluated by groups investigating the molecular mechanisms underlying bipolar disorder and schizophrenia, major depression, Alzheimer's disease, and AUD. Despite the preclinical evidence of HDAC and epigenetic dysregulation in AUD, limited investigation of the underlying processes and mechanisms relevant to AUD presents a significant knowledge gap between mechanisms measurable in animal models and human subjects. Prior to recent developments from our laboratory, the density and distribution of HDAC in the brain could not be quantified without sampling tissue. To overcome this limitation, we developed the first radiotracer, [ 11C]Martinostat, for non-invasive HDAC imaging via PET (Figure 1), which was recently approved by the FDA for first-in-human studies (IND # 123154). Our laboratory has now successfully imaged rodents, non-human primates, a growing cohort of healthy adults (18-65 years old) (Figure 2) and AUD patients (Figure 3) using [ 11C]Martinostat. Particularly, our PET HDAC imaging in AUD patients is the first time ever to visualize epigenetic expression in the brain of any SUD patient population. These early imaging data represent a major step forward in understanding epigenetic mechanisms in vivo and are already providing insights into regional HDAC expression. Methods: Interested subjects undergo a telephone or in-person screening to determine initial study eligibility. On the day of the scan, informed consent is obtained along with a urine test, medical history and physical examination, as well as information from the subject about any medication they are currently taking. All subejcts complete the M.I.N.I. International Neuropsychiatric Interview, sections I and J only, as well as MRI and PET screening forms. In addition, AUD patients will complete the Brief Addiction Monitor (BAM) at McLean Hospital. If the subject is a female of childbearing age, blood will be drawn for a serum blood test to rule out pregnancy on the day of the scan. The radiotracer, [ 11C]Martinostat, is injected through an intravenous catheter by a licensed nuclear medicine technologist while outside of the scanner. After a 20-minute uptake period, the subject is placed in the PET/MR scanner and scanned for 60 minutes. After the scan once the subject is deemed okay, they are cleared to leave. We have scanned 9 patients and 20 healthy controls. Results: In preliminary results, [11C]Martinostat shows high uptake with specific regional binding in healthy controls, specifically in the occipital lobe, putamen, and cerebellum with comparatively low uptake in the white matter. There is increased SUVR in patients in the precuneus and thalamus, and decreased SUVR in patients in the lingual gyrus. Conclusion: Using [11C]Martinostat to measure HDAC expression in AUD patients, preliminary data demonstrates regional differences in AUD compared to controls. Moving forward, we will scan additional AUD patients and attain an anesthesiolo- gist so we can include arterial blood sampling; specifically we will measure VT, which is the gold standard for PET quantita - tion. We will also determine the relationship between SUV and VT, and SUVR. This work will allow us to better understand HDAC expression in AUD in the living human brain and provide insight into molecular changes involved in this disorder.89 99 Michael R. Flaherty, D.O., Pediatrics Fire Pit-Related Burn Injuries in Children and Adolescents M.R. Flaherty1 and R. Sheridan2 1Pediatric Critical Care Medicine, Massachusetts General Hospital, Boston, MA, USA and 2Burn Sugery, Massachusetts General Hospital, Boston, MA, USA Introduction: Thermal burns are a leading cause of preventable injury in children and adolescents. Fire pits have become increasingly popular outdoor fixtures in US homes. We aimed to identify trends in pediatric burn injuries related to fire pits that presented to a representative sample of United States Emergency Departments (EDs). Methods: A retrospective analysis of annual ED visits from the National Electronic Injury Surveillance System (NEISS) from January 1, 2006 through December 31, 2017 using product codes specific to fire pits. United States Census population estimates were used to compute rates per 100,000 population. SAS and Joinpoint weighted regression analyses were used to analyze annual estimates and rate trends across the study period.Results: There were 10,951 (95 % CI 8,535-13,367) ED visits for burn injuries secondary to outdoor fire pits in patients 19 years of age and younger across the study period. The majority of injuries occurred in children under the age of 5, and were related to falls into or on a hot fire pit. The annual modeled rate change showed an overall significant increase in burn injuries of 7.16 per 100,000 annually from 2006-2017 (p=0.02).Conclusion: Outdoor fire pits represent an increasing hazard to young children who are particularly susceptible to burn injuries from falls in or around lit recreational fires. Product modifications and public awareness campaigns are necessary to prevent future life-altering injuries in pediatric patients.90100 Gaetano Florio, MD, Anesthesia, Critical Care and Pain Medicine A multidisciplinary critical care lung-rescue team improves survival of ARDS patients with class III obesity G. Florio1, R. De Santis Santiago1, L. Grassi1, R. Kacmarek1,4 and L. Berra1 1Anesthesia, Critical Care and Pain Medicine, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA, 2Heart Center, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA, 3Division of Pulmonary and Critical Care, Massachusetts General Hospital, Boston, MA, USA and 4Department of Respiratory Care, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA Introduction: Acute respiratory distress syndrome (ARDS) represents a common reason for admission to the Intensive Care Unit for people with class III obesity (body mass index >40Kg/m2). Obesity has been an exclusion criterion in most of the major ARDS trials. Considering the absence of definitive protocols of ventilation in the obese population and the recent worldwide increase in prevalence of obesity, studies are warranted. We designed a multidisciplinary study at Massachu- setts General Hospital to determine whether mortality is improved when a specialized multidisciplinary Lung-rescue team provides patients with an individualized titration of mechanical ventilation based on cardiopulmonary physiology compared to the use of standard universal protocols for ARDS patients without obesity (NIH/ARDSnet guidelines for ventilation of patients with ARDS[http://www.ardsnet.org/files/ventilator_protocol_2008-07.pdf]). Methods: All adult patients with class III obesity and ARDS who were receiving mechanical ventilation for more than 48 hours and were admitted to the surgical or medical ICU between 01-January-2012 and 31-December-2017 were eligible for enrollment in this observational study. Patients were collected in two cohorts. The first cohort of patients (2012-2014) had ventilator settings determined by the use of standard universal ARDS protocols. The second cohort of patients (2015-2017) had their ventilator settings determined by the Lung-rescue team. The Lung-rescue team is a multi-disciplinary hospital group with skills in anesthesia, respiratory care, and cardiac and pulmonary critical care who provide a titration of mechan - ical ventilation according to an individualized assessment of cardiopulmonary physiology. The Lung-rescue team utilizes esophageal manometry, hemodynamic measurements, and right heart echocardiography.Results: From 2012 to 2014, patients (n=70, BMI 49\u00b1 9 Kg/m 2) were ventilated according to a fixed protocol based on standard universal protocols for ARDS. The second cohort of patients (n=50, BMI 54\u00b1 13 Kg/m2) from 2015 to individ- ualized ventilation titrated by the Lung-rescue team. The risk of dying at 28 days (31% versus 16%, P =0.012; hazard ratio (HR): CI 95% 0.16-0.74) was almost double in patients treated according to the standard universal protocols for ARDS compared to those treated by the Lung-rescue team. The increased risk of death persisted in the first cohort of patients treated as per the standard universal protocols for ARDS at 1-year (incidence of death unchanged: of mechanical ventilation by an interdisciplinary Lung-rescue team improved survival of ARDS patients with class III obesity compared to standard universal protocols for ARDS. 91101 Ariel S. Frey-Vogel, MD, MAT, Health Professionals Education Research The development of an instrument for faculty to assess resident-led large group teaching A.S. Winchester, MA, 2Medicine, Massachusetts General Hospital, Boston, MA, USA, 3OB/GYN, MGH, Boston, MA, USA, 4Pediatrics, Dartmouth-Hitchcock, Lebanon, NH, USA and 5Pediatrics, Hasbro Children's Hospital, Providence, RI, USA Introduction: The Accreditation Council on Graduate Medical Education requires resident teaching competency. Despite existing approaches to assess teaching, residents desire more feedback. While much formal resident-led teaching occurs in large groups, the literature lacks an assessment instrument with validity evidence. We developed and piloted an instrument for faculty to assess resident-led large group teaching to gather validity evidence for the instrument.Methods: Literature review and our experience leading resident-as-teacher curricula informed initial instrument content. The instrument was developed in 2017-2018. Resident focus groups provided stakeholder input. A modified Delphi panel of international experts provided iterative feedback. We piloted the instrument in 2018 with eight video-recordings of resident-led teaching, developed a guidebook for instrument use, and finalized the instrument. We calculated Cronbach's alpha for internal consistency and intraclass correlation (ICC) for interrater reliability.Results: The instrument has six elements: learning climate, goals/objectives, content, promotion of understanding/retention, session management, and closure. Each element contains eleven sub-elements described by a total of thirty-three observable behaviors. Cronbach's alpha was 0.844. ICC was excellent for 6 sub-elements, good for 1, fair for 1 and poor for 3. Conclusion: We developed an instrument for faculty assessment of resident-led large group teaching. Pilot data showed assessed behaviors had good internal consistency, but inconsistent interrater reliability. With further development, this instrument has potential to assess resident teaching competency. Interrater reliability of the three raters who assessed eight resident teaching videos from all three sites during the pilot study Abbreviation: ICC = Intraclass correlation Categories of reliability derived from Cicchetti (1994) where ICC <0.40 is poor reliability, 0.40- 0.59 is fair reliability, 0.60-0.74 is good reliability and 0.75-1.00 is excellent reliability. Cicchetti D. Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instrument in psychology. Psychol Assess. 1994;6:284-290. 102 Ronna Fried, Ed.D., Psychiatry Can the CANTAB Identify Adults with Attention-Deficit/Hyperactivity Disorder? A Controlled Study R. M. DiSalvo1, H. Driscoll1 and J. Biederman1,2 1Psychiatry, MGH, Boston, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Attention-deficit/hyperactivity disorder (ADHD) is a prevalent and morbid neurobiological disorder estimated to afflict up to 5% of adults in the United States. It has been associated with a wide range of adverse Yet, the diagnosis of this disorder remains a challenge among many clinicians. Despite the documented importance of identifying and properly treating ADHD, there is a lack of consensus among clinicians on the best way to evaluate the disorder. The diagnosis combines 92clinical evaluation with a number of subjective reports from self-report and informant sources. The extant literature has documented the finding of a distinct adult executive functioning (EF) symptom factor and surmised that a deficit in executive functioning, or EF problems, appeared to be a cardinal feature of adult ADHD. However, neuropsychological testing has not been shown to be diagnostic, but rather is ancillary in identifying comorbid conditions associated with ADHD, individual - izing treatment planning, increasing awareness about abilities, and designing compensation strategies to ultimately improve treatment compliance and adherence. Utilizing computerized assessment batteries is one approach aiming to improve the assessment of ADHD. The Cambridge Neuropsychological Test Automated Battery (CANTAB) is one of the most widely used computerized assessment batteries, as it has several advantages over standardized clinical neuropsychological tests. More specifically, the CANTAB is more reliable and valid than many traditional clinical assessments. The CANTAB's method of administration is also extremely standardized, which results in fewer variations due to administrator/experimenter different or error. As far as we know, no previous studies have looked at utilizing the CANTAB as a diagnostic tool for ADHD, which the current paper aims to explore. The main aim of the present study was to evaluate the diagnostic utility of the CANTAB as an accurate tool in identifying adults with ADHD in a clinical sample. To this end, we administered seven subtests of the CANTAB that targeted domains of executive functioning. We hypothesized that there would be strong correlation of the scores on the CANTAB with the diagnosis.Methods: The sample consisted of clinically referred adults aged 18 to 60 years old, with (n = 474) and without (n = 163) DSM-IV diagnosis of ADHD. All subjects were administered seven subtests from the CANTAB that targeted domains of executive functioning and verbal memory, both of which are known to be abnormal in children with ADHD. This included Spatial Working Memory (SWM), Stockings of Cambridge (SOC), Intra-Extra Dimension Set Shift (IED), Reaction Time (RTI), Rapid Visual Information Processing (RVP), Verbal Recognition Memory (VRM), and Affective Go/No-go (AGN).Data were analyzed to identify which CANTAB tasks would best predict ADHD status. We randomly selected two-thirds of our control and ADHD groups to be part of a training set and the other one-third to be part of a test set. This approach allowed us to build a model to predict a diagnosis of ADHD in one data set (the training set) and evaluate how well the model works in a separate, unbiased dataset (the test set).Results: Our sample consisted of 474 ADHD subjects and 163 control subjects. There were no significant differences between the two groups in age, sex, race, or education beyond high school. There was a significant difference in full scale IQ, with the ADHD subjects having lower scores compared to controls. Given this difference, we controlled for full scale IQ in subsequent analyses. The only task that significantly differed between the two groups was AGN Total Commissions. ADHD subjects had significantly more total commissions compared to the control subjects. The effect sizes (Cohen's d) for the CANTAB tasks ranged from -0.01 for IED Total Errors to -0.50 for AGN Total Commissions, indicating that magnitude of the differences between the groups are small to medium at best. Using stepwise logistic regression (forward selection, p<0.05 for entry) in our training set (N=427), we arrived at a final model ( 2 2=32.24, p<0.001) that predicted CI=1.0003-1.008; z=2.09, p=0.04) tasks. When we applied the final model from the training set to our test set (N=210), it yielded an AUC statistic of 0.62 (95% CI=0.53-0.70), indicating that these two CANTAB tasks were only moderately better than chance at predicting ADHD.Conclusion: Our results failed to show any diagnostic utility for the CANTAB in adults with ADHD, even when using the most robust tests (AGN Total Commissions and RTI Simple Reaction Time) identified from stepwise logistic regression (forward selection; p>0.05 for entry). However, the CANTAB was helpful in identifying executive functioning disorder (EFD) in adults with ADHD when compared with controls subjects. Although the CANTAB indicates that neither a specific test or battery being useful in the diagnosis of ADHD, neuropsychological testing can be instrumental to the diagnosis process and treatment planning. For instance, subtypes of ADHD have different treatment needs, which can be recognized through neuropsychological testing. Additionally, identifying deficits such as working memory is useful to determine the best types of academic interventions to employ. Considering the morbidity and dysfunction associated with EFDs in adults with ADHD, efforts aimed at identifying EFD's in adults with ADHD are clearly important93103 Nina Fultz, Bachelor of Science, Athinoula A. Martinos Center for Biomedical Imaging High-quality FLAIR and diffusion imaging with the presence of EEG nets N. Fultz1,2, C. Poulsen3, A. Berman1,4, of Radiology, HST/MGH Martinos Center for Biomedical Imaging, Littleton, MA, USA, 2Department of Biomedical Engineering, Boston University, Boston, MA, USA, 3University of Oregon, Eugene, OR, USA, 4Department of Radiology, Harvard Medical School, Boston, MA, USA and 5Department of Radiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Patients undergoing EEG monitoring in the intensive care unit frequently need to undergo MR imaging. Typically, EEG nets must be removed prior to MR imaging, as they can severely impact image quality. This removal and re-application process is time-consuming and costly, and results in long periods of time where the EEG cannot be monitored, potentially impacting patient care. We investigated whether a new, 256-channel, high-resistance polymer thick film-based EEG net\u2014the InkNet 1 (Fig. 1)\u2014could allow high-quality clinical images to be acquired without the need to remove the EEG net. We tested how the InkNet impacts FLAIR, diffusion, MEMPRAGEs, and EPI resting-state image quality. When possible, we compared the InkNet to a standard 256-channel EEG net with copper leads ('CuNet') and a control condition with no net ('NoNet') at 3 and 7 Tesla. Methods: Two subjects were scanned with a 3T Siemens Skyra scanner and one subject was scanned with a 7T Siemens scanner. All subjects gave informed consent. At 3T, we used a 32 channel coil array. Each 3T subject was scanned in three conditions: a control with no EEG net (NoNet), the CuNet - a commercial copper-lead net (HCGSN-MR, Philips, Eugene, OR), and the prototype InkNet. Each condition followed the same imaging protocol. In each condition, we acquired a 0.9 mm isotropic resolution single-shot T1 MPRAGE. FLAIR images were acquired with 0.9 mm isotropic resolution with 192 slices with whole-brain coverage (TR=5000 ms, TE=387 ms, FOV=230 mm, TI=1800 ms). The diffusion images were acquired with 25 interleaved slices with whole-brain coverage (TR=3700 ms, TE=92.0 ms, FOV=220 mm, Voxel size=1.7x1.7x4.0 mm). B1 maps were acquired with 16 slices with whole-brain coverage (TR=11380.0 ms, TE=2.06 ms, FOV=208 mm, Voxel size=1.6x1.6x4.0 mm). Also, with the 3T data, we manually identified the regions of the scans where the CuNet artifact was present and compared these regions to our NoNet control. The B1 maps were processed and registered, and an ROI corresponding to the artifact region in the CuNet condition was segmented. This ROI was then mapped onto the InkNet and NoNet conditions. The flip angle for each voxel within this ROI was reported. The B1 Field Maps were compared to each other. For our subject at 7T, we used a 32 channel coil array. We acquired BOLD EPI resting-state data (TR=1120 ms, ms, FOV=1536 both the InkNet and the NoNet conditions. The CuNet was not acquired due to heating concerns. We calculated tSNR maps from this resting-state data. One phantom was scanned at 7T to compare the NoNet, CuNet, and InkNet conditions. We ms, FOV=240 mm, Voxel size=1.5x1.5x1.5 mm). Results: Visual inspection of the images identified clear artifacts induced in the images acquired with the CuNetat 3T and 7T (Fig. 2, 3). For the 3T data, we calculated the flip angle for both subjects in the ROI where the artifact was most severe and found consistently decreased values in the CuNet condition. For subject 1, the mean flip angles were 25.8\u00b0 (NoNet), 24.5\u00b0 (InkNet) and 5.2\u00b0 (CuNet). For mean flip angles were 12.8\u00b0(NoNet), 7.6\u00b0(InkNet), 3.0\u00b0(CuNet). The values in individual voxels demonstrate that the B1 field maps were consistent in the InkNet and NoNet conditions, and much lower in the CuNet condition, suggesting that the InkNet avoids inhomogeneity that can impact FLAIR and diffusion image quality (Fig. 4). To determine the clinical impact of these artifacts, a radiologist rated the presence of artifact and image quality, while blinded to condition (1 = no artifact present, high image quality; 5 = severe artifact present, low image quality). Both 3T subjects had FLAIR images ratings of: 1 (NoNet), 1 (InkNet), 2 (CuNet). Both subjects had diffusion images ratings of: 1 (NoNet), 1.5 (InkNet), and 2 (CuNet). For our subject at 7T, we saw similar signal values in our tSNR maps with both the NoNet and InkNet conditions (Fig. 5). When comparing the three conditions, we saw significant dropout with the phantom MEMPRAGEs with the CuNet condition (Fig. 6).Conclusion: Our data suggest that using the high-resistance-lead InkNet will allow researchers and clinicians to be able to use FLAIR, diffusion sequences, MEMPRAGE, and resting-state scans with MR-EEG without compromising image integrity. Comparison of the B1 maps, the intensity and signal differences of the images, and the qualitative evaluation by a radiologist all suggest that there are better signal quality and less artifact with the InkNet in comparison to the standard CuNet. The integration of the InkNet would also allow researchers to use EEG/fMRI at 3T and 7T without significant drop-out or signal loss. For clinicians, the use of the InkNet could allow them to save time and reduce spending due to not having to reapply EEG nets. Future work could examine whether the InkNet allows other sequences to be run simultaneously while still maintaining high-quality clinical images.94 104 Christopher J. Funes, MS, Psychiatry An Investigation into Mechanisms Underlying Anti-Suicidal Properties of Electroconvulsive Therapy (ECT) in Patients with Unipolar and Bipolar Depression C.J. Funes, V. Ho, S. Uribe, T. Barbour, M. Eldaief, K. Ellard and J. Camprodon Psychiatry, Massachusetts General Hospital, Ashland, MA, USA Introduction: Electroconvulsive Therapy (ECT) is the most effective treatment for mood disorders with response and remission rates favorable to those of pharmacological interventions. Additionally, ECT is one of few treatments that has demonstrated anti-suicidal properties. However, ECT's anti-suicidal properties are poorly understood with no clear mechanism of action. In the current study, we sought to understand how ECT-related changes in specific domains of dysfunction underlying mood disorder symptoms (anhedonia, rumination, emotion dysregulation) might relate to changes in dimensions of suicidality. Methods: We examined the relationship between the dimensions of anhedonia, rumination, and emotion dysregulation and specific dimensions of suicidal ideation (intensity, frequency, duration, controllability, deterrents for suicidality, and reasoning for suicidality). This study included 14 patients diagnosed with unipolar and bipolar depression being treated with ECT. Assessments were conducted at four time points over the course of treatment. Rumination was measured using the Rumination-Reflection Questionnaire (RRQ) and Rumination Response Scale (RRS). Anhedonia was measured using the Temporal Experience of Pleasure Scale (Anticipatory and Consummatory subscales; TEPS-A, TEPS-C), Snaith-Hamilton Pleasure Fun-Seeking subscales; BIS/BAS-R, BIS/BAS-F). Emotion Dysregulation was measured using the Affective Control Scale (Anger, Depression, Anxiety and Positive Affect subscales; ACS-A, ACS-D, ACS-Anx, ACS-P). Dimensions of Suicide were measured using the Columbia Suicide Severity Rating Scale (C-SSRS). Analyses consisted of a multi-level mixed-effects general linear model using the change in C-SSRS from baseline as the dependent variable; and time (visit number), RRQ, RRS, TEPS, SHAPS, and ACS as independent variables of interest.Results: Intensity of suicidal ideation at pre-treatment was significantly associated with anhedonia (TEPS-A: p = 0.004). Amongst ECT treatment completers, there was a significant change over time in anhedonia (TEPS-A, TEPS-C: For changes in intensityof suicidal ideation, a significant time x domain interaction effect was found for rumination (RRQ: control of 0.03). For changes in frequencyof suicidal ideation, a signifi-cant time x domain interaction effect was observed for reward motivation (BAS-Drive: Z= -2.11, For duration of suicidal ideation, a significant interaction effect was observed for perceived control of anger (ACS-Anger: Z= -3.38, p= -0.001). No significant time x domain effect was found for controllability, deterrents, or reasons for suicidal ideation.95Conclusion: Results from our study suggest anhedonia is a driving factor in the intensity and frequency of suicidal ideation both at baseline and following treatment with ECT. We found greater endorsement of anhedonia at baseline was associated with greater intensity of suicidal ideation. Following ECT treatment, patients who showed greater decreases in anhedonia over time showed greater decreases in the frequency of suicidal ideation. Rumination was also a significant factor, with greater ECT-related changes in rumination predicting greater changes in intensity of suicidal ideation. Surprisingly, changes in perceived control of anger and anxiety emerged as significant predictors of changes in intensity of suicidal ideation, and changes in perceived control of anger also predicted changes in duration of suicidal ideation. These results shed light on potential mechanisms behind anti-suicidal properties of ECT. Future studies will examine the relationship between results reported here and ECT-related changes in the functional neurocircuitry associated with anhedonia, rumination and emotion regulation. 105 Kayla Furbish, Psychiatry Challenges Using Duke Activity Status Index in a population of Veterans with Posttraumatic Stress Disorder and Traumatic Brain Injury H. Dotson1, M. Shea1, E. West1, and Sylvia1,2 1Massachusetts General Hospital, Boston, MA, USA and 2Harvard Medical School, Cambridge, MA, USA Introduction: The Duke Activity Status Index (DASI) is widely used to assess perceived physical functioning across a wide variety of clinical populations (e.g. cardiac, pulmonary, metabolic, oncology). We examined its utility in veterans with a diagnosis of either posttraumatic stress disorder (PTSD) or traumatic brain injury (TBI). Methods: Veterans completed the DASI and demographic questionnaire as part of a routine clinical intake. Correlations and chi-squared analyses examined the relationship between demographics and DASI scores. Participants (N=379) were mostly male (85.8%, Mage= 40.9, SD=9.1) and 78.3% had a primary Results: The mean DASI score was 42.89 (15.54). The distribution of scores was skewed, with 81.5% veterans reporting in the upper third, representing high perceived physical functioning. 38.4% veterans reported the maximum score (58.2). DASI scores were significantly associated with age (r=-.24, p<.001) and gender ( 2=114.3, p<.01). Conclusion: It may be challenging to use the DASI to assess perceived physical functioning among veterans with psychi- atric concerns. Caution should be used in interpreting the results; it's unclear if this population truly has better physical performance or if there was a reporting bias in our sample. This warrants further investigation via comparison to an objective measure of physical performance. 106 Aditya R. Gandhi, BA, Medicine Changes in Prescription of Antibiotics for Self-treatment of Travelers' LaRocque2,3,6 1Medical Practice Evaluation Center, Massachusetts General Hospital, Boston, MA, USA, 2Division of Infectious Diseases, Massachusetts General Hospital, Boston, MA, USA, 3Harvard Medical School, Boston, MA, USA, 4Biostatistics Center, Massachusetts General Hospital, Boston, MA, USA, 5Division of Global Migration and Quarantine, Centers for Disease Control and Prevention, Atlanta, GA, USA and 6Travelers' Advice and Immunization Center, Massachusetts General Hospital, Boston, MA, USA Introduction: International travelers are often prescribed antibiotics for self-treatment of Travelers' Diarrhea (TD) at pretravel consultations. The Food and Drug Administration (FDA) cautioned against the use of fluoroquinolones for non- serious bacterial infections in 2016 because of adverse events from this antibiotic class. Emerging antibiotic resistance among intestinal pathogens prompted the International Society of Travel Medicine (ISTM) to recommend avoidance of fluoro-quinolones in travelers to Southeast Asia or India in 2017. In this analysis, we sought to describe antibiotic prescriptions for self-treatment of TD in US international travelers evaluated at Global TravEpiNet (GTEN) sites from 2009 to 2018.Methods: We prospectively collected data regarding antibiotic prescriptions for self-treatment of TD from 26 GTEN- associated sites providing pretravel consultations in the United States. We excluded travelers <18 years and not traveling to a WHO-defined region, as well as adults with an existing prescription, antibiotics from another provider, only malaria prophylaxis, or an azithromycin pediatric dose (Figure 1). Prescribing patterns were classified into six groups: no antibiotics, azithromycin only, fluroquinolone only, rifaximin only, other antibiotic only, and >1 antibiotic. The other category included 96prescriptions for metronidazole, trimethoprim-sulfamethoxazole, amoxicillin-clavulanate, and doxycycline (not including prescriptions for malaria prophylaxis). We performed logistic regressions to assess for associations with prescribing antibi - otics (versus no antibiotics) and associations fluoroquinolones (versus azithromycin) in pretravel consulta - tions and specifically to travelers to Southeast Asia or India.Results: Of 103,843 pretravel consultations at GTEN sites from 2009 to 2018, 23,096 (22%) did not 1,051 prescribed >1 antibiotic (1%). (Figure 2A). Antibiotic prescriptions declined from 92% of all consultations in 2009 to 70% in 2018, as did the odds of receiving any antibi- otic over this timeframe all declined from 51% in 2009 to 14% in 2018 and were less likely to be prescribed than azithromycin [OR (95% CI): 0.77 (0.73-0.82); p<0.0001]. Southeast Asia or India, fluoroquinolone prescriptions fell from 36% in 2009 to 4% in 2018 and were less likely to be prescribed evaluated at GTEN sites received pretravel antibiotic prescriptions, but this proportion declined from 2009 to 2018. Consistent with the FDA warning and ISTM advice about fluoroquinolones, fewer of this class of antibiotics were prescribed over time, especially for travelers visiting Southeast Asia or India. Flow diagram of travel encounters included in study. Antibiotic prescriptions among all GTEN travelers to A) all destinations and B) Southeast Asia from 2009 to 2018. 107 Aditya Gandhi, BA, Medicine The clinical impact and cost-effectiveness of routine HIV screening and testing at infant immunization visits in C\u00f4te d'Ivoire (CI), South Africa (SA), and Zimbabwe (Zim) L. Dunning1, M. Penazzato2, A. Gandhi1, D. and A. Ciaranello1,6,7 1Medical Practice Evaluation Center, Massachusetts General Hospital, Boston, MA, USA, 2HIV and Hepatitis Department, World Health Organization, Geneva, Switzerland, 3Harvard T.H. Chan School of Public Health, Boston, MA, USA, 4Center for Health Economics, University of York, York, United Kingdom, 5Institute for Global Health, University College London, London, United Kingdom, 6Division of Infectious Diseases, Massachusetts General Hospital, Boston, MA, USA, 7Harvard Medical School, Boston, MA, USA, 8ICAP, Columbia University Mailman School of Public Health, New York City, NY, USA, 9Institute for Development Studies, Human Development and Health, University of Southampton, Southampton, United Kingdom and 10Medical Research Council Clinical Trials Unit, University College London, London, United Kingdom Introduction: Despite scale-up of prevention of mother-to-child transmission (PMTCT) and early infant diagnosis (EID) programs, many infants at risk for HIV infection are not tested, due to factors including loss to follow-up between antenatal care (ANC) and EID visits, missed maternal HIV testing in ANC, and incident maternal infection after testing. Considering high vaccination rates, screening for HIV exposure and testing for HIV infection at immunization visits may increase the number of infants tested and improve survival among children with HIV. We investigated the economic value of such programs.97Methods: We used the validated Cost-effectiveness of Preventing AIDS Complications (CEPAC)-Pediatric microsimulation model of infant HIV infection, disease progression, diagnosis, and treatment to simulate infants born in 2016 in CI, SA, and Zim. We examined two strategies: EID: Nucleic acid testing (NAT) for infants known to be HIV-exposed and presenting to 6-week EID testing visits; and Screen-and-test : 6-week NAT for infants known to be HIV-exposed, plus rapid diagnostic test (RDT)-based screening of mothers of infants not known to be HIV-exposed at 6-week immunization visits, with referrals to NAT (newly-identified exposed infants) and ART (newly-diagnosed mothers, with reduction in subsequent risk of breast-feeding transmission). Data inputs included maternal HIV prevalence in ANC (CI: 4.8%, assumed linkage to infant NAT was 71% and probability of initiating maternal ART after positive maternal RDT was equal to PMTCT coverage in each country. Risks for opportunistic disease (with potential for diagnosis of previously undiagnosed HIV) and mortality were from published data. Full program costs included $24/infant for NAT, $10/mother for screening, $5-30/month for ART, and $20-180/month for HIV care. Model outcomes included MTCT, life expectancy (LE), lifetime HIV-related costs, and incremental cost-effectiveness ratios (ICERs, from discounted [3%/year] LE and costs). We determined cost-effectiveness by comparing ICERs to two willingness-to-pay (WTP) $1,330). data inputs in sensitivity and threshold analyses. Results: Compared to EID, Screen-and-test was projected to reduce absolute postnatal MTCT by 0.2% in CI, 0.2% in SA, and 0.5% in Zim, due to maternal HIV diagnosis and ART initiation. Screen-and-test increased undiscounted LE among infants ever acquiring HIV by 1.2 years (CI), 1.2 years (SA), and 1.6 years (Zim). Gains in undiscounted LE for all infants (including HIV-infected, HIV-exposed/uninfected, and HIV-unexposed infants) were 0.2 months (CI), 0.6 months (SA), and 0.7 months (Zim), with increases in discounted costs/person of $10 in all settings. The ICERs of Screen-and-test compared to EID were $1,950/YLS (CI), $420/YLS (SA), and $460/YLS (Zim). A one-way sensitivity analysis of key input parameters in SA showed that Screen-and-test remained cost-effective and below the per-capita GDP across all plausible ranges. A one-way threshold analysis of key input parameters in CI revealed that Screen-and-test became cost-effective at the per-capita GDP threshold with lower screening costs, longer breastfeeding duration, higher maternal HIV prevalence, or lower knowledge of HIV status. However, neither plausible nor extended ranges of parameters resulted in an ICER that met the WTP criterion of 2 nd-line vs. 1st-line pediatric ART in CI. Conclusion: Maternal HIV screening at 6-week EPI visits, followed by EID for identified HIV-exposed infants, would improve survival for children with HIV, reduce postpartum HIV transmission, and offer a key opportunity to improve maternal health. Even with high current rates of maternal HIV testing, PMTCT coverage, and EID coverage, Screen-and-test is cost-effective in high-prevalence settings, such as South Africa and Zimbabwe. This strategy may also be of good value in low-prevalence settings with limited maternal HIV testing. One-way sensitivity analysis comparing Screen-and-test to EID in South Africa. One-way threshold analysis comparing Screen-and-test to EID in C\u00f4te d'Ivoire. 108 Ronak G. Gandhi, PharmD, Pharmacy Real-time review of positive blood cultures in hospitalized patients: Leveraging the electronic medical record as Diseases, MGH, Boston, MA, USA Introduction: Untreated and undertreated bloodstream infections can increase risk of morbidity and mortality, infectious sequelae, healthcare costs, and contribute to antimicrobial resistance. We identified a means for real-time monitoring of all positive blood cultures for admitted patients to our institution with the aim to optimize care and minimize complications of 98bacteremia. This project was designed to augment our current electronic medical record (EMR) without incurring costs of a third-party surveillance software. The purpose of this initiative was to identify patients with bacteremia in real-time and assess the impact of daily review by the Antimicrobial Stewardship Program (ASP) team. Methods: The ASP team, in conjunction with hospital informatics and the microbiology laboratory, created a workflow to route real-time alerts of all positive blood cultures for admitted patients to a shared ASP in-basket in the EMR. In-basket alerts were automatically updated with any changes or additions to microbiology or susceptibility reports. This process was initiated in the Fall of 2017 and implemented in February 2018. A member of the ASP team reviewed all positive blood cultures on weekdays until finalized to 1) assess if the blood culture represented a true bacteremia or contamination, 2) ensure the patient was receiving the optimal antimicrobial at the best dose and duration, and 3) recommend consideration for further testing or Infectious Diseases consultation. All recommendations were discussed with the primary provider, Infectious Diseases team (if consulted), and the unit-specific clinical pharmacist. ASP team members maintained daily documentation of the clinical assessment and any interventions made. Clinical interventions were classified as either 'major' or 'optimization.' Major interventions included therapy escalation, bug-drug mismatch, recommending an Infectious Diseases consult or further work-up, initiation of treatment (including for discharged patients) and implementing cost-effective therapy. Optimization interventions included dosing or pharmacodynamic enhancements, therapy de-escalation or consolidation, intravenous to oral conversion, or discontinuation of therapy. All interventions were queried monthly to assess the impact of the new service. Results: Between February 2018 and June 2019, 2351 patients with positive blood cultures were reviewed by the ASP team. Of those, 304 (12.9%) of patients required an intervention by the ASP team, 114 of which (4.8%) were considered major interventions. Major interventions consisted of initiating antimicrobials in patients not on active treatment, notifying primary care providers of culture positivity if the patient had been discharged from the hospital, or escalating therapy based on culture and susceptibility data. The remaining 190 (8.1%) interventions consisted of treatment or dosing optimization, de-escalation based on culture results, and discontinuation of therapy in the setting of likely contamination. Conclusion: Leveraging our EMR as an antimicrobial stewardship tool without reliance on third-party stewardship surveil-lance tools was a cost-effective strategy to optimize patient care for admitted patients with bacteremia. Additionally, this process provides opportunity for other healthcare systems without access to third-party surveillance tools to broaden antimi - crobial stewardship efforts and facilitate appropriate and judicious use of antimicrobial agents while improving patient care. Finally, the real-time nature of this tool represents a major advantage for ASP teams to optimize workflow, communicate with covering providers in a timely manner, and potentially reduce time to optimal antimicrobial therapy including transition to effective outpatient treatments. 109 Wolfgang Ganglberger, MSc, Neurology Sleep Staging with a Wearable Respiratory Device Deep Transfer Learning from an Inductance Plethysmography-based Model W. Ganglberger, H. Sun, R.A. Tesh, Paixao, M.J. Introduction: Electroencephalography (EEG) is used as the main signal to stage sleep. However, a more practical and patient-friendly assessment of sleep is in demand. Here, we assess the efficacy of a wearable respiratory device called AirGo (MyAir LLC). A sleep staging model with the wearable signal will, for example, enable assessing sleep quality in the intensive care unit and can help to investigate the relationships between sleep and delirium. Previously, a deep learning (DL) model, consisting of a convolutional neural network (CNN) and a long short-term memory network (LSTM), was trained to stage sleep for 30-second epochs from abdominal respiratory inductance plethysmography (RIP) signals on a dataset of more 99than 6000 annotated polysomnographies (PSG), recorded at the Massachusetts General Hospital sleep laboratory. The model showed good agreement with human experts on a test set of 1000 PSGs (Cohen's kappa of 0.579). The aims in this research are primarily: 1) Build a model that can stage sleep (Wake, N1, N2, N3, R) from a wearable respiratory signal. 2) Use the existing DL model based on RIP signal and adapt it to the wearable signal ('Transfer Learning'). Methods: 260 PSGs that contain the RIP and wearable respiratory signals are collected at MGH sleep lab, see Figure 1a for a comparison of the different respiration signals for a 35-minute segment. This data is then used to train and evaluate a model. 230 PSGs are used as the training set, 15 as a validation set during training and eventually, 15 PSGs are solely used to test the model's performance. The wearable signal is preprocessed (detrended, filtered) and 270-second segments are used to predict the center 30-second-epoch. The architecture and weights of the RIP DL model are used to start with. Initially, only the first and last hidden layers of the CNN are free to train, followed by all of the parameters of the CNN. Finally, all LSTM parameters are optimized, see Figure 1b for model building scheme. Agreement with the human experts, who annotate the PSG during the sleep study, is measured both with Cohen's kappa and with a confusion matrix. Results: Agreement to human expert (Cohen's kappa) for the task of classifying the five sleep stages is K=0.47, which is moderate. Stages like N1 are inherently difficult to score and classify, as N1 is a often a transition state between wake and N2 sleep and its boundaries are hard to assess. Therefore, the agreement with the human expert scoring super-stages as W+N1, N2+N3, R and W, N1+N2+N3, R is evaluated, which is K=0.61 and 0.62 respectively and can be considered as good. A more detailed view on the model's performance is provided by Figure 2a, showing the confusion matrix. An example of PSG file, a full night of a 73-year-old male, with hypnograms (sleep stage transition through the night) based on both the human expert and the model's prediction, together with exemplary signals for each sleep stage and a spectrogram of the respiration signal, can be seen in Figure 2b. The agreement for this file is K=0.49 and therefore comparable to the overall agreement of the test set. Conclusion: - Transfer learning provides a model to stage sleep with a wearable respiratory device. - The sleep stage-pre - diction model's performance and agreement with human expert is moderate to substantial. - More wearable device data and hyperparameter tuning might further increase performance. - Model can help with more practical and patient-friendly assessment of sleep. - Model can help to investigate the relationships of neurological diseases, delirium and sleep. 110 Anna H. Gao, BSN, RN, Dermatology Feasibility of Using Tethered Capsule Endomicroscopy to Diagnose Disease A.H. Gao1, and Tearney1,2 1Dermatology, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3The Center for Celiac Research and Treatment, Massachusetts General Hospital, Boston, MA, USA Introduction: Celiac disease (CD) is common worldwide, occurring in approximately 1% of the population. However, in the United States, only 17% of that 1% are currently diagnosed, making for a very high rate of underdiagnosis. An estimated 83% of Americans who have CD are undiagnosed or misdiagnosed. Currently, intestinal biopsies of the duodenum obtained via endoscopy are needed to confirm the diagnosis of CD. One of the challenges of obtaining a CD diagnosis is the limita - tions of endoscopic biopsy; endoscopy is not reliable for identifying the affected areas on the gut mucosa, the disease may be patchy and the aberrant areas may be missed by biopsy. In addition, the assessment of the biopsy can be challenging owing to difficulties orienting the specimen so that the cross-sectional architecture is properly observed under the microscope. To address these issues with obtaining a tissue diagnosis of CD, we have initiated a study to evaluate the duodenum using a swallowable, tethered capsule endomicroscope (TCE) that uses optical coherence tomography (OCT) to obtain microscopy images of the entire duodenum in living patients.100Methods: TCE involves swallowing a tethered capsule device that circumferentially scans an OCT beam inside the body as it traverses the gastrointestinal tract. Throughout the procedure, microscopic images are acquired in real time in an unsedated subject. The device is operated by experienced study staff trained in the procedure. Subjects are asked to sip water to facilitate swallowing the capsule. They are also given the options to use an over-the-counter water-based lubricating spray and/or a throat numbing spray to reduce the potential for throat irritation. Once the capsule has been swallowed, it slowly advances through the esophagus, stomach and then the duodenum. On average, it takes 1 hour and 30 minutes for the capsule to reach the duodenum. At the end of the imaging procedure, the capsule operator removes the capsule from the duodenum, stomach and then esophagus by gently pulling the tether ultimately removing the device out through the mouth. Feedback question- naires are conducted after the procedure.Results: To date, we have enrolled 33 subjects (17 healthy and 16 diagnosed CD) in PHRC IRB approved Protocol #2013-P001405. 27 subjects have been imaged with the capsule device (17 healthy and 10 diagnosed CD). The duodenum of 9 out of 14 CD subjects was successfully imaged (4 subjects were unable to swallow the capsule and in 1 subject the capsule did not enter duodenum). In OCT images of a healthy subject's duodenum, distinct finger-like projections of lining cell features were seen (Figure 1A). Flat, scalloped features, consistent with stage 3 Marsh score, were seen in the OCT images of a CD subject's duodenum (Figure 1B). The majority of patients who were diagnosed with CD had a stage 3 Marsh score. Based on the CD subjects that have been imaged, the average overall level of discomfort during the procedure on a scale of 0-10 (0 is no discomfort, 10 is extreme discomfort) was 2.11. 78% of CD subjects preferred the capsule device over endoscopy. No serious adverse events or complications occurred in this study.Conclusion: In this initial cohort of CD subjects, the TCE device was found to be well accepted and preferable to endoscopy. OCT images from our device can show cross-sectional, microscopic architectural features of the entire duodenal wall. The ability to identify and analyze microscopic morphology, including villous blunting, is crucial for the tissue diagnosis of CD. One of the challenges that we have with our current OCT imaging technology is that the microscopic morphology can become distorted when the surface of the duodenum comes in contact with the capsule's surface. To address this challenge, we are currently working developing new methodologies such as instillation of water in the intestinal lumen and the use of spasmolytics to further improve villous visualization. Figure 1: OCT image of healthy duodenum villi and OCT image of CD duodenum villi 111 DADI GAO, Center for Genomic Medicine (CGM) Computational prediction and molecular validation of novel therapeutic targets for potent splicing modulators D. GAO1,2,3, E. Morini1,2, M. Slaugenhaupt1,2 1Center for Genomic Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Neurology, Harvard Medical School, Boston, MA, USA and 3Program in Medical and Population Genetics and Stanley Center for Psychiatric Research, Broad Institute, Cambridge, MA, USA Introduction: While Mendelian genetics has seen remarkable advances in recent years in the discovery of novel genes and pathogenic mechanisms, therapeutic development remains incremental. One strategy for speeding development is to discover novel targets for known compounds. Here we applied a combination of in silico and molecular approaches to identify targets of splicing modulator compounds (SMCs), which target splicing in various diseases. Methods: We treated six independent human fibroblast cell lines from healthy individuals with BPN-15477, one of the SMC compounds originally designed to target ELP1 (IKBKAP ) splicing in familial dysautonomia (FD). We using RNA-seq by assessing all plausible exon 'triplets' (three consecutive exons) transcriptome-wide. We selected exon triplets representing drug-responding and non-drug-responding modules based on splicing changes estimated from RNASeq. We then trained a convolutional neural network (CNN) using corresponding genomic sequences around splice junction of this set of exon triplets, aiming to identify sequence patterns that associate with drug response. Our training model achieved an area-under-curve (AUC) of 0.83 and twelve motifs were considered strongly contribute to drug responses. With 101this trained model, we managed to predict novel therapeutic targets of the drug other than the original FD mutation. To do this, we first applied SpliceAI, a publish algorithm to determine if a mutation would cause splicing disruption, on all pathogenic mutations in the ClinVAR database. Then we applied our model on this filtered list of ClinVAR pathogenic mutations. Results: We identified twelve sequence motifs that determine drug response and our model identified 369 novel pathogenic mutations that lead to splicing deficiency could be rescued by the drug. To validate our predictions, we received patient cell lines from the original researcher reporting three of those mutations. All the validated cells shown exon skipping in the untreated condition while all exon skipping was reverted by the drug treatment. Conclusion: Thus, our method demonstrates the potential impact of coupling machine learning strategies with transcriptional signatures to identify novel targets for known splice modulator drugs. 112 Melissa V. Gates, B.A., Psychiatry Resilience and posttraumatic symptoms in dyads of neurocritical patients and their A. Vranceanu1 1Psychiatry, Massachusetts General Hospital/Harvard Medical School, Boston, MA, USA, 2Neuroscience Intensive Care Unit, Massachusetts General Hospital, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: Admission to the Neuroscience Intensive Care Unit (Neuro-ICU) is often sudden and almost always traumatic for both patients and their informal caregivers (family and friends who provide emotional and instrumental care). Both patients and caregivers are at high risk to develop clinically significant symptoms for post-traumatic stress during the patient's hospitalization. High rates of chronic PTS in ICU patients and Neuro-ICU caregivers has a profound health impact due to negative effects on patients' physical recovery and its persistent course. Resiliency (mindfulness, coping) has positive associations with physical and mental health outcomes. No prior study has examined the long-term trajectory and interde - pendence of PTS following a patient's admission or the potentially protective role of resiliency factors in dyads of patients and caregivers. We aim to explore the relationship between resiliency factors and PTS in dyads of patients admitted to the Neuro-ICU and their informal caregivers. We hypothesize 1) for both patients and caregivers, one's own baseline PTS symptoms and resilience will be associated with his/her own PTS symptoms and 2) for both patients and caregivers, one's own PTS symptoms and resilience at baseline will be associated with his/her partner's baseline and chronic PTS symptoms.Methods: This longitudinal prospective cohort study took place from 2015 to 2016 at the Massachusetts General Hospital in the Neuroscience Intensive Care Unit. 102 patients (89%) and 103 caregivers (90%) completed questionnaires and were included in the analyses. Dyads completed self-report measures for PTS, mindfulness, and coping. Inclusion criteria for patients were: 1) age 18 years or older; 2) English fluency and literacy; and 3) hospitalization to the Neuro-ICU. All patients were cleared for participation by the medical staff. Actor-Partner Interdependence Modeling (APIM), a nested multilevel model, was used to determine actor and partner effects. Actor effects are the degree to which a dyad member's distress or resilience impacted their own long-term distress. Partner effects are the degree to which a partner's status contributes to one's outcome above and beyond their own factors.Results: Clinically significant PTS symptoms were high at baseline in both patients (20%) and their informal caregivers (16%). PTS symptoms remained high through 6 months (25% in patients;14% in caregivers). Actor-partner interdepen- dence modeling demonstrated that severity of PTS symptoms was predictive of PTS symptoms at subsequent time points (ps<0.001), while high mindfulness and coping predicted less severe PTS symptoms in patients and caregivers (ps<0.001) at all time points. Own degree of PTS symptoms at 3 months was predictive of worse PTS symptoms in one's partner at 6 months, for both patients and caregivers (p=0.02).Conclusion: Following a patient's admission to the Neuro-ICU, rates of clinically significant symptoms of PTS were immediately high in dyads and tended to remain stable (caregivers) or increase over time (patients). High levels of PTS symptoms significantly predicted future PTS symptoms for both patients and caregivers. High levels of mindfulness and good coping skills were associated with decreased PTS symptoms at all time points. Long-term PTS symptoms were interde - pendent between patients and caregivers such that one member's symptoms increased likelihood of symptoms in the other member. Findings highlight the need to prioritize assessment and treatment of PTS in Neuro-ICU patients and their informal caregivers through a dyadic approach.102113 Yi Gong, Neurology Intrathecal Delivery of rAAV9-ABCD1 Gong MGH, Boston, MA, USA Introduction: X-linked adrenoleukodystrophy(X-ALD) is a devastating neurological disorder caused by mutations in the ABCD1 gene that encodes a peroxisomal ATP-binding cassette (ABC) transporter. The mouse model of X-ALD with ABCD1 deficiency develops a phenotype similar to adrenomyeloneuropathy (AMN), the most common phenotype of X-ALD. This manifests as spinal cord axonopathy and peripheral neuropathy that is currently untreatable. We previously reported successful transduction of central nervous system cells in vitro and in vivo using recombinant adeno-associated virus serotype 9 (rAAV9) vector for delivery of the human ABCD1 gene (hABCD1). Unfortunately, intravenous delivery in young mice was associated with cardiac toxicity due to transgene overexpression. We therefore set out to optimize delivery to the spinal cord while minimizing systemic leakage using an intrathecal osmotic pumpMethods: AAV9-hABCD1 was into pAAV-CBA vector and packaged into rAAV9 in 293T cells. Intrathecal to 2-3month old Abcd1-/- mice intrathecally (IT) to the spine region between L4-L5. Bolus injection via gas-tight Hamilton syringe attached to a 33-gauge steel needle (2min duration) was compared to an osmotic pump (24h duration) with PBS injection as sham control. Immunofluorescence: Mouse tissue were collected after two weeks of injection and fixed in 4% PFA followed by equilibration in 30% sucrose and then sectioned at 16 m thickness. Tissue sections were processed examined and imaged by regular fluorescence microscope and confocal laser microscope (Zeiss). Western blot: Tissue lysates were prepared RIPA buffer with 1% Halt Protease and Phosphatase Inhibitor ABCD1 antibody using b-ACTIN as loading control. Statistical analysis: Results were expressed as means \u00b1 SEM and analyzed for statistical significance by ANOVA followed by Fisher's protected least-significant difference, based on two side comparisons among experimental groups using the SPSS program. A p<0.05 was considered statistically significant.Results: 1.scAAV9-GFP delivered intrathecally by osmotic pump leads to widespread expression across CNS and dorsal root ganglia (DRG) in a dose dependent manner, with spinal cord and DRG showing higher expression compared to brain. 2.rAAV9-mediated ABCD1 gene transfer via intrathecal osmotic pump leads to widespread gene delivery to CNS with reduced leakage into the systemic circulation compared to intrathecal bolus and intravenous injection. 3. AAV9-ABCD1-HA provides a more reliable way to track ABCD1 expression after in vivo delivery. 4. rAAV9-mediated ABCD1 gene transfer via intrathecal injection targets mainly astrocytes and neurons in the spinal cord and neurons in the DRG. Conclusion: Intrathecal delivery of AAV9-ABCD1 provides a potential method for gen therapy of Adrenoleukodystrophy. 114 Sophie Greenebaum, BA, Psychiatry Psychosis as a predictor of suicidal ideation and action in bipolar disorder S. Greenebaum2, M. Kuperberg2,3, N. George2, Hospital, Somerville, MA, USA, 2Dauten Family Center for Bipolar Treatment Innovation, Massachusetts General Hospital, Boston, MA, USA and 3Medical School, Harvard University, Boston, MA, USA Introduction: Bipolar disorder (BD) impacts approximately 4.5% of the population. Over 50% of patients experience psychotic symptoms during the course of the condition. Previous research indicates a relationship between psychosis in BD and increased suicidal ideation and attempt along with worse BD outcomes. Yet, these studies have not analyzed the different manifestations of increased suicidality in this subpopulation. The purpose of this study was to examine the relationship between psychosis in BD and suicide.Methods: Participants (N=482) were adult outpatients with bipolar disorder enrolled in the Clinical and Health Outcomes Initiative in Comparative Effectiveness for Bipolar Disorder study (Bipolar CHOICE), a 6-month randomized controlled trial to examine the impact of quetiapine versus lithium on bipolar symptoms. The presence of psychotic symptoms was determined by a score of 3 or higher on the \"hallucination\" or \"delusion\" items of the Bipolar Inventory of Signs and Symptoms Scale (BISS). Given these criteria, only 4.35% (N=21) endorsed having psychotic symptoms and therefore, we performed multivariate matching at a ratio of 1:2 with non-psychotic participants. A multiple regression analysis examined whether the existence of 103psychotic symptoms was associated with increased scores on the Concise Health Risk Taking (CHRT-12). The CHRT-12 is a self-report measure that assesses the thoughts and feelings related to suicide, and is composed of two factors: \"suicidal thoughts\" based on three items, (\"I have been having thoughts of killing myself,\" \"I have thoughts about how I might kill myself,\" and, \"I have a plan to kill myself\") and \"propensity\" (including items related to pessimism, helplessness, social support, and despair). Results: We found that current psychotic symptoms significantly predicted the factor \"suicidal thoughts\" on the CHRT, after adjusting for common correlates of suicidality, such as previous suicide attempts and a current major depressive episode ( = 3.06, p < .001, R2 = .341). Secondary analyses showed that the presence of psychotic symptoms was a significant predictor of all three items composing the \"suicidal thoughts\" factor individually. However, psychosis was not a significant predictor of the total CHRT-12 risk score or its second factor, \"propensity.\"Conclusion: These data suggest that participants with current symptoms of psychosis are more likely to endorse thoughts of hurting themselves, suicide methods, and a suicide plan. The small sample size limits the study's findings but warrants further investigation into the relationship of psychosis and suicide in BD. 115 Wyliena Guan, MS, Medicine - Cardiology Development of an Electronic Phenotyping Algorithm for Cardioembolic Research Center and Cardiac Arrhythmia Service, Massachusetts General Hospital, Boston, MA, USA, 2Division of General Internal Medicine, Massachusetts General Hospital, Boston, MA, USA, 3Department of Neurology, Massachusetts General Hospital, Boston, MA, USA and 4Department of Medicine, University of Massachusetts Medical School, Worcester, MA, USA Introduction: Manual adjudication of disease classification is time-consuming and unscalable. The adoption of electronic health records (EHR) enables the development of digital phenotyping approaches which can be automated if sufficiently accurate. In ischemic stroke (IS), subtype classification is important for understanding the mechanism of the event and prevention of recurrent strokes. We sought to apply natural language processing to EHR data along with machine learning methods to automate the identification of cardioembolic strokes, since such strokes typically warrant treatment with oral anticoagulation, which differs from treatment for non-cardioembolic stroke subtypes.Methods: We developed methods to extract individual features comprising the TOAST-definition of cardioembolic stroke from the EHR using a sample of 30,716 individuals from the Partners HealthCare Biorepository. Individual features included mechanical prosthetic valve, mitral stenosis, atrial fibrillation, left myocardial infarction (>4 weeks, <6 months). We used diagnostic codes, procedure codes, and regular-expressions-based algorithms applied to echocardiography reports to identify features. We iteratively refined regular-expressions-based algorithms until a positive predictive value of at least 80% was achieved for each feature. We then applied the algorithms to 1598 IS patients from an independent observational registry in which TOAST subtyping was adjudicated by vascular neurology researchers based on established criteria. Individuals without any cardioembolic stroke features in a 90-day time window around the date of the stroke event, if not otherwise defined by TOAST algorithm, were excluded from the analysis (n = 530). Sensitivity analyses examined the effect of varying time windows. Several methods were applied, some of which were machine learning algorithms. Methods were compared to see which cardioembolic stroke sources most contributed to the presence of cardioembolic stroke per the TOAST criteria, and to create a single classifier for cardioembolic stroke. We performed 10-fold cross validation to assess the accuracy of the single classifier.Results: Preliminary versions of the regular expressions-based algorithms for feature extraction and ascertainment from the Partners HealthCare Biorepository have achieved an average PPV of 95.3%. Of the 1598 included IS patients, 661 were female and the mean age was 73.4 \u00b1 14.0 years. In total, 527 had either possible or definite cardioembolic stroke per the TOAST criteria. 1068 out of 1598 patients had any cardioembolic stroke feature within a pre-defined window around the date of the stroke event. Of 1068 patients, 454 were female and the mean age was 75.7 \u00b1 13.6 years. A logistic regression model demonstrating 68.0% accuracy shows that the features most strongly associated with cardioembolic stroke included atrial fibrillation (OR 2.02, 95% CI 1.17-3.56), hypokinetic left ventricular segment term (OR 0.17, 95% CI 0.09-0.34). Among the different statistical methods applied for the single cardioembolic stroke classifier, random forests had the best performance as determined by the accuracy statistic. The algorithm demonstrated 77.6% accuracy, 64.8% sensitivity, 90.6% specificity, 87.5% positive predictive value, and 71.6% negative predictive value for cardioembolic versus noncardioembolic stroke.104Conclusion: Our study demonstrates that a regular expression-based approach combined with diagnostic and procedure codes can accurately extract cardioembolic features from echocardiograms as well as identify patients with cardioembolic stroke. It also shows that electronic phenotyping of stroke subtypes is feasible using EHR data, though future work is warranted to improve the accuracy of automated cardioembolic stroke classifications. Extraction of features using textual data from the EHR as inputs for automated machine learning approaches shows agreement with manual TOAST classification and thus may be useful for delineating patients with cardioembolic stroke. A portable and accurate cardioembolic stroke algorithm could enable large-scale stroke epidemiology research. 116 Fernando Guastaldi, DDS, PhD, Oral and Maxillofacial Surgery Clear Cell Odontogenic Carcinoma: A Rare Jaw Tumor. A summary of 107 reported cases F. Guastaldi1, W.C. Maxillofacial Surgery, Massachusetts General Hospital, Boston, MA, USA, 2Pathology, Massachusetts General Hospital, Boston, MA, USA and 3Private Practice, Geneva, Switzerland Introduction: Clear cell odontogenic carcinoma (CCOC) is a rare intraosseous tumor of the jaws, characterized as a malignant epithelial odontogenic tumor occurring primarily in the posterior mandible of women in their fifth and sixth decades. Its biologic behavior is distinct from other tumors, benign and malignant. To date, only 107 cases have been reported in the literature since its first description by Hansen et al., in 1985. Given the limited number of cases reported, CCOC remains a largely uncharacterized tumor. CCOC was formerly known as clear cell odontogenic tumor (CCOT) or clear cell ameloblas - toma (CCA). The understanding of the biological and prognostic behavior of CCOC is still challenging due to the rareness of the lesion, resulting in diverse treatment strategies. As additional cases are described, and longitudinal follow-up is reported, the biologic behavior of CCOC continues to be elucidated. The present study aims to review the literature, summarize what is currently known of this rare tumor and create a library/cohort of cases for further evaluation. Methods: PubMed and Springer were used to collect all reports of CCOC. Searches were performed by searching the terms \"clear cell odontogenic carcinoma\", \"CCOC\", or \"clear cell ameloblastoma\". References in the publications were screened and cross-referenced for more cases. Data extracted included: demographics, clinical manifestation, radiologic, treatment, recurrence, histological and immunohistochemical features and follow-up data (as available). Results: The searche resulted in 75 reports detailing 107 cases between 1985 and 2018. Clinically CCOC usually manifests as a swelling in the posterior mandible (n=46), anterior mandible (n=33), and maxilla (n=28). Radiological analysis of 85 cases typically shows a poorly defined expansive radiolucency (n=83). Of the 70 cases that reported symptoms, 44 specified a swelling, 11 tooth mobility, 7 gingival/periodontal issues, 5 numbness, and 3 decreased jaw opening. One patient presented with a neck mass. Duration of symptoms was specified in 52 patients, prior to seeking care: 2 months to 1 year (n=34); 1 to 2 years (n=7); 2 to 4 years (n=2); 4 to 7 years (n=6); and 7 to 12 years (n=3). The incidence of recurrence appears to be 38 of the 88 cases where recurrence was reported. Three CCOC variants have been identified, based on the relative proportion of constituent cell types; biphasic (n=86), ameloblastic (n=13), and monophasic positive for one or more cytokeratin type. The most frequent genomic indicator is the presence of the EWSR1-ATF1 fusion protein. CCOC is most often treated with wide resection, with or without neck dissection.Conclusion: CCOC can be distinguished from other oral cancers by its distinctive histology and immunohistochemical characteristics alongside its radiographic characteristics and less aggressive behavior. Currently treatment should be early and aggressive, leaving clean surgical margins, followed by radio/chemotherapy and long-term follow-up. The overall goal is to collect a cohort of patients. 117 Moytrayee Guha, MPH, Emergency Cost-effectiveness of Implementing the Postpartum Hemorrhage First Response Bundle and Non-surgical Interventions for Refractory PPH in India: An ex ante study Massachusetts General Hospital, Boston, MA, USA, 2Harvard T.H. Chan School of Public Health, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: Despite encouraging declines in recent years, maternal mortality remains high in Uttar Pradesh (UP), India where 200 women die per 100,000 births. Postpartum hemorrhage (PPH) is the most common cause of maternal mortality, responsible for about 3,000 deaths per year in the state. Despite the availability of efficacious preventive and treatment 105strategies, the PPH burden remains high because of low uptake and inconsistent use of these strategies, resulting in limited effectiveness. In recent years, Care Bundles have been used in modern clinical practice to increase adherence to clinical guidelines and improve patient outcomes. The objective of this study was to develop a PPH cost-effectiveness model to estimate the potential health impact and cost-effectiveness of a quality improvement program for PPH management featuring a first response bundle and a set of refractory PPH interventions in primary health clinics (PHC), community health centers (CHC), and district hospitals (DH) in UP, India.Methods: A decision tree model was constructed to compare the status quo delivery of PPH care in UP to two alternative scenarios in which recommended intervention bundles for PPH management were optimally applied (Fig 1). The first alterna- tive is one in which PPH care is strengthened by a program featuring a bundled approach to first response including status quo interventions (IV, UT, UM) and TXA for all PPH cases, plus MRP and SUT when indicated (Fig 2). The second alternative is one in which PPH care is further enhanced through implementation of non-surgical interventions for managing refractory PPH (Fig 2). For each scenario, we conducted a Monte Carlo simulation of cohorts of one million women each, delivering at home, subcenters, PHCs, CHCs and DHs. The results were scaled to represent the annual number of deliveries in UP and their distribution across health facility types, using data primarily from India's National Family Health Survey. The model's main health outcomes were PPH cases, PPH deaths and PPH surgical procedures.Results: Compared to the Status Quo, perfect implementation of Enhanced PPH Care (first response and refractory interven- tions with greater active management of third stage of labor (AMTSL) uptake due to the intervention) would reduce the PPH-related maternal mortality rate in intervention facilities by 98%, from 10.7 to 0.3 per 100,000 deliveries, averting about 450 deaths per year in UP, India. While Enhanced PPH care would increase annual costs associated with AMTSL and non-surgical PPH case management by $190,000 and $300,000 respectively, the overall cost impact would be to reduce service delivery costs by $1.42 million per year, largely because there would be 25,500 fewer surgical procedures. The Strengthened PPH care strategy also would prevent PPH deaths and reduce overall service delivery cost compared to the status quo, but it only has about 66% of the health impact and 63% of the cost-savings of Enhanced PPH care. The up-front program implementation cost and the persistence of the change in PPH management after implementation are uncertain. However, if Enhanced PPH Care were implemented at an investment of $2,300 per facility, and the change in PPH care persisted for at least 2 years, the program would be cost-effective (less than 0.5*GDP per capita per life year gained). If the implementation cost were as high as $4,700 per facility, the program would need to have an effect persisting for 3 years to be cost-effective.Conclusion: Implementation of an Enhanced PPH Care intervention would be cost-effective and life-saving in UP, India. Moreover, Enhanced PPH Care would always realize cost-saving compared to Strengthened PPH Care due to the impact on reduction in number of surgeries needed. Figure 1. Policy Scenarios Figure 2. Modeled PPH Interventions106118 Melanie S. Haines, Medicine - Endocrine-Neuroendocrine Differences in trabecular plate and rod structure in premenopausal women across the weight spectrum M.S. Haines1,2, Unit, Massachusetts General Hospital, BOSTON, MA, USA, 2Harvard Medical School, BOSTON, MA, USA, 3Pediatric Endocrine Unit, Massachusetts General Hospital, BOSTON, MA, USA, 4Cambridge Eating Disorder Center, Cambridge, MA, USA, 5Beth Israel Deaconess Medical Center, BOSTON, MA, USA, 6Wilkins Center for Eating Disorders, Greenwich, CT, USA and 7Eating Disorders Clinical and Research Program, Massachusetts General Hospital, BOSTON, MA, USA Introduction: More plate-like and axially aligned trabecular bone, assessed by individual trabeculae segmentation (ITS), is associated with higher estimated bone strength. Premenopausal women with anorexia nervosa and obesity have elevated fracture risk, but trabecular plate and rod structure has not been reported across the weight spectrum. The objective of this study was to investigate trabecular plate and rod structure in premenopausal women across the weight spectrum. We hypoth-esized that 1) women with anorexia nervosa will have fewer trabecular plates, less axially aligned trabecular bone and less trabecular connectivity compared to healthy lean controls, 2) women with obesity will have more trabecular plates, greater axially aligned trabecular bone and greater trabecular connectivity compared to healthy lean controls, and 3) higher muscle mass, as well as higher serum insulin-like growth factor 1 (IGF-1) and 25OH vitamin D levels, will be associated with superior, while amenorrhea in women with anorexia nervosa will be associated with less favorable, trabecular plate variables.Methods: We performed a cross-sectional study at a clinical research center. We studied 105 women, 21-46 years old: 1) anorexia nervosa (AN)(n=46), 2) controls (HC)(n=29) and 3) eumenorrheic obese (OB)(n=30). Exclusion criteria for all participants included conditions other than anorexia nervosa or obesity that may affect bone or use of medications known to affect bone. The primary outcome was trabecular microarchitecture measured by ITS. Continuous variables were compared among the three groups using Fisher's Least Significant Difference Test. Categorical variables were compared among the three groups using Fisher's Exact Test.Results: Mean age was similar (28.9\u00b16.3y) and (16.7\u00b11.8 vs 22.6\u00b11.4 vs 35.1\u00b13.3 kg/m p<0.0001) across groups. Bone was less plate-like and axially aligned in AN (p0.01), and did not differ in OB, vs HC. After controlling for weight, plate and axial bone volume fraction and plate number density were lower in OB vs HC; some were lower in OB than AN (p<0.05). The relationship between weight and plate variables was quadratic (R=0.39-0.70, p0.0006), i.e. positive associations were attenuated at high weight. Appendicular lean mass and IGF-1 levels were positively associated with plate variables (R=0.27-0.67, p<0.05). After controlling for body weight, 25OH vitamin D deficiency was not associated with trabecular plate or rod variables. Amenorrhea was associated with lower radial plate variables than eumenorrhea in AN (p<0.05). Conclusion: In women with AN, trabecular bone is less plate-like. In obesity, trabecular plates do not adapt to high weight. This is relevant because trabecular plates are associated with greater estimated bone strength. This is the first paper to report differences in trabecular bone structure by ITS in premenopausal women across the weight spectrum, which may contribute to the higher fracture risk in women with anorexia nervosa and obesity. Therefore, interventions targeting trabecular bone structure may have therapeutic potential. Interventions that increase muscle mass or IGF-1 levels may also be able to mitigate some of the adverse effects of low weight or excess adiposity on bone. Table 1. Clinical characteristics in premenopausal women Abbreviations: aBMD- areal bone mineral density. Groups with different superscripts differ significantly at p<0.05. Figure 1. Mean radial and tibial plate bone volume fraction (a), axial bone volume fraction (b), and plate number density (c) were lower in women with anorexia nervosa, and similar in obesity compared to healthy lean controls (p0.01). Mean \u00b1 SEM. After adjustment for total body weight, mean radial and tibial plate bone volume fraction (a), axial bone volume fraction (b), and plate number density (c) were lower in both women with anorexia nervosa and obesity compared to healthy lean controls (p 0.05). Least square mean \u00b1 SEM.107119 Thomas Hank, MD, Surgery - General and Gastrointestinal Pancreatic Fistula in the Era of Neoadjuvant Chemotherapy is an Uncommon Complication but has Major Impact on Long-term Survival T. Hank, M. Sandini, C.R. Ferrone, Lillemoe and C. Fernandez-del Castillo Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Neoadjuvant therapy (NAT) is the standard treatment for patients with borderline and locally advanced pancreatic ductal adenocarcinoma (PDAC). The aim of this study was to assess postoperative complications in patients undergoing pancre-atectomy following NAT, with a particular emphasis on clinically relevant postoperative pancreatic fistula (CR-POPF). We then investigated whether POPF-related morbidity affects long-term outcomes in NAT and upfront resected patients. Methods: Patients who underwent pancreatic surgery for PDAC at the Massachusetts General Hospital between 2007 to 2017 were included. Data on demographics, fistula determinants, pathology and outcomes were collected. Postoperative complications were classified according to the Clavien-Dindo Classification and ISGPS recommendations. Follow-up and survival analyses for NAT and upfront resection groups were performed. Results: 753 patients were identified, of whom 346 received NAT and 407 underwent upfront resection. Patients in the NAT group were younger, had lower BMI and lower CA19-9 levels. NAT was also associated with more favorable post-treatment pathological findings, including smaller tumors, lesser frequency of nodal involvement and higher R0-rates. There were no significant differences in CDC-grade 3 complications or 90-day mortality between the groups. The rate of CR-POPF was 3.6-fold lower in NAT patients compared with the upfront resections. In addition, the determinants of CR-POPF changed after NAT, where only soft pancreatic texture was associated with a higher risk of CR-POPF. While no differences in survival were seen in patients with and without CR-POPF after upfront resection, long-term outcome was significantly reduced in the NAT cohort when CR-POPF occurred. This effect was independent of other established predictors of overall survival on multivariate analysis. Conclusion: Neoadjuvant therapy is associated with a significant reduction in the rate of CR-POPF. However, once this occurs, it is associated with a significant reduction in long-term survival. Additionally, standard determinants of POPF appear to be no longer applicable following NAT. 120 Stephanie M. Hansel, MSc, Emergency Cost-effectiveness of Implementing the PPH First Response and Non-surgical Interventions for Refractory Postpartum Hemorrhage in Kenya: An ex Health Innovation, MGH, Boston, MA, USA and 2Harvard TH Chan School of Public Health, Boston, MA, USA Introduction: Care bundles are used in medical emergencies to improve care to patients in crisis. In this study potential health impact, cost, and cost-effectiveness of interventions were estimated to promote postpartum hemorrhage care consistent with World Health Organization (WHO) guidelines for the management of post-partum hemorrhage (PPH) in Kenya. Methods: Monte Carlo simulation of individual deliveries at home and in 3 levels of public health facilities in Kenya. We modeled 3 levels of facility-based care, hospital, health centers and dispensaries. We scaled our results to all of Kenya and did 108not distinguish between public and private facilities of the same level. Three scenarios were modeled based on PPH severity, to reflect different management strategies (Figure 1). All scenarios assume Active Management of the Third Stage of Labor (AMTSL) for PPH prevention with varying treatment options. Status Quo (SQ) is current practice (which does not include tranexamic acid (TXA)), Strengthened care (ST) is a First Response Bundle of IV, oxytocin, uterine massage, and TXA, and Enhanced care (EN) is First Response Bundle + Refractory Interventions including UBT, aortic or bimanual compression, and NASG as needed.Results: Compared to the Status Quo, perfect implementation of Enhanced PPH care (First Response and Refractory) with enhanced AMTSL uptake due to the intervention would reduce the PPH-related maternal mortality rate in intervention facili - ties from 29.9 to 0.9 per 100,000 deliveries, avoiding 323 deaths per year in Kenya. Because AMTSL and non-surgical PPH care are very inexpensive compared to hysterectomy, the Enhanced PPH Care intervention saves about $10 million per year in service delivery costs. The up-front program implementation cost and the persistence of the change in PPH management after implementation is uncertain. However, if Enhanced PPH Care were implemented at an investment of $2,500 per health care facility, and the change in PPH care persisted at a facility for at least 2 years, the program would be cost-saving (Table 1). Even if the implementation cost were as high as $5,000 per facility, the program would likely be cost-effective (less than 0.5*GDP per capita per life year gained).Conclusion: Implementing an Enhanced PPH Care intervention is likely to be life-saving and good value for money in Kenya. Furthermore, Enhanced PPH Care is always cost-saving compared to Strengthened PPH Care due to the impact on reduction in number of surgeries needed. 121 Amritha Harikumar, MA, Psychiatry Responses to consummatory social reward in psychosis A. Harikumar1, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Boston University, Boston, MA, USA, 4Athinoula A. Martinos Center for Biomedical Imaging, CHARLESTOWN, MA, USA and 5Radiology, Massachusetts General Hospital, Charlestown, MA, USA Introduction: Several neuropsychiatric illnesses are associated with impairments in the ability to experience pleasure (anhedonia), particularly in the social domain, but these deficits are difficult to quantify, and their mechanisms are poorly understood. Thus, in the current study, an experimental paradigm measuring attractiveness ratings of human faces was developed and then administered to 36 healthy control subjects and 28 patients with psychotic disorders. We tested the hypothesis that impairments in the experience and/or anticipation of social rewards are present in individuals with psychotic disorders, and that variation in motivation/anhedonia can be linked to performance on this task.Methods: Face stimuli that were reliably rated as high, low or average in attractiveness (20 stimuli of each category) by an independent group of subjects were selected from a larger pool of stimuli. Participants rated each face (as well as 60 control stimuli presented in separate blocks) on -5 to 5 scale (-5 = very unattractive; 5= very attractive) using a laptop computer. Reward capacity was measured using the Temporal Experience of Pleasure Scale (TEPS) which includes both anticipatory (TEPS-A) and consummatory (TEPS-C) subscales.Results: As expected, we found a main effect of condition across all of the subjects in the attractiveness ratings of the face stimuli (attractive face ratings > average face ratings > unattractive face ratings). Also, a significant group x condition 109interaction was found, which was due to the significantly lower ratings of the attractive faces of the psychotic patients compared to the controls (t = -2.71; p = 0.01). In addition, a TEPS-C x condition interaction (e.g., average, attractive, ugly face ratings, p < 0.0001) was due to the increase in rating of attractive faces and decrease in rating of unattractive faces as TEPS-C scores increased (p<0.0001). Conclusion: These results suggest that this experimental paradigm may quantify aspects of the in-the-moment experience of socially rewarding stimuli and that psychotic patients experience rewarding social stimuli as less pleasurable than healthy individuals. In future studies, these objective outcomes could be used as intermediate endpoints in clinical trials of novel agents for treating impairments in social motivation and negative symptoms in schizophrenia. 122 Tyler Harkness, BS, Medicine - Allergy/Immunology/Rheumatology Serum IgG4 Concentrations Differ According to Race and Sex T. Harkness, X. Fu, Y. Zhang, H.K. Choi, J.H. Stone, K.G. Blumenthal and Z.S. Wallace Rheumatology, Allergy & Immunology, Massachusetts General Hospital, Boston, MA, USA Introduction: Serum immunoglobulin G (IgG) concentrations are integral to the work-up of immune deficiencies and IgG4-related disease (IgG4-RD). Demographic differences in IgG concentrations are poorly described but could influence test interpretation, contribute to racial disparities in primary immunodeficiency diagnosis rates, and explain demographic differences in IgG4 concentrations among IgG4-RD patients. We assessed differences in IgG and IgG subclass concentrations according to sex and race in a large cohort with demographic diversity. Methods: The Partners Research Patient Data Registry (RPDR) was queried to identify patients who had one IgG subclass concentrations measured between January 1, 1989 and March 15, 2018 within the Partners HealthCare Network. Immuno-globulin (Ig) concentration test results, sex, date of birth, self-reported race, billing ZIP code (to estimate socioeconomic status [SES]), and diagnostic billing codes (to determine clinical indication for testing) were extracted. The distributions of IgG and IgG subclass concentrations were compared across race and sex categories using t-tests and ANOVA tests, as appropriate. Multivariate-adjusted differences in IgG and IgG subclass concentrations and the proportion of subjects with results above the reference range across race and sex subgroups were estimated using linear and logistic regression, respectively.Results: Of the 12,851 subjects, the mean age was 54.7 (\u00b118.2) years, were female, 11,673 611 (5%) were Black, and 302 (2%) were Asian. Compared to White patients, Asian and Black patients had higher concen- trations of IgG 53.6 vs. 41.6 mg/dL; p < 0.001) (Figure 1). Female patients had lower IgG4 concentrations (37.4 [\u00b175.2] mg/dL vs 56.3 [\u00b1148.5] mg/dL, p < 0.001) than male patients but there was no significant difference in other IgG concentrations according to sex (Figure 1). In age- and sex-adjusted analyses, Asian subjects had over a 3-fold higher odds (aOR 3.6, 95% CI 2.7-4.7; Figure 2) and Black subjects had a 6-fold higher odds (aOR 6.4, 95% CI 5.3-7.6; Figure 2) of having an IgG above the upper limit of normal (ULN) than White subjects. In age- and race-adjusted analyses, females were 40% less likely than males to have an IgG4 above the ULN (aOR 0.6, 95% CI 0.5-0.6). These differences persisted but were attenuated after adjustment for SES. Our findings were similar when we stratified according to clinical indication.Conclusion: IgG and IgG subclass concentrations differ according to race and sex. These results have implications for the use of Ig concentrations in clinical settings and research. These variations may contribute to differences in the manifestation (e.g., IgG4 concentration elevations in IgG4-RD) and prevalence of various diagnoses (e.g., disparities in immunodeficiency 110diagnoses) across demographic groups. Further studies should confirm our observations in a healthy population, further examine potential genetic and environmental factors, and assess the impact of these observations on diagnosis rates across demographic subgroups. IgG and IgG Subclass Concentrations (mean and standard error) According to Sex and Race (* indicates P<0.05 for comparison across groups) Multivariate-Adjusted Odds Ratio (95% Confidence Interval) of Having an IgG or IgG Subclass Concentration Above (A) or Below (B) the Reference Range According to Race 123 Lauren Harnedy, B.A., Psychiatry Examining a positive psychology intervention for patients with heart failure L. Harnedy1, M.E. Freedman1, C.M. Celano1,2 and J.C. Hospital, Weymouth, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Adherence to health behaviors has been linked to improved health, but many patients with heart failure (HF) struggle to adhere to health behaviors, such as maintaining a low sodium diet and increased exercise. Psycholog-ical well-being has been associated with both improved health behavior adherence and cardiovascular health, and positive psychology (PP) interventions\u2014which involve the promotion of well-being through systematic exercises (e.g., writing a letter of gratitude, performing a kind act)\u2014consistently improve well-being. While PP interventions are effective at improving health behavior adherence in some populations, they have never been studied in patients with HF. We created a 12-week, telephone-delivered, PP-based health behavior intervention for patients with HF. In this three-arm, randomized pilot trial, we aimed to examine the intervention's feasibility, acceptability, and preliminary efficacy in 45 patients with HF and suboptimal health behavior adherence.Methods: Participants were outpatients with New York Heart Association class I, II, or III HF and suboptimal adherence to physical activity, diet, or medications. Following completion of baseline measures, participants were randomized to the PP-based intervention, an MI-based education condition, or usual care. Participants in the two active groups completed weekly phone calls with a study interventionist and received education related to diet, physical activity, and medication adherence. Those in the PP-based group additionally completed weekly PP exercises and set health behavior goals. Follow-up phone calls were conducted at Weeks 12 and 24. Feasibility (primary outcome) was measured by the number of PP sessions 111completed, while acceptability was measured by weekly ratings of ease and utility of the PP sessions. Preliminary efficacy was examined by between-group differences in changes in psychological constructs and health behavior adherence.Results: Of 75 individuals screened for eligibility, 50 enrolled, and 45 were randomized to one of the three groups (M age = 70.86 years, SD = 9.92 years; 82% white; 74% male). Eighty-six percent of participants completed the majority of exercises (M = 10.18 exercises completed). Participants' provided scores for ease of completion (M = 7.5) and utility (M = 7.51) of each exercise. Results show that the intervention is both feasible and with respect to cut-offs. Follow-up collection is currently ongoing, but efficacy results will be presented.Conclusion: Findings suggest that the intervention was feasible and acceptable to patients with HF. If the intervention appears to be effective, we will study it in a large, well-powered trial. Should it be effective, this intervention could be implemented in clinical settings as a way to help patients improve their adherence to health behaviors. 124 Stephanie G. Harshman, Ph.D, Medicine - Endocrine-Neuroendocrine Effects Of Intranasal Oxytocin On The Fasting Serum Proteome Healthy Massachusetts General Hospital, Boston, MA, USA and 2Children's Hospital of Philadelphia, Philadelphia, PA, USA Introduction: The neuropeptide oxytocin (OT) may have metabolic effects, including regulation of energy intake, fat oxidation, and insulin sensitivity. However, the mechanisms underlying the metabolic effects of OT are not well understood. Our objective was to examine proteome-wide alterations in response to intranasal OT in serum of healthy lean and obese men. Methods: 25 healthy men, [13 lean (mean\u00b1 SD body mass (BMI) obese (BMI crossover study of a single-dose of 24 IU intranasal OT. Fasting blood was drawn immediately prior (T0) and at 15, 30, and 55 minutes after OT/placebo administration for proteomic analysis (SOMAScan, SomaLogic, Inc.). The number of proteins and pathways that significantly changed from T0 to the mean post-dose, fasting measurement (T15-T55) in OT vs. placebo was determined. Normalized protein levels were compared between treatment and adiposity groups using rank product testing. Overrepresentation of known pathways containing proteins with significantly di!erent concentrations was evaluated by hypergeometric test. P-values were adjusted by the Benjamini-Hockberg method to determine false discovery rates (FDR). Significance at FDR of 5%. Results: A total of 4,785 proteins were identified in serum of lean and obese men. In response to OT, a total of 710 pathways were altered. In both lean and obese men, OT-responsive biological pathways included regulation of IL6, STAT1 and NFkB), insulin signaling (e.g., G-protein coupled receptor 2 and AKT), leptin signaling (e.g., STAT1, STAT3, and JAK2); and regulation of lipid metabolic processes (e.g., phospho-lipase and protein kinase C). In obese men, OT-responsive pathways included regulation of white adipocyte differentiation (e.g., cyclin kinase) and IL3 signaling and serine/threonine kinase). In lean men, OT-responsive pathways included regulation of the acute inflammatory response (e.g., C-reactive protein and IL6 signal transducer) and lipid localization (e.g., apolipoproteins B, F, and L1).Conclusion: Our results suggest that OT modulates metabolic and inflammatory processes in men, and the specific effects may vary by adiposity status. Further research is required to confirm these exploratory findings. A clearer understanding of the effects of OT on the serum proteome will improve the translational capacity of OT-based therapeutics for use in obesity and other metabolic diseases. 125 Bryan Hayes, Pharm D, Pharmacy Pharmacy/Hospital Medicine Unit Medication History Review Pilot B. Hayes2, D.J. Lucier3, J. Marshall2 and R. Gil1 1Center for Quality and Safety, Process Improvement Dept., MGH, Boston, MA, USA, 2Pharmacy, MGH, Boston, MA, USA and 3Hospital Medicine, MGH, Boston, MA, USA Introduction: Pre-Admission Medication List (PAML) discrepencies are common despite conscientious clinicians trying their best. Reasons include EMR incompatibility across systems, multiple outpatient prescribers, polypharmacy and potential lack of medication awareness or health literacy by our patients. PAML collection also: -Causes duplication of work by pharmacy, nursing and physician staff -Takes significant time to complete -Contributes to burnout due to time constraints -Is not \"top of license\" - does not require an MD, RN or PharmD degree to collect a list of medications -Contributes to 112capacity challenges - time spent by clinical staff on PAML collection is time taken from providing other aspects of clinical care A PAML error can persist through discharge, leading to inefficiencies in the discharge process, and adverse events after discharge. ED Boarder is a subservice of Hospital Medicine designed to care for medicine patients in the ED waiting for beds for greater than 2 hours. Approximately 60% of all general medicine patients go through the ED Boarder service before arriving at their inpatient bed upstairs. Many hospitals around the country utilize pharmacy-trained staff (often technicians) to obtain the PAML, resulting in improved efficiency and reduced medication discrepancies between the patient's pre-ex-isting EPIC list and what they are actually taking. The Hospital Medicine Unit within the Department of Medicine received Frigoletto grant funding to study this intervention on the ED Boarder service in relation to burnout. GOALS -Pharmacy to own all medication histories on admitted ED Boarder patients and to create the best possible medication history, based on at least 2 independent sources per medication (e.g. patient and pharmacy). Any concerns about fidelity to be noted (e.g. Patient says they take metoprolol faithfully but have not filled in months). -Standardize workflow for PAML to decrease discharge med reconciliation errors. -Eliminate the need for the ED Boarder Provider to conduct the Med History and ensure a \"gold standard\" one source of truth Med History prior to patient arriving on the unit. Methods: A pilot was launched in September 2018 aimed at standardizing the creation of the PAML for patients being admitted to Medicine. Workflow PAML collections in the ED A Licensed Pharmacy Intern and PGY-1 pharmacy resident(s) dedicated to the ED Boarder Team - follow a standardized workflow to obtain a complete an accurate PAML Intern/Resident coverage was from September 2018 through May 2019, 5 days per week (M-F, 7AM-8PM) then June 2019 to present 7 days a week. The Intern/Resident completes PAMLs for high risk Boarder patients prior to admission. At least 2 sources from the list below are used to create the best possible PAML: -Patient recall -Medication list provided by patient -Recall or list provided by family/friend -Verified with pill bottles -Called patient's home pharmacy -Called patient's provider office -List Provided by LTAC, Rehab, or OSH -Other PAML reviewed with ED Pharmacist [Findings documented (including all discrepancies) in an EPIC note visible to all A page sent to the clinical team once PAML is completed Data A survey aimed at understanding PAML completion barriers and measuring overall burnout on the ED Boarder service was completed prior to the start of the Pilot. A second survey measuring patient impact of the pilot, such as length of stay, mortality and readmission rate, is underway. Currently the Pharmacy Intern/Residents are gathering data in REDCap that captures: -PAML Collection by role (Pharmacy Intern as primary or secondary, Provider, RN, Other) -PAML validation methods -Number of discrepancies between the patient's pre-existing EPIC list and what they are actually taking when the Pharmacy Intern/Resident completed the PAML before a clinician review -Number of discrepancies between the patient's pre-existing EPIC list and what they are actually taking when the Pharmacy Intern/Resident completed the PAML after a clinician review -Types of discrepancies (e.g. dose change, missing medication, frequency change) Results: The volume of PAML completions is increasing as Pilot awareness continues to grow. PAML completions in REDCap captured from September 2018 - July 2019 demonstrate that: On average 9 PAMLs [LDJ1] for high risk patients are completed daily during the Pilot hours (min of 1 and max of 14 daily PAML completions) 89% of patients had at least one discrepancy from their pre-existing EPIC medication list when the pharmacy intern was the only role group to review/ complete the PAML On average, 2 medication discrepancies[LDJ2] from the patient's pre-existing EPIC list per patient, were captured by the Pharmacy Intern/Resident 79% of patients still had at least one discrepancy from their pre-existing EPIC medication list When the clinician (NP, PA, resident/fellow or attending from either medicine or ED) was the first role group to review/complete the PAMLConclusion: In conclusion, to reduce and correct PAML discrepancies and generate the best possible PAML, dedicated time is needed to complete a PAML accurately (15% of patients need an hour or more to complete), and time is a limiting factor for providers in the ED. We need to balance this time demand in the face of severe capacity issues while best utilizing everyone at \"top of license\" and keeping our patients safe. To avoid PAML discrepancies, we can clearly see the advantage of utilizing a Pharmacy Intern/Resident or Technician who can generate an accurate PAML that will be used throughout the patients hospital stay. 113126 Samantha E. Hines, BA, Psychiatry Association between cannabis use and delusional beliefs in a college student sample S.E. Hines1, W. Deng1, D.C. Massachusetts General Hospital, Charlestown, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Athinoula A. Martinos Center for Biomedical Imaging, Charlestown, MA, USA Introduction: Early-onset cannabis use in adolescents is associated with an increased risk of psychosis. Some studies have also reported an association between cannabis use and levels of subclinical psychotic symptoms. Moreover, there is evidence that high scores on self-report scales measuring subclinical psychotic symptoms predict the later development of psychotic disorders. Based on this literature, the present study further explored the relationship between cannabis use and subclinical psychotic symptoms in college students. Because symptoms of depression often co-occur with subclinical symptoms of psychosis, and these two symptom types are often correlated in severity, we also examined the role of depressive symptoms in these associations.Methods: On-campus mental health screenings were conducted at two Boston-area universities (N= 814). During these screenings, multiple self-report questionnaires were administered, including the 21-Item Peters et al. Delusions Inventory (PDI-short version), a questionnaire that captures the multidimensionality of delusions by assessing delusional distress, preoccupation, and conviction; the Monitoring the Future (MTF) scale, a survey with detailed questions about the use of 12 categories of substances that are commonly abused, including cannabis; and the Beck Depression Inventory (BDI), a 21-item questionnaire that assesses the severity of symptoms of depression. Results: Severity of depressive symptoms and delusion beliefs were both significantly correlated with current frequency of cannabis use. Levels of delusion beliefs (but not of depression), as well as greater delusion-associated conviction, were also linked to the age of first cannabis use, i.e., the earlier the first use of cannabis, the greater the severity of delusional beliefs and degree of conviction. In addition, participants with more depressive symptoms reported a higher frequency of lifetime cannabis use. Conclusion: Elevated levels of cannabis use (current and/or lifetime) are associated with depression and delusional beliefs in college students. In addition, the severity of delusional beliefs (but not of depression) and the level of conviction held about those beliefs are linked to early cannabis use. However, causal relationships cannot be inferred from these cross-sectional analyses. Further studies are needed to explore these findings, in particular to understand how they affect longitudinal health and academic outcomes in this population. 127 Julie A. Hinton, Psychiatry Parasympathetic Activity during REM Sleep, but not REM Sleep Architecture, Predicts Indices of Psychological Distress in Traumatized Individuals J.A. Hinton, K.I. Oliver, C. Daffre, J. Patients with Posttraumatic Stress Disorder (PTSD) have reported various sleep abnormalities, such as increased REM density, decreased slow wave sleep (SWS), greater REM fragmentation, and reduced parasympathetic activity during sleep-all measures indicative of physiological hyperarousal. In this study, we examine whether ambulatory polysomnographic (PSG) features of REM and SWS predicted self-reported indices of psychological distress as well as heart rate variability (HRV) during REM sleep. Methods: Participants' ages ranged from 18-40 and all had been exposed to a DSM-5 PTSD Criterion-A trauma within the past 2 years. Participants for whom relationships between PSG variables and psychological indices were examined consisted of 51-71 individuals (47 female) aged 18-39 (mean=23.605, SD=4.432), while those for whom relationships between PSG and HRV were examined consisted of 47-53 individuals (34 female) aged 18-34 (mean=22.907, SD=4.062). PSG predictor variables included REM density (REMD; number of rapid eye movements per minute REM), average REM period length (inversely associated with REM fragmentation), REM latency, the number of REM periods, and N3 as a percentage of total sleep time (N3%). Following an acclimation night, participants underwent a night of ambulatory polysomnography (PSG) using the Compumedics Somte monitor and records were staged by an experienced, registered PSG technician. To compute REMD, scored sleep recordings were analyzed using the MATLAB program from Yetton et al. (J.Neurosci. Meth., 2016;259; https://osf.io/fd837/). For each participant, REMD for all REM epochs within a night combined were calculated. Among psychological measures, participants completed the Clinician-Administered PTSD Scale (CAPS-5), the PTSD the Hyperarousal scale 114(HAS), the Symptom-Checklist-90 (SCL-90), the WHO Disability Assessment Schedule 2.0 (WHODAS), the Depression, Anxiety and Stress Scale (DASS), the Spielberger State-Trait Anxiety Inventory-Trait (STAI-T), the Difficulties in Emotion Regulation Scale (DERS), the Revised NEO Personality Inventory (NEO-PI-R) and the Connor-Davidson Resilience Scale (CD-RISC). Hyperarousal items from the CAPS-5 and the PCL-5 were combined into a Composite Hyperarousal Index (CHI) and scores on the SCL-90, WHODAS and DASS 5 were combined into a Composite Psychopathology Index (CPI). Measures of parasympathetic outflow were computed from ECG data during scored REM periods using Kubios software and included Average Root Mean Square of the Successive Differences (RMSSD) and High Frequency power (HF Power). Results: PSG variables predicted few of the psychological self-report measures: the number of REM periods was positively associated with PCL hyperarousal scores (R=0.245, p=0.039) and the CHI (R=0.237, p=0.048), and there was negative trend between PDI and N3% (R= -0.259, p=0.061). This lack of physiological to self-report associations contrasted strongly with a high degree of inter-correlation between self-report indices of psychological distress (with 12/15 regressions showing correlation at p<0.01 in predicted directions). REM HRV variables were stronger predictors of self-report measures: with RMSSD (R= -0.294, p=0.033) and showed a HF power (R= -0.237, p=0.088). Conclusion: It has often been suggested, that REM is an emotion regulatory period during sleep and that greater REMD represents a greater degree of CNS arousal during REM. However, REMD poorly predicts indices of psychological distress although it does predict lesser parasympathetic drive (lower RMSSD). Notably, if the number of REM periods is considered a reflection of REM fragmentation, its positive association with the hyperarousal items of the PCL and the CHI may suggest the post-trauma REM fragmentation that has been observed in previous studies. In contrast, lower parasympathetic drive during REM (lower RMSSD and HF Power) does predict higher levels in a number of trauma-related and trait-based indices of waking distress (PDI, NEO-PI, PCL-5). Thus, autonomic features of REM may better reflect neurocognitive disturbances following trauma than the cortical activity associated with higher REMD. 128 William W. Ho, Radiation Oncology Angiotensin Massachusetts General Hospital, Boston, MA, USA, 2Chemical Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA, 3Medicine, Massachusetts General Hospital, Boston, MA, USA, 4Center for Systems Biology, Massachusetts General Hospital, Boston, MA, USA, 5Ragon Institute, Cambridge, MA, USA, 6Hepatology, Nan Fang Hospital, Guangzhou, China, 7Surgery, Massachusetts General Hospital, Boston, MA, USA, 8Surgery, University of Pittsburgh, Pittsburgh, PA, USA, 9Howard Hughes Medical Institute, The Salk Institute for Biological Studies, La Jolla, CA, USA, 10Pathology, Massachusetts General Hospital, Boston, MA, USA, 11Gene Expression Laboratory, The Salk Institute for Biological Studies, La Jolla, CA, USA, 12Radiation Oncology, University Hospital Zurich, Zurich, Switzerland, 13Institute for Oncology and Radiology of Serbia, Belgrade, Serbia, 14Biomedical Engineering, Boston University, Boston, MA, USA and 15Internal Medicine III, Medical University of Vienna, Vienna, Austria Introduction: The resistance to therapy in pancreatic ductal adenocarcinoma (PDAC) is due in part to produce a highly desmoplastic and pro-inflammatory tumor microenvironment (TME). Our labora - tory has shown that the angiotensin receptor blocker (ARB) losartan reduces the CAF-density and desmoplasia, and improves vascular perfusion, and the uptake and efficacy of drugs in animal models of PDAC (Diop-Frimpong et al., PNAS 2011; Chauhan et al., Nature Comm. 2013). These pre-clinical findings led to a phase II trial at Massachusetts General Hospital in locally advanced PDAC patients, which showed that losartan combined with neoadjuvant-FOLFIRINOX chemotherapy followed by chemoradiation more than doubled the resection rate (Murphy et al., JAMA Oncology 2019). Moreover, in a retrospective analysis, we found that the chronic use of angiotensin system inhibitors was independently associated with longer overall survival in non-metastatic PDAC patients (Liu et al., Clin. Cancer Res. 2017). However, how ARBs affect CAFs and desmoplasia in human PDAC remains unknown.Methods: In this study, we performed immunohistochemistry on formalin-fixed paraffin-embedded sections to examine the effect of chronic use of ARB on CAF phenotypes and the extracellular matrix (ECM) in resected PDAC samples from 115untreated patients and patients treated with ARBs to control their hypertension. We also performed transcriptomic analysis of the bulk tumor as well as -SMA+ cells isolated from mice bearing orthotopic PDAC tumors and treated with losartan. Results: From immunohistochemistry of patient PDAC samples, we found that the chronic use of ARBs did not reduce the density of CAFs expressing the fibroblast activation protein (FAP), -smooth derived growth receptor-beta alone. However, ARBs increased the fraction of CAFs expressing both the angiotensin receptor 1 (AT1) and -SMA and decreased the fraction of CAFs expressing both AT1 and FAP. ARBs also reduced the intratumoral levels of suggesting that ARBs reduce desmoplasia in human PDAC. Transcriptomic analysis of mouse PDAC bulk tumor and isolated -SMA+ cells revealed that in both the tumor bulk and -SMA+ cells, losartan \"normalized\" the ECM/cytoskeleton interaction, glycosaminoglycan metabolism and platelet derived growth factor gene sets, and significantly reduced gene transcripts of S100A8 and S100A9. Gene products of S100A8 and S100A9 are S100 proteins that stimulate the development of myeloid-derived suppressor cells. In -SMA+ cells - but not in tumor bulk - losartan reduced cell cycle and muscle contraction gene sets. Conclusion: These results suggest that ARBs can reprogram -SMA+ cells, and normalize the TME in PDAC. In human PDAC, ARBs reduced the accumulation of hyaluronan and number of stromal cells expressing both AT1 and FAP 129 Juliana M. Holcomb, Bachelors of Arts (B.A.), Psychiatry Screening for internalizing problems in outpatient pediatric practice using the Pediatric Symptom Checklist A. Riobueno-Naylor1, J.M. Holcomb1, Murphy1,5 1Department of Psychiatry, Massachusetts General Hospital, Somerville, MA, USA, 2Department of Pediatrics, Massachusetts General Hospital, Boston, MA, USA, 3Department of Pediatrics, Harvard Medical School, Boston, MA, USA, 4The Catholic University of America, District of Columbia, DC, USA and 5Department of Psychiatry, Harvard Medical School, Boston, MA, USA Introduction: Screening for adolescent depression is an important quality metric supported by the American Academy of Pediatrics and other agencies. Many pediatricians use the Pediatric Symptom Checklist (PSC) to screen for overall psychoso- cial problems and the internalizing subscale (PSC-INT) has recently been endorsed as a depression screen. The current study assessed the prevalence of internalizing problems using the PSC-INT and investigated the relationship between screening for depression at the well-child visit (WCV) and behavioral health (BH) service use.Methods: Data were from a mixed socioeconomic status clinic which piloted a system that administers, scores, and stores PSC screens in the Electronic Medical Record (EMR) as a required component of the WCV. Data from patients aged 4-17 whose parents completed a PSC from January 2017 to June 30, 2018 were extracted from the EMR. Results: Of the 3,011 patients who were seen for a WCV, 2,516 (83.6%) completed a PSC and 9.0% (n=226) on the PSC-INT. Of these 226 patients, 14.2% had a history of BH service use six months prior to their WCV. At-risk patients with no previous BH service use were more likely to receive BH services post-WCV (44.3%) compared to those who did not score at-risk (7.0%; p<001). Chart reviews (n=20) showed that PSC-INT score was mentioned in considering next steps in the visit notes of 70% of positive screens.Conclusion: These findings suggest that using an electronic system to screen for internalizing problems with the PSC-INT is feasible and associated with an increase in treatment for youth with internalizing problems.116130 Fatemeh Homayounieh, M.D., Radiology Automatic segmentation CT: Differentiating benign and malignant hepatic lesions F. Homayounieh1, Nitiwarangkul2, R. Singh1, S. Saini1 and M. Kalra1 1Radiology, Massachusetts General Hospital, Chelsea, MA, USA and 2Radiology, Ramathibodi Hospital, Bangkok, Thailand Introduction: Characterization of liver lesions into benign and malignant categories is important for cancer staging and treatment. Multiphase contrast-enhanced CT and/or MRI protocols are often used for the detection and characterization of liver lesions. Multiphase CT for characterizing liver lesions involves image acquisition in the arterial, portal venous and delayed phases of contrast enhancement. It helps reduce the need for invasive imaging-guided or surgical biopsy of the lesions which are often performed to obtain a specific tissue diagnosis for atypical, indeterminate and suspicious lesions. Prior studies have applied radiomics on DECT performed for assessing prognosis and pathologic aggressiveness of lung cancer. To our best knowledge, there are no studies on the application of DECT radiomics for characterization of liver lesions. We hypoth-esized that the single-phase DECT-based radiomics can differentiate benign and malignant liver lesions, thus obviating the need for multiphase CT protocols. Since lesion segmentation is a time-consuming and subjective process for estimating the radiomics of lesions on both single- and dual-energy CT examinations, we used two dedicated prototypes for the study - the Dual Energy Tumor Analysis prototype (DECT-TA, Siemens Healthineers, Germany) for tumor segmentation and DECT feature extraction, and the machine learning-based radiomics prototype (Siemens Healthineers). We assessed both the DECT-TA and radiomics prototypes for differentiating benign and malignant liver lesions seen on DECT. Methods: Our IRB-approved study included 103 adult patients (mean age 65 \u00b1 15 years; 53 men, 50 women) with benign (n= 60) or malignant (n= 43) hepatic lesions on contrast-enhanced dual-source DECT (Siemens Force or Flash). Most malignant lesions had histologic proof; benign lesions were either stable on follow-up CT or had characteristic benign features on MRI. Low and high kV datasets were de-identified, exported offline, and post-processed with the DECT-TA for semiautomatic segmentation of the volume and rim of each liver lesion. For each segmentation, contrast enhancement and iodine concentrations, as well as 585 radiomics features were derived for different DECT image series. Statistical analyses were performed to determine if DECT-TA radiomics can differentiate benign from malignant liver lesions.Results: Iodine concentration, normalized iodine concentrations, mean iodine in the benign and malignant lesions were significantly different (p <0.0001-0.0084; AUC: 0.695 - 0.856). Iodine quantification and radiomics features from lesion rims (AUC up to 0.877) had higher accuracy for differentiating liver lesions as compared to the values from lesion volumes (AUC up to 0.856). Random forest classification yielded higher accuracy for differentiating liver lesions with both the DECT iodine quantification (AUC= 0.91) than DECT perform lesion segmentation as well as classification of lesion rim and volume for quantitative evaluation of iodine concentrations and radiomics features. These attributes enable differentiation of benign and malignant hepatic lesions on contrast-enhanced DECT examinations with high accuracy (AUC of up to 0.91). Furthermore, normalized iodine concentration was as accurate as single-energy and dual-energy CT radiomics for differentiating benign and malignant liver lesions. Iodine quantification and radiomics features in lesion rims are more accurate than the commonly used method of estimating them within an area- or volume-based region of interest. Accuracies of lesion segmentation and radiomics features Accuracies of lesion segmentation and radiomics features for differentiating benign and malignant hepatic lesions with in entire lesion volume, lesion rim and segmented lesion volume. The range suggests minimum and maximum values for different features within each group. An 83-year-old woman with gastrointestinal Viewing Imaging Studies: How Patient Location and Imaging Site Affect Referring Physicians F. Homayounieh1,2, M.K. Kalra1, S.R. Digumarthy1, R. Dreyer1 and T. Schultz3 1Radiology, Massachusetts General Hospital, Chelsea, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Partners Healthcare, Boston, MA, USA Introduction: Modern electronic medical record systems enable radiologists to have access to key clinical information at the time of protocoling and interpreting imaging examinations. Likewise, the referring physicians can access and view radiology reports and studies from these systems. Such bidirectional interaction help improve confidence, expediency, and communi - cation between the providers. With rising use of cross-sectional imaging in clinical diagnosis and patient management, many referring physicians routinely review radiology reports and images of their patients to understand findings and make clinical decisions regarding treatment. Several studies have assessed role of image-rich and structured radiology reports to improve consistency amongst radiologists and help referring physicians recall and analyze important radiology findings [1,2]. Research on radiology reports has also focused on the physician compliance with suggestions or recommendations for follow up or additional imaging in the radiology reports [3]. Others have assessed the efficiency of communication of urgent and actionable radiology findings to the referring physicians. Such investigations help understand and develop optimal ways for delivering radiology reports and studies related information to the referrers [4]. Understanding how the referring physicians and the caretaking team view radiology reports and studies of their patients may guide interpretation priorities for imaging exams based on patient location, imaging site and clinical indications. Knowledge of viewing rates for different clinical indications and imaging tests can help create better ways of communicating and relaying imaging findings to the referring physicians. Thus, the purpose of our study was to if clinical indications, patient location, and imaging sites can predict the viewing pattern of referring physicians for CT and MR examinations of the head, chest, and abdomen.Methods: Our study included 166,949 patients who underwent CT/MR of head/chest/abdomen in 2016-17 in the outpatient (OP, n=83,981 CT/MR), inpatient (IP, n=51,052) and Emergency (ED, n=31,920) settings. There were 125,329 CT/MR performed in the hospital setting and 41,624 in one of the nine off-campus locations. We extracted information regarding body region (head/chest/abdomen), patient location, and imaging site from the electronic medical records (EPIC). We recorded clinical indications, and the number of times referring physicians viewed CT/MR (defined as the number of separate views of imaging in the EPIC). Data were analyzed with Microsoft SQL and SPSS statistical software. Results: About 33% of IP CT & MR studies are viewed >6 times compared to 7% for OP and 19% of ED studies (p<0.001). Conversely, most OP studies (55%) were viewed 1-2 times only, compared to 21% for IP and 38% for ED studies (p<0.001). In-hospital exams are viewed ( 6 views; 39% than off-campus 6 views; 17% studies) (p<0.001). For head CT/MR, certain clinical indications (i.e. stroke) had higher viewing rates compared to other clinical indications such as malignancy, headache, and dizziness. Conversely, for chest CT, dyspnea-hypoxia had much higher viewing rates (>6 times) in IP (55%) & ED (46%) than in OP settings (22%).Conclusion: Imaging examinations for acute clinical indications generate greater viewing frequency in the medical records compared to non-acute indications. For similar exam types and clinical indications, onsite and inpatient imaging exams have higher viewing frequency compared to those performed at offsite imaging sites and/or at outpatient locations. Understanding viewing patterns of the referring physicians and the predictors (exam types, imaging site and patient location at time of imaging) of viewing frequencies can help guide interpretation priorities and communication strategies for imaging exams based on imaging modality, patient location, and clinical indications. This can help deliver efficient patient care. Viewing frequency and imaging equipment location (offsite versus onsite) for head, chest and abdomen CT and MR examinations. Viewing frequency and patient location (inpatient vs outpatient vs emergency department) for head, chest and abdomen CT and MR examinations.118132 Fatemeh Homayounieh, M.D., Radiology Accuracy of machine learning-based radiomics differentiating diffuse liver diseases on non-contrast M.K. 1Radiology, Massachusetts General Hospital, Chelsea, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Diffuse liver diseases, including hepatic steatosis and cirrhosis, are important causes of morbidity and mortality in the United States. Non-alcoholic fatty liver disease affects about 30-40% of the US population (100 million people). The amiodarone hepatotoxicity occurs in 1-3% of patients on chronic oral amiodarone for treatment of refractory ventricular arrhythmias and results in increased CT attenuation number (or density) with or without changes in hepatic enzymes and symptoms. Iron overload in liver, either primary (due to genetic disorder) or secondary (due to multiple blood transfusions or chronic diseases), also causes homogeneous increase in CT attenuation (72 HU) of the hepatic parenchyma on non-contrast abdominal CT with a high specificity (96%) but low sensitivity (63%). To the best of our knowledge, prior studies have not assessed radiomic features for multiple diffuse liver diseases including hepatic steatosis, cirrhosis, amiodarone effect, and iron overload. In this context, our study assesses the accuracy of machine learning (ML)-based radiomics for differenti - ating healthy liver from diffuse liver diseases (cirrhosis, steatosis, amiodarone toxicity, and iron overload) on non-contrast abdomen CT.Methods: Our IRB-approved study included 300 adult patients (mean age 63 \u00b1 16 years; 171 men, 129 women) who underwent non-contrast abdomen CT and had either a healthy liver (n= 100 patients) or evidence of diffuse liver disease (n= 200). The diffuse liver diseases included steatosis (n= 50), cirrhosis (n= 50), hyperdense liver due to amiodarone toxicity (n= 50), and high attenuation liver due to iron overload after multiple blood transfusion (n= 50). All patients had an abdominal MRI or a history compatible with their findings in the medical record system. De-identified DICOM image sets from the PACS workstation were post-processed on an offline Radiomics prototype (Siemens Healthineers). Semiautomatic segmentation of liver was performed in one section at the level of the porta hepatis (in all 300 patients), and then over the entire liver volume in all sections (in 50 patients). The prototype estimated first and higher order radiomics within the segmented portion of the liver and performed statistical comparison of healthy and abnormal liver with multiple logistic regression tests and random forest classifier. The area under the curve (AUC) were estimated for each analysis. Results: With random forest classifier, the AUC for radiomics ranged between 0.72 (iron overload vs. healthy liver) to 0.99 (hepatic steatosis vs. healthy liver) for differentiating diffuse liver disease from the healthy liver. Combined root-mean-square and gray level co-occurrence matrix (GLCM) demonstrated the highest AUC (AUC 0.99, p<0.01) for differentiating healthy liver from steatosis. These features plus small area low gray level emphasis were the most accurate differentiators of healthy liver and cirrhosis (AUC 0.88, p<0.0004). Radiomics were more accurate for differentiating healthy liver from amiodarone (AUC 0.93) than from radiomics of unenhanced healthy from hepatic steatosis, cirrhosis, amiodarone toxicity, and iron overload.119 Entire liver (A, C, E) and single-section (B, D, F) segmentation in 80-year-old woman with healthy liver (A, B), 42-year-old woman with hepatic steatosis and enlarged liver (C, D) and 62-year-old man with alcoholic cirrhosis and nodular liver surface (E, F). Forest plots summarize the most relevant and statistically significant radiomics features for differentiating healthy liver from hepatic steatosis (A), cirrhosis (B), amiodarone use (C) and iron overload (D) (Key: GLCM- gray level co-occurrence matrix, MCC- maximum correlation coefficient, GLSZM- gray level size zone matrix, NGTDM- neighboring gray tone difference matrix, GLRLM- gray level run length matrix, GLDM- gray level dependence matrix, Imc1- informational measure of correlation 1). 133 Fatemeh Homayounieh, M.D., Radiology Comparison of baseline, bone-subtracted, and edge-enhanced radiographs for detection of pneumothorax Digumarthy1 and M. Kalra1 1Radiology, Massachusetts General Hospital, Chelsea, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Ramathibodi Hospital, Bangkok, Thailand Introduction: Pneumothorax is defined as accumulation of trapped air or gas within the pleural cavity and based on severity of the collapsed lung(s), it may present with a wide range of clinical manifestations. It can occur spontaneously or due to blunt or penetration chest trauma or due to iatrogenic causes (medical procedures). Chest X-ray (CXR) is known as a primary initial diagnostic work up test for all patients with thoracic injury to visualize rib fracture, lung contusion, pneumothorax, and hemothorax. Post-processing techniques such as bone subtraction and edge enhancement has been previously assessed for lung findings. Bone suppression images increase accuracy of detection of underlying lung pathologies by suppressing opacity of bones. Radiologists performance has been improved in detection of focal pneumonia on CXRs while interpreting bone suppressed images. Moreover, previous studies have reported an improvement in pulmonary nodules detection of radiologist by subtracting the bone structures from chest radiographs. Chen et al. enhanced dual-energy CXR by subtracting boney structures in ICU patients to visualize pathological findings. Urbaneja et al. used bone subtraction in dual-energy digital radiographs to show the improvement of detected pneumothoraxes. A recent study has reported pneumothorax detection improvement in enhanced post processed images on portable chest radiographs. This study had two limitations. First, they just included portable CXR and just in patients admitted in Intensive care unit. Second, they did not have computed tomography within 72 hours as their gold standard for all their patients. By increasing the accuracy of interpretation of CXR reporting in emergency settings, diagnosis and prompt treatment can be expedited and facilitated. In this study, we aimed to 120assess accuracy of pneumothorax detection in post processed CXRs, both with bone subtraction and edge enhancement with reducing the limitations in previous studies.Methods: Our retrospective IRB approved study included 202 patients (mean age 53 \u00b1 24 years; 132 men, 70 women) who underwent frontal CXR and had trace (<5mm), moderate ( 5mm, <3cm), large ( 3cm), or tension pneumothorax. All patients (except those with tension pneumothorax) had concurrent chest CT performed within 1-72 hours of CXR for clinically indicated reasons. Two radiologists reviewed the CXR and chest CT for pneumothorax on Up CXR (ground truth). All Up CXR were processed to generate B- and E+ images (ClearRead X-ray, Riverain Inc). Two separate thoracic radiologists (R1, R2) sequentially assessed the Up, B- and E+ images and separately recorded the presence of pneumothorax (side, size and confidence for detection) for each image type. Area under the curve (AUC) was calculated with ROC analyses to determine the accuracy of pneumothorax detection.Results: There were 120 right, 95 left, and 13 bilateral pneumothoraces with 53 trace, 87 moderate, 29 large, and 46 tension pneumothoraces. B- images had the lowest accuracy for detection of pneumothorax compared to Up and E+ images (p<0.01). With B-, the sensitivity dropped from 91% to 84% on the right side and 83% to 77% on the left for R1 but remained relatively unchanged for R2 (87% vs 86%). Highest detection rates, and confidence was noted for the E+ images (empiric AUC for R1 and R2 0.95-0.99). No false positive pneumothorax was noted on either B- or E+ images.Conclusion: Enhanced CXRs are superior to bone subtraction and unprocessed radiographs for detection of pneumothorax. Enhanced CXRs improve detection of pneumothorax over unprocessed images; bone subtracted images must be cautiously reviewed to avoid false negatives. Two false negative cases. Top row images (A-C) belong to a 63 years old female with bilateral pneumothorax. Radiologist 1 called right apex falsely negative on the unprocessed and bone-subtracted images (50-75% and 25-50% confidence, respectively), while on the edge enhanced image called it truly positive (50-75% confidence). Second row images (D-F) belong to a 35-year- old man with bilateral apical pneumothorax. Radiologist 1 called bilateral apex falsely negative on the bone-subtracted (25-50% confidence) image, while called it truly positive on the unprocessed and edge enhanced images (50-75% and 75-100% confidence, respectively). Up = unprocessed image, [B-] = bone subtracted image, [E+] = enhanced image. AUC improved bilaterally on enhanced Fatemeh Low-dose, dual-source, dual-energy CT Radiation Dose F. Homayounieh1, C. Nitiwarangkul2, R. Singh1 and M. Kalra1 1Radiology, Massachusetts General Hospital, Chelsea, MA, USA and 2Ramathibodi Hospital, Bangkok, Thailand Introduction: Dual-energy CT is increasingly used for evaluating pulmonary vasculature and thoracic neoplasms. We assessed diagnostic quality, artifacts and radiation dose associated with low-dose, dual energy, dual source CT (DE-DSCT) of the chest. Methods: With IRB approval, we assessed low-dose, contrast-enhanced DE-DSCT of the chest performed in102 patients (23 men, 79 women; 68 \u00b1 16 years; 51 \u00b1 12 kg) performed at 80-140 kV (Siemens Definition Flash, n=64) or 80-150 kV (Definition Force, n=38) using automatic exposure control (Care Dose 4D). We recorded size-specific dose estimates (SSDE), CTDIvol, dose length product (DLP), and average and standard deviation of HU in main pulmonary trunk and thoracic aorta. Two radiologists independently reviewed the following DE-DSCT: perfused blood volume (PBV) and virtual non-contrast (VNC). They recorded all intrathoracic abnormalities and graded diagnostic quality (1= excellent quality, 4= unacceptable), image noise, and artifacts (such as contrast streaking, motion, metallic and beam-hardening artifacts) (1=none, 2= some artifacts that do not limit evaluation, 3= severe artifacts that limit evaluation. The PBV images were assessed for presence of perfusion defects, and VNC images were assessed for calcifications. Data were analyzed with Wilcoxon signed rank and Student t tests.121Results: The average CTDIvol were 3.5 \u00b10.5 (Flash) and 3.7\u00b1 0.3 (Force) mGy (p>0.1). Most frequent and pleural (2/102) had pulmonary emboli with mismatched perfusion defects on the PBV images. None of the studies were deemed as unacceptable for image noise or diagnostic quality. Apart from contrast streaking artifacts at the level of the shoulders which were most frequent on PBV images (31-35/102 patients), there were no major or severe artifacts on monoenergetic, PBV and VNC images. There was substantial interobserver agreement for evaluation of 4 mGy) of the chest with automatic exposure control provides acceptable diagnostic quality and interpretation without major artifacts.Low-dose dual-energy dual-source CT at < 4 mGy is sufficient for diagnostic interpretation in adult patients undergoing routine and pulmonary embolism protocol chest CT. 135 Michael C. Honigberg, Medicine - Cardiology Hypertensive Disorders of Pregnancy and Long-Term Risk Division, Massachusetts General Hospital, Boston, MA, USA, 2Yale University School of Medicine, New Haven, CT, USA, 3Department of Surgery, Massachusetts General Hospital, Boston, MA, USA, 4Cardiovascular Division, Brigham and Women's Hospital, Boston, MA, USA and 5Department of Biostatistics, Boston University School of Public Health, Boston, MA, USA Introduction: History of a hypertensive disorder of pregnancy (HDP, e.g., preeclampsia) among women may be useful to refine atherosclerotic cardiovascular disease risk assessments. However, future risk of diverse cardiovascular conditions in asymptomatic middle-aged women with prior HDP remains largely unknown. Methods: We included women in the prospective, observational UK Biobank aged 40-69 years who reported 1 live birth at enrollment. Cox models were fitted to associate HDP with incident cardiovascular diseases. Causal mediation analyses estimated the contribution of persistent systemic hypertension to observed associations.Results: Of 220,024 women included in our study cohort, 2,808 (1.3%) had prior HDP. Mean (\u00b1SD) age at baseline was 57.4\u00b1 7.8 years, corresponding to mean 31.4\u00b1 10.2 years from first live birth. Women were followed for mean 7 years. Women with HDP had elevated arterial stiffness indices and greater prevalence of hypertension (age-adjusted odds ratio [OR] 11.6, P<0.001) compared to women without HDP. Overall, 7.0 vs. 5.3 age-adjusted incident cardiovascular conditions occurred per 1,000 women-years for women with and without prior HDP, respectively (P=0.001, Figure 1). Prior HDP was associated with greater incidence of CAD (hazard ratio [HR] 1.8, P<0.001), heart failure (HR aortic 2.9, P<0.001), and mitral regurgitation (HR 2.1, P=0.02, Figure 2). In causal mediation analyses, hypertension explained 77% of HDP's association with incident CAD and association with incident aortic stenosis. Conclusion: Hypertensive disorders of pregnancy are associated with accelerated cardiovascular aging and more diverse cardiovascular conditions than previously appreciated, including valvular heart disease. These associations are partially mediated by chronic hypertension.122 Incident cardiovascular disease diagnoses per 1,000 woman-years of follow-up in women with and without prior hypertensive disorders of pregnancy. Incident cardiovascular disease diagnoses include coronary artery disease, heart failure, aortic stenosis, mitral regurgitation, atrial fibrillation and venous thromboembolism. HDP = hypertensive disorder of pregnancy. Hazard ratios for incident cardiovascular diagnoses in women with prior hypertensive disorders of pregnancy. X-axis is presented in log scale. HR = hazard ratio; CI = confidence interval. 136 Michael C. Honigberg, Medicine - Cardiology Association of Premature Natural and Surgical Menopause with Long-Term Incident Cardiovascular Risk M.C. Honigberg1, S.M. 1Cardiology Division, Massachusetts General Hospital, Boston, MA, USA, 2Yale University School of Medicine, New Haven, CT, USA and 3Department of Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Recent guidelines endorse using history of premature menopause (i.e., < age 40 years) to refine atherosclerotic cardiovascular disease risk assessments among middle-aged women, though robust data on cardiovascular disease risk in this population is lacking. Methods: We included women in the UK Biobank who were 40-69 years old and post-menopausal at enrollment. Women with congenital heart disease and missing menopause data were excluded. Premature menopause was defined as last menstrual period or bilateral oophorectomy before age 40. Incident cardiovascular diagnoses were captured from ICD codes. Cox models were used to associate premature menopause with incident cardiovascular diseases.Results: Of 164,112 post-menopausal women in our study cohort, 16,062 (9.8%) had premature natural menopause and 896 (0.5%) had premature surgical menopause. Median follow-up was 7 years. Women with and without premature menopause were 61.0 \u00b1 6.0 and 60.1 \u00b1 5.4 years old at enrollment (p < 0.001), respectively, with mean age at menopause of 35.4 \u00b1 4.4 vs. 50.3 \u00b1 4.2 years (p < 0.001). In analyses of time to first incident cardiovascular diagnosis, premature natural and surgical menopause were associated with hazard ratios (HRs) of 1.3 (95% CI 1.2-1.4, p < 0.001) and 1.8 (95% CI 1.2-2.5, p = 0.001), respectively, after adjustment for age, race, type 2 diabetes mellitus, smoking, hypertension, hyperlipidemia, and prior use of oral contraceptives (OCPs) and hormone replacement therapy (HRT). Premature menopause carried greater hazard of incident CAD, heart failure, mitral regurgitation, atrial fibrillation, and venous thromboembolism, with higher HRs observed for premature surgical vs. natural menopause (Table). Incorporating use of OCPs and HRT did not alter these associations.Conclusion: Premature, particularly surgical, menopause before age 40 is associated with long-term risk of diverse cardio - vascular diseases, which is not explained by use of HRT.123Hazard ratios for incident cardiovascular diagnoses in post-menopausal women with history of premature natural and surgical menopause. Model 1: Adjusted for age and race. Model 2: Adjusted for age, race, prevalent type 2 diabetes mellitus, ever-smoking, prevalent hypertension, prevalent hyperlipidemia, history of oral contraceptive use, and history of hormone replacement therapy use. 137 Michael Hu, Medicine - Primary Care Measuring and Analyzing Physician Work Using Electronic Health Records M. Hu1, S. Eisenstat2, R. Levi3 and W. O'Donnell2 1Operations Research Center, Massachusetts Institute of Technology, Cambridge, MA, USA, 2Massachusetts General Hospital, Boston, MA, USA and 3Sloan School of Management, Operations Management, Massachusetts Institute of Technology, Cambridge, MA, USA Introduction: As electronic health records (EHRs) have become more prevalent and complex over time, physician work has dramatically evolved with physicians spending substantial time completing EHR work. While physicians receive formal training in delivering patient-facing healthcare, they are now tasked with EHR responsibilities (e.g., inbox management, electronic patient communication, digitized billing processes, test and consult ordering, etc.), for which there are rarely well-established best practice workflows. As a result, observational studies indicate that ~50% of physicians' office time is spent on EHR work. Furthermore, many physicians report that their work burden forces them to complete EHR tasks outside of work hours, which has contributed to physician burnout. The effects of physician burnout in the United States have been estimated to cost up to $6.3 billion annually due to physician turnover and reduced clinical hours. The goal of this study is to establish rigorous and reproducible methodology to quantify and analyze primary care physician (PCP) EHR usage by leveraging EHR click-level event logging data. We use these analyses to (1) measure the quantity of EHR work being performed, (2) assess the relationship between EHR activity and time spent on direct patient care, (3) explore the phenomenon of \"after-hours\" work, and (4) quantify the prevalence of EHR work related to cross-coverage. Methods: The study was performed on a heterogeneous cohort of 102 attending PCPs in 12 primary care practices at MGH. Data on these PCPs were retrospectively gathered and analyzed from the time period June 1, 2016 - December 31, 2018. The Epic EHR system automatically keeps an event log of users' activities in the EHR. This log captures users' clicks associated with moving between modules, signing orders, accessing patient charts, etc. For each of these clicks, the event log includes information such as the user ID, a timestamp, a description of the click, and the patient to which the click pertained (if applicable). In this study, we developed algorithms that leverage these event logs to analyze physician workload. In particular, we use the data to quantify the following metrics: (1) the total time individual users are active in the EHR, (2) the amount of time users are working in the EHR for specific patients, and (3) when users are working in the EHR. Results: Observed EHR Time Relative to Expected Clinical Time The EHR time for each PCP is plotted as a percentage against the PCP's expected weekly clinical hours in Figure 1. The PCP represented by Point A was expected to perform ~12 hours of clinical work weekly and averaged 225% of this time (27 hours) in the EHR alone. The median observed EHR time as a percentage of expected work time was 136%, 133%, and 134% in 2016, 2017, and 2018, respectively. Figure 1 also displays the 10 th, 25th, 75th, and 90th percentile of physicians. The 10th percentile of PCPs across all the years indicates that approximtaely 90% of PCPs are logging more time in the EHR alone than their total expected clinical work hours. Lastly, the Kruskal-Wallis test indicates that there are no statistically significant differences in the year-over-year distributions from 2016-2018 (p=0.8206). Time Ratio Between EHR Work and Direct Patient Care We identify that for 50% of PCPs, each hour of direct patient care results in 1.5 hours of additional EHR work (outside of office visits). Time Distribution of EHR Work We find that only 5% of PCPs were able to maintain 20% of their EHR work occurring outside of their clinical hours. Furthermore, 60% of PCPs performed >50% of their EHR activity outside of clinical hours. Cross-coverage EHR Work The median PCP spent 12% of their EHR time on cross-coverage, and 72% of PCPs spent 10% of their EHR time on cross-coverage.124Conclusion: The ubiquity and increasingly complex nature of EHRs have spurred efforts to combat physician EHR burden and burnout. Our study proposes a method for measuring EHR work that can be utilized in health systems that use Epic. We quantify the tremendous EHR burden that PCPs are experiencing, and show that despite significant variability between PCPs, this is a pervasive issue affecting the vast majority of PCPs. Furthermore, there is no indication that this EHR burden has diminished over time. We also propose analytical methods and metrics that may be useful to health systems as they implement and evaluate the efficacy of different types of workflow interventions. 138 Patricia L. Hudson, MD, OB/GYN Professional Burnout Survey for Practicing Urogynecologists: A Cross-Sectional Study P.L. Hudson2, K. James1 and E. Von Bargen2 1Obstetrics and Gynecology, Massachusetts General Hospital, Boston, MA, USA and 2Female Pelvic Medicine and Reconstructive Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Burnout has been increasingly recognized as a maladaptive response to occupational stress that can have devastating consequences for practitioners and patients. The primary aim of this study was to characterize burnout profiles among active practicing members of the American Urogynecologic Society (AUGS) with the intention of guiding future interventions.Methods: This was an anonymous electronic survey of AUGS non-trainee physician members. Basic demographic, personal and professional characteristics were collected. Levels of emotional exhaustion, depersonalization and personal accomplish - ment, as defined by the Maslach Burnout Inventory-Human Services Survey (MBI-HSS), were utilized to categorize partic - ipants into burnout profiles. These profiles included Engagement, Ineffective, Overextended, Disengaged and Burnout. Descriptive statistics and models were used to summarize provider characteristics and to explore differences between the profiles.Results: Of the 1039 active members of AUGS, 280 (26.9%) responded to the survey. The mean age was 48.6 years (range 34-78, SD 8.8), and 56.4% (n=158) were female. Most were married or partnered (87.1%, n=244) and had children (87.9%, n=246). OB/GYN trained (96.8%, n=271) and 50% (n=140) of respondents were in academic practices. Sixty-four percent (n=180) of respondents had been in practice for 10 or more years. Burnout profiles were delineated using the MBI-HSS inventory (Table 1). Forty-three percent (n=105) fit the Engagement profile, while 13% (n=33) fit the Burnout profile. There were no significant differences among the profiles in regard to gender, age, number of years in practice, type of practice or work hours (all P > 0.05). Significant differences were seen in the burnout profiles for physicians with a current mentor (P = 0.016), positive depression screening tool (P < 0.001), those who experienced suicidal ideation (P = 0.018), those who had a feeling of control regarding their schedule (P < 0.001) and those that would become a physician again (P < 0.001) (Table 1). Conclusion: In this cross-sectional study, 13% of active AUGS members fit the Burnout profile on the MBI-HSS inventory. This data suggests that several factors may be associated with different burnout profiles; however, there is limited understanding on how these profiles may affect intervention efforts.125 139 Madeline E. Huff, Anesthesia, Critical Care and Pain Medicine Rescue Transesophageal Echocardiography as a Tool to Improve Cardiac Critical Care Health Literacy in the Intensive Care Unit M.E. Huff1,2, J.C. Crowley2 and K.T. Shelton2 1University of Texas Rio Grande Valley School of Medicine, Edinburg, TX, USA and 2Anesthesia, Critical Care, Pain Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Inadequate health literacy affects more than 90 million Americans and has been associated with adverse outcomes in the medical field, including increased hospitalization rates and greater mortality. Since surgical patients are often required to make complex decisions and adhere to complex instructions, increasing health literacy may have a profound impact in the surgical practice. Using visuals, such as echocardiography, is particularly useful when communicating with patients who may have trouble understanding medical concepts delivered in words. The main objective of the current study was to assess the value of transesophageal echocardiography (TEE) in the cardiac intensive care unit (CICU); particularly to determine if TEE could be used as a tool to improve the experience and health literacy of patients in the critical care setting. Methods: To evaluate the role of TEE in this setting, we selected 25 (and counting) patients undergoing open heart valve repairs/replacements, where we focused on surgeries involving the aortic and/or mitral valve. Prior to discharge from the CICU, pre-and-post-operative TEE echo loops were reviewed with the patients. Patients were assessed with a survey questionnaire before and after the echo loop review to evaluate their understanding of the disease process/surgical procedure, helpfulness in reinforcing their pre-existing knowledge, and whether this process was a valuable component in their periop-erative cardiac management.Results: Of the patients assessed, 92% claimed that they had never seen their TEE images/loops prior to the review meeting. We observed an average 38% increase (p<0.05) in overall patient understanding after explanation of the echo visuals. The overall understanding of the patients after the review meeting was then broken down and analyzed by question; with there being a 36% increase (p<0.05) in correct answers when asked which valve was diseased, 48% increase (p<0.05) when asked about the pathology, and a 32% increase (p<0.05) when asked about the surgical procedure. On a scale of 1-5 (5 being the absolute best), patients rated the echo review an average 4.80 on helpfulness in understanding the medical concepts and an average 4.76 on how valuable the process was to incorporate into a patient's perioperative management.126Conclusion: The echocardiography review displayed a significant increase in patient understanding and knowledge of the disease and surgery. It can be concluded that the patients worked well with visuals accompanied by a medical explanation, especially in the event of a language barrier between the patient and physician. This data suggests that TEE may be helpful in improving the health literacy, management, and overall outcome of patients in the cardiac critical care setting. 140 Kristin Hung, MD, OB/GYN Effect of commercial vaginal products on the growth of uropathogenic and commensal vaginal bacteria K. Hung, P. Hudson, N. Choksi, H. Hesham, A. Bergerat-Thompson and C. Mitchell OB/GYN, MGH, Boston, MA, USA Introduction: Postmenopausal women have high rates of recurrent urinary tract infection, mostly due to Escherichia coli . After menopause, many women lose vaginal colonization by lactobacilli , increasing the risk for infection. Half of postmeno- pausal women experience vaginal discomfort associated with genitourinary syndrome of menopause, for which they often use vaginal products for lubrication. The effect of vaginal products on uropathogenic and commensal vaginal bacteria is poorly understood. We evaluated the effect of five common vaginal products on growth and viability of Escherichia coli and Lactobacillus crispatus, hypothesizing that all products would have a negative effect on bacteria growth.Methods: Broth cultures of laboratory and clinical strains of Escherichia coli and a laboratory strain of Lactobacillus crispatus were co-cultured with methylparaben, lactic acid, KY Jelly, Replens Silky Smooth lubricant, coconut oil, Replens Long-Lasting moisturizer or Trimo-San. Bacteria were also co-cultured with vaginal epithelial cells and selected products. Bacterial growth, measured by change in optical density at 600 nm or change in colony forming units, was compared between co-culture with product and negative saline or cell culture media control using an unpaired t-test or ANOVA, as appropriate. Results: All products except for coconut oil significantly inhibited growth of Escherichia coli (p < 0.02). Laboratory and clinical strains of Escherichia coli responded similarly to products. Only two products (Replens Long-Lasting moisturizer and Trimo-San) significantly inhibited growth of Lactobacillus crispatus (p < 0.01), while the product Replens Silky Smooth stimulated growth (p < 0.01). Co-culture of selected products in the presence of vaginal epithelial cells eliminated the inhibi - tory effects of the products.Conclusion: In vitro exposure to vaginal moisturizing and lubricating products inhibits growth of Escherichia coli. However, the presence of vaginal epithelial cells mitigates this inhibition. Lactobacillus crispatus demonstrated less growth inhibition than Escherichia coli. Further study is needed to understand the effects of vaginal personal care products on the vaginal environment and urogenital health. Change in Escherichia coli growth with exposure to vaginal products. A. Final change in growth, as measured by OD600, for laboratory strain of E. coli is similar to that for representative strain (F) of clinical E. coli across all test conditions (p > 0.05). B. Final change in growth of E. coli , as measured in CFU (in 1:100 diluted product and KSF cell culture media as control), in the absence and presence of vaginal epithelial cells, demonstrating growth in Trimo-San and Replens Long-Lasting moisturizer in the presence of human cells (p < 0.05). Change in Lactobacillus crispatus growth with exposure to vaginal products. A. Final change in growth of L. crispatus, as measured in CFU (simple co-culture, without vaginal epithelial cells), demonstrating inhibition by Replens Long-Lasting moisturizer and Trimo-San (p < 0.01). L. crispatus growth was stimulated by Replens Silky Smooth (p < 0.01). As expected, methylparaben killed L. crispatus (p < 0.01), and lactic acid did not have a significant effect compared with control. B. Final change in growth of L. crispatus, as measured in CFU (in 1:100 diluted product and MRS/KSF cell culture media as control), demonstrating a lack of inhibition by KY Jelly, Replens Long-Lasting moisturizer and Trimo-San, regardless of the presence of human vaginal epithelial cells (p > 0.05).127141 Alina Husain, Medicine - Gastroenterology Association of Regular Aspirin Use and the and Translational Epidemiology Unit, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Harvard T.H. Chan School of Public Health, Boston, MA, USA and 4South Australian Health and Medical Research Institute, Adelaide, SA, Australia Introduction: Previous studies have proposed that the non-steroidal anti-inflammatory drug aspirin may influence how the gut microbiome impacts the development of colorectal cancer, though the mechanism through which aspirin exerts its effects is currently unknown1,2. Methods: We assessed the influence of regular aspirin use (2 times per week) on the gut microbiome of 308 elderly men followed biennially since 1986, who provided updated information on lifestyle and dietary factors, including aspirin use (n=303) and paired stool specimens over a six-month period3. We evaluated the association of regular aspirin use with metagenomically-derived taxonomic and functional profiles derived from these specimens. Using canonical discriminant analysis, we identified the major microbiological discriminators of regular aspirin users (n=184) and non-regular users (n=119). Results: Our regression models revealed that compared to non-regular users, regular aspirin users had a greater relative abundance of Bacteroides, Hungatella and Oscillibacter and a lower relative abundance of Eubacterium, Dorea and Odoribacter. By examining enzymes used by these species, we identified microbiome-wide potential functional associations, with which metatranscriptomic profiles are currently being associated. Conclusion: In the future, these findings will be validated within a recently completed randomized, placebo-controlled trial of aspirin (ASPIRED 4) where MGH participants provided stool samples before and after treatment with aspirin for 8 weeks. References 1Am J Gastroenterol. 2011 Mar; Ingerick, NA, Radiology The Effect of Hydrophilicity/Lipophilicity of Contrast Agents in Tissue-Specific Targeting and Imaging J.B. Ingerick Radiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Partition coefficient (logP) or distribution coefficient (logD), the ratio of concentrations of a compound in a mixture of two immiscible phases at equilibrium, is a very important consideration in pharmaceutical sciences and drug discovery. LogP generally refers to the concentration ratio of un-ionized species of a compound, whereas logD is defined as the concentration ratio of compound in the lipid phase to the concentration of both ionized and un-ionized species in an aqueous phase at a given pH. Dyes in the near-infrared (NIR; 650 - 900 nm) region have received significant attention due to their diverse applications in medical, biomedical, and pharmaceutical fields. LogD value is crucial for determining the solubility of an NIR fluorophore in aqueous solution as well as its biodistribution and targeting. There are several commercially available software to estimate the logP or logD values of NIR fluorophores such as ALOGPS and ChemAxon, however, the final values have a large variation due to the difference in calculation algorithms. In this study, we created a standard operating procedure (SOP) for the experimental measurement of logD values of representative NIR fluorophores and compared them with theoretically estimated values. Methods: 5L of each NIR fluorophore in DMSO stock solution (10 mM) was added in the mixture of phosphate-buffered saline, pH 7.4 (5 mL) and octanol (5 mL). The conical tubes that contain mixtures were vortexted for 30 min and centrifuged at 2,000 g for 3 min. The absorbance spectra of NIR fluorophores in the PBS phase and octanol phase were recorded by using an NIR spectrophotometer. Samples with higher concentration were diluted in each solvent to be less than 5 M in concentration for accurate measurement. Concentrations of tested samples in the PBS and octanol phases were determined by plugging each absorbance value into the Beer-Lambert Law. 4 different concentrations of each fluorophore were used to find a molar extinction coefficient in both PBS, pH 7.4 and octanol. LogD values at a given pH were recorded by logarithm (log 10). Results: Since all the tested fluorophores have green colors at the concentration of 5 M, the distribution phase of each solution was observed clearly by colors. The majority of hydrophobic indocyanine green (ICG) tended to stay in the octanol phase, whereas zwitterionic and charged NIR fluorophores stayed in PBS, pH 7.4. The experimental values were found to be similar to the theoretical values for all three hydrophilic NIR fluorophores, whereas ICG displays somewhat a large difference (4.97 vs. 1.80) due to the unstable equilibrium of ionized species.128Conclusion: The logD of ICG and other representative NIR fluorophores were tested based on the SOP modified from published methods. There is an obvious difference between the tested logD and the simply calculated values. There are no clear patterns found between the values for the positive, negative, and zwitterionic fluorophores. These values will be further used for determining biodistribution and targeting of each fluorophore in animal models. 143 Myrsini Ioakeim Ioannidou, MD, Radiation Oncology A proton and 3RADIATION Dana-Farber Institute, Boston, MA, USA, ONCOLOGY, Johns Hopkins, Baltimore, MD, USA and 5Radiation Oncology, University of Minnesota, Minneapolis, MN, USA Introduction: Malignant central nervous system (CNS) tumors are the most common cancer diagnosed in childhood and adolescence. Primarily arising from the posterior fossa, medulloblastoma is one of the most common histologies of brain tumors. Over the last four decades, improvements in multimodal therapeutic regimens for medulloblastoma has allowed the five-year overall survival (OS) rate to increase from approximately 50% to greater than 70%-85%. Radiation therapy (RT), including craniospinal irradiation (CSI), is a standard component of multimodal treatment medulloblastoma. (PRT) is postulated to improve long-term toxicity for pediatric patients with brain tumors; however, the acute hematologic toxicity of craniospinal irradiation (CSI) in pediatric patients has not been well-characterized. We aimed to examine the hematologic side-effects of proton and photon radiotherapy (RT), in pediatric patients with medulloblastoma receiving CSI plus tumor bed or whole posterior fossa boost as part of standard of care. Methods: Clinical and treatment characteristics were recorded for 97 patients diagnosed with medulloblastoma before 25 years of age who received CSI without concurrent chemotherapy or with concurrent single-agent vincristine from 2007-2017. 60 patients received PRT and 37 received XRT. Patients who received induction chemotherapy before RT were excluded. Clinical and treatment characteristics between the PRT and XRT cohorts were compared using Fisher exact tests or Chi-square tests. Overall survival (OS) was determined by Kaplan-Meier curves with log-rank test. Nadir hematologic toxicity during the 6-week course of RT was graded according to the NCI CTCAE v5. Comparisons of blood counts over 6-week course of RT were conducted using multiple t-tests with Bonferroni corrections.Results: Median age of patients receiving proton and photon CSI was 7.5 years (range: 3.5-22.7) and 9.9 years (range: 3.6-19.5), respectively (p=0.054). Most patients were diagnosed with standard risk medulloblastoma with 86.7% and 89.2% for PRT and XRT cohorts, respectively (p>0.05). Median total dose was 54.0 Gy/CGE and median CSI dose was 23.4 Gy/CGE for both cohorts. 7 patients (11.7%) and 2 patients (5.4%) in PRT and XRT, respectively, did not receive concurrent vincristine (p>0.05). No difference in overall survival was observed. While weekly white blood cell counts did not differ between the PRT and XRT cohorts, patients receiving PRT had significantly higher lymphocyte counts during weeks 1-6 of RT (p<0.01). We also found that 45 patients (76.3%) and all patients (100%) in PRT and XRT cohorts, respectively, had grade 3 lymphopenia (p<0.001). Weekly platelet counts were significantly higher in the proton cohort compared to the photon cohort in weeks 3-6 of RT (p<0.001). 17 patients (28.3%) in the PRT cohort and 17 patients (45.9%) in the XRT cohort had grade 1 thrombocytopenia (p=0.09).Conclusion: In conclusion, our multi-institutional analysis of pediatric patients with medulloblastoma treated with either proton or photon CSI with no concurrent chemotherapy or concurrent single-agent vincristine demonstrates that proton CSI leads to significant reductions in acute hematologic toxicity. Future studies with larger prospective cohorts are needed to better understand the clinical importance of reduced acute hematologic toxicity using proton therapy.129Patient clinical and treatment characteristics Overall survival for proton and photon cohorts with Kaplan-Meier curves. P-value>0.05, log-rank test. 144 Vladimir Ivkovic, Ph.D., Psychiatry NASA Integrated One-Year Mission Protocol: Evaluating Physiological Correlations with Impacted Operational 1Neural Systems Group, Psychiatry, Massachusetts General Hospital, Charlestown, MA, USA, 2International Space University, Illkirch-Graffenstaden, France, 3Center for Space Medicine, Baylor College of Medicine, Houston, TX, USA, 4Translational Research Institute for Space Health, Houston, TX, USA, 5School of Kinesiology, Louisiana State University, Baton Rouge, LA, USA, 6Department of Neurology, Massachusetts General Hospital, Boston, MA, USA and 7Division of Sleep Medicine, Brigham and Women's Hospital, Boston, MA, USA Introduction: The HRP Integrated Plan for Risk Reduction (Rev D) identifies the development of spaceflight-related sleep loss, adverse cognitive or behavioral conditions, altered immune response, and Spaceflight Associated Neuro-ocular Syndrome (SANS) as unsolved risks to the crew on deep space missions. Until very recently, objective and comprehen- sive characterization of sleep quality, cognitive and operational performance, fluid distribution, and immune function in spaceflight were impossible due to lacking technologies for in-flight/analog brain and physiologic monitoring. To address this gap, members of the Neural Systems Group at MGH have been developing portable brain and physiologic monitoring technologies for use in extreme and clinical environments. The latest iteration of the technology\u2014NINscan-M/SE instru- ment\u2014enables a suite of continuous physiological monitoring including imaging of the brain, EEG, ECG, respira- tion, accelerometry, and temperature in a robust mobile fashion which can withstand the challenges of spaceflight. We are designing a research study within the research project titled \"Characterizing the Baselines of Sleep Quality, Cognitive / Operational Performance, Immune Function, and Intracranial Fluids for Deep Space Expeditions\" for NASA's Human Research Program's (HRP) integrated One-Year Mission Project (i1YMP). Using NINscan in tandem with measures of operational performance, the Robotic On-Board Trainer for Research (ROBoT-R) and the Cognition Test Battery, as well as immunological (collaborators at Louisiana State University) and metabolomic studies (collaborators at MGH) during spaceflight will enable correlations between sleep quality (collaborators at MGH and Brigham & Women's Hospital), crew health, and operational performance. The outcomes of the study will contribute to quantification of performance risks associ - ated with human spaceflight, and aid in development of technologies for monitoring and mitigating impacts to crew health.Methods: We will collect data on crewmembers participating in the i1YMP aboard the International Space Station (ISS), and demographically matched control subjects in a spaceflight analog facility. We plan to use an integrated approach in order to achieve the aims of 1) characterizing cognitive task performance changes during the integrated 1 Year Mission Project (i1YMP) on the ISS, 2) characterizing brain and systemic physiology changes during i1YMP on the ISS 3) charac - terizing the effects of sleep duration and architecture on cerebral hemodynamics on ISS and in a spaceflight analog facility, and 4) quantifying the effects of sleep duration and architecture on immune response (Figure 1). Using a longitudinal repeated measures design, we propose to correlate objective measures of sleep quality and quantity including sleep onset latency (SOL), sleep efficiency (SE), sleep fragmentation index (SFI), sleep architecture, etc.) with cognitive (Cognition) and operational (ROBoT-R) assessments, biomarkers of sleep quality and (BDNF, growth hormone, prolactin, 130macroTSH, melatonin, cortisol and chromogranin A), distribution non-invasive measures of intracra - nial pressure. Serum and saliva samples will be collected post sleep opportunity and used to determine how changes in sleep efficiency during the mission affect soluble markers of immune function (pro-inflammatory IL-6 i1YMP has yet to formally begin. However, the NINscan-M/SE technology has been (a) successfully used in pilot sleep studies, (b) validated for sleep quality assessment and staging in HERA (c) validated for functional brain imaging in combination with cognitive and operational assessments within NASA's Standard/Behavioral Core Measures (Cognition) and ROBoT-R, and (d) validated for monitoring intracranial fluid distribution and cognitive performance in parabolic flight and head up/down tilt.Conclusion: As NASA and other space actors plan extended missions to explore and settle other celestial bodies, the ability to quantify the impact of the spaceflight environment on the ability of astronauts to perform mission critical tasks is crucial. Discovering the correlations between environmental stress and operational performance using continuous physiological monitoring may lead to the ability to predict performance using physiological markers. This has broad applications on Earth, especially for people involved in high stress, high impact professions such as surgeons or first responders. Furthermore, the continued testing and validation of the NINscan instrument constitutes a progression in continuous physiological monitoring technology within and outside of clinical settings. Tentative operations and data collection flow for research project \"Characterizing the Baselines of Sleep Quality, Cognitive / Operational Performance, Immune Function, and Intracranial Fluids for Deep Space Expeditions\" within the i1YMP. 145 Susan Jacob, PharmD, Pharmacy Development and Implementation of a Pharmacy and Anesthesia Collaborative Training (PACT) program S. Jacob1, T. Nguyen1, A. Zhu2, M. Harrell2 and J. Oliver1 1Pharmacy, Massachusetts General Hospital, Cambridge, MA, USA and 2Anesthesia, Critical Care and Pain Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: In 2019, Massachusetts General Hospital (MGH) implemented the PACT Program. This training program is a collaboration between the Department of Pharmacy and the Department of Anesthesia, Critical Care and Pain Medicine (DACCPM) and consists of pharmacists, anesthesia providers (anesthesia attendings, anesthesia residents, certified nurse anesthetists), anesthesia technicians, and pharmacy students. The pharmacy students lead the training sessions. They are part of MGH's Longitudinal Advanced Pharmacy Practice (LAPP) program which allows for this longitudinal service to be implemented as LAPP students are on site during the last year of their PharmD program for all six, six-week rotations. This education collaboration satisfies one of the teaching components for students in the LAPP program. The goal of PACT is to enhance patient safety through education for MGH anesthesia technicians on commonly used operating room (OR) medications, and improve communication between anesthesia providers, anesthesia technicians and the Pharmacy team. Currently, most anesthesia technicians that work at MGH do not receive formal education on operating room medications. However, one of their roles is to procure medications for anesthesia providers from automated dispensing cabinets during surgical procedures as anesthesia providers must always remain in the operating room with their anesthetized patient. In addition, current practice utilizes Vocera, a wireless voice communication device, which can sometimes be difficult to hear and can lead to an increased burden of calls and miscommunication between team members. Due to the urgent nature and 131complexity of medications commonly used in the operating room, increased education surrounding medications and formal closed- loop communication training was highly requested by all team members.Methods: The initial content outline is created by the research team through interviews with anesthesia technicians and anesthesia providers. After the initial content outline is established, LAPP students are assigned specific topics. For each topic, LAPP students are asked to create short presentations with the aid of a content expert as well as anesthesia providers and pharmacists. Presentations could also include simulated cases and demonstrations when applicable. Each education session for the anesthesia technicians is approximately two hours long with one hour focused on didactic learning and the second hour to hands- on practice. Results: As part of the study, data is collected on anesthesia technician understanding and comfort of medications after education. This data is being collected by self-evaluation of knowledge and comfort of the discussed medications via anonymous questionnaires. The study is also measuring Vocera call burden to the OR Pharmacy prior to, and after implemen-tation of the teaching sessions. We hypothesize that as anesthesia techs become more comfortable and knowledgeable about the drugs commonly requested perioperatively, we will see a decline in the number of calls to the OR Pharmacy requesting medicines. We acknowledge that this measure is neither specific nor sensitive, as many other calls are made to pharmacy such as questions by providers about mechanisms of action or dosing. Furthermore, many other factors influence whether a provider asks a tech to bring a drug vs. calling the pharmacy directly. We are conducting a two-part anesthesia clinician survey to help us measure closed-loop communication practice and clinician perception of anesthesia technician knowledge. This includes an initial survey conducted near the beginning of the education series, and a follow-up survey afterwards. We are also conducting a pre and post-survey of OR pharmacists to gain insight on how the OR pharmacists support the anesthesia technicians in obtaining the correct medications.Conclusion: The PACT Program allows for improved pharmacological understanding for anesthesia technicians, optimizes communication between members of the OR team and improves safety and efficiency in the OR. It also allows for better understanding of the role of the Pharmacy Department and opens further opportunities for future collaborations. 146 Annemarie D. Jagielo, BA, BS, Medicine - MGH Cancer Center Effect of Inpatient Palliative Care on Supportive Care Measures in Patients Undergoing Hematopoietic Cell Transplantation (HCT) A.D. Jagielo1, MGH, Boston, MA, MA, USA Introduction: Background: Inpatient palliative care integrated with transplant care has been shown to improve patient-re - ported quality of life (QOL), symptom burden, and psychological distress during hospitalization for HCT. However, the impact of palliative care on supportive care practices during HSCT remains unknown. Methods: Methods: This secondary analysis is based on a single-site randomized clinical trial of 160 patients with hemato - logic malignancies undergoing HSCT between 8/2014 and 1/2016. Participants received either inpatient palliative care integrated with transplant care (n = 81) or transplant care alone (n = 79). We used the electronic health record to obtain data on supportive care measures during HSCT including the use of patient-controlled analgesia (PCA), intravenous nausea/anxiety), psychostimulants, antidepressants, hypnotics, and the use of standing orders (as opposed to as needed 'PRN') for supportive care medications. We compared the proportion of subjects in each group receiving these supportive care measures using Fisher's exact test.Results: Results: Patients randomized to the palliative care intervention were more likely to use PCA (32.1% vs. 15.19%, P = 0.015), and atypical antipsychotics (35.8% vs. 17.7%, P = 0.012) compared to those receiving transplant care alone. Intervention participants were also more likely to have standing orders for their supportive care medications (74.1% vs. 56.9%, P = 0.030) compared to those receiving transplant care alone. Study groups did not differ in the of intravenous pain medications, psychostimulants, antidepressants, or hypnotics.Conclusion: Conclusions: Patients receiving inpatient integrated palliative and transplant care were more likely to utilize PCA and atypical antipsychotics during HCT compared to those receiving transplant care alone. Future work should examine whether these differences in supportive care practices mediate the effect of the palliative care intervention on patient-reported outcomes.132147 Isabel Janmey, OB/GYN Characterization of pelvic floor disorders among patients with concurrent Ehlers Danlos syndrome I. Janmey, P. Hudson and E. Von Bargen OB/GYN - FPMRS, Massachusetts General Hospital, Boston, MA, USA Introduction: Ehlers Danlos Syndrome (EDS) is a connective tissue disorder related to variable genetic defects involved with the synthesis and processing of collagen. Symptoms of EDS include skin hyperextensibility, joint hypermobility, and tissue fragility [1]. However, little is known about pelvic floor disorders in women with EDS. Women with connective tissue disorders, including EDS, may be at increased risk for developing pelvic organ prolapse or urinary incontinence [2]. Furthermore, recent evidence has shown that small fiber neuropathy, a common feature of EDS [3], may predispose patients to chronic pelvic pain [4]. The aim of this study was to describe symptoms of pelvic floor disorders in women with Ehlers Danlos syndrome.Methods: We conducted a case series of 37 women who had a diagnosis of EDS and who presented for a urogynecology consultation from January 2004 through June 2019. We used two subscales of the validated condition-specific Pelvic Floor Disorder Inventory (PFDI) to assess pelvic floor symptoms and associated bother: the Pelvic Organ Prolapse Distress Inventory (POPDI-6) and the Urinary Incontinence Distress Inventory (UDI-6). Higher scores indicate more bothersome symptoms [5]. Secondary outcomes included: pelvic organ prolapse as measured by the Pelvic Organ Prolapse Quantification System (POP-Q) [6], symptoms of urinary incontinence, pelvic pain and dyspareunia.Results: We identified 37 women with a concurrent diagnosis of EDS and pelvic floor disorders. All women were Caucasian. The mean age was 42.7 \u00b1 13.9 years. Seven women (18.9%) had one vaginal delivery, and seventeen (45.9%) had never been pregnant. More than half (62.2%) of women reported experiencing dyspareunia. Twenty two women (59.5%) reported symptoms of urinary incontinence; 63.6% reported symptoms inconti - nence, and 13.6% urgency incontinence. Nine women (24.3%) presented with rectal prolapse and 18 (48.7%) had pelvic organ prolapse on exam. Of the women with pelvic organ prolapse, over half (n=12, 66.7%) presented with Stage II pelvic organ prolapse, 22.2% (n=4) with Stage I prolapse, and 11.1% (n=2) with Stage III prolapse. Eleven women (29.7%) presented with symptoms of both urinary incontinence and pelvic organ prolapse. Twelve women (32.4%) had a diagnosis of small fiber neuropathy confirmed by skin biopsy; 32.4% had fibromyalgia; 43.2% had Raynaud's syndrome, and 13.5% had a Sjogren syndrome. Two women (5.4%) had all 4 conditions. The median POPDI-6 scale score was 38 (IQR 25-58), and the median UDI-6 score was 33 (IQR 25-54). Conclusion: Women with EDS may present with a variety of pelvic floor complaints, including pelvic organ prolapse, urinary incontinence, or pelvic pain. More than half of the women with EDS presenting to our outpatient pelvic floor disorder clinic had pelvic organ prolapse, and over half had mixed urinary incontinence. Women with EDS may experience chronic pelvic pain related to concurrent neuropathy or other pain-related syndromes and one may consider a skin biopsy to evaluate for small fiber neuropathy. Identifying connective tissue disease and pain-syndrome comorbidities in women presenting with urogynecology complaints may be helpful in treating patients with recurrent prolapse, urinary incontinence, or pelvic pain that has not responded to initial evaluation and management. Further research, including comparison studies, are needed in order to evaluate the pelvic floor disorders of patients with Ehlers Danlos Syndrome. Table 2: POPDI-6 questions with associated bother rating scale. Table 1: Demographics (N=37) 133148 Nikolaus Jilg, Medicine - Infectious Diseases HIV Controllers Maintain Viral Suppression despite Waning T Cell Responses on ART N. Jilg1,2, P. Garcia Broncano3, M. Peluso4, F. Pereyra5,2, P. Sax5,2 and J. Li5,2 1Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Ragon Institute of MGH, MIT and Harvard, Cambridge, MA, USA, 4University of California, San Francisco, San Francisco, CA, USA, 5Brigham and Women's Hospital, Boston, MA, USA and 6Harvard School of Public Health, Boston, MA, USA Introduction: Robust HIV-specific T cell responses are a hallmark of HIV controllers (HCs). We assessed the impact of antiretroviral therapy (ART) on HIV-specific T cell responses and the ability of HCs to maintain HIV suppression after discontinuation of ART. Methods: A5308 is a prospective, cp/mL for 12 months. HIV-specific T cell responses were measured by intracellular cytokine staining assays in response to HIV gag pool stimulation. Outcomes were evaluated by repeated measures GEE models. In addition, viral load outcomes from HCs in the UCSF SCOPE cohort were included if they had been treated with ART with subsequent VL measurements after ART discontinuation. Results: Thirty-five HCs completed 24 weeks of ART in A5308 and were analyzed. Before ART, higher levels of HIV- specific CD4+ and CD8+ T cell responses were associated with undetectable viremia either by the integrase-single copy assay or the Abbott viral load assay. After 24-48 weeks of ART, significant decreases were observed in a broad range of HIV-specific CD4+ and CD8+ T cell responses. These included CD4+ T cells expressing IFN (-0.32 percentage the percentages of polyfunc-tional HIV-specific CD4+ from the UCSF SCOPE study discon- tinued ART after a median [Q1, Q3] of 33 [25, 65] weeks of treatment. Two of the HCs had detectable VLs immediately preceding ART initiation. In the first 24 weeks after ART discontinuation, only 1 of the 9 HCs had a detectable VL (107 HIV RNA copies/mL).This participant also had the highest pre-ART VL (53 HIV RNA copies/mL).Conclusion: ART significantly reduces both HIV-specific CD4+ and CD8+ T cell responses in HIV controllers. ART did not adversely affect controller status as HIV controllers maintained a low viral load after ART discontinuation. 149 Zexi Jin, Bachelor of Science in Engineering, Radiology Machine Learning to Assess Sex from Chest Radiograph Z. Jin, V.K. Raghu and M.T. Lu Radiology, Massachusetts General Hospital, Boston, MA, USA Introduction: An individual's sex is a critical determinant of health and risk factor for disease. Machine learning can assess prognosis from medical images; determining sex from images is a step towards improving prognostic value. Convolutional neural networks (CNNs) are a type of machine learning technique that has proven particularly useful for image analysis. We developed and tested a CNN to classify sex based on chest radiographs (x-rays), the most common type of imaging in medicine. Methods: A CNN using a modified ResNet34 architecture was developed to classify sex based on a chest radiograph images. The CNN was developed in 41 856 people having lung cancer screening chest radiographs in the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial (PLCO). The final CNN was then tested in independent testing datasets from PLCO (N = 10 464) and the National Lung Screening Trial (NLST; N = 5 497). Area under the receiver operator characteristic curve for the two test datasets are reported. Results: There were 48.4% (5 060/10 464) women in PLCO 44.7% (2 458/5 497) women in NLST test datasets. Mean age was 62.4\u00b1 5.3 yrs and 61.7\u00b1 5.0 yrs, respectively. In the PLCO test dataset, the AUC was 0.9980, 95% CI [0.9970, 0.9989]. In AUC was 0.9376, 95% CI [0.9336, 0.9416].Conclusion: A CNN can accurately classify sex from chest radiograph images. Further research is necessary to determine how CNNs can improve the prognostic value of medical imaging.134150 Bailey Jones, Cancer Center Depression and Anxiety Symptoms in Bereaved Caregivers of Patients with Advanced Cancer B. Jones, J. Greer, V. Jackson, E. Gallagher, M. Kamdar, S. Rinaldi, J. Temel and A. El-Jawahri Massachusetts General Hospital, Boston, MA, USA Introduction: Caregivers of patients with advanced cancer experience substantial caregiving burden and psychological distress during the illness course. However, data on depression and anxiety symptoms in bereaved caregivers and factors associated with their psychological distress are lacking.Methods: We conducted a secondary analysis of 168 caregivers enrolled in a randomized trial of early palliative care integrated with oncology care versus oncology care alone for patients newly-diagnosed with incurable lung and non- colorectal gastrointestinal cancers and their caregivers who completed bereavement assessments at 3 months after their loved one's death. We used the Hospital Anxiety and Depression Scale (HADS) to assess patients' and caregivers' depression and anxiety symptoms at baseline within 8 weeks of diagnosis, and at 3-4 months after the patient's death (for caregivers). We asked caregivers to rate patient's physical and psychological distress in the last week of life on a 10-point scale. We used linear regression adjusting for randomization and cancer type to explore associations between patient and caregiver factors and bereaved caregivers' depression and anxiety. Results: 30.4% (51/168) and 43.4% (73/168) of bereaved caregivers reported clinically significant depression and anxiety symptoms, respectively. Younger patient age (B=-0.06, P=0.041), higher patient baseline anxiety (B=0.28, P=0.002), and caregiver of worse physical (B=0.28, P=0.035) and psychological (B=0.41, P<0.001) distress experienced by the patient at the end of life (EOL) were associated with worse depression symptoms in bereaved caregivers. Only caregiver factors, including age (B=-0.07, P = 0.004), female sex (B=1.60, P = 0.024), and rating of worse psychological distress experienced by the patient at the EOL (B=0.42, P<0.001) were associated with worse bereaved caregivers' anxiety symptoms.Conclusion: Bereaved caregivers of patients with advanced cancer experience substantial psychological distress which is associated with their perceptions of their loved one's distress at the EOL. Interventions to optimize EOL care for patients and reduce bereaved caregivers' psychological distress are needed. 151 Keun-Hwa Jung, MD, PhD, Athinoula A. Martinos Center for Biomedical Imaging Individual risk determination with pathophysiological typing of cerebral white matter signal abnormalities K. Jung1, K. Stephens1, K. Yochim1, J. Salat1 1Radiology, Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Lincoln, MA, USA and 2Department of Psychiatry, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA Introduction: Cerebral white matter signal abnormalities (WMSA) are a significant radiological marker associated with brain and vascular aging. However, understanding their clinical impact is limited due to their pathobiological heterogeneity. In the present study, we aimed to determine the individual risk with pathobiological typing of WMSA. Methods: Healthy volunteers aged > 50 years with moderate to severe WMSA were selected from Human Connectome Project-Aging, a consortium study with a local site at Massachusetts General Hospital, and the subjects' comprehensive clinical and imaging data were obtained. All WMSA variables were measured with a fully automated procedure. WMSA was quantified across a range of parameters including total lesion number, lesion size, and lesion contrast, and individuals were then classified by hierarchical clustering. An array of clinical variables was compared across individual WMSA type using a multinomial logit model. Results: A total of 130 subjects were included for this study (age: 71 [61-81] years, men: 47.7%). Based on the total and regional quantification of WMSA, lesion number, and lesion contrast, three categories of WMSA were detected through clustering. Type I was characterized by multiple, small, lower-contrasted lesions predominantly in the deep WM; type II by large, patch lesions in the periventricular and deep WM; and type III by higher-contrasted lesions only restricted to the juxtaventricular WM. Type II was more prevalent in older subjects with a higher burden of vascular risk factors. While lower total physical activity was associated with a higher risk of type II, vigorous physical activity tended to be associated with a higher risk of type I. A poor sleep quality was associated with a higher risk of type I and III.Conclusion: The healthy older adults with WMSA could be distinguished into one of three types, according to their WMSA phenotypes and clinical risk factors. This new method for identifying classes of WMSA will be important in understanding the pathophysiology underlying heterogeneous WMSA and in determining the impact on brain function.135 Figure 1. Individual classification based on distribution and nature of WMSA (A) Formation of three clusters in 3D scatter plots. Each cluster is specifically located in domains with tv, le.no, pdr, and le.ct. (B) WMSA distribution pattern in each type with WMSA percentage map. (C) WMSA number and volume per lesion. (D) WMSA volume per type. (E) Lesion contrast per type. The horizontal lines within the box indicate the median, while the whiskers indicate the interquartile range. The open circle indicates outliers. *p<0.01, **p<0.001 by the Kruskal-Wallis rank sum test followed by the Dunn test, WMSA WMSA lesion contrast Figure 2. Association between clinical variables and WMSA type The associations of age (A), sleep quality index score (B), and total physical activity levels (C) with the total WMSA volume are plotted for each sex. WMSA type is predicted from a multinomial logit model for a hypothetical subject with WMSA. The stacked-area effect plots display relationships between ASCVD (D), sleep quality index (E), and vigorous physical activity (F) and the predicted probabilities of each type. 152 Homan Kang, PhD, Radiology Renal Clearable Polymeric Nanochelator with Reduced Toxicity H. Kang, M. Jo, W. Stiles, S. Kim and H. Choi Radiology, Mass General Hospital, Charlstown, MA, USA Introduction: Iron overload disorders, including hereditary hemochromatosis and transfusional iron overload (e.g. thalas - semia and myelodysplastic syndrome), increase risk for heart failure, liver cirrhosis, arthritis and diabetes, affecting tens of million people worldwide. Several FDA-approved chelators, such as deferoxamine (DFO), deferiprone, and deferasirox, have been discovered to treat iron overload patients. However, their significant side effects due to non-specific tissue distribution have raised huge healthcare issues. For the amelioration of known DFO toxicity (i.e. gastrointestinal bleeding, kidney failure, liver fibrosis), we develop that renal clearable polymeric nanochelators provide favorable PK/PD as well as safety. Methods: For acute toxicity test, six weeks old CD-1 mice (male; 25-30 g) were intravenously injected with 394 (LD 50 of DFO), 79, and 16 mg kg1 of DFO alone. And, for DFO4-NP administration, we injected DFO-NP (1500, 300, and 60 mg kg1, respectively) that are equimolar to the DFO alone. A body weight, clinical signs (e.g. abnormal gait and posture or muscle tone), and mortality were monitored for 2 weeks. After that, all mice were euthanized, followed by a collection of blood, heart, liver, spleen, and kidneys for the analyses of biochemicals and histology. Results: With the injection of LD50 dose of DFO, all the mice died immediately (n=4). And medium doses of DFO increased the level of lactate dehydrogenase (LDH; marker of tissue damage) by 4.2-4.8-fold (p<0.0001), equimolar DFO-EPL only increased the LDH level by 1.3-2.7-fold (no significances). Similarly, DFO increased levels of aspartate transaminase 1.2-1.9-fold (p<0.05) respectively, while DFO-EPL increase the levels of AST and ALP only by 1.1-1.6 (p<0.05) and 1.2-1.3-fold (no significances) respectively. In H&E staining result of DFO-EPL, no evidences of tissue damage, inflammation, or morphological change in the histologic evalua-tion was found. In contrast, several signs of toxicity, such as perivascular hypercellularity in spleen, inflammatory infiltrate, hyaline casts, and in kidney; and inflammatory infiltrates, intra-alveolar, and intrabronchial deposits in lung, were observed in the DFO alone groups. Conclusion: In effort to develop a safe chelator, we have developed a DFO conjugated -poly-L-lysine (DFO-NP) for the amelioration of known DFO toxicity (i.e. gastrointestinal bleeding, kidney failure, liver fibrosis). In acute toxicity test result, DFO-NP not only improvs therapeutic efficacy but also greatly reduces the DFO toxicity, which provides a promising clinical advancement in safe iron chelation therapy for patients who suffer from iron overload disorders.136153 Julia Karady, MD, Radiology The agreement between three highly sensitive troponin assays by analytical benchmarks and subsequent management recommendations - A comparison in the ROMICAT trials J. Karady1, 1Radiology Department, Massachusetts General Hospital, Boston, MA, USA, 2Knight Cardiovascular Institute, Oregon Health and Science University, Portland, OR, USA, 3Department of Medicine, Tufts Medical Center, Boston, MA, USA, 4Division of Cardiovascular Sciences, National Heart, Lung, and Blood Institute, Bethesda, MD, USA, 5Department of Emergency Medicine, Baylor College of Medicine, Boston, MA, USA, 6Department of Emergency Medicine, Massachusetts General Hospital & Harvard Medical School, Boston, MA, USA and 7Department of Internal Medicine II and Cardiology, University of Ulm Medical Center, Ulm, Germany Introduction: Highly sensitive troponin (hsTn) assays can detect small degree of myocardial injury and may increase efficiency of management of patients presenting with acute chest pain to the emergency department (ED). However, the concordance of different hsTn assays in stratifying patients according to analytical benchmarks and subsequent management recommendations are unknown.Methods: In this cross-sectional study, we included acute chest pain patients who were prospectively enrolled in the ROMICAT (Rule Out Myocardial Infarction/ Ischemia mean age: 52.8\u00b1 10.0 years), who presented to the ED with low to intermediate likelihood of acute coronary syndrome (ACS) and who were referred to further non-invasive diagnostic testing. In ROMICAT I, blood was obtained after 4 hours, and in ROMICAT II at ED presentation, and 2 and 4 hours thereafter. Blood was tested with three state-of-the-art hsTn assays (Roche Diagnostics, Elecsys 2010 platform; Abbott Diagnostics, Abbott ARCHITECT i2000SR; Siemens Diagnos-tics, HsVista). In a per sample analysis, we compared the concordance of assays for analytic benchmarks (below the level of detection [<LOD], percentile, 99th [MI] threshold and above MI [>MI]). In a per patient analysis of ROMICAT II patients with available serial hsTn testing, we determined concordance of patient management recommendations (rule in/rule out/observe) based on 2015 European Society of Cardiology (ESC) guidelines. In addition, we compared the management recommendations with diagnostic test findings (>50% stenosis on coronary CT angiography or ischemia on stress test) and clinically adjudicated endpoints of ACS.Results: In a per sample analysis (n=1027), the proportion of samples classified according to the analytic benchmarks was significantly different across all assays (all p<0.05). Overall, only 34.4% (353/1027) of samples were classified into the same benchmark category by all assays (table 1). In a per patient analysis (n=242), the proportion of patients recommended for discharge and observation after the first hsTn measurement based on 2015 ESC guidelines were significantly different across the three assays (6.6%, 21.1%, (2.9%, 3.3%, 2.1%, all p>0.05). Furthermore, all 3 assays agreed on the same management for each patient only in 25.3% (49/242) patients. The concordance of management recommendations after the second hsTn measure - ment improved significantly to 67.4% (163/242; p<0.001) and was highest for discharge (67.1%), followed by admission (27.7%), and observation (14.4%). For each assay, at least 18.8% (range: 18.8 - 21.0%) of patients in whom discharge was recommended had a positive diagnostic test and at least 2.9% (range: 2.9 - 3.4%) had ACS. In addition, among 71 patients in whom the recommendations differed between assays but in whom the recommendation was discharge by at least one assay, five patients (7.0%) had ACS. Conclusion: The concordance among three different hsTn assays to classify samples within accepted analytical benchmarks and subsequent management recommendations based on 2015 ESC guidelines was limited. About 20% of patients recommended for discharge based on hsTn had obstructive CAD or myocardial ischemia and had ACS. Classification of measurements from three different hsTn assays in 1027 samples patients with acute chest pain according to analytical benchmarks. P<0.05 for LOD=Level of detection. Agreement between assays on a per patient level based on the 1st and 2nd HsTn measurement, when concordant is defined by agreement between all three assays, anything else considered as discordant.137154 Emilia R. Kaslow-Zieve, B.A., Cancer Center Satisfaction with Care among Hospitalized Patients with Cancer E.R. Kaslow-Zieve, I. Wang, E. Van Seventer, C. Azoba, C. D. O'Callaghan, R. R.D. Nipp Massachusetts General Hospital, Boston, MA, USA Introduction: Hospitalized patients with cancer experience high rates of both psychological and physical symptoms, which may influence their use of health care resources and satisfaction with care. However, research investigating symptom burden and satisfaction with care among hospitalized patients with cancer is lacking. In this study, we sought to explore relation - ships among satisfaction with care, physical and psychological symptom burden, and hospital length of stay in hospitalized patients with cancer. Methods: We prospectively enrolled patients with cancer admitted for an unplanned hospitalization at Massachusetts General Hospital from September 2014 to April 2017. Within the first five days of hospital admission, we assessed patients' satisfac - tion with care using two questions from the validated FAMCARE-Patient scale: (1) How satisfied are you with speed with which symptoms are treated? and (2) How satisfied are you with coordination of care? FAMCARE responses include a 5-item Likert scale (very dissatisfied, dissatisfied, undecided, satisfied, and very satisfied). We also assessed patients' physical (Edmonton Symptom Assessment System [ESAS]) and psychological (Patient Health Questionnaire 4 [PHQ-4]) symptom burden. We examined associations between satisfaction with care and patients' physical and psychological symptom burden and hospital length of stay using linear regression, adjusted for age, sex, marital status, Charlson Comorbidity Index (CCI), cancer type, whether the cancer was considered curable based on clinician documentation, time since diagnosis with cancer, and hospital floor of admission (Lunder oncology service vs other hospital floors). We also used linear regression to explore relationships between these sociodemographic and clinical factors and patients' satisfaction scores.Results: We enrolled 1580 of 1749 (90.3%) patients approached, and 1576 (99.7%) completed the FAMCARE satisfaction with care survey. Participants' mean age was 62.8 years, and over half were male (56.1%), married (63.1%), and considered to have a curable cancer by their treating oncologist (50.5%). The mean hospital length of stay was 7.5 days. The majority of patients reported being very satisfied (66.9%) or satisfied (22.1%) with the speed with which symptoms are treated. Similarly, the majority of patients reported being very satisfied (69.8%) or satisfied (20.3%) with coordination of care. Older age, having a cancer considered curable by their treating oncologist, lower CCI score, and being admitted to the Lunder oncology service were associated with higher satisfaction with the speed with which symptoms are treated. Similarly, older age and being admitted to the Lunder oncology service were associated with greater satisfaction with coordination of care. Higher satisfaction with the speed with which symptoms are treated was associated with decreased total psychological symptoms (PHQ4-total: B=-0.31, P=0.003), burden (ESAS-total: B=-2.44, P<0.001). Similarly, higher satisfaction with coordination of care was associated with decreased total psychological symptoms (PHQ4-total: B=-0.30, P=0.005), (ESAS-total: B=-2.75, P<0.001). Patients' satisfaction with the speed with which symptoms are treated (B=-0.47, P=0.026) and satisfaction with coordination of care (B=-0.50, P=0.030) were both associated with shorter hospital length of stay.Conclusion: A large proportion of hospitalized patients with cancer report high satisfaction with the speed with which their symptoms are treated and the coordination of their care. Our findings suggest that older age and being admitted to the Lunder oncology service were associated with higher satisfaction with care, and the mechanisms underlying this finding merit further study. We found that higher satisfaction with care was associated with lower psychological and physical symptom burden, underscoring the importance of efforts to enhance symptom management in this population. Moreover, we found that satisfac - tion with care was associated with shorter hospital length of stay, a hypothesis-generating finding that may relate to these patients also reporting lower symptom burden and thereby not requiring prolonged use of inpatient services.138155 Wataru Katagiri, MEng, MS, Radiology Real-Time Tracking of Vaccine Uptake by Antigen Presenting Cells Using NIR Fluorescence Imaging W. Katagiri, M. Tetrault, H. Kang, S. Kashiwagi and H. Choi Gordon Center for Medical Imaging, Department of Radiology, Massachusetts General Hospital, Boston, MA, USA Introduction: The efficient delivery of vaccines is essential for the induction of the protective immune response. However, as of yet, there remains a paucity of methodological tools to determine the biodistribution of vaccine antigens. Therefore, a new methodology to track the fate of injected vaccine is desired to establish an optimal dose, formulation, and route of clinical vaccines. Here we report a near-infrared (NIR) imaging method using a NIR fluorophore, ZW800-1C, conjugated with different sizes of vaccine antigens that shows numerous advantages for bioimaging of vaccines including minimal interaction with biological tissues and ultralow background, direct conjugation with biomolecules via conventional N-hydroxysuccinimide (NHS) ester chemistry and low toxicity with rapid renal clearance and allows for real-time monitoring of the fate of delivered vaccines in vivo. Methods: Silica nanoparticles with different diameters were conjugated with ovalbumin and ZW800-1C, which has an absorbance peak at 753 nm and emission at 774 nm, to formulate a model vaccine. The NIR fluorescent signal representing vaccine antigen in immune cells in the secondary lymphoid tissue was determined by histology. We also analyzed the uptake of model vaccines using flow cytometry by characterizing dendritic cells (DCs) and DC subsets in skin-draining lymph nodes (LN) at 72 h post-injection.Results: Histological analysis showed that the smallest vaccine was transported through the lymphatic vessels immediately after the injection, while larger vaccines migrated into LN with moderate kinetics (Figure a-g). Migratory DCs (migDCs) migrating into LNs from the skin tissues were defined by MHC class II hiCD11cint expression on flow cytometry analysis. The number of migDCs and their subpopulations positive for a model vaccine with 7.1 nm in diameter and smaller silica nanopar-ticles with 183 nm consistently showed a statistically significant increase compared to those for larger silica nanoparticles with 277 nm (a model vaccine vs. larger silica nanoparticles, P = 0.0450; smaller vs. larger silica nanoparticles) (Figure h). These results show that CD11c int MHC-IIhigh migratory dendritic cells were the major carrier of the large vaccine antigens, which is well agreeable with the literature. Conclusion: Together, these results demonstrate that the conjugation of the ultrasmall ZW fluorophores does not signifi- cantly alter the behavior of model vaccines and their immunological events in LN. This methodology can be broadly used for optimization of formulations and safety evaluation of clinical vaccines. Figure 1. Histological analysis of vaccine uptake by APCs in the draining LNs. Uptake of the model vaccines in APCs was assessed by histology 72 h post-intradermal injection. lines depict capsule and subcapsular sinus (SCS) of LNs. Scale bar = 100 m. g) Quantitative analysis of areas positive for fluorescence in SCS and cortex of LNs injection (a-g, n = 6, mean \u00b1 SEM.). h) cell count of OVA-ZW migDCs. (n = 4, median \u00b1 95% confidence interval). A P value of less than 0.05 was considered significant: *P < 0.05; **P by Tukey's multiple comparison test.139156 Raviv Katz, Masters of Science, Massachusetts Eye and Ear Infirmary (MEEI) - Ophthalmology Association Between Choroidal Indices and Visual Outcomes in Patients with Diabetic Macular Edema Treated with Anti-VEGF and Ear Infirmary, Boston, MA, USA, 2Association for Innovation and Biomedical Research on Light, Coimbra, Portugal, 3Retina, Massachusets Eye and Ear Infirmary, Boston, MA, 4Department Coimbra, Portugal, 6Coimbra Institute for Clinical and Biomedical Research. Faculty of Medicine, University of Coimbra, Coimbra, Portugal and 7Faculty of Medicine, University of Coimbra, Coimbra, Portugal Introduction: To evaluate the changes in choroidal vasculature features with anti-angiogenic therapy in patients with diabetic macular edema (DME), and their association with visual outcomes, using Swept-Source Optical Coherence Tomography (SS-OCT). Methods: A prospective, longitudinal study, including consecutive patients with treatment-naive DME. All patients received monthly intravitreal injections of ranibizumab for 3 months (loading dose), followed by a treat-and-extend regimen for a total of 12 months. For all participants, best-corrected visual acuity (BCVA) (ETDRS) and 3D horizontal volume macular SS-OCT scans were obtained before the first injection (M0), 1 month after the loading dose (M3), and at 6 (M6) and 12 months (M12). CCT was obtained using automated software, as the mean value in the central 1mm of the ETDRS grid. En face SS-OCT images of the choroidal vasculature were binarized to calculate choroidal vessels density (CVD) and volume (CVV). CVD was defined as the percent area occupied by choroidal vessels in the central macular region (6-mm diameter circle centered on the fovea), and throughout the posterior pole (12x9 mm). CVV was calculated only in the central macular region (6-mm diameter circle) by multiplying the average CVD by the macular area and choroidal thickness. Treatment visual outcome was defined as BCVA improvement after the loading dose (M3), and was categorized into two groups: good responders (5 letters) and poor responders (<5 letters).Results: We included 23 eyes (n= 23 patients) with na\u00efve DME, mean aged 66.2\u00b1 5.3 years, 20% (n=7) females. After receiving the loading dose of ranibizumab (M3), 17 eyes (73.9%) were considered good responders and 7 eyes (30.4%) were considered poor responders. At baseline, good responders presented a significantly higher macular CVD respectively). Choroidal thickness was also increased, but did not reach statistical significance (199.7\u00b179.6m vs 182.5\u00b160.4m; p=0.134). After the loading anti-angiogenic treatment, a significant decrease of CCT was seen in good responders (-11.3%, p=0.014), while poor responders presented a non-signif- icant increase (+ 8.5%, p=0.576). CVD and CVV showed analogous changes with significant reductions in good responders (CVV= -13.8%; p=0.008) and increases CVV= +34.1%; p=0.134). CVD at baseline was a good predictor of good response to anti-angiogenic treatment (ROC AUC=0.74; p=0.030).Conclusion: Baseline choroidal indices, such as CVD and CVV, discriminate good and poor responders to anti-angiogenic therapy in DME patients and may represent good predictors of treatment response. These results highlight the potential of SS-OCT for contributing to the management of DME. 157 Kiana Keller, Neurology Mild chronic kidney disease is independently associated with depression in people living with HIV K. Keller, L. Cheru, S. Looby, K. Fitch, S. Grinspoon, S. Mukerji and J. Lo Massachusetts General Hospital, Boston, MA, USA Introduction: Depression and chronic kidney disease (CKD) are common in people living with HIV (PLWH). This study aims to examine the relationship between CKD and depression in PLWH on antiretroviral therapy (ART). Methods: A retrospective, cross-sectional analysis from ART-treated PLWH enrolled in two clinical studies on subclinical atherosclerosis. Participants eligible for analyses were virally suppressed (HIV viral load (VL) < 50 copies/mL), on stable ART and had complete HIV and renal characteristics data (n=141). Depression was defined as current or reported history of depression or current use of antidepressants. Glomerular filtration rate (GFR) was calculated using the CKD-Epidemi - ology Collaboration equation and CKD Stage 0/1, 2 and 3 were defined as a GFR 90, 60-89 and 30-59 ml/min/1.73 m 2, respectively. Regression models included linear or categorical GFR and adjusted for total Framingham risk score (including age, gender, smoking, systolic blood pressure, total cholesterol, and HDL), HgA1c, and tenofovir-use. The original studies 140excluded participants with renal disease, but 5 individuals were found to have Stage 3 CKD. Sensitivity analyses excluded these participants to ensure analyses weren't skewed by limited number of individuals with moderate kidney disease.Results: The mean age was 48 years (standard deviation (SD): 7), 66.7% were male, and 54.6% were white. Mean duration of HIV infection, CD4+ T-cell nadir and CD4+ T-cell count (54.6%) depressed. The cohort's GFR range was 37.8-143.4 ml/min/1.73 m and 3.5% in CKD Stage 0/1, 2, and 3, respectively. In univariate analyses, PLWH with CKD Stage 2/3 were 2.2 times more likely to be depressed than PLWH with Stage 0/1 (95% CI, 1.1 to 4.5; p=0.02). This cohort's predicted probability for depression was 16% greater for a GFR=60 ml/min/1.73 m 2 than a GFR=90 ml/min/1.73 m2. In adjusted analyses, the association between GFR as a linear covariate and depression remained significant (p=0.02). Mild kidney disease remained an independent predictor for depression in adjusted analyses even when excluding CKD Stage 3 participants (p=0.01). Conclusion: Mild CKD is an independent risk factor for depression in ART-treated, virally suppressed PLWH. Targeting renal impairment may be a potential strategy to reduce depression in aging HIV+ individuals. 158 Rachel E. Keller, B.S, Orthopedics The Relationship of Stride Length and Joint Stresses in Adolescent Female Softball Pitchers R.E. Medicine Service, Department of Orthopaedic Surgery, Massachusetts General Hospital, Boston, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: The repetitive demands on various joints of a fastpitch softball pitcher contribute to the relatively high incidence of musculoskeletal injuries among adolescent female softball players. The most common non-contact injuries include those to the leading knee and throwing shoulder. (Loosli et al., 1992; Hill, 2004; Werner, 2006; Holtz and O-Connor, 2018). Lower extremity stride length is an important component of the softball pitch, since it facilitates the transfer of mechanical energy up the kinetic chain from the lower extremities to the throwing arm (Oliver GD, 2011). The leading lower extremity incurs braking forces as high as 115% of body weight and vertical ground reaction forces as high as 139% (Werner, 2005) at the time of stride. These stresses suggest that stride length may correlate with biomechanical factors resulting in greater injury risk. The purpose of this study was to investigate the relationship between stride length and 1) peak joint angles of the lead lower extremity at time of stride and 2) peak joint torques of the lead hip and throwing shoulder during the softball windmill pitch in a cohort of adolescent female softball pitchers.Methods: 17 high school female softball pitchers (mean age= 15.4 \u00b1 1.4 yrs) underwent 3D biomechanical pitch analyses [20 Vicon high speed motion capture cameras (240 Hz)]. Each athlete threw 6-12 pitches of each pitch type which they routinely use in competition, from a standard 43 ft distance toward a strike zone target. A Stalker 5.0 radar gun recorded pitch speed. Fastball pitches landing within the strike zone were included in the analyses, resulting in an average of 5 pitches per athlete. A 15-segment computerized model was created from 62 reflective markers that were affixed to each athlete. Ankle, knee, hip, and pelvis joint angles were collected in all planes at time of stride. Throwing limb's peak shoulder torques were calculated using Visual 3D biomechanical software. Time of stride was defined as the first point of contact in which the lead foot was flat on the ground. Stride length was normalized to subject height. Statistical analysis included Pearson correlations with 2-tailed significance set at p <.005 (SPSS Version 25).Results: The average stride length for this cohort was 102 \u00b1 12% body height. Stride length correlated positively with peak shoulder external rotation torque (r= 0.245, p = 0.029), ankle inversion (p = 0.018) and lead hip flexion (p= 0.0001) at time of stride (Table 1). Stride length demonstrated a negative correlation with lead knee valgus (p = 0.004), hip abduction (p = 0.019), hip external rotation (p = 0.042) and pelvis extension angle (p= 0.0001) at the time of stride (Table 1). No correlations were seen between peak hip rotational torque and stride length.Conclusion: This foundational study demonstrates that stride length has an association with kinematics of various joints at the time of stride as well as torque across the throwing shoulder. Longer stride length is associated with greater shoulder external rotation torque. Shorter stride length is associated with greater lead knee valgus, hip abduction, hip external rotation and pelvis extension angles at the time of stride. Given the prevalence of shoulder, hip, and low back injuries among adolescent female softball pitchers, these relationships suggest that modifiable biomechanics such as stride length can be used to track and alter joint kinematics and torques of softball pitchers.141 159 Endocrine-Neuroendocrine Modulation responses to visual food cues by 10-day overeating and fasting interventions L. Kerem1,2, L. Holsen1, MGH, Boston, MA, USA Introduction: Neural processing of food stimuli involves homeostatic, reward, and self-control brain circuity and is pertur- bated at extremes of weight. Human fMRI studies investigating the effects of diet interventions on processing of food cues could aid in understanding altered brain activation in states of unhealthy weight. We examined brain activity changes in response to 10 days of high-calorie diet (HCD), followed by 10 days of caloric deprivation in healthy adults, hypothesizing that HCD would decrease activation in homeostatic/reward regions, while fasting would increase activation in homeostatic/reward regions and decrease activation of self-control regions.Methods: Seven adults (2M/5F; BMI 26.4 Kg/m 2) completed fMRI scanning in a state, pre- and post-10-day HCD. Six adults (1M/5F; BMI 26.6 Kg/m2) completed fMRI scanning pre- and post-10-day caloric deprivation. The food cue paradigm included images of high and low-calorie foods, household objects, and fixation. Changes in BOLD response for contrasts of interest (food>objects; high-calorie>low-calorie) pre- vs. post-intervention in specific homeostatic, reward, and self-control regions of interest were examined using peak-level significance at p(FWE)<0.05. Results: Following HCD, hypothalamic fMRI activation, reflecting homeostasis control, was attenuated in response to high- vs. low-calorie foods. Following caloric deprivation, fMRI activation of self-control areas (dorsolateral prefrontal cortex) was reduced, while activation of homeostasis (hypothalamus) and food motivation brain areas (anterior insula and orbitofrontal cortex) was increased. Conclusion: Overfeeding and fasting for 10 days modulate brain activity in response to food stimuli, suggesting that in healthy adults, changes in energy balance affect saliency and reward value of food cues. Future studies are required to understand this interaction in states of unhealthy weight. 160 Doyun Kim, PhD, Radiology Frame-of-Interest for Explainable Germinal Matrix Hemorrhage Detection on Head Ultrasound Videos D. Kim, A. Parakh, K. Song, S. Yune, J. Baik, M. Kim, C. Yuan, M. Gee and S. Do Radiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Germinal matrix hemorrhage (GMH) is a major cause of morbidity and mortality in preterm infants and can lead to long term neurologic complications including cerebral palsy and developmental delay. In this paper, we evaluated the feasibility of determining Frame-of-Interest (FOI) for an explainable detection of GMH on neonatal head ultrasound (US) videosMethods: To avoid extreme data imbalance between normal and abnormal (GMH) images, an US-based deep-learning approach has been trained only with images containing relevant anatomy in an US video. However, when the algorithm is tested on the entire video dataset, we observed a high false positive rate that occurred in the images that were not in the training dataset. To solve this problem, we defined a new label, FOI, which represents whether the corresponding frame was part of the training dataset. Specially, FOI was designed to indicate whether a target relevant anatomy (i.e. lateral ventri - cles) was visualized in US images. Figure 1 shows label generation for GMH and FOI. We designed two convolutional neural networks (CNNs) and combined the two outputs to solve the problem of high false positive rate, and got a more 142understandable result by multiplying two heatmaps, as shown in Figure 2. The performance of the proposed model with FOI (CNN-1 and CNN-2) was compared to a model without FOI (CNN-1) for GMH detection. EfficientNet-b1 (parameters of 7.8 M) was used as the network model. From our institutional database, 145 neonatal head US performed between January 2016-July 2018 were retrieved. Static images of the US scans were annotated by a radiologist as follows: (1) GMH present on 263 images; absent on 1529 images in GMH training dataset, (2) FOI positive on 1792; negative on 1876 in FOI training dataset, (3) GMH present on 216; absent on 1372 in GMH testing dataset, and (4) FOI positives on 691; negative on 897 in FOI testing dataset. All US images were gray-scale and obtained in the coronal plane using a low frequency (4 - 8.4 kHz) probe from the anterior fontanelle. Results: For the test set, CNN-I for GMH detection had a specificity of 73 % at a sensitivity of 95 %. With the proposed model (the combination of CNN-I and -II), at the same sensitivity, the specificity significantly increased to 80 % (p<0.001). Figure 3 shows the predicted probabilities of GMH (blue bars) and FOI (red lines) for two videos of GMH presence and absence, and more understandable heatmaps when GMH and FOI detections were combined. Conclusion: Use of FOI can improve the specificity and provide understandable outputs for GMH detection on neonatal head US videos. FOI can be an innovative solution when we develop explainable deep-learning approaches on video data. It may be a useful aid to diagnose GMH in settings where subspecialty pediatric radiologist interpretation is unavailable. 161 Gwang-Won Kim, Psychiatry Subcortical volume changes associated with delusion severity in young adults G. Kim1,2, A. Farabaugh1, M. Nyer1, M. Fava1, Y. Deng1,2 and D. Holt1,2,3 1Department of Psychiatry, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA, 2Athinoula A. Martinos Center for Biomedical Imaging, Charlestown, MA, USA and 3Department of Psychology, Harvard University, Boston, MA, USA Introduction: Young adulthood is a high-risk period for developing psychiatric disorders. Mental health problems are on the rise among adolescents and young adults, and serious mental illnesses, such as schizophrenia and related psychotic illnesses, can often be treated more effectively when identified early. Yet currently there are no reliable quantitative methods for detecting risk for these illnesses. The emergence of delusional beliefs can be an early harbinger of psychotic illness; although the presence of such beliefs can be benign and transient, risk for clinical psychosis is proportional to the severity and persistence of delusional beliefs. Because psychotic illness has been reliably linked to abnormalities in the structure of the brain, it may be possible to identify young people who are particularly at risk for psychotic illness by measuring delusional beliefs and brain structure. Thus the purpose of this study was to evaluate subcortical volume alterations in participants with varying levels of delusional beliefs.143Methods: One hundred and twenty-one young adults (mean age = 19.5 \u00b1 1.3 years; 88 females) participated in this study. The symptom severity of delusional beliefs was evaluated using the 21-item Peters et al. Delusions Inventory (PDI). Magnetic resonance imaging (MRI) was performed on a 3.0-T Magnetom Tim Trio MR Scanner (Siemens Medical Solutions, Erlangen, Germany) with a 12-channel head coil. T1-weighted sagittal images were acquired using a three-dimensional magnetization prepared rapid acquisition gradient echo (3D-MPRAGE) pulse sequence with the following parameters: repetition time/echo time = 1100 ms/1.54 ms, inversion time = 1100 ms, field of view = 230\u00d7230 mm 2, and flip angle = 7\u00b0. The Enhancing Neuroimaging Genetics through Meta-Analysis (ENIGMA) protocol was used for outlier detection and visual inspection. T1 images were post-processed using FreeSurfer (Dev.) and Statistical Parametric Mapping (SPM 12) with the diffeomorphic anatomical registration through an exponentiated Lie algebra (DARTEL) algorithm. A partial correlation, with age, sex, education, and whole brain volume as covariates, was used to measure the correlation between subcortical brain volumes and PDI scores.Results: A subcortical-brain regression in the full sample (n = 121) was conducted to evaluate the relationship between levels of delusional beliefs (PDI total score) and brain volume. The PDI total scores were positively correlated with volumes of the left putamen (r = 0.30, p = 0.001), right putamen (r = 0.21, p = 0.020), and right hippocampus (r = 0.21, p = 0.026). In the voxel-wise analysis, gray matter volume of the left putamen was positively correlated with PDI scores (p < 0.05, FWE corrected).Conclusion: This study reveals that larger volume of the left putamen is associated with a greater severity of psychotic-like symptoms, specifically delusional beliefs. These findings may be useful for understanding the pathophysiological processes that characterize subclinical psychotic symptoms. Replication in an independent sample is needed. 162 Myeongchan Kim, MD, Radiology Adaptive Transfer Learning between Two Detector Types of Mammograms via Generative Adversarial Network M. Kim1, Do1 1Radiology, MGH, Boston, MA, USA and 2Engineering and applied sciences, Harvard University, Boston, MA, USA Introduction: Mammographic density is one of the most important risk factors of breast cancer. Recently, many deep- learning-based studies were performed to evaluate the breast density. Since deep learning applications are learned under the assumption that the distribution and characteristics of test and training data are the same, it is not guaranteed that the model would operate properly when the characteristics of target data are different from that of training data. We studied a transfer- able deep-learning model between two different types of mammograms, and reported results by experimenting with the Deep Learning regression model and the Cycle-Consistent Adversarial Networks (CycleGAN) model.Methods: We collected 4000 cases (3200 for training and validation, 800 for test) of mammograms taken by a direct-type detector (Dataset D) and 4000 by an indirect-type detector (Dataset I). Among datasets that were randomly selected from mammographic exams which were performed at our institution 2008 - 2011, cases that had 4 views (left mediolateral oblique view, right mediolateral oblique view, left craniocaudal view, and right craniocaudal view) were selected. We combined the images of 4 views into an input unit, and the median percentile values from each density label were used as ouput unit to train the regression model. Two types of models were trained from each dataset, one from Dataset D (M1), and the other from Dataset I (M2). We also developed two different augmentation methods to train transferable models between two types of datasets: the first method randomly applied flip, rotation, shear, resize, blur, and gamma change to the image (Classical augmentation) and the second one converted one type of data to another using CycleGAN (CycleGAN augmentation). We used four combinations (no CycleGAN augmentation, and CycleGAN and Classical augmentation) of the above methods during training. The predictions of M1 and M2 of each test set were compared using Spearman's rho values, whose changes with varying augmentation methods were inquired. The overview of the experiment and the model structure used for the training are shown in Figure 1.Results: In the case of Dataset D, Spearman's rho value between M1 and M2 prediction sets was 0.87 when not using augmentation, 0.94 when applying Classical augmentation, 0.89 when applying CycleGAN augmentation and 0.97 when both augmentation methods were applied. In the case of Dataset I, Spearman's rho value was 0.94, 0.92, 0.95, and 0.96, respectively. Figure 2 shows the results of each augmentation method.Conclusion: We developed a regression model that could quantify the mammographic density. Cycle GAN enables Adaptive Transfer Learning between images taken by direct-detector type machines and indirect-detector ones.144 Figure 1 An overview of the training structure. The architecture of the regression model for density quantification with 4 views (left mediolateral oblique view (LMLO), right mediolateral oblique view (RMLO), left craniocaudal view (LCC), and right craniocaudal view (RCC)) is shown as red. The direct type mammograms and models trained by them are shown as blue, and the indirect type mammograms and models as green. Figure 2 Correlation between a model trained with Dataset A and that trained with Dataset B. Dataset A was collected from indirect-detector mammograms, and Dataset B from direct-detector mammograms. Spearman's R scores were used for comparison. 163 Myeongchan Kim, MD, Radiology GrayNet: a versatile base model for practical deep learning CT applications M. Kim1, H. Lee2, K. Ramaraj1 Baik1 1Radiology, MGH, Boston, MA, USA and 2Engineering and applied sciences, Harvard University, Boston, MA, USA Introduction: Many Computed Tomography (CT) based deep-learning applications have been made assuming that lesions may exist at a target body part. But in order to use them in clinical practice, it is essential to determine the indication of applications from the patient's CT. In this study, we introduce the GrayNet model that learns common features from a comprehensive set of CT images. An overview of GrayNet usages is shown in Figure 1. We hope that GrayNet would improve the development of other deep learning applications.Methods: We collected 492 axial-view series from 6 CT machines. We divided this set of data into two groups of patients, training (90%) and validation (10%). Then, we labeled 24 body parts, age, sex, and weight for each CT slice and used them to train GrayNet. To confirm the above hypotheses, we conducted three experiments. First, we calculated the mean Area Under the Receiver Operating Characteristic curve (mAUC) for body parts and sex, and mean absolute-distance (MAD) for age and weight. Second, using the prediction of the body parts, we developed a range selector that returned the location corresponding to the ICRP-110 phantom. To measure the performance of the range selector, we compared the calculated scan range for 96 series and the ground truth values set by a radiologist. The mean absolute error (MAE) between the prediction and the ground truth was measured. Lastly, in order to show whether the GrayNet model improves the performance of other Deep learning applications, Dice scores of U-net models segmenting 3 organs (liver, spleen, and kidneys) were compared. Additional 42 whole-body CT exams not used in training GrayNet were collected, and one contrast CT series and one non-contrast CT series from each exam were used for the experiment. Results: The experiment for prediction performance indicated that the mAUC of 24 body parts prediction and sex prediction were 1.00 and 0.99, respectively. MAD of age prediction and weight prediction were 9.2 years and 6.7kg, respectively. In the case of the experiment for the range selector, MAE of location between ground truth and the prediction was 1.6 cm. Experiment improving organ segmentation models shows that the mean Dice score of liver, spleen, and kidney segmentation without GrayNet were 0.88, 0.90, and 0.57, respectively, and with GrayNet, 0.97, 0.95, and 0.93, respectively. Figure 2 shows the comparison of performance graphically.Conclusion: We developed GrayNet that predicts age, weight, gender, and body part in CT images and developed a range selector by applying GrayNet. GrayNet improved the performance of segmentation models by using the range selector and pre-trained model.145 Figure 1 Overview of GrayNet Usages for Deep learning application for CT images. Figure 2 Performance comparison among the segmentation models. Brown and orange bars show the performance of the segmentation model when using the GrayNet pre-trained model and GrayNet range selector, and gray bars show the performance without GrayNet. 164 Sonia Kim, B.A., Psychiatry A psychological-behavioral intervention for physical activity in type 2 diabetes: a randomized controlled trial S. 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Eighteen million Americans with type-2 diabetes (T2D) do not follow recommended guidelines for physical activity, resulting in greater risk for diabetes-related complications and mortality. Positive psychology (PP) intervention aims to enhance positive psychological constructs, such as happiness, self-esteem, and optimism. The intervention has been associ - ated with improved adherence in health behaviors in a subset of medically ill population (e.g. cardiac conditions). Further-more, PP intervention combined with motivational interviewing (MI) tools and techniques, such as systematic goal-setting, has also been proved to improve health behavior adherence and physical health outcomes. However, aside from a single proof-of-concept trial, PP-MI intervention has not been studied in patients with type-2 diabetes. For the current project, we have developed an 8-week, telephone-based, PP-MI intervention and explored its feasibility and acceptability in T2D patients in a randomized-controlled trial. Methods: Participants were randomized to receive a combined PP-MI physical activity intervention (n=30) or MI-only (n=30) condition. Participants in both groups completed weekly phone calls in a study trainer. Those in the PP-MI group completed weekly positive psychology exercises (e.g., using strengths in a new way) and set goals related to physical activity, while those in the MI-only group assessed current health behavior adherence, areas for improvement, and barriers and resources to health behaviors. Given the nature of this study, the primary aims were feasibility (assessed by % of total PP-MI phone sessions completed) and acceptability (assessed by participant ratings of the ease and utility of each PP-MI session) of the PP-MI intervention. We also assessed the impact of the intervention\u2014compared to MI-only\u2014on psycholog-ical constructs, health behavior adherence (including accelerometer-measured physical activity), and medical and functional outcomes. The PP-MI intervention will be considered feasible if the majority fully completes at least 6 of 8 PP-MI sessions and acceptable if they provide mean ratings of over 7/10 for ease and utility of the PP-MI sessions.Results: Overall, 63 participants enrolled and were randomized in the trial. Preliminary results show that ninety-four percent of PP-MI activities were completed, and participants rated the PP-MI content and sessions as easy (M = 8.5/10; SD = 1.84) and useful (M = 8.7/10; SD = 1.75). Follow-up data collection is still ongoing, but we hypothesize that PP-MI will be associated with greater improvements in psychological, behavioral, and functional outcomes compared to MI-alone.Conclusion: PP-MI is feasible and acceptable in patients with T2D. The results of this trial identify the need for a larger, well-powered efficacy trial to examine the impact of PP-MI on health behavior adherence and psychological and medical outcomes. If effective, this intervention has the potential to improve mental and physical health in the high-risk population of patients with T2D.146165 Allison Kimball, MD, Medicine - Endocrine-Neuroendocrine A randomized placebo-controlled of low-dose testosterone therapy in 1Neuroendocrine Unit, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Eating Disorders Clinical and Research Program, Massachusetts General Hospital, Boston, MA, USA, 4Walden Behavioral Care, Waltham, MA, USA, 5Cambridge Eating Disorder Center, Cambridge, MA, USA and 6Biostatistics Center, Massachusetts General Hospital, Boston, MA, USA Introduction: Anorexia nervosa is a serious psychiatric illness with significant mortality and no approved medical therapies. Comorbid affective and anxiety disorders are highly prevalent and are associated with worse eating disorder outcomes. Relative androgen deficiency is common in women with anorexia nervosa, and we have shown that free testosterone levels are inversely associated with depression and anxiety symptom severity. Therefore, we hypothesized that physiologic testosterone therapy in women with anorexia nervosa and relative androgen deficiency would improve weight, depressive and anxiety symptoms, and eating disorder symptoms and behavior.Methods: 90 women, 18-45 years old, with anorexia nervosa and free testosterone levels below the median for healthy young women were randomized to testosterone 300 mcg daily or placebo patch (Procter & Gamble) for 6 months. Percent free testos- terone was measured by equilibrium dialysis and total testosterone by LC-MS/MS (Mayo Medical Labs). Psychiatric disorders were diagnosed using the Structured Clinical Interview for DSM-IV. Depression and anxiety symptom severity were assessed using the Hamilton Depression Rating Scale (HAM-D) and Hamilton Anxiety Rating Scale (HAM-A), respectively. Eating disorder psychopathology and behaviors were assessed using the Eating Disorder Examination (EDE) and Eating Disorder Inventory-2 (EDI-2). The primary outcome was BMI. Secondary outcomes were HAM-D, HAM-A, EDE, and EDI-2 scores.Results: Baseline characteristics, including mean testosterone levels, did not differ between the active and placebo groups. Mean age was 27 \u00b1 7 (SD) years, BMI 18.3 \u00b1 1.6 kg/m (normal: 0.3-1.9 ng/dL). Sixty-four percent of subjects met criteria for major depressive disorder and 68% for generalized anxiety disorder. Mean HAM-D score was 15 \u00b1 4 (moderate depression severity) and mean HAM-A score was 15 \u00b1 5 (mild anxiety severity). Groups did not differ in percentage of subjects who started/discontinued (44%) or changed psychiatric medication dose (30%) during the study. The mean increase in serum free testosterone was 0.9 \u00b1 0.9 ng/dL in the testosterone group (Figure 1). Mean BMI increased by 0.0 \u00b1 1.0 kg/m 2 in the active group and by 0.5 \u00b1 1.1 kg/m2 in the placebo group (p=0.03) over 6 months (Figure 2). At 4 weeks, there was a trend toward a greater decrease in mean depression symptom severity score (HAM-D -1.6 \u00b1 2.8 vs -0.7 \u00b1 3.0, p=0.09) but not anxiety symptom severity score in the testosterone vs. placebo group. At 6 months, mean HAM-D and HAM-A scores decreased similarly in both groups [HAM-D \u00b1 4.4 (placebo), p=0.25]. There were no significant differences in EDE or EDI-2 scores between groups. Testosterone was safe and well-tolerated with no difference between groups in frequency of androgenic side effects. Conclusion: Contrary to what was hypothesized, low-dose testosterone therapy for 6 months in women with anorexia nervosa resulted in less weight gain, and did not lead to sustained improvements in depression, anxiety, or disordered eating symptoms, relative to placebo. Figure 1. Serum free testosterone in subjects receiving testosterone (black circles) or placebo (black squares). Horizontal dotted lines delineate the normal range of serum free testosterone for women of reproductive age. *p<0.001 for the difference between groups over 24 weeks. Error bars indicate SD. Figure 2. Mean body mass index (BMI) increased by 0.0 \u00b1 1.0 kg/m2 in the testosterone group and by 0.5 \u00b1 1.1 kg/m2 in the placebo group over 6 months (p=0.03). *p<0.05. Error bars indicate SD.147166 Christian Klemt, PhD, Orthopedics Does Component Alignment Affect Functional Outcomes of Bi-Cruciate Retaining Total Knee Arthroplasty? An In-Vivo Three-Dimensional Analysis C. Klemt, P. Arauz, Y. Peng, S. An, A. Byers, S. Limmahakhun and Y. Kwon Department of Orthopaedic Surgery, Massachusetts General Hospital, Harvard Medical School 55 Fruit Street Boston, MA 02114, USA, Boston, MA, USA Introduction: Anterior cruciate ligament posterior Bi-Cruciate retaining (BCR) total knee arthroplasty (TKA) has the potential to restore normal knee kinematics and thus patient satisfaction. Limited studies have examined the relationship between component alignment and patient reported outcome measures. This study aims to determine: (1) accurate 3-dimensional component alignment of BCR TKA, and (2) whether component alignment of BCR TKA affect patient outcomes.Methods: Three-dimensional modeling analysis was performed based on computed tomography images from 29 BCR TKAs (15 female [51.7%], age 65.7\u00b17.6 years and body mass index 29.8\u00b14.0 kg/m 2). Component orientations were quantified for femoral and tibial components in the sagittal, axial and coronal planes. The knee society scores (KSS) was collected preoper-atively and postoperatively at one-year follow-up. Multiple comparisons were performed to analyze the relationship between component alignment parameters and patient outcomes. Results: High variability was observed in the tibial component alignment mean values: coronal At one year follow-up, significant improvement in KSS was noted in BCR TKA patients (p<0.001). However, regression analysis adjusting for baseline KSS demonstrated the postoperative KSS was negatively associated tibial slope p=0.006). Conclusion: High variability was noted in the tibial component alignment of BCR TKA. Patients with greater posterior tibial slopes reported poorer postoperative knee society scores (KSS) at one-year follow up. Excessive posterior tibial slope may generate a greater posterior force on the femur that leads to continuous overloading of the preserved ACL or even ACL rupture, suggesting optimal sagittal plane alignment of the tibial component may be important for the optimization of postoperative functions of BCR TKA patients. 167 Filippos Kontos, M.D., Surgery - General and Gastrointestinal Differential role of Human Leukocyte Antigen (HLA) class I expression in the clinical course of colon and rectal cancer F. Kontos, T. Michelakos, T. Kurokawa, L. Cai, Z. Tu, S. Ferrone and C.R. Ferrone General Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Abnormalities in Human Leukocyte Antigen (HLA) class I expression are frequently present in colon and rectal cancer. Because of the crucial role of HLA class I antigens in the interactions of cancer cells with the host immune system, these abnormalities have an impact on the clinical course of the disease. Many studies have analyzed the association between total HLA class I antigen expression with patient survival in colorectal cancer, considering these two distinct types of cancer as one entity. In addition, there is only scant information in the literature regarding the role of the different HLA class I loci gene products (HLA-A and HLA-B/C) in the clinical course of these diseases. Given the different functional properties of the HLA-A and -B/C, we hypothesize that they might have a different impact on the clinical course of the disease. Moreover, colon and rectal cancer display unique features and follow distinct therapeutic modalities; thus, they cannot be considered one entity and have to be analyzed separately. The lack of information regarding the relationship between HLA-A and -B/C expression and clinical course of the disease in colon and rectal cancer poses a major limitation in the rational design of strategies to modulate HLA class I antigen expression on cancer cells for the treatment of these malignancies. To overcome 148this limitation, we aimed to analyze the association of HLA-A and -B/C expression level on colon and rectal cancer cells with the clinical course of the disease in a patient-level metanalysis.Methods: Patient-level clinicopathologic and HLA-A and -B/C expression data were collected from 5 independent studies. HLA-A and -B/C expression was determined by immunohistochemical (IHC) analysis. HLA-A and -B/C expression was considered absent, low or high when 0%, 1-10% or 11-100%, respectively, of tumor cells were stained. The IHC results were correlated with clinicopathologic variables including overall survival (OS) in a patient-level metanalysis.Results: A total of 2,632 colorectal patients were evaluated; 1189 and 1443 with rectal and colon cancer, respectively. Median age of the rectal cancer cohort was 71 years, 28% of patients had TNM stage II disease, 59% had high HLA-A expression and 69% had high HLA-B/C expression. Median OS for the rectal cohort was 95 months. On univariate analysis, high HLA-B/C expression was associated with significantly longer OS compared to patients with low HLA-B/C expression [median OS for high vs low HLA-B/C expression: 72 vs N/A (more than 50% of the patients with high HLA-B/C expression were alive at the end of the study) months, respectively, p=0.002]. On the contrary, HLA-A expression did not correlate with OS (p=0.76). Multivariate analyses demonstrated that higher tumor grade (HR: 1.2, p=0.01), higher TNM stage (HR: 1.9, p<0.001) HLA-B/C expression (HR: 0.8, p=0.02) were independent predictors of poor OS in rectal cancer patients. The colon cancer cohort had similar baseline clinicopathologic characteristics with the rectal cancer cohort; median age was 71 years, 40% of patients had TNM stage II disease, 60% had high HLA-A expression and 70% had high HLA-B/C expres- sion. Median OS for the colon cohort was 76 months. On univariate analysis, among patients with colon cancer, HLA-A and HLA-B/C expression did not correlate with OS (p=0.47 and p=0.22 respectively). Multivariate analyses demonstrated that older age at surgery (HR: 1.04, p<0.001), higher tumor grade (HR: 1.8, p<0.001) and higher TNM stage (HR: 1.6, p<0.001) were independent predictors of poor OS.Conclusion: High HLA-B/C expression was associated with longer OS in rectal but not in colon cancer. HLA-A expression did not correlate with OS neither in rectal nor in colon cancer. These results argue against the usual practice of combining colon and rectal cancer into one group and analyzing them as one entity. HLA-B/C is an independent biomarker for rectal cancer and strategies which can upregulate these molecules on rectal cancer cells may benefit patients with this type of cancer. 168 Jenna E. Korotkin, BS, Surgery - Surgical Oncology Preliminary results of a multi-center feasibility trial for real-time, intraoperative detection of residual breast cancer in lumpectomy cavity margins using the LUM Imaging System J.E. Korotkin1, Hospital, Boston, MA, USA, 2Breast Surgical Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA, 3Surgery, Duke University Medical Center, Durham, NC, USA, 4Surgery, Stanford University School of Medicine, Stanford, CA, USA and 5Lumicell Inc, Newton, MA, USA Introduction: Obtaining tumor free margins in breast cancer lumpectomies is essential to reduce the risk of ipsilateral breast cancer recurrence. At present, 10-40% of lumpectomy patients require a second surgery for positive margins, increasing patient discomfort, anxiety, and healthcare costs. Better intraoperative margin assessment tools are needed. We are conducting a multicenter feasibility trial testing the LUM Imaging System (Lumicell, Inc., Newton, MA) for real-time, intraoperative identification of residual breast cancer in lumpectomy cavity walls. The system includes (1) LUM015 (an intravenous cathepsin-activated dye), (2) a handheld optical head imaging device covered with a sterile sleeve for use inside lumpectomy cavities, and (3) image analysis software. Rates of tumor detection and rates of reduction of positive margins and second surgeries are being assessed. Methods: After IRB approval, a multicenter, single arm, open label trial was initiated at 16 sites in the United States. LUM015 was administered intravenously at 1mg/kg 4 +/- 2 hours prior to surgery in women with ductal carcinoma in situ and/or invasive breast cancer undergoing breast conserving surgery. After standard lumpectomy surgery, lumpectomy cavity walls were imaged with a 1.3 or 2.6 cm diameter optical head with image acquisition requiring only 1 second to acquire a 1.33 or 5.31 cm 2 cavity wall image, respectively. Areas of high fluorescence signal were excised and correlated with histopa- thology findings. Results: Standard of care lumpectomies with the LUM Imaging System were performed on 203 women with ductal carcinoma in situ and/or invasive breast cancer. Analysis is complete for 125 evaluable patients. LUM015 was injected before surgery and the lumpectomy cavities were imaged with the hand-held device in vivo for real-time margin assessment. 1-2 images were obtained from each cavity orientation when the 2.6 cm optical head was used and at least 4 images from each orientation when the 1.3 cm optical head was used. 35 patients had pathology confirmed positive margins after standard of care surgery. The LUM Imaging System detected residual cancer in 26 of the 35 (74%) positive margin cases. One patient had an allergic reaction to LUM015 injection but recovered completely. No other serious adverse events occurred in the study population.149Conclusion: Use of the LUM Imaging System may improve detection of positive tumor margins and reduce second surgeries for patients with breast cancer. This study is still enrolling patients and further trials involving both the system and its applica-tion to other indications are planned. 169 Menan Gerard Kouame, medical doctor, Medicine - AIDS Research Center Description of cancers in adults with newly diagnosed HIV in C\u00f4te d'Ivoire, West Africa M. KOUAME3,1,4, R. MOH3,1,2, Infectieuses et C\u00f4te d'Ivoire and 4Medical Practice Evaluation Center, Massachusetts General Hospital, Boston, MA, USA Introduction: With the scale-up of antiretroviral therapy (ART) in sub-Saharan Africa, the causes of death in people with HIV are no longer opportunistic infections. Currently they develop complications related to chronic diseases. In the U.S. and Europe, malignancy occurs at higher rates in persons with HIV compared to those without HIV. However, the current epidemiology of malignancy in persons with HIV in West Africa is not well characterized. Methods: : We conducted a secondary analysis of data that was prospectively collected as part of a randomized clinical trial, the Temprano ANRS 12136 trial. This trial was conducted in C\u00f4te d'Ivoire, a West African country with a population of 24 million and HIV prevalence of 2.7%. As part of this trial, adults with newly diagnosed HIV were randomized to immediately initiate ART or to initiate ART according to World Health Organization starting criteria at the time, which was based on CD4 count. In this secondary analysis, we assessed persons with cancer diagnosed during the trial. Cancer diagnosis was confirmed using anatomic pathology. Cancers were characterized as incident or previously present based on chronology of symptoms as adjudicated by trial staff.Results: Of the 2,056 persons who were prospectively followed for 9,404 person-years, 16 persons were diagnosed with cancer (incidence= 1.4 per 1,000 person-years). Women comprised 79% of trial participants and 75% of cancer diagnoses. Eighty-one percent of cancers were identified as incident, and 19% were judged to be previously present. Median age at time of cancer diagnosis was 38 years [range: 36- 44 years], similar to the general trial population. Median time to cancer diagnosis was 232 days [range: 323- 951 days] from trial enrollment. Six (37%) cancers HIV-related: 2 (33%) cervical cancers, 2 epidermoid (1), ocular (1) and maxillary (1)), 4 adenocarcinoma (breast (2) and prostate (1)), 2 pelvic carcinomas, and 1 pancreatic cancer. Five cancer-related deaths were observed: one HIV-related cancer and 4 non-HIV-related cancers.Conclusion: Persons living with HIV in Africa are at high risk for malignancy, even at relatively young age and within a short period of time from their HIV diagnosis. Cancer screening strategies tailored to people with HIV should be developed. 170 Augustus Kram Mendelsohn, Psychiatry Subjective Measures of Hyperarousal Predict Subjective Longitudinal and Retrospective Measures of Sleep Quality but not Objective Measures A. Kram Mendelsohn, K.I. Oliver, C. USA Introduction: Hyperarousal and disturbed sleep are both intrinsic symptoms of posttraumatic stress disorder (PTSD). We explored whether self-reported indices of hyperarousal could predict longitudinally measured objective, subjective, and retrospective evaluations of sleep quality.Methods: Subjects were individuals exposed to a DSM-5 PTSD Criterion-A traumatic event within the past two years (N=98, 66 females), aged 18-40 (mean 24.06, SD 4.76), 42.86% of whom for Among these partic- ipants, hyperarousal indices Administered Hyperarousal (HAS) and the Hypervigilance Questionnaire (HVQ). A Composite Hyperarousal Index (CHI) was derived from combined scores of the hyperarousal items in the CAPS-5 and PCL-5 as well as the HAS total score. Average objective sleep quality was computed from approximately 14 days of wrist actigraphy while average subjective sleep quality was computed from nightly sleep diaries completed across the same time period. Retrospective self-report of sleep quality was also obtained using the Pittsburgh Sleep Quality Index (PSQI, N=103-110). Mean total sleep time (TST), sleep onset latency (SOL), sleep efficiency (SE) and sleep midpoint were calculated from 150actigraphy data and subjective SOL and SE were calculated from diaries. The associations of the PCL-5, HAS, HVQ and CHI scores with the objective, subjective, and retrospective measures of sleep quality were evaluated using simple regression.Results: Self-reported indices of hyperarousal did not predict objective, actigraphy measures of sleep quality. However, longer diary-reported SOL was predicted by higher CHI (R=0.300, p=0.003) and PCL-5 hyperarousal (R=0.301, p=0.003). Similarly, all of hyperarousal predict subjective longitudinal measures of sleep quality, especially SOL, but do not predict objective measurements of sleep quality. Conclusion: Subjective measures of hyperarousal very strongly predict subjective retrospective measure of sleep quality (PSQI). Thus the experiential quality of hyperarousal during wakefulness predicts perceived sleep disturbance to a greater extent than it predicts actual degraded sleep quality. This finding bears striking similarity to the discrepancies between self-reported sleep difficulties and objective measures of poor sleep quality in Insomnia Disorder. Therefore, addressing posttraumatic sleep disturbance with cognitive-behavioral therapy for insomnia (CBT-I) may be an effective non-pharma - cological intervention for addressing patients' sleep-related distress. 171 Yamini Krishnamurthy, Medicine The Successful Implementation of a Virtual Visit Program for Adults with Congenital Heart Disease Y. Krishnamurthy1, J.A. Pagliaro2 and A.B. Bhatt2 1Medicine, Massachusetts General Hospital, Boston, MA, USA and 2Medicine, Division of Cardiology, Massachusetts General Hospital, Boston, MA, USA Introduction: The increase in longevity within the adult congenital heart disease (ACHD) population has led to increased use of emergent and inpatient medical systems with significant morbidity, mortality and resource utilization. Telemedicine has emerged as a strategy to increase access to subspecialty care. We describe the successful longitudinal implementation of synchronous videoconferencing virtual visits to increase ACHD subspecialty access, and overcome the barriers of distance, time, anxiety, and disease knowledge in obtaining life-saving longitudinal ACHD care.Methods: We enrolled 235 patients for synchronous videoconferencing virtual visits between October 2013 and March 2019. Virtual visits were formally scheduled in the electronic health record for 30 minutes. Baseline characteristics and demographics were obtained via chart review. Results: For the 235 patients enrolled in our virtual visit program, 322 virtual visits were scheduled. The average age of our population was 40 \u00b1 13 years. Majority of patients (63%) were identified as having moderate complexity congenital heart disease, as defined by the 2018 American Heart Association/American College of Cardiology Guideline for the Management of ACHD. The most common indication for a virtual visit was to review patient data (47%) including imaging and stress testing (Figure 1A). Twenty-three percent of patients were scheduled for more than one virtual visit. Round trip driving time and distance between the patient's home and MGH were 82 (interquartile range [IQR] 52-122) minutes and 67 (IQR 35-117) miles, respectively. Ninety-three percent of patients' primary residences were in Massachusetts (Figure 1B). Most patients (54%) were employed full time. The cancellation or no-show rate for virtual visits was 18%. We found a 6% cancellation or no-show rate for in-person appointments for the study population. Twenty-five percent of our study population have more than one cardiologist at MGH.Conclusion: Leveraging a virtual visit program for complex ACHD care is feasible, sustainable, and holds promise for increasing access to subspecialty care. ACHD programs should consider integrating virtual care into existing care delivery models. 151172 Patryk Kubiszewski, Bachelor of Arts, Center for Genomic Medicine (CGM) SSRI Use for Treatment of Depression in ICH Survivors: Evaluation of ICH Recurrence and Depression Risk P. Kubiszewski1,2, L. Diaz4, 1Center for Genomic Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Department of Neurology, Massachusetts General Hospital, Boston, MA, USA, 3Hemorrhagic Stroke Research Program, J. Philip Kistler Stroke Research Center, Massachusetts General Hospital, Boston, MA, USA and 4Florida Atlantic University, Boca Raton, FL, USA Introduction: Depression is among the most common stroke-related comorbidities, with a lifetime cumulative incidence of 55%. Stroke survivors diagnosed with depression are at higher-risk for poor long-term outcomes. Adequate treatment of post-stroke depression is therefore key to improving quality of life for stroke survivors. Selective Serotonin Reuptake Inhibitors (SSRI) are currently first-line agents for treatment of post-stroke depression. However, SSRIs are associated with increased incidence of primary intracerebral hemorrhage (ICH), raising concerns for their use among ICH survivors. We sought to address whether SSRI use is associated with ICH recurrence and severity of post-ICH depression symptoms.Methods: We analyzed data from two observational studies of ICH: (1) the Massachusetts General Hospital ICH study; (2) ERICH (Ethnic/Racial Variations of Intracerebral Hemorrhage) study. We conducted univariable and multivariable time-to-event analyses of association between SSRI exposure and: 1) ICH recurrence risk; 2) severity of post-ICH depres- sion symptoms. We conducted secondary analyses among ICH survivors' groups at higher risk for cerebral bleeding, as determined by patient characteristics (demographics, race/ethnicity, medical history), ICH characteristics (location, size, associated functional impairment), genetic (APOE genotype) and neuroimaging (CT and MRI) markers. Results: We will analyze data for 1524 primary ICH survivors (1070 enrolled in the MGH ICH study, and 454 enrolled in the ERICH study).Conclusion: Our results will clarify potential risks and benefits of SSRI use among ICH survivors, with specific focus on patient subgroup at high risk for recurrent hemorrhagic stroke. Ultimately, our findings will inform clinical decision-making in regard to the treatment of post-ICH depression. 173 Yi-Ling I. Kuo, Neurology Excessive Neural Activation in Cerebellar Network a C. Ludlow7 and T. Kimberley1,8,9 1Brain Recovery Laboratory, Massachusetts General Hospital Institute of Health Professions, Boston, MA, USA, 2Divisions of Physical Therapy and Rehabilitation Science, University of Minnesota, Minneapolis, MN, USA, 3Non-invasive Neuromodulation Laboratory, MnDRIVE Initiative, University of Minnesota, Minneapolis, MN, USA, 4MicroTransponder Inc., Austin, TX, USA, 5Department of Otolaryngology-Head and Neck Surgery, University of Minnesota, Minneapolis, MN, USA, 6Department of Communication Sciences and Disorders, University of Wisconsin River Falls, River Falls, WI, USA, 7Department of Communication Sciences and Disorders, James Madison University, Harrisonburg, VA, USA, 8Department of Physical Therapy, Massachusetts General Hospital Institute of Health Professions, Boston, MA, USA and 9Department of Neurology, Massachusetts General Hospital, Boston, MA, USA (AdSD) is a characterized by spasms in the thyroarytenoid (TA) muscles, leading to vocal fold hyperadduction. People with AdSD have difficulty controlling voluntary sound production in the vocal folds, which leads to difficulty speaking. Reduced inhibition in the primary motor cortex (M1) could contribute to involuntary movements in focal dystonias, including AdSD. Recent work has shown that decreased intracortical inhibition in the laryngeal motor cortex (LMC), measured by transcranial magnetic stimulation (TMS), is associated with increased blood-oxygen-level dependent (BOLD) activation, measured by functional magnet resonance imaging (fMRI), in the LMC over the left hemisphere during phonation (i.e. symptomatic task) in people with AdSD. Other work has demonstrated reduced intracortical inhibition in the M1 representing asymptomatic body parts. However, it is unclear if the widespread reduced intracortical inhibition in asymptomatic muscles is associated with BOLD activation during task fMRI in a whole brain analysis in people with AdSD. Thus, the purpose was to compare the BOLD activation during finger-tapping (i.e. asymptomatic task) between people with AdSD and all (51.5 \u00b1 7.9 yrs; 5 females; all right-handed) participated this study over two visits. On Day 1, participants performed an fMRI finger-tapping task at a constant and comfortable speed that alternated between left and right index finger movement and rest. On Day 2, TMS-evoked 152cortical silent period (cSP) and short-interval intracortical inhibition (SICI) were measured in the left M1 for the first dorsal interosseous (FDI). BOLD activation was compared between the two groups using a fixed effect general linear model (GLM) to determine the group (AdSD vs controls) by activation interaction. Voxel-wise (Z > 2.5 for between group analysis) signif- icance was set to p < 0.01 (multiple comparison corrected). Intracortical inhibition measures (cSP and SICI) were compared between AdSD and controls using independent t tests. Results: People with AdSD demonstrated significantly decreased hand cSP compared to controls (88.4 \u00b1 22.6 ms vs 111.2 \u00b1 31.3 ms, p = 0.025). Greater BOLD activation during finger tapping was observed in bilateral cerebellum (primarily anterior lobe), right motor cortex and left somatosensory cortex in people with AdSD compared to controls. There were no regions of greater activation in controls vs AdSD.Conclusion: Widespread over activation in the cerebellum, motor cortex and somatosensory cortex during an asymptomatic task is complementary evidence to TMS-measures of reduced inhibition in cortical excitability in people with AdSD. The highly activated cerebellum, which is primarily a hub of inhibitory processing, may contribute to the lack of suppression of involuntary movement in focal dystonia. 174 Alfred Kyrollos, BS, Dermatology Gastrointestinal Tearney1,2 1Dermatology, MGH, Boston, MA, USA, 2Harvard Medical School, Cambridge, MA, USA, 3Aga Khan University, Karachi, Pakistan and 4Bill and Melinda Gates Foundation, Seattle, WA, USA Introduction: Endoscopy is the gold standard for diagnosing gastrointestinal (GI) diseases. In 2017 alone approximately 75 million endoscopies were performed worldwide. The ability to image the inside of the GI tract, biopsy suspicious tissue and even provide interventional therapies all within a single procedure has revolutionized how GI Medicine operates. Although the endoscope is an indispensable tool, it does have limitations that can hinder its utilization, especially for high risk patient populations. At an average of $1000 per procedure, the cost of endoscopy can be a significant barrier to receiving adequate care, especially in low and middle-income settings. In addition, many healthcare facilities do not have the correct infrastruc - ture or staffing capabilities to support endoscopic procedures. Endoscopy is frequently done with sedation, which excludes certain patients from the procedure and can be very risky for those with comorbidities. To address these challenges, we have developed a tiny, 2 mm diameter transnasal endoscope (TNE) tube that can be implemented in adult and pediatric patients without sedation. The tube is outfitted with OCT endomicroscopy imaging technology that not only guides placement of the device in the gut but also provides microscopic image information underneath the luminal surface. The TNE device contains a distal balloon that can be inflated with a liquid metal that gives the device weight to facilitate passage through the pyloric sphincter into the duodenum. TNE also contains a central lumen through which specialized, miniaturized biopsy and other diagnostic and therapeutic devices can be inserted. Because the TNE device can be utilized in unsedated patients and provides all of the essential functions of endoscopy, it has the potential to transform how GI screening, diagnosis, and interventional procedures are conducted.Methods: We have deployed our TNE in several studies successfully and are able to replicate our methods consistently throughout. The studies currently utilizing TNE are PHRC IRB approved protocols #2017P001428, 2017P001219, 2013P0012405, and share a common goal of imaging intestinal architecture and morphology. The tip of the TNE catheter is lubricated with a water- soluble lubricant. Topical analgesics are offered to reduce patient irritation during catheter introduc - tion. Once the catheter is inside the subject, the imaging is turned on and confirmation of placement is obtained via OCT images. Once the TNE device is inside the stomach, the balloon is insufflated to add weight, to facilitate its motion towards the pylorus and into the duodenum. Once the catheter has passed through the pyloric sphincter, images of the duodenum are continuously acquired as it is propelled down the small intestine via peristalsis. After the final imaging, the device is pulled back until completely removedResults: -We have had a total of 20 successful trans-nasal insertions between 3 studies. -The total average gastric emptying time in all subjects is 45 minutes -The total average gastric emptying time in infantile subjects is 18 Minutes -The total average procedure time in all subjects is 1 hour 10 minutes -The total average procedure time in infantile subjects is 38 minutesConclusion: We have seen our TNE technology fulfill the needs where our TCE technology cannot, especially in patients with difficulty swallowing. With no added risk and significant benefit, TNE has the potential to be deployed in remote areas without access to endoscopy. Under PHRC IRB approved Protocol #2017P001219, in collaboration with Aga Khan University, we have successfully used our TNE device to image both adolescents and infants in Pakistan. Our work there 153has focused on identifying features of Environmental Enteric Dysfunction. This is where we have seen a dramatic decrease in emptying and procedure times, likely due to the ease our operator has when advancing the probe in infants versus adults. We have also successfully used the TNE device in Celiac patients here at MGH. Future work will include further improving villi visualization. The ability to reliably collect data like villous length to width ratios, will be key to better understanding the underlying causes of GI diseases like Celiac, Environmental Enteric Dysfunction and Intestinal cancers. (Fig a. Esophagus, Fig b/c. Duodenum) 175 Erin Lamberth, B.A., Psychiatry Urinary 11-nor-9-carboxy-tetrahydrocannabinol elimination in adolescent and young adult cannabis users during one month of sustained and biochemically-verified abstinence K. Potter1, R. Vandrey2, E. Lamberth1, and Schuster1,4 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Psychiatry and Behavioral Sciences, Johns Hopkins University School of Medicine, Baltimore, MD, USA, 3Biostatistics, Massachusetts General Hospital, Boston, MA, USA and 4Harvard Medical School, Boston, MA, USA Introduction: Adolescents and young adults are the most frequent users of cannabis; however information published to date on cannabis drug testing interpretation is based on data from adults. This disparity highlights the need to understand any differences in drug test results for adolescents and young adults as compared to older adults. This study aimed to define the time course of urinary 11-nor-9-carboxy-THC (THCCOOH) excretion among 70 adolescent and young adult cannabis users during one month of biochemically-verified cannabis abstinence.Methods: Urine specimens were collected at non-abstinent baseline and after 2, 3, 8, 15, 21 and 28 days of abstinence. Specimens were tested for THCCOOH with a \"rapid\" immunoassay drug test and a confirmatory assay using liquid chroma - tography-tandem mass spectrometry, with a 5 ng/mL limit of quantitation. Elimination rate was tested using a population pharmacokinetics model. Residual cannabinoid excretion was differentiated from new cannabis exposure by comparing CN-THCCOOH ratios for all specimen pairs collected 48h apart using the statistical model developed by Schwilke and colleagues (Schwilke et al., 2011) that yields an expected CN-THCCOOH ratio associated with specimen pairs collected at specified time intervals during abstinence. Observed ratios that exceeded this expected value were interpreted as new cannabis use. This study employed a 95% specificity threshold, allowing for a 2.5% false positive rate (i.e., a false interpre - tation of new cannabis use). Results: Participants had an average of 27 days of continuous abstinence (SD=6). Initial creatinine-adjusted THCCOOH concentration (CN-THCCOOH) was 148 was 2 days (SD=5), with a 10-day window of detection (estimated range: 4-80 days). At the final timepoint and among those with >25 days of abstinence (n=62), 40% (n=25) and 19% (n=12) were \"positive\" per federal drug testing guidelines. More frequent past month cannabis use was associated with higher baseline CN-THCCOOH concen-trations, but not with rate of elimination. Nested 5-fold cross-validation suggested high model reliability and predictive validity. Conclusion: Findings underscore that, as with adults, detectable cannabinoid metabolites do not necessarily indicate recent use in adolescents and young adults. Algorithms that account for THCCOOH levels, assessed longitudinally, and time between specimen collections are best equipped to confirm abstinence.154176 Jacqueline M. Lane, PhD, Anesthesia, Critical Care and Pain Medicine Genetic and molecular basis of circadian rhythm disorders: A patient-centered research study Do1 1CGM/DAACPM, MGH, Boston, MA, USA and 2Division of Sleep Medicine, Brigham and Women's Hospital, Boston, MA, USA Introduction: Circadian rhythms regulate human behavior and physiology within the 24-hour, with dysregulation of those rhythms associated with sleep disorders, cognitive and physical performance, cancer, and chronic metabolic and neurologic disease. Despite the importance of circadian rhythms to human health, little is known about the biological connection between human circadian rhythms and our health.Methods: We propose to identify novel genetic factors involved in circadian rhythms by recruiting, phenotyping, and sequencing the genome of extreme circadian rhythm disorder patients. To achieve this goal, we are launching a new patient-driven study of circadian rhythm disorders. We will recruit participants from online advocacy groups, through fliers in circadian disorder clinics, and advertisement on our study website. In order to reach a diverse set of patients our study will forgo traditional laboratory-based tests of circadian rhythms and instead develop a novel home-based circadian phenotyping kit which can also be used by clinicians to diagnose circadian rhythm disorder patients at MGH and beyond. We will perform exome sequencing on DNA extracted from participant saliva samples and analyze our cases using reverse regression (revreg). Results: This study is currently in the design phase and we have selected created our study portal host, study advertise- ments, prescreening questionnaire, study questionnaires, and created home dim-light melatonin assay kits. We are currently designing the portal and activity monitors.Conclusion: Although the short-term goal of the study is a genetic study of circadian rhythm disorders, we hope by making it a lot easier to participate, more people will participate, our study will represent the population better, and lead to generation of data, insights, and clinical diagnostic tools that do not exist today significantly impacting people with circadian rhythm and associated disorders. 177 Paula S. Lara Mejia, B.A., Athinoula A. Martinos Center for Biomedical Imaging fMRI cerebrovascular responses in chronic fatigue syndrome: Preliminary findings P.S. of Neurotherapeutics, Massachusetts General Hospital, Boston, MA, USA, 2Athinoula A. Martinos Center for Biomedical Imaging, Boston, MA, USA, 3Clinical and Experimental Psychology, Tufts University, Medford, MA, USA, 4Radiology, Massachusetts General Hospital/ Harvard Medical School, Boston, MA, USA, 5Infectious Disease, Massachusetts General Hospital, Boston, MA, USA and 6Pulmonary and Critical Care, Brigham and Women's Hospital, Boston, MA, USA Introduction: Chronic fatigue syndrome (CFS), also sometimes called myalgic encephalomyelitis (ME), is a complex and often disabling medical condition. Key symptoms include but are not limited to fatigue, autonomic dysfunction, muscle and joint pain, cognitive dysfunction (\"brain fog\"), and post-exertional malaise, which is a worsening of symptoms 24-72 hours after exertion that can last days or weeks. CFS commonly presents as a failure of symptom resolution after an acute infectious onset, and causes a wide range in symptom severity, with 25% of patients home- or bed-bound. Previous research in this condition has shown autonomic and neuroinflammatory involvement, suggesting there may be a disruption of normal vascular function in the central nervous system. In this study, we used functional magnetic resonance imaging (fMRI) to acquire blood oxygenation level-dependent (BOLD) images in CFS patients, both at rest and during breath-hold challenge. We related BOLD-fMRI data with ongoing peripheral physiology to create a measure of cerebrovascular reactivity (CVR) at rest and during breath-hold challenge.Methods: Six subjects that fulfilled International Consensus Criteria (2011) were included in the study. High resolution T1- and T2-weighted structural images as well as BOLD fMRI images were acquired from subjects at rest. Additional BOLD fMRI images were acquired during a breath hold challenge, in which subjects are asked to hold their breath for 30 second epochs. Physiological measurements including heart rate, respiration, partial pressures of carbon dioxide and oxygen, and oxygen saturation were acquired simultaneously with fMRI acquisition. Physiological changes were used in the regression analysis of BOLD-fMRI data to derive regional CVR at rest and during breath hold challenge.155Results: Four out of the six CFS subjects showed reduced resting state CVR in most brain regions compared to healthy controls. Altered CVR findings during breath hold challenge will also be discussed.Conclusion: Reduced CVR suggests abnormal cerebrovascular reactivity in CFS patients when they were at rest and under breath hold challenge. 178 Lauren Leavitt, MA, Medicine Integration of Shared Decision Making measures into the PROMs Platform L. Leavitt, L. Shea, K. Valentine, H. Vo, K. Sepucha and T. Cha MGH, Boston, MA, USA Introduction: Partners Healthcare System (PHS) has made a commitment to shared decision making (SDM) as a means to achieve patient-centered care and improve value. Current measurement approaches for SDM programs rely on adminis - trative data and are limited to reporting the rates of procedures or the rates of patient decision aid delivery; however, these measures do not provide meaningful information about the value of care. The Health Decision Sciences Center (HDSC) received external funding to support a project to integrate measurement of SDM for high cost elective surgical procedures. SDM performance measures were developed by investigators at the HDSC and have been endorsed by the National Quality Forum (NQF) to assess whether patients are involved in the decision-making process, well-informed, and receive treatments that reflect their goals. This project is a collaboration between HDSC, Partners Population Health and PHS Orthopedics and Neurosurgery Collaborative to examine the quality of elective surgical decisions for patients with hip and knee osteoarthritis, lumbar spinal stenosis and herniated disc. A core component of the project is the integration of the SDM measures into the Patient Reported Outcomes Measures (PROMs) platform to enable systematic data collection on the quality of surgical decisions for these four conditions.Methods: Members of the HDSC collaborated with the Orthopedic Chiefs across PHS and attended the PHS Orthope-dics and Neurosurgery Collaborative meeting in March 2018 to introduce the SDM measures and gain approval to move forward with the integration process. We then met with the PROMs team to consider assignment efforts with the platform. Further, we worked with spine and arthroplasty surgeons to identify the appropriate ORP codes that trigger the assignment of the PROMs. The 10-item SDM performance measures were appropriately added at the end of the Post-Op PROMs MSK questionnaires and integrated into the PROMs platform in May, 2018. The measures are triggered 2-3 months post-surgery with reminders for non-responders sent at 6 months, for the four elective procedures. The surveys assess whether patients are involved in the decision-making process (SDM Process score ranges 0-100% with higher scores indicating more shared decision making), well-informed (Knowledge Score ranges from 0-100% with higher scores indicating higher knowledge), and receive treatments that reflect their goals (Preference Score ranges from 0-100% with higher score indicating higher proportion who received preferred treatment) . We examined the completion rates and present descriptive results from the surveys across the conditions.Results: From June-December, 2018, 1414 patients, who underwent surgery for one of the 4 identified orthopedic conditions, completed a Post-Op PROMs MSK Questionnaire and 489 completed the SDM questionnaire (35%). There were 24 patients who completed the PROMs SDM questionnaire for herniated disc. They reported an average SDM Process score of 83%, a knowledge score of 47%, and 67% had a clear preference for surgery. There were 79 spine patients who completed the PROMs SDM questionnaire for spinal stenosis. They reported an average SDM Process score of 77%, an average knowledge score of 40%, and 81% had a clear preference for surgery. There were 183 OA patients who completed the PROMs SDM questionnaire for knee osteoarthritis. They reported an average SDM Process score of 69%, an average knowledge score of 68% and 86% had a clear preference for surgery. There were 177 OA patients who completed the PROMs SDM questionnaire for hip osteoarthritis. They reported an average SDM Process score of 70%, an average knowledge score of 64%, and 89% had a clear preference for surgery. Generally, spine patients report more SDM in their interactions with health care providers than the OA patients. However, OA patients have higher knowledge scores than spine patients. While we would expect all of these surgical patients to report a clear preference for surgery, 14%-33% indicated that they were unsure or preferred non-surgical options. Patient decision aids have been shown to improve patients' knowledge, help clarify patient's preferences, and to promote SDM conversations. Work is underway to integrate these tools into workflows in spine and OA across PHS. Conclusion: Elective procedures for these four conditions are common across PHS, and many patients are assigned PROMs as part of their care. The SDM set of items had lower completion rates, in part because it was the last section that patients were asked to complete. HDSC interns have recently started calling patients who have not completed their PROMs to encourage higher response rates. The Partners-wide collection of PROMs and SDM supports individual patient care. The data may be used to motivate practices to promote informed, patient-centered decisions to enhance quality and performance.156179 Priscilla R. Lee, Bachelor of Science, Medicine - Cardiology A pragmatic trial integrating routine screening for atrial fibrillation in older patients during primary care visits: Initial enrollment data from the Massachusetts General Hospital, Lexington, MA, USA, 2Cardiology, University of Massachusetts Medical School, Worcester, MA, USA and 3Internal Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Stroke risk among patients with atrial fibrillation (AF) is substantially reduced by oral anticoagulation (OAC). However, AF is often asymptomatic and may be first diagnosed only at the time of a stroke. Efficient and scalable screening for AF may prompt appropriate initiation of OAC and thereby result in the prevention of strokes. Our pragmatic trial, VITAL-AF (ClinicalTrials.gov NCT03515057), will evaluate whether adding a single-lead handheld ECG with an automated AF rhythm assessment as a vital sign among adults 65 years and older during primary care visits is an effective new strategy for detecting asymptomatic AF.Methods: VITAL-AF is being conducted within the MGH primary care practice-based research network (PBRN). Random-ization is at the practice-level with 8 practices randomized to AF screening and 8 to usual care. At intervention practices, all patients 65 years and older with an outpatient visit to a physician or nurse practitioner are eligible. Screening is embedded in standard clinic workflow performed by medical assistants, not research personnel. Incremental medical assistant time is supported by the study. All eligible patients are approached while checking vital signs and those who agree are screened using a 30-second, single-lead, FDA-cleared ECG device (AliveCor, US). Screening results are documented in the Epic Rooming tab, and clinicians are notified about patients having an AliveCor reading of \"possible AF.\" Clinicians can discuss screening results with patients and manage the ECG findings at their discretion. All single-lead ECGs are reviewed by a cardiologist. The primary study endpoint is incident AF, both screen-detected and clinically detected, during a 1-year enrollment period which began July 31, 2018. Secondary outcomes include new OAC prescriptions, ischemic stroke and major hemorrhage. Relevant clinical outcomes are identified using data from the Partners Research Patient Data Registry (RPDR) and sensitive algorithms developed and validated by the MGH primary care PBRN with final determination performed by manual review of the patient's electronic health record (EHR) by expert chart reviewers. With a sample size of 16,212 patients per group, an historical annual AF incidence of 1.6% and an 85% screening rate, the study will have 80% power to detect a 0.42% increase in annual AF incidence in the intervention practices. Comparison of ischemic stroke and hemorrhage rates will be underpowered but will provide estimates for future trials with these clinical events as primary outcomes. Results: As of May 31, 2019, 16,096 unique patients had an eligible appointment in the 8 intervention practices, with 15,163 (94.2%) patients approached for screening by a medical assistant and 14,269 (88.6%) who have completed screening. 33,629 eligible encounters have occurred with 29,351 (87.3%) encounters approached for screening, and 26,088 (77.6%) completing screening. Screening has been successfully implemented across practices with modest variation in patient encoun- ters screened (69.7 - 89.0%). Since patients are offered screening at all visits, 1,479 not initially screened were screened at a subsequent visit. AF incidence will be assessed at the end of the trial.Conclusion: VITAL-AF is the first U.S. randomized clinical trial to embed AF screening into routine primary care and the largest to date. The trial is facilitated by novel ECG technology performed at outpatient visits with outcomes derived from EHR data. Initial enrollment data show that adding automated AF rhythm assessment as a new vital sign among older, at-risk adults at primary care visits is feasible and accepted by patients. Our findings regarding screen-detected AF will inform even larger studies with stroke endpoints and outpatient screening using mobile devices. 180 Vivian W. Lee, BA, Medicine - General Internal Medicine Older patients' experience with discussions about stopping colorectal cancer screening: \"Give me a reason other than age\" V.W. Lee, L. Leavitt, F. Marques, L. Simmons and K. Sepucha Division of General Internal Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: There is a considerable amount of research and interventions that are focused on increasing rates of colorectal cancer screening (CRC) for people aged 50-75. However, United States Preventive Services Task Force (USPSTF) does not recommend routine screening for adults aged 76-85; rather, it recommends individualized decision making (C Recommendation) based on each patient's cancer risk, overall life expectancy, and preferences. While CRC screening decisions should be individualized, studies have found considerable gaps in physicians' ability to inform and meaningfully involve older patients in cancer screening decisions.157Methods: We conducted one-on-one semi-structured phone interviews with 20 patients. Study staff invited patients aged 70-95 who had provided consent to be recontacted for studies by the Health Decision Sciences Center or who gave permission to be contacted through their clinician. We asked patients about their CRC screening history, decision making experience, and preferences. We also asked patients open-ended questions about their overall experience. Patients received $40 for comple - tion of a 30-minute phone interview. All interviews were conducted over the phone and were audio recorded. Staff reviewed the tapes to take detailed notes in the standardized interview sheet. Patient responses were coded and analyzed in Excel.Results: 20 patient interviews were conducted with an average interview length of 30 minutes. The mean age of patients was 78 (+/- standard deviation of 6) and 12 patients were female. Most patients were well-educated: 70% having a college degree or higher, and 30% having a high school degree only. Nearly all patients' (95%) had undergone a colonoscopy in the past. More than half of patients also had other screening tests, such as a stool card (65%) or a sigmoidoscopy (30%). Some patients (40%) had a polyp removed during a previous colonoscopy. Half of the patients (10/20) stated that their physician did not explain that there were choices for CRC screening and that only colonoscopy was ever mentioned. One patient said that, \"It was all or nothing for colonoscopy.\" Half (10/20) had been presented with options for screening. Likewise, the majority (15/20) of patients stated that their physician had never discussed the option of stopping CRC screening. When asked their opinions about stopping CRC screening, patients reported a range of reactions: 7/17 stated they would be relieved and happy to stop, 1/17 would be neutral, 1/17 would be disappointed. About half, 8/17, of patients said their reaction would depend on their physician's reasoning. Several expressed concern about the risk of cancer, \"Unless there are more statistics that I do not need the screening, I would do it to make sure.\" Likewise, some said they would want to understand the clinical reasoning to accept the option of stopping screening, \"I'd want a reason beyond that I'm over the age limit. I'd want an explanation a little deeper than that in order for me to live with the decision.\" When asked about their reasons to continue screening, many patients (75%) wanted to prevent cancer and some patients (20%) were worried over past results of polyps. One patient (5%) wanted to stop screening because of the colonoscopy perforation risk. Importantly, regardless of their CRC screening preference, most patients (82%) stated they would prefer a discussion with their physician about stopping CRC screening. For example, one patient reflected a common theme among these patients, \"I want to be an informed consumer. More information any doctor gives you is better. I don't want anything to be shrouded.\" Likewise, another common theme among these patients is patient education. For instance, one patient who preferred discussion said, \"Doctors need to give information to patients, give them different options, different choices, and non-invasive options.\" Patients had different suggestions for the discussion: 5/14 wanted a good discussion (e.g., \"I would want the doctor to thoroughly respond [to my questions and concerns].\"), 5/14 wanted specific pros and cons, and 4/14 wanted their preferences to be included (e.g., \"I would prefer my doctor to listen to me and respect my wishes.\"). Only a minority of patients (18%) preferred to listen to their doctor's recommendation without discussion. Conclusion: Few older patients in the sample reported having a discussion about stopping CRC screening. Our findings suggest that the majority of patients want to have this discussion, and many would be happy to stop screening; particularly if their physician presented that as a reasonable option. These findings will be incorporated into new tools and a training intervention for primary care clinicians to improve these conversations with their older patients. 181 Courtney E. Leonard, BS, Medicine - Cardiology Acceptability of Screening for Atrial Fibrillation Among Primary Care Patients with and without a Prior Diagnosis of Atrial Fibrillation Hospital, Boston, MA, USA, 2Cardiology, UMass Memorial Medical Center, Worcester, MA, USA and 3Internal Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Oral anticoagulation may reduce the risk of stroke in patients with atrial fibrillation (AF) by about two-thirds, but approximately 40% of patients diagnosed with AF at elevated stroke risk are not anticoagulated. Whereas screening for AF in patients may identify undiagnosed AF, practice-wide screening may also raise physician awareness of those with a prior AF diagnosis and thereby prompt reconsideration of oral anticoagulation. Whether patients with AF would be willing to undergo routine electrocardiographic assessment for the arrhythmia, if embedded in clinical workflows, is unknown. We describe patient participation in the VITAL-AF study (ClinicalTrials.gov NCT03515057), a pragmatic randomized trial assessing screening for AF in patients 65 years and older at 8 MGH primary care practice intervention sites regardless of prior history of AF. We evaluated the acceptability of screening among patients with a prior diagnosis of AF. Methods: Medical assistants (MAs) approach eligible patients prior to a visit with a physician or nurse practitioner during normal vital sign assessment. Among consenting patients, MAs perform a 30 second screening using an FDA-cleared, single-lead ECG device (AliveCor Kardia Mobile). The handheld screening device gives one of automated results: \"Possible AF\", \"Normal\", \"Unclassified\", and \"No Analysis\". Physicians may a 12-lead ECG to confirm the diagnosis of AF at 158their own discretion. We compared the proportion of patients approached, screened, and those who declined to participate between those with and without a prior history of AF. We also compared the AliveCor automated results between groups. We ascertained prior history of AF for each patient in an automated manner using the problem list at their first eligible visit between November 12, 2018 and May 31, 2019. Results: 13,848 unique patients had an eligible visit between November 12, 2018 and May 31, 2019, and 1,891 (14%) had a prior history of AF. Approach rates for screening by MAs were similar among those with and without a prior history of AF (91% and 93%, respectively). Among those who were approached, 1,470 (86%) patients with a prior diagnosis of AF consented to screening as compared to 10,143 (91%) without a prior diagnosis of AF. Individuals with prior AF were more likely to have an automated AliveCor reading \"Possible vs 3%) and were less likely to have a reading of \"Normal\" (43% vs 83%) as compared to those without a prior diagnosis of AF (all p<0.001).Conclusion: Though patients with a history of AF were more likely to decline than patients without a prior AF diagnosis, the majority agreed to undergo electrocardiographic assessment. As anticipated, patients with a prior AF diagnosis had a greater proportion of abnormal single-lead ECG results (\"Possible AF\", \"Unclassified\", \"No Analysis\") compared to patients without a prior AF diagnosis. Final study results will reveal whether electrocardiographic assessment in patients with a history of AF results in increased rates of anticoagulation prescription. 182 Michael Leone, Neurology Sleep EEG-Based Brain Age of People Living with HIV M. Leone1, C. Boutros1, S. Mukerji1, G. Robbins1, R. Thomas2, M.B. and H. Sun1 1MGH, Boston, MA, USA and 2Neurology, Beth Israel Deaconess Medical Center, Boston, MA, USA Introduction: Age-related co-morbidities affect HIV+ adults at younger ages, raising concern for accentuated brain aging. Our lab previously showed that an EEG-based model of brain age reliably predicts chronological age in healthy adults. The differences between EEG-based brain age and chronological age, the brain age index (BAI), independently predicts mortality. BAI is also increased by cardiovascular comorbidities and dementia. We hypothesized that HIV+ adults have a relatively older brain age and thus higher BAI compared to match HIV-negative (HIV-) controls.Methods: HIV+ adults with sleep EEGs (n=39) from the MGH Sleep Lab were identified from the Partners HealthCare Systems Research Patient Data Registry (RPDR). The HIV+ cohort was matched by age, gender, and race to an HIV- cohort (n=86) of subjects lacking major neurological and psychological disease. BAI was compared for the two groups. We also gathered clinical data through chart review and the RPDR of antiretroviral therapy history and cardiovascular disease risk, and proposed a causal graph of HIV's effects on brain age.Results: Mean chronological age of the HIV+ and HIV- cohorts were 49 and 48, respectively. The mean HIV+ BAI was elevated (-0.15 vs. 5.48), which is +5.63 years higher than HIV- controls (p =0.002 < 0.05 by student's t-test). 72% of HIV+ subjects and 54% of HIV- subjects had higher brain age relative to their chronological age (p=0.054, normal test for proportions). Many EEG features are different in the HIV+ group; in particular, in HIV+ subjects, the delta to theta ratio during deep sleep (N3) is less compared to HIV- subjects (p=4 x 10^-6 < 0.05). The delta (slow) oscillation during deep sleep is a hallmark of restorative sleep.Conclusion: In a retrospective pilot study, using a previously developed model, we compared brain age in our HIV+ cohort versus matched HIV- controls from the MGH Sleep Lab. The results suggest that HIV+ adults experience accentuated brain aging that can be detected by sleep EEG. Slow (delta) waves during deep sleep are characteristic of healthy brains; therefore, decreased delta to theta ratio among HIV+ subjects is a possible indicator of poor brain health and aging. In future work, we will analyze the mediating effects of past efavirenz use, an antiretroviral medication known to disturb sleep, and cardiovascular co-morbidities on brain age. Subject Demographics Brain Age Index (BAI) is elevated in HIV+ subjects. Left: Scatter plot of chronological age (CA) versus brain age (BA). Red dots represent HIV+ subjects, grey dots represent matched HIV- controls. The dashed line is the identity line (BA = CA). Right: Box plot of brain age indices (BAI) of HIV+ and matched HIV- controls. Middle lines represent median BAI, upper and lower box bounds represent 25th and 75th quartiles, and whiskers represent 1.5x the interquartile range beyond the quartiles.Julie Levison, MD, Internal Medicine A randomized trial of a community health worker intervention to improve retention in HIV care for HIV-positive Latino migrants and immigrants J. Levison1, 2, L. Bogart3, M. Alegria4, 2, A. Tarbox1, C. 1, P. C. Safren7 Levison, A. Tarbox, C. Cubbison, D. Mejia, P. Bancalari, L. Yu, Y. Chang, DGIM, MGH, Boston, Massachusetts, UNITED STATES|J. Levison, M. Alegria, Y. Chang, Harvard Medical School, Boston, Massachusetts, UNITED STATES|L. Bogart, RAND Corporation, Santa Monica, California, UNITED STATES|M. Alegria, Massachusetts General Hospital, Boston, Massachusetts, UNITED STATES|C. Rios, Boston Healthcare for the Homeless Program, Boston, Massachusetts, UNITED STATES|H. Amaro, University of Southern California, Los Angeles, California, UNITED STATES|S. Safren, University of Miami, Miami, Florida, UNITED STATES| Introduction: HIV-positive Latino immigrants and migrants experience multi-level barriers to HIV care that predispose them to non-retention in HIV care, a risk factor for HIV transmission. Our goal was to conduct a pilot randomized trial comparing a newly-developed community health worker intervention for retention in HIV care to a treatment-as-usual (TAU) condition in this population. Methods: 67 Participants were randomized to a treatment as usual (TAU) (n=32) or community health worker intervention (ADELANTE) arm (n=35). Participants in the treatment as usual (TAU) condition received usual HIV care and were offered information on HIV treatment and care. Participants in the ADELANTE arm received a 5-session intervention using a problem-solving approach guided by a HIV-focused telenovela. The intervention addressed patient-level factors (e.g. self-efficacy, patient activation, quality of life, AIDS-related stigma) and structural factors by linking patients to resources. The primary data elements included: retention in care; viral suppression; mediating factors such as increased patient activation and self-efficacy, improvement in mental health, and reduction of depression, alcohol use, and AIDS-related stigma; and feasibility and acceptability of the intervention. Data were collected through patient and CHW-delivered questionnaires and participant medical records. Results: The intervention was feasible (91% retained at week-24 and 63% received all 5 CHW sessions) and acceptable (participants felt greater self-confidence for achieving a healthier life and format provided the role-modeling and opportunity for deep reflection). Furthermore, the intervention demonstrated improvements for some health mediating outcomes. The intent-to-treat analysis did not show statistically significant differences in retention in care. Conclusion: ADELANTE may have promise as a strategy for improving self-management behaviors in HIV+ Latinos. Intensive case management focused on transportation may be an important adjunct to improve completion of study visits and retention in HIV care. Expanding peer outreach and recruitment from substance use treatment facilities could enrich the cohort with participants not connected to HIV services.159183 Selena Li, Surgery - Surgical Oncology Impact of Treatment Sequencing on Survival for Patients with Advanced Gastric Cancer S. Li1, C. Costantino2 and J. Mullen2 1Harvard Medical School, Cambridge, MA, USA and 2Surgical Oncology, Massachusetts General Hospital, Boston, MA, USA Introduction: This study sought to determine impact of treatment sequencing on survival for advanced gastric cancer patients.Methods: Single institution, retrospective analysis of 220 advanced gastric cancer patients undergoing curative-intent surgery from 2001-2015. All patients met criteria for (neo)adjuvant chemotherapy and radiation therapy (RT). Treatment regimens were: (1) surgery first with adjuvant chemoradiation therapy (S+CRT); (2) perioperative chemotherapy + surgery + RT (Periop); and (3) total neoadjuvant therapy followed by surgery.(TNT).Results: We intended to treat 132 (60.0%) patients with S+CRT; 59 (26.8%) patients with Periop; and 29 (13.2%) with TNT. All intended therapy was received by 100% of TNT patients, 43.2% of S+CRT patients, and 49.2% of periop patients. TNT was associated with higher rates of complete pathological tumor response than perioperative chemotherapy (13% vs. 6.8%, p=0.001). At follow-up of 37 months, survival rates with TNT, Periop, and S+CRT, by intention to treat, were 48.3%, 48.3%, and 30.1%, respectively. On multivariate analysis, patients who completed all intended therapy with p=0.005), Periop (HR:0.34, p=0.004), and S+CRT (HR:0.34, p<0.001) demonstrated similar improvements in overall survival compared to patients undergoing surgery first while not completing adjuvant CRT. Partial completion of Periop did not significantly improve survival (HR: 0.61, p=0.103).Conclusion: The choice of treatment sequencing has a major impact on survival in patients with advanced gastric cancer, as less than 50% of patients treated with upfront surgery or perioperative chemotherapy receive all intended therapies. TNT ensures that all patients complete all intended therapy and enjoy higher rates of pathologic tumor response. 160184 Selena Li, Surgery - Surgical Oncology Completion of Multimodality Therapy Mitigates the Adverse Impact of Postoperative Complications on Survival in Patients Undergoing Gastrectomy for Advanced Gastric Cancer S. Li1, B. Udelsman2, A. Parikh2 and J. Mullen2 1Harvard Medical School, Cambridge, MA, USA and 2Massachusetts General Hospital, Boston, MA, USA Introduction: Objective: To determine the impact of major and minor postoperative complications (POCs) on multimodality therapy (MMT) completion rates and overall survival (OS) in patients with locally advanced gastric cancer. Methods: Design: Single institution, retrospective cohort study Setting: Academic medical center Patients: 206 patients with localy advanced gastric cancer undergoing curative-intent surgery from 2001-2015, excluding patients with T1/T2 N0 and M1 disease and who died within 90 days. Intervention: MMT for advanced gastric cancer. Main Outcome Measures: OS and MMT completion rates.Results: A total of 120 patients underwent surgery with adjuvant MMT (47.5% completed MMT), 58 received perioperative MMT (50% completed MMT), and 28 received total neoadjuvant therapy (all chemotherapy and radiation therapy prior to surgery, TNT). Minor POCs (Clavien Dindo I-II) occurred Dindo III-IV) in 39 patients (18.9%). At a follow-up of 37 months, patients with a major POC had a 3-year OS of 33.3%, compared to 56.9% with a minor POC, and 62.1% in patients with no POCs (p=0.023). In contrast, there was no difference in 3-year OS rates in patients experiencing major POCs if they completed all intended MMT analysis, non-TNT patients who experienced a major POC were less likely to complete all intended MMT (HR 0.36, p=0.017), and a major POC in these patients had a significant impact on OS (HR 2.76, 95% CI: 1.26-6.06, P=0.011), whereas it did not in patients all MMT (HR 1.58, 95% CI: 0.62-4.01, p= 0.336).Conclusion: Major postoperative complications adversely affect long-term survival after gastrectomy for gastric cancer, at least in part via lower completion rates of MMT. Treatment strategies designed to ensure the completion of MMT, such as TNT, may be preferable, particularly for patients at high risk for POCs. 185 Anna Liapaki, DDS, Oral and Maxillofacial Surgery The effect of age and osteoporosis/osteopenia treatment on implant osseointegration. A retrospective pilot study A. Liapaki1, F. Guastaldi1, M. Demay2 and M. August1 1Oral and Maxillofacial Surgery, Massachusetts General Hospital, Boston, MA, USA and 2Endocrinology, Massachusetts General Hospital, Boston, MA, USA Introduction: Dental implants are commonly used for the replacement of missing teeth. Assessment of a patient's medical status is essential for treatment planning. Osteoporosis is the most common metabolic bone disease and occurs primarily in post-menopausal women (type 1) and men over 70 years of age (type 2). Osteoporosis is characterized by low bone mineral density (BMD) and, when present, affects bone metabolism and healing. Calcium and vitamin D3 supplementation, antire - sorptive or anabolic therapy is often recommended to decrease fracture risk. The purpose of this retrospective pilot study is 161to assess the effect of these treatments on dental implant healing. Specific aims include determining the difference in implant osseointegration between patients with decreased BMD that are being supplemented vs those not supplemented. We also wish to compare this rate to age matched male controls and premenopausal women and younger men. Methods: To address our purpose, we designed a retrospective cohort study evaluating all patients receiving endosseous implants in the Department of Oral and Maxillofacial Surgery at Massachusetts General Hospital between 2015-2018. Inclusion criteria were: implant specific data and the status of implant stability at stage 2 uncovering. History of active smoking, alcohol use, metabolic bone disease and uncontrolled medical comorbidities resulted in exclusion. The patients were stratified into 5 groups: 1) premenopausal women (<50 yrs), 2) postmenopausal women (50 yrs) without osteopenia/ osteoporosis treatment, 3) postmenopausal women (50 yrs) with osteopenia/osteoporosis treatment, 4) men < 50 yrs, and 5) men 50 yrs. Successful osseointegration was defined as stability at uncovering (stage II) using a manual torque wrench or confirmed radiographically. Low BMD treatment included calcium and vitamin D supplementation, estrogens, bisphos-phonates and denosumab. The implant failure rates were calculated for each group of patients for both jaws as well as for each jaw independently. A p value .05 was considered statistically significant. Results: We identified 999 implants placed in 521 patients. Application of inclusion/exclusion criteria resulted in exclusion of 147 implants in 77 patients. A total of 852 implants (458 maxilla; 394 mandible) in 444 patients were evaluated. The 5 groups were as follows: 1) 122 implants, 2) 136 implants, 3) 149 implants, 4) 119 implants, and 5) 326 implants. Failure rates per group were 1) 2.4%, 2) 3.7%, 3) 1.3%, 4) 1.7%, and 5) 4.6%, respectively. Unsupplemented postmenopausal women had a lower failure rate than age-matched male controls (P= .67). Supplemented postmenopausal women had a lower failure rate than unsupplemented women (P= .20). Premenopausal women had a lower failure rate than unsupplemented postmenopausal women (P= .57). The maxilla/mandible failure rate for each group was as follows: 1) 1.7%/3.1%, 2) 6%/1.5%, 3) 0%/2.7%, 4) 1.4%/2.1%, and 5) 5.5%/3.5%. The highest failure rate was observed in the maxilla of unsupplemented postmenopausal women. Of note is that no maxillary implant failure was found in the postmenopausal supplemented group. Comparison of the non-osseointegration rate of implants between the maxilla and the mandible showed a higher failure rate in the maxilla of post-menopausal unsupplemented women (P= .16) and men 50yrs (P= .40), whereas the other groups demonstrated a higher failure rate in the mandible. None of the results achieved statistical significance in this small pilot study. Conclusion: It remains controversial to what extent osteopenia/osteoporosis affects the maxillary and mandibular bone. Previous work with osteoporosis and estrogen supplementation demonstrated an improved rate of integration in the maxilla. It is interesting to note in this study that the maxillary failure rate in older women (unsupplemented) and older men was higher than the mandibular failure rate, suggesting that the effects of type 1 and type 2 osteoporosis may differentially affect the more medullary maxillary bone. This pattern was seen to be reversed in the other 3 groups. Young women and men had a higher mandibular failure rate, as did the supplemented post-menopausal women. We demonstrated in a previous study that postmenopausal women receiving estrogen supplementation had a significantly lower rate of maxillary implant failure compared to unsupplemented women. We again see this trend when evaluating other types of supplementation, including bisphosphonates, calcium and vitamin D. The supplemented women received 76 maxilla implants and had no failures. The beneficial effects of these medications on bone healing may be more pronounced in the maxilla. Both unsupplemented women 50 and men 50 demonstrated the highest overall failure rates and higher failure rates compared to their younger counterparts. The effect of type 1 and type 2 osteoporosis may be contributory and further elucidation is needed. In this pilot study using 5 groups, the sample size is small not allowing for statistical significance. This small sample size is currently being expanded to better evaluate the effect of age and osteoporosis on implant healing. In addition, planned studies going forward will involve evaluation of bone cores obtained at the time of implant placement for histomorphometric and cellular evaluation to better understand the effect of both age and antiresorptive therapy on implant healing. 186 Alicia Lightbourne, MPH, Emergency Potential Impact and Cost-effectiveness of Ketamine Anesthesia for Emergency Cesarean Section in Kenya A. Lightbourne1, S.C. Resch2, S. T.F. Burke1,5,3 1Division of Global Health Innovation, Department of Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Department of Health Policy and Management, Harvard T.H. Chan School of Public Health, Boston, MA, USA, 3Harvard Medical School, Boston, MA, USA, 4RTI International, Durham, NC, USA and 5Department of Global Health and Population, Harvard T.H. Chan School of Public Health, Boston, MA, USA Introduction: In Kenya, maternal mortality is 362 per 100,000 births, and the late stillbirth rate is about 9 per 1000. Emergency Cesarean section could prevent many of these deaths. The overall Cesarean rate is 8.7%, but this average rate masks substantial heterogeneity in the availability of Cesarean in emergency situations. In 16 of 47 counties the Cesarean rate is less than 5%. Reasons for the under-provision of Cesarean in Kenya include a high proportion of deliveries occurring at home or facilities lacking emergency obstetric care, inadequate emergency medical transport system, a shortage and 162maldistribution of clinicians and equipment for performing surgery, including anesthesia. A program to supply nurse- administered ketamine in clinical settings where timely access to traditional anesthesia services is the bottleneck to emergency surgery could increase the use of Cesarean section and improve health outcomes. Methods: We extended previous work modeling the cost and impact of a Ketamine program in Kenya to estimate the potential lives saved and cost-effectiveness of the program. For each county in Kenya, we estimated the gap in Cesarean sections based on an assumed optimal rate of 15% of deliveries. We assumed the program would only have impact on facility-based deliveries, and that lack of anesthesia was the rate-limiting component for 30% of the surgery gap. We used the MANDATE model (www.mandate4mnh.org) to estimate the average fetal, neonatal, and maternal deaths averted by per additional emergency Cesarean, accounting for the incidence and case fatality rate of emergency obstetric conditions for which Cesarean is an indicated treatment. Program costs were estimated in a previous study and include initial and ongoing training, quality assurance, coordination, and overhead at the Ministry of Health and facility-level.Results: The gap in emergency Cesarean procedures in counties with a Cesarean rate less than 15% among facility deliveries is 38,165 procedures per year. A Ketamine program that reduces this gap by 30% (11,500 procedures), was estimated to cost $178,000 per year. Results from the MANDATE model indicate that 0.14 deaths would be averted per additional emergency Cesarean procedure provided, resulting in 5340 lives saved, at a cost of $33 per life saved.Conclusion: Until the human resources and infrastructure for transitional anesthesia service can be expanded to ensure reliable availability in emergency situations requiring basic surgery such as Cesarean section, a ketamine program is likely to save lives and offer very good value for money in Kenya. 187 David J. Lin, MD, Neurology Corticospinal Tract Injury Estimated from Acute Stroke Imaging Predicts Upper Extremity Motor Recovery after Stroke D.J. Lin1, A. Cloutier2, S. Cramer10 and L. Hochberg1 1Neurology, Massachusetts General Hospital, Boston, MA, USA, 2Neurology, Center for Neurotechnology and Neurorecovery, Boston, MA, USA, 3Occupational Therapy, MGH Institute of Health Professions, Boston, MA, USA, 4Allied Health Sciences, Department of Physical Therapy, Chapel Hill, NC, USA, 5Occupational Therapy, Massachusetts General Hospital, Boston, MA, USA, 6Physical Therapy, Massachusetts General Hospital, Boston, MA, USA, 7Neurology, University of Rome Tor Vergata, Rome, Italy, 8Neurology, University of Michigan, Ann Arbor, MI, USA, 9Neurology, Massachusetts General Hospital, Boston, MA, USA and 10Neurology, University of California at Irvine, Irvine, CA, USA Introduction: Injury to the corticospinal tract (CST) has been shown to have a major effect on upper extremity motor recovery after stroke. This study aimed to examine how well CST injury, measured from neuroimaging acquired during the acute stroke workup, predicts upper extremity motor recovery. Methods: Patients (N = 48) with upper extremity weakness after ischemic stroke were assessed using the upper extremity Fugl-Meyer (FM) during the acute stroke hospitalization and again at 3-month follow-up. CST injury was quantified and compared, using four different methods, from images obtained as part of the stroke standard-of-care workup. Logistic and linear regression were performed using CST injury to predict DFM. Injury to primary motor and premotor cortices were included as potential modifiers of the effect of CST injury on recovery. Results: N = 48 patients were enrolled 4.2 \u00b1 2.7 days post-stroke and completed 3-month follow-up (median 90-day modified Rankin Scale 3, IQR 1.5) this study. CST injury distinguished patients who reached their recovery potential (as predicted from initial impairment) from those who did not, with area under the curve (AUC) values ranging from 0.705 to 0.8. In addition, CST injury explained ~20% of the variance in the magnitude of upper extremity recovery, even after controlling for the severity of initial impairment. Results were consistent when comparing four different methods of measuring CST injury. Extent of injury to primary motor and premotor cortices did not significantly influence the predictive value that CST injury had for recovery.Conclusion: Structural injury to the CST, as estimated from standard-of-care imaging available during the acute stroke hospitalization, is a robust way to distinguish patients who achieve their predicted recovery potential and explains a signifi- cant amount of the variance in post-stroke upper extremity motor recovery.163 Figure 1. (A) Stroke lesion overlap maps for the 48 participants. All lesions were flipped onto the left hemisphere for display. Colorbar on the right with maximum value 22 (i.e. maximal overlap voxel, red). (B) Primary motor cortex (M1) - Corticospinal tract (CST) templates constructed using deterministic tractography methods. The light blue tract shows an M1-CST from 17 healthy controls at University of California at Irvine (UCI). The green tract shows an M1-CST from 28 health subjects at Johns Hopkins University (JHU). Note the tracts are slightly offset and that the JHU tract traverses down to the level of the medulla while the UCI tract stops at the level of the mid-pons. (C) of CSTs. Templates of primary motor (M1, dark blue), and premotor (pM), yellow) cortices overlaid on the JHU CST template (green). Figure 2. Proportional and limited recoverers. (A) Upper extremity Fugl-Meyer (FM) recovery curves between hospital admission and 3-month follow-up for 48 patients with stroke. Note that in severe patients (FM Init < 22) there is a group with limited recovery (dark gray lines). (B) Potential for (66 - FMInit) versus actual (FM3mo - FMInit) recovery of upper extremity impairment. The line (black-dashed) represents the amount of recovery as predicted by the proportional recovery model. Limited recoverers (dark gray squares) are distinguished from proportional recoverers by a model residual of greater than 10 from their 70% predicted recovery. The histogram inset shows the model residuals of proportional recoverers (light gray) and limited recoverers (dark gray). 188 Janice J. Lin, PharmD, Pharmacy Frequency of oral chemotherapy drug interactions in hospitalized patients K. Yamamoto3,2, J.J. Lin2, E. USA, 2Northeastern University, Boston, MA, USA and 3Pharmacy, Cambridge Health Alliance, Boston, MA, USA Introduction: With quickly increasing oral chemotherapy (OC) options, drug-drug interactions (DDIs) have increased. The benefit of OC regimens that allow patients to self-administer at home has a downside of potential unsupervised changes in home medications that may result in OC DDIs. In addition, the rapidity of new OC approvals and subsequent provider unfamiliarity may lead to a lack of knowledge regarding possible DDIs. Unfortunately, DDIs may negatively impact oncology outcomes by decreasing chemotherapy efficacy, or causing adverse reactions requiring treatment delay or causing patients to refuse further treatment. The purpose of this study was to evaluate OC DDIs in the inpatient setting and identify potential areas for pharmacist intervention and patient outcome improvement.Methods: A retrospective review of oncology patients receiving OC while inpatient at MGH from April 2, 2016 to April 2, 2018 was conducted. Patients were included if they were 18 years old or greater; had a cancer diagnosis; and received OC while hospitalized. Patients were excluded if they were enrolled in a clinical trial. The primary objectives of this study were to assess the prevalence of OC DDIs, computer system warnings regarding OC DDIs, the relevance of the warnings, incidence of pharmacist interventions and types of pharmacist interventions attempted. Secondary endpoints were DDI origin (home medication vs new medication inpatient), mechanism of DDIs, DDI severity and possible outcomes, reason given for overriding electronic warnings, and if the patient was discharged on interacting medication(s). The following was collected from the electronic medical record: demographic information; laboratory data; oral chemotherapy regimen; concurrent interacting medications; electronic DDI warnings; and associated pharmacist interventions. DDIs were identified by entering 164the medication list into Lexicomp\u00ae Drug Interactions database, and DDI severity ratings were assigned according to the American Society of Clinical Oncology Standards. Descriptive statistics were used for data analysis.Results: A total of 58 patients were included, representing 73 encounters. The most common cancer was leukemia (n=41, 56.2%), and most common OCs were dasatinib (n = 12, 16.4%) and mercaptopurine (n=12, 16.4%). interacting medications were ordered in 44 of 73 encounters (60.2%). Out of 111 total interacting medications ordered, 94 were administered (84.7%), with a median of 2 IMs administered (IQR 0-3) per encounter. The most commonly ordered interacting medications were acetaminophen (14%, n = 15), omeprazole (10%, n = 11), atorvastatin (6%, n = 7), and famotidine (6%, n = 7). The electronic medication ordering system generated 219 warnings involving oral chemotherapy (median 1.5 per encounter, IQR 0-4), which accurately identified 40 of 111 DDIs (36.0%). The most frequent DDI mechanism was additive pharmacodynamic effects (49.5%), followed by cytochrome P450 (27.9%), and gastric pH alterations (18.0%). Pharmacists made 29 intervention attempts in 18 of 44 encounters (40.9%) with DDIs; the most common interventions were additional chemotherapeutic monitoring (n = 15, 51.7%) and therapeutic monitoring (n = 4, 13.8%). The most frequent outcomes of the 29 interventions were IM stopped (n = 13, 44.8%) and extra monitoring occurred (n = 11, 37.9%). The most common documented reasons for overriding warnings selected by the pharmacist were \"will monitor\" (n = 22), \"patient tolerated before\" (n = 4), and \"benefit outweighs risk\" (n = 4). Home medication lists were the source of 46 IMs (41.4%). Sixty-five interacting medications (58.6%) were still present on discharge medication lists. Of the 111 total DDIs found, the majority were of moderate severity level (n = 64, 57.7%), and severity level (n = 30, 27.0%). The most common possible outcomes of the DDIs were that the interacting medication had additive pharmacodynamic toxicity that could potentially increase oral chemotherapy toxicity (n = 34), that oral chemotherapy had additive pharmacodynamic toxicity that could potentially increase interacting medica - tion toxicity (n = 30), oral chemotherapy could decrease interacting medication's metabolism (n = 28), and the interacting medication could decrease oral chemotherapy absorption (n = 20). Conclusion: Drug-drug interactions involving OC occurred in the majority of inpatient encounters where patients received OC (60.2%). Home medications that were continued while the patient was hospitalized accounted for 41.4% of inpatient OC DDIs, indicating that OC DDIs are also occurring in the outpatient setting. Of the interacting medications ordered, 58.4% were present on discharge medication lists, indicating that interacting medications that were added during admission were also continued on discharge. Documented attempts at pharmacist interventions were attempted only 40.9% of the time. A contributing factor to low intervention rates may be an inaccuracy of electronic system warnings. Electronic DDI recognition system improvements may increase DDI identification, but updates to these systems at MGH may be difficult or slow due to the use of a third party DDI library, necessitating need for careful clinician screening for DDIs. Other potential interven - tions include pharmacist discharge medication review of all patients on OC, establishment of inpatient and outpatient OC consult services, and implementation of outpatient OC pharmacist-led clinic. An OC discharge medication list review service was proposed for MGH oncology floors (Lunder 9, Lunder 10, and Ellison 16) at the April 2019 oncology pharmacy and therapeutics meeting. 189 India A. Lissak, Neurology Soluble ST2 is an Inflammatory Rosenthal* Department of Neurology, Massachusetts General Hospital, Boston, MA, USA Introduction: The biological mechanisms that influence abnormal cortical neurophysiology after aneurysmal subarachnoid hemorrhage (SAH) are uncertain. We hypothesized that soluble ST2 (sST2), a plasma marker of the innate immune response, is associated with events of electroencephalography (EEG) deterioration including new epileptiform abnormalities (EAs) or new EEG background deterioration. Methods: From a prospective IRB-approved biospecimen repository, we evaluated patients with at least 3 days of EEG monitoring and an early sST2 measurement (collected 5 days following SAH). EAs were defined as sporadic epilepti-form discharges, lateralized rhythmic delta activity (LRDA), lateralized periodic discharges (LPD), or generalized periodic discharges (GPD). Background deterioration was defined as decreasing Alpha Delta Ratio (ADR), Relative Alpha Variability (RAV) or worsening focal slowing. The association between sST2 level and EEG-identified EAs or new background deteri - oration was compared using the Wilcoxon Rank sum test.Results: 46 patients met inclusion criteria, Early sST2 was collected at mean 3.5\u00b1 0.9 days after SAH; 23 patients had a subsequent sST2 measurement at 10\u00b12.5 days. 17 (37%) patients developed new EAs during EEG monitoring, 21 (46%) developed new background deterioration, and 8 (17%) developed neither. Median sST2 in patients developing new EAs was higher (114.8 ng/ml [IQR 73.8-196.8]) than in patients who new EAs (74.7 ng/ml [IQR 44.8-101.5], 165p=0.024). This association between elevated sST2 and new EAs was not present for sST2 samples collected at later time points. There was no difference in sST2 levels between patients who developed new background deterioration (75.7 ng/ml [IQR 44.5-120]) compared 65.5-174.6], p=0.56). Conclusion: Among patients admitted with aneurysmal SAH, elevated sST2 in the first 5 days is associated with the develop- ment of new EAs on EEG monitoring. This association was not present at later time points, suggesting that the early inflam - matory response may be linked to abnormal cortical neurophysiology. 190 Jiaxuan (Jessie) Liu, MPH, Center for Genomic Medicine (CGM) The time-dependent association between socioeconomic position and DNA methylation during childhood: findings from a prospective, longitudinal British cohort J. Liu 2,1, J. Cerutti2, Y. Zhu2, of Epidemiology, Harvard T.H. Chan School of Public Health, Boston, MA, USA, 2Center for Genomic Medicine, Massachusetts General Hospital, Boston, MA, USA, 3Department of Psychiatry, Harvard Medical School, Boston, MA, USA and 4Stanley Center for Psychiatric Research, Broad Institute of MIT and Harvard, Cambridge, MA, USA Introduction: Growing up poor, or in a family of low socioeconomic position (SEP), is one of the strongest risk factors for future socioeconomic disadvantage and poor future health outcomes. Past research has shown that low childhood SEP is not only linked to socioeconomic wellbeing in adulthood, but also many lifelong physical and mental health risks. Growing evidence suggests that low-SEP may be particularly harmful when experienced very early in development. It has been reported in previous studies that children living in low-SEP neighborhoods were at an increased risk for mental health problems as early as age 2, and, by age 4, children from low-SEP families scored on average lower on standardized tests of cognitive development and academic achievement compared to their high-SEP peers. Although SEP is one of the biggest predictors of health and well-being across the life course, the biological mechanisms explaining these relationships, or how SEP \"gets under the skin,\" remains relatively unknown. One potential explanation is that the socioeconomic environment modifies the epigenome through DNA methylation (DNAm) marks, which in turn affect the expression of genes in ways that influence future health and disease risk. In fact, recent studies have found a significant association between different indicators of SEP and DNAm. However, few of these studies have examined the extent to which developmental timing of SEP in childhood matters with regards to DNAm patterns. Thus, this study sought to test the hypothesis that there are sensitive periods for childhood socioeconomic disadvantage that are associated with epigenome-wide DNAm alterations. Recognizing that there are alternative theoretical models that have been proposed to explain the epigenetic impacts of childhood SEP, we also tested two other models from life-course theory\u2014accumulation and social mobility\u2014to determine which one(s) explained the most variability in DNAm at age 7. Methods: This study used data from the Accessible Resources for Integrated Epigenomics Studies (ARIES), a subsample of mother-child pairs from the Avon Longitudinal Study of Parents and Children (ALSPAC; N=694-805). Six household- and neighborhood-level SEP indicators were assessed in very-early childhood (age 0-2), early childhood (age 3-5), and middle childhood (age 6-7), in various domains including parental employment, household income, financial hardships, and neighborhood quality. Epigenome-wide DNAm was measured from cord blood at birth and peripheral blood leukocytes at age 7 using Illumina Infinium Human Methylation 450k BeadChip microarray. For each of the six SEP measures, a two-stage structured life course modeling approach (SLCMA) was used to test three theoretical models to determine which model explained the most variability (meaning R 2) in age 7 DNAm: 1) a sensitive period model, in which the effect of low-SEP depends on the developmental time period of the exposure; 2) an accumulation model, in which the effect of low-SEP increases with the number of occasions exposed, regardless of timing; and 3) a mobility model, in which upward or downward change in SEP across development predicts DNAm patterns. See Figure 1 for more details on study design and SLCMA.We used Bonferroni and false discovery rate corrections for multiple testing to identify DNAm loci significantly associated with adversityand examined the biological significance of detected DNAm loci. As a sensitivity analysis, we further tested a fourth alternative model\u2014a recency model, in which the effect of low-SEP is stronger for more proximal events. Results: Preliminary analysis showed that DNAm is associated with SEP at both household- and neighborhood-levels. Different life-course theoretical models were selected for different SEP measures, suggesting that the mechanisms behind how SEP shaped DNAm profiles varied across SEP domains. Particularly, very-early childhood seemed to be a sensitive period when low income and income reduction affected DNA methylation profiles most, while middle childhood appeared to be a sensitive period for financial hardship and parental job loss. In addition, downward mobility of SEP was found to be associated with DNAm at several loci, including worsening major financial problems from early to middle childhood, and decreasing subjective neighborhood quality from very-early to early childhood as well as early to middle childhood.Conclusion: Our early results suggest important roles of developmental timing of socioeconomic disadvantage as well as time-dependent mobility during childhood in regard to its effect on DNAm. Future analysis will further assess the robustness 166of preliminary results and examine potential sex differences. The life course theoretical model(s) that best explain(s) the relationship between childhood SEP and DNAm alterations may help optimize the timing of interventions or programs aimed at reducing the harm of socioeconomic disadvantage throughout childhood. Figure 1 . Study timeline (A) and SLCMA models (B) 191 Yichuan Liu, Athinoula A. Martinos Center for Biomedical Imaging Automatic recognition of the modality of a MRI series from clinical stroke datasets with a recurrent convolutional Biomedical Imaging, Charlestown, MA, USA, 2J. Philip Kistler Stroke Research Center, Massachusetts General Hospital, Boston, MA, USA, 3Department of Neurology and Rehabilitation Medicine, University of Cincinnati College of Medicine, Cincinnati, OH, USA, 4Veterans Affairs Maryland Health Care System and University of Maryland School of Medicine, Baltimore, MD, USA, 5Department of Neurology, Universitat Autonoma de Barcelona, Barcelona, Spain, 6Department of Laboratory Medicine, the Sahlgrenska Academy at University of Gothenburg, Gothenburg, Sweden, 7Department of Neurosciences, KU Leuven - University of Leuven, Leuven, Belgium, 8Division of Endocrinology, Diabetes and Nutrition, University of Maryland School of Medicine, Baltimore, MD, USA, 9Department of Neurology, Mayo Clinic, Jacksonville, FL, USA, 10Department of Clinical Sciences Lund, Lund University, Lund, Sweden, 11Department of Neurology and Rehabilitation Medicine, Sk\u00e5ne University Hospital, Lund, Sweden, 12Henry and Allison McCance Center for Brain Health, Massachusetts General Hospital, Boston, MA, USA, 13Department of Neurology, University of Miami, Miami, FL, USA, 14Department of Neurology, Medical University Graz, Graz, Austria, 15Institute of Cardiovascular Research, Royal Holloway University of London, Egham, United Kingdom, 16Department of Neurology, Jagiellonian University Medical College, Krakow, Poland, 17Department of Neurology, Austin Health, Heidelberg, VIC, Australia, 18Departments of Neurology and Public Health Sciences, University of Virginia, Charlottesville, VA, USA, 19Stroke Division, Florey Institute of Neuroscience and Mental Health, Heidelberg, VIC, Australia and 20Department of Radiology, Sk\u00e5ne University Hospital, Lund, Sweden Introduction: Fundamental advances in precision stroke research will require collecting and pooling advanced imaging phenotype data from multiple centers. However, the mechanisms and infrastructure to enable data sharing and integration on a massive level have yet to be developed for imaging data. When data are shared, deidentification is mandated, but de-identification procedures often remove information that can help identify the image modality, such as the series descrip-tion. For studies involving stroke images from multiple centers, the data typically has to be viewed and manually labeled by experienced neuroimaging analysts for categorizing the MRI modalities. This process is labour intensive and may introduce human error. In this work, we present a recurrent convolutional neural network (RCNN) for the automatic recognition of the 167modality of an MRI series. Using the RCNN, we aim to significantly reduce the time and human effort required for curation and harmonization of clinical MRI datasets.Methods: We randomly selected 1000 subjects from the MRI-GENetics Interface Exploration (MRI-GENIE) study and partitioned them into 800 training, 100 validation and 100 testing subjects. We categorized the MRI modality of the image series into 24 group such as Fluid-attenuated inversion-recovery, T1-weighted, T2-weighted and Diffusion tensor imaging. The overall architecture of the proposed RCNN model is shown in Figure 1. It consists of a modified AlexNet to extract features from 2D slices. To reduce overfitting, AlexNet was pretrained on the ImageNet dataset. The fully connected layers of AlexNet were then finetuned on the MRI dataset. Since clinical MRI data may be 3-D or 4-D (time series or multiple- echoes), a gated recurrent unit (GRU) neural network was used for aggregating information from multiple 2D slices before making the final prediction.Results: We achieved an MRI modality classification accuracy of 97.5% on the testing set. An F1-score of over 95% is achieved for all categories except for DWI/BZERO, MRAraw and T1post. The macro F1-score and weighted F1-score 94.4% and 97.5%, respectively. Conclusion: In conclusion, a RCNN is designed for the classification of MRI modalities for automated curation and harmoni - zation of clinically acquired MRI dataset. Using this model, we implemented a pipeline on the American Heart Association's Precision Medical Platform that can automatically de-identify and harmonize clinically acquired MRI datasets. This pipeline can facilitate curation and harmonization of data across multiple centers to advance stroke research. Structure of the recurrent convolutional neural network. 192 Regina Longley, B.A., Psychiatry Total Health: Pragmatic Collaborative Care for Cardiac Inpatients with Depression or Anxiety R. Longley1, C.M. Celano1, MGH, USA and 3Psychiatry, BWH, Boston, MA, USA Introduction: Clinical depression and anxiety disorders are commonly present in patients hospitalized for acute cardiac conditions and are both independently linked to adverse patient-related outcomes (e.g., poor physical function, low health- related quality of life (HRQoL), readmissions, and mortality). Low identification rates and failure to initiate treatment for these psychiatric conditions in this highly vulnerable population at a key clinical timepoint contribute to adverse effects on cardiac outcomes. Collaborative care (CC) models utilize a non-physician care manager to identify psychiatric conditions and coordinate care for mental health treatment within patients' existing medical care. Standard CC has been used in cardiac patients and improved psychiatric symptoms; however, it has not reduced adverse medical outcomes. \"Blended\" CC models, which target both psychiatric and medical conditions concurrently, lead to improvements in both mental and physical outcomes, but they typically only focus on one psychiatric illness (e.g., depression) and have only been studied in outpatient settings. Accordingly, we developed a telephone-based, pragmatic CC program (i.e., Total Health) for the management of depression and two anxiety disorders in patients hospitalized for an acute cardiovascular event.Methods: In this randomized, controlled trial (N=260), we aim to examine the efficacy of the Total Health CC intervention compared to enhanced usual care (eUC; systematic notification of providers about psychiatric symptoms). Eligible partici - pants will consist of adults admitted to one of two academic medical centers with ACS (unstable angina or acute myocardial infarction) or acute HF and who are found to also be experiencing clinical depression, generalized anxiety disorder (GAD), and/or panic disorder (PD). After completion of baseline outcome measures in the hospital, patients are randomized to either the Total Health or eUC intervention. Total Health utilizes a nurse care manager (CM) to deliver (via phone) the 26-week long CC program in which the CM coordinates care among the patient, study team specialists, and patients' primary medical providers. This intervention enacts (1) treatment of psychiatric conditions until remission is reached, (2) evidence-based interventions to promote cardiac health behaviors via goal-setting and motivational interviewing (MI), and (3) cardiac illness management by combining pharmacotherapy with patient self-monitoring of weight and blood pressure to identify opportunities for treatment adjustment. Outcomes will be measured at 26 and 52 weeks via telephone by a blinded outcomes assessor. The trial's primary outcome is physical function, which will be measured by the Duke Activity Status Index (DASI). Psychological status, HRQoL, health behaviors, healthcare satisfaction, cost-effectiveness, and cardiovascular outcomes will 168also be measured. Between group differences on changes in outcomes will be assessed using an analysis of response profiles model with a categorical effect of time and an unstructured covariance matrix to account for within-patient correlation.Results: Recruitment began in October of 2017 and is currently ongoing. To date, a total of 96 eligible patients have been randomized (45 total health, 48 enhanced usual care). The mean age for subjects upon enrollment is 65.7 years, and 49.5% presented with a primary diagnosis of ACS. Pre-existing condition rates were measured for coronary artery disease (63.4%), ACS (38.7%), prior coronary artery bypass grafting (19.4%), Type 2 Diabetes (44.1%), hyperlipidemia (78.5%), and hypertension (85.0%). Sixteen participants are current smokers (17.2%). Baseline means for depression, scores measured by the Symptom Checklist-20 (range 0-80), and anxiety, scores measured by the Hospital Anxiety and Depression Scale - Anxiety subscale (range 0-21), were 31.7 and 8.9, respectively.Conclusion: The Total Health study will provide data regarding whether an intervention that simultaneously manages psychi - atric and cardiac disorders, while remaining cost- and time-efficient in the hospital setting, results in appreciable improve-ments in health behaviors, physical function, psychiatric symptoms, and medical outcomes in high-risk cardiac patients. 193 Sara E. Looby, Nursing Focus Group Informs Advertisement, Content, and Feasibility of an Online National Survey on Added Sweeteners for People with HIV E. Kileel1, C. Rivard1, K.V. Fitch1 and S.E. Looby1,2 1Metabolism Unit, Medicine, Massachusetts General Hospital, Boston, MA, USA and 2Yvonne L. Munn Center for Nursing Research, Nursing, Massachusetts General Hospital, Boston, MA, USA Introduction: Obesity is a rising health concern in people with HIV (PWH), and recent data show that poor diet quality and increased intake of added sweeteners in PWH, relative to the general population, may impact obesity rates in this population. Few studies to date have sought to better understand sweetener intake and importantly, how social determinants of health may affect sweetener intake in PWH. To develop a sustainable, patient-centered intervention to educate and moderate added sweetener intake in PWH, the investigators are currently conducting a national online survey to evaluate added sweetener knowledge and consumption in this population. This abstract shares results from a focus group among PWH conducted prior to launching the national survey study to determine clarity and length of the survey content, feasibility of use an online survey platform, and insights on advertisement strategies. Methods: PWH were recruited from the community to participate in a 90-minute, single session focus group. Investiga-tors monitored recruitment to ensure participants were of diverse sex, race, and education level. Participants completed a demographic questionnaire and rotated through 3 evaluation stations monitored by the investigators. Evaluation domains included: (1) Online Survey Content; (2) Online Survey Feasibility; (3) Advertisement Evaluation. Participants evaluated each domain via surveys comprised of multiple choice and open-ended questions, and engaged in open dialog. Survey data were analyzed using descriptive statistics. Field notes taken during the session were reviewed by the investigators. Feedback from the focus group was integrated into the survey content, online survey platform, and final advertisement for national outreach. Results: Ten PWH participated in the focus group:40% female;70% non-Caucasian; mean age 55\u00b17 yrs.; duration of HIV 20\u00b19 yrs. Based on participant feedback, the total number of survey questions was reduced by nearly 20 questions. Greater than 50% of participants found the feasibility of use of the online survey to be \"easy\" for all functionalities except for accessing survey from the online advertisement . Final advertisement image, font and tagline were chosen based on the most commonly selected options in the focus group survey. A frequent suggestion recorded in the field notes was to decrease and condense the total number of survey questions for ease and efficiency of survey completion.Conclusion: Feedback from focus group participants refined the content and length of the online survey and informed the final advertisement aesthetics. Engaging PWH from the community in the early phase of study development is an effective strategy to help investigators determine appropriateness, clarity, and feasibility. Gleaning input and perspectives from the intended study population regarding a proposed study may enhance engagement and enrollment, expedite study completion, and yield more meaningful results.169 194 Olivia M. Losiewicz, AB, Psychiatry From Ecology to Psychology: Critical Slowing Down as a Predictor of Panic Attack for Anxiety and Traumatic Stress Disorders, Massachusetts General Hospital, Boston, MA, USA and 2Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: Panic attacks, characterized by extreme arousal and perceived threat, often arise \"out of the blue.\" Yet, despite their seemingly unpredictable nature, previous research points to a phenomenon known as critical slowing down which may help signal their impending onset (Hoffman et al., 2016). Drawn from ecology literature, critical slowing down refers to a slowed recovery of a dynamical system as it approaches a tipping point into an alternative stable state (e.g. from lake to desert or from a state of well being to a state of mental disorder). For example, someone who experiences consistent family discord may exhibit a steady increase in overall stress and begin to struggle to recover from other minor life stressors. They may reach a tipping point after an unpleasant conversation and shift into a depressive state. One indicator of critical slowing is an increased autocorrelation in the components of the dynamical system (i.e. an increased correlation of the component with itself over time). In this study, we used a computational model of Panic Disorder to investigate whether an increased autocorrelation in arousal signals the onset of panic attacks.Methods: The computational model is a set of difference equations implemented in the statistical computing platform R that define the relationships among Panic Disorder symptoms and how they evolve over time. We used this model to simulate three conditions. In Condition 1, we increased the mean arousal linearly over one \"week\" until the system crossed its tipping point, fell into the \"vicious cycle\" of feedback between arousal and perceived threat, and entered a state of panic. In Condition 2, we increased the strength of the effect of arousal on perceived threat, thereby lowering the system's tipping point until it entered the \"vicious cycle\" and, thus, a state of panic. In Condition 3, we made no changes to the system. For all conditions, we measured the autocorrelation in arousal over time.Results: In Condition 1 and 2, the autocorrelation of arousal grew steadily leading up to the onset of the panic attack (r between time and the arousal autocorrelation = .80 and .85, respectively), providing evidence of critical slowing down. In our control condition, the autocorrelation remained comparatively flat over time (r = .22). Conclusion: Our results indicate that either increasing arousal or strengthening the relationship between arousal and perceived threat produces critical slowing down, as indicated by an increasing autocorrelation in arousal as the system approaches the 170tipping point. Notably, in Condition 2, we observed a modest decline in the autocorrelation immediately prior to the panic attack. Accordingly, increasing arousal autocorrelation may not indicate an imminent panic attack, but rather may be a warning signal for increasing panic attack vulnerability. The assessment of critical slowing down may be able to identify those who could benefit from intervention prior to developing Panic Disorder. Future studies should collect patient data in real time and use these computational models to identify optimal times for preventative intervention. 195 Kelsey Lowman*, AB, Psychiatry Behavioral Changes in Smokers from the PCORI Pragmatic Trial \"Integrated Smoking Cessation Treatment for Smokers with Serious Mental Illness\" K. Lowman*1, R. Plummer*1, M. Massachusetts General Hospital, Wakefield, RI, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Adults with serious mental illness (SMI) in the US have a 29-year mortality gap compared to the general population, primarily due to smoking-related illnesses. Little is known about factors contributing to smoking behavior change in this population. Methods: An ongoing 3-year pragmatic trial of a system-level intervention has enrolled 1166 smokers with SMI who receive psychiatric rehabilitation services from two community-based agencies in Greater Boston. The intervention consists of a provider-level educational intervention focused on evidence-based smoking cessation treatment for adults with SMI, alone or in combination with community health worker support. The study is currently in its third and final year. Participants completed a brief survey about their past-year smoking behavior at baseline and again one year later (Y1; N=876). Survey questions included type and total number of tobacco products smoked per day (TPPD, assessed as a continuous variable and in categories of 0=10, 1=11-20, 2=21-30, and 3=31 tobacco products) and the time between waking and smoking their first tobacco product (assessed categorically as 3=within 5 minutes, 2=6-30 minutes, 1=31-60 minutes, and 0=>60 minutes), which together comprise the Heaviness of Smoking Index (HSI; scores range from 0-6, where higher scores indicate greater severity of nicotine dependence). Participants reported past-year provider recommendations and prescriptions for smoking cessation pharmacotherapies and completed an expired carbon monoxide (CO) breath test. Finally, participants reported lifetime health conditions and demographic information. For these analyses, participants were classified as \"reducer\" (RR) if they reported a categorical decrease in self-reported TPPD from baseline to Y1, and \"non-reducers\" (NR) if they did not reduce TPPD from baseline to Y1. Analyses explored changes in smoking behavior (measured by HSI and exhaled CO) and health in the first two years of the project, and characteristics of participants who reduced the number of tobacco products smoked per day.Results: Of the 876 participants who completed surveys at both baseline and Y1, 38.9% (n=341) were RR, 54.2% (n=475) (n=49) had quit smoking at Y1. At baseline, there were no differences between RR and NR in age, sex, race, receipt of smoking cessation pharmacotherapy recommendation or prescription in the past year, or self-report of a lifetime smoking-related illness, including heart and respiratory diseases, diabetes, and cancer. Baseline TPPD, expired CO, and HSI score were higher in RR than NR (mean TPPD=20.9\u00b110.7 vs. 12.7\u00b18.7, vs. 2.5\u00b11.5, p<0.001). Time to first tobacco product was longer at baseline in RR than NR (M=2.2\u00b10.9 vs 2.0\u00b11.0; p=0.001). At baseline, RR were less likely to report their tobacco product of choice to be cigarettes (51.8%, n=176 vs. 62.0%, n=294; p=0.01) as opposed to mini-cigars, loose tobacco, or a combination of products. The mean change in HSI from baseline to Y1 for RR was -1.4\u00b11.3, and for NR it was 0.40\u00b11.3 (=-0.6, z=-13.2, p<0.001). Since TPPD (assessed categorically) is one component of the HSI that inherently changes for RR, we performed an explor-atory analysis isolating the change in the second component of the HSI - time to first tobacco product - as the dependent variable. RR reported significantly greater increase in time to first tobacco product compared to NR ( =-0.2, z=-2.9, p<0.005). Conclusion: Reduction in tobacco use was associated with reduced severity of nicotine dependence. At baseline, partici - pants who went on to reduce their tobacco use in Y1 (RR) smoked more tobacco products, had higher expired CO, and had greater severity of nicotine dependence, suggesting that heavier smokers were more likely to derive early benefit from the smoking cessation intervention. Those who did not reduce smoking in Y1 (NR) were more likely to endorse cigarettes as their tobacco product of choice, suggesting that this subset of participants may require augmented or alternate intervention efforts to aid smoking reduction or cessation. Importantly, RR reported increased time to first tobacco product from baseline to Y1 compared to NR. This finding suggests that smokers with SMI who reduce their tobacco intake may do so at least in part through this behavioral change.171196 Dennis Lu, Anesthesia, Critical Care and Pain Medicine Echocardiographic Assessment of Biventricular Function Predicts Survival in Patients on Extracorporeal Membrane Oxygenation due to Cardiopulmonary Arrest D. Lu, J. Ortoleva, G. Cudemus and A. Dalia Anesthesiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Extracorporeal membrane oxygenation (ECMO) is a therapeutic option for patients with refractory pulmonary, cardiac, or cardiopulmonary failure. Recently, VA ECMO has been expanded for patients in cardiopulmonary arrest, and in these cases is known as Extracorporeal Cardiopulmonary Resuscitation, or eCPR. However, survival rates of VA ECMO for eCPR patients are lower compared to non-eCPR patients (295 versus 42%). Given the lower survival of the eCPR population and the worse cardiac function in patients undergoing eCPR than those undergoing VA ECMO for non-eCPR indications, there is a particularly great need for validation of non-invasive or minimally invasive assessments of myocardial recovery. Since echocardiography is commonly used to guide placement of VA-ECMO, we wanted to assess whether cardiac function assessed either by transesophageal echocardiogram(TEE) or transthoracic echocardiogram(TTE) can predict outcome and survival in patient after VA ECMO for eCPR. Methods: Retrospective chart review-based study in a single quaternary care center from 2009 to 2018 for all patients receiving VA ECMO for the purpose of eCPR. Charts were reviewed for age, duration of the ECMO run, survival of the ECMO run, survival of the hospitalization, as well as echocardiographic assessments of biventricular function using either TEE or TTE. Biventricular function was assessed based on the American Society of Echocardiography guidelines. For all patients, a pre-cannulation biventricular function was assessed using the closest echo just before ECMO cannulation (\"pre\" study) and biventricular function during cannulation was assessed using the echo during ECMO cannulation (\"during\" study). If patient survived after ECMO decannulation, a post ECMO decannulation biventricular function was assessed using the closet echo study after decannulation (\"post\" study). In comparing the echocardiographic assessments of biventricular function in survivors and non-survivors, the F-test was utilized to assess for differences in variance between groups and subsequently, where appropriate, homoscedastic or heteroscedastic unpaired T-tests were used to determine statistically significant differences in biventricular function between survivors and non-survivors.Results: Patients that survived the ECMO run tended to have better RV function during ECMO (3.2/5 or moderate dysfunc-tion), compared to 2.5/5.0 in non-survivors (p =0.011). In those that survived ECMO, LV function averaged to 2.87 vs 2.375 in non-survivors. This difference was not statistically significant. However, survivors of the hospital admission had statistically significantly higher LV systolic function during ECMO than those that expired (3.0/5 vs 2.4/5, p = 0.038). LV function during the ECMO run in those that survived the hospitalization was clinically and statistically significantly higher (4.4/5 vs 2.5/5).Conclusion: In this cohort of 72 patients placed on ECMO for the purposes of eCPR, it appears that basic echocardiographic parameters of biventricular function during the ECMO period may have prognostic utility. These results should be verified by review of other eCPR cohorts for validity. 172197 Michael T. Lu, MD, Radiology Deep Learning to Assess Long-term Mortality From Chest Radiographs M.T. Lu1, A. Ivanov1, MGH, Boston, MA, USA and 2Radiation Oncology, Dana Farber Cancer Institute, Boston, MA, USA Introduction: Chest radiography is the most common diagnostic imaging test in medicine; it may also provide informa- tion about longevity and prognosis. The objective was to develop and test a convolutional neural network (CNN, named CXR-risk) to predict long-term mortality, including noncancer death, from chest radiographs. Methods: CXR-risk CNN development (n = 41 856) and testing (n = 10 464) used data from the screening radiography arm of the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial (PLCO) (n = 52 320), a community cohort of asymptomatic nonsmokers and smokers (aged 55-74 years) enrolled at 10 US sites from November 8, 1993, through July 2, 2001. External testing used data from the screening radiography arm of the National Lung Screening Trial (NLST) (n = 5493), a community cohort of heavy smokers (aged 55-74 years) enrolled at 21 US sites from August 2002, through April 2004. The CXR-risk score (very low, low, moderate, high, and very high)was determined based on CNN analysis of the enroll - ment radiograph. The main outcome was all-cause mortality. Prognostic value was assessed in the context of radiologists' diagnostic findings (eg, lung nodule) and standard risk factors (eg, age, sex, and diabetes) and for cause-specific mortality.Results: Among 10 464 PLCO testing participants (mean 62.4 [5.4] years; years [interquartile 6.0-6.7 years]), there was a graded association between CXR-risk score and mortality. The very high-risk group had mortality of 53.0% (PLCO) and 33.9% (NLST), which was higher compared with the very low-risk group (PLCO: unadjusted hazard ratio [HR], 18.3 [95% CI, 14.5-23.2]; NLST: unadjusted HR, 15.2 [95% CI, 9.2-25.3]; both P < .001). This association was robust to adjustment for radiologists' findings and risk factors (PLCO: adjusted HR [aHR], 4.8 [95% CI, 3.6-6.4]; NLST: aHR, 7.0 [95% CI, 4.0-12.1]; both P < .001). Comparable results were seen across age and sex strata, for lung cancer death, and for noncancer cardiovascular death and respiratory death.Conclusion: Based on a single chest radiograph, the deep learning CXR-risk score stratified the risk of long-term mortality. Individuals at high risk of mortality may benefit from prevention, screening, and lifestyle interventions. 198 Shu Y. Lu, MD, Anesthesia, Critical Care and Pain Medicine Impact of Rescue Echocardiography and Ultrasonography on Management of Hemodynamically Unstable High- Risk Cardiac Patients within a Medical-Surgical Intensive Care Unit S.Y. Lu and K.T. Shelton Anesthesia, MGH, Cambridge, MA, USA Introduction: Rescue echocardiography/ultrasonography is a point-of-care examination of the heart, great vessels, lungs, or abdomen in unstable patients used to identify etiologies for hemodynamic instability. This has revolutionized the manage - ment of critically ill patients and is recognized by the national board of echocardiography to be an integral part of critical care medicine. While the role of rescue echocardiography has been demonstrated to result in rapid changes in patient management in trauma and intraoperative settings, its use and impact in the ICU setting remains unclear. The aim of this study was to review rescue echocardiography/ultrasonography in a diverse surgical and medical cardiac ICU at a single tertiary care and ECMO Center of Excellence hospital.Methods: A retrospective observational study was performed on 189 rescue echocardiography/ultrasonography exams performed between November 2017-November 2018 on 141 patients within a 34 bed cardiac medical and surgical ICU. All the rescue exams were performed by the staff intensive care physician and reviewed by two independent reviewers. Data points collected include demographic data (age at the time of ICU admission, sex, ASA classification, and RCRI Score), time and type of rescue examination, indication for rescue examination, rescue exam finding, interventions, and time to interventions. Results: A total of 189 rescue echocardiography/ultrasonography (7% TTE, 19% TEE, 3% Lung ultrasound, 3% FAST exam, 1% vascular exam) were performed on 141 patients. The eight most common indications for rescue ultrasound included hypotension (n=90; 3%). 139 (74%) of the rescue exams demonstrated pathologies in (n=21; consult (n=12; 6%). Of the ultrasound studies which resulted in interventions, 105 interventions (n=81%) occurred within 1 hour.Conclusion: In our retrospective study, rescue echocardiography/ultrasonography demonstrated a wide variety of etiolo - gies of hemodynamic instability with the most common finding compassing less than 30% of total studies. Majority of the rescue echocardiography/ultrasonography resulted in rapid changes in management. The heterogeneity of diagnosis and the frequency of intervention as a result of rescue echocardiography/ultrasonography further supports its benefit in the manage - ment of unstable critically ill patients. 199 Bart Lubberts, MD, PhD, Orthopedics Arthroscopic Assessment of Syndesmotic Instability in the Sagittal Plane B. Lubberts, J. multi-directional, occurring in the coronal, sagittal, and rotational planes. Despite the multitude of studies examining such instability in the coronal plane, other studies have highlighted that syndesmotic instability may instead be more evident in the sagittal plane. The aim of this study was to arthroscopically assess the degree of syndesmotic ligamentous injury necessary arthroscopic evaluation of the syndesmosis, first with all syndesmotic and ankle ligaments intact and subsequently with sequential sectioning of the anterior inferior tibiofibular ligament (AITFL), (IOL), the posterior inferior tibiofibular ligament (PITFL), and deltoid ligaments (DL). In all scenarios, an anterior to posterior (AP) and a posterior to anterior (PA) fibular translation test were performed under a 100N-applied force. AP and PA sagittal plane translation of the distal fibula relative to the fixed tibial incisura was arthroscopically measured. Results: Compared with the intact ligamentous state, there was no difference in sagittal fibular translation when only one or two ligaments were transected. After transection of all the syndesmotic ligaments (AITFL, IOL, and PITFL), after partial transection the syndesmotic ligaments (AITFL, IOL) alongside the DL, fibular translation in the sagittal plane signifi-cantly increased as compared with the intact state (p-values ranging from p=0.041to p<0.001). The optimal cut-off point to distinguish stable from unstable injuries was equal to 2mm of fibular translation for the total sum of AP and PA translation (sensitivity 77.5%; specificity 88.9%).Conclusion: Syndesmotic instability appears in the sagittal plane after injury to all three syndesmotic ligaments or after partial syndesmotic injury with concomitant deltoid ligament injury. The optimal cut-off point to arthroscopically distinguish stable from unstable injuries was 2mm of total fibular translation. This data can help surgeons arthroscopically distinguish between stable syndesmotic injuries and unstable ones that require syndesmotic stabilization.174 Arthroscopic syndesmotic assessment of right-sided electronic goniometer, a standardized 100N posterior to anterior directed force is applied to the distal fibula via the electronic force gauge. Sum of the anterior and posterior translation (mean in mm) for three different sequences of ligamentous transection. (A) Intact, (C) Intact, PITFL, IOL, AITFL, and Error bars represent standard deviations. * Asterisks denote p-value<0.05. Each stage of transection was compared with the intact state. Broken lines are representing the cut-off value which had the highest sensitivity and specificity for the detection of Effect of levetiracetam on time to high-dose methotrexate clearance in patients with malignancies S. T. Nguyen3, A. Zhou2 and U. Lou1 1Department of Pharmacy, Massachusetts General Hospital, Boston, MA, USA, 2Northeastern University, Boston, MA, USA and 3Massachusetts College of Pharmacy and Health Sciences University, Boston, MA, USA Introduction: High-dose methotrexate (HDMTX) is a key therapy for treating hematological malignancies that involve or are at risk of involving the central nervous system (CNS) due to its ability to cross the blood-brain barrier. However, HDMTX clearance is affected [LUY1] by drug interactions, including interactions with antiepileptics, particularly older antiepileptics. HDMTX interactions with antiepileptics are concerning since some patients with CNS disease require seizure prophylaxis or control with antiepileptic drugs. Levetiracetam, a newer antiepileptic, has been proposed as an agent that may be used with HDMTX, but there has been conflicting data on its effect on HDMTX clearance. Two case reports have suggested that concomitant levetiracetam delayed HDMTX clearance while a retrospective case-control study suggested no clinically signif- icant interaction between the two medications. This retrospective case-control study investigated the association between methotrexate (MTX) clearance and concomitant levetiracetam.Methods: The institutional review board approved this study. The electronic medical record was used to generate a list of adult patients who received HDMTX at Massachusetts General Hospital, and patients were included if they received their first cycle of HDMTX, defined as MTX dose greater than or equal to 500 mg/m 2, between April 2, 2016 and October 1, 2018, for a hematologic malignancy. The electronic medical record was evaluated for the following: patient demographics and laboratory values; concomitant levetiracetam; time to MTX clearance; other non-levetiracetam interacting medications; HDMTX-related adverse events. Baseline characteristics and adverse events were characterized using descriptive statistics and compared using the independent t-test for continuous variables and either chi-square or Fisher's exact tests for categor - ical variables. The primary endpoint, time to HDMTX clearance, was evaluated using analysis of covariance (ANCOVA) 175while accounting for of times urine pH fell below 7; and hours urine output (UOP) was less than 100 mL/hr. Pre-planned subgroup analyses were conducted in patients receiving MTX doses less than or equal to 3500 mg/m2, and those receiving MTX doses greater than 3500 mg/m2. Correlation of levetiracetam dose with time to MTX clearance was also assessed using linear regression.Results: A total of 121 patients were included in the study. The study population was 59.5% (n = 72) male with a mean age of 61 years (standard deviation [SD] 16). The most common diagnosis was primary CNS lymphoma (47.9%, n = 58), followed by diffuse large B-cell lymphoma (33.1%, n = 40). The mean MTX dose was 4551 mg/m 2 (SD 2053). There was no significant difference in patient demographics between patients who received concomitant levetiracetam and those that did not. A total of 30 patients received concomitant levetiracetam (24.8%) at a median total daily dose of 1435 mg (range 900 to 3000 mg). Mean time to methotrexate clearance was 82.5 hours (SD 51.2; 95% confidence interval [CI] 69.4 - 95.7) in patients who received concomitant levetiracetam and 72.4 hours (SD 31.2, 95% CI 61.7 - 83.0) in patients who did not, which was not significantly different (p >0.05), even in the subgroup of patients receiving MTX doses >3500 mg/m 2, where mean time to clearance was 91.3 (SD 39.8) hours in the concomitant levetiracetam group versus 86.7 (SD 49.1) hours in the non-levetiracetam group. Out of the covariates placed in the model, only MTX dose and the number of co-administered non-levetiracetam medications significantly affected the time to MTX clearance (p<0.05), and concurrent use of levetirac - etam was not significantly associated with time to methotrexate clearance. Simple linear regression in patients who received concomitant levetiracetam indicated that levetiracetam explained 4.9% of the variance and total daily dose of levetiracetam was thus not predictive of hours to clear MTX with F(1,28) = 1.45, p = 0.24. Adverse events of any grade occurred in 96.7% (n= 29) of the concomitant levetiracetam group versus 95.6% (n = 87) of the non-levetiracetam group, with grade 3 or higher events occurring in 33.3% (n = 10) of the concomitant levetiracetam patients and 34.1% (n= 31) of the non-concomitant levetiracetam patients.Conclusion: These results do not support the case reports of levetiracetam affecting HDMTX elimination but are consistent with the larger retrospective study finding no effect. For patients with hematological malignancies receiving levetiracetam while receiving HDMTX, levetiracetam appears to have minimal clinical effect on HDMTX clearance. 201 Debra Lundquist, PhD, Nursing \"I'm Still Mom\": The Lived Experience of Young Mothers with Advanced Breast Cancer D. Lundquist1, D. Berry3, M. Boltz2, S. deSanto-Madeya4 and P. Grace4 1MGH Cancer Center, Boston, MA, USA, 2Penn State, University Park, PA, USA, 3University of Washington, Seattle, WA, USA and 4Boston College, Chestnut Hill, MA, USA Introduction: Little is known about daily life experiences of young mothers with advanced breast cancer. Limited research suggests they face unique challenges differing from those of women at other life stages and with earlier stages of breast cancer. We previously published findings from an hermeneutic qualitative study exploring the experience of young women living with advanced breast cancer. An overarching theme and five major themes were identified. The larger study aimed to describe and interpret the lived experiences of young women with advanced breast cancer to contribute to our understanding of the needs of this population. Their role as mothers emerged as a significant theme. The theme, \"I'm Still Mom\" emerged and yielded rich data reflective of the their roles as mothers in the context of living with advanced breast cancer. Improved understanding of the daily living experience of this population may help inform patient-centered education and interven- tions for this population. The purpose of this study was to conduct a detailed content analysis of the theme \"I'm Still Mom\" described by young women living with advanced breast cancer.Methods: As previously described in the original study, 12 women (aged 25-39) with advanced breast cancer were purposively recruited via private Facebook groups specifically for women with breast cancer. Enrollment continued until thematic saturation was achieved. Data were collected through one or more semi-structured interviews over 6 months depending upon participant willingness, desire, or ability. Journals were provided to write additional thoughts. Data were drawn from interviews about daily life experiences. A major theme related to their identities as mothers emerged. Within the theme, \"I'm Still Mom\", an in-depth qualitative analysis was conducted using Van Manen's method to establish subthemes. Member checking established credibility and provided the opportunity to check data, interpretations and conclusions with participants. Results: Twelve AYA women (mean age:35.9 years) were included. All had at least one child (10 months -14 years, median, 6.0 years). Twelve participated in the first interview, 9 in a second interview, and 6 in a third interview for the purpose of member checking. Three returned journals. The theme: \"I'm Still Mom\" had \"it's not easy for my kids\".176Conclusion: Being a mother is the first priority for these women with advanced breast cancer, but they are hindered in their parenting activities by physical effects of their cancer and its treatment, uncertainty about their future, and worry for their children's future. This study provides a base for further research on daily priorities for this population to inform patient-cen - tered education and future interventions to optimize quality of life consistent with their parenting priorities. 202 Alexandre A. Lussier, PhD, Psychiatry Genetic contributions to depressive symptom trajectories across childhood and adolescence A.A. Lussier1,2, M. E.C. Dunn1,2,5 1Department of Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Psychiatric and Neurodevelopmental Genetics Unit, Center for Genomic Medicine, Boston, MA, USA, 3Department of Psychiatry and Behavioral Sciences, University of Washington School of Medicine, Seattle, WA, USA, 4Department of Epidemiology, Harvard T.H. Chan School of Public Health, Boston, MA, USA, 5Center on the Developing Child, Harvard University, Cambridge, MA, USA and 6Veteran Affairs Puget Sound Healthcare System, Seattle, WA, USA Introduction: Depression is one of the most common, costly and disabling mental disorders worldwide, with lifetime prevalence estimates of 11.7% among adolescents and 16.6% among adults in the Unites States. Although depression can emerge at any point in the life course, nearly one third of individuals report their first onset of depression before age 21. These early-onset cases of depression are associated with worse illness course and outcomes in adulthood. While depression is known to be driven, in part, by genetic susceptibility, genetic studies have mainly focused on the presence vs. absence of lifetime depression or level of depressive symptoms obtained through cross-sectional measurements. To better understand genetic contributions to early-onset depression, present study examined different genetic influences on depressive symptoms trajectories spanning from childhood to adolescence. To achieve this goal, we first assessed the extent to which polygenic risk scores (PRS) could differentiate between different depressive symptom trajectory classes. Building on the PRS results, we attempted to identify the genetic loci that might be associated with depressive symptom trajectories, as well as the onset and persistence of depressive symptoms.Methods: Data came from the Avon Longitudinal Study of Parents and Children (ALSPAC), a prospective, longitudinal birth cohort. The current study is based on an analytic sample of 7308 children who met the following inclusion criteria: singleton births with genotype data and measures of depressive symptoms completed at 4, 7, 8, 9.5, 11.5, 13, and 16.5 years of age. Trajectories of depressive symptoms from age 4 through 16 were constructed using a growth mixture model. PRS values were generated using summary statistics for the GWAS of major depression from the Psychiatric Genomics Consortium wave 2 (PGC-MDD2; 59,851 cases, 113,154 controls). We used MAGMA to identify gene-level genome-wide associations with depression symptom trajectories (high/stable vs minimal symptoms), intercept of depression symptoms (onset), and area under the curve of symptoms (persistence).Results: We identified six distinct classes of depressive symptom trajectories in our analytic sample. The samples were assigned to the following classes as follows: 1872 in high/stable (25.6%), 432 in high/unstable (5.9%), 507 in childhood decrease (6.9%), 254 in late childhood peak (3.5%), 193 in adolescent spike (2.6%), and 4050 in minimal symptoms (55.4%). We identified a linear relationship between the PRS and membership to three classes. Specifically, as PRS decreased, the proportion of individuals in the minimal symptoms class increased (p=0.0018, r 2=0.095). By contrast, the proportion of the high/stable and high/unstable classes increased with at higher PRS (HS: p = 0.016, r2= 0.057; HU p = 0.034 ; r2 = 0.045). There was also evidence that the PRS could differentiate between the classes, with three contrasts showing significant 177differences following multiple-test correction (high/stable vs minimal symptoms; high/stable vs childhood decrease; high/ unstable vs minimal symptoms). Although no genes reached statistical significance in the genome-wide analysis of depres-sion symptoms trajectory (which compared high/stable symptoms vs minimal symptoms), two genes (ZFAND1 and SYCP2L) reached the suggestive threshold of p<1e-4. Furthermore, when modeling intercept and area under the curve (AUC) as continuous variables to capture depression onset and persistence, respectively, we again found no genes reached formal statistical significance (p < 2.93e-6). However, several genes approached the suggestive threshold across both analyses of onset and persistence (p<1e-4), including SIX5, DMPK, and DMWD, suggesting that common factors may influence these two phenotypes.Conclusion: Taken together, these findings suggest that certain trajectories of depressive symptoms from age 4 to 16 are influenced by genetic mechanisms, namely the high/stable, high/unstable, and minimal symptoms classes. Furthermore, common genetic loci may drive the onset and persistence of depressive symptoms across childhood and adolescence. Overall, these results provide insight into the genetic mechanisms leading to depression phenotypes in children and adolescents and should be combined with future studies to increase the statistical power to detect gene-level associations. 203 Elyse A. Lynch, B.A, Psychiatry Residual hyperarousal and mood symptoms and their relationship to sleep outcomes in a two-week intensive outpatient program for Veterans with PTSD 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3University of Illinois in Urbana-Champaign, Champaign, IL, USA Introduction: Post-9/11 veterans are at high risk for post-traumatic stress disorder (PTSD) following deployment. Extensive research has demonstrated an association between PTSD and insomnia, with sleep disruption often listed as a central feature of PTSD. Individuals with comorbid PTSD and sleep disturbance frequently present with increased symptom severity, with established links between insomnia and specific PTSD symptom clusters, namely criterion D, 'negative alterations in cognition and mood' and criterion E 'hyperarousal' symptoms. Trauma focused Cognitive Behavioral Therapies (tf-CBT), such as cognitive processing therapy (CPT) and prolonged exposure therapy (PE) can improve symptoms of PTSD and insomnia. Intensive outpatient programs (IOPs) for PTSD have demonstrated efficacy in the treatment of PTSD, however, little is known about residual PTSD symptoms and their relationship to sleep disturbance in an intensive treatment format. The current study aimed to examine the relationship between PTSD and insomnia in an IOP for post-9/11 veterans with PTSD. Methods: N=326 (82.9% diagnosed with PTSD participated in a two-week IOP in which they received daily, tf-CBT as well as group and integrative therapies. The Posttraumatic Stress Disorder Checklist (PCL-5) and the Insomnia Severity Index (ISI) were used to asses for symptom severity for PTSD and Insomnia, respectively. A multiple linear regression was employed to determine whether specific symptom clusters of PTSD were associated with insomnia severity at endpoint. PCL-5 scores were recalculated to omit the two sleep items, one in cluster B (intrusion symptoms) and one in cluster E (hyperarousal symptoms).Results: The results of the multiple regression indicated the four symptom clusters in the PCL-5 at baseline and endpoint, as well as baseline insomnia, explained 54.1% Endpoint predicted by two symptom clusters of PTSD at endpoint; both hyperarousal ( = .272, p<.01), and negative alterations in mood and cognition ( = .276, p<.05). When controlling for depression, findings held, therefore the original model was utilized.Conclusion: Residual hyperarousal and mood symptoms predicted insomnia symptoms for post-9/11 veterans with PTSD following tf-CBT in an IOP. Further research is warranted to determine if targeting hyperarousal symptoms may result in reduced insomnia severity following treatment. Future studies should consider employing objective measurements for insomnia.178204 Hannah Madden, BS, Anesthesia, Critical Care and Pain Medicine Disparities in Inadvertent Dural Puncture in Delivering Women H. Madden1, T. Houle1, C. Warrick2, M. Farber3 and L. Leffert1 1Anesthesia, Massachusetts General Hospital, Boston, MA, USA, 2Anesthesiology, University of Utah, Salt Lake City, UT, USA and 3Anesthesiology, Perioperative and Pain Medicine, Brigham and Women's Hospital, Boston, MA, USA Introduction: Recent studies in the obstetric anesthesia literature found that Spanish speaking Hispanic and other non- English speaking parturients are less likely to receive neuraxial anesthesia than their English speaking counterparts. The authors theorize that language barriers may be present hindering communication and understanding of analgesic options in the peripartum setting. We hypothesize that for similar reasons, complications of neuraxial anesthesia, specifically inadvertent dural puncture (ADP) and resulting postdural puncture headache (PDPH), will be increased in non-white women who report a non-English language preference.Methods: This multi-site retrospective study was conducted using electronic medical record data extracted for all deliveries between April 1, 2016 and December 1, 2017 at Massachusetts General Hospital (MGH) and Brigham and Women's Hospital (BWH), using the Research Patient Data Registry. Cases of inadvertent dural puncture (ADP) from an epidural procedure were identified by ICD-9 and ICD-10 codes for postdural puncture headache and epidural blood patch, and from complication clusters, with confirmation of each case by chart review. Data collected from the medical record included age, race, language preference, body mass index (BMI), gravidity, parity, hospital length of stay, and readmissions or revisits. An interim analysis was performed for one institution, MGH, in which parturients were classified by primary language (English vs non-English) and race (white vs non-white). Results are presented as multivariable adjusted odds ratios with 95% confidence intervals, with p value < 0.05 considered statistically significant. A secondary analysis was performed for both institutions, using propensity score matching to match controls to our primary cohort in a 2:1 fashion. Cases and controls were matched on institution, maternal age, race, language preference, mode of delivery, body mass index (BMI) (+/- 2.6), and parity. Results: Among the 6,306 overall deliveries at MGH, inadvertent dural puncture was present in 64 (1%) cases. Most parturients were white and reported English as their primary language. The highest absolute ADP event rate was observed in the non-white/non-English speaking parturients. In univariate analysis there was a small increase in risk of ADP observed in non-white/non-English speaking parturients compared with white/English speaking parturients. After controlling for age and BMI, there was a statistically significant increased risk for ADP in non-white/non-English speaking parturients compared with white/ English speaking parturients, aOR = 2.6 (95% CI 1.2 to 5.4) p = 0.016. In a secondary analysis using propensity score matching, 32 of the confirmed cases of inadvertent dural puncture at MGH were matched to 57 controls and 43 of the confirmed cases at BWH were matched to 62 controls. There was an observed increase in the rate of revisit/readmission and average length of hospital stay, for the confirmed cases overall. 49.3% of cases of parturients with ADP had a revisit or readmission within two weeks of delivery discharge compared with 26.9% of their matched controls ( 2 (1) = 10.1, p < .001). There was not a statisti- cally significant observed difference in the average length of hospital stay in the cases cohort vs controls cohort. The observed mean length of hospital stay for the cases was 2.63 days compared with a mean of 2.39 days for the matched control group. Conclusion: Successful neuraxial procedures are achieved through optimal positioning and effective communication, which may be limited by language barriers. Poor communication or lack of effective communication can increase the difficulty of performing neuraxial procedures leading to increased risk of inadvertent dural puncture. The number of non-English speaking parturients in the US population is increasing, therefore it is crucial that excellent medical interpreter services be available at all times to facilitate the best possible communication and care. These results warrant further validation in a larger cohort. 205 Susan Maher, MS-PhD-C, Anesthesia, Critical Care and Pain Medicine Association of pre-admission sleep disturbance with new-onset delirium in elderly acute care orthopedic trauma patients S. Maher1,2, S. Quraishi1, C. Zhou3, L. Ye4, M. Heng5, Pelt2,1, O. Johnson-Akeju1 and E. Franco Garcia6 1DACC, Massachusetts General Hospital, Boston, MA, USA, 2Nursing, Northeastern University, Boston, MA, USA, 3Beth Israel Deaconess Medical Center, Boston, MA, USA, 4Northeastern University, Boston, MA, USA, 5Orthopedic Trauma, Massachusetts General Hospital, Boston, MA, USA and 6Geriatric Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Delirium in hospitalized patients is associated with prolonged length of stay, increased utilization of healthcare services, and higher one-year mortality.1-4 Among elderly acute care orthopedic trauma patients, the incidence of delirium has been reported to be as high as 60%.5 Recent evidence suggests that a history of sleep disturbance may increase the risk 179of delirium in elective surgery patients;7-9 [A1] however, such an association remains underexplored in trauma patients. Therefore, our goal was to investigate the relationship between pre-admission sleep disturbance and new-onset delirium among elderly, acute care, orthopedic trauma patients. Methods: We performed a retrospective analysis of data from the Massachusetts General Hospital (MGH) Geriatric Inpatient Fracture Trauma Service (GIFTS) research repository. All patients who had received a GIFTS consultation between January 2017 through August 2018 were initially considered for inclusion in the study. From this cohort, we excluded all patients with a known history of dementia as well as those with moderate or severe cognitive impairment during initial GIFTS consultation. We also excluded patients with a secondary admission diagnosis of altered mental status or delirium. Among the remaining patients, to investigate whether pre-existing sleep disturbance prior to hospitalization is associated with new-onset delirium, we performed a multiple variable logistic regression analysis, while controlling for biologically relevant covariates including: marital status, and 6) number of sleep-related home medications. A history of sleep disturbance was defined as any pre-existing sleep disorder being documented in the medical record before hospital admission for fracture management or a self-reported history of chronic sleep disturbance. Delirium was assessed on admission and daily by a board-certified geriatrician over the course of hospitalization using the Confusion Assessment Method for the Intensive Care Unit (CAM-ICU) tool.Results: A total of 644 patients were identified within the GIFTS research repository over the study period. 132 patients met exclusion criteria, leaving 512 patients in the analytic cohort. The incidence of delirium, as defined by a positive CAM-ICU score during hospital admission, was 19% (n=99). Patients with a known history of sleep disturbance were 74% more like to develop delirium (aOR 1.74; 95% CI 1.02-2.95, p=0.04). When the analysis was further adjusted for admission to an intensive care unit during hospitalization, the results did not materially change (aOR 1.51; 95% CI 1.04-3.02, p=0.04). Additionally, age, frailty, and number of home insomnia medications were independently associated with new-onset delirium. Conclusion: Our results suggest that a pre-admission history of sleep disturbance increases the risk of new-onset delirium in hospitalized, elderly, acute care, orthopedic trauma patients. Future studies are needed to determine whether sleep optimiza - tion in this cohort of patients can reduce the incidence of delirium and improve overall healthcare outcomes. 206 Cindy Malhotra, PharmD, Pharmacy Evaluation of the use of erythropoietin-stimulating agents in a hospital setting to assess the necessity of a protocol- driven anemia management service C. Malhotra1, S. Patel1, M. Y. Ghoneim1, A. Fenves2 MGH, MA, USA and 2Nephrology, MGH, Boston, MA, USA Introduction: Erythropoietin stimulating agents (ESA) have revolutionized the management of anemia and have demonstrated substantial improvement in anemia. However, these agents are not always utilized with proper monitoring parameters, which can present significant safety concerns and unwarranted drug cost-expenditure. This study was designed to evaluate the utilization of ESAs at a large, academic medical center and assess the necessity for a protocol-driven anemia management service. This is a collaborative effort between memebers of the Pharmacy and Nephrology departments.Methods: This was a retrospective study that assessed the utilization of all ESAs in non-intensive care unit, hospitalized patients from August 18, 2018 to August 31, 2018 at a tertiary medical center using an established guideline-based assess-ment criteria. The guideline-based assessment criteria were developed by referencing the Kidney Disease Improving Global Outcomes and National Comprehensive Cancer Network best practice guidelines. Results: A total of 167 doses of ESA were administered and evaluated during the study period. Of these doses, 86% (n = 144) were utilized in accordance with the guideline-based assessment criteria since iron levels were obtained within 1 month of ESA administration. However, this study revealed that 24% (n = 40) of ESA doses were utilized in patients with active, untreated iron deficiency at the time of administration. Conclusion: The majority of ESA doses were utilized in accordance with the guideline-based assessment criteria established for this study. However, interventions can still be implemented to further improve anemia treatment. Implementing a protocol-driven anemia management service is one strategy that can improve patient care, advance patient safety, and be cost-beneficial.180207 Christian Mancini, BS, Medicine - Allergy/Immunology/Rheumatology High-Cost High-Need Patients: The Prevalence and Impact of Penicillin Allergy K. Blumenthal1,2,3, N. Oreskovic3,4,5, X. Fu1, F. Shebl2,3, Walensky2,3,6 1Division of Rheumatology, Allergy, and Immunology, Department of Medicine, Massachusetts General Hospital, Boston, MA, USA, 2The Medical Practice Evaluation Center, Massachusetts General Hospital, Boston, MA, USA, 3Department of Medicine, Harvard Medical School, Boston, MA, USA, 4Division of General Academic Pediatrics, Massachusetts General Hospital for Children, Boston, MA, USA, 5Integrated Care Management Program, Massachusetts General Hospital, Boston, MA, USA and 6Division of Infectious Diseases, Department of Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Over 90% of patients who report a penicillin allergy have the allergy disproved when tested. Unnecessary use of alternative (non-beta-lactam) antibiotics can result in more treatment failures and adverse reactions. We described the prevalence and impact of a reported penicillin allergy in high-cost high-need (HCHN) patients. Methods: We identified HCHN patients in a care management program of an urban academic medical center (01/01/2014- 12/31/2016). Using multivariable logistic regression models, we determined the association between a reported penicillin allergy and antibiotic use; we used multivariable Poisson regression models to determine the association between a reported penicillin allergy, with or without multiple drug intolerance syndrome (3 reported drug allergies) and healthcare utilization.Results: Of 1,870 HCHN patients, 383 (20%) reported penicillin allergy, 835 (45%) had multiple drug intolerance syndrome, and 290 (16%) had both. HCHN patients reporting penicillin allergy had an increased odds of beta-lactam alternative antibi - otic use (adjusted Odds Ratio ([aOR] 3.84 [95%CI 2.17, 6.80]). Healthcare utilization was significantly higher for patients reporting a penicillin allergy alone (adjusted relative risk [aRR] 1.13 [95% CI 1.03, 1.25]), and with concurrent multiple drug intolerance syndrome (aRR 1.20 [95% CI 1.08, 1.34]). Conclusion: HCHN patients had a high burden of reported drug allergy. A reported penicillin allergy conferred a 4-fold increased odds of beta-lactam alternative antibiotic use. Reporting penicillin allergy, with and without multiple drug intoler - ance syndrome, was associated with significantly more healthcare utilization. HCHN care management programs should consider systematic drug allergy evaluations to optimize antibiotic use in these fragile patients. 208 Felisha Marques, MPH, Medicine - General Internal Medicine Measuring Shared Decision Making (SDM) for elective orthopedic procedures K. Sepucha1, T. Cha2, K. J. Giardina1 and S. Atlas1 1Division of Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Orthopedics, Massachusetts General Hospital, Boston, MA, USA and 3Division of General Internal Medicine, Brigham and Women's Hospital, Boston, MA, USA Introduction: This study assessed the quality of surgical decision making for elective hip and knee replacement and spine surgery at four hospitals across Partners HealthCare System.Methods: Patients undergoing elective hip or knee replacement for osteoarthritis (OA), or spine surgery for lumbar herniated disc (HD) or spinal stenosis (SS) with 36 participating surgeons were invited to complete a post-operative survey via mail. Median time between surgery and survey completion was 155 days (IQR=30). Two NQF endorsed measures were used to assess the quality of decisions. The Shared Decision Making Process (SDMP) is a 4-item scale (scores ranging 0-4) with higher scores indicating more shared decision making. The Informed, Patient-Centered (IPC) surgery score is a binary 181indicator derived from responses to 5 multiple-choice knowledge items and one treatment preference item. Patients made an IPC decision if they met or exceeded established knowledge thresholds and had a clear preference for surgery. Analyses adjusted for clustering of patients within surgeons. Results: 646/875 surveys (73.9% response rate) were returned with sufficient responses for analysis. The sample was female (51%), white (94%), with an average age of 65 (SD=11). OA patients had lower SDMP scores SDOA=1.15; MS=3.11, SDS=1.07; b=-0.57, p<.001). to decrease (b =-0.01, p=0.03). The hospital, patient sex, age, and race were not predictive of SDMP scores (all ps>0.28). Patients who had OA surgery were more likely to make IPC decisions (OA=69%, Spine=14%; OR=14.62, p<0.001). Both knowledge scores (mean=72% OA mean=36% for spine) and surgical preference (84% for OA vs. 67% for spine) were lower for spine. Additionally, males (50% v.47%; OR=1.58, p =0.01), and white patients (59% v. 41%; OR=1.8, p =0.047), were more likely to make IPC decisions, and as age increased, likelihood of IPC decisions decreased (OR=0.98, p =0.03). The hospital was not predictive of IPC decisions (all ps>0.17). Conclusion: Spine surgery patients reported more shared decision making in pre-operative conversations, but OA surgery patients were more likely to make informed, patient centered decisions. These findings may reflect a more complex decision-making process for spine surgery. Strategies to successfully implement shared decision making into clinical practice may need to vary based on the specific disease as well as the patient characteristics. 209 Thomas Mayrhofer, PhD, Radiology Cost-effectiveness analysis of non-invasive FFRCT for stable chest pain evaluation - A comparison to coronary CTA and functional testing based on the PROMISE Trial T. Mayrhofer1, J. Karady1, B. Massachusetts General Hospital, Boston, MA, USA, 2Oregon Health and Science University, Knight Cardiovascular Institute, Portland, OR, USA, 3Evanston Hospital, Cardiology Division, Chicago, IL, USA, 4Tufts Medical Center, Division of Cardiology, Boston, MA, USA and 5Duke University School of Medicine, Duke Clinical Research Institute, Durham, NC, USA Introduction: In patients with stable angina pectoris (SAP) but without known coronary artery disease (CAD) the impact of non-invasive fractional flow reserve (FFRCT) derived from coronary computed tomography angiography (CTA) on subsequent invasive coronary angiography (ICA) and revascularization and consequent long-term clinical outcomes and healthcare costs compared to alternative non-invasive diagnostic testing is unknown. Methods: A Markov microsimulation-model was developed based on individual patient demographics and cardiovascular risk factors of 10,003 patients with SAP requiring further non-invasive testing enrolled in the PROspective Multicenter Imaging Study for Evaluation of chest pain (PROMISE) trial. The model was informed by baseline patient characteristics, diagnostic accuracy of non-invasive tests. The likelihood of positive test results, ICA, coronary revascularization, medical treatment and effects thereof were simulated based on the underlying CAD status and progression of CAD over time. The model was validated with head-to-head comparison of observed versus model estimated 60-day and two-year clinical- (downstream patient management), health- (major adverse cardiovascular outcomes [MACE]) and economic (cost) outcomes of CTA and functional strategies (SPECT, stress echocardiography or exercise tolerance test). Then we simulated CTA and functional strategies as well as a strategy of CTA supplemented with FFR CT(CTA+FFRCT) applied only in patients with 30 to 69% luminal narrowing (32% of patients) and compared the rate of ICA, coronary revascularization, health and economic outcomes and quality adjusted life years (QALY) over lifetime. Results: Model Validation: Distribution and results of baseline noninvasivetesting, 60-day rates of ICA, coronary revascu- larization and costs as well as two-year MACE were highly similar between the model and the actual observations for both anatomic and functional testing (i.e.: 12.2% vs. 12.3% and 8.1% vs. 8.2% for 2.1% vs. 2.3% and 2.2% vs. 2.4% for MACE; respectively). Model Simulation supplementing coronary CTA with FFR CT: Over 60 days following initial testing, a CTA+FFRCT strategy compared to CTA only resulted in 15% fewer ICA (12.3% vs. 10.5%; equal to 90 ICA in PROMISE) and 5% fewer percutaneous coronary intervention (PCI). CTA+FFRCT and CTA strategies had higher PCI rates than functional testing at 60 days. Over lifetime, the PCI rate was lowest for CTA+FFRCT, followed by CTA and functional strategy (7.4% vs. 7.6% and 8.9%; respectively), while no differences were observed in MACE rates between the strategies. QALYs were similar for CTA and CTA+FFR CT and lower for functional testing (24.429, 24.423 and 24.398) while costs were highest for CTA, followed by functional testing and CTA+FFRCT ($7,737; $6,696, and $6,112; respectively). In a subgroup analysis limited to patients with 30 to 69% luminal narrowing, adding FFRCT to CTA resulted in 22.4% fewer ICA and a 16% increase in the proportion of ICA warranting PCI (figure 1) resulting in CTA+FFRCT being cost-effective with a cost per additional QALY of $33,053.182Conclusion: Over lifetime CTA+FFRCT has similar long-term health benefits at lower costs as compared to CTA only and better long-term health benefits at lower costs as compared to functional testing in patients with stable chest pain. Rate of ICA and subsequent intervention based on CTA Strategy and CTA+FFRCT Strategy. CAD=Coronary artery disease; CTA=CT angiography; tomography; ICA=Invasive coronary angiography; Revasc=Revascularization. 210 Catherine McCarthy, BS, Medicine Standardizing the PCP Follow-up Request Process for Patients at High-Risk of Readmission C. McCarthy1, M. Chapman2 and E. Morris3 1Case Management, MGH, Boston, MA, USA, 2Hospital Medicine, MGH, Boston, MA, USA and 3Department of Medicine, MGH, Boston, MA, USA Introduction: The MGH Stay Connected Program (SCP) serves medicine patients at high-risk for readmission by utilizing a multi-disciplinary team-based approach to enhance care transitions from the acute setting through 30 days post discharge. A patient's risk of readmission is determined by a risk score, calculated by an algorithm embedded in Epic. Patients with either a risk score of 27% or a qualifying opt-in diagnosis (CHF, COPD, AMI, PNA or cirrhosis) are eligible for intervention. Timely follow-up in primary care within 7-10 days is essential to a smooth transition. (Jackson et al. 115). Assistance with scheduling primary and specialty care follow-up is one of four interventions in the SCP \"bundle\". The Inpatient Administrative Coordinators (IAC) on each unit engage with patients to complete this vital component with the patient and/or family prior to discharge. The current process is for the discharging physician to place a referral to the IAC in Epic as discharge approaches; however, variation in individual practice varies from unit to unit with patients experiencing a wait ranging from 4.7-25.7 days before their PCP follow-up appointment. In September of 2018, the SCP Community Resource Specialist (CRS) and the IAC team piloted a centralized IAC referral with the aim of standardizing the timeliness of PCP post-hospitalization follow-up care. Methods: The IACs work with patients across medicine at MGH and have a keen awareness of the hospital's focus on the high readmission risk patient population; scheduling post discharge follow-up is one of several tasks the IACs perform to support inpatient medicine units. This pilot compared the time between hospital discharge and PCP follow-up, with the goal timeframe of 7-10 days. We developed this intervention using root cause analysis to ID key drivers of the failed timely PCP follow-up. This pilot focused on patients accepted into the MGH Stay Connected Program on one hospital medicine unit, Ellison 12. Upon receiving an SCP consult for a high-risk patient on this floor, the SCP CRS initiated an automatic IAC consult to schedule PCP follow-up. The language of the request was standardized across all SCP patients as a best practice. The IAC then coordinated with the inpatient team regarding expected discharge date and worked with the patient or family directly to schedule a follow-up appointment. Once scheduled with the practice, details of the follow-up appointment were documented in the patient's Epic After Visit Summary. The initial pilot ran for 6 weeks. Results: The pilot included a small sample size of 11 patients and demonstrated a reduction in the time between patient's hospital discharge and PCP follow-up from 25.7 to 4.7 days. The percentage of SCP patients scheduled for PCP follow-up within 10 days of discharge increased from 33% to 100%. Initial findings demonstrated a proof of concept with this pilot such that the process was integrated into regular practice for the role groups involved on this floor at the end of October 2018. During the initial pilot and sustainability period, 9/17/18-3/31/19, 91.8% of the 98 patients reviewed, discharged home with a PCP follow-up appointment scheduled. Furthermore, 82.4% of these patients discharged home with PCP follow-up scheduled within 10 days of discharge. The average number of days from discharge to PCP appointment was 6.5 days. Anecdotally, the SCP Care Coordination team who follow this patient population telephonically for 30-days post discharge, have found the close PCP follow-up appointments helpful for their patients. The current data demonstrate the sustainability and benefit of the minimal change from the original work flow.Conclusion: The MGH SCP provides enhanced transitional support for high-readmission risk medicine and cardiology patients. Ultimately, we focus on outpatient follow-up to support a safe care transition, optimize the site of care and reduce readmissions. 183Through these tactics, we aim to reduce overall cost of care and improve patient experience. By scheduling appointments with the patient/family, we hope to also reduce waste in the system by reducing missed/canceled appointments and the time associated with rescheduling appointments. To support this vulnerable patient population, the SCP helps to bridge patients between the acute and community setting. Works Consulted Jackson, C., et al. \"Timelines of Outpatient Follow-up: An Evidence-Based Approach for Planning After Hospital Discharge.\" The Annals of Family Medicine, vol. 12, no. 2, 2015, pp. 115-122., Doi:10.1370/afm.1753. Average days until post-discharge PCP follow-up appointment 211 Megan D. McCarthy, B.S., Psychiatry Assessment of a Novel Health and Fitness Program for Veterans with PTSD and/or TBI C.T. Gupta, M.D. McCarthy, R.J. R. Vander Weit, A. Hernandez and L.G. Sylvia Psychiatry, Massachusetts General Hospital, Somerville, MA, USA Introduction: Veterans with psychological disorders, including PTSD and depression, are at increased risk of being overweight or obese. We developed a six-month group health and fitness program, Warrior Health and Fitness (WHF) for veterans. This analysis aims to present efficacy, feasibility and acceptability data from this pilot.Methods: WHF provides group strength and conditioning sessions, individual nutritional evaluations, education, and yoga. Participants (N=39) are post-9/11 veterans and Participants complete a fitness assessment and self-assessments of sleep disturbance, alcohol use, global health, and depressive and anxiety symptoms at pre- and post-program.Results: We found significant change in percent body fat from pre- to post-program (t(16)=3.70, p=.002, d=0.90). Partici - pants reported significant reductions depression (t(31)=2.27, p=.03, d=0.40), sleep improvement in global health (t(38)=-2.89, p=.006, d=-0.46). No significant change was observed for anxiety symptoms. Conclusion: These data suggest that an integrative health intervention offered in group and individual formats may improve physical, psychological, and behavioral health symptoms amongst post-9/11 Veterans and Service Members. 212 Natalie McCormick, Ph.D., Medicine - Arthritis Decomposition Analysis of Spending and Price Trends for Biologic Anti-Rheumatic Drugs in Medicare and Medicaid N. McCormick1,3, Z. C. Sacks2,3, J. Hsu5,3,4 and H. MGH, Boston, MA, USA, 2General Internal Medicine, MGH, Boston, MA, USA, 3Medicine, Harvard Medical School, Boston, MA, USA, 4Health Care Policy, Harvard Medical School, Boston, MA, USA and 5Mongan Institute, MGH, Boston, MA, USA Introduction: Billions of public dollars are spent each year on biologic disease-modifying anti-rheumatic drugs (bDMARDs), but the drivers of bDMARD spending and per-patient cost increases are unclear. We characterized changes in total spending and unit-prices for bDMARDs in Medicare and Medicaid and quantified the major sources of spending increases for these public programs and beneficiaries.184Methods: Data Source and Measures: We accessed Medicare Part B (physician-administered medicatons), Part D (self-ad- ministered medications), and Medicaid drug spending data for years 2012-2016. These contained aggregated prescription claims for the >41 million beneficiaries enrolled in Medicare Parts B or Part D or Medicaid. All bDMARDs with FDA approval for 1 rheumatic disease through Dec. 2014 were included. Statistical Analysis: We calculated five-year changes in total spending and unit-prices for each bDMARD and in-aggregate, after adjusting for general inflation to 2016 dollars. We then performed standard decomposition analyses to isolate the contributions of four sources of spending growth (drug prices, uptake [# recipients], treatment intensity [mean # of doses/claim], and annual # claims/recipient). We conducted our analysis including statutory Medicaid rebates (as these decrease public spending), and both excluding and including Medicare rebates (as these are paid by manufacturers to Part D plans, not government or patients). We used time-varying rebates reported by the Congressional Budget Office.Results: From 2012-2016, annual spending on the 11 included bDMARDs by US public programs and beneficiaries nearly doubled (from $5.3 billion to $10.3 billion); drug prices increased by a mean of 52% (median 50%) in Part D and just 20% (median 13%) in Part B ( Table ). Controlling for general inflation, unit-price increases alone accounted for 56% ($1.7 billion) of the five-year, $3.0 billion bDMARD spending increase within Part D (Figure ); increased uptake accounted for 37% ($1.1 billion). Accounting for time-varying rebates, price hikes were still responsible for 53% ($1.4 billion) of the Part D spending increase. Adalimumab and etanercept, two of the oldest bDMARDs, were prescribed to the largest numbers of Part D beneficiaries (>47,000 in 2016) and had the biggest unit-price increases: 84% and 88%, respectively. In absolute terms, costs rose from $2,468 to $4,564 per-person/month for etanercept, and from $2,655 to per-person/month for adalimumab, with no change in mean per-person dose. Medicaid spending and price trends were similar to Part D (Figure ). The majority of spending growth for the oldest Part B drugs (rituximab, abatacept, and infliximab) was from price increases (72-88%), while for the five newer drugs (golimumab, ustekinumab, tocilizumab, certolizumab, and belimumab), increased number of recipients was the main driver (63-81% of spending growth). Conclusion: In this national study, post-market drug-price changes alone accounted for the majority of recent bDMARD spending growth, and manufacturers' rebates had little impact on these findings. Beyond rebates, policy interventions that directly target price increases, particularly under Part D (where patient cost-sharing tends to be higher), may help mitigate public-payer drug spending and out-of-pocket costs for the elderly and disabled beneficiaries who rely on bDMARDs. Five-Year Changes in bDMARD Spending, Uptake, and Unit Prices (drugs ranked in descending order of 2016 spending) SC=subcutaneous, IV=infusion Amounts paid by Medicare, beneficiaries (as deductible, coinsurance, or copayment), and third- parties Component proportion of Biologic DMARD spending increases from 2012 to 2016 (drugs ranked by 2016 spending), by public program, before-and-after rebates185213 Sophia K. McKinley, MD EdM, Health Professionals Education Research Enhancing elements of the formal preclinical curriculum to improve medical student perception of surgery S.K. McKinley1, Phitayakorn1 of Surgery, Massachusetts General Hospital, Boston, MA, USA, 2Department of Obstetrics and Gynecology, Massachusetts General Hospital, Boston, MA, USA and 3Department of Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: This study aims to determine the effect of formal, preclinical curricular interventions on medical students' perceptions of surgeons and surgical learning objectives and concerns. Methods: Thirty-eight medical students underwent a newly required, formal introduction to surgery during the preclinical curriculum. Two months later, these students were given surveys regarding their perception of surgery before and after a bootcamp-style transition to the wards workshop that immediately preceded their core clinical rotations. Student responses were compared to historical peers.Results: Thirty-seven students participated in the study (97.4%). Relative to historical peers, students demonstrated improved overall perception of surgery (71.2 vs 66.6, p=.046). A smaller proportion of students indicated that they were worried about evaluation (18.9% in 2018 vs 55.3% in 2017, p=0.001) and interactions with surgical educators (18.9% vs 50%, p=0.005). Students' overall perception of surgery significantly improved after participation in the transition to the wards workshop (71.2 to 77.8, p=<.0001), as did student agreement with 9 of 21 specific items. Improvement in surgical perception across the bootcamp-style workshop was similar to that of a prior workshop (8.6 in 2018 vs 6.4 in 2017, p=0.21). Conclusion: A preclinical introduction to surgery can have a positive impact on medical student perception of surgery prior to entry to the wards and may mitigate student concerns regarding their surgical rotation. 214 Sophia K. McKinley, MD EdM, Health Professionals Education Research A Qualitative Study of the Perceived Value of Participation in a New Department of Surgery Research Residents as Teachers Program S.K. McKinley, D. Cassidy, N.M. Sell, J.T. Mullen, N. Saillant, E. Petrusa, R. Phitayakorn and D. Gee Department of Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Few surgical training programs offer formal education training or structured teaching opportunities. This study aims to understand the perspectives of surgical residents who participated in a new Research Residents as Teachers Program (RRATP) at an academic surgical residency.Methods: We launched a RRATP for research residents that included a 6 hour workshop followed by formal longitu - dinal teaching opportunities across an academic year. Resident teachers were recruited to participate in semi-structured interviews regarding their experience in the program. Interviews were audio recorded, transcribed, and inductively analyzed for prominent themes. Cohen's kappa was calculated to determine interrater reliability.Results: Eight surgical research residents completed the RRATP workshop and subsequently provided a total of 330 teaching hours across the academic year (median=26 hours, range: 8-105). Interview participation rate was 100%, and kappa was 0.81. Four main themes were identified. First, residents reported increased knowledge of teaching principles with subsequent changes to their teaching practices. Second, participants identified a variety of environmental and programmatic factors that contributed to their development as a resident teacher. Third, participants reported numerous other personal benefits to participation ranging from technical skill improvement to increased social connection. Fourth, resident teachers indicated that the RRATP had broad positive consequences for the surgical department including improving its learning environment and patient care.Conclusion: A RRATP can generate a significant number of teaching hours by surgical research residents. Resident teachers perceived numerous benefits that were both proximal (benefits to self and learners) and distant (departmental culture change, patient care), suggesting a high value of formal education training to surgical residency programs.186215 Sophia K. McKinley, MD EdM, Health Professionals Education Research Identification of Specific Educational Targets to Improve the Student Surgical Clerkship Experience S.K. McKinley1, D. Cassidy1, E. Petrusa1 and R. Phitayakorn1 1Massachusetts General Hospital, Boston, MA, USA, 2Cambridge Health Alliance, Cambridge, MA, USA, 3Beth Israel Deaconess Medical Center, Boston, MA, USA, 4Brigham and Women's Hospital, Boston, MA, USA and 5Mount Auburn Hospital, Cambridge, MA, USA Introduction: This study describes the relationship between medical student perception of surgery, frequency of positive surgery clerkship activities, and overall surgical clerkship experience. Methods: 179 medical students at four academic hospitals completed pre- and post-clerkship surveys assessing 1) surgery clerkship activities/experiences and 2) perceptions of surgery.Results: 91% of students completed both a pre- and post-clerkship survey (n=162). Student perception of surgery significantly improved across the clerkship overall (p<0.0001) and for 7 of 21 specific items. 86% of students agreed that the clerkship was a meaningful experience. 66% agreed that the operating room was a positive learning environment. Multivariable logistic regression identified one-on-one mentoring from a resident (OR[95%CI]=2.12[1.11-4.04], p=0.02) and establishing a meaningful relationship with a surgical patient (OR=2.21[1.12-4.37], p=0.02) as activities predictive of student agreement that the surgical clerkship was meaningful. Making an incision (OR=2.92[1.54-5.56], p=0.001) and assisting in dissec-tion (OR=1.67[1.03-2.69], p=0.035) were predictive of student agreement that the operating room was a positive learning environment. Positive student perception of surgery prior to the clerkship was associated with increased frequency of positive clerkship activities including operative involvement (r=0.26, p=0.001) and relationships with surgical attendings (r=0.41, p<0.0001), and patients (r=0.24, p=0.003).Conclusion: Interventions to improve surgery clerkship quality should target enhancing student relationships with residents and surgical patients as well as providing opportunity for student operative involvement beyond just suturing. Additionally, fostering positive perceptions of surgery in the preclinical period may increase meaningfulness and experience with the later surgery clerkship. 216 Sophia K. McKinley, MD EdM, Health Professionals Education Research Medical Students' Perceptions and Motivations Prior to their Surgery Phitayakorn1 1Department of Surgery, Massachusetts General Hospital, Cambridge, MA, USA and 2Department of Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: This study aims to determine the effect of a pre-clerkship workshop on medical students' perceptions of surgery and surgeons and to describe their concerns and learning goals. Methods: Thirty-nine medical students completed surveys before and after a workshop preceding their surgery clerkship. Quantitative data and free responses that were inductively coded were used to assess effectiveness.Results: Perceptions from 38 students (response rate = 97.4%) significantly improved for 11 of 21 items. At pre-workshop, the most frequently cited learning goals were improving technical skills (58%), surgical knowledge (53%), and understanding surgical culture and work (53%). Students' top concerns were meeting clerkship demands (68%) and being evaluated (55%). After the workshop, student learning objectives and concerns remained largely unchanged. Conclusion: A pre-clerkship workshop improved student perceptions of surgery and surgeons. Understanding students' intrinsic motivations may facilitate future clerkship curriculum improvement via better alignment of educator and student goals and objectives.187217 Sophia K. McKinley, MD EdM, Health Professionals Education Research Feasibility and perceived usefulness of using head-mounted cameras to create longitudinal resident video portfolios following the ACS/APDS Core Skills Curriculum S.K. McKinley, D.A. Hashimoto, A. Mansur, D. Cassidy, C. Valle, Titus, E. Petrusa, J.T. Mullen, R. Phitayakorn and D.W. Gee Department of Surgery, Massachusetts General Hospital, Cambridge, MA, USA Introduction: There is limited guidance on how to longitudinally administer simulation materials or to incorporate video recordings into assessment portfolios of simulated surgical skills.Methods: We launched a longitudinal weekly simulation curriculum for PGY1-PGY3 surgical residents based on the ACS/ APDS Curriculum. Residents underwent monthly Objective Structured Assessment of Technical Skills (OSATS) while wearing head-mounted cameras. Videos of OSATS performance accrued into individual online video portfolios. Residents were surveyed about their attitudes toward video recording.Results: Twenty-seven general surgical residents participated, completing 161 OSATS encompassing 11 distinct skills and generating 258 videos of simulated skills performance. The overall survey response rate was 88%. Residents viewed the curriculum favorably overall, and 36.4% of residents accessed their videos. Of those who did not watch their videos, 78.6% cited not having enough time, while 28.6% did not think the videos would be useful. Over 95% of surveyed residents expressed interest in having a video library of attending-performed procedures, 59.1% were interested in having their own operations recorded, and 45.5% were interested in video-based coaching. Conclusion: Residents viewed longitudinal administration of the ACS/APDS Curriculum positively. While video recording in simulation is feasible, resident interest may be higher for intraoperative recordings than for simulated skills. 218 Sophia K. McKinley, MD EdM, Health Professionals Education Research A Pilot Study of Inpatient Satisfaction Rating of Surgical Resident Care S.K. McKinley, B.M. Wojcik, M. Kochis, A. Mansur, C.B. Petrusa, J.T. Mullen, L. Traeger and R. Phitayakorn Massachusetts General Hospital, Boston, MA, USA Introduction: Objective: to describe inpatient satisfaction with surgical resident care given the increasing importance of patient satisfaction as a quality metric. Methods: Surgical inpatients were invited to complete a survey that addressed their satisfaction with and attitudes regarding surgical resident care. The survey was based on the Consumer Assessment of Healthcare Providers and Systems Surgical Care Survey (S-CAHPS). Patients were required to positively identify photos of resident physicians prior to providing ratings. Adapted S-CAHPS items were scored using the \"top-box\" method. The study included general surgery inpatients recovering from elective, major abdominal surgery at Massachusetts General Hospital. Participants were recruited on post-operative days 2-4.Results: Ninety-one percent of approached patients participated (102/112, mean age=62.9, 51.6% male). Patients positively identified both seniors and interns 88% of the time. Thirteen seniors and 19 interns were rated, with 1-14 evaluations per trainee. Overall quality of care ratings for seniors and interns were 9.35 and 9.09 respectively (0-10 scale, 10=\"best possible care\"). Sixty-three percent of senior resident evaluations and 60% of intern evaluations received a score of 10. The proportion of residents receiving top-box scores ranged from 59.5% to 97.7% depending on the item. Forty percent of senior resident and 38% of intern evaluations received top-box scores for all 8 items. Over 96% of patients reported strong or moderate agreement with the statements \"I feel it is important to help in the education of future surgeons.\" Conclusion: Surgical inpatients willingly completed ratings about their surgery residents, typically can recognize their resident physicians, and rate quality of care highly. Despite many high ratings, there is room for improvement in some S-CAHPS domains. These results indicate patients are a valuable source of feedback regarding a resident's progress in several core competencies such as interpersonal skills, communication, professionalism, and patient care.188219 Sophia K. McKinley, MD EdM, Health Professionals Education Research A Comparison of Patient Satisfaction when Office-based Procedures are Performed by General Surgery Residents versus an Attending Surgeon S.K. McKinley, B.M. Wojcik, N. Amari, D.C. Chang, H. Wachtel, E. Petrusa, J.T. Mullen and R. Phitayakorn Department of Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Patient satisfaction is an increasingly important quality metric nationwide. However, the impact that surgical trainees have on satisfaction when they perform operations independently has not been studied.Methods: Prospective study conducted at a single academic institution from 10/2016 to 6/2017. An office-based post-pro-cedure survey was developed by adapting questions from the validated S-CAHPS. Top-box scoring was used to determine satisfaction for categorical questions and a comparison of the means was used for overall quality ratings (scale 0-10). Patients indicated whether their operation was completed by an attending surgeon or a PGY-3 general surgery resident. The primary outcome measured was satisfaction with overall quality of care. Individual questions were grouped by phase of care and composite scores were measured as a secondary outcome.Results: The survey response rate was 87.4% (n=195). There were no differences in patient demographics or the types of procedures performed by residents or an attending surgeon. Excision of a soft tissue mass (i.e. lipoma) accounted for 89.2% of all procedures performed (n=174). There were no differences between pre-procedure (resident=92.5% vs attending=94.2%) or post-procedure (resident=95.3% vs attending=97.7%) composite scores. There was a significant difference in peri- procedure satisfaction (resident=78.7% vs attending=90.7%, p=0.02). There was no difference in overall quality of care ratings given by patients who had their procedure performed by residents (9.84 \u00b1 0.5) versus or an attending surgeon (9.93 \u00b1 0.3, p=0.15). Finally, on adjusted analysis, resident care did not independently impact likelihood of a \"best possible care\" rating for overall quality of care (OR 0.84 \u00b1 0.27, CI 0.45-1.57, p=0.58). Conclusion: Patient satisfaction was very high when residents independently performed minor surgery in an office-based setting. Importantly, there was no difference in satisfaction with overall quality of care compared to an attending surgeon. This study demonstrates that high resident operative autonomy and patient satisfaction are not mutually exclusive goals. 220 Sophia K. McKinley, MD EdM, Health Professionals Education Research \"Yes, I'm the doctor.\" A model of assessing and addressing gender-based discrimination in the modern medical training era S.K. McKinley, L.J. Wang, R. Mainthia, M. Westfal, A.L. Merrill, E. Petrusa, K. Lillemoe and R. Phitayakorn Department of Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: While gender-based bias and discrimination (GBD) is known to exist in medical training, there is limited guidance for training programs on how to combat this issue locally.Methods: As part of a comprehensive departmental initiative to address GBD, surgery, anesthesia, and internal medicine residents at two academic hospitals were surveyed regarding GBD to better understand its perceived sources, frequency, forms, and effects. These data were used to inform local interventions to combat GBD faced by physicians-in-training.Results: The survey response rate was 60% (371/616, 197 females). Women trainees were more likely to endorse personal experiences of GBD than men (93% vs 24%, p<0.0001) with no effect of specialty (p=0.16). Patients (83%) and nursing staff (80%) were most frequently identified by participants as sources of GBD. Women survey respondents were significantly more likely to report experience of sexual harassment during training (34% vs 5%, p<0.0001), with no difference across specialties (p=0.65). While an overwhelming majority of both men (86%) and women (96%) either experienced or observed GBD in the training environment, less than 5% of respondents formally reported such experiences, most frequently citing a belief that nothing would happen. Survey results were translated into a variety of interventions including addressing 1) nursing and patients as sources of GBD, 2) low confidence in formal reporting mechanisms, and 3) the pervasiveness of GBD, including sexual harassment, across specialties.Conclusion: GBD is ubiquitous and disproportionately affects women residents across medical specialties. Individual training programs can incorporate local GBD data when planning interventions to address GBD. Future work should evaluate the effectiveness of any proposed interventions in reducing the burden of GBD faced by medical trainees.189221 Matthew McLaughlin, Psychiatry \"I'm really going hour by hour:\" Time perspective and its relationship to resilience in a population of HIV-positive MSM who use substances M. McLaughlin1, J. Kim1, K. Mayer2, C. O'Cleirigh1,2,3 and A. Batchelder1,2,3 1Massachusetts General Hospital, Boston, MA, USA, 2Fenway Health/The Fenway Institute, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: Resilience is most commonly understood as the ability to adaptively function following an adverse experi - ence. Though resilience is known to be influenced by a range of factors, effective strategies for cultivating resilience among people living with HIV (PLWH) have not been extensively studied or determined. Moreover, most analyses of resilience tend to omit time as a relevant variable, despite the fact that time perspective has been shown to impact human behavior. The purpose of this qualitative analysis was to determine how time orientations and perspectives related to resiliency in a sample of HIV-positive men who have sex with men (MSM) who use substances.Methods: We conducted 33 semi-structured qualitative interviews with MSM living with HIV with active substance use disorders who are poorly engaged in care. While the primary goal of the study was to assess relationships between intersecting internalized stigmas, avoidance, substance use, and engagement in HIV care, we subsequently used thematic analysis, informed by grounded theory, to perform a secondary analysis of the content initially coded as \"resilience.\"Results: Among the 29 participants with content coded as resilience, content indicative of past, present, and future time orientations were differentially used in relation to descriptions of adaptive or maladaptive coping with adversity. Most partici- pants conveyed two or more time orientations in relation to resilience. Present-focused, compared to past or future, orientation was the predominant orientation within this sample, described almost equally in relation to adaptive (e.g. short-term progres-sive thinking and self-awareness) and maladaptive (e.g. preoccupations with present circumstances and avoidant coping) attitudes and behaviors. Slightly more than half of the participants indicated past orientation, which was described as largely adaptive via reflections on past ways of living and prior adversity. Similarly, future orientation was described in relation to primarily adaptive attitudes and behaviors (e.g. desire to fulfill a purpose and motivation to avoid future complications) and was indicated in more than half of the participants.Conclusion: Our findings suggest that differing time orientations and perspectives can contribute to or detract from resilience. In this sample, present orientation was revealed as being associated with both adaptive and maladaptive attitudes and behaviors, while past and future orientations surfaced predominantly as adaptive. These findings indicate a need to further investigate the benefits and challenges of present-focused strategies, such as mindfulness, in conjunction with past and future orientation among people living with HIV and substance use disorders. 222 Devon McMahon, BA, Dermatology Barriers and Facilitators to Diagnosis of Kaposi's Sarcoma in Western Kenya: A Qualitative Study D. McMahon1, Butler6, L. Chemtai2, F. Asirwa2,3, and E. Freeman1 1Department of Dermatology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA, 2AMPATH, Moi University, Eldoret, Kenya, 3Indiana University, Indianapolis, IN, USA, 4University of San Francisco, San Francisco, CA, USA, 5Department of Medicine, Massachusetts General Hospital, Boston, MA, USA and 6University of Connecticut, Storrs, CT, USA Introduction: Kaposi's one of most common HIV-associated malignancies in sub-Saharan Africa. Although 69% to 86% of patients in sub-Saharan Africa are diagnosed with KS at an advanced stage, little is known about factors that delay diagnosis. The aim of this study was to use qualitative semi-structured interviews with KS patients to better understand barriers and facilitators to early diagnosis of KS.Methods: All patients >18 years with newly diagnosed Kaposi's sarcoma between 2016-2019 within the AMPATH (Academic Model Providing Access to Healthcare) clinic network in Western Kenya were enrolled in the parent study. Of this cohort, 25 participants were purposively selected to participate in semi-structured qualitative interviews to evaluate barriers and facilitators to early diagnosis. We employed two behavioral models in developing the codebook for this analysis: the situated Information, Motivation and Behavior (sIMB) framework and the Andersen model of total patient delay. We then analyzed the interviews using framework analysis.Results: Patient factors that delayed diagnosis were lack of KS awareness, fear of cancer, fear of amputation, fear of HIV diagnosis, seeking traditional treatments, lack of personal efficacy, and lack of social support. Health system factors that delayed diagnosis included prior negative healthcare interactions, receiving an incorrect diagnosis, lack of physical exam, 190delayed referral, and lack of tissue biopsy availability. Financial constraints, predominantly around cost of transportation and cost of related medical fees were prominent barriers for patients being able to access and receive care. Facilitators for diagnosis included being part of an existing HIV care network, living near a health center, trust in the biomedical healthcare system, the desire to treat painful or disfiguring lesions, and social support. Conclusion: Lack of KS awareness among patients and providers, stigma surrounding HIV and cancer diagnoses, community beliefs in witchcraft and amputation, and health system referral delays were important barriers for patients in reaching KS diagnosis. Improved public health campaigns about the causes and symptoms of KS, increased availability of biopsy and pathology facilities closer to patients' homes, as well as health provider training about KS, are needed to improve early diagnosis of KS. 223 Nandini Meyersohn, M.D., Radiology Association of Hepatic Steatosis with Major Adverse Cardiovascular Events: Insights from the PROspective Multicenter Imaging Study for Evaluation of chest pain (PROMISE) trial N. Meyersohn1, T. Mayrhofer1, D. Bittner1, P. Staziaki1, MGH, MA, 2Medicine, MGH, Boston, 3Medicine, Duke, Durham, NC, USA Introduction: Hepatic steatosis (HS) is a novel biomarker for increased risk of major adverse cardiovascular events (MACE). It remains unclear whether HS is merely a bystander of coronary artery disease (CAD) or an independent marker of risk warranting targeted therapy. In a large prospective study of individuals with suspected stable CAD, we evaluated whether HS predicts MACE independent of CAD as assessed by comprehensive contrast-enhanced CT angiography characterization of coronary plaque and stenosis.Methods: We conducted a nested cohort study of individuals from the Prospective Multicenter Imaging Study for Evaluation of Chest Pain (PROMISE) trial. Subjects were stable, symptomatic outpatients who required noninvasive cardiovascular testing and received coronary CTA with median follow-up of 25 months. Independent blinded readers measured hepatic and splenic CT attenuation values as well as Agatston score on non-contrast CT images and evaluated each coronary artery segment for presence of plaque, type of plaque, significant CAD (50% left main stenosis or 70% in any coronary artery), and high-risk plaque features on coronary CT angiography, which were then used to calculate segment involvement score and CT-adapted Leaman score. HS was defined as liver attenuation <40 Hounsfield units (HU), difference between liver and spleen attenuation <1 HU, or liver to spleen ratio 1.1. The primary endpoint was an adjudicated composite of MACE (death, myocardial infarction, or unstable angina) during follow-up. Cox proportional hazards models assessed the relationship of HS to time to the first clinical event. The added predictive ability of HS was measured using continuous net reclassification improvement (cNRI).Results: Among 3,756 subjects (mean age 60.6 years, 48.4% men), 959 (25.5%) had HS. The presence of HS was associated with a significantly higher rate of MACE (HS present: 42/959, 4.4%; p=0.006). This association remained after adjustment for ASCVD risk score, significant stenosis and metabolic syndrome (hazard ratio 1.72, 95%CI 1.16-2.54, p=0.007) or obesity (hazard ratio 1.75, 95%CI 1.19-2.59, p=0.005). Addition of HS to ASCVD risk CAD and metabolic syndrome 0.02-0.42) or obesity (cNRI: 0.24, 95%CI 0.04-0.43) led to significant improvement in discrimination for MACE. HS also remained an independent predictor of MACE after adjustment for plaque measures including CAD-RADs category, presence of high-risk plaque features, segment involvement score, CT-adapted Leaman score, and Agatston score.Conclusion: Hepatic steatosis predicts MACE independent of traditional CV risk factors and severity and extent of CAD as assessed by coronary CT angiography. Hepatic steatosis may be a novel and important therapeutic target in the prevention of MACE.191Patient baseline characteristics stratified by the presence of hepatic steatosis on non-contrast CT 224 Theodoros Michelakos, MD, The effects of neoadjuvant FOLFIRINOX, photon and Surgery, Massachusetts General Hospital, Boston, MA, USA, 2Department of Pathology, Massachusetts General Hospital, Boston, MA, USA, 3Cancer Center, Massachusetts General Hospital, Boston, MA, USA, 4Department of Hematology/Oncology, Massachusetts General Hospital, Boston, MA, USA and 5Department of Radiation Oncology, Massachusetts General Hospital, Boston, MA, USA Introduction: Neoadjuvant FOLFIRINOX and chemoradiation have been utilized and locally advanced pancreatic ductal adenocarcinoma (PDAC). We compared the expression of immunologically relevant molecules and immune cell infiltration in the tumor microenvironment (TME) elicited by different neoadjuvant of resected patients who were treatment na\u00efve.Methods: Clinicopathologic variables were collected for surgically resected PDACs at General Hospital (1998-2014). 50.4Gy of photon chemoradiation. HLA class I and class II expression, as well as immune cell (CD4 +, FoxP3+, CD8+, Granzyme B+ and M2 (IHC) analysis. The IHC results were correlated with clinicopathologic variables. Results: A total of 248 PDAC patients were evaluated; 63 of them received FOLFIRINOX with or without chemoradiation, 30 protons (5days x 5Gy), 18 photons (50.4Gy) and 137 received no neoadjuvant therapy. Median age was 64y and 51% of patients were female. The frequency of HLA-A defects was significantly lower in FOLFIRINOX alone patients than in the other cohorts (p<0.001). All 248 surgically removed PDACs demonstrated tumor infiltrating immune cells, but the density was higher in patients receiving neoadjuvant therapy. The most robust CD8 + T cell density was identified in FOLFIRINOX alone, followed by patients who received sequential FOLFIRINOX and chemoradiation (p<0.001). T regulatory (Treg) cell density was lowest in the FOLFIRINOX alone and proton patients (p<0.001). M2 macrophage density was lowest in the FOLFIRINOX alone and photon patients (p<0.001). In FOLFIRINOX patients, HLA-A expression defects correlated with higher T stage (p=0.03) and perineural invasion (p=0.002), and high M2 macrophage density with In treatment-na\u00efve M2 macrophage density predictors of poor OS. Conclusion: Neoadjuvant FOLFIRINOX was associated with the most significant changes in the tumor microenvironment. Those included increased CD8 + T cell density, decreased frequency of HLA-A defects, as well as decreased Treg cell and M2 macrophage density. These results are compatible with the possibility that the survival benefit associated with neoadjuvant FOLFIRINOX in PDAC patients is mediated by the induction or enhancement of an antitumor immune response. Thus, patients receiving neoadjuvant FOLFIRINOX may potentially benefit from combination immunotherapeutic strategies which may take advantage of the favorable FOLFIRINOX-induced TME changes.192 225 Rebecca D. Minehart, MD, MSHPEd, Anesthesia, Critical Care and Pain Medicine Optimizing antepartum maternal resuscitation during cardiac arrest after 20 Critical Care and Pain Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Obstetrics and Gynecology, Massachusetts General Hospital, Boston, MA, USA and 3Nursing and Patient Care Services, Massachusetts General Hospital, Boston, MA, USA Introduction: Resuscitation of antepartum maternal cardiac arrest (AMCA) for EGA >20 weeks is distinct from manage - ment of postpartum arrest, due to a need for perimortem delivery (PD) within 4 minutes if return of spontaneous circulation (ROSC) is not achieved (1,2). This gives a narrow window of 4 minutes during which a team must identify reversible causes and initiate empiric treatments to improve chances of survival and maintenance of pregnancy if maternal ROSC is attained. Because of this limited timeframe during which maternal ROSC would alter the course of mother and fetus by obviating the need for PD, we sought a pragmatic approach to empirically treating etiologies which could be reversed within the first 4 minutes after AMCA. Methods: AHA/ACC guidelines for maternal cardiac arrest were reviewed, for a total of 37 etiologies for all arrest conditions. Etiologies were subjected to the following criteria: 1) direct causality vs indirect causality for cardiac arrest; and 2) potentially reversible in the first 4 minutes, given ideal conditions with ready access to all needed resources. Treatment options were identified that would be helpful for reversible causes, and would not be contraindicated in or worsen other conditions, for potential empiric treatments during the first 4 minutes in an AMCA (>20 wks EGA). These options were considered in addition to accepted ACLS guidelines, including: initiation of high-quality chest compressions with manual left uterine displacement, early defibrillation, and administration of 1mg epinephrine IV at the 4 minute mark if no maternal ROSC was achieved (1).Results: 24 etiologies were identified as direct causes of cardiac arrest; 9 of those were deemed to be treatable within 5 minutes in \"best\" and tension pneumothorax. For all but 2 conditions, the following treatments were deemed to have favorable risk/benefit ratios as empiric treatments for AMCA within the first 4 minutes after ACLS initiation (as described above): intubation and ventilation, small dose epi (50-100mcg), intralipid bolus +/- infusion, rapid IV fluid administration, and calcium 1gm IV push. The remaining two etiologies (cardiac tamponade and tension pneumothorax), which necessitated needle decompression, were deemed to be too risky for empiric use.Conclusion: While focus should continue on practicing and preparing teams to treat AMCA with PD to improve maternal survival if ROSC is not achieved within 4 minutes (3), we believe that including a standard empiric approach aimed at optimizing maternal resuscitation prior to PD should be considered. References: 1. Jeejeebhoy FM, et al. Circulation 2015; 132(18): 226 Rebecca D. Care and Pain Medicine Name/Claim/Aim for Obstetric Crises: A New Paradigm 1Anesthesia, Critical Care and Pain Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Center for Medical Simulation, Boston, MA, USA, 3Obstetrics and Gynecology, Massachusetts General Hospital, Boston, MA, USA and 4Obstetrics and Gynecology, Brigham & Women's Hospital, Boston, MA, USA Introduction: Obstetric emergencies occur infrequently [1], but like any crises, require organized application of crisis resource management (CRM) principles to facilitate teamwork and provide optimal care [2]. Despite knowledge of CRM principles as described by David Gaba and colleagues decades ago [3], it is challenging for teams to recall and apply these high-level concepts during a stressful maternal and fetal crisis. At the Center for Medical Simulation (CMS) (Boston, MA), a freestanding and high-volume healthcare simulation center which holds over 120 courses per year, we sought to create an easy-to-remember mnemonic that incorporated all CRM principles and was informed by a robust teamwork theoretical base.Methods: We sought to underscore the importance of psychological safety and fostering speaking up in our diverse groups, which would maximize input and distributed leadership throughout the management of a crisis. Therefore, we promoted the role of an \"Event Manager,\" someone designated to encourage team organization as well as updated communication and input from the team. In addition, we encouraged group members to hold a \"Basic Assumption\" about each other, that everyone was attempting to do his or her best work. To minimize cognitive load under stress and highlight key actions, we focused our creation on an easy-to-remember mnemonic, into which we incorporated all 11 of Gaba's CRM principles into a Pre-Name phase and then to \"Name/Claim/Aim\" to facilitate: 1) \"Naming\" the clinical problem out loud; 2) \"Claiming\" the role of \"event manager\" and asking others to state their roles; and 3) \"Aiming\" the team with a brief list of interventions. The Pre-Name phase incorporates knowing the environment, anticipating and planning, and calling for help early. The \"Name\" phase includes effective communication, anticipating and planning, and mobilizing resources. The \"Claim\" phase is the most extensive, and includes all CRM principles except those included in Pre-Name. We included a cognitive aid of suggested roles for teams to fulfill. The \"Aim\" phase includes effective communication, mobilizing resources, using all available information, and using cognitive aids.Results: To date, we have taught >120 courses from September 2016 to April 2019 where we have introduced N/C/A through RCDP, and explored anesthesiologists' and obstetric team members' frames. Preliminary results indicate there are still a number of barriers for perioperative teams organizing effectively in a crisis. These include high cognitive load, lack of familiarity or practice with team organizational skills, institutional or practice culture, and cognitive biases.Conclusion: Many teams do not organize in crises, despite decades of training. \"Name/Claim/Aim\" may be an efficient and effective way to quickly apply 10 of 11 crisis resource management principles. Optimal training is yet to be determined; further studies are ongoing at sites other than CMS and MGH. References: 1. 23: 359-72. 3. Gaba D, et al. Crisis Management in Anesthesiology, 2nd ed. Saunders, 2014 194227 Elisabeth Mitchell, PharmD, Pharmacy Implementation of a post-discharge pharmacy follow up phone call in an adult medicine population E. Mitchell1, C. Do1, T. Spracklin1, S. Jacob1, MGH, MA, USA and 2Nursing, MGH, Boston, MA, USA Introduction: As medication experts, pharmacists and pharmacy students can play an integral role in preventing post- discharge medication errors, which may reduce hospital readmissions. Studies have shown decreased hospital readmission rates in patients who receive a pharmacist-led, post-discharge telephone call. At Massachusetts General Hospital (MGH), the standard of care for adult medicine post-discharge follow-up care includes a phone call led by an attending nurse (ARN). A portion of the call is spent on addressing any concerns the patient has about their medications. More time dedicated to medication counseling may be necessary to identify the patients' comprehension of their medication indications, administra - tion instructions, and potential side effects. We hypothesize an additional post-discharge phone call led by pharmacy students will identify additional medication discrepancies, reduce readmissions, and improve patient post-discharge satisfaction as compared to the standard of care at MGH. Methods: Pharmacy students worked closely with the ARNs on two adult internal medicine units, Bigelow 11 and Ellison 12, to identify patients who were being discharged home and booked telephone appointments within 72 hours after discharge with patient permission. Patients were given an appointment card which included the date and time of the phone call and an appointment reminder was also documented in the Pharmacy section of their After-Visit Summary (AVS). Patients were instructed to have all their medications and discharge medication list ready at the time of the phone call. During the phone call, pharmacy students reviewed each medication with the patient confirming the name, strength, dose, route, frequency and duration. Patients were asked a series of questions to probe adherence, proper storage of medications, and general understanding of their medications. The phone call ended with the students asking to rate the satisfaction of the phone call. Discrepancies noted during the phone call were escalated to a pharmacist, who collaborated with the medical team to determine appropriate management, and patients were called back to discuss resolution plans. All data points were collected on REDCap TM and documentation of the call was done in the Patient Call ManagerTM. Results: As the pilot is still in process, data is still being collected. Our study team has collected data on ten patients (n= 10). The median age was 71, 60% were male and the average number of discharge medications was 8.6. Five patients had a high-risk condition on admission (Table 1). 70% of patients had all medications next to them while the call was conducted. The ARN called 50% of patients prior to their pharmacy student phone-call appointment. Of these patients, all confirmed they understood how they were taking their medications, why they were taking the medication and any potential side effects. However, when the pharmacy students conducted their phone call, it was noted that 30% of patients either partially knew or did not know any indications for their medications. Medication discrepancies were also found: four patients (40%) had one medication that was not picked up at the pharmacy. Three patients had an error with a new medication at discharge; this included a prescription that was never picked up from the pharmacy, which the patient was advised to obtain and take right away. Of all the medications listed on the patients' discharge paperwork, a total of 16 medications (18.6% of all medications) were not being taken. Furthermore, six medications were being taken at the incorrect dose, three medications at the incorrect frequency, and two medications with incorrect directions. The average time spent making a phone call was 20 minutes (range= 5 to 50 minutes) and 90% of patients reported they were extremely satisfied and very satisfied with the phone call (Table 3). Hospital readmission rates will be evaluated as the pilot progresses and more patients are enrolled.Conclusion: Post-discharge follow-up care is essential, especially surrounding medication use. This pilot has shown that patients continue to have questions with their medications after they leave the hospital. A pharmacy student-led phone call program, dedicated to discussing patients' medications after they have been discharged from the hospital, has led to identi - fication of medication discrepancies and improved medication adherence.195 Table 1- Primary Problem (High Risk Category) Table 2- Patient Satisfaction with the call 228 Julian Mitton, MD, MPH, Medicine - General Internal Medicine \"You're always jumping through hoops\": journey mapping care experiences among individuals with opioid use disorder-associated endocarditis J. Mitton and B. Bearnot Medicine, Division of General Internal Medicine, Massachusetts General Hospital, Cambridge, MA, USA Introduction: Infectious complications of opioid use disorder (OUD), including endocarditis, are rising in the U.S. Individ- uals with OUD-associated endocarditis have poor clinical outcomes and their care is not well understood. Our objective was to perform journey mapping, a qualitative tool that visually displays an individual's movement through a complex system. Traditionally used in consumer analysis, journey mapping is now increasingly used in healthcare settings to capture common trajectories and patterns of care for people with OUD-associated endocarditis. Methods: This was an exploratory analysis of qualitative data collected through interviews of individuals who received care at a single health system for OUD-associated endocarditis. We extracted details of participants' care experience, recording dates of presentation, care settings, diagnoses, care plans, drug use history and transitions in care. These details were displayed and modified in an iterative journey mapping process. We then used a grounded theory approach when reviewing the maps to help highlight patterns in participants' care and identify emerging themes. Results: We reviewed ten patient care experiences using a novel patient journey mapping approach to characterize common trajectories and patterns of care. No participants described a simple or linear episode of care from hospitalization to post-acute care and home. A more typical episode included multiple interactions with the health care system before hospitalization, prolonged stays in the hospital and post-acute care, leaving care settings by choice and frequent rehospitalizations and return to substance use. Similar care patterns of care were identified, including early addiction treatment and intensive outpatient care preceding periods with no rehospitalization (Figure 1, Insert A), while a return to substance use often directly preceded rehospitalization ( Figure 1, Insert B ). Participants frequently left care by choice, often in response to stigmatizing care experiences, and proactively reengaged with care during periods of substance use. Falling out of intensive outpatient care and returning to substance use were observed to often directly precede rehospitalization. Conclusion: Journey mapping is a novel, patient-centered approach to capturing the care experiences and trajectories of a stigmatized patient population who commonly engage with the healthcare system in unexpected and challenging ways. For patients with OUD-associated endocarditis, we identified critical moments before a return to substance use as opportunities to support and engage patients in early addiction and intensive outpatient care to prevent rehospitalization. Participants identi- fied family members, social workers, patient navigators and recovery coaches as frequently providing much needed support in these critical times. Healthcare providers should engage with these sources of support in order to keep patients engaged in care, provide continuity, and help prevent a return to drug use and rehospitalization. Participants also described multiple instances of making intentional, proactive decisions about their care, including leaving care by choice and re-engaging in addiction care during periods of drug use. These decisions were often made in reaction to feelings of stigma and isolation. Stigma continues to be a barrier to care and patients frequently leave care by choice as a result. More can be done to overcome stigma by using non-judgemental, medically accurate language and framing addiction as a chronic but treatable illness. This includes avoiding the description of patients being 'discharged against medical advice,' instead using more descriptive and less pejorative terms such as 'patient directed discharge' or 'leaving care by choice'.196 Figure 1: Journey maps illustrative of early addiction treatment and intensive outpatient care observed to precede periods without re-hospitalization. 229 Melanie F. Molina, MD, Emergency Prevalence of Health-Related Social Needs in the Emergency Department M.F. Molina, C.N. Camargo and M. Samuels-Kalow Emergency Department, Massachusetts General Hospital, Boston, MA, USA Introduction: The emergency department (ED) cares for vulnerable populations whose members often have unmet health-re- lated social needs (HRSN). Recent recommendations for standardized screening focus on five domains: housing instability, food insecurity, transportation needs, utility needs, and interpersonal safety. Our objective in this study was to evaluate the extent of HRSN among patients in a large urban ED.Methods: We designed a screening tool for HRSN using publicly available questions in the five domains outlined by the National Academy of Medicine. We then conducted a cross-sectional study with 48 hours of time-shift sampling (24 hours of weekday and 24 hours of weekend) in each of the five areas of a large urban ED. Bilingual (English-Spanish) research assistants screened patients for eligibility, consenting patients or parents of pediatric patients completed a brief demographic questionnaire and the HRSN assessment. We used standard descriptive statistics to describe the prevalence of HRSN.Results: Of the 614 participants who were screened, 483 (79%) were eligible and 276 (57%) consented to and completed the survey. The primary reasons for ineligibility included intoxication and medical acuity. Eligible patients who declined partici- pation cited disinterest, pain, or were pulled away for a diagnostic procedure. Of the 276 participants, 81 (29%) had completed only elementary/high school education, and 120 (49%) had public or no health insurance. Twenty six-participants (9%) chose to complete the survey in Spanish. Overall, 103 patients (37%) screened positive for an HRSN. By domain, 61 (23%) were positive for housing insecurity; 45 (17%) were positive for food insecurity; 24 (9%) reported transportation needs; 11 (4%) reported utility needs, and 45 (17%) reported safety concerns. Results for the individual questions are shown in the Table. Conclusion: These data demonstrate that HRSN screening is feasible in ED patients, and revealed that more than one in three screened patients (37%) have at least one HRSN. These needs are unlikely to be identified during routine clinical care yet may significantly affect a patient's healthcare needs and utilization. Further work is needed to determine optimal screening and linkage strategies to successfully connect patients to community organizations.197Prevalence of health-related social needs by question and by group 230 Denise M. Molk, VMD, MS, Medicine - Mongan Institute for Health Policy Tail Snip Pain Management in Mice Assessed via Behavioral Video Monitoring D.M. Molk and M. Ahl-Cheverie Massachusetts General Hospital, Charlestown, MA, USA Introduction: Using a small amount of tail tissue for DNA analysis is a common genotyping technique in mice. It has been published that ossification of the distal tail tip of mice occurs between 17 -28 days of age, depending on the strain. Many institutions have created IACUC policies requiring analgesics and/or anesthetics for tail snips based on this factor. While Massachusetts General Hospital (MGH) has an IACUC standard in place for the collection of tail tissue for genotyping, further investigation was done into which anesthetic agent is most effective at alleviating the pain associated with the tail snip procedure. Methods: We screened sixteen different anesthesia and analgesia scenarios using combinations 1mg/ml PO and topical 20% Benzocaine in C57Bl/6 mice from 19 days to 3 months of age. Mouse activity was video recorded pre-procedure and for 60 minutes post-procedure. Videos were viewed by a panel of reviewers blinded to the treatment scenarios, who then scored the videos based on published behavioral observations that are consistent with pain and distress in mice. Results: Findings indicated that 20% benzocaine, without concurrent anesthesia, administered topically to the distal 1/3 of the tail, immediately pre and post biopsy sufficiently alleviated the pain associated with tail snips in mice of various ages, as indicated by the lack of pain-associated behavioral observations post- procedure. DNA quality was not effected. Conclusion: Based on these findings, topical 20% benzocaine (Oragel\u00ae) can be recommended as an analgesic for the purpose of tail snips in mice which is also easy to procure, administer and has little to no influence on other aspects of animal health or DNA quality. 231 Kamyar Mollazadeh Moghaddam, PharmD, Emergency Mechanical Properties of the Every Second Matters for Mothers- Uterine Balloon Tamponade (ESM-UBT) Device: in vitro tests for safety of use Mollazadeh Moghaddam 1,2, M. Dundek1,2, S. Rushforth2 and T. Burke1,2,4 1Division of Global Health Innovation, Department of Emergency Medicine, Harvard Medical School, Boston, MA, USA, 2Vayu Global Health Innovations, Boston, MA, USA, 3Department of Obstetrics and Gynecology, School of Medical Sciences, University of Campinas, Campinas, Brazil and 4Harvard T. H. Chan School of Public Health, Boston, MA, USA Introduction: Uncontrolled postpartum hemorrhage (PPH) is a life-threatening emergency and the most common cause of maternal death and disability worldwide. Women in resource-poor settings are at greatest risk from PPH. Treatments for uncontrolled PPH includes uterine massage, uterotonic medications, uterine balloon tamponade (UBT), etc. A UBT device is a balloon attached to a semi-rigid large bore catheter that acts as both an introducer and as a channel to rapidly inflate the balloon once it is placed into the uterine cavity. Reports on patient outcomes associated with commercial UBT devices are encouraging, however, their high cost (up to$400 USD) is a barrier to implementation across resource-poor settings. An improvised condom-catheter UBT is an alternative to a commercial uterine balloon tamponade device. These devices 198require assembly; however, their low cost of materials makes them attractive options for resource-poor settings. The 'Every Second Matters for Mothers - Uterine Balloon Tamponade' (ESM-UBT) device is a condom-catheter balloon tamponade device designed in 2010 by the Massachusetts General Hospital Division of Global Health Innovation to optimize safety, efficacy, and ease of use. The purpose of this study was to provide a comprehensive evaluation of the mechanical properties of ESM-UBT devices to confirm its mechanical safety for use.Methods: Intraluminal pressures (ILP), diameters, and burst volumes of condom uterine balloons were measured in open air and inside uterus models (simulated situation). The uterus model designed with the sizes of 100mL, 250mL and 500mL for the simulated study. ILP, diameters, and burst volumes of Foley catheter balloons were measured in the next step. Finally, the condom-catheter O-ring attachment tensile strength was evaluated. A sample size of 28 was used for each experiment, including each uterine size.Results: The FDA recommended injection of 500 mL of normal saline expanded a condom uterine balloon's diameter to 88.1 \u00b1 3.8 mm in the open-air test which can entirely fill the post-delivery uterine cavities. Additionally, all samples of ESM-UBT condom uterine balloons maintained their integrity for at least three hours when subjected to pressures of 200 mmHg or greater across each of the tested uterine volumes in simulated situation test. This pressure is at least two times more than the real life ILP of the device inside a human body. Moreover, none of the ESM-UBT device condom uterine balloons burst with volumes less than or equal to 5,000 mL of saline. Foley catheter balloons expanded 34.1 \u00b1 0.7 mm, beyond the average postpartum cervical orifice opening size after instillation of recommended injection of 15 mL of saline. Additionally, the ESM-UBT device Foley catheter balloons burst with volumes more than or equal to 35 mL of saline. O-ring secured attach - ment of the condom uterine balloon to the Foley catheters withstood forces of 15.4 \u00b1 2.1 N, and condom uterine balloons stretched to 35.8 \u00b1 2.1 cm without loss of integrity. Therefore, the O-ring connection is considered secure for what would occur in clinical circumstances Conclusion: The mechanical properties of the ESM-UBT device shows it can keep its integrity through the application and be safely used for the patients. The results make ESM-UBT attractive for scale across resource-poor settings. 232 Derek Monette, MD, Health Professionals Education Research A Two-Year Experience Creating a Sustainable In-Situ Simulation Program for Interprofessional Education in an Urban Academic Emergency Department D. Monette, D. Hegg, A. Chyn, J. Gordon and J.K. Takayesu Emergency Medicine, Massachusetts General Hospital, Cambridge, MA, USA Introduction: In-situ simulation (ISS) provides an alternative learning environment to a dedicated simulation center. Previous studies show ISS may create opportunities to identify latent system threats, understand culture, and improve team dynamics. However, there are limited resources to guide development and implementation of ISS at academic medical centers. Our objective is to describe the implementation of ISS in a high-volume urban emergency department to understand the requirements and limitations of successful program design.199Methods: Key departmental stakeholders met to define the goals of the experience, identify the physical space and optimal timing, and develop protocols to ensure safe equipment use and subsequent patient care. An ED bay was re-configured to store a simulation mannequin for easy deployment. Simulation faculty designed cases that incorporated departmental clinical guidelines, QA measures, and local case reports. Pilot simulations were used to identify operational barriers and gather partic- ipant feedback. We employed an iterative process of review during the academic year to refine the program and identify key themes that seem essential to implementation.Results: 19 of 22 (86%) scheduled sessions occurred during the academic year. 65 individual learners participated in at least one session, a cohort that included 37 nurses, 16 resident physicians, 8 physician assistants, and 4 allied health profes- sionals. An interprofessional team of educators facilitated the pre-brief, simulation, and structured debriefs, which focused on teamwork, closed-loop communication, and clinical management. Identified themes and practical points for implementation include consideration of session timing, participant buy-in, flexibility, and threats to professional identity.Conclusion: This report demonstrates the feasibility of implementing an ISS program for interprofessional education within the physical and resource constraints of a high-volume ED. It requires a team of stakeholders to develop learning goals and safe practice. Embedding ISS as a routine expectation in the clinical learning environment can be an important element of a comprehensive program to enhance the operations and advance the safety climate of a high-risk patient care setting. Image 1. In Situ Simulation Initiative Timeline: Planning to Implementation 233 Sara Moradi Tuchayi, MD, MPH, Dermatology Injectable Slurry Photomedicine, Department of Dermatology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA and 2Center for Systems Biology, Massachusetts General Hospital, Boston, MA, USA Introduction: Obstructive sleep apnea is a common sleep disorder associated with significant morbidity and mortality, and no minimally invasive cure. Accumulation of fat deposits around the upper airway is a major risk factor for narrowing the airway in obstructive sleep apnea patients. We previously showed that fat can be selectively targeted by tissue cooling, which led to the development of a widely used non-invasive bodysculpting procedure called cryolipolysis. However, the current cryolipolysis technique uses topical cooling which is not ideal for targeting deep deposits of adipose tissue within the body. To overcome the limitations of cryolipolysis via topical cooling we developed an injectable ice slurry, as a minimally-invasive novel method of adipose tissue removal. The aim of this study was to investigate safety and efficacy of ice slurry injection for selective destruction of obstructive sleep apnea associated fat deposits in the neck in mouse model. Methods: We used the New Zealand obese (NZO) mice that are a commonly used model for obstructive sleep apnea. These animals have increased volume of anterior neck and parapharyngeal fat. Baseline MRI imaging was performed prior to treatment to obtain fat tissue volume measurements. Animals in the test group were injected with 1.0 ml of cold ice slurry and were subjected to topical cooling. Animals in the control group were injected with the same volume of melted slurry at room temperature. Treatments in each group were repeated at 8 weeks, and follow up MRI imaging was performed at 12 weeks. Tissue samples were harvested from the treated area for histologic analysis. Paired two-tailed Student's t test was used to compare body weight, and fractional neck fat of animals in each group before and after treatment. Multiple Student's t test with adjusted p value was used to compare body weight between test and control groups before and after treatment. Two-tailed Student's t test was used to compare changes in fractional anterior neck fat pad volume from baseline per body weight between treatment group and control group. p value < 0.05 is considered significant. All the bar graphs show mean + SEM.200Results: Histological analysis of the biopsy samples in treated area of the neck did not show any damage to the surrounding tissues. Body weight of mice in control and test groups at 12 weeks increased significantly in comparison to the baseline (control group 52.11g \u00b1 4.45g vs 58.93g \u00b1 6.13g \u00b1 3.52g vs 59.78g \u00b1 6.61g at 12 weeks, p < 0.01). There was no significant difference between test and control groups at each time point. Using MRI imaging, changes in the fraction of anterior neck fat pads volume per neck tissue volume in the region of interest were calculated at 12 weeks post first treatment in comparison with baseline. Results were normalized per animal weights. Changes in fractional anterior neck fat pad volume from baseline per body weight in test mice was significantly different in comparison with the control group (-1.09%/g \u00b1 1.24%/g vs 0.68%/g \u00b1 1.34%/g; p < 0.01) (Figure 1E). A limitation of our study was the low volume of ice slurry that could be injected in mice due to the risk of volume overload in these animals. Conclusion: In this study we examined safety and efficacy of ice slurry injection for selective removal of anterior neck fat deposits in sleep apnea mouse model. We demonstrated that local injection of ice slurry safely, effectively and selectively removes upper airway fat. Injection of the slurry could potentially serve as a non-surgical treatment for sleep apnea. 234 Leila Mostafavi, MD, Radiology Prediction and Measurement of Treatment Response in Metastatic Liver Disease with Machine Learning Radiomics L. Mostafavi1, MA, USA and 2Siemens Healthineers, Berlin, Germany Introduction: To assess if machine learning (ML) based-radiomics can predict and measure treatment response in patients with metastatic liver disease in patients with breast cancer.Methods: Our IRB approved study included 98 adult women (mean age 54\u00b111 years) with metastatic liver disease from breast cancer. All patients underwent contrast abdomen-pelvis CT in portal venous phase at two (BL: pre-treatment) and follow-up (FU: between 3-12 months following treatment). Patients were subcategorized into three subgroups based on RECIST 1.1. criteria (Response Evaluation Criteria in Solid Tumors version 1.1): 32 with stable disease (SD), 32 with partial response (PR) and 34 with progressive disease (PD) on follow up CT. CT images from BL and FU were deidentified and exported to radiomics prototype (eXamine, Siemens Healthineers). The prototype enabled semiautomatic segmentation of the target liver lesions for extraction of first and high order radiomics. Statistical analyses with logistic regression and random forest classifiers was performed with the prototype to assess how well BL radiomics predicts treatment response, and whether radiomics can differentiate SD from PD and PR on the two timepoints.Results: BL radiomics differentiated SD from PR (AUC 0.718) and also SD from PD (AUC 0.797). There was no significant difference between the radiomics on BL and FU CT images of patients with SD (P= 0.998). Busyness (an NGTDM feature) and surface volume ratio (a shape feature) were the most powerful predictors of PD between the BL and FU exams (AUC 0.892). BL and FU (AUC 0.938; p= 0.026 with multivariate logistic regression) and random forest classification (AUC 0.78).Conclusion: Radiomics can predict and measure treatment response in patients with metastatic liver disease. Sarah Morris, Pediatrics A systematic literature review of the 'Helping Babies Breathe' training program from initial implementation to clinical impact S. Morris1, A. Ruman2, L. Wibecan2, E. Fratt3, J. Rodriguez4, B. D. Nelson5, 2, 1 S. Morris, B.D. Nelson, Division of Pediatric Global Health, Massachusetts General Hospital, Boston, Massachusetts, UNITED STATES|A. Ruman, L. Wibecan, B.D. Nelson, Department of Pediatrics, Massachusetts General Hospital, Boston, Massachusetts, UNITED STATES|E. Fratt, Dana Farber Cancer Institute, Boston, Massachusetts, UNITED STATES|J. Rodriguez, University of Wyoming, Laramie, Wyoming, UNITED STATES|B.D. Nelson, Division of Neonatology, Massachusetts General Hospital, Boston, Massachusetts, UNITED STATES| Introduction: Developed and introduced in 2010, Helping Babies Breathe (HBB) has found great success globally as an easily-implementable neonatal resuscitation training program for healthcare providers working in resource-limited settings. As HBB approaches a decade of in-field use in over 80 countries, this literature review aims to identify challenges, knowledge gaps, and successes associated with each stage of HBB programming. Methods: A systematic literature review was conducted on studies related to HBB implementation, acquisition and retention of HBB knowledge and skills, changes in provider behavior and clinical care, and the impact on newborn outcomes. Databases utilized in the search included Medline, POPLINE, Index A search for \"helping babies breathe\" returned 241 articles. After screening, the review included 80 articles that assessed HBB impact, implementation, knowledge acquisition, and skills retention. Key findings included identification of several barriers to implementation and HBB scale-up as well as the success of interventions in maintaining provider knowledge and skills. Several studies reported an increase in newborn resuscitation skills is not synonymous with clinical utilization. While often difficult to assess in these settings, a decrease in neonatal mortality following HBB training was observed in multiple studies. Conclusion: Studies have shown that HBB training programs have had significant impact on provider-level knowledge and skills, and effective retention strategies have been identified. However, the impact of HBB on provider behavior and newborn outcomes remains difficult to assess.201235 Danny Mou, MD, Surgery The Physician's Perspective of Patient-reported Data D. Mou1,7, R.C. Sisodia1,4, D. Horn1,3, M. S. Chaguturu6,3, T. Ferris1,3 and M. Heng1,2 1MGPO, Massachusetts General Hospital, Boston, MA, USA, 2Orthopedic Surgery, Massachusetts General Hospital, Boston, MA, USA, 3Internal Medicine Associates, Massachusetts General Hospital, Boston, MA, USA, 4Obstetrics and Gynecology, Massachusetts General Hospital, Boston, MA, USA, 5Gynecology and Oncology, Massachusetts General Hospital, Boston, MA, USA, 6Population Health Management, Partners HealthCare, Boston, MA, USA and 7General Surgery, Brigham and Women's Hospital, Boston, MA, USA Introduction: In 2014, Partners HealthCare (Partners) implemented a systems-wide Patient-Reported Data (PRD) initia - tive that has thus far collected 4,753,285 questionnaires across 231 clinics, 56 specialties, and 98 geographical locations. Currently, over 80% of Partners PRD are completed on iPads in the clinic waiting rooms. The PRD data are instantly uploaded to the EHR for the physician to review with the patient. With any intervention, the end-user's experience should be carefully considered. It is imperative to refine the PRD system with feedback from physician end-users. We currently lack an in-depth understanding of the end-user physician's perspective of PRD. We addressed this by conducting interviews and surveys with Massachusetts General Hospital (MGH) primary care physicians (PCPs). The PCP PRD include but are not limited to depression, anxiety, and substance abuse screening questions, review of systems, and questions regarding social determinants of health (SDOH) such as food insecurity. We hypothesize that there are significant unmet needs for the Partners PCP PRD system.Methods: Seven semi-structured 15-20 minute interviews were conducted with MGH PCPs to understand their opinions of the Partners PRD platform. This information guided the development of a PCP PRD survey. Six 15-20 minute survey validation PCP interviews were conducted to validate the survey interpretability. The survey was then administered via REDCap to all MGH PCPs (n=244). Ten more 15-20 minute PCP interviews were conducted, recorded, transcribed, and coded for qualitative analysis. Unpaired t-tests were used to compare PCP survey responses with GraphPad Prism software.Results: The PCP PRD survey yielded a 48% (117/244) response rate. 85% of female PCPs reviewed PRD with their patients compared to 67% of male PCPs (p=0.022). 69% of PCPs with over 30 years in practice reviewed PRD compared to 100% PCPs with fewer than 5 years of practice (P=0.037). From the interviews, seven primary barriers to PRD usage were identi - fied, which include repetitive questions, iPad technical problems, irrelevant survey content, poor data format interpretability, extra work for staff, lack of patient interest, and long survey length. When ranked, data format interpretability, survey length, and iPad technical issues were ranked as the first, second, and third most significant barriers, respectively (Figure 1). PCPs had varied responses to the usefulness of individual PRD tools (Figure 2). On a five-point Likert scale ranging from very helpful to very unhelpful, most PCPs found the Patient Health Questionnaire (PHQ) and general anxiety disorder (GAD) questionnaire to be 'very helpful'. Most PCPs were 'neutral' about the utility of falls screening and pain intensity screening. Themes emerged from the PCP interviews, which we categorized into clinical impact, clinic efficiency, and data format. For clinical impact, PCPs like to use PRD as a starting point for conversation. PCPs also enjoy the SDOH questions that can elicit financial concerns. For clinic efficiency, PCPs see the PRD as a productive way of getting patients to answer required clinical questions in the waiting room, which saves valuable clinic time. PCPs also enjoy the ability to pull in PRD directly to clinic notes, which facilitates EHR documentation. For data format, PCPs repeatedly complain about the difficulty discerning between abnormal and normal findings. PCPs fear that they may miss a concerning response, such as an affirmative answer to suicidal ideation.Conclusion: Partners has invested significantly in a best-in-class EHR-integrated PRD system. For it to function well, we must understand the physician end-user experience. We surveyed and interviewed the PRD PCP end-users at MGH to understand their PRD user experience. Though 77% of PCPs review PRD data with their patients, there remains significant unmet need. The top 3 barriers to PRD use include data format interpretability, long survey length, and iPad technical issues. Of the PRD tools, MGH PCPs found the PHQ and GAD questionnaires to be most helpful. PCPs voice a strong need to display PRD in a way such that abnormal findings are highlighted. PCPs appreciate the existing PRD system for using clinic waiting room time for required screening questions, facilitating documentation, and catalyzing relevant conversations in clinic. We have identified actionable next steps for Partners leadership to further refine the PRD platform for PCPs.202 236 Rebecca V. Mountain, Psychiatry Using Shed Teeth to Obtain Early Clues into Risk for Mental Health Problems Unit, Center for Genomic Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Psychiatry, Harvard Medical School, Boston, MA, USA, 3Forsyth Institute, Cambridge, MA, USA, 4Department of Developmental Biology, Harvard School of Dental Medicine, Boston, MA, USA and 5Henry and Allison McCance Center for Brain Health, Massachusetts General Hospital, Boston, MA, USA Introduction: Mental disorders, including depression, and anxiety, are highly prevalent, affecting an estimated 1.1 billion people around the world. The perinatal period, commonly defined as the time period from 22 weeks' gestation through seven days after birth, is a known sensitive period in development when life experiences can shape long-term risk for mental health problems. Perinatal-focused risk factors including shorter gestations, winter births, more \"stressful\" deliveries, obstetric complications, and other maternal stressors are robust predictors of later mental health outcomes in offspring. However, researchers studying the perinatal determinants of mental health often face a number of measurement challenges that hinder efforts to characterize both general and specific perinatal risk factors and understand their links to offspring mental health. One possible new marker of perinatal stress exposure and associated mental health risk may be the neonatal line, which is one of the earliest and most prominent growth lines that forms within an individual's developing primary teeth. The neonatal line is thought to delineate the transition from intrauterine to extrauterine environment and thus acts as a kind of permanent record of a developmental process. For decades, the neonatal line has been widely used in anthropology as a way to distin- guish between pre- and post-birth exposures and characterize the overall stress experienced by the offspring during the birth process. Previous research has found that the neonatal line varies in width and chemical composition based on features of the perinatal environment. Notably, many of these anthropological studies have found wider neonatal lines among offspring born under more \"stressful\" perinatal conditions; these conditions, including a complicated delivery, longer duration of delivery, pre-term birth, or winter birth have also that have also been linked to mental health problems. However, to the best of our knowledge, neonatal line characteristics have not yet been explored in the field of psychiatry as possible indicators of risk for mental health outcomes later in life. The goal of this study was to investigate the extent to which characteristics of the neonatal line associate with psychopathology outcomes in children. We tested the primary hypothesis that children with a wider neonatal line had higher psychopathology scores. Methods: Using data collected from 71 children participating in Avon Longitudinal Study of Parents and Children (ALSPAC) whose parents donated at least one shed canine tooth to ALSPAC's biorepository for analysis, we compared data on neonatal line morphology from shed primary canine teeth to children's psychopathology symptoms as assessed using the Strengths and Difficulties Questionnaire (SDQ). Neonatal line morphology was assessed using light microscopy after the tooth had been embedded and longitudinally sectioned. Neonatal line width was measured three times at three locations, the proximal third closest to the center of the tooth, the middle third, and the cuspal or outer third of the line. Average measures at each location, as well as a total average neonatal line width for the entire tooth, were also calculated providing a total of 13 measures relating 203to neonatal line width. The SDQ was completed by the mothers via mailed questionnaires when the child was 8 and 11 years old. We utilized a total SDQ score, summing across items on the first four subscales (conduct problems; emotional symptoms; hyperactivity; peer problems), which has been shown in previous studies to correlate highly with questionnaire and interview measures of psychopathology. We first performed Spearman's rank correlation analyses to examine the bivariate associations between neonatal line measures, maternal psychosocial stress during pregnancy from 8 to 32 gestational weeks, total SDQ scores, and social-demographic factors (gestational age, sex, and racial minority status). We are now performing multiple regression analyses to further examine the relationship between the neonatal line, maternal psychosocial stress, and total SDQ scores while controlling for confounding variables including seasonality of birth, sex, socioeconomic status, malnutrition, and racial minority status.Results: Preliminary results show a moderate negative association between maternal history of severe depression and the width of the neonatal line (r = -.228, p = .046), indicating that children of mothers with self-reported severe depression were more likely to have wider neonatal line measures. Additionally, some measures of the neonatal line were positively correlated with child SDQ measures, suggesting that children with wider neonatal lines were also more likely to have higher psychopathology symptoms scores both at age 8 (r=.240, p=.031) and 11 years (r=.284, p=.024). Neonatal line width was also significantly negatively correlated with gestational age, indicating that children of younger gestational age, potentially born prematurely were more likely to have wider neonatal line measures than their older counterparts (r=-.289, p=.011). Conclusion: In these preliminary results, children with wider neonatal line showed, on average, greater psychopathology symptoms in childhood. If these results persist after accounting for key covariates, they could suggest neonatal line morphology may hold promise as a new non-invasive biomarker to predict future mental health risk. The neonatal line measure could also lead to the identification of children who are most vulnerable to developing mental health problems in the future and thus who could be targeted for prevention programming, years before the onset of symptoms. 237 Giannis A. Moustafa, MD, Massachusetts Eye and Ear Infirmary Adnexal Lymphoma in and Ear, Boston, MA, USA and 2Coalition of Cancer Cooperative Groups, Philadelphia, PA, USA Introduction: Current understanding of ocular adnexal lymphoma (OAL) epidemiology and outcomes in the pediatric population is based on case reports and data from series pertaining to the adult lymphoma literature. This study aims to investigate the incidence, clinicopathologic features, and survival of OAL in the pediatric population and compare these data with adults, using data from a national population-based cancer registry.Methods: Design: Retrospective cohort study. Participants: The Surveillance, Epidemiology, and End Results database was accessed to identify individuals with OAL less than or equal to 18 years of age, diagnosed between 1973 and 2015. OAL located in the eyelid, conjunctiva, lacrimal apparatus, and orbit were included. An adult cohort was queried for comparison. Methods: Age-adjusted incidence rates (IRs) and descriptive statistics were calculated for comparison of clinicopathologic characteristics. Overall (OS) and cancer-specific (CS) survival were evaluated with Kaplan-Meier curves and compared among subgroups using the log-rank test. Main Outcome Measures: IRs per 1,000,000 population at risk, descriptive statistics of clinicopathologic features, OS and CS Results: The IR of pediatric OAL was 0.13 (95% confidence interval [CI], 0.09-0.17) per 1,000,000. OAL IRs showed a higher trend towards pediatric males and Blacks: males 0.17 (95% CI, 0.12-0.25), CI Hispanics (95% CI, 0.06-0.20), and Asians 0.03 (95% CI, 0.00-0.17). The conjunctiva was the most common site (45.0%), as opposed to adult OAL which originated primarily in the orbit (58.7%). The majority of pediatric OAL were categorized as localized SEER stage (66.7%) at the time of diagnosis. T-cell and lymphoblastic lymphoma comprised 5.0% and 15.0% of pediatric OAL, but only 0.2% and 0% of adult OAL, respectively. Advanced SEER stage, orbital involvement, diffuse-large-B-cell lymphoma, and anaplastic-large-cell lymphoma subtype were associated with increased mortality. In the pediatric cohort, the 5-year OS and CS was 91.0% (95% CI, 79.6%-96.2%) and 92.6% (95% CI, 81.4%-97.2%), respectively. final OS CS 85.7% (95% CI, 71.9%-93.1%) and 89.6% (95% CI, 76.3%-95.7%), respectively. Both OS (p<0.001) and CS (p=0.02) were superior in pediatric individuals compared to adults.Conclusion: Compared with adults, OAL in the pediatric population is characterized by significant clinicopathologic differ-ences and better OS and CS. These results can assist clinicians in predicting long-term outcomes and in educating patients and their families.204 238 Amulya Nagarur, MD, Medicine Internal Medicine Student Education on Direct-Care Hospital Medicine Services: Results of a National Survey A. Nagarur1,2, C.J. Lai3, L. Simmons1,2, M. Kisielewski4, MGH, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3University of California San Francisco School of Medicine, San Francisco, CA, USA, 4Alliance for Academic Internal Medicine, Alexandria, VA, USA, 5University of Texas Health San Antonio, San Antonio, TX, USA, 6University of South Florida Morsani College of Medicine, Tampa, FL, USA and 7Medical College of Wisconsin, Milwaukee, WI, USA Introduction: Traditionally, internal medicine (IM) inpatient student rotations at clinical sites include teams of students, residents, and an attending physician. With the exponential growth since its relatively recent introduction, hospital medicine as a field has had an expanding and enduring presence in academic medicine. In 2010, the Clerkship Directors in Internal Medicine (CDIM) Annual Survey found that 91.0% of responding programs included students learning under the guidance of hospital medicine physicians. Hospital medicine attendings make up a growing portion of IM inpatient teaching faculty and are perceived by trainees to be more effective general medicine ward teachers than other medicine or traditional subspecialist attendings. Few data are available about teaching on direct-care hospitalist services (DCHS) where medical students are paired directly with hospital medicine attendings, without interns or residents. In 2010, 9.0% of clerkship directors reported having medical students work directly with hospitalists. In the context of hospital medicine's expanding role in medical education, and coincident with an increasing number of medical school matriculants, the authors hypothesize this percentage has increased over the past decade. Contemporary information regarding teaching on DCHS is lacking and could allow for a better understanding of the advantages and challenges of this rotation model. The authors queried clerkship directors responding to the 2018 CDIM national survey to investigate current practices related to medical student education on DCHS. Methods: The 2018 Annual Survey was sent to all CDIM members designated as \"clerkship director\". Survey questions were first proposed by CDIM members during the spring of 2018 and then selected for inclusion by the CDIM Survey and Scholarship Committee. In addition to descriptive statistics for the summary results, Pearson's Chi-square and Fisher's Exact test were used for statistical comparisons between groups of categorical variables. A total of 48 short phrase comments were written about perceived barriers. Investigators (AN, KJ) independently analyzed the codes using thematic analysis and generated codes that were reviewed and consolidated into 5 domains. The final coding list was reconciled with a third investigator (CL) and applied to the transcript.Results: The survey response rate was 82.0% (110/134). Fifty (45.5%) respondents reported having DCHS-based teaching models at their institution. Of the 60 (54.5%) respondents not using DCHS, 37 (61.7%) are considering them. Compared with traditional teaching services, 40 (90.9%) of the respondents using DCHS for student education reported DCHS students work with \"about the same\" or \"fewer\" faculty, and 39 (88.6%) respondents reported students cover \"about the same\" or \"more\" patients. Of respondents using DCHS for clerkship education, 33.0% reported that DCHS represent the student's only inpatient IM exposure. Although 70.0% of clerkships with DCHS models have some form of faculty development, most (66.7%) are not specific to the needs of direct-care hospitalists teaching core clerkship students. Systems and stakeholder considerations are identified thematic challenges to implementing medical student rotations on DCHS. Conclusion: This national survey of clerkship directors provides an initial perspective on the current distribution of IM rotation student teaching models in the U.S. It highlights the previously under-recognized widespread use of DCHS in education delivery. As individual programs and educators grapple with and consider this emerging teaching model, dedicated research is needed to track the experiences of students, faculty, and program directors, as well as the effect on student career selection in IM. Generic faculty development tools and inpatient teaching curricula should be tailored to meet the needs of DCHS participants, leveraging the unique aspects of apprenticeship afforded by this model. DCHS is poised to be a predominant form of medical education for our clinical students; further study of this model and structured curricular initiatives are needed to ensure its future success. By 205formalizing and supporting the DCHS model, investing in its teaching faculty, and preparing students to thrive in such an environ - ment, this commonly employed rotation may evolve into a successful and enduring apprenticeship model for education delivery. Clerkship Directors in Internal Medicine (CDIM) Survey Respondents Utilizing Direct-Care Hospitalist Services (DCHS) for Student Education. Comparison of Student-Faculty and Student-Patient Interactions Between Direct-Care Hospitalist Services (DCHS) and Traditional Teaching Services (TTS). Forty-four responded to this subquestion. 239 Zahra Nasiriavanaki, M.D, Psychiatry High resolution imaging of a parieto-occipital network involved in monitoring peri-personal space Z. Nasiriavanaki1,2, A. Holt1,2,3 1Psychiatry, Massachusetts general Hospital, Charlestown, MA, USA, 2Medical department, Harvard University, Boston, MA, USA, 3Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Boston, MA, USA and 4Radiology, Massachusetts General Hospital, Boston, MA, USA Introduction: One type of non-verbal social communication that is altered in several neuropsychiatric conditions, including autism and schizophrenia, is social spacing or \"personal space\", i.e. the preferred physical distance maintained from other individuals. Although many of the psychological features of personal space have been well-studied, the neurobiological mechanisms governing its regulation are less well-understood. We have found that a parietofrontal cortical network, that has a well-established role in monitoring peripersonal space in primates, selectively responds to stimuli that appear to enter personal space in humans (Holt et al, 2014). To better understand the functioning of this neural system, we are conducting high resolution functional imaging of this system, with a focus on single subject level data. Obtaining reliable data in individual subjects is a necessary prerequisite for understanding individual differences and for clinical translation. Methods: Seven subjects (three males, mean age 24.5) participated in this study. All subjects had normal or corrected to normal visual acuity, without history of neuropsychological disorders. Each subject was scanned in four separate sessions, on different days using a Siemens 7T MRI scanner, with two different tasks. In one task (the Looming task), subjects viewed computer-generated human face stimuli that appeared to approach or withdraw from the subjects (16 s for each condition). Following the scan, preferred \"distances\" to each of the 16 faces viewed were measured while the subject remained in the scanner. To minimize possible attentional biases across different conditions, the participants performed a \"dummy attention task\" during these scans. The second task (the Stereopsis task) measured fMRI activity evoked by random dot stereograms (RDS) (Bela Julesz, 1971; Anzai et al., 2011; Nasr et al., 2016). In this task, subjects viewed a stereo pair of red-green images of RDS with the aid of red-cyan filters. Two RDS were fused through all experiment blocks and formed a stereoscopic percept of a cuboid with varying depth between 0\u00b0 and 22\u00b0, either in front or behind a fixation target at the center of the screen. Each stimulus block lasted for 24 seconds and each run contained 9 stimulus blocks. As a control attention task, the fixation target shape changed from circle to square or vice versa and the subjects were asked to press a button whenever they saw a change in the shape of the fixation target. All the fMRI data were analyzed using FreeSurfer (http://surfer.nmr.mgh.harvard.edu). Results: During the Looming task, discrete patches of activation were observed in the superior and inferior parietal cortex in response to the approaching vs withdrawing faces. The locations of the patches of activation were consistent across the two scans collected for each subject and across subjects. During the Stereopsis task, patches of activation to near vs. far contrast were found in the occipital and inferior parietal lobes. In each subject, the patterns of activation elicited by the two tasks were largely interdigitated, rather than overlapping.Conclusion: These preliminary data suggest that lower level visual information about spatial location, signaled by the disparity of visual images and the relative position of the eyes, is processed in locations in the occipital and parietal cortices that are adjacent to those that respond to higher-order information about the location of stimuli in space in relationship to the body. Additional data using other tasks are currently being collected in these subjects to further understand the circuitry that generate the neural and behavioral responses to information in peri-personal space.206240 Emma Needles, Psychiatry - Benson Henry Institute for Mind Body Medicine A SMART Approach to Reducing Atrial Fibrillation Symptoms E. Needles1, Z. Donahue1, J. Denninger1, J. Ruskin2 and MGH, MGH, Boston, MA, USA Introduction: Atrial fibrillation (AF) is the most common cardiac arrhythmia with a lifetime risk of 1 in 6 individuals, and its incidence increases with age. In patients with paroxysmal atrial fibrillation (PAF), acute stress and negative emotions increase the likelihood of an AF episode 2-5 fold, whereas happiness decreases the likelihood of AF by 85%. A study of yoga and relaxation techniques for patients with PAF suggests that training in mind-body skills can reduce AF episodes, anxiety, and depression in this patient population. We previously conducted an uncontrolled pilot study of the Benson-Henry Institute's (BHI) Stress Management and Resiliency Training Program (SMART-3RP) in patients with PAF (n=8) and found a significant decrease in anxiety and a non-significant trend toward improvement in AF symptoms, perceived stress, depression, and mindfulness in participants. The current study aims to expand on those preliminary findings using a randomized, wait-list controlled trial design to test the effects of the SMART-3RP on quality of life (QOL), mental health, arrhythmia burden, and heart rate variability (HRV) in patients with PAF. Methods: Eighteen subjects were randomized 1:1 to receive the SMART-3RP immediately or in a delayed fashion (wait-list control group) three months after the immediate group started the program. Three SMART-3RP groups were delivered virtually using a secure video conferencing platform. The SMART-3RP is an 8-week multimodal resiliency program that targets stress with four main components: mind-body skills, traditional stress management techniques, healthy lifestyle behaviors, and cognitive reappraisal and adaptive coping skills. Subjects completed measures at baseline, 3 months, and 6 months (wait list control group only). Measures included 24-hour ambulatory HRV monitoring and self-report questionnaires assessing distress (0-10 distress scale), AF symptom severity and burden (Atrial Fibrillation Symptom and Burden questionnaire), AF-related quality of life (AFEQT), happiness (1-5 happiness scale), stress (PSS-10), positive affect (GAD-7), depression (PHQ-8), worry (CamsR), stress reactivity/coping (MOCS), resiliency (Current Experience Scale from the PTGI), gratitude (GQ-6), empathy (Interpersonal Reactivity Index), and practice adherence. We performed paired and independent samples t-tests in SPSS.Results: Following the SMART-3RP intervention, subjects (n=18, pre vs. post program analysis for both study arms combined) reported significant reductions in the impact of AF on health-related quality of life (p = 0.005, Cohen's d = 0.75), AF symptom severity (p = 0.026, Cohen's d = 0.59), distress (p = 0.014, Cohen's d = 0.64), and depression (p = 0.05, Cohen's d = 0.50). Subjects also reported increases in positive affect (p = 0.003, Cohen's d = 0.81) and coping with stress (p = 0.001, Cohen's d= 0.97). In comparing the immediate (n=9) vs. waitlist control subjects (n=9), those in the immediate group reported significantly higher positive affect (p = 0.021, Cohen's d = 1.20) and enhanced coping with stress (p = 0.011, Cohen's d = 1.36) with a trend toward improved AF health-related quality of life (p = 0.086, Cohen's d = 0.86), happiness (p = 0.089, Cohen's d = 0.85), and depression (p = 0.073, Cohen's d = 0.90). Results of the HRV data analysis will be forthcoming. Conclusion: Preliminary results indicate that participation in the SMART-3RP intervention, delivered virtually, may enhance positive emotions and coping with stress as well as decrease negative emotions and AF symptoms. These results warrant a larger scale study to better understand the potential benefits of using mind-body techniques to help manage PAF. Furthermore, the ability to deliver this intervention virtually will help facilitate teaching these tools to a large population of patients as part of clinical care. 241 Anne M. Neilan, MD, MPH, Pediatrics Higher acuity resource utilization with older age and poorer HIV control in adolescents and young adults in the HIV Research Network A.M. Neilan1,2,3, F. Lu4, K.A. Gebo5, Agwu5 1Division of General Academic Pediatrics, Department of Pediatrics, Massachusetts General Hospital, Boston, MA, USA, 2Division of Infectious Disease, Department of Medicine, Massachusetts General Hospital, Boston, MA, USA, 3Medical Practice Evaluation Center, Massachusetts General Hospital, Boston, MA, USA, 4Biostatistics Center, Massachusetts General Hospital, Boston, MA, USA, 5Johns Hopkins University School of Medicine, Baltimore, MD, USA, 6Harvard Medical School, Boston, MA, USA, 7Department of Epidemiology, Harvard T.H. Chan School of Public Health, Boston, MA, USA and 8Center for Biostatistics in AIDS Research, Boston, MA, USA Introduction: Adolescents and young adults (AYA) with HIV experience poorer health outcomes compared to adults with HIV. To improve care for AYA with HIV, information about patterns of costly healthcare resource utilization is needed. We analyzed resource utilization - including outpatient, emergency, and inpatient hospital care - among 13-30-year-olds in the HIV Research Network. We also examined resource utilization associated with specific AIDS-defining conditions (ADCs).207Methods: Among 18 US sites within the HIV Research Network, we estimated average utilization and 95% confidence intervals of outpatient visits (primary care, social work and nurse visits), ED visits and inpatient hospital days per person-year stratified by mode of acquisition (perinatally acquired HIV [PHIVY] versus non-perinatally acquired HIV count (<200, 500 cells/L), and the presence or absence of VL suppression (<, 400copies/mL[c/mL]) combined with ARV use. Person-time spent in CD4 count and VL/ARV strata was estimated by calculating person-time in between each change in CD4 count, VL/ARV status, and age strata. We also quantified resource utilization associated with AIDS-defining conditions. Results: Among 500/ L (54%), For PHIVY and NPHIVY, outpatient visit rates were higher at younger ages (13-17y and 18-23y), lower CD4 (<200, 200-499/ L), and among those prescribed ARVs. Rates of ED visits and inpatient days were higher during PT spent at older ages (18-23y, 24-30y), lower CD4 (<200, 200-499/L), and VL 400c/mL. All of utilization were higher among PHIVY than NPHIVY (outpatient: 12.1 vs. 6.0/PY; vs. 0.8/PY) (Figure 1a-c). The rate of AIDS-defining conditions was 4.5/100PY. Resource utilization associated with specific AIDS-defining conditions is shown in Figure 2. Conclusion: More ED visits and inpatient days were observed at older ages, lower CD4 counts, and VL 400c/mL. Interven- tions to improve retention in care, virologic suppression, and immune response may improve outcomes, and thus decrease costly resource utilization, for AYA with HIV as they transition to adulthood. Figure 1. (A) Outpatient visits, (B) Emergency department (ED) visits, and (C) Inpatient days per person-year Figure 2. Primary care outpatient visits, emergency department visits and inpatient days per AIDS-defining condition208242 Brett D. Nelson, MD, MPH, DTM&H, Pediatrics Clinicians and trainees performing outside scope of training (POST) during international global health activities A. Doobay-Persaud3,7, Global Health and Neonatology, Department of Pediatrics, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Department of Medicine, Feinberg School of Medicine, Northwestern University, Chicago, IL, USA, 4University of California San Francisco, San Francisco, CA, USA, 5Center for Bioethics and Humanities and Division of General Internal Medicine, University of Colorado, Denver, CO, USA, 6Department of Preventive Medicine, Feinberg School of Medicine, Northwestern University, Chicago, IL, USA, 7Institute for Global Health, Feinberg School of Medicine, Northwestern University, Chicago, IL, USA, 8Department of Global and Community Health, George Mason University, Fairfax, VA, USA and 9Department of Neurology and Department of Medical Education, Feinberg School of Medicine, Northwestern University, Chicago, IL, USA Introduction: Every year, thousands of health professionals and medical trainees engage in short-term experiences in global health. These experiences typically involve health professionals from high-income countries traveling to low- and middle-income countries to support clinical care, training, and research. While abroad, clinicians and trainees often face ethical challenges, including being asked to perform outside their scope of training (POST). Emergencies, perceived lack of available resources, or time constraints may make a situation seem so dire that to POST appears to be the only option. Even though existing ethical practice guidelines generally recommend against POST, anecdotal evidence indicates that this practice continues. The frequency, circumstances, and consequences of POST have not been well-explored. Methods: To address this knowledge gap, we conducted an international web-based survey of health professionals and trainees from high-income countries who had worked or volunteered in low- or middle-income settings in the past five years. Study participants were recruited through snowball sampling within international academic global health professional networks. Given the absence of an existing survey instrument about POST, we created a de novo survey of 39 items and piloted it at three institutions. Items queried respondent demographics, their experiences with POST, and reactions to POST. Quantitative data underwent univariate and bivariate analyses. Open-ended responses were coded using an emergent thematic style of qualitative content analysis.Results: A total of 223 survey responses met inclusion criteria. Half (49%) of respondents reported having been asked to POST in the previous 5 years; of these, 61% reported that they did POST. Medical students (100%) and residents and fellows (81%) reported rates of POST nearly twice that of licensed physicians (51%) (p=0.001). Of respondents who did engage in POST, 33% indicated it was \"very\" or \"completely\" likely they would make the same decision in similar situations; only 15% felt \"very\" prepared to manage requests to POST. Common reasons for deciding to POST were a mismatch with host expectations (37%), suboptimal supervision at host sites (21%), inadequate preparation to decline (19%), and a perceived lack of alternative options (13%). Qualitative responses described a wide range of emotional responses related to POST, including anxiety, anger, frustration, and even excitement. While often viewed as acceptable during emergencies, many respondents who had engaged in POST expressed moral distress that persisted over time.Conclusion: The high rate of being asked to POST, and the high rate of doing so, were notable. Based on these findings, the authors endorse approaches to reduce the incidence of POST during global health electives, including pre-departure preparation to respond to requests, multi-directional communication between various stakeholders regarding expectations, and a commitment to local capacity-building and sustainable interventions. 209243 Emily Nguyen, B.S., Dermatology Assessing the incidence of skin and soft tissue infection in patients on biologics E. Nguyen and D. Kroshinsky Dermatology, Massachusetts General Hospital, Boston, MA, USA Introduction: Skin and soft tissue infections (SSTIs) often occur at a site of disruption in the epidermal layer, creating an entry point for infection. Risk factors that predispose patients to SSTIs include physical breakages in the skin from trauma, burns, intravenous drug use, or the existence of comorbidities such as diabetes mellitus, malignancy, or inflammatory disorders. For instance, SSTIs occur three times more frequently in patients with Rheumatic Arthritis compared to the general population, which can be attributed to complications of the disease itself or the use of immunosuppressive agents which pose a risk of increased infection due to compromised defense. Biologic agents have been established in the treatment of moderate-to-severe plaque psoriasis, psoriatic arthritis, and Crohn's disease. The immunomodulating effects can predispose surgical patients to SSTIs. In light of this, several guidelines recommend discontinuing this agent for at least 4 half-lives prior to surgery; however, given the lack of understanding of the real risk of infection associated with the use of these agents, it is difficult to ascertain if these guidelines are necessary. The aim of this project is to assess the incidence of skin and soft tissue infections in patients on biologic agents for all clinical indications. A secondary aim is to assess those undergoing surgical procedures to determine if there is increased risk of post-operative cutaneous infections.Methods: A retrospective cross sectional study at Massachusetts General Hospital and Brigham and Women's Hospital from June 2013-2018. 827 patients at least 18 years of age with at least 2 consecutive injections of adalimumab, etanercept, ustekinumab, or infliximab at maintenance intervals were identified. We performed a subgroup analysis of 223 patients who underwent surgery.Results: Average age in our entire cohort was 43.9 years, 57% were female, and majority (87.8%) were Caucasian. The majority of patients in our group was treated for psoriasis, followed by Crohn's disease. In our entire cohort, a total of 110 instances of skin and soft tissue infections (SSTIs) occurred across all agents. Although adalimumab was the most frequently used biologic in our group, etanercept seems to be associated with the most SSTIs, with an incidence of 17%. Patients who developed SSTIs during their time on biologics were associated with concomitant corticosteroid use (p=0.0004) and former smoking status (p=0.01). 10.5% of patients who were on biologics alone developed SSTIs, and this rate nearly doubles in patients who received both biologics and corticosteroids to 19.7%. In our analysis of the 223 surgeries among 180 patients, the mean number of surgeries was 1.34 (range 1-5). Of 223 surgeries, 24 patients (10.8%) developed post=operative skin infections. 19.5% of patients who had undergone abdominal surgery and neurosurgery developed post-operative skin infection. Orthopedic procedures had a much lower 3.6% post-operative infection risk (p=0.05). 76 patients stopped the agent and 147 patients continued through surgery. No statistical significance in developing post-surgical skin infections was found between these groups. Of note, in our group, 8 patients who stopped their biologic peri-operatively did not develop post-operative skin infection (as is the hope) but did have a significant reported disease flare of psoriasis, psoriatic arthritis, and rheumatoid arthritis.Conclusion: Of all baseline risk factors assessed, use of corticosteroids and former smoking status were found to be predic - tors of SSTIs in our cohort (p=0.0004). SSTI risk increased about two-fold (20.7%) in patients who were concomitantly on a biologic and corticosteroids - suggesting that corticosteroids conferred the true risk in SSTIs, not the biologic agents. In our subgroup analysis of the surgical patients, stopping biologic agents pre-operatively did not lead to a statistically significant decrease post-surgical skin infections. (p=0.11) While our sample size is low, our results also suggest that the time frame of stopping biologic agents within the appropriate number of half-lives does not make a significant difference. (p=0.5) In terms of stratifying infection risk based on the type of surgical procedure, about 22% of patients on a biologic undergoing an abdominal procedure developed a post-operative skin infection. This varies from the approximately 4 percent of patients undergoing orthopedic procedures who would then have a post-operative SSTI. This difference is probably due to the aforementioned infection risk associated with clean-contaminated higher-risk procedures versus clean orthopedic surgeries. These findings suggest that continued use may be safe in the perioperative period. Corticosteroid use may be the significant factor in predisposing patients to SSTIs while on biologics, not the biologic agent itself. This study is limited by its retrospective design and limited power. A larger prospective study is necessary to validate these findings. In the end, a case-by-case approach could be taken when evaluating these patients prior to surgery, keeping in mind that discontinuation of biologic therapies may not be prudent or necessary.210244 Long H. Nguyen, M.D., M.S., Medicine - Gastroenterology The sulfur microbial diet, sulfur-metabolizing bacterial communities, and risk of colorectal cancer Nguyen1,2, Song1,2, and A.T. Chan1,2,6 1Gastrointestinal Unit, Massachusetts General Hospital, Boston, MA, USA, 2Clinical and Translational Epidemiology Unit, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA, 3Division of Public Health Services, Washington University School of Medicine, St. Louis, MO, USA, 4Siteman Cancer Center, Washington University School of Medicine, St. Louis, MO, USA, 5Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA, USA, 6Broad Institute of MIT and Havard, Cambridge, MA, USA, 7University of Nebraska, Lincoln, Lincoln, NE, USA, 8Nutrition, Harvard T.H. Chan School of Public Health, Boston, MA, USA, 9Epidemiology, Harvard T.H. Chan School of Public Health, Boston, MA, USA and 10Dana-Farber Cancer Institute, Boston, MA, USA Introduction: Hydrogen sulfide (H2S) produced by gut bacteria may contribute to inflammation and carcinogenesis in humans. To help establish a link between dietary determinants of microbial community membership and sulfur metabolism pathways that may mediate colorectal cancer (CRC), we assessed the relationship between dietary patterns and the abundance of sulfur-related bacterial species. Methods: In a development cohort nested within the Health Professionals Follow Up Study (HPFS) for whom long-term dietary intake and longitudinal stool metagenomes were available (n =307), we used reduced rank regression to generate a sulfur microbial diet, defined by food groups associated with the abundance of 43 species of interest. This pattern was characterized by food groups known to be associated with CRC risk, including higher intake of processed meats and liquor and lower intake of legumes and vegetables. We then evaluated adherence to this pattern in the remaining individuals in the HPFS. Dietary intake was assessed every 4 years from 1986-2012 by validated food frequency questionnaire.Results: Among 48,239 individuals, we documented 1,264 cases of incident CRC. Increased sulfur microbial diet scores (highest vs. lowest quartile) were significantly associated with risk of distal colon and rectal cancers after adjusting for potential confounders [hazard ratio 1.43, 95% CI: 1.14-1.81; p-trend = 0.002]. No increased risk in proximal colon cancer was observed. Despite several shared food groups among them, sulfur microbial diet scores were not associated with so-called Western dietary scores\u2014a pattern of intake previously associated with CRC and indicative of a diet high in saturated fats, sugars, and processed foods (Spearman = -0.009 at study baseline). This would suggest the sulfur microbial diet is capturing a different signal in the well-established diet-CRC relationship.Conclusion: Our findings support a potential mediating role for sulfur-metabolizing bacteria in CRC etiopathogenesis, driven by dietary determinants. We offer first-of-its-kind evidence linking targeted microbiome discovery, dietary intake, and CRC risk. Further studies are needed to determine mechanistically how diet and sulfur-metabolizing bacterial communities may influence gut inflammation and tumorigenesis. Figure 1: Experimental Design. (A) Participants in the MLVS provided up to four stool samples over a six-month study period with concurrent measurement of long-term dietary intake via FFQ, identical to the FFQ given to participants in the HPFS. Stool underwent metagenomic and targeted metatranscriptomic sequencing. (B) In MLVS participants with longitudinal stool metagenomes and long-term diet, we used supervised clustering and regression techniques to determine the foods most commonly associated with increased abundance of sulfur-metabolizing bacteria to generate the sulfur microbial diet score. (C) Leveraging access to the much larger HPFS cohort with diet assessed every 4 years since inception, as well as other factors that may confound the relationship between diet and CRC, we calculated sulfur microbial diet scores in all 51,529 eligible participants of the HPFS, with higher scores reflecting closer adherence to a diet predicted to enrich for sulfur- metabolizing bacteria. Figure 2: Sulfur microbial diet and risk of colorectal cancer. Multivariable modeling demonstrating an association between increased adherence to the sulfur microbial diet and risk of distal colon and rectal cancer. Abbreviations: RR, relative risk; CI, confidence interval. Models adjusted for age, family history of CRC, BMI, physical activity, alcohol consumption, smoking, aspirin use, total caloric intake, prior endoscopy, and recent physical exam. Tests for trend were conducted using the median value of each quartile category as a continuous variable.211245 TuTran Nguyen, PharmD, Pharmacy Development and Implementation of a Pharmacy Student-led Technician Continuing Education Program T. Nguyen, S. Jacob, A. Tatara and S. Okrzesik Pharmacy, MGH, Quincy, MA, USA Introduction: Led by the Pharmacy Grand Rounds (PGR) Committee, the Massachusetts General Hospital's (MGH) Depart- ment of Pharmacy offers a robust Continuing Education (CE) Program to its employees. The PGR Committee provides, on average, 12 hours of CE credit to pharmacists and 1 hour to pharmacy technicians, annually. Historically, MGH's CE program has provided significantly more CE lectures for pharmacists. These were primarily led by pharmacists and pharmacy residents. Certified pharmacy technicians are required to obtain 20 CE credits biannually. To support the pharmacy techni - cians in their professional development, the need for more pharmacy technician CEs was recognized. Utilization of Longitu - dinal Advanced Pharmacy Practice (LAPP) students in their final year of a PharmD program was suggested to satisfy both the learning needs of the pharmacy technicians and teaching requirements for the students in the LAPP program.Methods: In 2019, the PGR Committee surveyed pharmacy technicians to solicit CE topics of interest. The topic list was compiled and CE topics were selected based on pharmacy technician recommendations that coincide with the Pharmacy Technician Certification Exam Blueprint. LAPP students received lectures on the purpose and process of creating a CE. The technician CE topics were assigned to each LAPP student. Selected presentation mentors, content mentors, and the PGR Committee members served to provide guidance to each student. Clear deadlines were set far in advance. The deadlines and mentorship ensured that each student would be able to deliver a timely, high-quality CE for the pharmacy technicians. Pharmacy technicians were required to be in attendance and complete a post-CE questionnaire to qualify for CE credit. Results: With the help and guidance of assigned members of the PGR Committee and expert pharmacist mentors from several areas within the department, LAPP students were successfully able to create and deliver 8, thirty-minute CEs over the course of 2 months. CE topics covered areas of pharmacology, pharmacy law and regulations, sterile and non-sterile compounding, medication safety, and pharmacy quality. Conclusion: The pharmacy student-led technician CE program at MGH allows for improved job satisfaction for the pharmacy technicians, robust teaching opportunities for the LAPP students, as well as further service for the pharmacy department. 246 Shinsuke Nomura, M.D., Radiology Infrared Fluorophores for Targeting Gastrointestinal Stromal Tumors S. Nomura Radiology, Gordon center for medical imaging/MGH/HMS, Charlestown, MA, USA Introduction: The surgical principle of non-metastatic gastrointestinal stromal tumors (GIST) treatment is complete resection and avoidance of perforation. GIST would be resected more reliably with real-time image navigation. In this study, ZW800-1C, zwitterionic near-infrared (NIR) fluorophore, was conjugated with a CD117 factor for GIST targeting for intraoperative imaging.Methods: Bioconjugation of SCF was performed form of ZW800-1C in phosphate-buffered saline, pH 7.8 for 3 h. The labeling ratio, calculated by the extinction coefficient of SCF at 280 nm and ZW800-1C at 760 nm, was found to be about 0.5. ZW800-1C conjugated SCF (SCF800) was then used to GIST-T1 (receptor- positive) and GIST-5R (receptor-negative) cell lines.Results: We confirmed the specific targeting of SCF800 on the surface of GIST-T1 cells using the NIR fluorescence micros-copy, while negligible fluorescence signals were observed in receptor-compromised GIST-5R cells. Intraoperative imaging and image-guided tumor resection will be followed in GIST-bearing xenograft mice and genetically engineered animal models. Conclusion: This result suggests that SCF800 can be used for highly sensitive, rapid, and non-radioactive imaging of GIST for early diagnosis and intraoperative tumor imaging.212247 Sai Santosh Nooney, Neurology Severity and Duration Neurologic Deterioration Influence Neurologic Status at Discharge: An Informatics Approach in nearly 5,000 MGH Neurocritical Care Admissions S. S. Sivakumar, B.M. E.S. Rosenthal Neurology, MGH, Boston, MA, USA Introduction: Patients requiring neurocritical care frequently have neurologic fluctuations of uncertain significance. We hypothesized that severe and prolonged events of neurologic deterioration (ND) have the greatest impact on discharge neurologic status and serve as intermediate indicators of poor outcome.Methods: We extracted nurse-documented GCS scores from electronic health record (EHR) data of consecutive patients admitted to a Neurosciences Intensive Care Unit (ICU) or undergoing intracranial pressure monitoring (April 2016 - March 2019). For each patient, the GCS trend was evaluated for the following: 1) best initial 24-hour GCS (bestGCS-24h), 2) GCS prior to an event of neurologic decline (pre-declineGCS), 3) maximum magnitude of GCS decline (maxGCSdecline), 4) duration of the episode of maximum GCS decline (dur-max), and 5) the maximum duration of any GCS decline >=3 points (max3pt-dur). We fit a 10-fold cross-validated logistic regression model predicting the final GCS 3-8 (comatose at discharge) vs. 9-15 (emerged from coma at discharge) and tested it in a 30% hold-out sample. We then evaluated the rates of poor outcome for combinations of these parameters. Results: 4949 consecutive admissions met inclusion criteria. Pre-decline GCS, maxGCSdecline, dur-max and max3pt-dur, respectively were independently associated with poor discharge GCS (OR per standard deviation were 0.49[95%CI 0.48-0.51], 1.39[1.35-1.46], 1.18[1.13-1.20], 1.18[1.13-1.21]; F1(testing)=0.79). Patients with higher GCS with smaller maxGCSdecline had lower risk for poor outcome if the neurological decline persisted for a short duration(less than 24 hours) but their risk increased if the decline lasted longer than 24 hours [Figure 1]. Patients with 2-pt drop in GCS for less than 24 hours recovered faster and to a better outcome compared to patients with 4-pt drop in GCS for less than 24 hours [Figure 2]. Patients in moderate group (GCS preceeding neurologic decline, 9-12) with 2 point maximum drop had higher GCS at discharge compared to patients with 4 point maximum drop within the same group. Mean discharge GCS for patients with 2 point drop for less than 12 hours in moderate group of pre-decline GCS was 13.32 compared to 11.46 for patients with 4 point drop for less than 12 hours in the same group. Conclusion: Discharge outcomes are better predicted by an updated GCS baseline and the severity and duration of neurologic deterioration events, than by initial GCS score alone. These empiric, informatics-derived thresholds may serve to define meaningful events of neurologic deterioration that can be predicted in real time in order to develop a target for intervention and expected arc of recovery for comparing clincial interventions.213 Figure 1. Discharge GCS (color scale) depends on the starting GCS score (y axis), the maximal magnitude of neurologic deterioration (x axis), and the duration of maximal neurologic deterioration (panels). Once the duration of neurologic decline exceeds 24-36 hours (right panel), less severe events of neurologic deterioration nevertheless convert to a worse discharge GCS. Figure 2. The arc of neurologic recovery has a lower ceiling and is more delayed when events of maximal neurologic decline are greater than 24 hours (solid lines) vs. less than 24 hours (dashed lines), when events are at least 4 points in deterioration on the GCS scale (red) vs. 2 points (yellow), and when the updated baseline GCS is in the severe range (lower panel) vs. the moderate (middle panel) or upper range (upper panel). 248 Ayush Noori, Neurology Meta-Analysis of Human Brain Transcriptomics Pan-Neurodegenerative Expression Signatures across ALS-FTD, and Hospital, Charlestown, MA, USA, 2Massachusetts Alzheimer's Disease Research Center, Boston, MA, USA and 3MGH BioMedical Informatics Core (BMIC), Cambridge, MA, USA Introduction: Neurodegenerative disorders such as Alzheimer's disease (AD), Lewy body diseases (LBD), and amyotrophic lateral sclerosis and frontotemporal dementia disease spectrum (ALS-FTD) are defined by the accumulation of specific misfolded protein aggregates, however the processes by which proteinopathy leads to neurodegeneration remain elusive. To 214date, most efforts in drug development have focused on preventing or removing these protein inclusions but with no success at improving clinical outcomes. Identifying the mechanisms linking proteinopathy with neurodegeneration could facilitate development of neuroprotective therapeutic agents to slow down or even arrest the disease course. Recent advances in -omics technologies and potent bioinformatics tools have made possible the discovery of drivers of pathophysiology across big data from large patient samples in an unbiased fashion. We conducted a meta-analysis of publicly available human brain transcrip - tomics datasets to test the hypothesis that there is a pan-neurodegenerative signature as well as disease-specific pathways underlying these diverse proteinopathies, and that targeting these mechanisms holds therapeutic promise. Methods: 34 Affymetrix microarray datasets from brains of AD, LBD, and ALS-FTD patients (N=1242) and matched controls (N=1059) were retrieved from the Gene Expression Omnibus (GEO) database, categorized by both disease and brain region, and analyzed individually. Briefly, our pipeline consisted of (1) differential expression analysis, (2) pathway enrichment analysis, (3) meta-analysis, and (4) validation via network analysis. More specifically, a linear model was fitted to log-normalized data to compute the average expression value for each probe, and an empirical Bayesian moderated t-statistics test ranked genes by B-statistic, or the posterior odds of differential expression. Statistically significant DE genes (p < 0.05) were subjected to pathway enrichment analysis against the Kyoto Encyclopedia of Genes and Genomes (KEGG) and Gene Ontology: Biological Processes (GO) databases. For each pathway, meta-analytic p-values were calculated using Lipt\u00e1k's weighted Z-test. Significant pathways across all three diseases were clustered via Jiang-Conrath similarity to define the pan-neurodegenerative expression signature. Finally, to confirm the coordinate regulation of the pathways within this signature, we applied weighted gene co-expression network analysis (WGCNA).Results: Our meta-analytic approach yielded a pan-neurodegenerative expression signature comprised of several upregulated (Figure 1) and downregulated (Figure metabolic process\". The components of this signature included relevant biological processes such as RNA metabolism (specifically, \"regulation of mRNA metabolic process\"), innate immune system (exemplified neurodevelopmental pathways (such \"cell phase transition\"). Conclusion: The use of robust bioinformatics tools on a large sample of AD, LBD, and ALS-FTD patients as well as age-matched controls enabled us to find a pan-neurodegenerative expression signature common to all three neurodegener - ative diseases. Of note, while the innate immune system is known to be involved in all neurodegenerative diseases, RNA metabolism had only previously been implicated in ALS-FTD. Among the RNA metabolic processes, Nonsense-mediated mRNA decay was the pathway significantly upregulated in the most datasets analyzed. These results underscore the power of bioinformatics applied on big data and could inform efforts aimed at developing neuroprotective therapeutic strategies against neurodegenerative diseases.215 Figure 1. Figure 2. 249 Richard Norton, BS, Psychiatry Qualitative Evaluation of Heated Yoga for Depression R. Norton1, S. Tuchman1, D. Mischoulon1,2, L. Uebelacker3 and M. Nyer1 1Psychiatry, Massachusetts General Hospital, Wakefield, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Butler Hospital, Providence, RI, USA Introduction: Current treatments for Major Depressive Disorder (MDD) are often ineffective and fail to offer sustained remission. Complementary and alternative treatments are frequently sought by individuals with depression. Randomized controlled trials (RCTs) have demonstrated that whole body hyperthermia (WBH) and non-heated yoga are potential treatments for MDD. Bikram yoga (BY) combines both of these elements. BY is a standardized form of yoga practiced in a heated room (105\u00b0F). We are in the process of conducting the first RCT of BY for depressive symptoms. We present a qualitative analysis exploring trends in exit interview data from study participants with available data thus far. We hypothe-sized that participants would report improvements in depression and related symptoms as a result of their BY participation.216Methods: Qualitative exit interviews were collected from 43 participants (36 completers, 7 early terminators) in both randomized groups: 1) immediate BY for 8-weeks, vs. 2) delayed BY after an 8-week waitlist period. Interview questions examined participants' feelings about BY, aftereffects of BY, study procedures/aspects of the study they would change, and possible independent continuation of the intervention. The questionnaire was administered by study clinicians and transcribed by research coordinators. Exact wording was confirmed by the administering clinicians and checked by a third party to ensure accuracy. The codebook was adapted from a published, peer-reviewed manuscript reporting on perceptions of hatha yoga from an RCT in a depressed population (Uebelacker et al., 2017). Two research coordinators coded responses using thematic analysis by coding the sample independently and resolving discrepancies through consensus to find acceptable concordance. Results: The most frequently reported improvements in DSM-V related symptoms of MDD were: mood (n non-specific positive self-efficacy (n=8, 18.6%). The most frequently reported negative effects of BY were: negative physical effects (n=9, 20.9%), heat-related discomfort (n=8, 18.6%), and difficulty of BY practice (n=4, 9.3%).Conclusion: The results of this qualitative analysis support the continued exploration of BY as a potential treatment for depression. Participants reported improved sleep, energy, mood, and reduced anxiety associated with BY. Most participants expressed positive feelings about BY. Most negative evaluations about the intervention were related to general physical discomforts. Future qualitative information from studies of BY should be gathered to further understand the feasibility and acceptability of this potential treatment modality in a depressed population. 250 Ugoji Nwanaji-Enwerem, Dermatology Novel Strategy to Treat Melanoma based upon A Potential Anti-Tumor Mechanism of LSD1 in Therapy U. Nwanaji-Enwerem1,2, S. Kato1 and D. Fisher1 1Dermatology, Massachusetts General Hospital, Boston, MA, USA and 2Brown University, Providence, RI, USA Introduction: Melanoma is a malignant skin cancer caused by transformation of melanocytes. A therapeutic dilemma exists for treatment efficacy in microphthalmia associated transcription factor (MITF)-low and receptor tyrosine kinase AXL high melanomas (MITF-low/AXL-high melanomas), where resistance to treatment is elevated due to an undifferentiated tumor cell state. Lysine specific histone demethylase 1 (LSD1) is a recently identified and vulnerable epigenetic regulator in MITF-low/AXL-high melanomas. Although pharmacological or genetic inhibition of LSD1 strongly induces growth defects of MITF-low/AXL-high melanomas by upregulating tumor suppressor gene, N-myc Downstream Regulated Gene 1 (NDRG1), the potential mechanism for the growth inhibitory effect of LSD1 inhibition through NDRG1 remains unclear. Our preliminary studies have suggested that the potent and selective LSD1 inhibitor, SP2509, induces G2/M cell cycle arrest and apoptosis. The objective of this study is to investigate if LSD1 inhibition causes cell cycle arrest and apoptosis, thereby leading were exposed to interval treatment times of SP2509. mRNA was harvested from melanoma cells exposed to SP2509 treatment. Expression of eight apoptotic genes in treatment exposed melanoma cell lines was measured using qPCR. Annexin-V staining was conducted to assess early and late apoptotic cells in MITF-low and MITF-high melanoma cell lines and to determine if tumor suppressor gene, NDRG1, causes apoptosis. Apoptosis induction and the role of NDRG1 was further tested by Western Blotting analyzing cleaved caspase 3 and PARP.Results: qPCR analysis revealed that SP2509 the pro-apoptosis genes, BAX, BAK1 melanoma cell lines, but not in MITF-high/AXL-low melanoma cell lines. Apoptosis induction was further confirmed by Western Blotting for apoptotic markers, cleaved caspase 3 and induction, suggesting that NDRG1 essential mediator of apoptosis in MITF-low/AXL-high melanomas. Conclusion: These studies suggest that pharmacologic targeting of LSD1, a vital dependency factor, may offer a novel therapeutic approach in the key treatment-resistant subset of melanomas characterized by a low-MITF state.217251 Anne ODonnell Luria, MD, PhD, Medicine - Analytic and Translational Genetics Unit (ATGU) Defining the phenotypic spectrum of a novel intellectual disability syndrome due to de novo variants in KMT2E A. O'Donnell Luria1,2,3, L. Pais1,2, J. and D. MacArthur1,4,2 1Analytic and Translational Genetics Unit, Massachusetts General Hospital, Boston, MA, USA, 2Program in Medical and Population Genetics, Broad Institute of MIT and Harvard, Cambridge, MA, USA, 3Pediatrics - Genetics and genomics, Boston Children's Hospital, Boston, MA, USA and 4Center for Genomic Medicine, Massachusetts General Hospital, Boston, MA, USAIntroduction: Despite significant advances in genomic technologies, it is estimated that more than half of the genes that cause rare diseases remain undiscovered. Using large-scale human genomics of healthy individuals, we can identify genes that are depleted for loss-of-function or missense variation, known as constrained genes. The Genome Aggregation Database (gnomAD) project is a large-scale reference database with high quality, jointly processed exome or genome data from 141,456 individuals. Leveraging the gnomAD dataset, we have defined the tolerance of each gene to loss of function variation in the general population, identifying thousands of genes that are intolerant of loss-of-function variation. Over two-thirds of these candidate haploinsufficient genes are not yet associated with any human disease phenotype. KMT2E (MLL5) is one such candidate haploinsufficient gene in which there is only one high-quality nonsense or canonical splice site mutations seen in gnomAD, while 79 are expected based on the gene length and sequence. KMT2E has previously been implicated (but not definitely proven) as a candidate autism disease gene in exome sequencing studies but no further phenotype information is available. Loss of function in KMT2E is predicted to cause a disorder of transcriptional regulation. KMT2E encodes a histone methyltransferase epigenetic protein, a transcriptional regulator reported to play key roles in diverse biological processes, including cell cycle progression, genomic stability maintenance, adult hematopoiesis, and spermatogenesis.Methods: We have identified 33 affected individuals (1-35 years of age) with heterozygous variants in KMT2E that were mainly ascertained from Matchmaker Exchange. Almost all variants were shown to have occurred de novo and include twenty-four protein truncating variants (PTVs), five missense variants, and four encompassing KMT2E. Most affected individuals with protein truncating variants presented with mild intellectual disability and/or autism. Additional common features included relative macrocephaly, hypotonia, and mild neuroimaging abnormalities including white matter volume loss and thinning of corpus callosum. There appears to be a subtle facial gestalt characterized by dolichocephaly, deep set eyes with downslanting palpebral fissures, and infraorbital creases. Results: Epilepsy was not a common feature for individuals with truncating variants. In contrast, for the five probands with distinct missense variants often presented with epilepsy, including infantile-onset epileptic encephalopathy. Of note, one infant with epilepsy had severe developmental regression when started on the ketogenic diet, as ketones have been shown to inhibit histone deacetylases. Haploinsufficiency versus gain-of-function of KMT2E may explain this divergence in phenotype, but requires independent validation. Also, there was a male predominance of cases, suggesting possible sex- related penetrance or expressivity, although the mechanism behind this is not yet known. Conclusion: This case series supports KMT2E as a novel intellectual disability and autism disease gene. 252 Sarah ODor, Psychiatry Structural Brain Alterations in Childhood Psychiatric and Autoimmune Disorders Related to OCD Symptom Severity S. O'Dor1, N. Hadjikhani2, N. Ward2, H. Smilansky1 and K. Williams1 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA and 2Radiology, Massachusetts General Hospital, Charlestown, MA, USA Introduction: Pediatric Autoimmune Neuropsychiatric Disorders Associated with Streptococcal Infections (PANDAS) refers to the acute and dramatic onset of neuropsychiatric syndrome in children, including obsessive-compulsive disorders (OCD) and/or tics (motor and vocal), as well as by motor and sensory symptoms. PANDAS is hypothesized to result from an autoimmune reaction directed against the basal ganglia nuclei, initiated by a Group A Streptococcal infection, in a manner similar to the previously mentioned diagnosis of Sydenham Chorea. To date, only one cohort study utilizing MRI in subjects with PANDAS has been published, and results implicated neuroinflammatory responses in the basal ganglia and its associated networks. The current study was conducted to add to this literature and examine potential neurophysiological changes related to neuroinflammation in children with PANDAS (1) compared to healthy controls and (2) in relation to OCD symptom severity.218Methods: PANDAS subjects (ages 6-17) and age- and gender-matched healthy controls (HCs) were recruited through the MGH Pediatric Neuropsychiatry and Immunology Program. PANDAS subjects were required to 1) meet all diagnostic criteria for PANDAS, 2) have had a documented streptococcal infection in the 6-8 weeks prior to symptom onset, 3) meet criteria for Obsessive Compulsive Disorder (OCD), and 4) have a OCD severity score (measured by the Children's Yale Brown Obsessive Compulsive Severity Scale [CYBOCS]) >20. Healthy control subjects were required to have no current or previous history of psychiatric or behavioral diagnoses. All diagnoses were verified using the MINI-KID (Mini-International Neuropsychiatric Interview Child Version 6.0), and OCD severity was measured using the CYBOCS. MRI scans were conducted using a 3T Siemens Scanner. Volumetric, Diffusion Tensor Imaging, and T1/T2 weighted sequences were obtained for all subjects. Data was analyzed for eight a priori regions of interest (ROIs) of the basal ganglia nuclei, hippocampus, and orbitofrontal cortex (OFC). Structural volumes generated using automatic segmentation in FreeSurfer; T1 and T2 and DTI maps were generated using FSL and FreeSurfer. ANCOVAs were conducted comparing volumes of ROIs between children with PANDAS and HCs while controlling for age, gender, and total brain volume. Partial correlations were used to assess the relationship between MRI metrics and symptom severity (CYBOC) in children with PANDAS while controlling for age, gender, and for volumetric analyses, total brain volume.Results: Seventeen PANDAS subjects (avg age=8.75\u00b11.8 yrs) and eight HCs (avg age=9.5\u00b11.6 yrs) produced usable scans for analysis. No significant differences were found in cortical or subcortical volumes between children with PANDAS and controls. When controlling for age, gender, and total brain volume, significant correlations were found between CYBOCS severity score and left pallidal volume (p=.031), left pallidal T1-relaxation time (p=.048), and Fractional Anisotropy values in the left palladium (p=.017), along with the left and right OFC (p=.03 and .04, respectively).Conclusion: The results of this pilot analysis of multimodal MRI analysis in children with PANDAS suggest that structural alterations and measures of structural integrity may correlate with symptom severity in children with PANDAS. More work is required to elucidate the role that neuroinflammation may play in the mechanisms associating these potential disease sequalae. 253 Sharon Odametey, MPH, Medicine The impact of predictive analytics and tailored interventions on healthcare utilization of older patients with multiple comorbidities S. Odametey 1,2,3, R. Palacholla1,2,3, S.B. Golas1,2, M. Buijs4, K. Jethwani1,2,3 and A. Oreinstein4 1Partners Healthcare Pivot Labs, Boston, MA, USA, 2Massachusetts General Hospital, Boston, MA, USA, 3Harvard Medical School, Boston, MA, USA and 4Philips Research, Eindhoven, Netherlands Introduction: The most common cause of emergency department (ED) admissions in the aging population is health deterio - ration due to chronic conditions. The associated soaring healthcare costs necessitate the development of innovative strategies to deliver high-quality care. Currently, many healthcare organizations employ intensive population health management strategies targeted at patients in the top segment of the cost acuity pyramid, such as the integrated care management program (iCMP) at Partners Healthcare. Due to the transition of these high-cost patients to lower cost segments in subsequent years, a modified strategy is to target population health interventions at patients in the middle segment, using connected solutions that seamlessly integrate data and provide actionable insights. Such a solution is provided by Philips Lifeline's CareSage risk assessment system that utilizes data from a Personal Emergency Response System (PERS) to identify patients at high-risk of ED admissions. Objective The goal of this study is to evaluate the impact of a predictive analytics-based risk assessment system (CareSage) and tailored interventions on the healthcare utilization of patients in the middle segment of the cost acuity pyramid.Methods: This study is a two-arm randomized controlled trial with 370 patients that had at least one episode of Partners home healthcare prior to the study period. Additionally, all patients used PERS at home during the study period. PERS is a home monitoring service consisting of three components: a help push button, an in-home communication system and an emergency response center. During a 6-month intervention period, the CareSage predictive model classified patients in the intervention group as \"high\" or \"low\" risk for ED admissions in the upcoming 30 days. All patients flagged as \"high risk\" received nurse triage calls to assess their needs. Then, personalized interventions such as patient education, home visits, and tele-monitoring were provided. The primary outcomes were the number of 90- and 180-day ED visits. Secondary outcomes were mortality rates, time to first readmission, (avoidable) readmission rates, and total medical expenditure. The two study groups were compared by t-test (two-tailed) for normally distributed and Kruskal-Wallis Rank Sum test for skewed continuous variables, respectively. The chi-square test was used for categorical variables. Results: Patient enrollment was completed in July 2018. Data analysis is currently ongoing, and results will be completed by the time of the MGH Clinical Research Day.219Conclusion: This study is unique in utilizing an integration of PERS connected technology and the EHR as a source of data to identify patients at risk of ED admissions from the middle cost acuity segment. Innovative solutions based on integrated EHR and PERS data that combine actionable data analytics with personalizing interventions can help healthcare organizations to manage their population health in home or community settings. These solutions facilitate the delivery of value-based care, improve patient health outcomes, and decrease healthcare costs. 254 Moshood Olanrewaju, MBBS, MPH, PhD, Emergency Effectiveness of the ESM-UBT Package for Management of Uncontrolled Post-partum Hemorrhage among Referral Facilities in Maharashtra and Madhya T.F. Department, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Harvard T.H. Chan School of Public Health, Boston, MA, USA Introduction: The Uterine Balloon Tamponade (UBT) is recommended for management of post-partum hemorrhage, but evidence of its effectiveness in low-resource settings is mixed. The 'Every Second Matters for Mothers - Uterine Balloon Tamponade' (ESM-UBT) package includes a training program that emphasizes effective provider performance in UBT device placement and overall PPH care. Evaluation of the package has been limited to case series and qualitative assessments. We conducted a retrospective difference-in-difference evaluation of an ESM-UBT program implemented in India.Methods: Selected Obstetrician-Gynecologists from nine medical colleges were trained. They conducted training activities for colleagues in their departments over a period 7 months (Jan-Jul 2017), designated as the intervention period. Relevant data were obtained from each facility's record, for 12 months preceding program commencement, for the 7-month intervention period and the following 12 months. Diagnosis of PPH was based on the judgement of the clinicians. Comparable data was obtained from two comparison facilities. A retrospective difference-in-difference design was used to compare PPH-related adverse maternal outcome rates. The primary outcome was a composite variable indicating the rate at which women experi - enced death and/or emergency hysterectomy and/or other operative interventions among PPH cases in a facility per month. Using difference-in-difference Poisson models with robust confidence intervals, we estimated the impact of the program. Results: 213,497 women delivered in the period covered in our analyses.69 women died, 114 women had hysterectomies and 321 women either died, had a hysterectomy or another surgery. Of the 213,497 deliveries, 178,966 occurred in the intervention facilities, while 34,531 occurred in the control facilities. The average rate of the primary outcome was not statistically different between the control and intervention facilities before the intervention (-34.5, 95%CI: -154.4, 85.5). However, the difference became significant after the intervention period (134.3, 95% CI: 39. 6, 229.0). In the final models, compared to the control facilities prior to the intervention, there was a 65% (95%CI: 42%-79%) reduction in the composite primary outcome in the intervention facilities after the intervention. There was no statistically significant reduction in death rates in the final model. Conclusion: Implementation of the ESM-UBT package across medical colleges in India was associated with improved outcomes of women with PPH primarily through a decrease in surgical interventions. 255 Marcus Ortega, MD, OB/GYN Presacral anatomy in women with a horseshoe kidney M. Ortega1, T. Pierce2, A. O'Shea2, K. James1, Von Bargen1 and M. Weinstein1 1Obstetrics and Gynecology, MGH, Boston, MA, USA and 2Radiology, MGH, Boston, MA, USA Introduction: Horseshoe kidney (HSK) is the most common renal fusion defect that can alter vascular and upper urinary tract anatomy. Anatomic variations in the presacral space can make dissection of the anterior longitudinal ligament at the time of sacrocolpopexy challenging. This study aims to characterize presacral anatomy in women with HSK. Methods: Large academic center database was quarried to identify imaging studies in adult females with HSK. Available multiplanar CT and MRI images were reviewed independently by two radiologists and relevant vascular and upper urinary tract anatomy was measured and compared with published normal values (Table 1). Study population was compared to the general population using Student's t-test, chi-squared test, or Fisher's exact test as appropriate.Results: 178 women were identified and 20 confirmed to have HSK on imaging. The mean age was 54.5 and BMI 27.3. In our cohort, 10% of the patients were African American, 65 % Caucasian, 5% Hispanic, 5% Asian, 5% other and 15% were unknown. Women with HSK had a narrower angle of aortic bifurcation (39.1 +/- 18.7 degrees vs. 51.55 +/- the right ureter was closer to midline (22.9 +/- 8.8 mm vs. 32.3 +/- 1.2 mm) when compared to general population anatomy. In 95% of women with HSK the inferior kidney pole was below L3-L4, with 15% at the level of S1 compared to normal anatomic location at T-12-L3 (Table 1). Conclusion: Ureteral, renal and vascular anatomic alterations in females with HSK may make sacrocolpopexy challenging by obscuring surgical exposure to the anterior longitudinal ligament anchoring point (Figure 1). Preoperative imaging may be helpful to determine the safety and feasibility of sacrocolpopexy in HSK patients. Table 1. Horseshoe kidney anatomy results * Study population was compared to the general population using Student's t-test, chi-square test, or Fisher's exact test as appropriate. 1. Deswal A, Tamang BK, Bala A. Study of aortic- common iliac bifurcation and its clinical significance. J Clin Diagn Res. 2014 Jul;8(7):AC06-8. Epub 2014 Jul 20. 2. Wieslander CK, Rahn DD, McIntire DD, Marinis SI, Wai CY, Schaffer JI, Corton MM. Vascular anatomy of the presacral space in unembalmed female cadavers. J Dec;195(6):1736-41. 3. Odegard SE, Abernethy MG, Mueller ER. Does Side Make a Difference? Anatomical Differences Between the Left and Right Ureter. Female Pelvic Med Reconstr Surg.2015 Sep-Oct;21(5):249-51. Figure 1: 57 year old kidney. A) Coronal reformatted computed tomography (CT) of the aortic bifurcation angle. B) Axial image at the level of the aortic bifurcation (arrow) which corresponds to the horizontal reference line on the mid sagittal reformatted CT image in figure part C at the L4-L5 level. Aortic bifurcation height is depicted in figure part C as the distance from the bifurcation to the anterior superior S1 endplate. D) 3D volume rendered image depicts the heart, aortoiliac system, the horseshoe kidney, and thoracoabdominal osseous structures. The green overlay highlights the optimal anchor site during sacrocolpopexy surgery. 256 Joseph Owuor, BSc Public Health and Health Sciences, Emergency Perceptions, Barriers, and Facilitators Associated with Management of Postpartum Hemorrhage and use of Uterine Balloon Tamponade in Western Owuor1, N. Omotayo1 1Division of Global Health Innovation, Massachusetts General Hospital, Everett, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Harvard T.H. Chan School of Public Health, Boston, MA, USA Introduction: To understand provider perceptions about postpartum hemorrhage (PPH) care and barriers and facilitators associated with Uterine Balloon Tamponade (UBT) among Primary Health Care Centers (PHCs) in Western Kenya using case study methodology. Methods: PHCs in western Kenya were purposively sampled to maximize diversity across geography and levels of UBT uptake: 'UBT training and UBT use', 'UBT training and no UBT use', and 'no UBT training and no UBT use'. One health - care provider was selected and interviewed from each facility. Reproductive Health Coordinators (RHC) from county and sub-county health administrations were also interviewed. Interviews were recorded, transcribed verbatim, and analyzed for theory-driven and emergent themes. Results: Twenty-six providers from 12 PHCs and six RHCs from three counties were interviewed. Providers had good understanding of the principles of PPH prevention and were familiar with guidelines, but workloads and limited opportuni-ties for hands-on training were identified as barriers to adhering to recommended practices. Providers were aware of UBT but reported limited opportunity to become skilled with the technique. UBT devices were often not available in commodity supply lines.221Conclusion: Providers in primary healthcare facilities in western Kenya are aware of PPH management guidelines as well as UBT. High workloads and few opportunities for practical training are barriers to adherence to PPH guidelines and UBT uptake. Interventions to improve provider self-efficacy in managing PPH and integrating UBT devices into the supply chain are needed. Emerging themes classified into three broad domains 257 Catherine R. Pappano, BA, Psychiatry Developing a population health tool to identify patients with serious mental illness at cancer diagnosis C.R. Pappano1, V. Fung2, Irwin1 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Mongan Institute Health Policy Center, Massachusetts General Hospital, Boston, MA, USA, 3Massachusetts General Hospital, Boston, MA, USA and 4Partners eCare Research and Innovation, Partners Healthcare, Boston, MA, USA Introduction: Individuals with serious mental illness (SMI) experience increased cancer mortality, are less likely to receive timely cancer care, and have higher health care costs. Despite increasing recognition that psychosocial care is essential for quality cancer care, mental illness remains underrecognized and undertreated. Involving psychiatry early may prevent cancer care disruptions. Therefore, we developed an EHR-based algorithm to proactively identify patients with SMI and cancer who may benefit from targeted interventions.Methods: We utilized structured codes from a widely-used, commercial EHR to identify patients with preexisting SMI (psychotic disorders, bipolar disorder, major depression) and new breast, lung, head/neck, and gastrointestinal cancers seen in the Massachusetts General Cancer Center. We developed and iteratively refined an algorithm using ICD-10 diagnostic codes, medical history, appointment types, and consultations from across the health system. The algorithm updates every 24 hours to facilitate rapid intervention. We embedded a classification system to prioritize high-risk subgroups (e.g., psychotic disorders, substance use disorders) and key time points (new oncology diagnosis, appointment in 60 days). We validated psychiatric diagnoses, cancer types, and appointments by randomly selecting 20 charts within each category and conducting a manual chart review.Results: Over 60 days from May to July 2019, the center saw 7674 patients in breast, thoracic, head/neck, and gastrointestinal oncology including 1674 patients (22%) with documented SMI. We identified 69 people (0.9%) with psychotic disorders, 111 (1.4%) with bipolar disorder, and 1,494 (19.5%) with major depression. Among patients with major depression, we priori- tized recurrent major depression (N=199), inpatient psychiatry consultation or hospitalization (N=84), alcohol use disorders (N=77), or opioid use disorders (N=25). SMI was well-represented across cancers including 24% (916/3,815) in breast, 25% (291/1,164) of 24% (180/738) gastrointestinal oncology. Conclusion: We developed an EHR-based tool that identified a substantial population of patients with SMI and cancer and prioritized subgroups at high risk of cancer care disruptions. Leveraging available data in the EHR is a feasible and promising strategy to detect and triage a vulnerable population in need of targeted psychosocial interventions.222258 Kate Park, PhD Candidate, Radiology Longitudinal trafficking of biological tissues using lysosome-fixable agents K. Park1, S. Sajedi1, M. T\u00e9trault1, M. Henry2 and H. Choi1 1Radiology, Massachusetts General Hospital, Concord, NH, USA and 2Chemistry, Georgia State University, Atlanta, GA, USA Introduction: Longitudinal trafficking of live cells in living organisms is crucial to understand the function, toxicity and therapeutic mechanism of systemically or implanted stem cells in clinical use. The ability to monitor the fate of administered cells for a long period of time using high-resolution in vivo optical imaging techniques would be critical for the development of cell-based therapeutic interventions. To determine the presence of administered cells from the host tissue using in vivo real-time imaging, labeling of the target cells with a nontoxic and stable contrast agent is a prerequisite. However, long-term live cell trafficking is currently limited by the lack of available fluorophores in the near-infrared (NIR) wavelengths with steady optical and physicochemical properties. Here we report, for the first time, the design of fixable cell tracking NIR fluorophores (CTNFs) with high extinction coefficients and quantum yields, excellent cell permeation and retention, and high stability in chemical treatment. We demonstrate the efficient cellular labeling and tracking of CTNFs using real-time intraoperative optical imaging and epifluorescence microscopy to follow the fate of NIR fluorescnet cells from the time of injection into animals to ex vivo single cell analysis after resection of target tissues. Methods: We developed heptamethine near-infrared (NIR) fluorophores for cellular tracking in living organisms. A series of ionizable docking functional groups were substituted to the heptamethine cyanine imaging core to compare selective cellular retention and further docking in the subcellular component. Chloride (-Cl), secondary amine (-NH-) were introduced as a pendant domain on the heptamethinebackbone. Results: CTNF126, sequesters inside the lysosomes, which prevent cellular efflux and improve cellular retention. CTNF126 also outperformed all commercially available visible-wave - length fluorophores due to the use of NIR window (650-900 nm) (Fig 1).Conclusion: A novel NIR fluorophore CTNF126 was synthesized for the purpose cell tracking that can withstanding all steps in H&E histological tissue processing while retaining excellent NIR fluorophore properties. We demonstrated that our novel lysosome fixable NIR fluorophore, compared with commercially available visible ranged fluorophore, exhibited outstanding physicochemical and optical properties in serum containing media as well as organic solvents, enabling tracing of single cells for determining the mechanism of various diseases in the body. Longitudinal cell tracking process of NIR fluorophores and lysosomal sequestration of CTNF126. The thickness of red arrow represents ionization tendency.223259 Angelique C. Paulk, Ph. D., Neurology Bidirectional control human emotional conflict resolution with intracranial stimulation A.C. Paulk1,4, A. Yousefi4,3, Hospital, Boston, MA, USA, 2Department of Radiology, Massachusetts General Hospital, Boston, MA, USA, 3Department of Mathematics and Statistics, Boston University, Boston, MA, USA, 4Nayef Al-Rodhan Laboratories, Department of Neurosurgery, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA, 5Department of Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 6Department of Neurology, Brigham and Women's Hospital, Boston, MA, USA, 7Department of Neurosurgery, Brigham and Women's Hospital, Boston, MA, USA, 8Department of Neurology, Beth Israel Deaconess Medical Center, Boston, MA, USA, 9Department of Neurosurgery, Beth Israel Deaconess Medical Center, Boston, MA, USA, 10Department of Neurosurgery, Albert Einstein College of Medicine, Montefiore Medical Center, Bronx, NY, USA and 11Picower Institute for Learning & Memory, Massachusetts Institute of Technology, Cambridge, MA, USA Introduction: A key aspect of normal human behavior is adaptive navigation through our complex social world. Consequently, difficulties in processing and responding to emotional stimuli underlie many psychiatric diseases ranging from depression to anxiety. Processing and responding to emotional stimuli make up the complex framework underlying behavior which is made up of shifting hidden states including attention and emotion reactivity and that these states, in turn, are products of underlying neural activity states. Directly measuring the hidden, cognitive, emotional, and attentional states contributing to emotion conflict resolution, however, is challenging. State-space representations are a powerful method for investigating hidden states underlying a complex system. Here, we leverage state space representations to describe, and predict, behavior in a standard emotion conflict resolution (ECR) task. Methods: Using state space models, we 1) determined what features of the task drove reaction time in a series of interpretable state space models; 2) determined which of these models additionally correlated with independently sampled self-report emotion reactivity questionnaires; and 3) determined which models could then best predict reaction times across a data set of healthy controls (N=42), individuals diagnosed with psychiatric diseases (N=16), and individuals with intractable epilepsy who were undergoing neuromonitoring in the process of their clinical care (N=41). We then settled on a subset of models which survived the criteria of best fit, interpretability, correspondence with self-report questionnaires, and high predictive capability. If the state-space model features indeed described an underlying hidden cognitive state, we hypothesized that we could perturb the state space behavior with the use of direct electrical stimulation in brain regions known to be involved in the ECR task. Working with participants with intractable epilepsy undergoing clinically indicated intracranial recordings while they performed ECR (N=17), we used brief trains of focal high frequency electrical stimulation (400 ms in duration, 160 Hz) to alter behavior in the ECR task in these different brain regions to induce behavioral changes, including changes in state, depending on the brain region stimulated which included the dorsolateral and Results: Using stimulation in the dlPFC, dmPFC, amygdala, dACC, and rACC during the task, we found that we could reliably alter behavior; with neural stimulation in the dmPFC and dACC having significant but opposing effects on the hidden state of one particular model, Emotion Conflict-Adaptation (N=13; p=0.012; Kruskal-Wallis multiple comparisons test). To validate these results, we performed real-time closed-loop adaptive stimulation in the dmPFC and dACC and found we could bi-directionally alter behavior over the course of the task (N=3). In all three participants, dmPFC stimulation increased a behavioral state both during and after stimulated trials. In contrast, dACC stimulation decreased the same behavioral state. In addition, two of the participants spontaneously volunteered their subjec-tive impression of the testing. One participant stated they felt like \"the answers were more immediate.... It shows up and it was a reactionary I hit the button\". A second participant stated \"It was harder to, like, think... but the task seemed easier... I didn't have to think about [the task] as much\". These results indicate both direct and indirect effects of enural stimulation on behavior in the ECR task.Conclusion: In a series of investigations to model, and modulate, hidden cognitive states of emotion conflict resolution, we demonstrate that 1) Not all state space models can fit the behavior in a cognitive test; but 2) the behavioral state space approach can be used to improve our understanding of what cognitive states drive behavior in a task; and 3) by altering the underlying network through neural stimulation, we can causally, and bidirectionally, identify separable hidden cognitive states along with the brain regions involved in these states. These approaches represent a principled first step toward identi - fying and describing the complex hidden dynamics underlying emotion conflict resolution while separating the effects of attention and the difficulty of the task and allow us to arrive at a better understanding of how to sample, and understand, emotional processing in a way which could be leveraged in neuromodulatory therapy for disorders of emotional regulation, particularly in using direct electrical stimulation.224260 Elizabeth M. Pearce, BA, Dermatology Using a CO2 Fractional Laser to Promote Normal Scar Remodeling in Patients with Radiation Injury E.M. Pearce1, T.T. Tran4, Y.E. Chen2 and R. Anderson4,3 1Wellman Center for Photomedicine, Massachusetts General Hospital, Boston, MA, USA, 2Francis H. Burr Proton Therapy Center, Massachusetts General Hospital, Boston, MA, USA, 3Dermatology, Harvard Medical School, Boston, MA, USA and 4Wellman Center for Photomedicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Radiation therapy is an effective cancer treatment; however significant cutaneous adverse events can occur as a result of the skin's exposure to ionizing radiation. Chronic Radiation Dermatitis (CRD) also known as chronic radiation injury, can be a debilitating, disfiguring and painful condition resulting from ionizing radiation. CRD can result in permanent changes to the skin, including fibrosis, telangiectasias, and skin atrophy. This can negatively impact patients' quality of life, due to pain, discomfort, limited mobility and reduced cosmesis. There are currently limited treatment options for radiation dermatitis and no gold standard of care. Prior studies have shown that fibrotic scars can be normalized using fractional laser treatment (FLT), which leads to tissue repair. Therefore, we hypthesize that FLT can promote normal scar remodeling in patients with chronic radiation induced fibrosis. Methods: A prospective, randomized study of patients with significant radiation induced fibrosis. Each study site is treated with a CO 2 fractionated laser and has an internal control which does not receive any intervention. Evaluations include a subjective rating using the SF-36 Health Survey, clinical photographs, and objective measurements: scar thickness measured by ultrasound, scar compliance measured by Derma Torque Meter and erythema and pigmentation measured by a DermaSpec - trometer. These evaluations occur at an eligibility visit, after 3 laser treatments, and 3, 6 and 12 months after the last treatment.Results: Preliminary data shows that elasticity improves in the 12 months following 3 laser treatments, indicating a reduction in fibrosis. Clinical photographs show that telangiectasias also improves with laser therapy. To date, 9 subjects have been recruited for this study, and two subjects have completed all study visits. Conclusion: Initial analysis indicates that FLT can improve cosmesis and decrease functional limitations associated with chronic radiation dermatitis. More research is needed to understand the mechanisms of chronic radiation injury and devise appropriate interventions to treat radiation injury. Study images of subject 01 261 Neelam A. Phadke, MD, Medicine - Allergy/Immunology/Rheumatology A. Banerji1,2 and K. Blumenthal1,2,7 1Allergy/Immunology, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Medicine, Mongan Institute, Boston, MA, USA, 4Performance Analysis and Improvement Unit, Massachusetts General Hospital, Boston, MA, USA, 5Massachusetts General Physicians Organization, Boston, MA, USA, 6Cardiology, Massachusetts General Hospital, Boston, MA, USA and 7Edward P. Lawrence Center for Quality and Safety, Massachusetts General Hospital, Boston, MA, USA Introduction: Allergic condition management more often requires allergist guidance than allergy testing; necessary testing may be unavailable at initial allergy consultations. Electronic consultations (e-consults) provide expedited, problem-focused, potentially cost-saving care in other medical specialties, but have not yet been studied in Allergy/Immunology. The objective of this study was to describe e-consult use at an academic allergy/immunology practice.225Methods: E-consult data (August 10, 2016 through July 31, 2018) and in-person consult data (August 1, 2014 through July 31, 2018) were reviewed to determine consult volume, outcomes, indications, and timing. Referral reasons and wait times were compared with chi-squared tests. Results: E-consults grew from 1% to 10% of all new consults with concurrent growth in in-person consults. Of 306 completed e-consults, 41 (13.4%) made diagnostic, therapeutic, or alternative referral recommendations with 30 (73%) recommenda - tions followed; 183 (59.8%) patients required an in-person Allergy/Immunology consult, and only 5 (<2%) patients saw an allergist without an e-consult recommendation to do so. E-consults were used more often than in-person consults for adverse drug reactions (66% vs 9%, p<0.001), especially penicillin allergy (132, (15% vs 2%, p<0.001). Allergists completed e-consults in a median of 11 minutes with a median turn-around time of 22 hours. E-consult implementation was associated with a decreased median in-person consult wait time (1.5 fewer calendar days, p<0.05).Conclusion: E-consults were increasingly used, particularly for historical adverse drug reactions and immunodeficiency. Implementation of an e-consult program resulted in decreased in-person wait times despite an increase in overall consult volume, supporting this model's ability to provide expedited, problem-focused care. Indications for Electronic and In-Person Consults* *Data shown as n (%) P-values are calculated using Chi-squared test For e-consults, includes issues related to atopic dermatitis, and other types of dermatitis **For e-consults, includes issues related to asthma, cough, and shortness of breath; ADR = Adverse Drug Reaction, AFR = Adverse Food Reaction Allergy/Immunology E-Consult Outcomes: This flow chart describes all 321 e-consults ordered. E-consults were analyzed based on need for in-person allergy/immunology consult (n=183) and other diagnostic (n=28), therapeutic (n=9), and referral (n=8) recommendations. Some e-consults made multiple recommendations. Recommendation percentages are out of 306 completed e-consults; recommendations followed percentages are of relevant recommendations made (e.g., 66.7% is 16 of 24 diagnostic testing recommendations). 262 Olivia Pickett, Psychiatry The use of shed primary teeth as a new and objective biomarker of early life stress exposure: Exploring issues of study feasibility O. Pickett2, R.V. Mountain2 and E.C. Dunn2,1,3 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Center for Genomic Medicine, Massachusetts General Hospital, Boston, MA, USA and 3Henry and Allison McCance Center for Brain Health, Massachusetts General Hospital, Boston, MA, USA Introduction: Mental disorders, including depression, and anxiety, are highly prevalent, affecting an estimated 1.1 billion people around the world. One of the strongest predictors of mental illness is exposure to early life adversity, including poverty, child maltreatment, and prenatal stress exposure. However, one of the biggest barriers impeding prevention efforts for mental disorders is the lack of tools to measure the occurrence of early life adversity. Current \"gold standard\" measures rely largely on retrospective or prospective self-reports, each being susceptible to major memory and recall biases. To address this challenge, we have begun to think creatively about a new measure of early life adversity that could be derived through the use of children's shed primary (\"baby\") teeth. As teeth provide a record of their formation process, similar to rings on a tree marking their growth, our hypothesis is that exposure to early life adversity can be non-invasively measured in shed teeth. Methods: Through the use of a qualitative survey, we will begin to address the question of feasibility with respect to collecting children's shed teeth for research purposes.Scientists routinely collect biological specimens from parents of young children, including saliva, hair, blood, and other measures. However, teeth have only recently begun to be collected in scientific studies and little is known about how willing parents may be to donate their children's teeth to scientists. To our knowledge, no study has evaluated parental beliefs and attitudes related to the use of their children's teeth for research; such insights are needed to understand what \"will work,\" and how best to design data collection protocols in light of such issues. Qualitative methods - such as survey research - provide the opportunity to strengthen and add nuance to our understanding of research-related problems and strategies to overcome them, ultimately leading to improvements in the design of future 226research studies. We have designed a survey to assess parental attitudes on this topic and are now working with pediatric and dental clinics at Mass General to implement the survey.Results: The survey queries parents or caregivers on 3 core topics: (1) general practices when children's teeth are naturally shed, (2) Tooth Fairy traditions, and (3) willingness to donate teeth compared to other commonly collected biomarkers. Conclusion: Upon completing this project, we will be able to identify principles to guide the development of strategies and best practices to support study participants from different backgrounds, design optimal protocols, promote study engagement, and serve as a standard for future studies collecting shed primary teeth. 263 Madeline Plansky, Psychiatry Common genetic variation that influences educational attainment associates with cross-sectional and longitudinal academic functioning in youth referred for neuropsychiatric Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Psychiatry, Massachusetts General Hospital, Boston, MA, USA and 3Harvard Medical School, Massachusetts General Hospital, Boston, MA, USA Introduction: A recent genomewide association study (GWAS) of over one million individuals (Lee et al. Nat Genet, 2018) has identified a significant polygenic contribution to educational attainment (EA). In turn, this common genetic variation has been shown to relate to EA as well as a range of relevant constructs in population-based cohorts, including brain morphology, cognition and even academic achievement and grade point average in adolescents. While there is no doubt that environmental influences also play an important role in educational outcomes, these data raise the possibility that polygenic variation associ-ated with EA could help to stratify youth at risk for school failure who could benefit from additional academic supports. Such an objective risk indicator could be particularly valuable in the child psychiatric setting given that youth with neuropsychiatric illness, on average, experience elevated rates of academic underachievement but also show significant individual differences in academic outcomes. To lay the groundwork for these efforts, we aimed to establish the relevance of EA-related polygenic variation to cross-sectional and longitudinal academic phenotypes in youth consecutively referred to an MGH outpatient clinic for neuropsychiatric evaluation.Methods: Participants were 477 youth, ages 5 to 18, consecutively referred to the MGH Learning and Emotional Assessment Program and enrolled in research (i.e. the Longitudinal Study of Genetic Influences on Cognition; LOGIC). We used linear regression and mixed effects models to examine 1) associations between EA-related polygenic variation and achievement in specific academic domains including reading, math and spelling and 2) the extent to which youth with the lowest (bottom 30%) educational attainment score showed a different trajectory of academic achievement over time. Additionally, we used structural equation modeling (SEM) to examine the extent to which cognition mediated the relationship between polygenic variation and achievement. We corrected for multiple testing and controlled for age, sex, psychotropic medication use and the first 5 principal components of ancestry.Results: In our overall sample of 477 clinically-referred youth, polygenic variation underlying EA associated with cross- sectional academic performance in reading, math and spelling. The strongest associations were found for numerical operations, with the strongest effect (R 2=5.8%) at the most inclusive threshold from the discovery GWAS. Here, a one standard deviation reduction in EA polygenic variation resulted in a score of 3.56 points lower score on this subtest. In a subset of 116 youth with longitudinal data, the high risk group (lowest 30%) had a significantly different developmental trajectory compared to the low risk group for both reading and math achievement. For example, with every year of age, the score on word reading in the high risk group was 1.44 index points lower than the low risk group. No significant difference in trajectory was found for spelling. Finally, using SEM, we identified a significant direct effect of EA polygenic variation on a latent academic functioning variable (\u00df=.13, z=2.96, p = .003) as well as a significant indirect effect that was mediated by full scale IQ (\u00df=.19, z=4.95, p<.0001). This direct effect accounted for 40.6% of the total effect (\u00df=.32, z=5.58, p<.0001) of EA-related genetic variation on academic functioning. in SEM, we identified both a direct effect of EA polygenic variation on academic achievement as well as an indirect effect through full scale IQ. SpecificallyConclusion: Our data suggest that EA-related polygenic variation could serve as a tool in the outpatient child psychi-atric setting to aid in the stratification of youth at risk for poor academic outcome. Exactly how such risk scores could be implemented in the clinical setting will require further empirical investigation.227264 Nicole Polanco, Bachelor's of Art, Dermatology Evaluating the Accuracy of the VitalWellness Device N. Polanco1,2, S. Odametey1,2,3, N. Derakhshani1,2, Devoe1,2, K. Jethwani1,2,3 and S. Kakarmath1,2,3 1Pivot Labs, Partners HealthCare, Boston, MA, USA, 2Massachusetts General Hospital, Boston, MA, USA, 3Harvard Medical School, Cambridge, MA, USA and 4Vital USA, West Palm Beach, FL, USA Introduction: Wellness devices for health tracking have gained popularity in recent years. Additionally, portable and readily accessible wellness devices have several advantages when compared to traditional medical devices found in clinical environ- ments. Building tools for patients to manage their health independently, may benefit their health in the long run by improving health care providers (HCPs) awareness of their patients' health information outside of the clinic. Increased access to portable wellness devices that track vital signs may increase how patients and HCPs track and monitor chronic conditions which can improve health outcomes. The VitalWellness device (VW) is a portable wellness device that can potentially aide vital sign measuring for those interested in tracking their health. In this diagnostic accuracy study, we evaluated the clinical perfor-mance of the VW a wireless, compact, non-invasive device that measures four vital signs using the index finger and forehead, against reference vital signs devices used in the hospital setting. Methods: Volunteers age 18 years were enrolled to provide blood pressure (BP), heart rate (HR), respiratory rate (RR) and body temperature. We recruited volunteers with vital signs that fell within and outside of the normal physiological range, depending on the measurements they consented to undergo. A sub-group of eligible volunteers were asked to undergo an exercise test, aerobic step test and/or a paced breathing test to analyze the VW device's performance on vital signs outside of the normal physiological ranges for HR and RR. Vital signs measurements were collected with the VW device and FDA- approved reference devices. Mean, standard deviation, mean difference, standard deviation of difference, standard error of mean difference and correlation coefficients were calculated for measurements collected; these measurements were plotted on a scatter plot and a Bland-Altman plot. Sensitivity analyses were performed to evaluate the performance of the VW device by gender, skin color, finger size and in the presence of artifacts. Results: 265 volunteers enrolled in the study and 2 withdrew before study completion. Majority of the volunteers were female (62%), predominately white (63%), graduated from college or post college (67%) and employed (59%). There was a moderately strong linear relationship between VW BP and reference BP, r = 0.7, P<0.05; and VW RR and reference RR measurements, r=0.7, P <0.05. The VW HR readings were significantly in line with the reference HR readings, r= 0.9, P <0.05. There was a weaker linear relationship between VW temperature and reference temperature, r = 0.3, P<0.05. There were no differences in performance of the VW device by gender, skin color or in the presence of artifacts. Finger size was associated with differential performance for RRConclusion: Overall, the VW device performed well in taking BP, HR and RR when compared to FDA-approved reference devices and has potential serve as a wellness device. To test adaptability and acceptability, future research may evaluate user's interactions and experiences with the VW device at home. In addition, the next phase of the study will evaluate transmitting vital sign information from the VW device to an online secured database where information can be shared with HCPs within seconds of measurement. 265 Kathryn E. Post, PhD, RN, ANP-BC, Nursing Understanding Patient Engagement In Breast Cancer Survivorship Care: A National Web-Based Survey K.E. Post Cancer Center, Massachusetts General Hospital, Somerville, MA, USA Introduction: The transition to survivorship requires breast cancer survivors to actively engage in self-managing their care, but little is known about patient engagement (PE) into survivorship care (SC) and what factors may contribute to this. Information is needed to further explore PE into SC, what factors may contribute to it and which patients are more likely to engage in SC and thus be better equipped to self-manage during survivorship. The purpose of this study was to explore how demographic/personal factors and survivorship variables are related to and may contribute to PE in early stage breast cancer survivors. Methods: A cross-sectional, web-based self-report national survey was conducted using measures assessing personal/demographic factors, survivorship variables: health-related quality of life (HRQOL), fear of cancer recurrence (FCR), cancer health literacy (CHL) and two measures of PE (patient activation (PA) and knowing participation in change (KPC). There was one open-ended question regarding additional survivorship concerns not addressed in the survey. Participants were recruited 228using Dr. Susan Love's Army of Women Research Foundation and Craigslist in major metropolitan areas across the United States. Data were analyzed via bivariate associations and backwards linear regression modeling in SPSS.Results: The final sample included 303 participants (301 females and 2 males), mean age 50.70 years. The sample was predominantly White, non-Hispanic and equally dispersed across the United States. PE was significantly correlated with 13 predictors. There were 10 predictors resulting in significant ANOVA relationships with PA and KPC. The final variables included in the regression models were HRQOL, FCR, social support, level of education, income and receipt of a survivorship care plan. In both the KPC and the PA regression models, HRQOL was significantly associated with PE (p .001), see Tables 1 & 2. In the PA regression model, social support contributed unique variance (10.5%) but was not statistically significant, see Table 1. In the PA regression model, as HRQOL and social support increased, so did participants' activation level. In the KPC regression model, social support and level of education were significantly associated with PE (p .001) and receipt of a survivorship care plan contributed unique variance to the model (9.1%) but did not meet statistical significance, see Table 2. Thus, as HRQOL and social support increased, participants' KPC increased. Additionally, if participants received a survivorship care plan, their KPC increased. The open-ended question response categories included: physical and mental health concerns, financial toxicity, social support, body image concerns, other concerns or no concerns/none.Conclusion: This study provides preliminary evidence that personal/demographic factors and survivorship variables may contribute to PE in early breast cancer survivors. Using assessment tools measuring factors such as HRQOL, social support, education level and PE may give providers some insight as to which survivors may be ready to engage in SC and those that may need more resources and support. Additional studies are needed to replicate and validate these results. More research is needed aimed at maximizing patient-centered care, PE and ultimately improving SC for cancer survivors and their families. Table 1 PA Backward Regression Model Summary R2 = .136; adjusted R2 = .129; Standard error of estimate = .69783; SS = 131.958 N = 237Table 2 KPC Backward Regression Model Summary R2 = .320; adjusted R2 = .308; Standard error of estimate = 9.20113; SS = 28521.162 N = 234 266 Ali Pourvaziri, Radiology Successful Endovascular Thrombectomy Significantly Reduces Infarct Growth in both Early and Late Time Windows, but not for Patients with \"Large\" Admission Infarcts F. MA, USA Introduction: Endovascular thrombectomy (EVT) of acute stroke patients with large vessel occlusion (LVO) and small infarcts (<50ml) has been proven to be of benefit up to 24-hrs post ictus. Our purpose was to investigate the relationship between degree of recanalization and infarct growth, stratified by admission infarct volume and time-post-ictus. Methods: We retrospectively studied 223 consecutive LVO patients who underwent EVT between 6/1/2012 and 12/31/2017. 92/233 met inclusion criteria including available admission and CT or MR 12-hrs to / [admission volume] * 100, (measured as L*W*H/2). Degree of recanalization was determined according to the AOL and TICI scores, as good (AOL 2B/3, TICI 3/4), poor (AOL 0/1, TICI 0-2), or intermediate (everything in-between). Patients were stratified according to admission infarct volume (< vs > 50 ml) and time-post-ictus at presentation (< vs > 6 hours).Results: 92/233 patients; 53 men/39 women. Mean age 68. LVO location included 25 ICA, 84 M1, and 27 M2. Success of recanalization was 43/92 (47%) good, 19/92 and 20/92 (22%) patients with admission infarct volume less than and greater than 50 ml, respectively. There were 68/92 (74%) and 24/92 (26%) patients treated less than and greater than 6-hrs post-ictus, respectively. Mean infarct growth was significantly different among the 3 recanalization groups (good 13.4 ml, 76%; intermediate 45.5 ml, 203%; and poor 102.1 ml, 482%; p<0.01). These differences remained significant when stratified by time-post-ictus (10.8 vs 42.7 vs 116.4 ml, and 81 vs 194 vs 498 %; p<0.01) in the early window group; and remained significant in the late window group (24.5 vs 56.0 vs 80.6 ml, and 58 vs 235 vs 464 %; p<0.01). These differences also remained significant when stratified by admission infarct volume (9.0 vs 46.4 vs 118.4 ml, and 87 vs 229 vs 630 %; p<0.01) in the <50 ml group; but did not remain significant in the >50 ml group (30.0 vs 40.7 vs 57.4 ml, and 37 vs 66 vs 63 %; p=0.4).Conclusion: Successful, robust recanalization following EVT results in significantly less core infarct growth - compared to intermediate and poor recanalization - for both early (<6hrs) and late (>6 hrs) time-window patients. Similarly, intermediate 229recanalization results in less infarct growth compared to poor recanalization. There was no significant benefit of EVT for infarct growth, however, for the subgroup with large (>50 ml) admission infarct volumes. 267 Carolyn L. Qian, B.A., Medicine - MGH Cancer Center Clinicians' perspectives on the essential elements of care for a geriatric oncology intervention C.L. Qian6, and Massachusetts General Hospital, Sparkill, NY, USA, 2Medical Oncology, Massachusetts General Hospital, Boston, MA, USA, 3Geriatric Medicine, Massachusetts General Hospital, Boston, MA, USA, 4Ambulatory Oncology, Massachusetts General Hospital, Boston, MA, USA, 5Palliative Care, Massachusetts General Hospital, Boston, MA, USA and 6Cancer Outcomes Research Program, Massachusetts General Hospital, Boston, MA, USA Introduction: Cancer disproportionately impacts older adults, yet data are lacking regarding clinician perspectives of the distinct care needs of the geriatric oncology population. In addition, older adults with cancer often present a unique set of challenges for the clinicians caring for them, but interventions are lacking that seek to enhance clinicians' ability to provide the highest quality care for older patients with cancer. To improve our understanding of clinicians' perspectives of the unique care needs of the geriatric oncology population and develop an intervention targeting these specific needs, we conducted a qualitative study to explore clinicians' perceptions of the essential elements of caring for these patients.Methods: From February 2019 to March 2019, we conducted separate focus groups with clinicians at Massachusetts General Hospital from the specialties of medical oncology, geriatrics, palliative care, physical and occupational therapy, pharmacy and nutrition, and psychosocial services. We recruited individuals from a variety of subspecialties involved in geriatric oncology care to understand multiple unique perspectives about the multifaceted concerns associated with this population. We used a semi-structured interview guide and asked clinicians to report their: (a) experiences and perceptions of caring for older adults with cancer; and (b) suggestions for improving the care of older cancer patients. Using a framework approach, two team members independently reviewed all data from the focus groups and worked to identify major and minor themes. Results: We conducted six focus groups (27 clinicians enrolled, mean age of 40.2 [SD=10.9] years; 85.2% [n=23] female). Clinicians reported a median of 9.0 (range: 1.0-41.0) years since completing specialty training and a median of 30.0 hours caring for patients in clinic each week. Clinicians estimated that over half of patients with cancer would be considered in the geriatric oncology population (age 70 years). Regarding the unique needs of older adults with cancer, clinicians commented on their experiences related to assessing and managing older patients' physical, cognitive, and social functioning. Clinicians felt that older adults with cancer, compared with younger patients, often have differing care needs related to their comorbid conditions (e.g. pre-existing medical issues may influence treatment decisions, functional and cognitive limitations can influence treatment tolerability), medication management (e.g. complications of polypharmacy and educational needs related to numerous new medications), symptom management (e.g. clinicians must account for polypharmacy and comorbid conditions when prescribing new medications to address patients' symptoms), and illness understanding (e.g. clinicians may need to discuss prognosis in different ways, with multiple members of the family, and clearly document for all members of the care team in case the patient has trouble remembering). Clinicians stated that older adults with cancer could benefit from efforts to help with education about treatment options and potential side effects to expect. Regarding suggestions for 230future interventions for the geriatric oncology population, clinicians commented on the potential benefits of targeted efforts that prioritize helping patients with: designating a healthcare proxy, improving symptom management, addressing physical and cognitive functioning, enhancing coping and prognostic understanding, and fostering empowerment to communicate regularly and effectively with the care team. Clinicians also felt that patients would benefit from this type of targeted interven- tion earlier in the disease course to help patients with treatment decision-making and strategize ways to maintain their quality of life. They emphasized that intervention visits need to occur at times when the patient is already visiting the cancer center for other appointments to try to minimize the burden of additional visits. Additionally, clinicians commented on the need to better facilitate communication between patients' various care teams, and they also mentioned the potential value of collaborative joint visits where clinicians from different specialties see the patient concurrently to foster consistency of information among care teams and ensure that all questions and concerns of the patient and their loved ones are comprehensively addressed.Conclusion: Clinicians perceive that older adults with cancer represent an oncology population with distinct care needs. We found that clinicians caring for the geriatric oncology population report that these patients have unique physical, psychosocial, and cognitive issues meriting further attention and that older patients with cancer often need care tailored to their distinct needs. Our findings underscore the importance of integrating additional support for older adults with cancer targeting the specific care needs of this population, including tailored symptom management, focused attention to comorbidities and polypharmacy, and efforts to enhance treatment decision-making and communication confidence. 268 James P. Quinn, PhD, Neurology Characterizing the proteoform profile of neuropeptide, VGF, as a of dementia J.P. Quinn, S.E. Kandigian, B.A. Trombetta, S.E. Arnold and B.C. MA, USA Introduction: The neuropeptide, VGF (non-acronymic) a crucial role signaling, neurogenesis and synaptic maintenance. VGF is downregulated in the brains and cerebrospinal fluid (CSF) of patients with Alzheimer's disease (AD) and other forms of dementia. VGF can also be detected in plasma samples. AD represents approximately 60% of all dementia subtypes and causes progressive neurodegeneration resulting in memory loss and cognitive impairment. Novel pathophys-iologically relevant biomarkers are urgently required to diagnose AD patients earlier and improve likelihood of clinical trial success. The functional form of neuropeptides is as secreted, protease-cleaved peptide \"proteoforms\". The proteoform composition of processed peptides from VGF is currently poorly understood, as are the proteases involved in their cleavage.Methods: We have used mass spectrometry (MS) of intact processed peptides to produce a detailed map of the VGF proteo-form profile in the CSF of 5 control and 5 AD patients. We have compared the identified CSF profile against known VGF processed peptides, those used in CSF targeted MS assays, brain fractionation MS data and plasma MS data. Results: Using tryptic and non-tryptic techniques, we detected the existence of distinct VGF peptide profiles, suggesting that tryptic digestion of VGF underestimates the complexity of VGF proteoforms. We also identified potential differences in the CSF VGF proteoform profile between AD patients and controls.Conclusion: We are now developing a targeted MS method using data-independent acquisition to characterize the VGF proteoform profile in different forms of dementia, different brain regions and in a larger number of patient CSF and plasma samples. Our results show that (i) standard tryptic digestion methods underestimate the proteoform complexity of VGF, and (ii) that the VGF proteoform profile may differ between control and AD. Understanding the VGF proteoform profile may be crucial in unravelling dementia pathogenesis to identify potential therapeutic targets and novel diagnostic biomarkers. 269 Marina Rakhilin, B.S., Psychiatry Adherence to Online Interventions to Increase Physical Activity in Individuals with Mood Disorders M. Rakhilin, S. Amado, N. George, S.L. Greenebaum, A.P. Shannon, T. Deckersbach, L. Sylvia and A. Nierenberg Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: Online behavioral interventions and activity monitoring technologies present a scalable, cost-effective way to help increase physical activity in individuals with mood disorders who are at-risk for cardiovascular disease (CVD). While greater exposure to online interventions is consistently associated with greater benefits, it is still unclear which populations are best suited to online interventions, and which groups demonstrate high levels of nonadherence and dropout. The goal of this study is to assess which treatment groups as well as which patient characteristics across treatment groups predicted adherence or dropout.231Methods: The Healthy Hearts Healthy Minds study recruited 505 participants who are at risk for CVD and have experi- enced a major depressive episode. Participants were recruited from two online communities, MoodNetwork.org (patients with mood disorders) and Health-eHeartStudy.org (patients at-risk of or diagnosed with CVD). Every eligible participant received a Fitbit Charge 2 to track their number of daily steps. Those who synced their fitbit were randomized to a treatment arm; those unable to sync their fitbit with the online platform were contacted twice, and subsequently considered lost to follow-up. Participants were then randomized to one of three treatment groups: 1) an adjunct 8-week web-based CBT plus plus a fitbit, or 3) fitbit alone. Participants in the CBT and MCBT groups received weekly online interventions and all groups received email reminders to answer biweekly, online, self-report questionnaires. In order to compare adherence and dropout rates within the study, we examined baseline characteristics of participants, or diagnosis (major depressive disorder, MDD, or bipolar disorder, BD), age, sex, race, marital status, and education that may impact adherence to the interventions (i.e., number of intervention e-visits completed).Results: Of the 505 participants eligible for the study, 145 participants (28.7%) were lost to follow-up due to inability/unwilling- ness to sync their fitbit with the online platform. Participants with MDD (N=173; 48%) were more likely than participants with BD to be lost due to follow-up ( 2(1, 6.09), p =0.01). However, of those who overcame this technological hurdle, adherence was high: participants completed 84.25% of their assigned e-visits on average. Interestingly, participants randomized to CBT were more likely to attend the intervention sessions than those randomized to MBCT (R 2=.19, F(6,360)=2.35, p=.03 (b=-5.54, p=.03). Older adults were also more likely to adhere to the intervention sessions than younger participants (b =.27, p<.01). Race, sex, and diagnosis (MDD or BD) did not significantly predict adherence to the intervention sessions. Conclusion: Contrary to prior studies, our online program had high rates of adherence, although the initial onboarding process posed a barrier to many participants (particularly those with BD). These data suggest that older adults and those with MDD as opposed to younger adults and those with BD may be more likely to adhere to an online intervention to increase physical activity in individuals with mood disorders. It is also possible that online CBT may be more acceptable than online MBCT to increase physical activity in individuals with mood disorders. This may help clinicians better understand who is best suited to start an online behavioral intervention for physical activity, but further research is needed to explore predictors of adherence. 270 Chelsea Rapoport, BA, Psychiatry Associations of opioid medication beliefs with pain severity and interference among patients prescribed long-acting opioids for chronic cancer pain C. Rapoport1, E. Wright2, El Jawahri3, J. Temel3 and L. Traeger1 1Psychiatry, MGH, Cambridge, MA, USA, 2Harvard Chan School of Public Health, Boston, MA, USA and 3Hematology/ Oncology, MGH, Boston, MA, USA Introduction: At least two-thirds of patients with incurable cancer experience moderate to severe cancer pain. Although long-acting opioids are a gold standard treatment for chronic cancer pain, patients with cancer commonly report concerns about opioids, such as fears of addiction and other negative effects. This study explored the relationships between patient perceptions of opioid medications and patient experiences of pain.Methods: We analyzed cross-sectional data from an observational study of MGH ambulatory patients with locally advanced or metastatic cancer, who were currently taking a prescribed long-acting opioid for chronic cancer pain (n=131). Participants completed self-report measures of demographic variables, pain, opioid use, and quality-of-life. Patient cancer diagnosis and pain history were collected from electronic medical records. The current analyses focused on negative patient perceptions about pain medications (9-item Family Pain Questionnaire-Knowledge [FPQ-K]) and pain severity and interference with quality of life in the past 24 hours (items from the Brief Pain Inventory). We conducted a series of multiple linear regression models, adjusting for age, gender and race, and long-acting opioid dose (morphine equivalent dose), to evaluate associations of negative patient perceptions of pain medications with pain severity and pain interference at p<.05. Results: Participants (71% female, 85.5% white, medication (75.6%) or a (SD=128.3). 91.6% had prescribed opioid for breakthrough pain. 52.3% of partic - ipants reported at least moderate pain ( 4/10) on average in the past 24 hours. In separate multiple regression models, negative perceptions about pain medications (total FPQ-K score) were associated with greater pain severity on average (B=.42[SE=.11], p<.001) and pain interference with daily quality of life (B=.43[SE=.13], p=.001). In follow-up analyses of the individual FPQ-K items, we further identified that 1) concerns that most patients on pain medicine will become addicted to the medicine and 2) concerns that pain medicine is dangerous and can interfere with breathing were both associated with greater pain severity (addictive: B=.12[SE=.06, p=.04; B=.20[SE=.06], p=.002).232Conclusion: Most patients reported at least moderate pain on average in the past 24 hours despite being prescribed a long-acting opioid regimen for chronic cancer pain. Patients with more negative perceptions of pain medication, including stronger beliefs about pain medication addiction and danger, reported greater pain severity on average and greater pain interference, independent of opioid dose and relevant demographic factors. Results suggest potential barriers to optimal pain control among patients with opioid-related concerns. Results also highlight the importance of addressing patients' opioid-related concerns, especially in the context of current national attention to opioid overuse in the general population. Longitudinal studies are needed to evaluate whether pain medicine beliefs may influence pain over time, via patient pain management behaviors such as opioid non-adherence or gaps in communication about residual pain between patients and their prescribing clinicians. 271 Krishna P. Reddy, Medicine - Pulmonary A higher-sensitivity LAM assay for TB testing in Evaluation Center, Massachusetts General Hospital, Boston, MA, USA, 2Department of Medicine, Harvard Medical School, Boston, MA, USA, 3FIND, Geneva, Switzerland, 4London School of Hygiene & Tropical Medicine, London, United Kingdom, 5Malawi-Liverpool-Wellcome Trust Clinical Research Program, Blantyre, Malawi, 6School of Public Health, University of the Witwatersrand, Johannesburg, South Africa, 7Infection and Immunity, University of Western Australia, Perth, WA, Australia, 8Desmond Tutu HIV Centre, University of Cape Town, Cape Town, South Africa, 9Division of Pulmonary and Critical Care Medicine, Massachusetts General Hospital, Boston, MA, USA and 10Division of Infectious Diseases, Massachusetts General Hospital, Boston, MA, USA Introduction: Testing urine for lipoarabinomannan (LAM) can increase tuberculosis (TB) diagnostic yield and improve outcomes among hospitalized people with HIV (PWH), who are often unable to provide a sputum specimen for standard TB testing. However, despite trial-documented benefits, the first-generation lateral flow LAM assay has suboptimal sensitivity for TB South Africa and Malawi.Methods: We used the CEPAC-International microsimulation model to project clinical and economic outcomes of three TB testing strategies. In line with World Health Organization guidelines, we considered the standard of care to be testing of a sputum specimen by Xpert MTB/RIF, a nucleic urine FujiLAM (Xpert+FujiLAM) . The modeled cohort matched that in the trial (median CD4 236/L [South Africa], 219/ L [Malawi]). We applied diagnostic yields - against a composite microbiologic/clinical reference standard - from a TB diagnostics study among hospitalized PWH in South Africa (yields for Xpert /Xpert+AlereLAM /X- Costs per Xpert/ AlereLAM/FujiLAM were US$15/3/6 (South Africa) and $26/3/6 (Malawi). We projected mortality, life expectancy, costs, and incremental cost-effectiveness ratios (ICERs). LAM strategies were cost-effective if their ICERs were less than those of 2 nd-line antiretroviral therapy: US$940/year-of-life (YLS) saved (South Africa) and $750/YLS (Malawi). In sensitivity analysis, we varied TB prevalence (5-45%, base case 24-29%), sputum provision probability case probability and Xpert+FujiLAM increased life expectancy by 0.8y and 0.4y in South Africa and by 0.6y and 0.3y in Malawi. Xpert+FujiLAM was cost-effective in both countries (Table). Xpert+FujiLAM for all patients was optimal compared with CD4-stratified testing strategies. In multi-way sensitivity analysis, Xpert+FujiLAM remained cost-effective except in scenarios where both sputum provision and empiric treatment probabilities were very high.Conclusion: FujiLAM for TB testing in hospitalized PWH is likely to increase life expectancy and be cost-effective in South Africa and Malawi and should be utilized in TB-endemic settings. 233272 Colin M. Rivet, MS, Medicine - Cardiology Medical Assistant Perception of Implementing Rhythm Assessment Screening as a New Vital Sign in Primary Care Practices C.M. Rivet1, General Hospital, Boston, MA, USA, 2Internal Medicine, Massachusetts General Hospital, Boston, MA, USA and 3Cardiology, UMass Memorial Medical Center, Worcester, MA, USA Introduction: Atrial fibrillation (AF) is a common arrhythmia associated with a substantially increased risk of stroke. Since AF may be asymptomatic and related strokes may be preventable with anticoagulation, attention has focused on assessing the utility of screening for AF. Pragmatic trials are necessary to assess the clinical effectiveness of screening for AF. VITAL-AF is a randomized clinical trial of screening for AF at MGH outpatient primary care practices (ClinicalTrials. gov NCT03515057) using an AliveCor Kardia Mobile device. AF screening is embedded into routine care for patients aged 65 years and older and is performed by practice medical assistants (MAs) rather than research personnel. As part of the study, we evaluated MA experience with AF screening during routine vital sign assessment.Methods: Eight outpatient clinics within the MGH primary care practice-based research network were randomly selected as intervention sites for AF screening using the AliveCor Kardia Mobile single-lead ECG device. Practice MAs were trained by clinical research coordinators (CRCs) prior to study initiation during an initial 30-minute training presentation followed by hands-on demonstration of the AliveCor Kardia Mobile device. MAs were trained to obtain verbal consent and perform screening at the time of routine vital sign assessment. The initial training was followed by a 3-week period of daily CRC support which included AF screening observation and assistance with integration into daily MA workflow. Ongoing device monitoring and MA support was provided by CRCs at each clinic to facilitate study implementation. Newly hired MAs are trained on an ongoing basis during the study period. Additionally, MAs received monthly refresher presentations by CRCs, which include a review of patient approach and screening rates at each practice and reminders regarding the screening protocol. During each monthly refresher, MAs complete surveys that include questions about study staff support, integration of a new vital sign into clinic workflow, ease of performing the screening for AF, functionality of the AliveCor Kardia Mobile device, and willingness of patients to complete screening at additional practice visits after completing an initial screening. We present MA feedback from surveys completed between November 2018 and January 2019 which was approximately 4-6 months after study implementation. Results: At the time the survey was administered, 60 MAs in 8 primary care practices had been trained by study staff, and 31 monthly refresher group training sessions had been held. Survey results assessing MA experience with the study, support, and patient acceptance of repeat screening are summarized in Table 1. Among active MAs, 38 completed the survey (response rate 63.3%). As of January 31, 2019, the MAs approached 86.2% of eligible encounters for screening and completed screening at 77.5% of encounters. Conclusion: More than 95% of the MAs reported feeling supported by the research staff and comfortable using the AliveCor Kardia Mobile device to screen for AF as part of routine patient care. In addition, most MAs indicated they found performing the screening using the AliveCor Kardia Mobile device to be easy. Furthermore, MAs reported that patients were usually willing to repeat screening at subsequent encounters. Preliminary results of this study show that in this pragmatic screening trial, MAs successfully integrated the AF screening technology as part of usual vital sign assessment. Table 1: 234273 Perla M. Romero, MD, Psychiatry Delirium Systematic Review P.M. Romero Psychiatry, MGH, Cambridge, MA, USA Introduction: Delirium complicates patient care and is associated with negative outcomes including significant increases in the requirement for institutional care, length of hospital stay, functional decline, mortality rates, and healthcare costs. Currently, management strategies for delirium consists of both pharmacological and non-pharmacological strategies. Although no medication has been approved by the Food and Drug Administration (FDA) for the treatment of delirium, several drugs including antipsychotics, sedatives, melatonin agonists and dexmedetomidine have been trialed as pharmaco - therapies in the care of delirious patients, but little is known about which type of medication works best depending on the delirum type and illness setting (ICU vs medicine unit vs. surgical unit). To assist psychiatrists and other healthcare providers in optimizing the treatment for delirium, the purpose of this systematic review is to provide a comprehensive review of the scientific literature that encompasses the use of medications for the management of the different types of delirium in different clinical settings. Methods: The guidelines and criteria outlined in Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) were applied to ensure proper reporting of the data, using PubMed, PsychINFO, Scopus, Medline and Embase. The search term delirium or psychomotor agitation was paired with terms relating to different medications, treatment and/or therapy in adults. Systematic searches reveled a total of 9358 studies, and 89 were ultimately included. Results: Currently, data from these studies are being extracted and organized. We will describe the types of medications most commonly studied, the evidence base for their use, and areas for further research. The results of this review will inform clinicians' practices as they make treatment decisions for patients with this high risk illness in different treatment settings.Conclusion: 274 Valerie L. Ruberto, Psychiatry The impact of family psychiatric history on bipolar disorder severity and treatment response O. K\u00f6hler-Forsberg3,1,2, General Hospital, Cambridge, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Psychosis Research Unit; Psychiatry; Department of Clinical Medicine, Aarhus University Hospital, Aarhus, Denmark Introduction: Bipolar disorder is a heritable psychiatric disorder and those with a higher genetic load are more likely to have difficult to treat disorders. Few studies have analyzed the impact of family history of mental disorders on treatment response. The purpose of this study was to assess the impact of family history in first-degree relatives of a broad range of psychiatric disorders on the severity and course of bipolar disorder. Methods: We performed a secondary analysis of data from the Clinical and Health Outcomes Initiatives in Comparative Effectiveness for Bipolar Disorder Study (Bipolar CHOICE) (Nierenberg et al., 2014) and the Lithium Treatment Moderate-Dose Use Study (LiTMUS) (Nierenberg et al., 2009): two 6-month multi-site, randomized comparative effectiveness hybrid trials with a combined total of 757 participants. Correlations between familial psychiatric history with treatment response, disease severity, socioeconomic status, and cardiometabolic markers were analyzed by linear regression and mixed effects linear regression analyses.Results: Among 757 patients, 644 (85.1%) reported at least one first-degree relative with a severe mental disorder (mean=2.8; standard deviation=2.2; range=0-13). Depression (67.1%), alcohol abuse (51.0%) and bipolar disorder (47.0%) were most frequent. Patients with more relatives with a broad range of psychiatric disorders experienced more depressive and manic episodes, psychiatric hospitalizations, suicide attempts, and had a younger age of onset. There was a trend towards more familial psychiatric disorders correlating with a greater number of psychotropic medications needed at every visit (p=0.054). Familial psychiatric history was not correlated with treatment response (p=0.62). There was no correlation between any cardiometabolic marker and familial psychiatric history. Conclusion: Patients with a greater family psychiatric history were more likely to have disorders that were difficult to treat. However, these patients responded similarly to patients without family psychiatric history.235 275 Daniel Rubin, M.D., Ph.D., Neurology Clinical and Electrographic Predictors of Medication Responsiveness in Acute Brain Injury D. Rubin, S. Nooney, I. Lissack, B. Angelini, M. Westover, S. Cash, S. Zafar and E. Rosenthal Neurology, Massachusetts General Hospital, Brighton, MA, USA Introduction: Seizures and ictal-interictal continuum (IIC) activity may impact recovery from acute brain injury (ABI). Empiric antiepileptic drug (AED) intensification for electrophysiologic activity of uncertain significance is challenging to evaluate given structural neurologic deficits, variable pharmacodynamics, and potential sedative effects. We analyzed the EEG and electronic medical records to identify electrographic biomarkers predicting clinical response to AED therapy. Methods: We ascertained patients undergoing continuous electroencephalography (cEEG) during admission for ABI from a prospective big data repository of clinical data including regularly sampled Glasgow Coma Scale (GCS) scores and medica - tion dose administration records. Frequency-specific spectral power Hz, theta 4-7 Hz, and delta 0.5-4 Hz) and graph theoretical metrics of EEG functional connectivity were compared at time intervals before and after AED therapy. Results: 308 patients met inclusion criteria. 9,291 AED doses were administered (mean 1.64 +/- 0.96 unique AEDs per patient). Initiating the first AED was followed by a 0.44-point average improvement in GCS (p=4.32 x10 -7); initiating a second or third AED yielded no significant change, and adding a fourth, fifth, or sixth AED was followed by a 0.69-point worsening in GCS (p=0.044). Improvement in GCS 6 hours after AED administration was heralded by decline in EEG delta power and rise in network density in the hour following treatment. Decline in GCS was heralded by an early rise in delta power and decline in network density. Patients with the highest tertile of EEG improvement (greatest combination of rising EEG density and declining delta power) had a consistently improving GCS trajectory in the 48 hours following medication administration, whereas those in the lowest tertile had a consistently worsening GCS trajectory. Conclusion: Empirically intensifying AED treatment for disorders of consciousness after ABI has diminishing benefit after the initial agent. Quantitative EEG biomarkers of early treatment response appears to robustly predict clinical response following AED treatment. 236276 Natali Rychik, Bachelor of Science, Psychiatry Limited Evidence for Sustained Alcohol Substitution during One Month of Cannabis Abstinence among Adolescents M. Hareli2, H. Broos2, N. Rychik2, Medical School, Boston, MA, USA and 2Center for Addiction Medicine, Department of Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: Cannabis (CB) use is common among adolescents, and rates are likely to increase as states adopt more permis- sive stances on CB legality. While most are concerned about the high rates of youth use, many are hesitant about intervening for fear that this may inadvertently result in an increase in alcohol consumption (i.e., substitution hypothesis). The current study examined the effects of one month of monitored CB abstinence on patterns of alcohol use among adolescents.Methods: Participants were adolescents (N=207), aged 13-25, with (n=141) and without (NU; n=66) current, regular (1x/week) CB use. At baseline, CB users were randomized to one month of biochemically-verified CB abstinence (CB-Abst; n=90) or monitoring (CB-Mon; n=51). Data were drawn from baseline andfour weekly follow-up assessments. Participants completed a Timeline Followback interview at baseline to quantify frequency and amount of past 90-day alcohol and CB use, and at subsequent visits to approximate use in the prior week. The Alcohol Use Disorder Identification Test (AUDIT) and Cannabis Use Disorder Identification Test (CUDIT) were used to assess problematic substance use.Results: At baseline, CB users endorsed more frequent (M=1.4 days, SD=1.1) and higher quantities (M=6.7 drinks, SD=7.8) of past 90-day alcohol use compared days, SD=0.3; amount: M=0.8 drinks, SD=1.7; p<0.001). Among CB users, CB use severity (i.e., age of initiation, frequency and amount of use, and CUDIT scores) was correlated with similar indices of alcohol use severity (r-values: 0.15 to 0.58, p's<0.05). Repeated-measures ANOVAs were conducted to determine if past-week alcohol use changed over the one-month observation period uniquely by group. For days of alcohol use, there were effects of time (p<0.001) and group (p<0.001), as well as a significant interaction (p<0.001). The number of past-week drinking days changed over time only among CB-Abst (p<0.001; CB-Mon and NU: p's>0.57). Drinking days increased in CB-Abst from baseline to week 1 (p<0.001), but decreased by week 2 (p<0.001). By week 2, the number of drinking days was comparable between CB-Abst and CB-Mon (p's>0.05). For amount of past-week alcohol use, there was only a main effect of group, with CB users consuming more alcohol each week than NU (p<0.001). The effect of time and the interaction were not significant (p>0.08). Conclusion: This study replicated prior reports that CB use is dose-dependently linked with level and severity of alcohol involvement in adolescents. Although CB abstinence appears to be associated with an initial increase in frequency of alcohol use, levels return to baseline by the second week of abstinence. Findings do not support concerns for substitution of alcohol for CB in adolescents. 277 Farzin Sadeq, Psychiatry Compliance of Treatment for Burn Injuries in Children with Autism Spectrum Disorders and Attention Deficit/ Hyperactivity Disorder Boston, USA, 2Shriners Hospitals for Children-Boston, Boston, MA, USA, 3Harvard Medical School, Boston, MA, USA and 4Northeastern University, Boston, MA, USA Introduction: Children with autism spectrum disorders (ASD) and attention deficit/hyperactivity disorder (ADHD) are at a higher risk of injuries compared to children with neurotypical development but little is known about the epidemiology of burn injuries in children diagnosed with ASD and ADHD. Research within the pediatric burn population using the Burn Outcomes Questionnaire (BOQ) has been successful in capturing levels of risk on burn-recovery specific domains such as compliance to treatment, pain, and parental concerns.Methods: The current study retrospectively assessed BOQ data completed by parents of children with burn injuries who also had ASD/ADHD symptoms. Longitudinal scores were collected for pain, itch, compliance, parental concerns and satisfaction domains. Results: Of the 593 patients who completed a BOQ, 186 patients were identified as having clinically significant symptoms of ASD or ADHD. 127 patients had multiple parent-reported symptoms which align with diagnoses of ASD and ADHD, including 116 (63.4%) with attention or behavioral problems, 42 (23.3%) with a developmental delay, 7 (3.9%) with an intellectual disability and 90 (49.7%) with learning problems. 66 (37.3%) patients had pre-existing comorbid psychiatric diagnoses of depression. Results indicated that males in the ASD/ADHD sample were at higher risk of burn injury. There 237were no statistically significant differences in parental concerns of children with ASD/ADHD symptoms and of children without ASD/ADHD symptoms, though significant differences were found in longitudinal scores for satisfaction and itch domains. Conclusion: These findings suggest that children with ASD/ADHD symptoms may have heightened difficulties in adhering to wound care, medications, or physical therapy associated with burn care and recovery. While it is important to educate all children in burn prevention efforts, special attention may be necessary to assure specific parent training with children with developmental disabilities. 278 Pariya Salami, Neurology Quantifying the seizure onset zone P. Salami and S.S. Cash Neurology, Massachusetts General Hospital, Boston, MA, USA Introduction: Seizures exhibit great diversity in their region of onset, their electrographic patterns, and their underlying pathology. It is natural to consider, then, whether certain types of seizures result from distinct neuronal mechanisms. Our current work explores the potential role of interactions between local and more distant networks during seizure generation. We measured network interactions by quantifying changes in cross-frequency coupling (CFC) in seizures with different electrographic patterns, arising from different regions to understand how the strength of these values might reflect clinically relevant information about the seizure's neurophysiological dynamics. We then examined how closely CFC strength is associated with onset channel activity, and whether the clinically identified onset zones in recorded seizures reflect the actual onset zones.Methods: Seizures (n=368 from 43 patients) recorded from patients with medical refractory epilepsy who underwent presur-gical evaluation with intracranial electrodes were analyzed. CFC between low (delta and theta) and high (ripples and fast ripples) frequency bands at seizure onset was measured using a generalized linear modeling (GLM) framework in MATLAB. To evaluate seizure dynamics in the onset channels, CFC was measured only in the seizure onset channel around the start of the seizure. In a subsequent analysis, a subset of seizures (n=48 from 15 patients) were selected to identify whether CFC strength has a stronger correlation with onset channels around the time of seizure initiation compared to the rest of channels. Results: We identified five different electrographic patterns in seizures arising from six different categories of cortical and subcortical regions. We found that while each region could give rise to seizures with multiple kinds of patterns, certain patterns were more likely to be associated with particular brain regions. Intriguingly, we found that changes in CFC dynamics between different bands were closely related to seizure onset region. Further, we found that in all patients with a beneficial surgery outcome (Engel =1 or 2) and only one onset focus (5/15 patients), the channels that were clinically identified as the onset channels had much greater CFC values in their signature coupling pattern suggested by the first analysis (ie. ripple or fast ripple coupling). In cases where the clinical reviewer identified more than one onset focus, the CFC analysis could only identify the correct onset zone if the onset contact was observed to be in a region with a higher CFC value in our first finding (5/15 patients). For the remaining 5 patients CFC failed to identify the onset since the focus of interest was in a region with relatively lower CFC strength. Conclusion: Through our findings, we demonstrate the importance of seizure onset region as a factor that should be consid-ered along with other clinical factors, such as electrographic pattern and pathology, in any mechanistic inference made about seizure generation. This study highlights the substantial differences between seizures arising from different regions in terms of what should be considered pathologic, and provides evidence that oscillatory networks recruited during seizure initiation can be distinct between regions and in order to employ relevant spectral information a normalization to account for these differences should be taken into account. 279 Aubrey L. Samost-Williams, MD, MS, Anesthesia, Critical Care and Pain Medicine A Novel Application of a Systems Theory-Based Prospective Safety Model to Medication Use in the Operating Room Environment A.L. Samost-Williams and K. Nanji Department of Anesthesia, Critical Care, and Pain Medicine, MGH, Boston, MA, USA Introduction: Despite significant interest and investment in patient safety, medical errors continue to plague our healthcare system. In the operating room environment, medication errors have been shown to impact 5-10% of medication adminis - trations in prior work. While much of the safety work in healthcare has involved retrospective analyses of accidents and 238reactive safety measures, there is a growing movement towards prospective analysis of our safety systems, and proactive safety measures are now being implemented. Prospective safety system studies have so far been hampered by the limitations of the engineering tools used. Commonly, analyses are done with models that make assumptions about the causal factors of accidents, such as independence of system components and linear chains of causality. The purpose of this project was to determine the safety hazards in our current perioperative medication use process, using a robust, prospective model called systems-theoretic process analysis (STPA).Methods: Two analysts built the STPA model with multidisciplinary input from OR leadership, clinical, and pharmacy subject matter experts. We built the model in four stages: defining system accidents and hazards, building a hierarchical control structure of the system, defining unsafe control actions, and creating causal scenarios for the unsafe control actions. Accidents are events that lead to a loss to the system, such as patient harm or death. Hazards are conditions set up by the system that combine with environmental conditions to lead to an accident. A hierarchical control structure is a pictorial representation of the process that models each component (person or software) as a controller who has the responsibility to constrain the behavior of the process underneath them and receives control from the controller above. For example, physicians (controller) might dictate treatment plans to nurses (controlled process and controller) who execute treatment plans on the patients (controlled process). Each controller has control actions such as writing policies or administering infusions of medications. To define unsafe control actions, we systematically asked if each control action could be unsafe under the following four conditions: if the action is taken, if the action is not taken, if the action is taken too early or late, or if the action is applied for too long or short of a duration. To create causal scenarios for each unsafe control action, we analyzed why that unsafe control action may have occurred, using input from OR leadership, clinical and pharmacy subject matter experts. For example, was there a lack of feedback from the controlled process that led to a poor decision by the controller? We incorporated feedback form our subject matter experts until we reached information saturation, the point at which we were no longer gaining new information or insights from successive consultations. Results: Hazards for this system were defined as medication errors. Accidents were defined as adverse drug events (or patient harm) secondary to medication errors. We built a hierarchical control structure to describe the medication use process (Figure 1). At the highest levels of the control structure are the leadership teams involved in perioperative medication use - surgical, pharmacy, and anesthesia leadership. Nurse-administered medications were outside the scope of this analysis. The next tier included those with managerial roles in the medication use process, but who do not directly handle or administer medications to patients. For example, some functions within the OR pharmacy, the surgical attending, and the anesthesia attending were managerial roles. In the final tier were the role groups directly involved in administering medications and/or interacting with the medications, including the OR pharmacists and the anesthetist (resident anesthesiologist, certified registered nurse anesthetist, or attending anesthesiologist). We identified 61 possible unsafe control actions and 279 causal scenarios that could lead to the unsafe control actions in the medication use process. See Table 1 for representative examples.Conclusion: Using STPA, we were able to identify almost 300 hazardous scenarios in our current medication use process, ranging from problems with the frontline providers to the policies and culture set by perioperative leadership. The ability to model and understand a wide range of system factors such as communication, the role of policies, and human factors is a strength of STPA compared to more commonly used prospective risk analysis tools in healthcare. Our model also shows that more complex safety engineering tools can be adapted to help understand problems in healthcare, leading to more compre-hensive, targeted and effective solutions. Table 1. Representative Unsafe Control Actions and Causal Scenarios Figure 1. Hierarchical control structure of the medication use process in the OR239280 Go Sato, MD, Orthopedics Foot alignment can be a risk factor of Achilles Tendinopathy? G. Sato, R. Bhimani, J. Saengsin, G. Waryasz, D. Guss and C. Digiovanni Orthopedic, foot and ankle, MGH, Brookline, MA, USA Introduction: Achilles tendinopathy is often classified into insertional and non-insertional pathologies, but the general etiology remains unclear. Purported causes include traumatic, vascular, muscular, overuse, genetic, and systemic factors, but some believe that foot malalignment may also plays a role in precipitating the pathogenesis. No consensus exists, however, regarding the predilection of a given foot malalignment to developing Achilles tendinopathy. The objective of this study was to evaluate for any association between alignment patterns in the foot and the development of Achilles tendinopathy. Methods: The clinical database of two large academic centers and one community hospital were searched for all patients presenting with Achilles tendinopathy of either the insertional or non-insertional form between 2016 to 2018. Included were patients older than 18 years and excluded were all patients who had weight bearing films of the affected ankle but did not also have weight bearing x-rays of the ipsilateral foot. A total of 288 feet were screened from our database and, of these, 100 feet were available for final analysis. 59 patients had non-insertional group Achilles tendinopathy and 41 had insertional tendinopathy. Demographic data such as age, gender, BMI, smoking status, laterality, diabetes, and patient activity profile were collected. Radiographic parameters were measured on the anterior-posterior and the lateral weight-bearing views of foot and included calcaneal inclination angle, calcaneal pitch, Hibbs angle, Meary's line, tibio talar angle, and metatarsal parabola angle (Figure1). The normal values of these angles, which served as the control group, were calculated as the mean of the normal values and their standard deviations as previously defined in the literature. Mann Whitney U test was used to assess the relationship between the insertional group, the non-insertional group, and the control group. A p-value < 0.016 was considered statistically significant for this analysis.Results: No demographic differences were noted between the insertional and the non-insertional groups in our study, although the average body mass index (BMI) was observed to be greater than 30 in both groups (Table1). Meary's line on the lateral view was significantly more planus in the non-insertional tendinopathy group compared to the control group (2.19 \u00b1 6.6 degrees vs. 6.2 \u00b1 7.8 degrees; p = 0.013). Meary's line on the AP view showed hindfoot valgus alignment in both the insertional and non-insertional Achilles tendinopathy group compared to the control group (6.99 \u00b1 4.1 degrees in the insertional group, 6.29 \u00b1 4.0 degrees in non-insertional group as compared to 3.9 \u00b1 4.7 degrees in control group; p = < 0.001 and 0.007 for insertional and non-insertional group respectively). Both the calcaneal pitch and metatarsus adductus angles were also significantly higher in the insertional tendinopathy group compared to non-insertional tendinopathy group (p = 0.043 and 0.003 respectively) (Table2). The inter-observer reliability yielded very good results in 50% of the angular measurements, and the remaining 50% of the angular measurements yielded good results.Conclusion: These data suggest that valgus malalignment and obesity may predispose patients towards developing Achilles tendinopathy. Moreover, a higher sagittal arch may be a risk factor for insertional but not non-insertional Achilles tendinop - athy. Such findings may have implications for future prediction of and treatment for these disorders. Table1. Demographic data 240281 John A. Sbarbaro, BS, Medicine - Cardiology Upright Exercise Hemodynamic Profiles Effectively Risk Stratify Patients with Heart Failure with Preserved Ejection Fraction and Elevated Resting Filling Pressures J.A. R.V. Shah, J.E. Ho, R. Malhotra and G.D. Lewis Heart Failure and Transplant, Massachusetts General Hospital, Boston, MA, USA Introduction: A resting supine pulmonary capillary wedge pressure (rsPCWP) of 15 mmHg measured by right heart catheterization is considered the hemodynamic \"gold standard\" diagnostic criteria for heart failure with preserved ejection fraction (HFpEF). However, right heart catheterization is typically performed as a one-time measurement in patients in the supine position. In this study we sought to define the incremental value of evaluating PCWP measurements in response to upright exercise in predicting outcomes among patients with elevated rsPCWP. We hypothesized that patients with elevated rsPCWP and a shallow PCWP increase relative to cardiac output (PCWP/CO< 2 mmHg/L/min) during exercise will have superior HF-event free survival compared to individuals with elevated rsPCWP and abnormally steep PCWP/CO 2mmHg/L/min. Methods: We performed upright maximum effort incremental ramp cycle ergometry cardiopulmonary exercise testing with invasive hemodynamic monitoring in patients referred to Massachusetts General Hospital for evaluation of exertional dyspnea. Direct Fick CO and PCWP were measured at rest and during each minute of exercise to derive PCWP/CO slopes and patient outcomes were assessed with Cox regression models. Results: In 182 patients with rsPCWP 15mmHg in 72 patients (40%) and 2mmHg/L/min in 110 (60%). After adjustment for age, sex, and BMI (Figure 1), PCWP/CO < 2mmHg/L/min was associated with a lower hazard for combined HF or death (hazard comparison 2mmHg/L/min. who meet resting hemodynamic criteria for HFpEF 40% had normal upright PCWP/CO slope of < 2 mmHg/L/min with relatively favorable outcomes in comparison to patients with exercise PCWP/CO slope >2mmHg/L/min. These findings highlight the potential incremental value of assessing upright exercise hemodynamic measurements in patients to complement resting hemodynamic evaluation of suspected HFpEF. 282 Mark Schoenike, B.S., Cardiology Systemic Vascular Distensibility Is An Important Determinant Of Exercise Performance In Heart Failure With Preserved Ejection Fraction M. Schoenike, J. Sbarbaro, R. Farrell, K.M. Hardin, T. Cunningham, J. Ho, G. Lewis and R. Malhotra Heart Failure and Transplantation, Massachusetts General Hospital, Somerville, MA, USA Introduction: During exercise, the systemic vasculature must vasodilate to accommodate increased cardiac output and delivery of oxygen to the peripheral tissues. Systemic vascular distensibility (SVD) represents the ability of systemic arteri - oles to vasodilate for a given increase in pressure. However, the effects of SVD on exercise performance in patients with heart failure with preserved ejection fraction (HFpEF) are unknown. We hypothesized that abnormal SVD is an important determinant of exercise performance in HFpEF.241Methods: Patients with HFpEF (n= LVEF 50%, and 15 PCWP/Cardiac output slope 2.0 mmHg/L/min) and controls (n= 119, peak > LVEF 50%, supine PCWP <15mmHg, and PCWP/CO slope < 2.0) underwent cardiopulmonary exercise testing with invasive hemodynamic monitoring. A distensible vessel model was used to calculate the systemic vascular pressure-flow relationship, using measures of mean arterial pressure, right atrial pressure, and cardiac output at rest and multiple points during exercise. Utilizing an iterative approach with least-squares methodology we determined SVD.Results: SVD was lower in the HFpEF group (Figure, 0.27\u00b1 0.12% per mmHg, mean, SD) than in the control group (0.32\u00b10.12% per mmHg, p<0.0001). SVD was associated with cardiac output at peak exercise as well as augmentation of cardiac output both in the control population (=0.22, p=0.019) and HFpEF population (=0.15 p=0.009). Higher SVD was associated with improved ventilatory efficiency measured by VE/VCO2 slope in =-0.18, p=0.0017). vascular distensibility is lower in patients with HFpEF compared to controls and is a predictor of cardiac output augmentation and ventilatory efficiency in patients with HFpEF. We conclude that SVD is an important determinant of exercise performance in HFpEF. 242283 Anna Schwartzberg, BA, Psychiatry Interpretation biases and threat reactivity in body dysmorphic disorder: A virtual reality study A. Schwartzberg, B. Summers and S. Wilhelm Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: It has been shown that individuals with body dysmorphic disorder (BDD) interpret ambiguous situations in a threatening or negative manner, misperceiving everyday social information (e.g., benign comments, neutral facial expres- sions) as judgment, mocking, or rejection (Buhlmann et al., 2002; Clerkin & Teachman, 2008). These maladaptive interpreta-tion biases are thought to maintain BDD symptoms and perpetuate disorder sequelae. Currently, these problematic cognitive patterns are largely measured via retrospective accounts or asking patients to imagine hypothetical situations; methodology with inherent limitations. The current study sought to elicit and measure appearance-related biases in individuals with BDD using virtual reality (VR) technology to better understand momentary thought and physiological processes and inform novel intervention strategies. VR has been used in the investigation and treatment of many psychiatric conditions, including anxiety, schizophrenia, substance-use problems, eating disorders, and specific phobias (Freeman et al., 2017; Parsons & Rizzo, 2008). However, this technology has not yet been harnessed to understand BDD. Since VR presents an immersive, 360\u00b0 environment that simulates reality, it represents an ecologically valid means of provoking and measuring natural reactivity and allows for the investigation of fear and threat structures.Methods: Two groups of participants were recruited: individuals with BDD (N=16) and individuals without any psychiatric conditions (healthy controls; N=15). All participants completed a clinical interview, established interpretation bias question- naires (i.e., the Word-Sentence Association Paradigm (WSAP; Summers & Cougle, 2018) and the Interpretations Question-naire (IQ; Buhlmann et al., 2002)), and the VR paradigm in which participants viewed 13 brief scenes that depicted everyday ambiguous social encounters (e.g., others looking at the participant, having one's picture taken, engaging with coworkers at a meeting). We hypothesized that individuals with BDD would endorse significantly greater negative/threat interpretation biases and lower benign biases relative to healthy controls, both in their responses to questionnaires and their responses to the in vivo VR stimuli. We also anticipated that the BDD participants would endorse greater threat, distress, urges to avoid the situation, and urges to check their appearance in response to the VR stimuli. Results: Consistent with our hypotheses, analyses revealed that individuals with BDD significantly endorsed more negative interpretations of ambiguous scenarios than healthy controls on all three subscales of the IQ, including appearance (F = 65.51, p < .001, p2 = .70), social (F = 44.52, p < .001, p2 = .61) and general (F = 47.99, p < .001, p2 = .63) biases. While healthy controls endorsed more benign interpretations of appearance-related situations on the WSAP (F = 29.21 p < .001, p2 = .50), individuals with BDD endorsed more negative interpretations of these situations (F = 27.37, p < .001, p2 = .49). With regard to in vivo biases in response to stimuli presented via VR, individuals with BDD endorsed more negative, appearance-focused interpretations (F = 26.27, p < .001, p2 = .48), and healthy controls significantly endorsed more benign/healthier interpreta - tions (F = 4.56, p = .04, p2 = .14). Individuals with BDD exhibited greater reactivity ratings in response to the VR scenes, including threat perceived (F = 16.13, p < .001, p2 = .36), distress experienced (F = 18.37, p < .001, p2 = .39), urge to avoid (F = 22.08, p < .001, p2 = .43), and urge to check their appearance (F = 32.24, p < .001, p2 = .53). Conclusion: These results corroborate previous research suggesting that individuals diagnosed with BDD are more likely to interpret ambiguous information in a negative or threatening manner. This study extends the literature by demonstrating that these characteristic interpretations can be elicited in vivo via VR technology, as evidenced by group comparisons and parallel patterns of endorsement between VR ratings and established dispositional measures (i.e., WSAP and IQ bias questionnaires). The findings of this initial pilot study are promising and suggest that VR may have utility in treatment as a means of eliciting and potentially facilitating in vivo remediation of characteristic maladaptive cognitions thought to maintain this debilitating illness. Individuals with BDD endorsed more negative, appearance-focused interpretations Individuals with BDD exhibited greater reactivity ratings in response to ambiguous situations243284 Naomi M. Sell, MD, MHS, Surgery Variable Life Adjusted Display (VLAD): A Metric for Quantification of Patient Lives Gained Following Pancreatic Surgery N.M. Sell1, M. Massachusetts General Hospital, Boston, MA, USA and 2Vizient Inc., Chicago, IL, USA Introduction: Variable Life Adjusted Display (VLAD) is a metric that examines lives gained, based on predicted mortality. If patients survive, their risk-adjusted mortality is equivalent to the proportion of a life gained. The model utilizes the cumulative sum of risk taken to calculate lives accrued over time, and thereby rewards risk. Methods: The Vizient \u00ae risk-adjusted database was used to calculate VLAD for patients undergoing pancreatectomy at 284 hospitals in the United States between FY2016-2018. Hospitals that performed fewer than 5 pancreatectomies per year were excluded. Pancreatectomies from the remaining 169 hospitals were included in this study. Results: A total of 39,289 pancreatectomies were performed within the study period. VLAD illustrates substantial variability of accrued lives gained or lost at each hospital (1a). Excellent performers were hospitals that operated on high-risk patients who survived, and thus accumulated lives gained. Conversely, poorer performing hospitals experienced worse outcomes, including with low-risk patients. Graph (1b) depicts the upper, middle, and lower third curves of hospital VLAD performance. In examining VLAD stratified by hospital volume over time using a cutoff of 20 (\"The Pledge\"), high-volume hospitals demonstrated substantial accumulation of lives gained, compared with low-volume hospitals (1c). Similarly, teaching hospitals demonstrated an enhanced ability to save lives when compared with non-teaching hospitals (1d).Conclusion: VLAD is a metric suitable for use in pancreatic surgery and has the potential to quantify aggregate lives gained based on risk-adjusted patient mortality among hospitals and surgeons. This metric demonstrates substantial variability in performance following pancreatic surgery in the United States. Figure 1. 285 Jeehye Seo, Ph.D., Psychiatry Relationships of sleep with cortical thickness among trauma exposed individuals J. Seo1,2, K.I. Oliver1,2, C. Daffre1,2 and E.F. Pace-Schott1,2 1Psychiatry, Massachusetts General Hospital and Harvard Medical School, Chestnut Hill, MA, USA and 2Athinoula A. Martinos Center for Biomedical Imaging, Charlestown, MA, USA Introduction: Sleep difficulties may contribute to the functional abnormalities of neural circuitry that underlie the develop - ment and persistence of posttraumatic stress disorder (PTSD) patients. PTSD patients also demonstrate abnormalities in attentional control, threat sensitivity, episodic memory retrieval, and fear extinction - functions subserved by forebrain cirtuits involving the amygdala, insula, prefrontal cortex and hippocampus. However, no studies to date have investigated differences in the cortical thickness of fear-related networks between trauma-exposed controls and individuals with PTSD, nor the associ-ation between cortical thickness and sleep difficulties. We examined associations between sleep physiology, hyperarousal symptoms and regional cortical thickness in trauma-exposed individuals experiencing a wide range of PTSD symptoms.244Methods: Persons exposed to trauma within the past 2 years (N=77) completed hyperarousal measures derived from the Clinician Administered PTSD scale and PTSD Checklist-5, as well as 2 weeks of actigraphy and sleep diaries, an acclimation and baseline night of ambulatory polysomnography (PSG), and a 3T structural MRI scan. Slow wave sleep (SWS)% and REM% were computed from PSG and mean sleep efficiency (SE) was calculated from 2 weeks of actigraphy. We obtained high-resolution 3D T1 whole brain images using 3T whole body MRI system imaging device with 32-channel head coil. Cortical treconstruction and parcellation were performed with FreeSurfer V6 and then cortical thickness was calculated as the distance between the gray/white matter boundary and the pial surface at each point on the cortical mantle. Among all subjects, correlation maps were generated between cortical thickness and hyperarousal and sleep measures. A Monte Carlo simulation with 10,000 repetitions ensured a family-wise error of <0.05.Results: Self-reported hyperarousal score was negatively correlated with cortical thickness in the left paracentral/precuneus and positively correlated with cortical thickness in the left insular cortex. Moreover, sleep efficiency was positively correlated with cortical thickness in the isthmus-cingulate/precuneus, medial orbitofrontal cortex and right middle temporal gyrus. Additionally, REM % was positively correlated with posterior cingulate while SWS % negatively correlated with corticla thickness posterior cortex and right rostral middle frontal cortex.Conclusion: Greater hyperarousal - a prominent symptom of PTSD - was associated with lesser thickness in parietal portions of the default mode network (DMN). In contrast, greater sleep quality (SE) was associated with greater thickness in frontal and parietal portions of the DMN. SE is often reduced in PTSD. Thus, better sleep may protect, but hyperarousal may degrade thickness and perhaps function of the DMN in trauma-exposed individuals. 286 Isabella Sereno, M.Ed, Psychiatry Association Between Physical Functioning, Coping, & Quality of Life in Patients with Chronic Graft-versus-host Disease I. Sereno1,2,3, J.M. Massachusetts General Hospital, Chelsea, MA, USA, 2Cancer Center, Massachusetts General Hospital, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: cGVHD is a major complication of allogeneic stem cell transplantation and is the leading cause of non-relapse mortality in transplant survivors. Allogeneic stem cell transplant survivors with cGVHD experience significant psycholog-ical distress and substantial impairments in their QOL. However, the relationship between patients' physical functioning, symptom burden, coping strategies, and QOL over time is currently unknown. Methods: We conducted a longitudinal study of patients with moderate to severe cGVHD recruited from a single institution. We assessed patient-reported psychological distress (Hospital Anxiety and Depression Scale), QOL (Functional Assessment of Cancer Therapy- General), physical functioning (Human Activity Profile), cGVHD symptom burden (Lee Symptom Scale), and coping (Coping Inventory for Stressful Situations) at baseline, 3 months, and 6 months. Using mixed linear effects models, we longitudinally examined the relationship between patients' QOL and their physical functioning, cGVHD symptoms, and coping strategies.Results: We enrolled 53 patients with moderate (71.7%, 38/53) or severe (28.3%, 15/53) cGVHD. The rate of clinically significant depression and anxiety symptoms at baseline was 32.1% (17/53) and 30.2% (16/33), respectively. Depression and anxiety symptoms did not change substantially over time. Patients reported impaired QOL at baseline [FACT-G: mean=70.33, SD=18.96] which did not change significantly over time [ =-0.66, SE=1.11, P=0.550]. Higher physical functioning was associated with better QOL [ =0.17, SE=0.05, P=0.001] over time, while higher symptom burden was associated with worse QOL [=-0.38, SE=0.06, P=0.001] over time. The use of emotion-oriented coping was associated with lower QOL over time [=-0.70, SE=0.14, P<0.001].]. In contrast, the use of avoidance-oriented coping was associated with higher QOL [=0.38, SE=0.10, P<0.001] over time. Task-oriented coping was not significantly associated with patients' psychological distress or QOL.Conclusion: Patients with moderate or severe cGVHD report substantial psychological distress and persistently impaired QOL over time. Higher physical function and lower symptom burden are associated with improvement in patients' QOL over time. The use of certain coping strategies, such as avoidance-oriented coping strategies, was associated with improvement in QOL, whereas other coping strategies, such as emotion-oriented, was associated with worsening QOL over time. These data underscore the need for supportive care interventions that incorporate screening for psychological distress and adapting cognitive based therapy to promote effective coping strategies and enhance physical functioning in patients with cGVHD.245287 Alberto Serrano-Pozo, Neurology Characterization of the 18 kDa Translocator Protein (TSPO) Expression in Post-Mortem Normal and General Hospital, Boston, MA, USA, 2Massachusetts Alzheimer's Disease Research Center, Boston, MA, USA, 3Harvard Medical School, Boston, MA, USA, 4Sir Run Run Shaw Hospital of Zhejiang University, Zhejiang, China and 5MGH BioMedical Informatics Core, Cambridge, MA, USA Introduction: The 18 kDa translocator protein (TSPO) is a widely used target for microglial PET imaging radioligands, but its expression in post-mortem normal and diseased human brain is not well described. We aimed at characterizing the TSPO expression in human control (CTRL) and Alzheimer's disease (AD) brains. Specifically, we sought to: (1) define the cell type(s) expressing TSPO; (2) compare tspo mRNA and AD CTRL brains; TSPO levels with reactive glia and AD neuropathological changes; and (4) investigate the SNP on tspo mRNA We performed quantitative immunohistochemistry on post-mortem formalin-fixed paraffin-embedded sections from the temporal neocortex (Brodmann Area 38) and Western blot on frozen brain samples from temporal, frontal and cerebellar cortex of CTRL and AD subjects. We also analyzed publicly available RNA-seq datasets obtained from cell sub-populations isolated from mouse and human brains.Results: We found that: (1) TSPO is expressed not just in microglia, but also in astrocytes, endothelial cells and vascular smooth muscle cells (Figure); (2) there is substantial overlap of tspo mRNA and TSPO levels between AD and CTRL subjects and in TSPO levels between temporal neocortex and white matter in both groups; (3) TSPO cortical burden does not correlate with the burden of activated microglia or reactive astrocytes, A plaques or not significantly tspo mRNA or TSPO levels, the magnitude of glial responses, the cortical thickness, or the burden of AD neuropathological changes.Conclusion: In summary, we show that TSPO is not only expressed in microglia but also in astrocytes, endothelial and vascular smooth muscle cells, and that there is a substantial overlap of tspo mRNA and TSPO levels between CTRL and AD subjects, no correlation between TSPO levels and glial responses or AD neuropathological changes, and no impact of the rs6971 TSPO SNP on tspo mRNA or or AD neuropathological changes. Our post-mortem findings cast some doubts on the usefulness of TSPO-based radioligands for in vivo brain PET imaging of activated microglia. More research is needed to understand the cell type-specific role of TSPO in AD and other brain diseases, and to develop reactive glia-specific PET radioligands. Confocal endothelial cells (empty arrowheads) and astrocytes (solid arrowheads) (M-O) (green). Nuclei are stained with DAPI (blue). Abbreviations: CD68 = cluster differentiation 68; IBA1 = molecule 1; protein; VIM = vimentin.246288 Sara Shahbazi, Medicine - General Internal Medicine Differences in the Content of Office Visits by Physician Specialty J.D. Goodson1,2, S. Shahbazi1, R.D. Karthik1 and Hospital, Boston, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Office visits are the most common service delivered in the U.S. health care system. The existing evaluation and management (E/M) service codes were developed from the wide range of specialty-specific service codes used before the Medicare Physician Fee Schedule (MPFS). The collapse of all the outpatient codes for new and established patients to a total of ten\u2014five for new patients and five for established patients\u2014required all specialties to choose from a narrow set of options for billing purposes. This set, now almost 30 years old, may not adequately capture the nuances and topology of cognitive work\u2014the critical thinking involved in data gathering and analysis, planning, management, decision making, and judgement in ambiguous or uncertain situations. When patients present for office visits, the physician medical decision making \u2014the cognitive work\u2014is more complex than it was 30 years ago. If the E/M definitions are not updated to reflect the increasing complexity of cognitive work for the most intense levels of E/M care, payment distortions may become exacerbated, creating income disparities among specialties and leading to geographic areas with inadequate access to lower-paid specialties. The objective of this study is to determine if there are differences in the content of the services provided by specialties whose income is highly dependent on E&M services to physicians more dependent on procedures. Methods: To compare complexity of visits seen by different specialties over similar time intervals, we studied data from the 2013-2016 National Ambulatory Medical Care Survey (NAMCS). We included only visits to physicians who specialized in General and Family Practice (GP/FM), Internal Medicine (IM), Neurology, Dermatology, Ophthalmology, and Orthopedic Surgery. In addition, we restricted our analysis to only the established outpatient visits to compare the complexity of continuity care of different provider groups, leaving a final analytical sample of 53,670 established office visits to the above special - ists between 2013 and 2016. We categorized time spent with the physician based on the typical times defined by Current Procedural Terminology (CPT) guidelines for E/M codes as and >40 minutes (99215). We considered two measures to represent visit complexity: the number of diagnoses listed and number of medications prescribed. In our analysis, complex visits are defined as those with over two diagnoses (the mean diagnoses per visit) and/or those with over three prescription medications (the mean medica - tions per visit). Uaing 2015 Medicare data, we identifed specilaties whose income are highly dependent on E&M services (GP/FP, IM, and Neurology) versus specialties who are more dependent on procedures (Dermatology, Ophthalmology, and Orthopedic Surgery). The distributions of the complex established patient's office visits by time spent with the physician and physician specialty were analyzed. All analyses were performed using Stata version 15.1. All data were publicly available, thus exempt from review by the Institutional Review Board. Results: The Figure illustrates the differences in the percentages of high complexity visits for each time category by specialty. For example, for those visits where the time would correspond to a level 4 (99214) established outpatient, the percentage that were high visit diagnosis complexity were as data raise important issues that deserve closer attention. If the MPFS is to provide accurate and reliable pricing for all physician services, a public and nationally representative process to assess the intensity and complexity of the services provided is needed. This may be more straightforward for procedures since the service\u2014a completed procedure\u2014 can be assigned a time based on surgical logs averaged and adjusted for confounders such as patient acuity and concurrent conditions. For the cognitively-oriented E/M services, our study suggests that within specific time intervals, there may be wide differences in intensity and complexity that should be captured in the pricing and service definition processes. For the workforce to meet national needs for primary care, specialty care, and procedural care, any inherent biases in the fee schedule that would under-compensate one type of service with respect to another could lead to system-wide workforce deficiencies. There should be a solid knowledge-base to ensure accuracy, especially around the outpatient E/M service codes. These codes capture the largest category of Medicare payment for professional services, thus, the pricing of physician services ought to be derived from a regularly updated, valid, and representative evidence-based process.247 289 Alec P. Shannon, Bachelor of Science, Psychiatry Quetiapine for treatment of bipolar disorder and increased risk of Type 2 Diabetes and Cardiovascular Disease A.P. Shannon1, USA and 2Psychiatry, Aarhus University Hospital, Aarhus, Denmark Introduction: Patients with bipolar disorder have substantially higher rates of mortality and decreased life expectancy compared to the general population. Excess mortality in this patient population is largely attributed to the increased risk of developing cardiovascular disease (CVD). While clinical guidelines emphasize the importance of monitoring patients at an increased risk of CVD, few studies have characterized the cardiometabolic risk profile of people with bipolar disorder throughout 6-months outpatient treatment.Methods: We analyzed data from the Clinical and Health Outcomes Initiatives in Comparative Effectiveness for Bipolar Disorder Study (CHOICE) (Nierenberg et al., 2014) and the Lithium Treatment Moderate-Dose Use Study (LiTMUS) (Nierenberg et al., 2009). These were two 6-month multi-site, randomized, comparative effectiveness trials with a combined total of 765 participants, comparing add-on lithium to add-on quetiapine in CHOICE, and add-on moderate doses lithium to optimized treatment in LiTMUS. Other treatments needed for mood stabilization were administered in a guideline-informed, empirically supported, and personalized fashion to participants in all treatment arms. These studies assessed cardiometa- bolic risk markers including fasting glucose, total cholesterol, HDL and LDL and triglycerides. In addition, we calculated long-term CVD risk for each participant using the following risk assessment tools: the 10-year and 30-year Framingham Risk Scores (FRS) and the Metabolic Syndrome Severity Z-score (cMets Z-score), which is associated with long-term risk for type 2 diabetes mellitus and CVD. We analyzed the development of cardiometabolic markers over 6 months of treatment and compared these risk markers across the four treatment arms. Results: The cMets Z-score was the only outcome variable significantly predicted by treatment arm: randomization to treatment with quetiapine was a significant positive predictor ( = .384, t = 2.1, p = .036), while the other treatment arms were not (s < .163, n.s). Our analysis controlled for additional significant predictors of cMets Z-score, including age, cigarette smoking, diagnosis of diabetes and being treated for hypertension. Conclusion: Quetiapine was associated with an increase in the risk of type 2 diabetes mellitus and CVD that emerged over 6 months of treatment.248290 Krishan K. Sharma, MD, Health Professionals Education Research Assessing the real and perceived impact of pairing medical students on clerkship teams: A mixed methods study K.K. Sharma1, Y. Chang1 and E.M. Miloslavsky2 1Medicine, Massachusetts General Hospital, Boston, MA, USA and 2Division of Rheumatology, Allergy and Immunology, Massachusetts General Hospital, Boston, MA, USA Introduction: Medical students are often paired together on clinical teams during their clerkships, but this practice has not been previously investigated. The primary objectives of this study were (1) to retrospectively assess whether pairing students on medical teams during their sub-internship affected their grade, (2) to survey medical students' attitudes and preferences towards pairing, and (3) to understand which factors of the learning environment may alter student attitudes and perceptions towards the practice of pairing.Methods: Utilizing an explanatory sequential mixed methods design, we first analyzed 186 student pairings within the medicine sub-internship at 3 hospital sites of Harvard Medical School (HMS) from 2013-2017. Employing contingency table analysis, we examined categorical grading data comparing expected and observed grading distributions, controlling for performance from the third year internal medicine clerkship. We also developed and administered a survey assessing student attitudes and perceptions of pairing to the 2018 graduating class of HMS. In addition, we conducted 17 semi-structured interviews of HMS students. Three investigators analyzed the transcripts using a structured qualitative framework approach, which was informed by literature on sociocultural theory and the clinical learning environment. Results: Among 186 student-pairs, there was no deviation between the expected and observed distribution of student grades, even when controlling for prior internal medicine clerkship performance (p=0.28), suggesting that pairing had no effect on the sub-internship grade. Ninety-nine students responded to the survey (59% response rate). Ninety percent and 87% of respondents felt that being paired affected their evaluations by resident and attending physicians, respectively. Despite this, 49% of students reported that being paired had increased their overall learning, compared to 32% reporting no change, and 19% reporting decreased learning. Students independently reported varying degrees of competition and collaboration with their partner, with majority describing their relationship as moderately collaborative (49%), and simultaneously slightly competitive (48%). Using the qualitative analysis, we then developed a model of the paired students' learning environment. Key domains in the model included student characteristics, student-partner dynamics, perceptions and expectations, student evaluation, and clinical team factors.Conclusion: Our analysis suggests that pairing medical students together on clerkships may not affect student grades, despite the majority of surveyed students believing that pairing affects their evaluations. The inter-student dynamic is complex and has both a positive and negative impact on the learning environment. Multiple factors can influence this system, some of which may be amenable to intervention. Awareness of student perceptions regarding pairing can inform clerkship structure and be utilized to address student concerns. Interventions aimed at improving the pairing dynamic between students may hold promise for an enhanced clerkship experience. 291 Leah Shaw, MPH, Emergency Perceptions of Chronic Kidney Disease in an indigenous, rural population in Guatemala L. Shaw1, S. Kurschner1, M. Nandi6,1, Y. Research Center, Wuqu' Kawoq | Maya Health Alliance, Hampstead, NH, USA, 2Weill Cornell Medical College, New York City, NY, USA, 3University of Michigan Medical School, University of Michigan, Ann Arbor, MI, USA, 4Harvard Medical School, Boston, MA, USA, 5Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 6Warren Alpert Medical School of Brown University, Providence, RI, USA and 7Departments of Global Health Equities and General Internal Medicine, Brigham and Women's Hospital, Boston, MA, USA Introduction: Chronic kidney disease (CKD) affects over 10% of the population worldwide, and prevalence is increasing in low and middle-income countries. CKD is a leading cause of death and disability in Guatemala, where the majority indige - nous population is excluded from diagnosis and treatment. Little is known about how traditional risk factors of diabetes and non-traditional risk factors including tropical agricultural work are associated with CKD prevalence in indigenous Guatemalan communities. Furthermore, experiences of indigenous Guatemalans with CKD have not been studied to date.Methods: We present findings from one portion of a mixed-methods study examining incidence, risk factors, and understand-ings of CKD in two rural indigenous Guatemalan populations with different exposures to diabetes and agricultural risk factors. This project is a collaboration between Guatemalan and US institutions building local chronic disease research 249capacity. The quantitative portion of this project is a retrospective case-control study used to identify incidence and risk factors for CKD. Through random sampling and household visits, 270 participants have been screened for CKD through lab work and evaluated for risk factors through structured surveys. Survey data are collected in RedCap and analyzed with STATA 14 to determine risk factors associated with CKD in our target population. The qualitative portion of this project, reported here, is based on semi-structured interviews with thirty-nine participants with abnormal results (GFR < 90, following KDIGO criteria for population diagnosis of CKD). Interviews focused on reactions to abnormal test results as well as percep - tions of CKD and its causes. Additionally, participants were given explanations of major treatment modalities for advanced CKD (hemodialysis, peritoneal dialysis, and renal transplant), and were asked about general perceptions of these treatment options. Qualitative data were coded for dominant themes using Microsoft Excel.Results: Participants reacted to abnormal test results in three major ways. One quarter of participants reacted with neutrality, as they still felt well; one quarter rationalized abnormal test results based on previous and/or ongoing symptoms or health problems that they now attributed to kidney disease; one fifth expressed fright or worry. When asked about causality, two thirds of participants attributed CKD to nutrition and diet, but otherwise had limited knowledge of diabetes and hypertension as risk factors. When given explanations of different treatment modalities for CKD, three quarters of participants expressed willingness to pursue at least one treatment type if necessary and financially possible. However, participants highlighted multiple barriers including transportation to centralized hemodialysis units, the lack of a clean space at home for peritoneal dialysis, and the financial burden of purchasing medication for life following a transplant. Conclusion: Individuals with abnormal test results expressed uncertainty about CKD causality and demonstrated limited awareness of diabetes and hypertension as risk factors for CKD. The majority of participants expressed willingness to pursue treatment for advanced CKD, but expressed apprehension about financial barriers. Limitations in this research include lower recruitment of male participants. Strengths of the study include development of a robust infrastructure to conduct research in locally-spoken Mayan languages as well as multi-institution collaboration. Table 1. Demographics Table 2. Perceived causality of CKD 292 Hannah Shields, BS, Psychiatry Sex Differences in Strategies for Maintaining Intact Cognitive Aging in Early Midlife H. Shields3, K. Konishi3, H. Aizley3, A. Remington3 and J.M. Goldstein3,1,2 1Department of Medicine, Harvard Medical School, Boston, MA, USA, 2Department of Obstetrics and Gynecology, Massachusetts General Hospital, Boston, MA, USA and 3Department of Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: By 2050, approximately 13.8 million people in the US are projected to have Alzheimer's disease, two-thirds of whom will be women. Processes involved in vulnerability for age-related memory disorders begin in early midlife, providing an opportunity to identify early the differential impact of sex on memory decline. We know that men and women use different strategies for memory performance beginning just post-puberty through aging. In general, men tend to use serial strategies, while women tend to use verbal semantic strategies. In previous studies, we and others have shown that, in women, reproduc-tive age in early midlife has a stronger impact on verbal memory decline than chronologic age. In this study, we aim to test whether there are sex differences in strategies used to maintain intact memory performance, and the impact of reproductive vs. chronologic age in women and chronologic age alone in men on performance decline. Methods: 212 early midlife adults (ages 45-55; 106M:106F) were recruited from the New England Family Study cohort. Within a larger battery of neuropsychological tests, Digit Span Forwards [DSF] and Backwards [DSB] and Verbal Fluency FAS [FAS] and Categories [CAT] were administered to assess attention, verbal fluency, and aspects of working memory that implicated serial, phonetic, and semantic processing strategies. We used STRAW-10 criteria, reproductive history and serologic evaluation (FSH, LH, estradiol, progesterone) to determine women's reproductive stage (pre- or perimeno - pause [pre/peri] and postmenopause [post]). A 5-year follow-up interview was conducted over the phone in a subset of these adults (n=148, 73M:75F) during which updated health information including reproductive stage was collected and a 250neuropsychological battery that included DSF, DSB, FAS, and CAT. Sex and reproductive stage differences were analyzed using univariate and repeated measures ANOVAs, controlling for age and number of follow-up years.Results: We found that men performed significantly better than women using serial processing (DSF: F=6.61, p=0.02), a sex difference that attenuated 5-years later (F=0.79, p>0.05). In contrast, women performed significantly better than men on semantic processing (CAT: F=7.70, p=0.01), an effect that attenuated over reproductive aging, controlling for chronologic age. Only pre- and perimenopausal women showed a significant advantage vs. men on semantic processing (CAT: F=10.05, p<0.001), an advantage that was no longer present in postmenopausal women vs. men (F=0.73, p>0.05). Both postmeno- pausal women and men declined in phonetic ability over of chronologic aging (FAS, Post: p=0.05; Men: p<0.001). Conclusion: In this study, we found that men showed an advantage in serial processing that attenuated over chronologic age, whereas women showed better semantic processing abilities that declined over reproductive age. Postmenopausal women and men showed similar declines in phonetic processing that were associated with chronologic aging. Findings demonstrated that chronologic age is more highly related to memory decline in men in early midlife, whereas reproductive age plays a larger role until postmenopause in women. These results contribute to our understanding of how reproductive and chronologic age affect cognition differentially in men and women in early midlife. Verbal tasks are more frequently used to diagnose cognitive impairment with age than serial tasks, and thus, sex-dependent advantages may delay detection in women or lead to over-diagnosis in men. Overall, understanding how men and women use different strategies may help inform early detection and therapeutic strategies to maintain intact memory function with aging. 293 Jessie Signorelli, PharmD, Pharmacy Incidence of Invasive Fungal Infections in Acute Myeloid Leukemia with No Antifungal Prophylaxis J. Signorelli1, Lam1, M. Jalbut2 and A. Brunner3 1Pharmacy, Massachusetts General Hospital, Chelsea, MA, USA, 2Medicine, Massachusetts General Hospital, Boston, MA, USA and 3Hematology, Massachusetts General Hospital, Boston, MA, USA Introduction: Patients with acute myeloid leukemia (AML) are at high risk of invasive fungal infections (IFI), which may occur in up to 24% of patients and are a significant cause of morbidity and mortality. Antifungal prophylaxis during AML induction is recommended based on the risk of IFI, with national guidelines recommending posaconazole prophylaxis in areas where rates of IFI are >6%, and fluconazole prophylaxis in areas with > 6-10% risk of invasive candidal infections. Risks of azole prophylaxis are felt to outweigh benefits in areas where the risk of IFI is lower. We sought to characterize rates of IFI and the usage patterns of fluconazole prophylaxis to determine whether broader spectrum antifungal, fluconazole, or no prophylaxis is warranted.Methods: We retrospectively identified patients with AML 18 years old undergoing induction chemotherapy at Massachu- setts General Hospital (MGH) during 2 periods: fluconazole prophylaxis (FP) August 1, 2013-September 30, 2015; and period 2: no prophylaxis (NP) October 1, 2015-December 31, 2017. The primary outcome was a composite of proven or probable IFI, and secondary outcomes included possible IFI, types of IFI, and overall survival (OS) at 12 weeks from diagnosis. IFI was defined by European Organization for Research and Treatment of Cancer/Mycoses Study Group Consensus (EORTC/MSG). Categorical data was analyzed using chi-square or Fisher's exact test. Continuous variables were analyzed with Mann-Whitney U test. Survival was estimated using Kaplan-Meier method, while comparison between groups were analyzed using the log-rank test.Results: A total of 152 patients were screened with 149 included. In the FP versus NP groups results were as follows: proven or probable IFI in 4/88 FP patients (5%) versus 12/61 NP patients (20%) was 3/88 4/61 versus 8/61 (13%) (P<0.01). Probable IFI were driven by a positive galactomannan in all cases. Fungemia, possible IFI, and empiric antifungal use were not significantly different between FP and NP patients. There was too low of an incidence of IFI to detect resistance patterns. OS at 3 months from diagnosis was significantly longer in the fluconazole group (hazard ratio [HR], 0.26; 95% confidence interval [CI], 0.09-0.72; P=0.01). Conclusion: Observed rates of proven or probable IFI were lower in the FP versus NP group. OS at 3 months was higher in the FP versus NP group. Per national guidelines the rates of IFI observed warrant antifungal prophylaxis. Although fluconazole was used, many cases were galactomannan positive suggesting invasive aspergillus; thus, even in areas where presumed aspergillus IFI is lower than 6%, our study suggests fluconazole prophylaxis may nonetheless be associated with increased OS.251294 Arabella L. Simpkin, MD, MMSc, Health Professionals Education Research Assessment of job satisfaction and feeling valued in academic medicine A.L. Simpkin1,2, Y. Chang1,2, L. Yu1, E. Campbell3, K. Armstrong1,2 and R. Walensky1,2 1Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3University of Colorado, Denver, CO, USA Introduction: Mounting evidence suggests that faculty in medicine are increasingly unhappy, dissatisfied, and burnt-out. Although purported to be a national crisis, the actual understanding of the origins, consequences, and effective approaches to prevent and treat burnout remains limited. Academic medical centers have a tripartite mission to provide high quality clinical care, to advance knowledge through research, and to train the next generation of healthcare providers, each in the context of increased financial pressures and administrative burdens. Comprehending what affects satisfaction at work in academic health care centers is an important step towards addressing faculty burnout.Methods: We conducted an online cross-sectional confidential survey in June 2016 of all 988 faculty members holding full-time appointments in the Department of Medicine (DOM) at the Massachusetts General Hospital (MGH) to examine the culture of workplace respect, collegiality, satisfaction, and mentoring. The primary outcome was faculty member's overall professional satisfaction (strongly satisfied), and the secondary outcome was that member's reported sense of feeling valued (strongly agree). Using multivariable logistic regression (SAS Institute, Cary, NC), we examined the associations of these outcomes with demographic information; personal and professional characteristics; and perceptions of leadership, diversity, collegiality and collaboration, together with other domains relating to mentoring reported elsewhere. The final models included only those with a bivariate p<0.1 from chi-squared tests.Results: 553 faculty (56%) responded; respondents were similar in gender and rank to the universe of DOM faculty. Multivariable analyses identified the following factors that were significantly associated with job satisfaction: feeling valued (OR 4.73, 95% CI 2.35-9.51; p<0.001), feeling treated with respect (OR 3.45, 95% CI 2.07-5.75; p<0.001); and working in a social and supportive environment (OR 1.80, 95% CI 1.05-3.09; p=0.03). Gender, race, and rank were not significantly associated with satisfaction after controlling for other factors. The following variables were significantly associated with outcome of feeling valued: feeling cared about as a person (OR 28.0, 95% CI 15.3-51.3; p<0.001); not feeling taken for granted (OR 4.52, 95% CI 2.28-8.97; p<0.001); feeling resources were provided for his/her professional growth (OR 2.38, 95% CI 1.16-4.89; p=0.02) and not feeling discriminated against by gender (OR 2.29, 95% CI 1.02-5.16; p=0.046). Race, rank and feeling fairly compensated were not independently associated with feeling valued. See Table.Conclusion: At a time when concern about faculty wellbeing is high, with much speculation about causes of burnout, we find that investment in social capital and sense of value and respect for employees may be most critical. A challenge of the fast-paced, technology-driven environment that is rapidly growing around us, is the imperative to stay connected personally - not electronically - and reduce isolation. Intentional efforts to establish and nurture social and supportive environments - modifiable factors for all organizations that require relational, rather than financial, investment - will be ever-more critical in the years ahead. Importantly, we found no association with job satisfaction or feeling valued with gender, rank, or feeling compensated fairly, highlighting that financial incentives may not be effective alone in boosting satisfaction in the workplace. Limitations of this study include its modest size and single site, though there is no reason to postulate that similar predictors of satisfaction would be unique to medicine or this academic health center. Our findings call attention to the importance of promoting a sense of value and respect in the work environment for academic faculty, identifying and eliminating sources of discrimination, and expanding endeavors to facilitate collegiality. As we think about the urgent call to alleviate burnout, efforts focused in the domain of social capital seem vital.252 Multivariable Analysis of Professional Satisfaction and Reported Sense of Feeling Valued 295 Arabella L. Simpkin, MD, MMSc, Health Professionals Education Research Stress from Uncertainty Predicts Resilience and Engagement Among Subspecialty Fellows A.L. Simpkin1,2, S. Hata1,2, M. Logan1,2 and K. Armstrong1,2 1Medicine, Massachusetts General Hospital, Boston, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Burnout is an accelerating phenomenon in the healthcare environment, with implications for physician wellbeing, patient safety, and quality of healthcare. While many studies and interventions have been directed at medical students, residents, and faculty to better understand what impacts burnout, subspecialty fellows have been largely untargeted, despite being a vulnerable population, critical to the future of patient care. Despite the gap in research relating to fellows, in 2017 the ACGME changed their common program requirements to include provisions for well-being and burnout prevention in accredited fellowship training programs, reflecting the urgent need for this to be brought to the top of hospital priority lists. Fellowship training presents unique challenges, often coinciding with geographic uprooting, increased responsibility, pressure to enter the job market, and loss of the team structure that can define residency. These extrinsic challenges add to the intrinsic challenge of handling uncertainty in the clinical learning environment. The ability to manage uncertainty has been identified as a potentially important determinant of burnout in physicians. Accepting, understanding, and managing uncertainty has increasingly become a recognized competency for trainees, highlighted by the ACGME. Studies in residents and faculty have linked intolerance of uncertainty to burnout, ineffective communication strategies and inappropriate resource use. No prior studies have evaluated the association of fellows' stress from uncertainty with burnout metrics, including resilience and engagement.Methods: We conducted an online cross-sectional confidential survey in July 2018 of all 317 sub-specialty fellows in the Department of Medicine at Massachusetts General Hospital to examine burnout, resilience, engagement, empowerment, and stress from uncertainty to enable the department to understand the needs and experiences of fellows. The primary outcome was stress from uncertainty. Using t-tests and Pearson chi square tests, we examined the association of stress from uncertainty with demographic information; personal and professional characteristics; burnout; resilience; work engagement and work empowerment. In addition to correlations among continuous scale scores, levels of burnout and resilience were categorized 253into high vs low using previously established thresholds. Analyses were performed using commercially available statistical software (STATA version 15.0; Stata-Corp LP, TX, USA). Tests with p<0.05 were considered statistically significant.Results: 111 fellows (35%) responded. 33 fellows (32%) met the criteria for burnout. In bivariate analyses, stress from uncertainty was significantly higher among women, younger fellows (<30 years of age), and graduates of US medical schools. Stress from uncertainty did not vary significantly by marital status, or location of residency. Stress from uncertainty was inversely correlated with work engagement (r=-0.25; p=0.009), work correlated with burnout (r=0.34; p<0.001). In addition, fellows who met criteria for high burnout were more stressed by uncertainty (45.9(11.2) vs 40.2(10.4), p=0.01) as were fellows who met criteria for low resilience (44.7(10.2) vs 37.9(10.9), p=0.001). See Table.Conclusion: Despite the remarkable trajectory of biomedical research over recent decades, uncertainty will always be part of clinical medicine. Indeed, it is likely to increase with the growth of data, informatics, and treatment options. This study suggests stress from uncertainty is correlated with burnout among subspecialty fellows, as well as the related experiences of resilience, work engagement, and work empowerment. While we found females to have higher levels of stress from uncertainty, previous studies have been inconsistent in finding gender differences. We found fellows less than 30 years of age and those that trained at US medical schools to have highest stress from uncertainty. This may reflect less experience in managing uncertain situations and may reflect an increased propensity for those with higher tolerance of uncertainty to seek to leave their home country and embrace the uncertainty of geographic relocation. The ability to deal with uncertainty is increasingly recognized as a major goal of medical education and growing evidence suggests it may be possible to decrease stress from uncertainty by talking openly about uncertainty in clinical settings, proactively addressing it in management plans, and communicating uncertainty to patients. This study supports the hypothesis that efforts to improve management of uncertainty may be useful for addressing burnout among trainees. Limitations of this study include its small size and single site, though there is no reason to postulate that similar predictors of stress from uncertainty would be unique to medicine or this academic health center. 254296 Mozhdeh Sojoodi, PhD, Surgery - Surgical Oncology The Autotaxin-Lysophosphatidic Acid Signaling Axis Stimulates Aggressive Tumor Biology and the Desmoplastic Response in Tanabe1 1Surgery, Mass General hospital, Harvard medical school, Boston, MA, USA, 2Pathology, Mass General hospital, Harvard medical school, Boston, MA, USA, 3Medical Oncology, Dana-Farber Cancer Institute, Boston, MA, USA, 4Radiology, Martinos Center for Biomedical Imaging, Charlestown, MA, USA and 5Mass General Hospital, Center for Cancer Research, Boston, MA, USA Introduction: Autotaxin (ATX) is a secreted lysophosphatidyl- to the bioactive lipid lysophosphatidic acid (LPA). ATX-LPA signaling is a known driver of organ fibrosis. Given the high expression of ATX in the pancreas, we hypothesized that ATX-LPA signaling plays a critical role in the desmoplastic response and growth of PDAC.Methods: Plasma ATX levels in 66 human PDAC patients were compared to healthy controls, and levels were associated with clinicopathologic traits and outcome measures. The effects of LPA on pancreatic stellate cell (PSC) activation and carcinoma proliferation were investigated using cell lines. To examine the effects of ATX on tumor growth and chemore - sistance in vivo, C57BL/6 mice (n = 10 per group) were orthotopically implanted with 104 syngeneic Hy15549 PDAC cells with ATX knock-in). Mice were randomized to receive either vehicle control, an ATX inhibitor (AM063, 2.5 mg/kg), FOLFIRINOX, or the combination of AM063 and FOLFIRINOX. Mice were sacrificed on day 14 post operation, when control tumors had reached 6-8 mm in size. Results: Plasma ATX levels were significantly increased in a subset of human PDAC patients and were associated with more aggressive disease. Histologically, ATX was upregulated in peri-tumor pancreatitis and desmoplastic tissue within the tumor microenvironment, which translated to increased local LPA levels. In vitro, exogenous LPA induced proliferation and activation of murine and human PSCs. Phosphokinase analysis revealed that LPA signals primarily through AKT, ERK and Rho pathways via the LPA1 receptor in PSCs. LPA also induced murine and human carcinoma cell proliferation, migration, and epithelial-to-mesenchymal transition (EMT), which was associated with chemoresistance to oxaliplatin. In vivo, ATX knock-in tumors had increased expression of mesenchymal markers and were associated with chemoresistance to FOLFIRINOX and increased metastatic potential to the liver. Treatment of both control tumors and ATX knock-in tumors with AM063 reduced tumor size and fibrosis and was associated with increased sensitivity to FOLFIRINOX. Conclusion: These findings strongly support the role of the ATX-LPA pathway in pancreatic cancer pathogenesis. Inhibition of this pathway reduces tumor size and stromal activation in a syngeneic orthotopic murine PDAC model, which is associated with increased sensitivity to FOLFIRINOX 297 Robert K. Sommer, Medicine - MGH Cancer Center Communication about Prognosis and Illness Understanding Among Adults Receiving Targeted Therapy for Metastatic Non-Small Care, Massachusetts General Hospital, Boston, MA, USA, 2Department of Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 3Cancer Center, Massachusetts General Hospital, Boston, MA, USA and 4Department of Thoracic Oncology, Massachusetts General Hospital, Boston, MA, USA Introduction: Tyrosine kinase inhibitors that target the effects of lung cancer driver mutations such as epidermal growth factor receptor (EGFR) have markedly improved the life expectancy and symptom burden of metastatic lung cancer for a subset of patients. Little is known about how patients learn about and understand their diagnosis and prognosis in this shifting landscape. To address this gap, we conducted a study to explore the perceived facilitators and barriers of communication about the goal and likely outcomes of targeted therapy treatment for metastatic lung cancer from the point of view of patients and caregivers, and assess patient and caregiver understanding of these topics.Methods: We conducted a multi-methods study of adults with metastatic NSCLC with targetable mutations and their caregivers. We conducted semi-structured interviews about communication and prognostic understanding with patients and caregivers and administered surveys to all participants. The survey included the Hospital Anxiety Depression Scale (HADS) as well as questions to assess information preferences and understanding of prognosis and the goal of targeted therapy. 255We used a framework approach to code the transcribed semi-structured interviews and synthesized findings from coded transcripts into themes.Results: We included 32 patients with metastatic NSCLC with mutations in EGFR (58%), ALK (29%) or (13%), and 14 caregivers. Patients were 53% female, 87% non-Hispanic white, and 78% had at least a college education. Caregivers were 36% female and 86% non-Hispanic white, and 58% had at least a college education.Twenty-eight patients and 12 caregivers completed surveys. One third of patients (32%) and 58% of caregivers met criteria for clinically significant anxiety by the HADS. Most patients (65%) and nearly all caregivers (92%) felt that it is very or extremely important to know about prognosis; 70% of patients discussed prognosis with their oncologist at least \"sometimes,\" and 85% of patients had not discussed the care they would want to receive if they were dying with their oncologist. The majority (69%) of patients endorsed feeling \"relatively healthy and terminally ill,\" and 58% of patients and 58% of caregivers believed there is at least some chance (25%) that targeted therapy will cure their lung cancer or were unsure. We identified the following themes. 1) Adults with metastatic NSCLC with targetable mutations and their caregivers were often shocked by the diagnosis of metastatic lung cancer, especially because patients' age and smoking status did not align with lung cancer stereotypes. Despite this initial shock, many patients and caregivers remembered hearing from their oncology team that targetable mutations were an auspicious sign, \"the best kind of lung cancer to have,\" because of the options for treatment that meaningfully prolong survival. 2) In general, patients were either satisfied with the amount of information they received about what to expect in the future from their cancer or wanted more information, but simultaneously acknowledged that the future was unknow- able, especially as treatment options are so rapidly evolving. 3) Patients and caregivers reported a feeling of uncertainty as a constant in their lives, which affected their ability to make decisions and sometimes deepened their appreciation for their situation and relationships. They adopted strategies of distraction, altruism, avoidance, and intellectualization to cope with uncertainty and knowledge of their mortality. 4) Patients and caregivers also described a gradual process of learning about targetable mutations from lung cancer support groups and Internet research. They gravitated to stories of individuals who outlived expectations as a symbol of hope. Many expressed a desire for greater personal interaction with a people going through a similar experience of lung cancer with a targetable mutation, or wished to have been connected to such a community sooner in their illness. Conclusion: Patients with metastatic NSCLC with targetable mutations and their caregivers share many common experiences of learning about and coping with their diagnosis. They gain knowledge and support about lung cancer from their peers and oncologists, yet desire more information about what to expect in the future from their specific illness course. In addition, patients and caregivers in our study harbored some misconceptions about the goal of targeted therapy, which may reflect hope that a cure is around the corner in their uncertain future. This study illustrates the need for interventions aimed specifically at this population, to help patients and caregivers cope with the uncertainty of their disease trajectory and prepare for what lies ahead. 298 Samantha L. Speroni, Bachelor of Arts, Center for Genomic Medicine (CGM) Quantitative analysis of cerebrovascular lesion burden in patients with Arginine Boston, USA, 2Neuroradiology, Goethe University, Frankfurt, Germany, 3Neurosurgery, Boston Children's Hospital, Boston, MA, USA, 4Internal Medicine, University of Texas Health Science Center at Houston, Houston, TX, USA and 5Radiology, Massachusetts General Hopsital, Boston, MA, USA Introduction: Mutations in the Arginine 179 of the alpha smooth muscle actin isotype 2 (ACTA2) gene causes multisystemic smooth muscle dysfunction syndrome with severe cerebrovascular small and large vessel involvement. Early diagnosis can be made through gene sequencing in infants presenting with congenital mydriasis and patent ductus arteriosus or aorto-pul- monary window, two of the most common early symptoms. Further symptoms include progressive dilatation and fusiform aneurysms of the aorta and other elastic arteries, chronic lung disease, pulmonary artery hypertension, hypotonic bladder, intestinal malrotation, gastroparesis, constipation, gallbladder stones, hydronephrosis, abdominal wall defects, and inability to adequately regulate blood pressure and body temperature; however, the involvement of brain vasculature and its onset during childhood distinguishes ACTA2 Arg179 from all other mutations. Large vessel steno-occlusive disease, white matter injury and arterial ischemic stroke have all been previously reported. The aim of this study is to perform a systematic charac - terization and quantification of cerebral disease burden and to evaluate longitudinal changes. Methods: N=83 cerebral MRI scans of 21 patients were evaluated (p.Arg179Cys:p.Arg179His mutation= 5:6), follow up (FU) MRI was available in 81% of cases with a mean observation interval in years \u00b1 SD: 4.51 \u00b1 3.68. Baseline and follow-up scans were screened for presence of ischemic strokes due to large artery occlusions and cystic white matter lesions. WMH were quantified on T2-/T2-FLAIR weighted imaging using planimetric methods. Cerebral MR-Angiographies were screened for presence of critical stenosis and vessel diameter were quantified.256Results: Vessel and acute and subacute white matter abnormalities were detected in 5/6 patient's imaging during infancy. Quantifiable WMH were present in all patients >1.4 years and while their volumes did not significantly change (baseline vs. FU median cc. [IQR]: 53.17 of cystic lesions increased over time (baseline vs. FU, median no. [IQR]: 14 [8.75] vs. 14 [23.25]; p=0.03, n=11) Arterial ischemic strokes were present in 42.9% of patients and corresponded in frequency with those vascular territories that show critically stenosed vessels. Large artery IS were present in 42.9% while dilatation of the distal carotid artery, straightening of vessels, narrowing of the cerebral peduncles the angle of the corpus callosum forceps in 100% of non-infant patients. Conclusion: We found quantifiable cerebral lesion pattern in patients with ACTA2 Arg179 mutations as early as 2 days of age. Our longitudinal data suggest that while steno-occlusive disease progresses over time parenchymal injury can remain stable over long periods. Above evaluated metrics can be used to monitor future treatment attempts such as cerebral bypass surgery and gene therapy. Left panel: A) and B) Maximum intensity projections of time of flight angiographies of two patients illustrating typical vessel abnormalities associated with ACTA2 (R179) gene mutations with intermitted stenotic changes in vessel caliber and abnormal straight course of the basal cerebral arteries. C) Frequencies of involved vascular territories of IS related to large artery occlusion and D) Frequencies of critical stenosis within the vascular territories in ACTA2 (R179) patients Right panel: E) White matter abnormalities typically found in patients with ACTA2 (R179) gene mutations. F) Acute and chronic ischemic stroke lesions related to large artery occlusion. G) Cystic white matter lesion frequently found in ACTA2 (R179) gene mutation patients. Arrow indicates angulation of the lesser forceps of the corpus callosum is increased. H) The midbrain shows narrowing of the cerebral peduncles produced by straightening of the posterior cerebral arteries (PCA). Left panel: Longitudinal data on white matter hyperintensities, numbers of cystic lesions (CL and critical stenosis (CS). Right panel: A) Representative FLAIR images to illustrate the range of white matter hyperintensities found in ACTA2 (R179) patients, both patients are between 11-12y of age. B)Representative T2 weighted images of a 4.79 year old male and follow up visit 10 years later indicating stable extension of white matter hyperintensities. C) Baseline and D) 2 year follow up FLAIR sequences of the same ACTA2 (R179) patient illustrating appearance of additional CL in the white matter (white arrows). E) and F) representative magnifications of maximum intensity projections of time of flight angiographies of the same ACTA2 (R179) patient illustrating the appearance of new CS defined as intermittent signal loss following the vessels course (white arrows). 299 Tasleem Spracklin, PharmD, Pharmacy Characterization of apixaban bleeding rates correlated with dosing in patients with chronic kidney disease R. Sevinsky1, T. Spracklin1, A.W. Tatara1 and A. Fenves2 1Pharmacy, Massachusetts General Hospital, Boston, MA, USA and 2Nephrology, Massachusetts General Hospital, Boston, MA, USA Introduction: Apixaban is an oral anticoagulant or venous thromboembolism (VTE). However, the optimal dose in patients with advanced chronic kidney disease (CKD) is unclear. The objective of this study was to evaluate the safety and efficacy of apixaban 5 mg twice daily (BID) versus 2.5 mg twice daily for NVAF or VTE in patients with CKD stage 4 and 5, including those on hemodialysis.Methods: This was a retrospective, single-site cohort study. Patients were identified for inclusion after an index hospitaliza - tion at Massachusetts General Hospital (MGH). Patients aged 18 years or older were screened for inclusion if they received at least one dose of apixaban while hospitalized at MGH between January 1, 2013, and August 31, 2018, and had recorded evidence of CKD stage 4 or 5 by International Classification of Diseases, Ninth Revision (ICD-9), and International Classi- fication of Diseases, Tenth Edition (ICD-10). Patients were excluded if they were receiving anticoagulation for an indication other than NVAF or VTE, had a history of an abnormal bleeding or coagulation disorder, had bleeding due to other causes such as trauma, received more than one dose of other oral anticoagulants in the previous five days, were nursing or pregnant, were receiving forms of renal replacement therapy other than HD, or if CrCl could not be calculated because baseline weight or serum creatinine was missing. Patients who met the inclusion criteria were categorized into groups based on the dose of apixaban received on the index date. The index date was defined as the date of the first documented administration of apixaban while hospitalized at MGH within the specified time frame. Information about bleeding and thrombotic events were collected for each patient from the index date until the last date of follow-up available in the EMR. The last date of follow-up was defined as the date apixaban was discontinued, the date of change in apixaban dose, or, if neither of the previously stated events were documented, the date of the last documented note in the patient's medical record. The primary safety outcome 257of the study was major bleeding events during the study period. The primary efficacy outcome was ischemic stroke or VTE during the study period. The secondary outcome was a composite of major bleeding events, clinically relevant nonmajor bleeding (CRNMB) events, and minor bleeding events during the study period. The definition of major bleeding from the International Society of Thrombosis and Hemostasis (ISTH) was used to identify major bleeding events. Events were catego- rized as major bleeding if they were fatal, accompanied by a drop in hemoglobin level of 2 g/dL or more within a 48 hour time frame, or in a critical area or organ such as intracranial, intraspinal, intraocular, retroperitoneal, intraarticular, pericardial, or intramuscular with compartment syndrome. CRNMB was defined as clinically overt bleeding that did not satisfy criteria for major bleeding but led to physician-guided medical or surgical treatment, hospitalization, or change in antithrombotic therapy. All other bleeding events were categorized as minor bleeding. Ischemic stroke and VTE events were included if they were documented in the patient's electronic medical record and confirmed with imaging techniques (ventilation/perfusion scan, magnetic resonance imaging, computed tomography, or doppler ultrasound).Results: 290 patients were screened for inclusion and a total of 98 patients were included. Of the included patients, 22 were receiving the 5 mg twice daily dose, 73 were receiving the 2.5 mg twice daily dose, and 3 were receiving the 10 mg twice daily dose. Results of major, and all other bleeding events are presented in Table 2. No significant differences between groups were observed. Subgroup analyses were performed on bleeding event outcomes in patients with severe renal impairment not on HD and patients on HD, and no significant differences were observed between dose groups. In the patients who were inappropri- ately dose reduced according to manufacturer recommendations, 7 patients (14.6%) experienced major bleeding events and 34 patients (70.8%) experienced any bleeding event. None of the patients in the 10 mg twice daily group experienced bleeding events. Results of thrombotic events are presented in Table 3. No significant differences between groups were observed. Two patients in the 2.5 mg twice daily arm experienced ischemic strokes, and 1 patient in the 5 mg twice daily arm experienced a VTE while receiving apixaban. None of the patients in the 10 mg twice daily group experienced ischemic stroke or VTE. Conclusion: In patients with advanced CKD, no difference was observed between the apixaban 5 mg twice daily and 2.5 mg twice daily groups with regards to the percentage of patients who experienced major bleeding events, any bleeding event, ischemic stroke, or VTE. Table 2 and Table 3 300 Rachel Staley, BA, Psychiatry Optimization of Transcutaneous Auricular Vagal Nerve Stimulation for the Regulation of Cardiac Autonomic Function in Hypertension R. Staley1, J. Stowell1, Goldstein1,3,7 Massachusetts General Hospital, Charlestown, MA, USA, 2Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA, 3Innovation Center on Sex Differences in Medicine, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA, 4Radiology, Logan University, Chesterfield, MO, USA, 5Electronics, Information, and Bioengineering, Politecnico di Milano, Milan, Italy, 6Anesthesia, Critical Care and Pain Medicine, Massachusetts General Hospital, Boston, MA, USA and 7Obstetrics and Gynecology, Massachusetts General Hospital, Boston, MA, USA Introduction: Individuals with hypertension are at greater risk for developing cardiovascular disease (CVD), the leading cause of global mortality. Previous research has recognized an association between reduced cardiac vagal tone, as measured by heart rate variability (HRV), and a worse cardiovascular prognosis in hypertensive patients. The evaluation of HRV may 258be of significance in determining the therapeutic benefits of novel interventions in hypertension. While invasive Vagal Nerve Stimulation (VNS) has shown some promising effects on cardiac autonomic regulation, research has yet to evaluate these potential benefits in hypertensive subjects. Our research team has recently developed an optimized, transcutaneous Vagus Nerve Stimulation technique by gating stimulation of the auricular branch of the vagus nerve to the exhalatory phase of respiration. The objective of this study was to identify potential frequency-dependent effects of this novel technique called Respiratory-gated Auricular Vagal Nerve Stimulation (RAVANS) on patients.Methods: Twenty hypertensive subjects (54.55\u00b16.23 years of age; 12 females and 8 males) participated in this study. Each participant had been diagnosed with primary hypertension and were on stable doses of antihypertensive medications for at least 30 days prior to enrollment. 10 subjects were on multiple antihypertensive medications and 10 subjects were being treated via monotherapy. Exclusion criteria included history of other cardio-, cerebro-, or peripheral vascular diseases, diabetes mellitus, morbid obesity (BMI 40 kg/m 2), secondary hypertension, kidney or liver failure, thyroid disorders, traumatic brain injury with cognitive sequelae, and psychiatric disorders involving psychosis. All subjects had five stimula - tion sessions in which they received exhalatory-gated RAVANS (monophasic rectangular pulses; 300 s pulse width; 1s delivered with a UROstim transcutaneous electrical stimulator (Schwa Medico, Germany). Each session consisted of a 10-minute baseline period, 30-minute stimulation period, and 10-minute recovery period. Participants received RAVANS at a different frequency for each session (i.e. 2Hz, 10Hz, 25Hz, 100Hz, or sham stimulation) in a randomized order. RAVANS electrodes were placed over the left cymba concha, a vagal-innervated auricular region, and stimulation amplitude was calibrated to an intensity that produced a moderate, non-painful sensation. For sham stimulation, subjects were told that stimulation intensity would be decreased to a level below their sensory threshold and the UROstim was gradually turned down before being turned off. Electrocardiogram signals were acquired with electrodes placed on the chest using a Grass LP511 AC amplifier (Grass Technologies, Astro-Med, Inc. Product Group, Rhode Island, USA) and pulse rate was collected through a piezo device placed on the subject's thumb with a 16-channel PowerLab DAQ System (ADInstruments, Colorado, USA). Heart rate and measurements of HRV - standard deviation of RR intervals (SDRR), root mean square of successive RR interval differences (RMSSD), and relative power of the high-frequency band (HF power (%)) - were computed over the rest, stimulation, and recovery periods using LabChart (ADInstruments). General linear models (GLM) were implemented to evaluate the specific effects of RAVANS stimulation at different frequencies on heart rate and HRV modulation (SDRR, RMSSD, HF power) during the stimulation and recovery periods. Baseline HRV values and sex were included in the models and adjusted regression coefficients were calculated using STATA (StataCorp, College Station, Texas, USA).Results: Our analysis revealed a significant effect of exhalatory-gated RAVANS at 100 Hz in the modulation of HF the ( =39.08, t (39)=2.14, p=0.039, Adj R 2=0.09) after adjusting by sex and baseline values. No significant effects were observed with RAVANS stimulation at other frequencies on heart rate or other HRV indexes. No significant side effects or adverse events were experienced by subjects during the study. Conclusion: Our results show that Respiratory-gated Auricular Vagal Nerve Stimulation (RAVANS) effectively upregulates cardiovagal activity in hypertensive subjects. Furthermore, our study reveals that RAVANS at a frequency of 100Hz seems to be optimal for modulation of parasympathetic activity in this population. The potential therapeutic effects of this technique should be further explored in future longitudinal studies. 301 Fatima C. Stanford, MD, MPH, MPA, Health Professionals Education Research Coverage of Obesity on American Board of Medical Specialty (ABMS) Examinations F.C. Stanford1, S. Yarlagadda3 and C. Palad2 1Neuroendocrine and Pediatric Endocrinology, Massachusetts General Hospital/ Harvard Medical School, Boston, MA, USA, 2New York Medical College, Valhalla, NY, USA and 3Harvard College, Cambridge, MA, USA Introduction: Obesity is a widespread disease adversely affecting adult and pediatric populations in the U.S. and negatively impacting all organ systems. It is imperative physicians across all specialties approach obesity as a highly important and distinct condition. Currently, medical knowledge and practice standards deemed important for achieving proficiency in each specialty are influenced by the American Board of Medical Specialties (ABMS) in conjunction with the 24 Member Boards. Each board, except for the American Board of Colon and Rectal Surgery, has a publicly available syllabus for their board certification exam.Methods: We parsed through the 23 syllabi for topic notation indicating coverage of obesity and categorized them into three tiers: Tier 1 indicates specific mention of \"obesity,\" Tier 2 indicates mention of related terminology, but not of obesity, and Tier 3 indicates no mention of obesity or related terminology. Results: Our review found 7 syllabi rated tier 1, 10 rated tier 2, and 6 rated tier 3. Our findings highlight specialties strongly affected by obesity, such as Orthopaedic Surgery, that lack exam coverage of obesity.259Conclusion: Given the importance of ABMS in shaping medical practice standards, we advocate for increased coverage of the diagnosis, prevention, and management of obesity across all board examinations. 302 Hannah S. Stein, B.S., Orthopedics The Kinematic Sequence and Associated Shoulder and Elbow Torques during the School, Boston, MA, USA and 2Sports Medicine Service, Department of Orthopaedic Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Successful baseball batting requires efficient timing of body segments' successive rotational movements to maximize power generation and bat velocity (Fleisig et al, 2013). The Kinematic Sequence (KS) is defined as the timing of peak angular velocities generated across interconnected body segments. Previous baseball pitching studies have described an ideal KS where the timing of each body segment's peak velocity occurs in a proximal-to-distal pattern from the pelvis, trunk, arm, forearm, and out towards the hand. Based on models of the baseball pitch, this KS results in greater ball velocity and decreased upper extremity joint torques (Scarborough et al, 2018; Fortenbaugh et al, 2009). However, the concept of the Kinematic Sequence has yet to be investigated in baseball batting. Injuries to the lead shoulder and elbow as well as oblique muscle strains are attributed to the repetitive nature of the baseball bat swing (Lintner et al, 2008). Therefore, exploration of the relationship of KSs to biomechanical metrics associated with injury risk is needed. The purpose of this study was to 1) evaluate the feasibility of using the KS to characterize the baseball swing and 2) identify KSs performed during batting and relationships with upper extremity torques. Methods: Twenty-three baseball players (professional= 2, collegiate= 11, school= 10) with a mean age of 17.9 \u00b1 2.6 years participated in this cross-sectional study. Each player underwent 3D biomechanical analysis of their baseball swing using a SwingAway. Sixty-two reflective markers were affixed to anatomical landmarks following a standardized protocol. A 15-segment model for each subject was constructed from the marker positional data collected using a 3D motion capture system (20 Vicon MX T-series cameras, 240 hz). Each player performed 3-5 batting swings with the baseball at belt height in 4 different positions over home plate. Study analyses were performed on swings with the baseball in the middle of the plate. Kinematic variables including bat velocity and peak sagittal, frontal, and transverse plane shoulder and elbow torques were calculated using Visual 3D software (C-motion). The timing of peak angular velocity of the pelvis, trunk, arm, forearm, and hand segments were recorded to define the KS pattern of each swing. KS patterns were grouped for analysis by the first body segment that disrupted the ideal proximal-to-distal pattern into the following groups: closest to analyses (SPSS V25) included an ANCOVA with maximum bat velocity as a covariant.Results: Eleven unique KS patterns were performed during a total of 41 swings from 23 batters. The most common KS pattern type was DUE (n=23 swings), followed by PUE (n=13 swings), and PDS (n=5 swings). No batter displayed the exact proximal-to-distal KS pattern. However, 5 trials grouped as PDS displayed a very close approximation of the ideal proximal- to-distal pattern with the forearm and hand velocities peaking simultaneously. Peak elbow extension torque was found to differ significantly between the three KS groups (F(2,37)=4.949, 2=0.211, p= 0.012) with significantly All other recorded peak shoulder and elbow torque values did not reach the established level of significance for differences between KS groups. Peak bat velocity (mean velocity= 30.23 \u00b1 3.06 m/s, p= \u00b1 3.92 m/s, p= differences among KS groups. Conclusion: This study establishes a classification framework for evaluating the baseball swing utilizing the concept of the Kinematic Sequence (KS). The successful classification of forty-one swings into 3 distinct KS pattern groups suggests the utility of the KS as an evaluation tool. Statistically lower elbow extension torques within the PDS group are consistent with the idea that execution of a proximal-to-distal KS may result in decreased upper extremity joint stress. Previous studies have identified elbow extension torque as one of the most influential factors on bat speed, suggesting that further study of swing KS patterns may also have implications for batting performance outcomes (Koike & Mimura, 2016). This foundational study is the first to apply the KS classification system to the baseball swing, which could potentially guide clinicians and coaches in hitting instruction to maximize efficiency and minimize biomechanical injury risk factors during batting.260 Figure 1. Mean peak elbow extension torque during baseball batting across Kinematic Sequence groups: extremity (DUE). 303 Derek Stenquist, M.D., Orthopedics The Impact of a Handoff Improvement Program on the Quality of Inpatient Orthopaedic Surgery Handoffs D. Chen2 and M.O. Harris1 1Orthopaedic Surgery, Massachusetts General Hospital, Boston, MA, USA and 2Orthopaedic Surgery, Brigham and Women's Hospital, Boston, MA, USA Introduction: Medical errors are a leading cause of death among Americans, and interventions to improve patient handoff communication have been shown to reduce medical errors. The I-PASS tool (Illness severity, Patient summary, Action list, Situational Awareness, Synthesis by Receiver) is a structured handoff tool that reduces errors and preventable adverse events. I-PASS communication is considered the gold standard for patient handoffs by the Agency for Healthcare Research and Quality. However, adoption of I-PASS on surgical services has been inconsistent due to high patient volume. Meanwhile, duty hour restrictions and the use of physician extenders have increased the number of patient handoffs performed daily. These changes highlight the need for high quality but efficient handoff communication in orthopaedic surgery. Furthermore, standardization of patient handoff communication is a National Patient Safety Goal defined by the Joint Commission. Studies of handoff quality in orthopaedics are sparse. The purpose of this study was to evaluate the quality of patient handoffs before and after I-PASS adaptation in an orthopaedic surgery department. Methods: A prospective intervention study of an orthopaedic handoff improvement program across two hospitals was conducted. A pre-intervention handoff needs-assessment survey was administered to orthopaedic providers, followed by an intervention consisting of electronic handoff and communication training to introduce a standardized handoff template. The electronic template adapted the I-PASS tool specifically for orthopaedic surgery patients and was designed through an iterative process involving faculty, residents, nurses, and mid-level providers. Compliance and handoff quality were measured through active surveillance. All handoffs were performed via secure email. Electronic handoffs were analyzed for the presence of key data elements defined by I-PASS, including the use of two patient identifiers, patient illness severity (stable/watcher/unstable), a comprehensive patient summary, pertinent past medical history, action list of tasks for the receiving provider, situational awareness, and contingency planning. Quality of communication regarding plans for surgical anticoagulation and post-surgical antibiosis was also evaluated. A pre-hoc power calculation demonstrated that 304 total (152 per group) handoff observations were needed to have 80% power to detect a change of 5% difference with an alpha error of 0.05. Chi-squared testing was used for categorical analysis with a p-value<0.05 as the criteria for statistical significance.Results: A total of 56 orthopaedic providers (NPs, PAs, residents, and fellows) completed the handoff needs assessment survey. Before the intervention, 82% of providers reported that they relied solely on electronic communication for handoff. Only 25% of respondents felt that handoffs they received were \"often\" or \"always\" adequate. 56% reported that they were \"sometimes\" or \"often\" uncertain about making a clinical decision because they lacked patient information from a handoff. 91% of respondents stated that they would support a standardized electronic handoff template. A total of 405 electronic patient handoffs were analyzed for the inclusion of key quality elements (Figure 1). There were 203 pre-intervention and 202 post-intervention handoff observations. Overall adherence rate to the standardized handoff format was 71.8%. The handoff intervention produced significant improvement (p<0.001) in 6 out of 9 targeted quality elements (Illness Severity, Past Medical History, Action List, Contingencies, Anticoagulation Plan, and Antibiotic Plan).Conclusion: This prospective study determined that this handoff improvement program resulted in a significant improve - ment in the objective quality of orthopaedic handoffs at two hospitals, which has the potential to standardize care by meeting targeted quality elements, along with potentially preventing delays in care and reducing medical errors. Data collection is ongoing with plans for development of orthopaedic-specific illness severity criteria, and the Global Trigger Tool will be used to study the impact on rates of preventable adverse events.261 Nomura, G. Kate Park, Kai Bao, Homan Kang* & Hak Soo Choi W. Stiles, K. Bao, H. Kang and H. Choi Gordon Center for Medical Imaging, MGH, Boston, MA, USA Introduction: Despite advances in imaging techniques and the development of new localization procedures, tumors less than 1 cm in size remain difficult to localize by conventional means due to the difficulty in specific delivery to the tumor site and low tumor-to-background ratio (TBR) that results from high nonspecific uptake and background retention. Here, we report that ultrasmall (< 5.5 nm) zwitterionic nanocarriers (a.k.a. H-Dots) can systemically travel the whole body through the bloodstream without nonspecific tissue uptake, and then eventually clear out to the urine. The basic design of H-dots is composed of 3 specific functional domains: 1) a targeting cavity to deliver specific anticancer drugs to the tumor site, 2) a charge balancing domain to minimize nonspecific uptake, and 3) an imaging domain to track the progress of tumor targeting, monitor the delivery state of drugs, and evaluate pharmacokinetic values. Therefore, H-Dots targeted to GIST provide high TBR imaging for image-guided cancer surgery, selective delivery of imatinib anticancer drug to mutated KIT receptors, and efficient treatment of GISTs.Methods: Imatinib/H-dot complexes were administered into GIST-bearing xenograft tumor mice and knock in mouse models harboring Kit K641E to validate their in vivo biodistribution, targetability, and specificity. Results: Imatinib/H-dot demonstrates lower immune system uptake, improved tumor selectivity, and better tumor growth suppression than free imatinib.Conclusion: These precisely designed H-Dots could be used as a promising theranostic nanoplatform which can potentially reduce the side effects of conventional chemotherapies. 305 Evan Stone, Radiology Severe Frostbite and Thrombolysis: An Evaluation of Salvage Rates and Treatment Protocols at Massachusetts General Hospital E. Stone, M. Tanaka, Z. Irani and T. Walker Interventional Radiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Prolonged exposure to cold temperatures can result in deep tissue freezing, or frostbite, initially involves the distal extremities and can extend proximally. This often leads to significant morbidity due to amputations of affected digits and extremities. Cold-induced endothelial damage and subsequent arterial thrombosis have demonstrated in angiography to coincide with poor perfusion and risk of amputation. The ultimate goal in frostbite treatment is to maximize salvage of functional tissue. Individual hospitals have variable algorithms for addressing severe frostbite, although routine use of tPA has been utilized as a foundational treatment. This study aims to identify differences in clinical outcomes between our institu- tion's tPA treatment protocol for frostbite compared to previously published studies.262Methods: A retrospective review was performed of cases of severe hypothermia requiring angiographic investigation and treatment with tPA at Massachusetts General Hospital for the winter of 2017-2018. All patients were seen by the burn surgery and interventional radiology teams. After being deemed a candidate for intra-arterial TPA, patients underwent diagnostic angiograms in the interventional radiology suite, where the extremities to be treated with tPA was determined. Patients were transferred post-procedurally to intensive care for monitoring and also received intravenous heparin in conjunction with intra-arterial tPA. Patients were brought back to Interventional Radiology for follow up angiograms 24 hours after the initial angiogram, and a final angiogram 48 hours after initial angiogram. Responses to tPA were categorized as Full, Partial, or None as modeled after Gonzaga et al's meta-analysis, and outcomes with a primary endpoint of digital amputations were calculated as both digital salvage rates and an adaptation of the Hennepin score, as proposed by Nygaard et al, for comparison to the tPA responses.Results: 28 patients were admitted for up to 48 hours of tPA therapy for severe frostbite, with 8 excluded due to incomplete follow-up data or comorbidities. A total of 147 digits' at-risk digits were identified in the 20 patients. Observing individual digit outcomes (Table 1), there was a total of 32 digital amputations with 14 occurring in digits demonstrating distal blush, 16 occurring in digits with partial angiographic flow, and 2 occurring in digits without demonstrable angiographic flow. When grouping by clinical outcomes (Table 2), amputations were not performed on any of the 87 at-risk digits in patients with full angiographic response, whereas 32/60 digits (53%) in patients demonstrating partial angiographic response underwent partial or complete amputation. Overall Hennepin salvage rate was 80.6%, with a salvage rate for patients demonstrating partial angiographic response of 64.8%. Conclusion: Our findings demonstrated a clear indication for the utilization of both angiogram and tPA in cases of severe frostbite. Patients who demonstrated complete return of angiographic response had a 0% rate of amputation. Of note, our institutional analysis noted that no anti-platelet or anti-coagulant agents were utilized following the cessation of tPA/heparin, which differs from other hospital protocols. Due to complete salvage of digits with full response, we propose that anti-coagu- lation following tPA, and anti-platelet if contraindications, may not be necessary if angiography demonstrates full response. Furthermore, digital amputations affected 63% of our partial responders, involving levels at or proximal to the level of final angiographic perfusion in 53% of affected digits following tPA, which may relate to the lack of anti-coagulant/anti-platelet therapy or suggest a role for longer treatment times if continued angiographic improvements are noted. These findings have led to a quality improvement initiative with the interventional department and burn unit to refine our protocol for managing severe frostbite. Digit-Focused Outcomes Clinical-Focused Outcomes 306 Daniela I. Ketamine in Resource-limited Surgeons' MGH, Boston, MA, USA, 2Department of Obstetrics, Gynecology and Reproductive Science, Magee Women's Hospital of UPMC, Pittsburgh, PA, USA, 3University of Pittsburgh School of Medicine, Pittsburgh, PA, USA, 4Harvard Medical School, Boston, MA, USA and 5Harvard T.H. Chan School of Public Health, Boston, MA, USA Introduction: Ketamine's wide safety margin has led to its use as a sole anesthetic agent in resource-limited settings. However, there are few recommendations on approaches to associated intraoperative challenges. The objective of this study was to gain surgeons' perceptions on performing operations supported by ketamine and to recommend best practices and techniques. Methods: A qualitative study was conducted using semi-structured interviews of surgeons experienced with performing operations supported with ketamine as the sole anesthetic agent. Interviews continued until thematic saturation. Open- response data was analyzed using thematic analysis as well as iterative group discussions about emergent themes.Results: Sixteen surgeons were interviewed regarding their operative experiences supported by ketamine across 12 countries. Surgeons universally felt that ketamine is safe, saves lives, and that they would administer it to a loved one in support of an operation if no anesthetist was available. Although lack of muscle relaxation with ketamine may require additional strategies to gain exposure, few surgical technical changes are necessary. While ketamine side effects are manageable, a single provider must always be dedicated to ketamine administration and patient monitoring. Surgeons should advocate for global policies, training and access.Conclusion: Ketamine is safe, can provide increased access to emergency and essential surgery, and requires few operative technical changes. Global standards on Ketamine training and use should be established.263307 Tony Succar, PhD, Massachusetts Eye and Ear Infirmary (MEEI) - Ophthalmology Surgical Implantation of Human Retinal (hRPC) in Patients with Retinitis Pigmentosa: First- in-human Safety and Tolerability Results of a Dose Escalation Phase I/II Clinical Trial T. Succar1, D. Terrell1, R. Huckfeldt1, and J. Comander1 1Ocular Genomic Institute & Inherited Retinal Disorders Service, Massachusetts Eye & Ear, Harvard Medical School., Boston, MA, USA and 2ReNeuron, Inc, Boston, MA, USA Introduction: There is currently no effective clinical treatment for retinitis pigmentosa (RP), a progressive neurodegener - ative retinal disease which results in irreversible night blindness and peripheral visual field defects progressing to central vision impairment and in some cases complete blindness. Human retinal progenitor cells (hRPCs) have shown promise in preserving visual function in pre-clinical animal models, and this trial represents subretinal implantation of hRPC. The objectives of this trial were to evaluate the safety, tolerability and preliminary efficacy of three different doses of hRPC after a single administration of hRPC in RP subjects (n=12), and to measure changes in visual function over 2 years.Methods: An open-label, dose-escalation study of a single unilateral subretinal injection of hRPC was conducted with 12 subjects diagnosed with RP and best corrected visual acuity (BCVA) of 35 letters or less (approximately 20/200 or worse). The eye with worse vision was enrolled as the study eye, with the fellow eye assessed as the control eye. Doses included: 250,000 cells (low dose), 500,000 cells (Medium dose) and 1,000,000 cells (high dose). Doses were chosen based on pre-clin-ical findings that showed cell survival and integration in minipigs and efficacy in Royal College of Surgeons (RCS) rat models. At the high dose, a cryopreserved cell formulation was tested. All subjects underwent a standard 3-port 25 gauge pars plana vitrectomy under general anesthesia. After formation of a \"bleb\" of saline, the solution of hRPCs (0.1 cc) was infused into the subretinal space using a 38 gauge cannula attached to 1 mm syringe. The injection region was documented for pre- and post- treatment monitoring. Safety outcomes included monitoring of inflammation, various measures of visual function and structure, through a combination of Adverse Event (AE) reporting, clinical evaluations (systemic and ocular) and laboratory testing. Exploratory outcome measures of visual function included changes in BCVA letter scores, Goldmann visual fields and retinal sensitivity in the area overlying the implanted hRPC as compared with untreated retina. Anatomical endpoints related to persistence of the transplanted cells as demonstrated on Spectral domain-OCT (SD-OCT), color fundus photography and fundus autofluorescence. Changes in the appearance of retinal layers in the treated and untreated areas were analyzed using SD-OCT. The sample size of this study was selected to provide initial safety and tolerability information and was conducted in compliance with the protocol approved by the regulatory authorities, Institutional Review Board, and in accordance with all relevant Code of Federal Regulations on the Protection of Human Subjects.Results: Twelve subjects (9 males and 3 females) underwent implantation without intraoperative complications. The primary endpoint to determine the safety of the sub-retinal implantation of hRPCs was achieved. On clinical examination none of the subjects experienced a proliferative vitreoretinopathy (PVR) response or persistent intraocular inflammation. Visible cell deposits thinned over 1 month as determined by wide field fundus photography, autofluorescence and SD-OCT. Five subjects experienced delayed subretinal bleb reabsorption likely related to a formulation with higher osmolarity. Surgically, vitreoschisis was common, and preexisting membranes / adherent posterior hyaloid caused vitreomacular traction in 3 subjects. In this initial dose escalation phase of the trial, visual function was limited at baseline (e.g. 0-1 ETDRS letter) and no improvement in visual acuity >1 line was demonstrated. Subjective patient-reported observations were recorded (in the context of an open-label study). Five subjects reported subjective improvement of their vision and their responses were described as improvements in central acuity, color vision, seeing hand motion and light in the center of their vision, seeing lights as brighter and the computer easier to see. Four subjects did not experience any appreciable effect on their vision, and three subjects reported subjective worsening of their vision. Conclusion: This study investigated subretinal injection of hRPCs to treat retinitis pigmentosa. The initial results of this trial demonstrated that the subretinal injection of hRPCs is well tolerated without evidence of dose limiting adverse events. These interim results support continued assessment of safety and potential efficacy of hRPCs in subjects with better baseline vision.264308 Haoqi Sun, Neurology Large-Scale Sleep Staging from Heartbeats and Breathing Deep Learning H. Sun1, W. Ganglberger1, M. Leone1, S. Quadri1, R. Tesh1, R. Thomas2 and M. Westover1 1Neurology, Massachusetts General Hospital, Boston, MA, USA and 2Pulmonary, Critical Care & Sleep Medicine, Beth Israel Deaconess Medical Center, Boston, MA, USA Introduction: Cortical, subcortical, and brainstem systems are highly interactive throughout sleep. NREM sleep is charac - terized by strong sinus arrhythmia and stable breathing or flow-limitation. REM sleep is characterized by highly recogniz - able respiratory rate and surges in heart rate and blood pressure. Wake demonstrates dominance of low-frequency heart rate variability and large amplitude movements. These observations suggest that accurate sleep staging might be possible from non-EEG signals influenced by the autonomic nervous system, such as the ECG or respiratory signals. An accurate non-EEG method for staging state characterization would have several advantages including continuous monitoring in both hospitalized and critically ill patients. Methods: The Partners Institutional Review Board approved the retrospective analysis of polysomnograms (PSG) acquired from 2009 to 2016. PSGs were recorded adhering to American Academy of Sleep Medicine (AASM) standards. The sampling frequency is 200 Hz for all signals. The entire dataset includes 10,121 PSGs; 9,644 were exported successfully without time mismatch or missing sleep stage annotations. We excluded PSGs with fewer than 100 artifact-free 30-second epochs, resulting in 8,682 PSGs (Table 1). Changes in heart rhythms and respiration often occur over longer time scales. For this reason, we use 270-second epoch (nine 30-second epochs) centered on each 30-second epoch to be scored. The goal of the deep neural networks presented herein is to classify the sleep stage of the middle 30-second epoch using information from the 270-second epoch. We trained five deep neural networks based on the following input signals and their combinations: 1) ECG; 2) CHEST (chest respiratory effort); 3) ABD (abdominal respiratory effort); 4) ECG+CHEST; and 5) ECG+ABD. Results: Using both ECG and ABD as input signals yields the best prediction results on the testing set. This network is correct in 81.8% of closed eye wake, 55.8% of N1, 66.3% of Most are found between W N1, N1 vs. N2, and N2 vs. N3. ECG+ABD has the highest Cohen's kappa, with values of 0.6 (all five stages), 0.74 (W+N1 vs. N2+N3 vs. REM) and 0.762 (Wake vs. NR vs. REM). Reduced staging accuracy is associated with older age and/or more severe sleep apnea. We show some example whole night recordings in Figure 1. We can see a visible correspondence between the spectrogram and the sleep stages, as well as the mismatch between the spectrogram and EEG-based sleep stage.Conclusion: Utilizing a large-scale dataset consisting of 8,682 PSGs, we have developed a set of deep neural networks to classify sleep stages from ECG and/or respiration. ECG and respiratory effort provide substantial information about sleep stages. The best staging performance is obtained using both ECG and abdominal respiration. Staging performance depends to some extent on age, apnea-hypopnea index, and sleep study type. Dataset Summary Figure 1. An example 47-year male. (A) The sleep stages over the whole night annotated by the technician (hypnogram). (B) The predicted sleep stages from the deep neural network using ABD respiration as input. (C) Example 60-second ABD segment from each sleep stage which is correctly classified and has the highest predicted probability of that stage. Different colors correspond to the triangle markers on other panels, which indicate the location of the example in the whole night recording. The number above each example signal indicates the probability of being that stage as predicted by the deep learning network. (D) The spectrogram of the ABD respiratory signal. The y-axis indicates the frequency.265309 Kristin Sweeney, Oral and Maxillofacial Surgery Natural History of Fibrous dysplasia and Sweeney1,2 and L.B. Kaban1,2,3 1Oral & Maxillofacial Surgery, Massachusetts General Hospital, Boston, MA, USA, 2Harvard School of Dental Medicine, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: The natural history and progression of fibrous dysplasia and McCune-Albright Syndrome(FD/MAS) have not been thoroughly examined and documented1. Both exhibit the same GNAS1 post-zygotic mutation and have a spectrum of clinical manifestations2. This study was designed to answer the question: In patients with craniofacial FD/MAS are there demographic, clinical and radiographic characteristics that reflect the natural history and progression of disease? We predict that patients with MAS, Polyostotic FD(PFD), and Monostotic FD(MFD) in descending order will demonstrate higher disease severity, greater incidence of complications, and undergo more operations. Methods: This is a retrospective cohort study of patients with FD/MAS evaluated at MGH from 2000-2018. Patients were identified through Hospital Research and Patient Data Registries using ICD-9(2000-2016) and ICD-10(2017-present) codes. Patients of all ages and genders were included who had adequate clinical and radiographic records. Patients with inadequate records or unsubstantiated ICD code diagnoses were excluded. Outcome variables included severity of disease at initial presentation(aggressive=rapid growth plus significant complications; non-aggressive growing=slow increase in tumor size, \u00b1mild complications; quiescent=asymptomatic, incidental findings, no growth) 1, number of operations, and subsequent onset of complications: pain, paresthesia, visual or hearing changes, pathologic fracture, airway obstruction, failure to thrive, infection/osteomyelitis, or dental complications. Predictor variables were diagnosis of MAS, PFD, or MFD, age, and gender. Data were de-identified, recorded, and analyzed using SPSS statistical software v. 25(IBM, New York, NY). Descriptive statistics (mean, range, frequencies) were performed and data were assessed for significance using Chi-squared tests and ANOVA (p<0.05).Results: There were 114 patients identified of whom 70 (61.4%) subjects (48 females) met the inclusion criteria. Average age at diagnosis was 23.5 (2.5-66.0) years and at presentation to MGH 27.7 MAS, n=9, PFD, n=24, & MFD, n=37. At initial presentation, subjects had the following signs & symptoms: pain, n=29; sensory abnormal - ities, n=13(hypoesthesia, migraines, taste, hearing changes, and vision changes); facial deformity or swelling, n=54; dental findings, n=25. Subjects had 0-13 (mean=2.3\u00b1 2.32) lifetime operations with 32 having 0 or 1 procedure. At the time of presentation, disease severity was 77.8% aggressive, 11.1% non-aggressive and 11.1% quiescent in in PFD, and 29.7,29.7,40.5% respectively in MFD. Patients with MAS were diagnosed at a younger age than those with MFD (8.5\u00b15.75 vs. 29.0\u00b117.37, p=0.004) and PFD (20.1\u00b114.44, NS). MAS patients were more likely to have pathologic fractures than those with MFD (p=0.000) or PFD bones involved than MFD (p=0.000) and PFD (p=0.000), more frequent bilateral versus unilateral disease than MFD (p=0.000) and PFD (p=0.039), and more frequent visual symptoms than MFD (p=0.001) and PFD (p=0.010). MAS patients also had a significantly greater number of lifetime operations (mean=4.2 \u00b14.18) than those with MFD (mean=1.7 \u00b11.28, p=0.010). Over time, MAS subjects were more likely to develop pain than those with MFD (p=0.002) or PFD (p=0.027) and to have failure to (MFD, p=0.002; PFD, p=0.017). MAS subjects also developed nasal/airway loss (p=0.006), dental problems (p=0.044), and infections/osteomyelitis (p=0.016) more than those with MFD. PFD patients were more likely to develop nasal/airway obstruction than those with MFD (p=0.004). Conclusion: Results of this study indicate subjects with MAS, despite having the same mutation, are more likely to have aggressive disease, complications and a greater number of operations than those with MFD. PFD subjects have more variable severity and complications than MAS/MFD. Most frequent complications are pain, swelling, sensory abnormalities, facial deformities or swelling, and dental abnormalities.266Findings of Subjects with Craniofacial Fibrous Dysplasia at Presentation to MGH (Total Cohort, n=70) fibrous dysplasia lesion extending zygoma to the skull base. Cortical thinning, perforation, and expansion are evident. This lesion obstructs the right nasal passageway and narrows the left nasal passageway. 310 Nobuhiro Takahashi, M.D., Surgery Evaluating Ovarian Cancer Liabilities in Patient Samples Using CRISPR N. Takahashi1,2, L. Zhang1,2, Laboratories, MGH, Cambridge, MA, USA, 2Department of Surgery, Harvard Medical School, Boston, MA, USA, 3Department of Biology, Whitehead Institute for Biomedical Research and Massachusetts Institute of Technology, Cambridge, MA, USA and 4Department of Chemistry, Massachusetts Institute of Technology, Cambridge, MA, USA Introduction: Epithelial ovarian cancers are the fifth leading cause of cancer-related deaths for American women, with a five year overall survival rate for patients diagnosed at advanced stages lesser than 30%. High-grade serous ovarian cancers (HGSOCs), the most common histological subtype, often initially respond to platinum-based chemotherapy. However, over 80% of patients experience chemoresistant recurrence in advanced stage disease.Methods: We have established more than 40 patient-derived HGSOC cell lines from ascites of patients with such recurrences and hypothesize that these refractory HGSOC cell lines may each possess genetic liabilities and be dependent on unique pathways for survival that can be identified using an unbiased screening approach for the development of personalized therapies. We identified candidate genes which are uniquely essential to a single HGSOC patient (ptD), as proof of concept, using a sgRNA library, containing vector.Results: The unbiased CRISPR screen identified BRAF (p=2E-6) as a specific susceptibility of ptD, confirming the known oncogene addiction of this primary cell line carrying a V600E mutation, as well as a significant sensitivity to JUNB deletion (p=6E-6). Knockout of the JUNB gene in ptD cells, with lentiviruses expressing guide RNAs and Cas9 protein targeted at JUNB, repressed cell proliferation in vitro. Moreover, in a xenograft mouse model, JUNB KO ptD cells displayed slower tumor growth capacity than control JUNB WT cells.Conclusion: We have confirmed in a single patient cell line our ability to identify and validate unique genetic liabilities, suggesting the approach can be scaled to rapidly screen primary HGSOC samples for personalized therapy.267311 Idy Tam, Medical Doctorate Candidate, Dermatology Non-invasive tape stripping to analyze epidermal gene expression profiles in patients with allergic contact dermatitis I. Tam2,1, K. Hill3, J. Park3,4 and J. Yu1,4 1MGH Contact Dermatitis and Occupational Dermatology Clinic, Massachusetts General Hospital, Jamaica Plain, MA, USA, 2Tufts University School of Medicine, Boston, MA, USA, 3Cutaneous Biology Research Center, Massachusetts General Hospital, Boston, MA, USA and 4Harvard Medical School, Boston, MA, USA Introduction: Allergic contact dermatitis (ACD) affects 15-20% of adults and children and accounts for more than 90% of occupational skin diseases involving the skin.1 Detection and avoidance of offending allergens are imperative for resolution of dermatitis, which can often be debilitating. In 2013, the American Academy of Dermatology estimated that over 13 million Americans sought care for contact dermatitis, making it the 5 th most common skin condition seeking treatment in the United States (US) and accounting for over 1.5 billion dollars in healthcare spending (AAD Impact of Skin Diseases). Furthermore, the loss in productivity due to contact dermatitis was estimated to be near 700 million dollars (AAD Impact of Skin Disease). Currently, patch testing is the gold standard for diagnosis of ACD but accuracy of the results is highly patient and physician dependent. Molecular methods are being explored with the aim of developing better objective tests to accurately identify contact allergens, which aids in the clinical differentiation between ACD and other mimickers such as atopic dermatitis and irritant contact dermatitis (ICD). Tape stripping is a noninvasive technique, first described by Fritsch et al., that has been widely used in dermatological research to obtain samples of the stratum corneum (SC), the outermost layer of the epidermis. 2 Studies suggest that viable suprabasal keratinocytes obtained through tape stripping could distinguish between inflammatory mediators of ACD vs. ICD3, analyze gene expression in patients with atopic dermatitis through RNA analysis4, and detect invasive melanoma with high accuracy.5 In this study, we examined whether skin samples obtained through tape stripping can provide additional molecular information beyond the expression profiles of the superficial epidermal layer and, if so, whether such data from a wider repertoire of skin cells can be used to differentiate between ACD and ICD without invasive skin biopsies. Methods: Patient with suspected diagnosis of ACD who were referred to the MGH Contact Dermatitis and Occupational Dermatology Clinic were recruited. Each participant was patch tested to potential contact allergens as appropriate for their dermatitis based on history and physical exam. A negative control of 100% petrolatum vehicle and 2% and/or 4% sodium lauryl sulphate (SLS) solution was applied to induce ICD. At 48 hours patch tests were removed. At 96 hours patch test sites were evaluated by a single observer. Twenty consecutive D-Squame (CuDerm, Dallas, TX) tape strips were collected on sites of positive allergic contact reactions and sites of negative control/induced ICD. The first 10 tape strips were discarded, and total RNA was isolated from the latter 10 tape strips. cDNA was synthesized from RNA and subjected to quantitative real-time PCR to analyze molecular markers for keratinocytes inflammatory responses (S100A8, S100A9, CXCL2).Results: Skin samples were obtained from 5 adults (3 females and 2 males; age 25-45) who had demonstrated at least one positive allergic reaction on patch testing. The molecular markers were quantified from RNA isolated from tape strips for all participants. Gene expression analyses showed that tape stripping could sample not only superficial epidermal keratino - cyte markers (LOR, FLG, KRT1, KRT10) of keratinocytes residing in the basal epidermal layer (KRT14). In addition, (ITGAM, CD14, CD68, detected from tape stripping. Furthermore, contact allergen-induced skin reactions were accompanied by a reduction in the signatures of differentiated keratinocytes (S100A8, LOR, KRT1) and dermal dendritic cells (CD1C). Of note, ICD induced by SLS exposure exhibited an increase in cellular stress responses (S100A8, S100A9) and a subset of myeloid reduction in dermal macrophages (FCGR3A, CD68), demonstrating a gene expression pattern distinct from the ACD signatures.Conclusion: These observations suggest that gene expression data obtained from tape stripping can provide multidimen - sional information on the phenotype of clinical specimens that encompasses both epithelial and hematopoietic-derived skin- resident cell types. Taken together, our findings illustrate divergent epidermal and dermal molecular changes associated with ACD versus ICD and present skin tape stripping as a novel strategy that enables non-invasive and molecular marker-based diagnosis of ACD.268 312 Adam Tanious, Surgery - Vascular Gender-Based Discrimination is Prevalent in the Integrated Vascular Trainee Experience and Serves as a Predictor of Burnout A. Tanious1, L. Wang2 and M. Eagleton1 1MGH, Boston, MA, USA and 2MGH, Boston, MA, USA Introduction: Objective: Trainee burnout is on the rise and negative training environments may contribute. In addition, as the proportion of women entering vascular surgery increases, identifying factors that challenge recruitment and retention is vital as we grow our workforce to meet demand. This study sought to characterize the learning environment of vascular residents and to determine how gender-based discrimination and bias (GBDB) affect the clinical experience. Methods: Methods: A survey was developed to evaluate the trainee experience; demographics and a two-item burnout index were also included. The instrument was sent electronically to all integrated vascular surgery residents in the United States. Univariate analyses were performed and predictors of burnout identified.Results: Results: A total of 284 integrated vascular residents were invited to participate and 212 (75%) completed the survey. Participants were predominantly male (64%) and white (56%), with a median age of 30 years (interquartile range, 28- 32 years). Seventy-nine percent of respondents endorsed some form of negative workplace experience and 30% met high-risk criteria for burnout. More than a third (38%) of residents endorsed personally experiencing GBDB, with a significant differ-ence between men and women (14% vs 80%; P < .001). Women were more likely than men to report witnessing GBDB (76% vs 56%; P \u00bc .003). Patients and nurses were the most frequently cited sources of GBDB (80% and 64%, respectively), with vascular surgery attendings cited by 41% of trainees. One in four female resident respondents indicated being sexually harassed during the course of training; this was significantly higher than for male residents (25% vs 1%; P < .001). Nearly half (46%) of trainees who witnessed or experienced GBDB thought that quality of patient care, job satisfaction, personal well-being, and personal risk of burnout were directly affected as a result of GBDB. GBDB was predictive of burnout (odds ratio, 1.9; 95% confidence interval, 1.1-3.5; P \u00bc .04), as were longer work hours (>80 h/wk; odds Conclusions: GBDB was experienced by 38% of integrated trainees, with women significantly more affected than men. GBDB is predictive of burnout, and this has significant implications for our specialty in the recruitment and retention of female physicians. Resources addressing these issues are needed to maintain a diverse workforce and to promote physician well-being.269313 Alexandra W. Tatara, PharmD, Pharmacy Implementation of daily pharmacy student new medication education during hospitalization to improve patient satisfaction A.W. Tatara1, C. Ji1, S. Wood2, S. Jacob1 and J. Marshall1 1Pharmacy, Massachusetts General Hospital, Cambridge, MA, USA and 2Nursing, Massachusetts General Hospital, Boston, MA, USA Introduction: Hospitalized inpatients are medically complex and are often started on several new medications during their hospital stay. Studies have shown that patients would like to receive more education than what they received while hospitalized, and nurses primarily take on this responsibility, as they are directly interacting with patients while administering medications. Patient satisfaction is measured using post-discharge surveys and Hospital Consumer Assessment of Health-care Providers and Systems (HCAHPS) scores, and higher patient satisfaction has been correlated with lower mortality and fewer hospital readmissions. We hypothesize that by introducing pharmacy students to provide new medication education to inpatients, HCAHPS scores of units where this is performed will increase for the two medication-related questions. Methods: This was a quasi-experimental study. Four pharmacy students were assigned 4-5 beds each in one 24-bed general medicine inpatient unit to provide daily education on new medications during the study period, June-September 2019. Patients were included if they were age 18 or older, admitted to White 11 for 24 hours or more, and were started on new medications, which was defined as any medication that was not present on the prior to admission medication list. Patients were excluded if they were admitted for less than 24 hours, had active discharge orders, were incarcerated, terminally ill or on comfort care measures only, or had ongoing altered mental status (alert and oriented <x3). Patients were provided with a medication side effects sheet which was left at their bedsides for their future reference. Students counseled patients on 1-2 new medications per session and focused on the purpose and side effects of each medication. The primary end point was change in HCAHPS scores for the medication communication domain on White 11 compared to a control medicine unit that was not receiving the intervention (White 10) for the pre-intervention (March-May) through the intervention period. Results: Upon preliminary analysis, a total of 52 patients were educated during the intervention period, with a mean age of 66.5 \u00b1 17.7 and mean number of new medications 2.25 \u00b1 1.25. On average, each patient was educated on 1.8 \u00b1 1.11 medications during their hospital stay on White 11. The most common categories of medications included anticoagulants (34, 28.8%), antibiotics (22, 21.1%), and analgesics (13, 12.5%). The average length of stay during the study time frame was 4.1 \u00b1 1.04 days. Average HCAHPS scores for the medication communication domain for White 11 and White 10 in the pre-intervention period (March-May) were 68% and 78%, respectively. During the intervention period, HCAHPS scores for the medication communication domain were 88% and 100% for White 11 and White 10, respectively. The difference between the pre-intervention and intervention periods was 20% and 22%. Conclusion: In this quasi-experimental study of HCAHPS scores before and after a new intervention utilizing pharmacy students to provide daily medication education to medical inpatients, a comparable increase in the medication communica - tion HCAHPS score between White 10 and White 11 was seen after the intervention was implemented. This study has the potential to drive change at MGH by implementing pharmacy students throughout inpatient units to educate patients on new medications to improve satisfaction. Our results are currently limited by a small sample size of patients responding to the HCAHPS questionnaire, which we hope will increase as more time has passed. 314 Ryan Tesh, BSc, Neurology ICU-SLEEP: Investigation of Sleep in the Intensive Care Unit S. Quadri1, E. Panneerselvam1, M. Leone1, R. Tesh1, and M. Westover1 1Massachusetts General Hospital (MGH), Boston, MA, USA and 2Beth Israel Medical Center (BIDMC), Boston, MA, USA Introduction: Sleep disruption in the intensive care unit (ICU) is hypothesized to be a major modifiable risk factor for delirium. ICU delirium contributes to long-term cognitive disability following hospital discharge. Dexmedetomidine (Dex) reduces delirium incidence by unknown mechanisms. We aim to compare the burden of delirium in ICU patients receiving placebo vs. biomimetic sleep, induced by continuous overnight low- or very-low-dose Dex. This study will investigate whether Dex reduces delirium by improving sleep, compare the effects of low- and very-low doses, and evaluate if ICU sleep disruption is related to long-term cognitive outcomes.Methods: ICU-SLEEP is a single-center, phase II, double-blind, placebo-controlled, three-arm, parallel-group, mechanistic, randomized trial that will enroll 750 ICU patients over a period of 5 years. Non-ventilated patients over the age of 50, admitted to medical and surgical ICUs, are randomized into three arms to receive either very-low-dose (0.1 mcg/kg/h) or low-dose (0.3 270mcg/kg/h) continuous infusion of Dex or placebo. Delirium-free days are calculated via in-person patient interviews and chart review. Delirium assessments are conducted twice daily via the Confusion Assessment Method (CAM-S score/CAM-ICU) while the patient is in the ICU and 7 days following movement out of the ICU. A neuropsychiatric assessment of cognitive speed, attention, and short-term recall is performed on the day after the patient leaves the ICU to assess sleep debt. Long-term neuropsychiatric outcomes in ICU survivors will be assessed via telephone at 3, 6, and 12 months following enrollment. In order to better understand the relationship between sleep and delirium, we acquire a variety of clinical signals including blood pressure, oxygen saturation, electrocardiography (ECG), and respiration in addition to ambient noise and light. By monitoring and statistically analyzing these clinical signals, we will stage sleep and quantify sleep quality, investigate how noise and light disturbances affect sleep, and monitor the safety of Dex administration. In previous work, we utilized over 8000 polysom-nographies (PSGs) from the MGH sleep lab to build a deep learning model that is able to stage sleep (W,N1,N2,N3,R) from respiration and ECG-derived signals with high accuracy. Adapting this model with transfer-learning approaches for usage in the ICU will make it feasible to assess sleep and to characterize sleep features, architecture and fragmentation. Results: This randomized control trial has a study duration of 60 months (I.e. 5-yrs, study will remain open until the comple - tion of data analyses). Following the start of trial recruitment in May 2018, recruitment remains active. As of July 25, 2019 - 125 total patients have been enrolled.Conclusion: There exists a critical unmet need to understand how sleep physiology relates to delirium and long-term brain health. ICU-SLEEP is a longitudinal RCT that will provide insight into the mechanisms by which Dex prevents delirium and guidance on the optimal dosing strategy for prophylactic Dex which can be used to design pivotal phase 3 trials. ICU-SLEEP Study Schema Understanding sleep and delirium through clinical signals 315 Marc-Andre Tetrault, Radiology A new high resolution PET scanner for imaging: the SAVANT M. Tetrault1, R. and G. El Fakhri1 1Radiology, Massachusetts General Hospital, Charlestown, MA, USA, 2Radiology Medicine, Universite de Sherbrooke, 4Imaging Research and Sherbrooke, QC, Canada Introduction: Research and clinical PET neuroimaging relies predominantly on clinical whole-body scanners. With a spatial resolution of approximately 5 mm (volumetric resolution ~125 mm3) and sensitivity of a few percent, these devices are clearly suboptimal for brain studies. There exist a limited number of specialized scanners designed to achieve high spatial resolution and high sensitivity for brain imaging. The High Resolution Research Tomograph (HRRT), developed 20 years ago by CTI/Siemens, remains the standard bearer for performance in brain PET, with ~15 mm3 volumetric resolution. We are building the Scanner Approaching in Vivo Autoradiographic Neuro Tomography (SAVANT), a next generation PET scanner for ultra-high resolution imaging of the human brain.Methods: The basic detection unit of the SAVANT scanner consists of a 4 x 8 array of 1.12 x 1.12 x 12 mm3, dual-layered LGSO scintillators coupled one-to-one to a 4x8 monolithic APD array with a 1.2 mm pitch. Four of these detectors are 271assembled on a daughter board in a configuration to form a 128-channel front-end detector module. The scanner is based on 1 008 of these front-end detector modules arranged in a cylindrical configuration, forming 144 rings of 896 pixel detectors per ring, with a diameter of 39 cm and an axial length of 23.5 cm. The highly integrated electronic front-end, based on a dual-threshold time-over-threshold method, enables the signal from every individual pixel detectors to be processed and recorded independently. Imaging simulations were conducted with the proven GATE package with various phantoms, while images were reconstructed using the Castor package.Results: GATE simulations predict the SAVANT will reach peak 3.5% sensitivity and 1.35 mm3 volumetric resolution, with 1.32 mm FWHM spatial resolution at the center of the field of view and 2.02 mm FWHM 10 cm off the center. In a hot spot phantom, the SAVANT can resolve spots down to 1 mm. Figure 1 compares a transverse brain image from a Zubal phantom and shows the anticipated resolution increase from the SAVANT.Conclusion: The SAVANT brain PET scanner will have volumetric resolution an order of magnitude better than the best available dedicated brain PET scanners and two orders of magnitude better than general purpose clinical PET scanners. Such an improvement will allow visualization and quantification of in vivo physiological events as never before possible, enabling detection of neurodegenerative disease at a much earlier stage and permitting the study of small nuclei important to afferent pathways of catecholamine neurotransmitter systems. 316 Janis R. Thamm, Oral and Maxillofacial Surgery Clinical application of three-dimensional printing for maxillofacial reconstruction. A review of reported cases F.P. Guastaldi, J.R. Thamm, M. Mueller and M.J. Troulis Oral & Maxillofacial Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Currently, the standard for reconstruction of maxillofacial bone defects is the use of autogenous bone grafts harvested from the iliac crest, calvaria or vascularized free flaps. The drawbacks of these procedures are associated with donor site morbidity. Also, treatment of large maxillofacial bone defects is clinically challenging due to the limited availability of transplantable autogenous bone grafts and the complex geometry of the bones. The ability to reconstruct bone defects that replicates the anatomy and eliminates morbidity would revolutionize treatment options. The overall goal is to create custom- ized bone scaffolds, or co-constructs (bone, teeth, nerve, etc.) with precise form and shape to reconstruct missing structures. Defects can be analyzed by imaging techniques like computed tomography (CT) or magnetic resonance image (MRI). The data is processed with CAD software creating a digital prototype enabling 3D-printing of a patient-specific implant for bone reconstruction. First implementations of this technique have already been performed in humans, yet indicating the early stage, but promising positive approaches of personalized tissue-engineered devices and scaffold in the clinical setting of maxillofa - cial reconstruction. The purpose of this study was to summarize the published cases, using three-dimensionally (3D) printed devices and/or tissue-engineered (TE) approach to reconstruct segmental bone defects of the jaws.Methods: PubMed database was used to conduct the searches. Search was limited to studies published in English language from January 2004 to December 2018. Narrowing our findings, the focus was only on human case reports, reporting the use of tissue engineering approaches or 3D-printing techniques as a critical step in order to reconstruct segmental bone defects of the jaws. In addition, articles reporting minor cleft defects (length of the size of one tooth) were excluded. The same exclusion criteria were applied for human case studies, investigating only conventional bone autografts and the treatment of minor, non-segmental defects, such as bone augmentation procedures. Data collected included: demographics, location and defect size, diagnosis, type and material of the 3D-printed device and/or TE approach, 3D-printing technologies, outcome and follow-up. Results: Overall, 920 manuscripts were identified. Exclusion criteria narrowed the findings to 61 papers. After excluding duplicate case reports, 37 preselected articles were reviewed. Finally, 15 original publications, reporting 19 cases, were found eligible after applying the inclusion and exclusion criteria Fifteen publications met our criteria, containing 19 case reports. Among them 6 were females and 10 were males. The mean adult age was 52 years (n=12 cases), ranging from 25 years to 82 272years. The mean pediatric age was 10 years (n=4), ranging from 8 years to 13 years of age. Treatment of segmental defects of the mandible (n=12) and of the maxilla (n=7) were cases: clefts (n=4), and as: mesh (n=4), plate (n=1) and (n=4). One case described a 3D-printed Polycaprolactone (PCL) scaffold. In four cases a 3D-printed device (mesh n=1; plates n=3) together with a TE approach was applied. Five patients were treated by only a TE approach. Stereolithography (SLA; n=6), selective laser sintering (SLS; melting (SLM; n=3) were the 3D-printing technologies used. The devices were mainly manufactured out of titanium (Ti; n=15). The mean follow-up period was 16,6 months. Conclusion: In conclusion, the data presented here summarizes the advances and latest technologies that are reported, in order to restore segmental bone defects of the jaws, with a focus on 3D-printed devices and/or TE approach, over the last 15 years. These case reports are an important step to encourage tissue engineering research groups, biomedical engineers and surgeons to debate existing challenges and act at the frontier of knowledge to bring innovative and less invasive solutions for evidence based clinical practice using these technologies and approaches in a safe and effective way to benefit patients. 317 Julia C. Thierauf, Pathology Clinically Integrated Molecular Diagnostics in Adenoid Lennerz1 1Pathology, Center for Integrated Diagnostics, Massachusetts General Hospital, Boston, MA, USA, 2Otorhinolaryngology, Head and Neck Surgery, Heidelberg University Hospital, Heidelberg, Germany, 3Pathology, Brigham and Women's Hospital, Boston, MA, USA, 4Pathology, Computational Pathology, Massachusetts General Hospital, Boston, MA, USA, 5Pathology, Head and Neck Pathology, Massachusetts General Hospital, Boston, MA, USA, 6Otorhinolaryngology, Massachusetts Eye and Ear Infirmary, Boston, MA, USA, 7Cancer Center, Massachusetts General Hospital, Boston, MA, USA, 8Massachusetts Eye and Ear Infirmary, Boston, MA, USA and 9Research Group Molecular Mechanisms of Head and Neck Tumors, German Cancer Research Center (DKFZ), Heidelberg, Germany Introduction: Adenoid cystic carcinoma (ACC) is an aggressive salivary gland malignancy without effective systemic therapies. Delineation of molecular profiles in ACC has led to an increased number of biomarker-stratified clinical trials; however, the clinical utility and US-centric financial sustainability of integrated next-generation sequencing (NGS) in routine practice have, to our knowledge, not been assessed.Methods: In our practice, NGS genotyping was implemented at the discretion of the primary clinician. We combined NGS-based mutation and fusion detection, with MYB fluorescent in situ hybridization (FISH) and MYB immunohistochemistry (IHC). Utility was defined as the fraction of patients with tumors harboring alterations that are potentially amenable to targeted therapies. Financial sustainability was assessed using the fraction of global reimbursement.Results: Among N=181 consecutive ACC cases (2011-2018), prospective genotyping fusions. Overall, these 3 alterations (MYB/MYBL1/NOTCH1) made up 65% of patients and this subset had a more aggressive course with significantly shorter progression free survival. In 75% (n=6/8) of non-resectable patients we detected potentially actionable alterations. Financial analysis of the global charges, including NGS codes, indicated 63% reimbursement, which is in line with national (US-based) and international levels of reimbursement.273Conclusion: Prospective routine clinical genotyping in ACC can identify clinically relevant subsets of patients and is approaching financial sustainability. Demonstrating clinical utility and financial sustainability in an orphan disease (ACC) requires a multi-year and multi-dimensional program. Figure 1. Anatomic Location and Mutational Landscape. A. Anatomic location of each case shown with salient clinical and key mutational findings (i.e. NOTCH1/MYB/MYBL1 findings). B. Mutational landscape provided for each sample (column) along with clinical and molecular features (rows). Abbreviations: CNG, copy 2 . Outcome Analysis and Clinical Utility. A. (1) Identification of potential therapeutically actionable alterations in patients with unresectable tumors and (2) prognostication in resectable patients. B. Progression free survival of patients by NOTCH1/MYB/MYBL1 status. C. Timeline shows dates of service and dates of charges. In 2015, CPT codes for NGS-based panel testing were available and introduced (vertical line). We excluded cases with miscellaneous CPT codes (applied before availability of NGS codes indicated by open grey diamonds) and those payed for from alternative sources (indicated by filled grey diamonds). D. Bar graphs comparing charged (left) and reimbursed amounts (right) separated by NGS- and non-NGS related PFS, progression free survival 318 Fangyun Tian, Anesthesia, Critical Care and Pain Medicine Functional disruption of intracranial EEG dynamics during Boston, MA, USA, 2Massachusetts Institute of Technology, Cambridge, MA, USA, 3Boston University, Boston, MA, USA and 4Albert Einstein College of Medicine, Bronx, NY, USA Introduction: Ketamine is a widely used anesthetic. At higher doses, ketamine induces unconsciousness and immobility, whereas at lower or subanesthetic doses, it produces a dissociative state, which includes altered sensory perception and a sense of disembodiment. The mechanism whereby subanesthetic doses of ketamine disrupts functional brain activity to produce the dissociated state remains unclear. The objective of this study is to investigate in humans the effects of subanesthetic doses of ketamine on brain dynamics and the dissociative state.Methods: Five epilepsy patients were implanted with intracranial depth, surface, and strip electrodes for detection of seizure foci. We recorded during a ketamine infusion administered just prior to the electrode removal surgery. Baseline signals were recorded for 5 minutes. Then each patient received an infusion of a subanesthetic dose of ketamine (0.5 mg/kg over 14 minutes). During ketamine administration, the patients performed an auditory task consisting of interleaved verbal and click stimuli presented every 3.5-4.5 seconds. Patients completed an abbreviated version of the Clinician-Administered Dissocia-tive States Scale (CADSS) questionnaire at the conclusion of the ketamine infusion. Results: The responses on the CADSS questionnaire and the intermittent responses to the auditory stimuli confirmed that our subanesthetic ketamine administration paradigm induced a dissociative state. This state was associated with decreased of alpha oscillation power in precentral gyrus, similar to a prior report showing that subanesthetic doses of ketamine induces a dissociative state by disrupting multisensory integration in the precuneus and temporal-parietal junction that are modulated by alpha waves. Patients also exhibited an increase of gamma oscillation power in both frontal and temporal lobes under subanes- thetic ketamine. Additionally, global coherence analysis demonstrated that administration of a subanesthetic ketamine dose leads to the onset of broadband low gamma/high beta coherence. Finally, phase amplitude coupling between slow (0.1-1Hz) and beta oscillations displayed a transition from coupling in which beta oscillations are larger in the troughs of the slow oscillation (\"trough-max\") at baseline, to another form of coupling in which beta is largest at the peaks of the slow oscillation (\"peak-max\") after the ketamine infusion.274Conclusion: Collectively, these findings indicate that disruption of the dynamic coordination between brain regions may play an important role in mediating the ketamine-induced dissociative state. 319 Keenae Tiersma, Radiology Targeted lung cancer screening education and shared decision-making educational tool for individuals with serious mental illness K. Tiersma 1,2, C. Pappano1,2, J. Neil3, Park2 and E. Flores1 1Radiology, Massachusetts General Hospital, Boston, MA, USA, 2Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 3Health Policy Research Center, Massachusetts General Hospital, Boston, MA, USA and 4Freedom Trail Clinic, Massachusetts General Hospital, Boston, MA, USA Introduction: Lung cancer continues to be the leading cause of cancer mortality despite the availability of early detection methods like lung cancer screening (LCS). For individuals with serious mental illness (SMI), particularly bipolar disorder and schizophrenia, lung cancer outcomes are worse due to high smoking prevalence, limited knowledge about LCS, and challenges related to fragmentation of their care. Furthermore, the Shared Decision Making (SDM) session between patient and provider required for insurance to cover LCS may be limited as patients with SMI have difficulties with abstract thinking and executive functioning. Recognizing the unique barriers to LCS for this patient population, our purpose was to use a qualitative process through focus group discussion to target the SDM process to the needs of individuals with SMI. Methods: From October 23 to November 7, 2018 we conducted three focus group discussions with semi-structured interviews, stratified by key stakeholder groups (radiology, primary care and mental health clinicians). The focus group discussion gathered input on the visual layout and content of the SDM tool. Stakeholders received patient handouts displaying the SDM elements required for SDM LCS counseling. Additionally, we showed a short educational video documenting the process of a patient getting LCS. All focus groups were audio recorded and multiple coders conducted a rapid qualitative analysis to inform intervention development. We independently coded each transcript and participated in a consensus meeting to iteratively develop a codebook. After reaching consensus, we identified and categorized specific areas to improve the educational tool. Finally, we compared feedback from the three stakeholder groups. Data informed improvements to the handout and video. Results: Stakeholder groups emphasized the usage of person-centered language to describe the low-dose CT scan performed for LCS. For example, primary care providers suggested differentiating between CT and MRI scans by describing a CT as \"a donut, not a box\" and as the \"short [quiet] one versus the long [loud] one.\" Additionally, the patient handout was modified: primary care physicians focused more on language used, mental health clinicians emphasized the importance of the co-delivery of the SDM tool from radiologists and mental health clinicians, while radiologists addressed the benefit on cancer mortality and participation in LCS. Significant changes were also made to the patient screening video in response to stakeholder feedback. We adjusted the angle of the video to show the patient more clearly entering the CT scanner to display the donut shape. Finally, mental health clinicians emphasized the importance of showing how patients will store their belong-ings during the procedure which may impact patients who suffer from paranoia. Therefore, in the final video, the patient was shown maintaining possession of their belongings throughout the procedure. Stakeholders emphasized the need to simplify language and adapt the screening video to more accurately reflect the population who will receive LCS.Conclusion: Targeting the SDM tool for individuals with SMI through a qualitative process resulted in changes to key components of the material and video. The SDM tool was adapted to use concrete examples and repeats key ideas to promote understanding in a population with deficits in abstract thinking and difficulty with executive functioning. The three stakeholder groups underscored the importance of collaboration among different disciplines as each group brought a unique perspective on how to close the health care gap for individuals with SMI. 320 Emma R. Toner, B.A., Psychiatry Exploring the effect of shame and guilt on post-loss psychopathology in bereaved Boston, MA, USA, 2Harvard University, Cambridge, MA, USA, 3Temple University, Philadelphia, PA, USA and 4Harvard Medical School, Boston, MA, USA Introduction: Guilt and shame are moral emotions that tend to coincide with self-blaming cognitions. Guilt arises when one negatively evaluates a specific behavior (i.e., \"I did a bad deed\"), whereas shame arises when one negatively evaluates the whole self (i.e., \"I am a bad person\"). Shame and guilt correlate with symptoms of anxiety and depression (see Tangney 275et al. 2007, Annual Review of Psychology). However, when one controls for the shared variance between shame and guilt, only shame remains a consistent predictor of psychopathology. Indeed, guilt independent of shame may even be adaptive, prompting guilt-motivated reparative behaviors (e.g., apologizing) that lead to positive outcomes. It is unclear how guilt may relate to psychopathology when reparative behaviors are impossible, such as following the death of a loved one. Self-blame following bereavement has been implicated in the development of post-loss psychopathology; however, studies have not distinguished between shame and guilt when measuring self-blame. To address this gap in the literature, we examined bereavement-related state shame and guilt and their relationship to complicated grief (CG) and depression in bereaved adults. Methods: Participants were 92 adults (65.2% female) who ranged in age from 21-65 years old (M = 45.12, SD = 12.51), and had experienced the death of a family member at least 1 year prior to study entry. All completed the Inventory of Complicated Grief (ICG), the Quick Inventory of Depressive Symptomatology (QIDS), and the State Shame and Guilt Scale (SSGS) during a laboratory visit as part of a larger study.Results: Both shame (r = .45, p < .001) and guilt (r = .39, p < .001) were positively correlated with ICG scores. In multiple regression analyses that controlled for the shared variance between shame and guilt, shame remained a significant predictor of ICG scores (B (se) = 1.59(.53), p = .003), whereas guilt did not (B (se) = .82(.42), p = .056). We conducted follow-up analyses to examine whether shame moderated the effect of guilt on ICG scores. Results indicated a significant interaction between shame and guilt (B(se) = -.19(.08), p = .020). We probed this interaction by using the Johnson-Neyman technique, which evaluated the effect of guilt on ICG scores at different levels of shame. At high levels of shame (SSGS shame scores 8.99), the effect of guilt on CG symptoms was negligible. However, at low levels of shame (SSGS shame scores < 8.99), greater guilt surrounding the loss predicted more severe symptoms of CG. We observed the same pattern of results when QIDS scores were the outcome variable.Conclusion: Our findings are consistent with research identifying shame as a more pathogenic emotion than guilt, but also suggest an unexpectedly more nuanced relationship between shame, guilt, and psychopathology. Whereas guilt in the absence of shame may at times be adaptive, we found that guilt surrounding the death of a loved one predicted greater psychological distress at low levels of shame. In the context of bereavement, both shame and guilt may contribute to increased risk for CG and thus could be important targets of clinical intervention. Future studies should explore whether these findings are specific to bereaved individuals. 321 Carlisle E. Topping, Cancer Center Relationship between perceptions of treatment goals and psychological distress in patients with advanced cancer C.E. Topping1, D. Forst1, J. Greer2, R. Nipp1, J. Temel1 and A. El-Jawahri1 1Cancer Center, Massachusetts General Hospital, Boston, MA, USA and 2Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: Several studies have demonstrated discordance between how patients perceive their goal of treatment versus how they perceive their oncologist's goal. Studies evaluating the extent and risk factors of this discordance are lacking. Methods: We conducted a cross-sectional study of 559 patients with incurable lung, gastrointestinal, breast, and brain cancers. We used the Perception of Treatment and Prognosis Questionnaire to assess patients' perceptions of both their treatment goal and their oncologist's goal and categorized responses: 1) patients who reported that both their goal and their oncolo-gist's goal was concordant (either to cure or not to cure); and 2) patients who reported discordant perceptions of their goal versus their oncologist's goal. We assessed patients' psychological distress using the Hospital-Anxiety-and-Depression-Scale and used linear regression to assess the relationship between patients' perceptions of their treatment goal and psychological outcomes.Results: 61.7% of patients reported that both their goal and their oncologist's goal was non-curative; 19.3% reported that both their goal and their oncologist's goal was to cure their cancer; and 19.0% reported discordance between their goal and their perception of the oncologist's goal. Older age (OR=0.98, P=0.01), non-Hispanic ethnicity (OR=0.31, P=0.049), and higher education (OR=0.62, P=0.042) were associated with lower likelihood of reporting discordant goals. Patients with discordant perceptions of their goal and their oncologist's goal reported higher anxiety (B=1.56, P=0.003) compared to those who reported that both their goal and their oncologist's goal was curative. Patients who reported both their goal and the oncologist's goal was non-curative had higher depression symptoms (B=1.06, P=0.013) compared to those who reported that both their goal and the oncologists' goal was curative. Conclusion: One-fifth of patients with advanced cancer report discrepancies between their perceptions of their own and their oncologists' treatment goal which is associated with psychological distress. Tools are needed to identify patients at risk of cognitive dissonance about their prognosis.276322 Bianca A. Trombetta, B.A., Neurology MassGeneral Institute for Neurodegenerative Disease (MIND) Cerebrospinal Fluid Tissue Bank for Biomarker Discovery B.A. Trombetta, P. Kivis\u00e4kk, A.A. Wills, B.C. Carlyle, B.T. Hyman and S.E. Arnold Neurology, Massachusetts General Hospital, Charlestown, MA, USA Introduction: The purpose of the MassGeneral Institute for Neurodegenerative Disease (MIND) Tissue Bank is to collect and distribute cerebrospinal fluid (CSF), paired plasma, and associated clinical information to qualified researchers. CSF reflects the biochemical milieu of the brain. Diverse assays of the nucleic acid, protein, carbohydrate, lipid and metabolic composition of CSF are rapidly improving in their utility. Developing reliable biomarkers will inform diagnosis, mechanistic understanding, and tracking of disease progression. The MIND Tissue Bank contains CSF from subjects diagnosed with a range of neurodegenerative and other central nervous system diseases.Methods: Samples are collected from consenting participants in conjunction with clinically-indicated lumbar punctures through the Outpatient and Inpatient Neurology services at Massachusetts General Hospital (MGH). Subjects are asked to donate 10-15cc of spinal fluid and plasma for research. After sufficient sample has been collected for clinical lab purposes, additional CSF is collected into low-adherence polypropylene tubes. CSF is then aliquotted into 500l fractions and stored in siliconized polypropylene cryotubes at -80 C within 5 hours. Approximately 20cc of blood is drawn using standard venipuncture for plasma extraction. The participant's medical record number is assigned a de-identified alphanumeric code which exists on a limited-access, secure database. Clinical data is obtained through a systematic review of subjects' electronic medical record. De-identified samples are stored at MGH for distribution to qualified researchers. Results: The MIND Tissue Bank has collected CSF and curated clinical data from over 650 subjects to date. 102 of these subjects consented to donating paired plasma following the addition to the study protocol in 2018. Ages range from 18 to 100 years of age (mean = 59 years, standard deviation = 17.8 years), including 328 (49.5%) males and 335 (50.5%) females. Diagnoses are ascertained based upon review of all available records. The most common diagnoses in this cohort are Alzhei - mer's disease (AD, n=136), idiopathic and normal pressure hydrocephalus (NPH, n=51). The Bank has also collected samples from the following diagnoses: peripheral neuropathy, autoimmune etiology, neuropathy, dementia, Parkin-son's disease, multiple sclerosis (MS), primary progressive aphasia (PPA), motor neuron disease, paraneoplastic syndrome, posterior cortical atrophy (PCA), Lewy body system progressive supranuclear palsy (PSP), traumatic brain Injury (TBI), corticobasal degeneration, and Creutzfeldt-Jakob disease (CJD). Several ongoing and completed research studies have requested CSF from the MIND Tissue Bank to investigate diseases of the nervous system. For many cases with neurodegenerative diseases, longitudinal data is available via standardized follow-up assess- ments in the MGH Memory Disorders Unit. Conclusion: Due to the broad scope of inclusionary criteria and a wide range of diagnoses, researchers can request de- identified samples from subjects with diverse diseases of interest. We look forward to continuing the expansion of this database and collaborating with researchers pursuing biomarker discovery projects. 323 Alexander C. Tsai, Psychiatry College affirmative action bans and smoking and alcohol use among underrepresented minority adolescents in the United States: A difference-in-differences General Hospital, Boston, MA, USA, 2Department of Medical Ethics and Health Policy, University of Pennsylvania Perelman School of Medicine, Philadelphia, PA, USA, 3Analysis Group, Boston, MA, USA, 4La Follette School of Public Affairs, University of Wisconsin, Madison, WI, USA, 5Social and Behavioral Sciences, Harvard T.H. Chan School of Public Health, Boston, MA, USA and 6Health Care Policy, Harvard Medical School, Boston, MA, USA Introduction: College affirmative action programs seek to expand socioeconomic opportunities for underrepresented minori- ties. Between 1996 and 2013, 9 US states\u2014including California, Texas, and Michigan\u2014banned race-based affirmative action in college admissions. Because economic opportunity is known to motivate health behavior, banning affirmative action policies may have important adverse spillover effects on health risk behaviors. We used a quasi-experimental research design to evaluate the association between college affirmative action bans and health risk behaviors among underrepresented minority (Black, Hispanic, and Native American) adolescents.277Methods: We conducted a difference-in-differences analysis using data from the 1991-2015 US national Youth Risk Behavior Survey (YRBS). We compared changes in self-reported cigarette smoking and alcohol use in the 30 days prior to survey among underrepresented minority 11th and 12th graders in states implementing college affirmative action bans (n=9 states) versus outcomes among those residing in states not implementing bans (n=35 states). We also assessed whether underrepresented minority adults surveyed in the 1992-2015 Tobacco Use Supplement to the Current Population Survey (TUS-CPS) who were exposed to affirmative action bans during their late high school years continued to smoke cigarettes between the ages of 19 and 30 years. All regression models adjusted for individual demographic characteristics, state and year fixed effects, and state-specific secular trends. Results: In the YRBS (n = 34,988 to 36,268, depending on the outcome), cigarette smoking in the past 30 days among underrepresented minority 11th-12th graders increased by 3.8 percentage points after exposure to an affirmative action ban (95% CI: 2.0, 5.7; p < 0.001). In addition, there were also apparent increases in past-30-day alcohol use, by 5.9 percentage points (95% CI: 0.3, 12.2; p = 0.041), and past-30-day binge drinking, by 3.5 percentage points (95% CI: 0.1, 7.2, p = 0.058), among underrepresented minority 11th-12th graders, though in both cases adjustment for multiple comparisons resulted in failure to reject the null hypothesis (adjusted p = 0.083 for both outcomes). Underrepresented minority adults in the TUS-CPS (n = 71,575) exposed to bans during their late high school years were also 1.8 percentage points more likely to report current smoking (95% CI: 0.1, 3.6; p = 0.037). Event study analyses revealed a discrete break for all health behaviors timed with policy discussion and implementation. No substantive or statistically significant effects were found for non-Hispanic White adolescents, and the findings were robust to a number of additional specification checks. The limitations of the study include the continued potential for residual confounding from unmeasured time-varying factors and the potential for recall bias due to the self-reported nature of the health risk behavior outcomes.Conclusion: In this nationally representative study of US adolescents, we found that rates of cigarette smoking among underrepresented minority adolescents increased after exposure to affirmative action bans. Concern about these acute and contemporaneous adverse effects was corroborated and further magnified by our finding, in a separate dataset, that the apparent effects of affirmative action bans on smoking persisted into young adulthood. As expected, we found no evidence of changes in health risk behaviors after affirmative action bans in our falsification sample of non-Hispanic White individuals, and results were robust to several other specification checks designed to probe the key causal identification assumptions of our difference-in-differences model. These findings suggest that social policies that shift socioeconomic opportunities could have meaningful population health consequences. Difference-in-differences estimates for underrepresented minority and non-Hispanic White respondents Estimates are presented as percentage point changes, which are computed by taking the linear probability model regression coefficient and multiplying by 100. We computed p-values for each YRBS outcome, within each racial/ethnic group, using the Sidak-Holm step-down method, which adjusts for the family-wise error rate. Event study estimates for underrepresented minority respondents278324 Alexander C. Tsai, MD, Psychiatry Association between automotive assembly plant closures and opioid overdose mortality in the Hospital and Harvard Medical School, Boston, MA, USA, 2Medical Ethics and Health Policy, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, USA and 3Sociology, Yale University, New Haven, CT, USA Introduction: Fading economic opportunity has been hypothesized as an important driver of the U.S. opioid overdose crisis. The closure of an automotive assembly plant represents a culturally and economically significant in many regions of the U.S. The purpose of this study was to estimate the extent to which automotive assembly plant closures were associated with increasing opioid overdose mortality rates among working-age adults.Methods: We used a quasi-experimental, retrospective cohort study spanning 1999 to 2016, analyzing changes in age- adjusted, county-level opioid overdose mortality rates before versus after automotive assembly plant closures in manufac - turing counties affected by plant closures, compared with changes in manufacturing counties unaffected by plant closures. The study setting included 112 manufacturing counties (located primarily in the U.S. South and Midwest) located in commuting zones with at least one operational automotive assembly plant as of 1999. Participants included adults aged 18-65 years. The primary exposure was closure of automotive assembly plants in the commuting zone of residence. The primary outcome was the county-level age-adjusted opioid overdose mortality rate. Secondary outcomes included the overall drug overdose mortality rate, the prescription overdose mortality rate, and the illicit drug overdose mortality rate. Estimates were stratified by age, sex, and race/ethnicity.Results: During the study period, 29 counties were exposed to an automotive assembly plant closure, while 83 counties remained unexposed. Automotive assembly plant closures were associated with statistically significant increases in opioid overdose mortality in manufacturing counties. By 5 years after a plant closure, there were 8.6 more opioid overdose deaths per 100,000 (95% CI: 2.6, 14.7, p=0.006) in exposed counties compared to unexposed counties (Figure 1). In analyses strati - fied by age, sex, and race/ethnicity, the largest increases in opioid overdose mortality were observed among non-Hispanic White men 18-34 years of age (20.1 deaths per 100,000; 95% CI: 8.8, 31.3, p=0.001) and years deaths per 100,000; 95% CI: 5.7, 20.0, p=0.001). We observed similar patterns of prescription vs. illicit drug overdose mortality. Estimates for non-manufacturing counties were not statistically significant (Figure 2). Conclusion: During 1999-2016, automotive assembly plant closures were associated with large increases in opioid overdose mortality. These findings highlight the potential importance of fading economic opportunity as a driver of the U.S. opioid overdose crisis. Figure 1. Event study estimates of differences in opioid overdose mortality rates (primary outcome) and overall drug overdose mortality rates, prescription drug overdose mortality rates, and illicit drug overdose mortality rates (secondary outcomes) for each year before versus after automotive assembly plant closures in exposed versus unexposed manufacturing counties Figure 2. Event study estimates of differences in opioid overdose mortality rates for each year before versus after automotive assembly plant closures in exposed versus unexposed non- manufacturing counties279325 Chieh-En J Tseng, Athinoula A. Martinos Center for Biomedical Imaging Epigenetic mechanisms in autism spectrum disorder (ASD) studied with Biomedical Imaging, Massachusetts General Hospital, Charlestown, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Lurie Center for Autism, Massachusetts General Hospital, Lexington, MA, USAIntroduction: One in 59 school-aged children develop autism spectrum disorder (ASD) (Baio et al., 2018). However, the etiology of this highly prevalent disorder is still unknown. While ASD has high heritability, genetic factors cannot fully explain its occurrence, and environmental factors appear to contribute (Chaste & Leboyer, 2012). Epigenetic processes, such as the histone deacetylase (HDAC) family of enzymes, can alter gene expression without changing the DNA sequence, a mechanism through which environmental factors can influence genetic programs. Population, histone acetylome-wide association, and preclinical studies all suggest that HDAC alterations are present in ASD (Christensen et al., 2013; Kataoka et al., 2013; Moldrich et al., 2013; Qin et al., 2018; Sun et al., 2016). To date, no study has investigated HDAC expression in the living human brain of individuals with ASD. Positron emission tomography-magnetic resonance imaging (PET-MRI) with [ 11C]Martinostat allows us to image HDAC expression in vivo (Wey et al., 2016). The aim of this study is to use [11C]Martinostat PET-MRI to determine if HDAC alterations are present in the brain of individuals with ASD. Methods: Six participants with ASD (5M/1F, mean age = 26.8\u00b17.3 years) and six control participants (CON) (5M/1F, mean age = 26.7\u00b15.3 years) completed a [ 11C]Martinostat PET-MRI scan at the A. A. Martinos Center for Biomedical Imaging in the MGH Charlestown Navy Yard campus. Individuals with ASD met diagnosis according to DSM-5, supported by standard cutoff scores on the Autism Diagnostic Observation Schedule, second edition (ADOS-2; Lord et al., 2012) and Autism Diagnostic Interview-Revised (ADI-R; Rutter, Le Couteur, & Lord, 2003), and did not have epilepsy. Controls did not have a first-degree relative with ASD, or a history of neurological or psychiatric disorders. All participants had no PET-MRI contraindications, were not taking Depakote (valproic acid) or any illicit drugs or marijuana. Participants were scanned on a simultaneous PET-MRI that consisted of a 3T Siemens TIM Trio with a BrainPET insert. [ 11C]Martinostat (ASD mean dose: 5.3\u00b10.4 mCi; CON mean dose: 4.8\u00b10.3 mCi) was injected and PET data were collected from 60-90 minutes post-injection. A multi-echo magnetization prepared rapid acquisition gradient echo (MEMPRAGE) et al., 2012) was used. Standard uptake value (SUV) maps normalized to the whole brain mean and SUV ratio (SUVR) maps were generated. SUVR maps were registered to the MNI template and smoothed with a 8mm kernel. Whole brain voxelwise analysis was performed using FSL's FEAT (FMRIB software library) with ordinary least squares mixed-effects modeling. Age and sex were entered as regressors of non-interest and the statistical threshold was set at Z score>2.3, with a cluster- corrected P=0.05. Results: Lower HDAC orbitofrontal cortex, insula, and globus pallidus in ASD compared to CON (Figure 1). These brain regions are associated with socio-cognitive processing and have previously been implicated in ASD in functional MRI studies (Adolphs, 2001). At this statistical threshold, there were no regions where HDAC expression was increased in ASD compared to CON was found.Conclusion: This is the first time HDAC expression has been studied in ASD in vivo. We show interesting findings of decreased HDAC brain expression in ASD compared to CON. A decrease in HDAC expression is also in line with previous reports of decreased acetylation near HDAC genes in ASD (Sun et al., 2016). [ 11C]Martinostat PET-MRI data from a larger sample size will be required to determine if HDAC alterations represent a common feature in ASD. Further, we are currently investigating how HDAC alterations relate to ASD-associated symptoms. If dysregulation of epigenetic enzymes HDACs is observed, findings from this work can potentially open avenues for HDAC-related therapeutic interventions in ASD.280 Figure 1. A) Group mean SUVR of the ASD (n=6) and CON (n=6) groups. B) Regions of decreased HDAC expression in the ASD group compared to the CON group. 326 Sarah E. Turbett, MD, Medicine - Infectious Diseases Acquisition of highly antibiotic-resistant bacteria by U.S. international travelers S. Turbett2, E. Oliver1, G. Mellon1, A. MGH, MA, USA, 2Microbio, MGH, Boston, MA, USA and 3Broad Institute, Cambridge, MA, USA Introduction: Antibiotic-resistant bacteria are an increasing health threat in the U.S., with an estimated 2 million cases and 23,000 deaths annually. The polymyxin antibiotic colistin and antibiotics in the carbapenem class are the 'last-resort' treatment for highly antibiotic-resistant Gram-negative organisms. Increasing evidence shows that international travelers asymptomatically acquire antibiotic-resistant bacteria in the gut while abroad and may contribute to the global spread of these medically important organisms.Methods: We recruited participants among international travelers seen in the Global TravEpiNet network (GTEN) immedi - ately prior to and following international travel. GTEN is a consortium of 18 clinics across the US. Data were collected from November 2017 to April 2019. Participants were invited to performed stool samples before and after their trip. We used selective media to identify carbapenemase-producing carbapenem-resistant Enterobacteriaceae (CP-CRE) and mcr-mediated colistin for extended spectrum beta-lactamase production (ESBL). Results: We collected a total of 412 stool samples. We identified 20 U.S. international travelers (5%) who acquired MCR organisms, and two who acquired CP-CREs. The most common destinations associated with acquisition were South-East Asia and Peru; three travelers to South-East Asia acquired multiple MCR strains. All resistant organisms were E. coli . Fourteen of the colonized travelers (67%) experienced diarrhea on their trip and 7 (33%) self-treated with antibiotics. We performed whole-genome sequencing and polymerase chain reaction on the bacterial isolates. Most MCR strains carried the mcr-1.1 gene, although two travelers returning from South-East Asia carried strains with mcr-3.1, in one case co-located with mcr-1.1. One strain from a traveler to Liberia carried a novel variant of mcr-1, differing by a single amino acid from mcr-1.1. Six of the 24 MCR strains additionally carried bla CTX-M genes conferring extended spectrum beta-lactam (ESBL) resistance. One traveler to Vietnam was colonized with two distinct strains carrying an identical 16kb plasmid-borne multidrug resistance element containing mcr-3.1, as well as genes conferring ESBL resistance (bla CTX-M-55), aminoglycoside resistance (aac3-IIa) and fluoroquinolone resistance (qnrS1). Both also carried genes conferring ESBL resistance (blaNDM). Conclusion: These are the first cases of mcr-mediated colistin resistance to be identified among U.S. international travelers. These MCR acquisition rates are similar to previous, smaller European studies, but we additionally observed multiple strain 281carriage, a diversity of mcr variants, and multidrug resistance associated with MCR acquisition. The co-localization of a genetic element that confers resistance to colistin with multiple other resistance elements on two transmissible plasmids is of high concern. Prevention measures and screening policies, including genomic surveillance, for U.S. international travelers may be needed to help contain the global spread of drug resistance elements. 327 Mai Uchida, M.D., Psychiatry Cingulum Bundle Anomalies in Children with Emotional Dysregulation: A Diffusion Tensor Imaging Study M. Uchida1,3, Y. J.D. Gabrieli2 Biederman1,3 and Research Programs in Pediatric Psychopharmacology and Adult ADHD, Massachusetts General Hospital, Boston, MA, USA, 2Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA, USA and 3Psychiatry, Harvard Medical School, Cambridge, MA, USA Introduction: Emotional dysregulation in youth has been frequently shown to predispose individuals to adverse health outcomes and increase risk for mood disorders. However, the underlying neurodevelopmental mechanism of emotional dysregulation remains unclear. The goal of this study is to determine whether pediatric emotional dysregulation is attributed to specific neural abnormalities using advanced neuroimaging techniques. Methods: 43 healthy children and children with emotion regulation difficulties (mean age, 10.3 years; standard deviation, 2.3 years) underwent diffusion tensor imaging. The imaging yielded diffusivity measures (i.e., fractional anisotropy, mean diffusivity, radial and axial diffusivity), which characterized the strength of directionality in neural connectivity along the brain's connection pathways. Emotional dysregulation severity was measured by the empirically-derived Child Behavior Checklist Emotional Dysregulation Profile, including Attention, Aggression, and Anxiety/Depression subscales. Whole-brain tests demonstrated diffusivity in the dorsal cingulum bundle and the anterior corpus callosum areas significantly increased with higher emotional dysregulation severity.Conclusion: This is the first study to provide whole-brain structural connectivity evidence in youth. Our results suggest impaired microstructural connectivity in the cingulum bundle pathways may underlie neurodevelopmental susceptibility for mood disorders as implicated in pediatric emotional dysregulation. Moreover, altered dorsal cingulum and anterior corpus callosal connectivity may manifest as a susceptibility neural biomarker or a developmental precursor of a pathological course toward mood disorders. Our findings provide biologically-relevant neurodevelopmental targets as well as contribute to improving early identification and prevention efforts aimed to mitigate the compromised course of mood disorders for susceptible cohorts. 328 Nneka Ufere, Medicine Multicenter Study of Influence of Gender on Resident Assessment in Internal Medicine Residency Training Programs N. Ufere1, R. Klein2, S. Rao5, A. Warner1, V. Thompson6 and K. Palamara-McGrath1 1Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Medicine, Emory University School of Medicine, Atlanta, GA, USA, 3Medicine and Pediatrics, University of Chicago, Chicago, IL, USA, 4Medicine, University of Alabama, Birmingham, Birmingham, AL, USA, 5Biostatistics Center, Massachusetts General Hospital, Boston, MA, USA, 6Medicine, University of California, San Francisco, San Francisco, CA, USA and 7Medicine, University of Louisville, Louisville, KY, USA Introduction: The Accreditation Council for Graduate Medical Education (ACGME) adopted the Milestones-based Next Accreditation System in 2013, which provides a standardized framework for faculty to quantitatively evaluate residents across 6 core competency domains: Patient Care, Medical Knowledge, Systems-Based Practice, Practice-Based Learning & Improvement, Professionalism, and Interpersonal & Communications Skills. While evaluations are an important aspect of resident professional development, the influence of gender on evaluations of internal medicine trainees is unknown. This study aims to assess the influence of gender on resident assessment in Internal Medicine residency training.Methods: We conducted a multicenter, retrospective, cross-sectional cohort study involving 6 internal medicine training programs across the United States from July 2016 to July 2017 using data from faculty evaluation forms of internal medicine trainees based on the ACGME milestones-based evaluation system. The study examined 3,600 faculty evaluations of 700 internal medicine residents by 604 faculty members. Because evaluation scales varied across training programs, we generated standardized rating scores based on the rating distribution, mean and standard deviation within each site. We calculated standardized scores for each core competency domain and report these as standard deviations from the mean. To examine 282the association between resident gender and scores obtained on each core competency domain, we used linear mixed effects models adjusting for time of year evaluated, rotation setting, faculty rank, faculty division, residents' percentile scores on the Internal Medicine In-Training Examination, and for clustering by study sites. We additionally assessed the interactions of resident gender, faculty gender, and post-graduate year (PGY) of training on scores across all 6 core competencies. Results: A total of 3,600 evaluations were collected for 700 residents (55% male, 45% female) by 604 faculty members (53% male, 47% female) during the study period. Overall, 45% of assessments were by female faculty and 46% of assess-ments were of female residents. We found a significant interaction between resident gender and post-graduate year (PGY) in standardized scores across the 6 core competencies. During the first year of training (PGY-1), there were no statistically significant difference in standardized scores for female and male residents across the 6 core competencies. In the second year of residency (PGY-2), female residents scored significantly higher than male residents in 4 of 6 domains: Patient Care (0.222 vs. 0.096, p = 0.03), Systems-Based Practice (0.128 vs. -0.043, p < 0.01), Professionalism (0.204 vs. -0.039, p < 0.01), and Interpersonal & Communications Skills (0.318 vs. 0.077, p < 0.01). By the final year of residency (PGY-3), female residents scored significantly lower than male residents in 5 of 6 domains: Patient Care (0.308 vs. 0.453, p = 0.04), Medical Knowledge (0.227 vs. 0.462, p <0.01), Systems-Based Practice (0.109 vs. 0.286, = 0.02), Problem-Based Learning & Improvement (0.145 vs. 0.381, p < 0.01), and Professionalism (0.162 vs. 0.331, p = 0.03). Comparing the overall standardized scores across post-graduate years, we found a significant difference in scores between female and male residents in all 6 core competen - cies (p 0.01) [Figure 1]. We did not find any statistically significant differences in standardized scores between female and male residents based on faculty gender across in all 6 core competencies; however, we did find differences in scores based on faculty-resident gender pairings. Male residents received significantly higher evaluation scores from male faculty compared to female faculty in 5 of 6 domains: Patient Care (0.082 vs. -0.053, p < 0.01), Medical Knowledge (0.034 vs. -0.079, p = 0.01), Systems Based Practice (-0.056 vs. -0.184, p < 0.01), Problem-Based Learning & Improvement (0.085 vs. -0.065, p < 0.01), and Professionalism (0.044 vs. -0.104, p < 0.01). Female residents received significantly lower evaluations scores from female faculty compared to male faculty in 3 of 6 domains: Systems-Based Practice (-0.149 vs. -0.049, p = 0.03), Professionalism (-0.049 vs. 0.076, p = 0.01), and Interpersonal Communication (-0.028 vs. 0.090, p = 0.02).Conclusion: We observed gender differences in evaluations of internal medicine residents that varied based on year of training; despite female residents receiving higher scores in multiple core competency domains than their male counterparts by PGY-2, male residents outperformed female residents in almost all core competency domains by PGY-3. We addition - ally found differences in scores between female and male residents based on faculty-resident gender pairings. This study highlights the need to assess for factors contributing to this gender gap in evaluations as trainees progress through residency toward independent practice in order to develop interventions to mitigate gender bias. 329 Amy Daniel Ulumben, Radiology Real-Time Imaging of Vaccines Using a Zwitterionic NIR Fluorophore A. Ulumben Gordon Center for Medical Imaging, Massachusetts General Hospital, Boston, MA, USA Introduction: The efficient delivery of vaccines to the secondary lymphoid tissue is essential to confer protective immune responses with vaccination. In order to optimize vaccine formulations, a reliable method to determine the longitudinal biodis-tribution of injected vaccine antigens in vivo is critical, but only insufficient methodologies are available currently, as most studies focus on monitoring immune cell behavior only. In response to this, we have chosen a near-infrared (NIR), zwitte - rionic fluorophore offering numerous advantages for bioimaging of vaccines including minimal interaction with biological tissues and ultralow background, direct conjugation with biomolecules via conventional N-hydroxysuccinimide (NHS) ester chemistry and low toxicity with rapid renal clearance.283Methods: To determine the biodistribution of vaccine antigen in mice, we conjugated an NHS form of zwitterionic NIR fluorophore ZW800-1C to silica nanoparticles with different diameters on the surface of model vaccines. A multispectral real-time fluorescence imaging system (the KFLARE system) was used to monitor the behavior of the conjugated vaccine in an established mouse model of intradermal vaccination up to 72 hours. Biodistribution of the conjugated vaccine was determined by quantifying the fluorescence signal in lymph nodes and major organs on the NIR images. We also compared the renal clearable ZW800-1C with a conventional organic dye Cy5 in this system.Results: A model vaccine 10 nm in size was observed to be transported through the lymphatic vessels immediately after the injection, with the signal-background ratio (SBR) in the draining lymph node peaking at 1.57 \u00b1 0.31 1 hour after injection. In contrast, larger 180 and 270 nm model vaccines showed moderate kinetics with SBRs peaking at 1.24 \u00b1 0.16 and 1.16 \u00b1 0.18 at 6 hours after the injection, respectively. As expected, ZW800-1C-conjugated vaccine showed much higher SBR compared to Cy5-conjugates (Figure), confirming that the zwitterionic ZW800-1C is a reliable tool to monitor the fate of injected vaccine. Conclusion: Together, these results suggest that the NIR fluorescence signals derived from ZW800-1C faithfully represents the in vivo behavior of vaccines under the real-time NIR imaging system. This imaging platform would offer a reliable tool for determination of an optimal formulation, dose, and injection route of clinical vaccines. Since this conjugation strategy is biocompatible, this imaging strategy can be broadly applied for the efficacy and safety evaluation of immunotherapeutics and biologics. 330 Sofia Uribe, Bachelor s, Psychiatry Psychotropic medication effects on repetitive transcranial magnetic stimulation (rTMS) treatment response S. Uribe, C.J. Funes, V. Ho, J.A. Camprodon and T. Barbour Psychiatry, MGH, Boston, MA, USA Introduction: Repetitive transcranial magnetic stimulation (rTMS) is an established treatment for major depressive disorder (MDD). A treatment course of rTMS consists of 36 treatment sessions. Many of the patients who receive rTMS for MDD are also on a regimen of concomitant psychotropic medications. However, it is unclear how psychotropic medications and rTMS interact to affect treatment response. Therefore, to investigate this critical question, we retrospectively examined this interaction in patients receiving rTMS treatment for MDD. Methods: We analyzed the relationship between treatment response to rTMS and the rate of the response to the use of medica- tions divided by medication class. Our patient population consisted of 181 patients diagnosed with treatment-resistant MDD, ages 15-80, who received a treatment course of rTMS in a clinical setting. Depressive symptoms were assessed at baseline and every ten sessions using the Hamilton Depression Scale (HAMD-17). Medication use per class was also recorded at baseline and sessions 10, 20, 30, and 36. We performed a multi-level mixed-effects general linear model using the change in HAMD-17 score from baseline as the dependent variable; time (session number), medication use antipsychotics, stimulants, anxiolytics, and sedative-hypnotics), and the interaction between time and medication use by class were used as independent variables of interest. Time was treated as a continuous variable and medication use by class as a factor variable. Baseline HAMD-17 score, age, gender, stimulation dose (120% MT), and cumulative number of stimulation pulses were included in the model as covariates. Subject identification was included in the model as a random effects variable.Results: As expected, there was a main effect of time (z = -6.84, p = 0.00001), meaning that depressive symptoms improved with more rTMS treatments. There was also a significant interaction between time and anticonvulsant use (z = -3.12 p = 0.002). Surprisingly, this interaction was due to greater rate of depressive symptom improvement in those taking anticonvul- sants (n = 79) than those not taking anticonvulsants benzodiazepine, antidepressant, antipsychotic, stimulant, anxiolytic, or sedative-hypnotic use on change of depressive symptoms.Conclusion: Our findings indicate that anticonvulsant use improves rTMS response for MDD. This may be due to modulation of glutamate, as glutamate has a role in neuroplasticity. A better understanding of the interaction between medications and rTMS can help optimize rTMS treatment for MDD. 331 Stylianos Vagios, OB/GYN Assessment and Selection of Human Embryos Destined to Implant using Deep Convolutional Neural Networks (CNN) and H. Shafiee2 1Obstetrics/Gynecology/Reproductive Endocrinology and Infertility, Massachusetts General Hospital Fertility Center, Boston, MA, USA and 2Division of Engineering in Medicine, Department of Medicine, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, USA Introduction: To evaluate whether an artificial intelligence (AI) framework can be reliably used to select embryos for single embryo transfer (SET) at the cleavage and blastocyst stages. Methods: Design: Historical Prospective Cohort Study. A dataset of 3,469 embryos was used to train and test the CNN model created to classify images of embryos captured at 70 hours post insemination (hpi) and 113 hpi. The CNN was evaluated in selecting an embryo for SET at 70 hpi and 113 hpi using embryo cohorts of 97 patients with implantation outcomes after fresh transfer. A second analysis was performed incorporating implantation outcomes from prospective data of cryopreserved embryos that were thawed and transferred after selected by the CNN. Results: A total of 748 embryos from 97 patients were examined by the CNN. All 748 embryos were imaged at 70 hpi and 742 of them were imaged at 113 hpi. The CNN selected a single embryo from each of the 97 embryo cohorts. Fresh implantation outcomes were available for 22 of the embryos selected at 70 hpi and for 44 of them selected at 113hpi. The accuracy of the CNN to select an embryo for SET at 70 hpi and 113 hpi based on implantation outcomes was 54.5% and 59.1%, respectively compared to 44.1% implantation rate of 102 transferred blastocysts at our institution. Of note, average implantation rates in the literature for manual-based embryo selection and transfers at the cleavage and blastocyst stages are approximately 27% and 34%, respectively (1). In addition, 4 cryopreserved embryos selected by the CNN at 70 hpi and 5 at 113 hpi were transferred after being thawed. Of the 4 frozen embryos selected by the CNN at 70 hpi and 5 at 113 hpi, 3 and 4 embryos, respectively, led to successful implantation outcomes. When both fresh and frozen embryo transfers were considered, the accuracy of the CNN in successful SET at 70 hpi and 113 hpi, was 57.7% and 61.2%, respectively compared to 48.5% for blastocyst transfers at our institution.Conclusion: In this study, we demonstrated that the CNN we developed has the ability to select embryos for SET, at both the cleavage and blastocyst stages, with higher implantation rates compared to traditional manual selections. Thus, the incorpo- ration of artificial intelligence technology in an embryology laboratory may aid in improved clinical outcomes. References: W. P. Martins, C. O. Nastri, L. Rienzi, S. Z. van der Poel, C. Gracia, C. Racowsky, Blastocyst vs cleavage-stage embryo transfer: systematic review and meta-analysis of reproductive outcomes. Ultrasound in obstetrics & gynecology: the official journal of the International Society of Ultrasound in Obstetrics and Gynecology 49, 583-591 (2017) Support: This work was partially supported by the Brigham Precision Medicine Developmental Award (Brigham Precision Medicine Program, Brigham and Women's Hospital) and R01AI118502, R01AI138800, and R21HD092828 (National Institute of Health). 332 Matthew W. Vanneman, Anesthesia, Critical Care and Pain Medicine A focused liver transplant echocardiography protocol for intraoperative hypotension and management M.W. Vanneman, A. Dalia, J. Crowley, K. Luchette, H. Chitilian and K. Shelton Anesthesia, Critical Care and Pain Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Transesophageal echocardiography (TEE) for liver transplantation (LT) provides unique diagnostic informa- tion and is associated with changes in intraoperative management. However, TEE is inconsistently deployed in LT. Anesthe- siologists report complexity and lack of training as two barriers to more frequent TEE use in LT. Simplified, focused TEE exams have been successfully developed for diagnostic \"rescue\" in patients with hemodynamic instability in other settings. The impact of a focused TEE protocol on patients undergoing LT is unknown.285Methods: An abbreviated, 5-view \"Liver TEE\" protocol was adapted from our institutional \"Rescue TEE\" protocol, and assessed in a retrospective cohort of 106 patients at our institution who received TEE during LT. The pre-specified primary outcome was the frequency the Liver TEE protocol detected a composite of five pre-specified causes of hypotension. Post-hoc analysis investigated intraoperative management changes made within 10 minutes of TEE images.Results: TEE detected 87 abnormal findings in 43 of 106 patients (41%). There were 24 occurrences of one of the pre- specified causes of hypotension in 16 of 106 patients (15%); the Liver TEE protocol detected 22 of 24 (92%, 95% CI: 74%-98%) occurrences. Intraoperative management changes occurred in 15 of 16 (94%) patients with one of the pre-specified causes of hypotension, compared to 1 of 27 (3.7%) patients with TEE findings outside those diagnoses (p < 0.001). Conclusion: In a retrospective cohort of 106 patients undergoing LT with TEE, a focused, 5-view Liver TEE protocol successfully detected 92% of pre-specified causes of hypotension and was correlated with management changes in 94% of those patients. Larger, prospective studies including non-expert echocardiographers are needed to further assess the protocol's generalizability and risk. 333 Dagny Vaughn, BA, Psychiatry Randomized Trial of a Hospice Video Decision Aid for Patients with Advanced Cancer and their Caregivers D. Vaughn, O. Vanbenschoten, A. Fenech, L. Traeger, J. Greer, A. Volandes, J. Temel and A. El-Jawahri Massachusetts General Hospital, Boston, MA, USA Introduction: Background: Although hospice provides high-quality end-of-life care for patients with advanced cancer, the service remains underutilized in part due to lack of adequate information provided to patients and families about hospice care. Methods: Methods: We conducted a single-site randomized clinical trial of a hospice video decision aid versus a verbal description in 150 hospitalized patients with advanced cancer and their caregivers. Patients without an available caregiver were eligible to participate. Intervention participants (75 patients; 18 caregivers) received a verbal description about hospice plus a six-minute video depicting hospice care. Control participants (75 patients; 26 caregivers) received only the verbal description. The primary endpoint was patient preference for hospice care immediately after the intervention, adjusting for baseline preferences. Secondary outcomes included patient and caregiver knowledge and perceptions of hospice, and hospice 2/2017 and 1/2019, we enrolled 55.7% (150/269) of potentially eligible patients and 44 caregivers. Post-intervention, patients assigned to the video group were more likely to prefer hospice care (86.7% vs. 82.7%, OR=2.85, P=0.08), but this was not statistically significant. Patients in the video group reported greater knowledge about hospice (B=0.50, P=0.024) and were less likely to endorse that hospice care is only about death (6.7% vs. 21.6%, OR=0.28, P=0.035). Among patients who died (n=116), those assigned to the intervention were more likely to utilize hospice (85.2% vs. 63.6%, P=0.01) and had a longer hospice length-of-stay (LOS) (median 12 vs. 3 days, P<0.001). Post-intervention, caregivers assigned to the video were more likely to prefer hospice care for their loved ones (94.4% vs. 65.4%, P=0.031), reported greater knowledge about hospice (B=1.94, P<0.001), and were less likely to endorse that hospice care is only about death (0.0% vs. 23.1%, P=0.066).Conclusion: Conclusions: Patients with advanced cancer and their caregivers who viewed a hospice video decision aid were more informed about hospice, reported more favorable perceptions of hospice, and were more likely to utilize hospice and have a longer hospice LOS. 334 Ajaykumar Vishwakarma, DDS, MS, Medicine - MGH Cancer Center Single-cell Transcriptome Maps the Immune Landscape in Clear Cell Renal Cell Carcinoma A. Vishwakarma1,2,3 1Medicine, MGH/HMS, Ashland, MA, USA, 2Laboratory of Systems Pharmacology, Harvard Medical School, Boston, MA, USA and 3Carver College of Medicine, University of Iowa, Iowa City, IA, USA Introduction: The phenotypic and functional profile of immune cells in the tumor microenvironment is now well known to influence clinical prognosis and disease outcome. Immunotherapies targeting T cells as standard of care has opened new perspective in many cancer types. However, a substantial subset of patients still does not respond to these therapies and patients who initially do can eventually progress. Other abundant immune players in the tumor microenvironment include monocytes, dendritic cells and tumor-associated macrophages that are now being harnessed for therapy but remain poorly understood. Previous research has highlighted vast intra-tumor heterogeneity and states within heterogeneous immune cell 286populations, a common feature across diverse cancer solid types that ultimately shapes anti-tumor response. Understanding how heterogeneity, in relation to the composition and state of both lymphoid and myeloid cell types within the tumor impacts clinical outcome presents a persistent challenge to identifying novel immunotherapy targets. The tumor microenvironment of clear renal cell carcinoma(ccRCC) is immunologically distinct compared to other solid tumor types. In a study comparing favorable prognosis of ccRCC compared to other cancers, the prognostic value of T lymphocytic-specific markers was associ-ated with poor prognosis. Also, the T cell percentage and the absolute number of T cells increased with dedifferentiation of ccRCC. Furthermore, tumor-infiltrating myeloid cells are abundant in the microenvironment of ccRCC but they remain far less studied than T cells. Most studies characterizing the complexity of tumor microenvironment in ccRCC have largely relied on single cell methods e.g. flow cytometry and CyTOF mass cytometry together with bulk RNA sequencing. The single cell methods are restricted to using single antibody targeting known immune cell components which is inefficient of capturing the full phenotypic heterogeneity. Their limitations represent immune cell states in tumor immune microenvironment as discrete phenotypes when in vivo they typically display diverse spectrum of differentiation or activation states. Despite extensive research, the diverse transcriptomic immune cell states in clear renal cell carcinoma, the most common type of malignant renal cancer remains poorly characterized. scRNA-seq has enabled profound characterization of individual immune cells providing deeper insight into the heterogeneity of tumor-immune microenvironment. At present, this is most appreciated for CD8 T cells wherein the field has explored differentiation states of these cells at single-cell level to demonstrate that response to immunotherapy requires specific transcriptional and functional state expressing high amounts of Tcf7/Tcf1. Tumor- associated macrophages have also been observed to polarize pro- and anti-tumor activities existing in a spectrum of plastic cell states. Likewise, with the recent ability to explore TCR, scRNA-seq has allowed measurement of clonal T cell response to cancer at an unprecedented depth identification of neo-antigen reactive T cell clonotypes. Further studies employing high-throughput TCR sequencing patient's peripheral blood have now begun to explore clonal expansion of principal T cell states for monitoring immune response. Methods: Here, we apply single-cell RNA sequencing paired with T cell receptor sequencing to profile transcriptomes of 25,688 individual CD45+ lymphoid and myeloid cells in matched tumor and blood from patients with clear cell renal cell carcinoma. Results: We demonstrate low inter-individual variability in ccRCC that can be explained by extensive overlapping of gene expression profile across three-individuals with renal cancer. We identify 10 T cell phenotypes, 6 myeloid phenotypes, 3 NK and B cell phenotypes; further annotated to spectrum of distinct cell states. We further delineate TCR subclonal structure of T cell states and identify that enriched T cell sequences share commonality among different T cell subsets, but are typically unique to each patient. Conclusion: In our study, single-cell RNA-seq reveals distinct lymphoid and myeloid immune landscape in ccRCC tumors paired with peripheral blood. We show that both CD8 and CD4 T cells exhibit a continuum of cell states within tumor across individuals. The enriched TCR sequences of T cells in patient blood partially overlap with those in their tumors and are shared between subsets of T cells. We define unique markers for intra-tumoral monocytes, dendritic cell subsets and tumor-associated macrophages. This study determines the ccRCC immune landscape contributing to this unique tumor type for identifying potential markers as well as for the future design of immunotherapies. 335 Ha Vo, MPH, Medicine - General Internal Medicine How accurate are surgeons at diagnosing treatment preferences of patients with hip and knee osteoarthritis? M. Mangla2, H. Vo1, S. Daggett3, S. Mwangi4, K. Valentine1 and K. Sepucha1 1Division of General Internal Medicine, Massachusetts General Hospital, Boston, MA, USA, 2New York Institute of Technology College of Osteopathic Medicine, New York, NY, USA, 3Tufts University School of Medicine, Boston, MA, USA and 4New York, New York, NY, USA Introduction: Shared decision making (SDM) is an interactive process between a patient and their clinician. A core skill for SDM is the clinician's ability to understand and elicit their patient's preferences. The goal of this study was to assess the degree to which arthroplasty surgeons accurately assess their patients' preferences for hip or knee osteoarthritis (OA) treatment. Methods: This is a secondary analysis of data collected as part of a larger randomized comparative effectiveness trial. All patients received a decision aid as part of the study and completed a short survey assessing their treatment preference (surgery, non-surgical treatment, not sure) before their initial visit with a surgeon (pre-visit) and again about one week after (post-visit). Five surgeons filled out a 6-item survey for a randomly selected subset of study patients immediately after the visit. Surgeons indicated the patients' treatment preference (surgery, non-surgical treatment, not sure) and their treatment recommendation for the patient (surgery, non-surgery, no recommendation made). Surgeons' perceptions of patients' prefer - ence were compared to both patients' pre-visit and post-visit responses. We hypothesized that the concordance between 287surgeon perceptions and patient stated preference would be higher post-visit compared to pre-visit. Similarly, concordance between surgeon recommendation and patient stated preference will be higher post-visit compared to pre-visit. Percentages of matched pairs for treatment preference and treatment recommendation were compared. In addition, we examined whether concordance was related to patients' gender (female vs male), literacy (high vs low), and age, using 2-sample t-tests for continuous variables and 2 tests for categorical variables. Results: There were 176 provider surveys that were matched with patient survey responses. The patient responders were on average, 63.9 years old, 62% female and 71% had knee OA. 53% of wanted 19% wanted non-surgical options, and remained unsure. Surgeons accurately assessed 58% of patients' pre-visit preferences. For the remaining 42% of patients, the majority (33%) went into the visit unsure, while 9% had a preference in mind. In contrast, surgeons accurately assessed 79% of their patients' post-visit preferences. They correctly indicated that 62% of their patients preferred surgery, 16 % preferred non-sur- gical treatment, and 1% were undecided or unsure. However, the mismatches detected showed that surgeons thought 4% of their patients wanted surgery when they did not, 9% wanted non-surgical when they did not, and 8% were unsure when they had a clear treatment preference. The surgeons' recommendations matched the patient's pre-visit preference for 52% of patients. At post-visit, 46% of patients who preferred surgery and 6% who preferred non-surgical treatment received a matching recommendation from their surgeons. However, 22% of patients were unsure about treatment preference for which the surgeon recommended surgery. Post-visit differences in treatment matches between genders and literacy levels did not arise (p=0.07 and p=0.52 respectively). For pre-visit, matches were significantly younger (61.3 \u00b1 9.1 years) compared to non-matches (65.8 \u00b1 9.4 years), t(168)=-3.090, p=0.002. However, this relationship did not hold for post-visit matches (63.2 \u00b1 9.3 10.6), t(174)=-0.842, p=0.401. Conclusion: About half the time surgeon reports of patient preferences matched their patient-reported preferences. Surgeon reports of patient preference were more consistent with patient preferences post-visit as compared to pre-visit. These findings highlight that many patients coming to see the surgeon are still unclear about what treatment they want and thus the encounter with the surgeon \u2014including the surgeon's recommendation\u2014may be integral to their ultimate decision. In addition, the shifts in patient preference from unsure to selecting a treatment highlight how crucial the conversation between the surgeon and patient is to ensure patients feel comfortable and engaged in their treatment decisions. Routine collection and review of patient preferences by surgeons will promote more systematic and high-quality decision making. Future work will map how patients' preferences can evolve before and after their consultations and how feedback to surgeons about their accuracy understanding patient preferences can enhance SDM during the visit. 336 Andre Vogel, Neurology Sleep, Depression, and Social Integration in People with Poorly Controlled Epilepsy in Bhutan: An Observational Study M. Stauder1, A. Vogel1, L. Tshering2, Dorji2, U. Dema2, D. Nirola2 and F. Mateen2 1Neurology, Massachusetts General Hospital, and 2Jigme Dorji Wangchuk Hospital, Thimphu, Bhutan Introduction: The burden of epilepsy is shared unequally among low-, middle-, and high-income countries. >85% of people with epilepsy (PWE) live in low- and lower-middle-income countries (LLMICs). Although effective and inexpensive epilepsy treatments exist, >70% of people with epilepsy in LLMICs remain untreated. The effects of untreated epilepsy on quality of life are generally negative: PWE report higher rates of depression, anxiety, and suicidal ideation; face social stigma that can prevent educational achievement and employment; and experience sleep disturbances. The inter-related effects of sleep, depression, and social integration are understudied in LLMICs. We performed a structured interview on a cohort of people with poorly controlled epilepsy for sleep quality, social integration, and depressive symptoms in the Kingdom of Bhutan, a lower middle income country in Southeast Asia without neurologists. Our goal was to analyze the possible associations between sleep quality, social integration, and depressive symptoms among people with poorly controlled epilepsy in this setting.Methods: Setting : Bhutan is a landlocked country with a per capita gross national income of 3,080 USD per capita in 2018. Bhutan is bordered by India in the South and South-East and China in the North and North-West. There are no neurologists in Bhutan and limited services for epilepsy. The major medication options for patients include older generation antiepileptic drugs: carbamazepine, phenytoin, phenobarbital, and valproic acid. Three care to patients at the Jigme Dorji Wangchuck National Referral Hospital in Thimphu, the capital city. Participants: PWE, the age of 18 years old and above, who had one or more seizures in the prior year and presented to the Jigme Dorji Wangchuk National Referral Hospital in 2019 were recruited. After consent and a clinical evaluation in the Epilepsy Clinic, each participant was adminis - tered surveys by a trained Bhutanese coordinator. Survey instruments: Participants were administered the Pittsburgh Sleep Quality Index (PSQI), the Berkman-Syme Social Networks Integration survey (SNI), and the Patient Health Questionnaire 288(PHQ-9). The PSQI queries factors related to sleep quality, including sleep latency, duration, and disturbances, and use of sleep medications. Scores for the PSQI range from 0 (high sleep quality) to 21 (poor sleep quality). The SNI evaluates social integration across 4 domains: marital status, close friends and relatives, group participation, and religious participation. Scores for the SNI range from 0 (low social integration) to 4 (high social integration). The PHQ-9 evaluates depressive symptoms and queries patients across 9 questions scored from 0 to 3. Total scores range from 0 (no depressive symptoms) to 27 (high depressive symptom burden). Analyses: We performed Pearson's product-moment correlation tests between PSQI and PHQ-9, PSQI and SNI, and PHQ-9 and SNI. We hypothesized that there would be a positive correlation between PHQ-9 and PSQI and negative correlations between PHQ-9 and SNI and SNI and PSQI. Linear regression using stepwise additive models was then used to determine the relationship between poor sleep quality and depressive symptoms after controlling for potential confounding variables.Results: A total of 86 adult participants were enrolled to date (mean age 30.0 years, standard deviation 8.8, 49% female, average seizure frequency in the past month 1.8). The PSQI score in Bhutanese PWE reflected somewhat reduced sleep quality (mean 3.85 points, standard deviation 2.45 points; range 0-13). The mean social network index was 1.62, standard deviation 0.9. The mean depressive symptom burden was 4.27 points, standard deviation 5.49 points. There was no difference by gender for either PSQI or PHQ9 score, but women reported less social integration compared to men (1.39 points vs. 1.88 points, p=0.014). Sleep quality and depressive symptoms were correlated (r=0.28, p=0.01). After controlling for age, sex, and seizure frequency in the prior month, the effect of depressive symptoms on sleep quality reduction remained statistically significant (p=0.01). There was no statistically significant correlation between social integration and social integration (r=-0.078, p=0.479) or depressive symptoms (r=0.073, p=0.505).Conclusion: We report the inter-relationships of sleep quality, depressive symptom burden, and social integration among a cohort of PWE in a LLMIC. We found that an increase in depressive symptoms was correlated with poor sleep quality among study participants. Although our sample size is limited, we demonstrate relatively low social integration of PWE in this setting, with worse scores among women. These findings may prompt more detailed evaluation on the impact of treatment of depressive symptoms, medication choice, and lifestyle modifications among PWE. They may also encourage social outreach programs to improve the opportunities for PWE to more fully integrate into their communities. 337 Ayan Waite, Anesthesia, Critical Care and Pain Medicine Spectral analyses of propofol-induced neural oscillations in human P. Purdon1 and E. Brown1,2,3 1Anesthesia, Critical Care and Pain Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Picower Institute for Learning and Memory, Massachusetts Institute of Technology, Cambridge, MA, USA and 3Institute for Medical Engineering and Science, Massachusetts Institute of Technology, Cambridge, MA, USA Introduction: General anesthesia (GA) is a drug-induced reversible state characterized by antinociception, unconsciousness, amnesia and akinesia along with maintenance of physiological stability (Brown et al. 2010). Anesthetic agents, e.g. propofol, maintain these behavioral states by binding to specific neuronal receptors and inducing structured oscillations that disrupt communication among brain regions (Brown et al. 2011). Such propofol-induced oscillatory patterns have been observed in power spectra of scalp electroencephalogram (EEG) in humans (Purdon et al. 2013) and local field potentials (LFPs) in non-human primates (NHP) (Ishizawa et al. 2016). Using the modulation of these spectral patterns due to changes in drug infusion rate, one can gauge the unconsciousness level (Purdon et al. 2013) and automatically titrate anesthetic dose in a closed-loop anesthesia delivery (CLAD) system et al. 2019 Human- and NHP-electrophys- iology recordings under propofol anesthesia are relatable owing to the shared anatomical and functional brain homology between humans and macaques. NHP studies can help us better understand observed EEG in humans under anesthesia. However, due to practical constraints, conducting parallel multi-species experiments is a challenge, making it a challenge to compare data sets. Therefore, it can be beneficial to undertake retrospective analyses of data from approximately parallel experiments. We present here a quantitative framework to co-analyze propofol-induced spectral modulation in macaque LFP and human EEG.Methods: The human EEG data are from a previous study (Purdon et al. 2013) with 10 healthy young adult volunteers who underwent propofol-induced unconsciousness while a 64-channel scalp EEG recording was being acquired. For the analysis in each subject, we use the time-series of the estimated effect site concentration of propofol and pre-processed EEG data from one of the frontal channels using Laplacian referencing. The NHP dataset consists of LFP recordings from 2 macaques for a total of 5 sessions. The LFP signals were recorded from the prefrontal cortex using microelectrode arrays while the animal received an intravenous propofol infusion through an ear vein cannula. In both groups, there exists long anesthetic induction periods characterized by monotonically non-decreasing infusion rates. We normalize the corresponding effect-site drug trajectory during the induction period by its maximum value, and treat the result as indicative of the level of anesthetic 289depth in each group. We next select 6 segments (20 seconds each) from each recording session that are equi-spaced between normalized anesthetic levels of 0 and 1. We estimate the within-group median of the difference spectrum for each of the anesthetic levels, 1 through 5, relative to the baseline spectrum at level 0. We estimate the confidence intervals on each of these difference spectra by using a multitaper frequency domain bootstrap (FDB) method. Through this method, we generate 1,000 bootstrap replicates of the relevant statistic, and then determine the 2.5 and 97.5 percentile for each frequency bin (Kim et al. 2018). Finally, for each anesthetic level (1 to 5), we analyze the difference spectra to identify frequencies at which the difference becomes statistically significant. Results: The FDB 95% confidence intervals for the 5 anesthetic levels for the 2 groups are reported in Figs. 1A and 1B for 0-50Hz. For the NHP case, at level 1 there is no significant change with respect to baseline in 0-50 Hz. At level 2, we observe a rise in power in approx. 10-30 Hz band. At level 3, the power spreads over frequency range of approx. 0-30 Hz. At level 4, the difference is high in the approx. 0 - 12 Hz band. At level 5, the frequency bands where the power is significantly high further shrinks to 0-4 Hz. For the human dataset, at level 1 there seems to be less power in 0-10 Hz and high power in 10-30 Hz relative to baseline. At level 2, there is significantly high power in the 0-4 Hz and 8-30 Hz ranges. At level 3, the power is significantly high between 0-30 Hz. At levels 4 and 5, the power remains significantly high between 0-20 Hz. A key feature in the human data is that the power spectrum at the deep levels (3-5) are concentrated near the 0-2 Hz and 8-12 Hz bands. A common feature between the two groups is that in the 15-30 Hz range, both the NHP and human spectra show an increasing trend from levels 1-3 followed by a decreasing trend in levels 4-5. Conclusion: We have developed an analysis paradigm to estimate confidence intervals on median of difference spectra from distinct anesthetic levels separately for NHP and human propofol anesthesia datasets. Comparison of the human and NHP groups suggests a biphasic pattern in the trend in the 15-30 Hz power with increasing effect-site anesthetic concentrations. A key contrast in the NHP data, unlike the human data, do not show a spectral peak in the 8-12 Hz band. It is very important to acknowledge key confounding factors due to the difference between the two groups. These are dosing regimen, recording sites, pharmacokinetic estimation, and recording setup. Nevertheless, our exploratory study is a first step towards establishing an objective, quantitative paradigm to relate NHP and human neurophysiological studies of propofol anesthesia. Figure and of group-wise median of log10(Si/S0) for the i-th segment relative to the baseline 338 Rebecca Wales, B.A., Psychiatry The Course of ADHD across Pregnancy and the Postpartum: Investigating ADHD Symptoms and Functioning among Women Who Chose to Discontinue, Alter, or Maintain Pharmacologic Treatment for ADHD in the Perinatal Period A.S. Baker, L.S. Cohen, R. Wales, O.B. Noe and M.P. Freeman Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: Attention deficit hyperactivity disorder (ADHD) has been increasingly recognized in girls in recent years, and a growing number of women enter their reproductive years treated with medication for ADHD. There have been no studies evaluating the course of ADHD across pregnancy to date. Recent studies have evaluated the reproductive safety of maternal and fetal exposure to ADHD medications; however, no studies have examined functional impairment, symptom severity, and associated comorbidities of ADHD during pregnancy. It was hypothesized that risk for ADHD symptom severity and functional impairment will be greater among women who stop/change the dose of stimulants compared to those who continue treatment with these agents.Methods: Pregnant women ages 18-45 were prospectively followed during pregnancy using 3 structured clinical interviews at timepoints of <20 weeks, 24 weeks, and 36 weeks pregnant. Women were grouped into 3 groups based on their decision to discontinue, continue, or adjust their ADHD medications during their pregnancy. ADHD symptoms were recorded at 290each timepoint using the AISRS. Additionally, symptoms of anxiety, depression, stress, and functional impairment were monitored.Results: 28 women with ADHD were followed through pregnancy. A total of 26 were eligible for analysis: 12 women continued their stimulant medication with no changes, 8 women discontinued medications, and 8 women changed their medication regimen. No significant difference in AISRS score was observed between groups. Women who discontinued ADHD medications had significant increases in MADRS score (effect size=3.80, z=2.2, CI: 0.41,7.18, p=0.028) size=2.08, z=2.1, CI: 0.14,4.03, p=0.036) as compared to patients who continued their ADHD medications at their stable dose. Conclusion: Past studies have not systematically examined the course of ADHD across pregnancy, including ADHD symptoms, functional impairment and comorbid psychiatric disorders. These results suggest that the decision to discontinue ADHD medication is associated with an increase in functional impairment and depressive symptoms across pregnancy. This preliminary prospective data is the first of its kind and offers insight into course of the disease in the perinatal period. 339 Paul Walker, Orthopedics Does Ethnicity Influence Complication Rates Following Revision Surgery for Failed Total Joint Arthroplasty? P. Walker2,3,1, C. Klemt2, V. Tirumala2 and Y. Kwon2 1Weill Cornell Medical College, New York, NY, USA, 2Bioengineering Laboratory, Department of Orthopedic Surgery, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA and 3Center for Diversity and Inclusion, SRTP, Massachusetts General Hospital, Boston, MA, USA Introduction: Over 2 million total joint arthroplasties (TJA) are performed within the United States each year; this is expected to rise to 4 million TJAs performed annually by 2030. Although complication following TJA is low, revision surgery is a complex procedure that remains a burden in patient outcomes and healthcare costs. Racial and ethnic disparities in healthcare utilization and outcomes have been widely reported and have significant implications for individual patient and healthcare infrastructure. To date, no study has compared outcomes following revision TJA in the variety of ethnic groups within our society, which is essential to identify potential disparities and to optimize patient outcomes.Methods: A single institution retrospective analysis of a consecutive series of 4285 revision TJA was conducted. Patients were stratified into 5 cohorts based on race: 3961 (92.4%) white, 166 (3.9%) African American (AA) 66 (1.5%) Hispanic, and 45 (1.1%) Asian. Charts were reviewed for patient demographics and clinical outcomes.. Students t-test and Fisher's exact test were used to quantify differences between the groups.Results: There was no significant difference between the groups with respect to comorbidities. Compared to whites, African Americans had significantly higher BMI (p=0.04), 10-year death rate (p=0.01), ASA score (p=0.04) and infection risk (p=0.04). Compared to whites, Hispanics demonstrated significantly higher BMI (p=0.03), infection risk (p<0.01), and 10-year deaths rate (p=0.008). Compared to whites, Asians demonstrated a significantly lower ASA score (p=0.02) and re-revision rate (p=0.02).Conclusion: The patient demographic in this study aligns with previous reports of TJA underutilization in underrepre- sented populations. Our findings suggest that minorities have similar outcomes compared to whites following revision TJA. However, African Americans and Hispanics carry a higher risk of re-revision due to infection, which is typically the most severe complication. 340 Meredith J. Ward, B.A., Psychiatry Depressive Symptomatology Associated with Somatization Symptoms Among Anxiety Patients M.J. Ward1, E. Bui1, O. Losiewicz1, S. Philip2, R. N. Simon3 and E. Hoge2 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Georgetown Medical Center, Washington, DC, USA and 3Psychiatry, New York University Langone Health, New York, NY, USA Introduction: Comorbid major depressive disorder (MDD) among patients with an anxiety disorder is associated with more frequent somatic complaints (Bekhuis, Boschloo, Rosmalen & Schoevers, 2015). The cardinal features of MDD are depressed mood and loss of interest, i.e. anhedonia (American Psychiatric Association, 2013). Generalized anxiety disorder (GAD), social anxiety disorder (SAD), panic disorder (PD) and agoraphobia (AGO) are all independently associated with somatic complaints, when controlling for presence of major depressive disorder (Bekhuis, Boschloo, Rosmalen & Schoevers, 2015). Yet, depressive symptomatology may partially contribute to somatic complaints in patients with anxiety disorders, even in 291the absence of DSM-5 diagnosed MDD. Moreover, no research has addressed whether one dimension of depression, either depressed mood or anhedonia, is a stronger predictor of such somatic complaints. Anhedonia may be a stronger predictor of somatic symptoms in individuals with a primary anxiety disorder due to the \"emotional numbing\" it is associated with, causing psychological distress to somaticize in bodily complaints rather than expressed emotions. Methods: Treatment seeking adults (N = 81; M age = 31.4, SD = 11.2; 73% female) meeting DSM-5 criteria for a primary anxiety disorder at baseline of an ongoing randomized controlled trial were assessed with a demographic questionnaire and the Structured Interview Guide for the Hamilton Anxiety Scale (SIGH-A: Shear, Bilt, Rucci, SIGH-A somatic subscale score was created using item scores from (7) Somatic (muscular), (8) Somatic (sensory), (9) Cardiovascular, (10) Respiratory, (11) Gastrointestinal (12) Genitourinary and (13) Autonomic symptoms. Participants then completed the Quick Inventory of Depressive Symptomatology Self-Report (Rush et al., 2003), from which item scores from (5) Sadness and (13) General interest were used to measure depressed mood and anhedonia narrowly.Results: There were no significant associations between somatic symptoms and sex, age, race, ethnicity, education level, marital status, and employment status (all p's > .05). Somatic symptoms were significantly correlated with depressive symptom severity (r = .43, p < .001), anhedonia (r = .32, p < .01), and sadness (r = .30, p < .01). A multivariate linear regression (F (2, 79) = 5.67, p < .01) indicated that anhedonia was independently associated with somatic symptoms (B(SE) = 1.46(0.71), p < .05) while sadness was not (B(SE) = 0.97(0.69), p = .16). Conclusion: Our results suggest that anhedonia is independently associated with somatic symptom severity among individ - uals with a primary anxiety disorder. However, the cross-sectional design precludes determination of causality. Future research should assess the casual relationship between somatic complaints and anhedonia, for it may be that somatic complaints decrease interest in previously pleasurable activities among individuals with anxiety. 341 Tina Watson, BA, Radiology Cost-effectiveness analysis of pembrolizumab combination and avelumab combination therapies for first-line treatment of advanced renal cell carcinoma in the United States T. Watson Institute for Technology Assessment, Massachusetts General Hospital, Great Neck, NY, USA Introduction: The treatment paradigm for patients with advanced renal cell carcinoma (RCC) has recently undergone dramatic changes, with immune checkpoint inhibitor combination therapy becoming the standard of care in the first line. The two most recently approved regimens - pembrolizumab plus axitinib and avelumab plus axitinib - have great therapeutic potential given the eligible patient population, but their high cost is a concern from a societal perspective. In our study, we estimate the cost-effectiveness of these treatments relative to sunitinib. Methods: Using a primary microsimulation model and data from the KEYNOTE-426 clinical trial, we evaluated pembroli - zumab plus axitinib versus sunitinib as first-line treatment strategies. We included avelumab plus axitinib as a third strategy based on the JAVELIN Renal 101 clinical trial. Incremental cost-effectiveness ratios (ICERs) were calculated and compared to a willingness-to-pay threshold of $100,000/quality-adjusted life year (QALY). Progression-free survival and overall survival was modeled using Kaplan-Meier estimates and time on treatment data from the clinical trials. To project survival beyond trial data, we fitted exponential functions to the Kaplan-Meier survival curves. Cancer-related costs in the model included drug acquisition and administration, general treatment during progression-free survival and best supportive care, treatment of major adverse events, scans, and death costs. Results: Treating an RCC patient with sunitinib in the first line averaged $307,106 and 2.63 yielded a mean cost of $511,226 and mean QALYs of 3.52 and weakly dominated avelumab combination, leading to an ICER of $229,564 between pembrolizumab combination and sunitinib.Conclusion: Using a microsimulation modeling approach, we determined that pembrolizumab combination would not be cost-effective at a willingness-to-pay threshold of $100,000/QALY, although it dominated avelumab combination therapy and, thus, represents the more favorable treatment strategy. Given the survival benefit of pembrolizumab combination over sunitinib, price reductions are necessary to ameliorate its poor cost-effectiveness and help deliver high quality care to patients with advanced RCC.292342 Maximilian Weniger, MD, Surgery - General and Gastrointestinal Impact of Klebsiella Pneumoniae and Quinolones on Survival of patients Treated With Gemcitabine for Pancreatic Cancer M. Weniger1,2, T. Hank1, 1Department of Surgery, Massachusetts General Hospital, Boston, MA, USA and 2Department of General, Visceral and Transplantation Surgery, Ludwig-Maximilians-University, Munich, Germany Introduction: An increasing body of evidence suggests that microbiota promote progression of pancreatic ductal adenocar - cinoma (PDAC), which is frequently treated with gemcitabine-based chemotherapy. Since gammaproteobacteria that are abrogated by treatment with quinolones promote chemoresistance towards gemcitabine in colon cancer models, and quinolones induce cell cycle arrest in PDAC cells, we hypothesized that quinolones as well as gammaproteobacteria, such as Klebsiella pneumoniae (KP), could potentially impact survival in PDAC. Methods: Survival data, antibiotic use, and results from intraoperative bile cultures from patients undergoing neoadju-vant treatment and curative pancreatoduodenectomy for PDAC were collected on patients from the Massachusetts General Hospital, USA and Ludwig-Maximilians-University, Germany. Survival was analyzed using Kaplan-Meier-Curves, Cox- and multivariate linear regression. Bacterial diversity in pancreatic cancer specimens was assessed using 16S RNA sequencing.Results: In 197 patients who underwent neoadjuvant treatment and surgery, an increasing number of pathogens found in intraoperative bile cultures was associated a decrease in progression-free survival (PFS) (-1.9 months/species; p<0.001). Furthermore, adjuvant treatment with gemcitabine in addition to neoadjuvant treatment and surgery was associated with improved PFS in patients negative for KP compared to patients who did not undergo additional adjuvant therapy (PFS 26.3 vs. 15.2 months, p<0.001). Interestingly, there were no differences in PFS between patients who received additional adjuvant therapy compared to those who did not among KP-positive patients (PFS 19.5 vs. 13.2 months, p=0.23). Quinolone treatment of any kind was associated with improved median overall survival (OS) independent of KP status (OS 44.8 vs. 24.0 months, p<0.001) and in KP-positive patients (OS 38.1 vs. 18.1 months, p=0.04). Importantly, patients with quinolone-resistant-KP had shorter PFS than those with quinolone-sensitive KP (PFS 5.0 vs. 18.8 months, p=0.002).Conclusion: KP may promote chemoresistance to adjuvant gemcitabine. Moreover, quinolone treatment markedly improved survival and may abrogate the deleterious effects of KP in pancreatic cancer patients. 343 Timothy E. Wilens, MD, Psychiatry It's not just what you do, it's how you do it: Variation in substance use screening outcomes with commonly used screening approaches in primary care clinics J. McNeely1, Rotrosen9 1Department of Population Health, NYU Langone Health, New York, NY, USA, 2Medicine, The Mount Sinai Hospital, New York, NY, USA, 3Psychiatry and Behavioral Health, Stony Brook University, Stony Brook, NY, USA, 4Primary Care, MGH, Boston, MA, USA, 5Department of Psychiatry, NYU Langone Health, New York, NY, USA, 6Center on Drug Use and HIV Research, New York University, New York, NY, USA, 7National Institute on Drug Abuse, National Institutes of Health, Bethesda, MD, USA, 8The Emmes Corporation, Rockville, MD, USA and 9Department of Psychiatry, NYU Langone Health, New York, NY, USA Introduction: Primary care clinics often struggle to choose the approach to alcohol and drug screening that is best suited to their resources, workflows, and patient populations. We are conducting a multi-site study to inform the implementation and feasibility of electronic health record (EHR)-integrated screening. Methods: In two urban academic health systems, researchers worked with stakeholders from 6 clinics to define and implement their optimal screening approach. All clinics used single-item screening questions for alcohol/drugs followed by AUDIT-C/DAST-10. Clinics chose between: (1) screening at routine vs. annual visits; and (2) staff-administered vs. computer self-administered screening. Results were recorded in the EHR, and data was extracted quarterly to describe implementation outcomes including screening rate and detected prevalence of unhealthy (moderate-high risk) use among those screened. Findings are from the first 3-12 months post-implementation at each clinic.Results: Across sites, of 84,311 patients with primary care visits, 58,492 (69%) were screened. In the 4 clinics with mature (9-12 months) implementation, screening rates ranged from 42-95%. Rates were lower (10-22%) in the 2 clinics that recently launched. Screening at routine encounters, in comparison to annual visits, achieved higher screening rates for 293alcohol (90-95% vs. 42-62%) and drugs (90-94% vs 38-60%). Staff-administered screening, in comparison to patient self- administered screening, had lower rates of detection of unhealthy alcohol use (2% vs. 15-37%). Detection of unhealthy drug use was low, ranging from 0.3-1.5%. Conclusion: EHR-integrated screening was feasible to implement in at least 4 of the 6 clinics; 1-year results (available Fall 2019) will determine feasibility at all sites. Self-administered screening at routine primary care visits achieved the highest rates of screening and detection of unhealthy alcohol use. Although limited by differences among clinics and their patient populations, this study provides insight into outcomes that may be expected with commonly used screening strategies in primary care. 344 Benjamin W. Wipper, Psychiatry 1-Year Longitudinal Data from the National RLS Opioid Registry: An Observational Study Examining the Safety and Efficacy of Long-term Opioid Treatment of Restless Legs Syndrome B.W. Wipper1, J. Purks2 and J.W. Winkelman1,3 1Psychiatry - Research, Massachusetts General Hospital, Boston, MA, USA, 2Larner College of Medicine, University of Vermont, Burlington, VT, USA and 3Harvard Medical School, Boston, MA, USA Introduction: Restless Legs Syndrome (RLS), also called Willis-Ekbom disease, is a sensory-motor neurological disorder characterized by an irresistible urge to move the legs that is often paired with leg discomfort, occurring primarily during the evening or nighttime. Clinically significant RLS is present in roughly three percent of the population. RLS is associated with physical distress and lack of sleep, which both contribute to the high levels of morbidity and poor quality of life observed in these patients. Low-dose opioid medications are frequently used in patients who have become refractory to first-line RLS treatments, particularly dopamine agonists (e.g. ropinirole). In the present work, we aim to collect observational longitu-dinal data over at least 5 years on the RLS treatment efficacy, dosage changes, and tolerability of opioid medications in this population to better inform clinical decisions. Methods: Recruitment for the registry occurred through the RLS Foundation, social media, and individual clinicians treating patients with RLS. Subjects must have been taking an opioid for diagnosed RLS and must have shown a previous therapeutic response to dopamine agonists. Baseline information on initial and current opioid dosages, side effects, past and current RLS treatments, current RLS severity, psychiatric history and current symptoms, and opioid abuse risk factors were collected through a 45-minute phone interview and online (REDCap) survey. Online follow-up surveys are performed every 6 months thereafter. All data is stored in a deidentified, secure online database.Results: Baseline Data: Since recruitment began in December 2017, 451 participants have enrolled in the registry: 57% female, 73% aged 60 or older, and 98% white. Participants had been taking opioids for a median of 1-3 years upon entry into the registry. Over half of participants were on opioid monotherapy, with an additional 22% taking a concomitant A2delta ligand (e.g. gabapentin) and 16% taking a concomitant dopamine agonist. 46% of all participants were on methadone, with an additional 27% on oxycodone formulations. Median morphine milligram equivalents (MME) was 37.5 (methadone = 40.0, oxycodone = 30.0). The average International Restless Legs Scale (IRLS) score was significantly lower for participants taking methadone (9.9) than for participants taking oxycodone (18.0) (p < 0.00001). 1-Year Longitudinal Data: Of pants reaching 1-year follow-up, 6 participants discontinued opioid treatment, 6 were lost to follow up, and 1 was excluded (93.2% retention rate). IRLS scores did not significantly change from Baseline to 1-year follow-up. Mean and median values for RLS-Quality of Life, Insomnia Severity Index, PHQ-9, and Generalized Anxiety Disorder scales did not change from Baseline. Median and mode change in MME at 1-year is 0 mg. Roughly one-quarter of all participants increased their opioid dose at 1-year, and approximately a third of those increased by less than 10 MME. Less than 10% of all participants increased their opioid dose by > 25 MME and 5 (2.6% of all participants) increased their opioid dose by > 50 MME. Participants with > 50 MME opioid dose increases tended to be among those that had the largest doses upon entry into the registry (> 90 th percentile MME) and those that used opioids to treat medical conditions in addition to RLS (i.e. arthritic pain).Conclusion: Opioid doses are generally low in this population treated for RLS. Approximately half of the registry's partici - pants are on opioid monotherapy. Nearly half of all participants use methadone, and those using this opioid show significantly lower IRLS scores than those taking oxycodone, even at equivalent MME doses. At 1-year follow-up, sleep quality and RLS symptom control was similar to baseline. The vast majority of participants did not increase opioid dose. Of the 25% of participants that increased their dose, approximately one-third increased by a small amount (<10 MME). A small minority increased their opioid dose by > 50 MME.294345 Bingyu Xu, Psychiatry The Presence of Intimate Partner Violence in Military Widow Survivors of Suicide: Implications for PTSD and Complicated Grief Outcomes in an Intensive Treatment Program A. Blackburn, B. Xu, L. Brenner, C. Moore, T. Spencer and B. Ohye Home Base, MGH, Charlsetown, MA, USA Introduction: Background: U.S. Veterans are at risk for suicide, with an average of 30.1 veterans per every 100,000 dying by suicide each year. Family members who have survived a veteran's death frequently struggle with Posttraumatic Stress Disorder (PTSD), Complicated Grief (CG), and depression. Survivors of intimate partner violence (IPV) are also at increased risk for PTSD. Evidence-based treatments such as Prolonged Exposure therapy (PE) have been demonstrated to effectively treat PTSD associated with witnessing the death of a loved one, as well as abuse by an intimate partner; however, research on the intersection of these populations is wanting. Evidence is lacking to guide treatment of survivors of intimate partner violence whose partners died by suicide. Aim: The aim of the current study is to examine whether the presence of IPV impacts treatment outcomes in a two-week intensive outpatient program (IOP) for suicide bereft military widows when utilizing PE for PTSD and CG compared to widows without a history of IPV in their relationship with their deceased husband. Methods: N=24 female participants participated in a two-week IOP for partners of veterans who died by suicide. Presence of IPV was determined through chart review and confirmed by clinician report. PTSD, depression and CG were diagnosed by clinical interview. Severity of PTSD, depression and CG were assessed using the Posttraumatic Stress Disorders Check List (PCL-5), the 8-item Patient Health Questionnaire (PHQ-8) and the Inventory for Complicated Grief (ICG) respectively. A series of mixed-model regressions were employed to examine effects of IPV, time and their interaction on measures of psychopathology.Results: 8 of the 24 (33%) participants identified IPV in their relationship with their deceased veteran partner. The mixed model regression with PCL-5 scores as a dependent variable indicated a significant effect of time (Coefficient -8.68, SE= 3.46, p =.012) as well as a significant interaction between time and IPV (Coefficient = -12.70, SE = 6.01, p =.035), but no signif- icant effect of IPV. The model for the PHQ-8 showed a significant effect of time (Coefficient= -5.47, SE= 1.27, p > 0.001) IPV (Coefficient= 4.73, SE=2.42, p= .048), but no significant interaction. The model for ICG scores indicated a significant effect of time (Coefficient -10.80, SE=2.4, p<.001), but no significant effect of IPV, or significant interaction. Conclusion: Participants showed significant reductions in PTSD, CG, and depression. Survivors of IPV showed significantly greater reductions in PTSD symptoms compared to other widows. More research is needed to understand trauma and recovery for survivors of IPV whose abusive partners have died by suicide. 346 Chengeng Yang, Radiology Targeted Contrast Agents for Real-Time Lung Cancer Surgery C. Yang Gordon Center for Medical Imaging, Massachusetts General Hospital, Boston, MA, USA Introduction: Lung cancer is one of the most frequent cause of death among malignant diseases. Surgery is the main treatment of choice for patients with non-small cell lung cancer (NSCLC). However, surgery is performed blindly without real-time image-guidance. Since near-infrared (NIR) fluorescence light penetrates deep inside soft tissue, intraoperative NIR imaging can serve an important role in the diagnosis, staging, and definitive management of NSCLC. In this study, we designed several types of novel targeted NIR fluorophores for lung cancer surgery and tested them in a mouse model of lung cancer. Methods: To prepare tumor-targeted NIR fluorophores, we conjugated zwitterionic a phenoxypropionic acid as a linker, while ZW800-1C and ZW800-PEG have a C-C coupled linker and a PEG linker, respectively. The bioconjugation was further performed by the conventional N-hydroxysuccinimide (NHS) chemistry in phosphate-buffered saline (PBS), pH 8.0. The physicochemical and optical properties of targeted compounds were compared in different solvents, and their biodistribution and targetability were evaluated in CD-1 mice and syngeneic mice of lung cancer.Results: ZW800-1A exhibits good optical properties and solubility, while failed to remain stable in fetal bovine serum (FBS) buffered with 50 mM HEPES due to the fragile ether linkage on the meso carbon of heptamethine core. On the other hand, ZW800-1C shows good stability in FBS/HEPES for over 24 h. However, the hydrophobic C-C linkage induces H-aggregation 295in aqueous solutions regardless of serum or not. Furthermore, due to the cytotoxic Pd catalyst used during the final step of liker coupling, ZW800-1C raises safety concerns. To avoid this issue, ZW800-PEG is composed of an FDA-approved water-soluble polyethylene glycol (PEG) linker, which improves solubility of the final compound significantly with avoiding intermolecular aggregation. Additionally, it reveals excellent optical properties and high stability in serum-containing buffers. ZW800-PEG displays a reasonable blood half-life (46.83 min), suggesting fast clearance after appropriate targeting once conjugated with cRGD.Conclusion: An ideal candidate of NIR fluorophore for molecular cancer targeting is supposed to meet several specific criteria, including an imaging moiety with fine optical properties, robust stability in living organisms, as well as appropriate solubility in serum-containing buffers and a targeting moiety with specific affinity to the tumor site. Additionally, an ideal targeted agent should be eliminated from the normal tissue and eventually from the body in a short period of time after appropriate targeting. cRGD-ZW800-PEG presents higher stability in serum containing media as well as in animal models. Besides, ZW800-PEG minimizes safety issues due to its greener synthesis approach and faster clearance in vivo. Together, these excellent properties of ZW800-PEG make it a highly potential targeted imaging moiety, enabling effective imaging guidance with minimal nonspecific background signals during lung cancer surgery. 347 Jimmy Yang, Neurosurgery Microscale dynamics of electrophysiological markers of epilepsy J. Yang1, Paulk2, D. Cleary3, Soper2, S. Dayeh4 and S. Cash2 1Neurosurgery, Massachusetts General Hospital, Boston, MA, USA, 2Neurology, Massachusetts General Hospital, Boston, MA, USA, 3Neurosurgery, University of California, San Diego, San Diego, CA, USA, 4Jacobs School of Engineering, University of California, San Diego, San Diego, CA, USA, 5Neurology, Brigham and Women's Hospital, Boston, MA, USA, 6Radiology, University of California, San Diego, San Diego, CA, USA and 7Neurosurgery, Brigham and Women's Hospital, Boston, MA, USA Introduction: Recent advances in microelectrode technology have revealed new characteristics of interictal discharges and seizures, including the appearance of events on the spatial order of microns (Schevon 2008, Stead 2010). However, prior studies utilizing microelectrode arrays have relied on penetrating technologies, such as the NeuroPort array (400 m, Schevon 2008) or millimeter-level spatial resolution (Stead 2010). Progress in microelectrode technologies have led to increased flexibility with electrode arrangements and spatial resolution for detecting neural activity. Interictal discharges are considered neurophysiologic markers of epilepsy and may represent irritative cortex (Wilke 2009, Dworetzky 2009, Rosenow 2001). They are considered neurophysiologic abnormalities that are taken into consideration during clinical management of patients who present with seizure (Wirrell 2010). More recently, high frequency oscillations (HFOs) have been identified as another marker for epileptic brain regions, though their specificity has been debated (Jefferys 2012, Worrell and Gotman 2012, Cimbalnik 2018, Burnos 2016). Finally, the intersection of these two markers, spike-ripples, have additionally been identified as potentially more specific markers for seizure onset zones (van Klink 2016), with their resection being correlated with improved post-surgical outcome (Wang 2013, Wang 2017). To better characterize these neurophysiologic markers at the microscale, we utilized PEDOT:PSS (a conductive organic polymer) microelectrodes in acute intraoperative recordings in patients undergoing neurosurgical procedures. We found that local events, confined to a few electrodes of the array, can occur, which may offer insight into the underlying microarchitecture of epileptic networks.Methods: Recordings were performed in 21 subjects who were already scheduled for a neurosurgical procedure. This study was approved by the Partners IRB. As part of their clinical care, a subset of patients were administered a medication to provoke the appearance of interictal epileptiform discharges or intraoperative irrigation with cold saline. PEDOT:PSS devices were fabricated using previously established protocols (Sessolo 2013, Khodagholy 2015, Ganji 2018) and were comprised of 2 different arrays. One was designed as a bi-linear array, with 64 channels arranged in two columns, with an electrode diameter of 30 m and interelectrode distance of 50 m. A second design was comprised of a circular grid, arranged as concentric rings at varying distances from the center. Research recordings were conducted using an Intan recording system with a sampling rate of 30 kHz. Data analysis was subsequently performed with MATLAB scripts. Interictal discharges were first automatically detected using an algorithm that first filters the data into a 10-60 Hz band, applies an envelope and then finds an appropriate threshold value based on a statistical distribution (Janca 2015). HFOs were first automatically detected using a previously published algorithm (Salami 2012). In brief, the detector first filters the data into two bands, a ripple band (80-200 Hz) and a fast ripple band (250-500 Hz), followed by using a reference time window in order to detect peaks. All detections were visually reviewed.Results: Patients had a mean age of 37.7. The majority of patients were right-handed, and the majority underwent a neurosur - gical procedure for epilepsy. Of the patients undergoing a neurosurgical procedure for epilepsy, 8 (53%) were treated with a 296medication, as part of their intraoperative clinical management, to activate epileptiform abnormalities. In review of the overall quality of the research recordings, 18 subjects were included for analysis of interictal epileptiform discharges. In examining the high frequency domain for the analysis of HFOs, 16 subjects were included for analysis. Overall, interictal discharges that were seen across the entire array were detected in 94% of the subjects. On the other hand, local events, defined as those seen over <50% of the microelectrode array, were only seen in 67% of the subjects. Few examples (in 2 subjects) were found of repeating interictal discharges, which are generally described as periodic discharges in a clinical setting. In examining the subgroup of subjects (N=5) who received a provoking medication (methohexital or alfentanil), a significant increase was seen in the frequency of general (p=0.0039, Kruskal-Wallis) but not local (p=0.39, Kruskal-Wallis) interictal epileptiform discharges, though, notably, local events were only seen in 3 of these 5 subjects. When investigating the effects of cold saline, there was no statistically significant change in the frequency of general or local events after cold saline irrigation, though this was likely affected by the low number of subjects who underwent this maneuver (N=3). Similar trends were found in detections of high frequency oscillations. Overall, high frequency oscillations that could be detected over the majority of the microelectrode array were found in 94% of the subjects, while local events were only detected in 75% of the subjects. The frequency of these events was not significantly affected by provoking medication or cold saline, though again these results may have been affected by the low number of subjects in the analysis. Additional analyses of the interictal discharges demonstrated that these phenomenon can be detected as \"traveling\" over the microelectrode array in specific patterns, while high frequency oscillations that were detected in specific electrodes could be detected repeatedly during a recording. Conclusion: Using new technologies of PEDOT:PSS microelectrode arrays, localized markers of epileptiform activity can be identified at high spatial and temporal resolution. Overall, this data may suggest micro-domains of irritable cortex and may therefore offer insight into the underlying epileptic and pathologic networks that are involved in neurologic disease. 348 Elissa M. Ye, Masters, Neurology Validation of EEG-based Brain Age Index for Dementia-related Diseases E.M. Ye, H. Sun, A. Lam and M.B. Westover Neurology, Massachusetts General Hospital, Boston, MA, USA Introduction: Dementia is a very common neurodegenerative disease in the elderly population. The progressive damage to the neurons causes gradual loss of memory, communication, reasoning, and can lead to sleep disturbances such as insomnia, as well as psychiatric disorders such as depression. Human sleep undergoes robust and predictable changes with age, reflected in both overall sleep architecture and electroencephalogram (EEG) oscillations/waveforms. Accentuated aging of the brain can also be reflected as the deviation of these parameters from the age norm. The deviations from the normal aging trajectory, known as Brain Age Index (BAI), have shown potential to serve as biomarkers for cognitive impairment, neurological or psychiatric disease, or death. Here we validate a biomarker of brain health - sleep EEG-based brain age in demented patients. Using a dataset of 11,039 sleep studies, we aim to investigate the association between Brain Age and dementia. We hypothe-size that patients with dementia-related diseases have significantly higher brain ages indexes than patients without dementia.Methods: Brain ages were computed based on an EEG-based brain age algorithm and categorized into dementia, mild cognitive impairment (MCI), ambiguous, and non-dementia groups based on clinical diagnosis and MoCA/MMSE scores of patients. Dementia patients were identified based on dementia-related medication and keywords, MoCA scores in range 19-27, or MMSE scores less than 26. MCI patients were identified based on MCI keywords or MoCA scores in range 20-27. Only patients of age 50 and above were included, and any patients with brain tumors, strokes, seizures, or developmental delays were excluded. Results: We found that the BAI of patients with dementia (mean = 4.11 yrs, std = 10.02 yrs) were significantly higher than those of patients with no dementia (mean = 0.516 yrs, std = 10.40 yrs), with p-value of 0.002 for independent-samples t-test, and those of healthy patients (mean = -0.67 yrs, SD = 9.52 yrs), with p-value of 0.0002. The BAI of patients with MCI (mean = 2.29 yrs, std = 8.13 yrs) were also significantly higher than those of healthy patients. BAI and MoCA score were negatively correlated with correlation coefficient of -0.211 (p-value = 0.036) as expected. Conclusion: Our results show the validity of sleep EEG-based Brain Age Index as an indicator of dementia. Further research is required to adjust for confounding variables, and understand what EEG features contribute the older brain age index. This could open new possibilities for using BAI as a diagnosis tool or a predictive biomarker of cognitive impairment.297Group Criteria 349 Kanhua Yin, MD, MPH, Surgery - Surgical Oncology Ambiguity, Accuracy, and Accessibility of Legacy Genetic Testing Results for Cancer Susceptibility: How Common Are Conflicting Classifications and Can They be Found? Y. Liu2,3, K. Hughes1,14 1Division of Surgical Oncology, Massachusetts General Hospital, Boston, MA, USA, 2Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA, USA, 3Department of Data Sciences, Dana-Farber Cancer Institute, Boston, MA, USA, 4SHEER Services, Kathmandu, Nepal, 5Texas Oncology, Dallas, TX, USA, 6Advanced Surgical Care of Northern Illinois, Barrington, IL, USA, 7New Mexico Comprehensive Breast Care, Albuquerque, NM, USA, 8Comprehensive Breast Care, Troy, MI, USA, 9Nashville Breast Center, Nashville, TN, USA, 10The Breast Health and Wellness Center, Grand Rapids, MI, USA, 11Johns Hopkins Hospital, Baltimore, MD, USA, 12Newton-Wellesley Hospital, Newton, MA, USA, 13Lyons Care Associates, Kahului, HI, USA and 14Harvard Medical School, Boston, MA, USA Introduction: The interpretations of germline variants' clinical significance, known as variant classifications, may differ between labs or change over the time, which can lead to inappropriate prophylaxis recommendations or clinical management. We aim to apply a variant harmonization tool to identify these discordant variant classifications in a large multi-practice variants report dataset. Methods: A total of 7496 variants entries sequenced between 1996 and 2019 were collected from 11 different clinical practices across the United States. After data cleaning, a variant harmonization tool, Ask2Me VarHarmonizer, was applied to harmonize and map the variant entries to ClinVar, a public archive of human genetic variants. Lab-reported and ClinVar variant classifications information were analyzed and compared. The discordances of variant classification were evaluated from three aspects: between lab-reported and ClinVar discordances, lab-reported discordances practice, lab-reported across practices. Results: A total of 4798 unique variants were identified, 3699 (77%) of which were mappable to ClinVar. Among all mappable variants, VUS was the most prevalent classification accounting for 74% of lab-reported classifications and 60% of ClinVar classifications. The unmappable variants were associated with remarkably high lab-reported pathogenic classi-fication (50% vs 21%) and low discordances classifications, and unique variants) had lab-reported discordances across practices.Conclusion: Various types of variant classification discordances occur frequently in the current practices. With the help of Ask2Me VarHarmonizer, clinicians may be able to identify discordance variant classifications and provide patients with the most up-to-date management recommendations.298350 Kanhua Yin, MD, MPH, Surgery - Ask2Me VarHarmonizer: A Python-Based Tool to Harmonize Variants from Cancer Genetic Testing Reports and Map them to the ClinVar Database Y. Liu2,3, K. Hughes1,14 1Division of Surgical Oncology, Massachusetts General Hospital, Boston, MA, USA, 2Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA, USA, 3Department of Data Sciences, Dana-Farber Cancer Institute, Boston, MA, USA, 4SHEER Services, Kathmandu, Nepal, 5Texas Oncology, Dallas, TX, USA, 6Advanced Surgical Care of Northern Illinois, Barrington, IL, USA, 7New Mexico Comprehensive Breast Care, Albuquerque, NM, USA, 8Comprehensive Breast Care, Troy, MI, USA, 9Nashville Breast Center, Nashville, TN, USA, 10The Breast Health and Wellness Center, Grand Rapids, MI, USA, 11Johns Hopkins Hospital, Baltimore, MD, USA, 12Newton-Wellesley Hospital, Newton, MA, USA, 13Lyons Care Associates, Kahului, HI, USA and 14Harvard Medical School, Boston, MA, USA Introduction: As genetic panel testing for germline mutations has become popular, increasing amount of variant-level data have been accumulated within recent decades. Although public variant databases such as ClinVar have curated and structured a large proportion of variant data, the variant names that clinicians and patients receive from genetic testing reports are not always consistent across the labs who perform the tests and are not easily mappable to those databases. A tool that can automate the process of harmonizing variant names and mapping variants to databases is needed to allow clinicians to keep track of the interpretation of the genetic testing results and ensure those are up-to-date.Methods: We present a Python-based tool, Ask2Me VarHarmonizer, which incorporates data cleaning, variant name harmonization, and a four-attempt mapping procedure to map variants into ClinVar. The impact of the availability of transcript reference sequence on mapping was evaluated by comparing the mapping results with and without adding transcript information in the mapping query. We applied this tool to a pilot variant dataset collected from 11 clinical practices.Results: The Ask2Me VarHarmonizer identified 6027 variant entries of interest from the pilot dataset and successfully mapped 4728 variant entries (78%) to ClinVar. There was very little impact of adding the transcript reference sequence to the procedure, with 99% consistency with and without this additional information. 4798 unique variants were identified. 427 (9%) unique variants had multiple names, of which 343 had multiple names within-practice.Conclusion: Our tool aggregates and structures variants data from clinical practices. It harmonizes variant names and maps variants to ClinVar. Performing this harmonization removes the ambiguity and redundancy for variants from different sources. We hope it can help clinicians keep track of variants from genetic testing reports ensuring variant classifications are up to date. 351 Sehyo Yune, MD, MPH, MBA, Radiology A comprehensive deep learning model for automated classification of chest radiographs S. Yune, M. Kim, D. Kim, J. Chung, J. Baik, M.H. Lev, A. Pourvaziri and S. Do Radiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Despite the wide variety of findings that can be seen in a chest radiograph, most algorithms for automated classification of chest radiographs targeted limited number of findings. In this study, we developed a convolutional neural network (CNN) model that comprehensively detects a wide variety of visual findings from nonselective chest radiographs by using natural language processing (NLP) for automated labeling.Methods: To extract keywords, we first developed a rules-based algorithm that matched the reports to our vocabulary that included all RadLex terms and terms manually added by manual review of reports. The vocabulary included 3373 visual findings, 587 devices, and 423 procedures. Negated and past findings were detected with a modified version of the NegEx algorithm. By using this algorithm, we extracted keywords from randomly selected 10,000 chest radiograph reports. Two radiologists and an internist reviewed every keyword and categorized them with consensus. The \"clinical\" class included four categories; history, clinical diagnosis, procedure, and foreign body. History and clinical diagnosis classes were not used for image labels, because they often describe conditions not visualized in a chest radiograph. The \"finding\" class included 12 categories; lung opacity, decreased lung density, hypoaeration, airway disease, cavity, pleural finding, cardiomegaly, hilar/mediastinal finding, aortic lesion, abdomen/diaphragm, fracture, and non-fracture skeletal finding. Lung opacity was further categorized into 5 subcategories, decreased density into 3 subcategories, and pleural finding into 2 subcategories. Of the 1037 keywords analyzed, 197 keywords were not assigned to any category. The rest of 3,346 keywords that did not appear 299in the 10,000 reports were also not categorized. Based on the keywords and categories from radiology reports, we labeled training data with the categories and used it to train a multiclass classification CNN model. Radiographs of which the report did not include any keyword were labeled \"no finding\". The training data was made from 25,317 randomly selected chest PA images from consecutive cases obtained from our institutional database. To validate the model, a radiologist and an internist manually reviewed 622 chest radiograph reports and labeled them to be used as a test dataset.Results: The average area under the receiver operating characteristic curve (AUROC) was 0.800 across 21 categories including subcategories. The highest AUROC was achieved at 0.936 (95% confidence interval (CI), 0.909-0.964) pleural lowest AUROC was 0.653 radiograph was 0.787 (95% CI, 0.747-0.827). The ROC curves for most common six categories are shown in figure 1. Examples of keywords, frequency of each category, and AUROC for the image classification model is shown in table 1.Conclusion: A comprehensive deep learning model that classifies nonselective chest radiographs can be developed by automated labeling of radiology reports. Examples of keywords, frequency of each category in training data, and AUROC of multiclass image classification model Only top 5 keywords in each class by frequency in 10,000 sample reports are shown. Frequency is counted from 25,317 reports used for training of the image classification model. Figure 1. Receiver operating characteristic (ROC) curves for the most common 6 categories. 352 Sarah Zapetis, B.S., Psychiatry Connectivity of a peripersonal space-monitoring network predicts personal space and social motivation in schizophrenia S. Zapetis2, Z. USA, 2Psychiatry, Massachusetts General Hospital, Charlestown, MA, USA, 3Harvard Medical School, Boston, MA, USA, 4Royal's Institute of Mental Health, Ottawa, ON, Canada, 5Psychology, Harvard University, Boston, MA, USA, 6Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Boston, MA, USA and 7Radiology, Massachusetts General Hospital, Charlestown, MA, USA Introduction: Personal space, the distance that people comfortably maintain from others, is an automatic form of social communication that is altered in a number of neuropsychiatric disorders. We have previously linked abnormalities in personal space in schizophrenia to negative symptoms, particularly social withdrawal; however, the neurocognitive mechanisms underlying this association remain unknown. To further investigate this association, we measured personal space and social anhedonia in three cohorts of subjects (two healthy control groups, HC1, HC2; n=25, 36, respectively; and one schizo-phrenia group, SCZ; n=33). In addition, using functional magnetic resonance imaging (fMRI), we also measured functional connectivity between two brain networks involved in social behavior in the patients with schizophrenia (SCZ) and their demographically-matched healthy control subjects (HC2). Methods: Social motivation/anhedonia was measured with the Time Alone Questionnaire (TAQ) in all three groups, as well as with the Social Anhedonia Scale-Revised (SAS-R) in the two groups (SCZ, HC2) who underwent MRI scanning. The size 300and flexibility (\"permeability\") of personal space was measured in all subjects using the classic Stop Distance Procedure. In the SCZ group, symptoms were assessed using the Positive and Negative Syndrome Scale (PANSS). Resting state fMRI scans were acquired using a 3T Siemens MRI scanner and analyzed using Freesurfer 6. A region-of-interest (ROI) analysis was conducted using ROIs defined in an independent dataset. The ROIs correspond to nodes of two different networks: a parietofrontal cortical network known to be involved in monitoring peri-personal space (PPS) and the default mode network (DMN). Correlation coefficient (r) values were converted to z scores using Fisher's r-to-z transformation method. Spearman's correlations were used to measure the strength of the relationships between network-to-network connectivity (between the PPS and DMN networks, which are typically anti-correlated with one another) and behavioral and personal space measures. Results: Scores on the two measures of social anhedonia, the percentage time preferred alone subscale of the TAQ and the SAS-R score, were significantly correlated with each other in the HC2 and SCZ groups (all r >.325, all p < .05). In the two healthy control groups, the size (in both HC1 and HC2) and permeability (in HC1 only) of personal space correlated with the percentage time preferred alone (all r > .409; all p < .034). In the SCZ group, the permeability, but not the size, of personal space correlated with SAS-R score and negative symptom severity (all r > .450; all p< .005). In the combined HC2 + SCZ group (n=69), all four correlations (between the two social anhedonia and the two personal space measures) were significant (all r > .352; all p <.003). In addition, the functional connectivity analysis revealed significant negative correlations between PPS-DMN connectivity and the permeability of personal space in the SCZ, HC2, and SCZ + HC2 groups (p= .04, .05 and.004, respectively), i.e., lower connectivity (i.e., stronger anti-correlations) between these two networks was associated with higher permeability of personal space. Consistent with this finding, in the SCZ group only, PPS-DMN connectivity was also positively correlated with SAS-R scores (p=.05); stronger connectivity (i.e., lower anti-correlations) between these two networks was associated with higher levels of social anhedonia.Conclusion: A preference for greater personal space and/or a more rigid personal space boundary may be associated with diminished social drive in both schizophrenia and healthy groups. Moreover, greater connectivity (i.e., weaker anti-correla - tions) between the PPS-monitoring system and the default network is associated with reduced permeability of personal space, as well as with higher levels of social anhedonia, in schizophrenia. These findings suggest that investigating the function of the neural system governing personal space-related behaviors may help us to better understand the social deficits observed in schizophrenia, as well as variation in social drive in the general population. 353 Rina Zelmann, PhD, Neurology Interictal discharges modulate the evoked response to closed-loop electrical stimulation in humans R. Zelmann1, 2Brown University, Providence, RI, USA and 3Brigham and Women's Hospital, Boston, MA, USA Introduction: Interictal discharges (IIDs) are believed to result from desynchronization of a large region of cortex. We hypothesize that stimulating at the time of IIDs will result on a different evoked response to stimulation compared to random stimulation. Methods: We have developed a closed-loop stimulation system for electrical stimulation in humans (CLoSES) to detect IIDs and in real-time send a single-pulse direct electrical stimulation (SPES) to a nearby electrode pair. An IID was detected if the smooth absolute power of the intracranial EEG was above threshold for 12.5ms-25ms. Random interleaved stimulation was used as comparison. Three patients with pharmaco-resistant complex partial seizures implanted for clinical reasons voluntarily participated after fully informed consent (1 or 2 stimulation sites each). SPES pulses were 4mA, biphasic width=90us, inter-pulse interval = 53us. inter-SPES interval was 2s. For each stimulation site (N=5), we analyzed the cortico-cortical evoke potentials (CCEP) in the channel where IIDs were detected and in another channel also adjacent to the stimulation pair (OTHER channel). K-means clustering separated sharp waves from spike and wave and distinct detected phases within a channel (total 11 clusters). We computed maximum absolute amplitude (10-100ms following stimulation) and latency of the z-normalized CCEP with respect to baseline (500ms duration, 100 ms before stimulation). ANOVA, followed by multiple comparison, compared detected and stimulated IIDs (DetSTIM), detected IIDs but not stimulated (DetNoStim), and random stimulation (RandSTIM).Results: Our system successfully detected IIDs and delivered SPES in real-time (latency <10ms). On detected channels, DetSTIM was similar to DetNoSTIM. When separating IIDs in clusters, DetSTIM was significantly different than RandSTIM in 7 out of 11 clusters and DetSTIM was not significantly different than DetNoSTIM in all clusters. On OTHER channels, DetSTIM was similar to RandSTIM in all channels but one in which there were large IIDs (simultaneous to the detected IIDs). For clusters, DetSTIM was not significantly different than RandSTIM in 9 out of 11 clusters. Importantly, RandSTIM was similar in Detected and OTHER channels (pooled t-test: p=0.3).301Conclusion: Brain's response to stimulation was modified when stimulation occurred during an IID. This could be related to the IIDs depleting the region of activation. If confirmed in larger cohorts, this could have implications for neuromodulatory therapies. 354 Jenny J. Zhang, Psychiatry JUUL in school: Pervasive, persistent, but not clearly tied to smoking R.M. Schuster1,2, P. Hajek3, Evins1,2 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Queen Mary University of London, London, United Kingdom and 4Pediatrics, Massachusetts General Hospital, Boston, MA, USA Introduction: Experimentation with electronic cigarettes (ECs), particularly with JUUL, has become so prevalent among young people, the U.S. Food and Drug Administration in 2018 called the surge in youth use an epidemic. There is limited available information on prevalence and persistence of regular JUUL use, and on whether JUUL use is associated with subsequent use of combusted tobacco. Data are also lacking on the rate at which occasional JUUL users progress to at least weekly use of JUUL or combusted tobacco; and whether these rates are higher for JUUL than other EC products. This study aimed to examine rates of escalation from experimentation to frequent use of JUUL and/or to combusted tobacco smoking among high school students.Methods: Students, aged 12 to 19 and enrolled in 9th through 12th grade, were surveyed at two large public high schools in greater Boston (N=1,628). Paper and pencil surveys were administered during lunch period in School 1 on a single school day and electronic RedCap survey links were emailed to students and completed over a one-week period in School 2. The study questionnaire included demographic information, questions concerning cannabis and alcohol use in the past three months, and questions concerning lifetime nicotine use (combusted tobacco, oral tobacco, JUUL, and other ECs). Those who reported using nicotine in their lifetime were asked about frequency of use in the past 30 days, whether they ever used a nicotine product for 30 or more consecutive days, and whether first nicotine exposure was via combusted tobacco or an EC.Results: Lifetime JUUL use and current regular use were 3 to 6 times more common than smoking, with approximately 29%, 21%, and 6% of respondents endorsing lifetime, past month, and daily JUUL use (vs 11%, 7%, and 1% for lifetime, past month, and daily smoking). While more students initiated nicotine use with ECs (including JUUL) than combusted tobacco (69% and 31%, respectively; p<0.0001) and current at least weekly smoking was more common among those with lifetime smoking than lifetime JUUL use (28% and 9%, respectively; p<0.0006), nearly 17% of current at least weekly tobacco smokers initiated with EC use. Adolescents were more likely to transition from past daily smoking to current at least weekly JUUL use (65%), than from past daily JUUL use to at least weekly smoking (20%; p's<0.0001). Those who ever used JUUL daily were highly likely to persist with use (89% and 65% remaining current at least weekly and daily JUUL users), compared to past daily smokers (50% and 17% remaining current at least weekly and daily smokers; p's<0.0004). Conclusion: JUUL use is far more prevalent and persistent than combusted tobacco use among youth in these high schools, significant findings for a product that is so new to the market. We found higher rates of JUUL experimentation in high 302school students than previously documented. In this sample, nearly 30% of the high school youth experimented with JUUL and 21% used it in the past month. The addictive potential of JUUL seems similar to other EC products in some respects, though JUUL is more popular. Experimenters of both tobacco and other ECs were more likely to switch to JUUL than to other alternatives. While this study did not support the notion that JUUL is a gateway for smoking for the average adolescent vaper, a significant minority of JUUL users progress to regular combusted tobacco use. Longitudinal studies are needed to provide more definitive answers, and careful monitoring of future trends is warranted for both addictive patterns of EC use and for smoking prevalence in adolescent cohorts over time. 355 Lili Zhang, MD, Radiology Racial Differences between Non-Hispanic Black and White Americans with Stable Chest Pain in Coronary Artery Disease, ASCVD Risk Score, and Cardiac Events: Insights from the PROMISE Trial D. Olalere1, T. Mayrhofer1, Zhang1, and M.T. Lu1 1Radiology, Massachusetts General Hospital, Boston, MA, USA and 2Duke Clinical Research Institute, Durham, NC, USA Introduction: Race is a component of the Atherosclerotic Cardiovascular Disease (ASCVD) risk score, How ASCVD risk relates to cardiac events and epicardial coronary artery disease (CAD) in blacks vs whites with stable chest pain is not known.Methods: The PROMISE trial included 8,764 non-Hispanic blacks (12%; 1,071) and whites (88%; functional testing or anatomic coronary CT angiography (CTA). We compared black vs white ASCVD risk score, incident cardiac events (death, myocardial infarction, hospitalization for unstable angina), and in the subgroup having CTA (N=3,323), coronary artery calcium (CAC) and significant CAD (stenosis50%).Results: Between blacks and whites (mean age 59\u00b18 vs 61\u00b18 years; female 60% vs 52%), vs hypertension 7.5% (78% vs 67%; p<0.001). Yet there was difference in 2-year cardiac events (3.0% vs 3.2%; p=0.84). In the CTA subgroup, blacks had less CAC>100 (21% vs 33%; p<0.001) and less significant CAD (9% vs 15%; p=0.001), and these differences persisted after adjustment for ASCVD risk (p<0.001 and p=0.001).Conclusion: Despite higher ASCVD risk than whites, blacks had a similar low rate of adverse events and less significant epicardial CAD. This hypothesis-generating result suggests the need for further race-based calibration of the ASCVD risk calculator and other risk scores. 303356 Yiwen Zhu, MS, Center for Genomic Medicine (CGM) Childhood adversity and DNA methylation at genes involved in regulating sensitive period in development Y. Zhu1, Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Applied Statistics Group, University of the West of England, Bristol, United Kingdom, 3MRC Integrative Epidemiology Unit, School of Social and Community Medicine, University of Bristol, Bristol, United Kingdom, 4School of Mathematics, Statistics and Applied Mathematics, National University of Ireland, Galway, Ireland, 5Department of Psychiatry, Harvard Medical School, Boston, MA, USA and 6Stanley Center for Psychiatric Research, The Broad Institute of Harvard and MIT, Cambridge, MA, USA Introduction: Sensitive periods are developmental stages when life experiences can have a particularly potent impact on various outcomes, such as neurological functions, behavior, and disease risk. The impact of these life experiences may also be heightened based on biological factors, including genes that regulate when these sensitive periods occur and how long they last. From animal model experiments, about 60 genes have been identified as involved in regulating the timing and duration of sensitive periods in the mouse visual system. These genes regulate molecules that initiate or delay the onset/termination of a sensitive period (i.e., opening and closing) and enhance molecular networks to prolong the sensitive period (i.e., expression). A prior study from our research group found that a subset of these 60 sensitive period-regulating genes showed developmentally regulated patterns of expression in samples of post-mortem brain tissues; that is, for several of them, there was a decrease in gene expression observed between ages 1 to 5 years, suggesting that these genes varied in their expression profile across time, as would be expected for a sensitive period-regulating gene. We also found that a subset of these developmentally-regulated genes were also implicated in risk for major depressive disorder. Building from these findings, we examined in this current study the extent to which these sensitive period related genes were also involved in regulating childhood-adversity dependent DNA methylation changes. DNA methylation has been proposed as a mechanism through which gene by environment interactions can give rise to differences in gene expression. Using data from a prospec-tive birth cohort, we examined whether the timing of exposure to childhood adversity was associated with differential DNA methylation profiles at genes involved in regulating sensitive period functioning.Methods: Analyzing data from a subsample of the Avon Longitudinal Study of Parents and Children (n = 691-774), we assessed the associations between the age at exposure to childhood adversity and DNA methylation values at CpG sites annotated to 53 sensitive period regulating genes. We hypothesized that because these genes are involved in regulating brain plasticity during development, exposure to adverse experiences during a particular time period with heightened molecular activities may result in differential DNA methylation at these genes. In other words, the relationships between exposure to childhood adversity and DNA methylation profiles of these sensitive period genes may be time-dependent . We tested the hypothesis by performing analyses at two levels. First, to begin with a straightforward probe-by-probe approach (meaning running one regression model for DNAm at each CpG site), we used a two-stage structured life course modeling approach (SLCMA) that selected the time period during which exposures to childhood adversity had the strongest association with DNAm at a particular CpG site. Second, to build a novel multivariate method that characterized the relationship at multiple loci simultaneously, we performed linear discriminant analysis at the gene level, to assess whether exposure to childhood adversity had time-dependent effects on DNA methylation profiles of each of the 53 genes.Results: At the CpG site level, we did not see evidence for associations between childhood adversity and DNA methylation at CpG sites located in the 53 sensitive period genes. However, at the gene level, there was preliminary evidence that exposure to childhood physical or sexual abuse at 6.75 years was linked to DNA methylation at GRIN2A, a sensitive period opening gene that encodes a subunit of a glutamate-gated ion channel protein (canonical R 2 = 0.14, Wilks' = 0.86, p-value = 0.0001). Conclusion: Translating findings from animal model studies to human research, we found that exposure to early life stress, such as childhood adversity, may impact the epigenetic regulation of genes involved in sensitive period functioning. The comparison between probe-level and gene-level results suggested that using novel statistical methods to assess the DNA methylation profile of an entire gene would yield insights that would have been masked in CpG level investigation, as certain biological changes may be induced at the system level. Carefully characterized exposure and behavioral data from longitu - dinal epidemiological samples makes it possible for translational research to take place and uncover knowledge that may inform future prevention and intervention effort in human populations.304 De p a r t m e n t InDe x MGH Page No.Anesthesia, Critical 54, 58, 59, 62, 79, 84, 90, 125, 154, 171, 172, 178, 192, 193, 237, 273, 284, 288 Athinoula A. Martinos Center 33, 49, 51, 71, 74, 104, 107, 161, 196, 197, 219, 220, 248, 262 Health Professionals 77, 91, 185, 186, 187, 188, 198, 248, 251, 252, 258 Massachusetts Eye and Ear 32, 95, 96, 150, 155, 182, 204, 218, 281 Medicine - AIDS Research 38, 156, 180, 195, 246, 286 Medicine - Infectious 55, 61, 67, 98, 102, 139, 151, 158, 162, 164, 212, 213, 223, 230, 235, 237, 245, 264, 269, 276, 4, 19, 27, 53, 59, 75, 140, 147, 173, 239, 259, 260, 290 80, 97, 111, 130, 163, 174, 179, 194, 211, 250, 256, 21, 35, 42, 46, 63, 66, 81, 85, 86, 87, 91, 94, 95, 101, 102, 108, 110, 113, 115, 129, 142, 145, 149, 153, 167, 169, 170, 176, 177, 183, 189, 202, 205, 215, 217, 221, 225, 226, 230, 231, 234, 236, 242, 243, 244, 247, 249, 257, 274, 276, 278, 281, 283, 285, 289, 290, 292, 293, 294, 299, 301 Psychiatry - Benson Henry Institute for Mind Body 15, 16, 18, 82, 88, 116, 117, 118, 119, 120, 127, 133, 135, 136, 138, 141, 143, 144, 172, 181, 190, 200, 211, 222, 228, 261, 270, 274, 282, 291, 294, 298, 302 h o r In d e x Page No. h o r In d e x Page No. Page h o r In d e x Page No. Page h o r In d e x Page No. t h o r In d e x Page No. Page h o r In d e x Page No. Page h o r In d e x Page No. Page h o r In d e x Page No. h o r In d e x Page No. h o r In d e x Page No. Page t h o r In d e x Page No. Page h o r In d e x Page No. h o r In d e x Page No. h o r In d e x Page No. Page t h o r In d e x Page No. 120320 Au t h o r In d e x Page No. t h o r In d e x Page No. Page h o r In d e x Page No. Page t h o r In d e x Page No. Page "}