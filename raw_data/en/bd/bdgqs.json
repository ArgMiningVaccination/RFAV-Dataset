{"title": "PDF", "author": "PDF", "url": "https://www.bauer.uh.edu/rsusmel/4397/Fec%20-%20Lecture%20Notes_2023.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "Financial Econometrics - 2023 Lecture Notes Rauli Susmel Dept of Finance Bauer College of Business University of Houston \u00a9 R. Susmel, 2022 - For private use only, not to be modified, posted/shared online without written consent from author. Lecture 1 - Review of Statistics and Linear Algebra (NOT Covered) This lecture reviews basic proba bility concepts, from random va riables to the Law of Large Numbers and the Central Limit Theory. In the Ap pendix, the lecture introd uces Linear Algebra and its compact notation. Random Variable In probability and statistics, a random variable (RV), or stochastic variable , is described informally as a variable whose values depend on outcomes of an experiment (or phenomenon). An experiment is an act or a process with an unknown outcome. For example, the CEO of Apple announces a new product, the effect on the pri ce of Microsoft is unknown, thus, the price (and return) of Micr osoft is a RV. Examples: 1. We throw two coins and c ount the number of heads. 2. We define X = 1 if the economy grows two consecu tive quarters and X = 0, otherwise. (This is an example of a Bernouille (or indicator ) RV.) 3. We read comments from IBM' s CEO and compute IBM's return. 4. We count the days in a week the stock market has a positive return. 5. We look at a CEO and write his/ her highest education degree. \u00b6 For some RVs, it is easy to enumerate all possibl e outcomes. For instance, for the fourth example above: {0, 1, 2, 3, 4, 5}. But, some RV, it can be complicated. For example, for the third (IBM) example: {(-100%, }, where is a large positive number. The set of all possible outcomes is called sample space, denoted by . An event A is a set containing outcomes from the sample space. For example, for the IBM example, the returns are between 2% and 12.5% is an event. The collection of all possible events is . For example, for the IBM general, a RV is a function whose domain is the sample space. It produces numbers. (The name \"random variable\" is conf using; it is just a function!) Definitions & Notation: : The sample space -the set of possi ble outcomes from an experiment. An event A is a set containing outcomes from the sample space. : The collection of all possible even ts involving outcomes chosen from . (Formally: is a - algebra of subsets of the sample space.) P is a probability measure over . P assigns a number between [0,1] to each event in . Remarks: - A random variable is a convenient way to express the elements of as numbers rather than abstract elements of sets. - A random variable X is a function. - It is a numerical quantity whose valu e is determined by a random experiment. - It takes single elements in outcome set , which can be abstract elements, and maps them to points in R. Example: We compute the weekly sign of stock return s of two unrelated firms: Positive (U: up) or negative (D: down). The sample space is = {DD; DU; UD; UU}. Possible events ( A): - have the same signed return: {U,U} & {D,D}. - At least one firm has positive returns: {U,U}; {D,U} & {U,D}. - The first firm has positive returns: {U,U} & {U,D} Collection of all \"Number of Up cycles.\" Recall, X takes into , & induces P X from P. Then, Number of Ups P X P 1 0 Assuming U and D have the same probability, P[U] = P[D] = \u00bd, we define P X: Prob. of 0 Ups = P X[0] = P[{DD}] = \u00bc Prob. of 1 X[1] = P[{UD; DU}] = \u00bd Prob. of P X[2] = P[{UU}] = \u00bc Prob. of 0 or = P[{DD; UD; DU}] = \u00be X[{0; 2}] = P[{DD; UU}] = \u00bd Prob. of = P[{DU; UD; DD}] = \u00be Prob. 2}] = P X[] = P[] = 0 The empty set is simply needed to complete the -algebra (a technical point ). Its interpretation is not important since P[ ] = 0 for any reasonable P. Technical detail: P is the probabili ty measure over the sample space, , and P X is the probability measure over , the range of the random variable. Example: IBM Returns We buy an IBM share at USD 120 today and plan to sell the share next week. The return of IBM next week, , depends on how the market values IBM next week -this is the experiment. The sample space is continuous, from -100% (worst case scenario) to potentially a huge undefined positive number. We set = { : [-1, K], K > 0}. Possible events: - IBM returns are positive. - IBM returns are higher than 0.5%. - IBM returns are lower than 10%. - IBM returns are between -2% and 4%. The collection of all possible events, , is very, very large. We use a probability distribution, for example, the normal distribution, to desc ribe the likelihood of possible events. Probability Function & CDF Definition - The probability function , p(x), of a RV, X. For any random variable, X, and any real number, x, we define p(x) = P [ X = x ] = P[ {X = x } ], where { X = x} = the set of all out comes (event) with X = x. Definition - The cumulative distribution function (CDF), F(x), of a RV, X. For any random variable, X, and any real number, x, we define F(x) = P [ X x ] = P[ {X x} ], where { X x} = the set of all out comes (event) with X x. Example: Two dice are rolled and X is the sum of the two upward faces. Sample space S = 12}. Graph: Probability function: Note: { X = x } = for all other x. 0.000.060.120.18 23456789 1 0 1 1 1 2p(x) 122 1 , 136pP X P 233 1 , , X 2 , 3 , 136pP X P 456545, 6, 7, 8, 36 36 36ppppp 32110 , 11 , 1236 36 36ppp and 0 for all other px xGraph: CDF PDF for a Continuous RV Definition : Suppose that X is a random variable. Let f(x) denote a function defined for - < x < with the following properties: 1. 0 2. 1. 3. Then, f(x) is called the probability density function (pdf) of X. The random variable X is called continuous. PDF 00.20.40.60.811.2 05 1 0 1 36 3 36 6 36 10 36 15 36 21 36 26 36 30 36 33 36 35 360 2 23 34 45 56 67 78 89 91 0 10 1111 12 12 1x x x x x x x x x x x x F(x) is a step function Fx If X is a continuous random variable with probability density function, f(x), the cumulative distribution function of X is given by: CDF Also because of the FTC ( fundamental theorem of calculus ): dF xFxf xdx f(x) PDF for a Discrete RV A random variable X is called discrete if 1 All the probability is accounted for by values, x, such that p(x) > 0. Fx 01 -1 0 1 2 For a discrete random variable X the probability dist ribution is describe d by the probability function p(x), which has the following properties: 1. 0 1 2. 1 3. Bernouille and Binomial Distributions Suppose that we have a Bernoulli trial (an experiment) that has 2 results: 1. Success (S) 2. Failure (F) Suppose that p is the probability of success (S) and q = 1 - p is the probability of failure (F). Then, the probability distribu tion with probability function: 0 1qxpx PX xpx is called the Bernoulli distribution. We observe an independent Bernoulli trial ( S, F) n times. Let X be the number of successes in the n trials. Then, X has a binomial distribution : 0,1,2, ,xn x npxP X x p q x nx where 1. p = the probability of success ( S), and axbPa x b px a b 2. q = 1 - p = the probability of failure ( F) Example: If a firm announces profits and they are \"s urprising,\" the chance of a stock price, P, increase is 85%. Assume there are n=20 (independent) announcements. Let X be the number of increases in the stock price following surprising announcements in the n = 20 trials. The Poisson Distribution Suppose events are occurring randomly and uniformly in time. The events occur with a known average. Let X be the number of events occurr ing (arrivals) in a fixed peri od of time (time-interval of given length). Typical example: X = Number of crime cases coming before a criminal court per year (original Poisson's application in 1838.) Then, X will have a Poisson distribution with parameter : 0,1, 2, ,xn x npx PX x p q x nx 20 0.0000 0.0000 0.0000 0.0000 0.0002 0.0011 0.0046 0.0388 -0.05000.10000.15000.20000.25000.3000 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20p(x) 0,1, 2,3, 4,!x px e xx The parameter represents the expected number of occu rrences in a fixed period of time. The parameter is a positive real number. Example : On average, a trade occurs every 15 sec onds. Suppose trades are independent. We are interested in the probability of observing 10 tr ades in a minute (X=10). A Poisson distribution can be used with = 4 (4 trades per minute). Poisson probability function Poisson Distribution: Illustration Suppose a time interval is divided into n equal parts and that one event may or may not occur in each subinterval. - Event occurs - Event does not occur X = # of events is Bin(n,p) As n , events can occur over the continuous time interval n subintervals time interval X = # of events is Poisson () Poisson Distribution: Comments The Poisson distribution arises in connection w ith Poisson processes - a stochastic process in which events occur continuously and independently of one another. It occurs most easily for time-events; such as the number of calls passing through a call center per minute, or the number of visitors passing th rough a turnstile per hour. However, it can apply to any process in which the mean can be shown to be constant. It is used in finance (number of jumps in an asse t price in a given interval); market microstructure (number of trades per unit of time in a stock market); sports economics (number of goals in sports involv ing two competing teams); insurance (number of a given disaster - volcano eruptions/hurricanes/ floods- per year); etc. Example : The number of named storms over a period of a year in the Caribbean is known to have a Poisson distribution with = 13.1 Determine the probability function of X. Compute the probability that X is at most 8. Compute the probability that X is at least 10. Given that at least 10 hurricanes occu r, what is the probability that X is at most 15? Solution: x p(x) x p(x) e xx 13.1 13.1 0,1,2,3,4,!x exx The Normal distribution A random variable, X, is said to have a normal distribution with mean m and standard deviation s if X is a continuous random variable with probability density function f(x): 1 2exp 2 Normal distribution: Properties 1. Indexed by two parameters: (central parameter) & (spread parameter). 2. Symmetric around , which is the location of the maximum of f(x). Check: at most 8 8PP X 0 1 8 .09527pp p at least 10 10 1 9PP X P X 1 0 1 9 .8400 pp p at most 15 at least 10 15 10PP X X 15 10 10 15 10 10PX X PX PX PX 10 11 150.708.8400pp p 1 2exp 2 exp = 0 The last equality holds when = x. Thus, is an extremum point of f(x). Since f(x) is a pdf, it is the mode. 3. The inflection points of f(x) are - , + . (Check: set f''(x) = 0 and solve for x.) Normal distribution: Comments The normal distribution is often used to describe or approximat e any variable that tends to cluster around the mean. It is the most assumed distribution in economics and finance: rates of return, growth rates, IQ scor es, observational errors, etc. The central limit theorem (CLT) provides a ju stification for the normality assumption when n is large. Notation: PDF: x ~ N(, 2) C D F : (x) The Expectation of X: E(X) The expectation operator defines the mean (or population averag e) of a random variable or expression. Definition Let X denote a discrete RV with probability function p(x) (probability density function f(x) if X is continuous ) then the expected value of X, E(X) is defined to be: and if X is continuous with probability density function f(x) Sometimes we use E[.] as E X[.] to indicate that the expe ctation is being taken over f X(x) dx Interpretation of E(X) 1. The expected value of X, E(X), is the center of gravity of the probability distribution of X. 2. The expected value of X, E(X), is the long-run average value of X. (To be discussed later: Law of Large Numbers ) E[X]: The Normal Distribution Suppose X has a Normal distribution with parameters m and s. Then, E[X] = m. Proof: exp exp Making the substitution: Then, Using the following results: exp 1 and exp 0 Thus, Expectation of a function of a RV 00.10.20.30.4 01234567 E(X) xz 1 and dz dx x z 2 21 2zEXz e d z 22 221 22zzed z z ed z Let X denote a discrete RV with probability function , then the expected value of , , is defined to be: and if X is continuous with probability density function : Examples : g(x) = (x - )2 E[g(x)] = E[( x - )2] g(x) = (x - )k E[g(x)] = E[( x - )k] Example : Suppose X has a uniform distribution from 0 to b. Then: 1/ 0 x b 0 0, Find the expected value of A = X2 . If X is the length of a side of a squa re (chosen at random from 0 to b) then A is the area of the square = 1/3, the maximum area of the square Median: Another central measure A median is the numeric value separating the higher half of a sample, a population, or a probability distribution, from the lower half. Definition: Median The median of a random variable X is the unique number m that satisfies the following inequalities: P( X m) \u00bd and P(X m) \u00bd. For a continuous distribution, we )( )( mXm X dxxf dxxf Note: If the mean > median > mode (= most popular observation), the distribution will be skewed to the right. If the mean < median < mode , the distribution will be skewed to the left. Calculation of medians is a popular technique in summary statistics and summarizing statistical data, since it is simple to understand and easy to ca lculate, while also giving a measure that is more robust in the presence of ou tlier values than is the mean. 22 2 1b ba aEXx f x d x x d x 33 3 2 1 00 33 3b bx bb b An optimality property A median is also a central point which minimizes the average of th e absolute deviations. That is, a value of c that minimizes E(| X - c|) is the median of the probability distribution of the random variable X. Example : Let X have an exponential dist ribution with parameter . The probability density function of X is: 0 00xexfx x The median m solves the following integral of X: That is, m = ln(2)/. Moments of Random Variables The moments of a random variable X are used to describe the beha vior of the RV (discrete or continuous). Definition : K th Moment Let X be a RV (discrete or continuous), then the kth moment of X is: if is discrete if is continuous Definition: Central Moments Let X be a RV (discrete or continuous). Then, the kth central moment of X is defined to be: if is discrete if is continuous where m = m 1 = E(X) = the first moment of X. The central moments describe how the probability distribution is distribute d about the center of gravity, m. 2/1 )( mX dxxf 2/1 | m mx mx mxe e dxe dxe The first central moments is given by: The second central moment depends on the spread of the probability distribution of X about m. It is called the variance of X and is denoted by the symbol 2 = var(X): = var(X) = The square root of var(X) is called the standard deviation of X and is denoted by the symbol s = SD(X). We also refer to it as volatility : = Moments of a RV: Skewness The third central moment: contains information about the skewness of a distribution. A popular measure of skewness: Distribution according to skewness: 1) Symmetric distribution 2) Positively (right-) skewed distri bution (with mode < median < mean) 00. 010. 020. 030. 040. 050. 060. 070. 080. 09 0 5 10 15 20 25 30 350,0 3) Negatively (left-) skewed distri bution (with mode > median > mean) Skewness and Economics - Zero skew means symmetrical gains and losses. - Positive skew suggests many sma ll losses and few rich returns. - Negative skew indicates lots of minor wins offset by rare major losses. In financial markets, stock returns at the firm level show positive skewness, but at the aggregate (index) level s how negative skewness. From horse race betting and from U.S. st ate lotteries there is evidence supporting the contention that gamblers are not necessarily risk-lovers but skewness-lovers: Long shots are overbet (positive skewness loved!). Moments of a RV: Kurtosis The fourth central moment: It contains information about the shape of a distribution. The property of shape that is measured by this moment is called kurtosis, usually estimated by 00. 010. 020. 030. 040. 050. 060. 070. 08 0 5 10 15 20 25 30 35 00. 010. 020. 030. 040. 050. 060. 070. 08 0 5 10 15 20 25 30 350,0 0,0 = . The measure of (excess) kurtosis : 2 = 3 = 3 Note: We subtract 3, because the kurtosis of the Normal distribution is =3. Distributions: 1) Mesokurtic distribution 2) Platykurtic distribution 3) Leptokurtic distribution (usu al shape for asset returns) 00. 010. 020. 030. 040. 050. 060. 070. 080. 09 0 5 10 15 20 25 30 350, moderate in size 0 02 0 4 0 6 0 8 00, small in size 0 0 2 0 4 06 08 00, large in size Moments and Expected Values Note that moments are defined by expected values. We define the expected value of a function of a continuous RV X, , as If X is discrete with probability function p(x) Examples : = (x - )2 E[] = E[( x - )2] = (x - )k E[] = E[( x - )k] We estimate expected values with sample aver ages. The Law of Large Numbers (LLN) tells us they are consistent estimators of expected values. Estimating Moments We estimate expected values with sample averages. For example, the first moment, the mean, and the second central moment, th e variance, are estimated by: = ( N-1 adjustment needed for E ) Besides consistent, they are both are unbiased estimators of their respective population moments (unbiased = \"on average, I get the population parameter\"). That is, E \"population parameter\" E The Law e Numbers (LLN) Long history: Gerolamo Cardano (1501-1576) stat ed it without proof. Jacob Bernoulli published a rigorous proof in 1713. Theorem (Weak LLN) Let X 1, ... , XN be n mutually independent random variables each having mean and a finite variance 2 -i.e, the sequence { xN} is i.i.d. Let . Then, for any > 0 (no matter how small) P[ | - | < ] = P[ - < < + ] 1, as N There are many variations of the LLN. It is a general result: A sample average as the sample size goes to infinite tends to its expected value. Also written as: N . (convergence in probability) The Central Limit Theorem (CLT) The Central Limit Theorem (CLT) states conditions for the sequence of RV { } under which the mean or a sum of a sufficiently large number of xi's will be approximately normally distributed. Let , , ..., be a sequence of i.i.d. RVs with finite mean , and finite variance 2. Then, as N increases, N, the sample mean, approaches th e normal distribution with mean and variance 2/N. This theorem is sometimes stated as: means \"the limiting distribution (asymptotic distribution) is\" (or convergence in distribution ). Many version of the CLT. Two versions ar e commonly used in economics and finance: - The one above is the Lindeberg-L\u00e9vy CLT , with { x N} are i.i.d., with finite and finite 2. - The other one is the Lindeberg-Feller CLT. It requires { xN} | |2 2 n xi nn n i idxxfiix s Note: Lindeberg-Levy assumes random sampling -observations are i.i.d., with the same mean and same variance. Lindeberg-Feller allows for heterogeneity in th e drawing of the observa tions --through different variances. The cost of this more genera l case: More assumptio ns about how the { x N} vary. The CLT gives only an asympto tic distribution. We usually take it as an approximation for a finite number of observations. In these cases, the notation goes from to . Technical Note: The Berry-Esseen theorem (Berry-Esseen inequality ) attempts to quantify the rate at which the convergence to normality takes place. 2/13|)( )(| nCx xFn where = E(| X|) < and C is a constant (best current C=0.7056). Asymptotic Distribution An asymptotic distribution is a hypo thetical distribution that is the limiting distribution of a sequence of distributions. We will use the asymptotic distribution as a finite sample approximation to the true distribution of a RV when N -i.e., the sample size- is large . Practical question: When is N large? Sampling Distributions All statistics, T( X), are functions of RVs and, thus, th ey have a distribution. Depending on the sample, we can observe different values for T (X), thus, the finite sample distribution of T( X) is called the sampling distribution . For the sample mean, , if the Xi's are normally distributed, then the sampling distribution is normal with mean and variance 2/N. Or ~ N(, 2/N). Then, E[ ] = Var[ ] = 2/N variance of sample mean decreases as N increases! The SD of the sampling di stribution is called the standard error (SE). Then, SE( ) = /sqrt( N). We usually associate the standard error with th e precision of the estimate . That is, the precision of the estimation of the mean increases as N increases. Below, we show the sampling distribution fo r the sample mean of a normal population for different sample sizes ( N). Note: As N, -i.e., the distribution becomes a spike at ! , Note: If the data is not normal, the CLT is us ed to approximate the sampling distribution by the asymptotic one, usually, after some manipulatio ns. Again, in those cases, the notation goes from to . For the sample variance 2, if the Xi's are normally distributed, th en the sampling distribution is derived from this result: (N - 1) s2/2 ~ . It can be shown that a random variable that follows a distribution has a variance equal to 2 times the degrees of freedom (=2* v). Then, Var[( N-1) s2/2 ] = 2 * ( N - 1) Var[ s2] = 2 * 4 /(N - 1) Then, SE( s2) = SD( s2) = 2 * 2/ - 1. Note: If the data is not normal (& N is large), the CLT can be used to approximate the sampling distribution by the asymptotic one: N(2, 1/ ) where = (recall when data is normal, = 3). Remark: The precision of the estimation increases as N increases. This remark is especially relevant in Finan ce, where we derive relations between expected returns and risk factors, like market risk or vol atility. As we gather more data, expected returns and the volatility of returns will be more precisely estimated. Hypothesis Testing A statistical hypothesis test is a method of making decisions us ing experimental data. A result is called statistically significant if it is unlikely to have occurred by chance. These decisions are made using (null) hypothes is tests. A hypothesis can specify a particular value for a population parameter, say q=q 0. Then, the test can be used to answer a question like: Assuming q 0 is true, what is the probability of observing a value for the (test) statistic used that is at least as big as the value that was actually observed? Uses of hypothe sis testing: - Check the validity of theories or models. - Check if new data can cast doubt on established facts. In general, there are two kinds of hypotheses: (1) About the form of th e probability distribution Example : Is the random variable normally distributed? (2) About the parameters of a distribution function Example : Is the mean of a distribution equal to 0? The second class is the traditional material of econometrics. We may test whether the effect of income on consumption is greater than one, or whether the size coefficient on a CAPM regression is equal to zero. Hypothesis testing involves the comparison between two competing hypothesis (sometimes, they represent partitions of the world). - The null hypothesis, denoted H 0, is sometimes referred to as the maintained hypothesis. - The alternative hypothesis, denoted H1, is the hypothesis that will be considered if the null hypothesis is \"rejected.\" Idea: We collect a sample of data X = {X 1, ..., XN}. We construct a statistic T( X) = f(X), called the test statistic . Now we have a decision rule: - If T(X) is contained in space R, we reject H0 (& we learn). - If T(X) is in the complement of R (RC), we fail to reject H0. Note: T(X), like any other statistic, is a RV. It has a distribution. Example: Suppose we want to test if th e mean of IBM annual returns, IBM, is 10%. That is, H0: IBM = 10%. From the population, we get a sample: {X1962, X1963, XN=2020 }, with N=59. = , which is unbiased, cons istent, and, assuming X is normally distributed, we know its distribution, ~ N(, 2/N). Now, we need to determine the rejection region, R, such that if T(X) = [T LB, TUB] Reject H 0: IBM = 10%. That is, R = [ T LB, TUB ] Calculate T(X) = Get a sample (size N) IBM returns {X1962, X1963, ..., X2020} Q: How do we determine T LB and TUB and, thus, make a decision? Q: How do we determine T LB and TUB and, thus, make a decision? As shown below, we use the distribution of , which is derived under H 0. Note: In the Graph above, the blue area gives us the associated pr obability with R. That is, the probability, under H 0, that the observed T(X) = falls in the rejection re gion. The blue are is called the significance level . Hypothesis Testing: Steps We present the classical approach , a synthesized approach, known as significance testing . It relies on Fisher's p-value : the probability, of observing a result at least as extreme as the test statistic, under H0. We follow these steps: Step 1. Identify H 0 & decide on a significance level (%) to compare your test results. Step 2. Determine the appropriate test statistic T( X) and its distribution under the assumption that H 0 is true. Step 3. Calculate T( X) from the data. Step 4. Decision Rule: Reject H 0 if the p-value is sufficiently small, that is, we consider T( X) in R (we learn). Otherwise, we reach no conclusion (no learning). TLB TUB RC: (TLB, TUB). Q: What p-value is warrant rejection of H 0? Rule: If p-value < (say, 5%) test result is significant: Reject H0. If the results are \" not significant ,\" no conclusions are reached (no learning here). Go back gather more data or modify model. The father of this approach, R onald Fisher, favored 5% or 1%. Example: From the U.S. Jury System H 0: The defendant is not guilty H 1: The defendant is guilty In statistics we learn when we reject. In this cas e, we learn a defendant is guilty when the jury finds the defendant guilty, by rejecting H 0. Example: From the U.S. Jury System Step 1. Identify H 0 & decide on a significance level (%) H 0: The defendant is not guilty H 1: The defendant is guilty Significance level = \"beyond reasonable doubt ,\" presumably small level. Step 2. After judge instructions, each ju ror forms an \"innocent index\" T( X) i. Step 3. Through deliberations, jury reaches a conclusion T( X) = T(X)i . Step 4. Decision Rule: If p-value of T( X) < Reject H 0. That is, guilty! If p-value of T( X) > Fail to reject H 0. That is, non-guilty. Alternatively, we build a rejection region around H 0. Note: Mistakes are made. We want to quantify these mistakes. Failure to reject H 0 does not necessarily mean that the defendant is not guilty, or rejecting H0 does not mean necessarily the defendant is guilty. Type I error and Type II error give us an idea of both mistakes. Definition : Type I and T ype II errors A Type I error is the error of rejecting H 0 when it is true. A Type II error is the error of \"accepting\" H0 when it is false (that is, when H1 is true). Notation: Probability of Type I error: = P[X R |H0 is true] Probability error: = P[X RC |H1 is true] State of World Decision H 0 true H 1 true (H 0 false) Cannot reject (\"accept\") H 0 Correct decision Type II error Reject H 0 Type I error Correct decision Need to control bot h types of error: = P[rejecting H 0 |H0 is true] = P[not rejecting H 0 |H1 is true] Example: From the U.S. Jury System Type I error is the error of finding an innocent defendant guilty. Type II error is the error of finding a guilty defendant not guilty. In general, we think Type I error is the worst of the two errors, we try to minimize the error of sending to jail an innocent person. Actually, we would like Type I error to be zero. However, the only way to do this (100% of innocent defendants are found not guilty) is to never reject H 0. Then, we maximize Type II error . There is a clear trade-off between both errors. Traditional view: Set Type I error equal to a small number (defined in the U.S. court system as \" beyond reasonable doubt \") and design a test that minimizes Type II error . The usual tests F-tests tests) incor porate this traditional view. Example: We want to test if the mean is equal to 0. Then, 1. H 0: = 0. H 1: 0. 2. Appropriate T( X): t-test (based on unknown and estimated by s). Determine distribution of T( X) under H 0. Sampling distribution of , under H 0: ~ N(0, 2/N). Then, distribution of T( X) under H 0: t = s ~ t N-1 - when N > 30, t ~ N(0, 1). 3. Compute t, t , using , 0, s, and N. Get p-value (t). 4. Rule: Set an level. If p-value (t) < Reject H 0: = 0. Alternatively, if |t | > tN-1,/2 (=1.96, if =.05) Reject H 0: = 0. Technical Note 1: In step 2, the distribution of th e t-test, t, is exact only if {X} follows a normal distribution, otherwise, the di stribution is asymptotic (f or this we need a large N); that is t = s N(0, 1) . Technical Note 2: In step 2, we determine the di stribution of t, by using the sampling distribution of under H 0. If H 0 is not true, suppose = 1, then ~ N(1, 2/N), and, thus, t is distributed N(0, 1) only under H 0, since only under H 0 the E[ ] = 0. Lecture 1 - Appendix: Revi ew of Linear Algebra A Matrix A matrix is a set of elements, organized into rows and columns a and d are the diagonal elements. b and c are the off-diagonal elements. Matrices are like plain numbers in many ways: they can be added, subtracted, and, in some cases, multiplied and inve rted (divided). Example: ; . \u00b6 Dimensions of a matrix: numbers of ro ws by numbers of columns. The Matrix A is a 2x2 matrix, b is a 1x3 matrix. A matrix with only 1 column or only 1 row is called a vector . If a matrix has an equal numbers of rows and columns, it is called a square matrix. Matrix A, above, is a square matrix. Usual Notation: Upper case letters matrices Lower case vectors Matrices - Information Information is described by data. A tool to orga nize the data is a list, which we call a vector. Lists of lists are called matrices. That is , we organize the data using matrices. We think of the elements of X as data points (\"data entries\", \"observations\"), in economics, we usually have numerical data. We store the data in rows. In a Txk matrix, X, over time we build a database: X d cb arows columns Once the data is organized in matrices it can be easily manipulated: multiplied, added, etc. (this is what Excel does). In econometrics, we have a model y = f(x 1, x2, ... , xk), which we want to estimate. We collect data, say T (or N) observations, on a dependent variable, y, and on k explanatory variables, X. Under the usual notation, vectors will be column vectors: y and x k are Tx1 vectors: & j = 1,..., k X is a Txk matrix: X Its columns are the k Tx1 vectors xj. It is common to treat x1 as vector of ones, . Special Matrices - Identity and Null Identity Matrix: A square matrix with 1's along the dia gonal and 0's everywhere else. Similar to scalar \"1.\" I 1 0 0 1 Null matrix: A matrix in which all elements are 0's. Similar to scalar \"0.\" 0 0 0 0 0 Both are diagonal matrices off-diagonal elements are zero. Note: Both are examples of symmetric and idempotent matrices. As we will see later: - Symmetric: A = AT - Idempotent: A = A2 = A3 = ... Elementary Row Operations Elementary row operations: - Switching: Swap the positions of two rows - Multiplication: Multiply a row by a non-zero scalar - Addition: Add to one row a scalar multiple of another. An elementary matrix is a matrix which differs from the identity matrix by one single elementary row operation. If the matrix subject to elementary row operations is associated to a system of linear equations, then these operations do not change the solu tion set. Row operations can make the problem easier. Elementary row operations are used in Gaussian elimination to reduce a matrix to row echelon form. Matrix multiplication: Details Multiplication of matrices requires a conformability condition The conformability condition for multiplication is that the column dimensions of the lead matrix A must be equal to the row dimension of the lag matrix B. If A is an ( mxn) and B an (nxp) matrix ( A has the same number of columns as B has rows), then we define the product of AB. AB is (mxp) matrix with its ikth element is = . Example: Suppose we have a 1x2 vector a, and a 2x3 matrix B. What are the dimensions of the product: a*B? Dimensions: a(1x2), B(2x3) c(1x3). \u00b6 Example to multiply A (2x2) and B (2x2), where A has elements and B has elements . Recall the ikth element is = A = 21 79 B = 10 23 C = 21 7910 2321122013 71927093 Dimensions: A(2x2), B(2x2) C(2x2), a square : We want to multiply X (2x2) and (2x1), where X has elements and b has elements : X = & = We compute = X Recall the ith element is = Then, = = = Dimensions: X(2x2), (2x1) (2x1), a row vector. \u00b6 Transpose Matrix The transpose of a matrix A is another matrix AT (also written A) created by any one of the following equivalent actions: -write the rows (columns) of A as the columns (rows) of A T -reflect A by its main diagonal to obtain AT Formally, the ( i,j) element of A T is the ( j,i) element of A: [AT]ij = [A]ji Example : 389 10 4 31 80 94. \u00b6 Results: - If A is a m \u00d7 n matrix AT is a n \u00d7 m matrix. - (A')' = A - Conformability changes unless the matrix is square. - (AB)' = B'A' Example: In econometrics, an important matrix is X'X. Recall X: X a ( Txk) matrix Then, X' a ( kxT) matrix. \u00b6 Basic Operations Addition, Subtraction, Multiplication Just add elements Just subtract elements Multiply each row by each column and add Multiply each row by each column and add Example: 21 7931 0252 71 1 Addition 21 7910 2343 18 Scalar multiplication. \u00b6 Basic Matrix Operations: In Least Squares (LS) estimation, we minimize a sum of square errors ( for = 1, 2, ..., ): S ( , ) = Let be the x1 vector of errors. We use linear algebr a to write the sum of squares of its elements as (dot product of 2 x1 vectors): S( , ) = = Check: = [ .... ] = [ + + .... + ] = Thus, if we define = X + , LS estimation picks to minimize: S ( , ) = = ( - X) ( - X). Basic Matrix Operations: X X A special matrix in econometrics, XX (a kxk matrix): Recall X (Txk): X= & X' X' X = = = = Basic Matrix Operations: X Recall is a column vector of ones (in this case, a Tx1 vector): =1 1 ... 1 Given X ( Txk), ' X is a 1x k vector: 'X ... = ... Note: If x 1 is a vector of ones (representing a consta nt in the linear classical model), then: ' x1 = = = T (\"dot product \") Inverse of a Matrix Identity matrix: AI = A 100010001 3I Notation: Ij is a jxj identity matrix. Given A (mxn), the matrix B (nxm) is a right-inverse for A iff AB = Im Given A (mxn), the matrix C (mxn) is a left-inverse for A iff CA = In Theorem : If A (mxn), has both a right-inverse B and a left-inverse C, then C = B. Proof: We have AB = Im and CA = In. Thus, C (AB) = C I m = C and C(AB) = (CA)B = InB = B C(nxm)= B(mxn) Note: - This matrix is unique. (Suppose there is another left-inverse D, then D = B by the theorem, so D = C). - If A has both a right and a left inverse, it is a square matrix. It is usually called invertible . We say \"the matrix A is tricky: (ABC ) -1 = C-1 A-1 : If A (mxn) and B (nxp) have inverses, AB is ( AB)-1 B-1A-1 Proof: We have AA-1 = Im and A-1A = In BB-1 = In and B-1B = Ip Thus, B -1A-1(AB) = B-1 (A-1A) B = B-1 In B = B-1 B = Ip ( AB) B-1A-1 = A (BB-1) A-1 = A I n A-1 = A A-1 = Im AB is invertible and ( AB)-1 = B-1A-1 Note: It is not possible to divide one matrix by a nother. That is, we can not write A/B. For two matrices A and B, the quotient can be written as AB-1 or B-1A. In general, in matrix algebra AB-1 B-1A. Thus, writing A/B does not clearly identify whether it represents AB-1 or B-1A. We'll say B -1 post-multiplies and B-1 pre-multiplies A (for B-1A) Transpose and Inverse Matrix (A + B)' = A' + B' If A' = A, then A is called a symmetric matrix. Theorems : - Given two conformable matrices A and B, then ( AB)' = B'A' If A is invertible, A-1)' = (A')-1 (and A' Properties of Symmetric Matrices Definition: If A' = A, then A is called a symmetric matrix. In many applications, matrices are often sy mmetric. For example, in statistics the correlation matrix and the variance covariance matrix . Symmetric matrices play the same role as real numbers do among the complex numbers. We can do calculations with symmetric matrices like with numbers: for example, we can solve B 2 = A for B if A is symmetric matrix (& B is square root of A.) This is not possible in general. Theorems : - If A and B are nxn symmetric matrices, then ( AB)' = BA - If A and B are nxn symmetric matrices, then ( A+B)' = B+A - If C is any nxn matrix, then B = C'C is symmetric. - ( Spectral decomposition ) If A is nxn symmetric matrix, then it can be diagonalized as B = X -1AX, with an orthogonal X. Useful symmetric matrices: V = X'X P = X(X'X) -1X ' P : Projection matrix M = I - P = I - X(X'X)-1X' M : Residual maker Var[ b] = 2 (X'X)-1 OLS Variance of b Application 1: Linear System There is a functional form re lating a dependent variable, , and explanatory variables, X. The functional form is linear, but it depends on unknown parameters, . The relation between and X is not exact. There is an error, . We have T observations of and X. Then, the data is generated according to: = + i = 1, 2, ...., T. Or using matrix notation: y = X + where y & are ( Tx1); X is (Txk); and is (kx1). We will call this relation data generating process (DGP). The goal of econometrics is to estimate the unknown vector . Assume an economic model as system of linear equations with: a ij parameters, where i = 1,.., m rows, j = 1,.., n columns x i endogenous variables ( n), d i exogenous variables and constants ( m). + + ... + = d1 + + ... + = d2 + + ... + = dm We can write this system using linear algebra notation: A x = d ... = ... Summary: System of linear equations: Ax = d where A = (mxn) matrix of parameters x = column vector of endogenous variables ( nx1) d = column vector of exogenous variables and constants ( mx1) Solve for x*. Questions: - For what combinations of A and d there will zero, one, many or an infinite number of solutions? - How do we compute (characterize) those sets of solutions? Theorem : Given A (mxn) invertible. Then, the equation Ax = d has one and only one solution for every d (mx1). That is, there is a unique x*. x* = A -1 d Linear dependence and Rank: Example A set of vectors is linearly dependent if any one of them can be expressed as a linear combination of the remaining vectors; otherwise, it is linearly independent. Formal definition: Li near independence (LI) d = column vector A = (mxn) matrix x = column vector The set {, , ..., } is called a linearly independent set of vectors iff + + .... + = 0 = = ... = = 0. Notes: - Dependence prevents solvi ng a system of equations. Mo re unknowns than independent equations. - The number of linearly independent rows or columns in a matrix is the rank of a matrix (rank( A)). Examples: (1) 51 2 10 24 51 0 12 24 (a 2x2 matrix) 20 1 cannot invert A (2) 2 7;1 8;4 5; 214 785 3262 121 6 45 32 2. \u00b6 A matrix A has full row rank when each of the rows of th e matrix are linearly independent and full column rank when each of the columns of the ma trix are linearly independent. For a square matrix these two concepts are equivalent and we say matrix A has full rank . Determinant Test We can check if a matrix square matrix A has fu ll rank, that is, all its rows/columns are linearly independent by computing the determinant. If a square matrix A has full rank, it is invertible. That is, the determinant of a square matrix A detects whether A is invertible: If det( A) = 0 then A is not invertible (equivale ntly, the rows/columns of A are linearly dependent). Lecture 2 - Introduction: Re view, Returns and Data All the information and material is on my webpage: https://www.bauer.uh.edu/rsusmel/4397/4397.htm Textbook: Required: Introductory Econometrics for Finance , Cambridge University Press; 4th edition or older, by Chris Brooks. Recommended: R Guide for Introductory Econometrics for Finance, written by Chris Brooks. You can download it from my homepage (pdf format). It's also available for free through Amazon (kindle format). Install R in your machine. Many students strong ly prefer R Studio. Both will do fine. We will run programs and do some simple programing. Two midterms and a final (optional paper for MBA/MS class). There is a project in between midterms. Three homework: Two before first Midt erm and one before second Midterm. This Class This is an applied technical class, with some econometric theory and many stats concepts, followed by related financial applications. We will review many math and statistical topics. Some technical material may be new to you, for example Linear Algebra. The new material is introduced to simplify the exposition of the main concepts. You will not be required to have a deep understanding of the new mate rial, but you should be able to follow the intuition behind it. This is not a programming class, but we will use R to do computation and to estimate models. I will cover some of the basics in class. But, the more you know, the more comfortable you will be running the programs. For some students, the class will be dry (\" He fried my brain ,\" a student recently wrote.) Main Topics We will go over a lot of statistics and math tools: tests of hypothesis, bootstrap, linear regression, time series modeling, etc. But keep in mind that the goal of this class is to use statistical tools to understand financial issues. In this class we will try to answer the following questions: - How do we measure returns a nd risks of financial assets? - Can we estimate expected returns with preci sion? What about the va riance of returns? - Is the equity risk premium (excess returns of stocks over bonds) too high? - Can we explain asset returns? - How can one explain variations in stock returns across various stocks? - Is the CAPM a good model? What about the Fama-French factor models? - Do we need normality to test financial economics hypothesis or models? - How do we incorporate structural breaks in our models? Do we have to do it? - Are asset returns predictable? In the short run? In the long run? - How do we select a model to forecast asset returns? - Are markets efficient? - Does the risk of an asset vary with time? Wh at are the implications? How can one model time- varying risk? Topics Not Covered This course provides an introduction to the basics of financial econometrics, focusing on estimation of linear models and analysis of time series. There are many more topics in financial econometrics that we will not cover, among them: - Credit risk management and probability of default - Interest rate models and term structure models - Analyzing high-frequency data an d modeling market microstructure - Estimating models for options - Multivariate time series models - Technical methods such as state-space models and the Kalman filter, Markov processes, . 1 (1933), defined the field: \"Experience has shown that each of these three view-points, that of statistics , economic theory , and mathematics , is a necessary, but not by itself a sufficient, conditi on for a real understanding of the quantitative relations in modern economic life. It is the unification of all three aspects that is powerful. And it is this unification that constitutes econometrics.\" Mathematical Statistics Data Econometrics Economic Theory Financial Econometrics is applied econometrics to financial data. Th at is, we study the statistical tools that are needed to analyze and addre ss the specific types of questions and modeling challenges that appear in analyzing financial data. Always keep in mind that almost in all cases, fi nancial data is not \"experi mental data.\" We have no control over the data. We have to learn how to deal with the usual problems in financial data. Typical applications of ec onometric tools to finance: - Describe data. For example, expected returns & volatility. - Test hypothesis. For example, are stocks riskier than bonds? - Build and test models. For example, the different Fama-French factor Models. In general, in finance we deal with trade-o ff. The usual trade-off: Risk & Return. Then: - How do we measure risk and return? - Can we predict them? - How do we measure the trade-off? - How much should I be compensated for taking a given risk? Thus, we will be concerned with quantifying re wards and risks associated with uncertain outcomes. Trade-off application: Fund Management A fund manager has to allocate money across potentially many diffe rent investment alternatives to form portfolios. At the time of the investment, the fund manage r does not know what the return will be on each investment opportunity. (As we will s ee soon, returns are random variables.) However, the fund manager can sti ll make good investment decisions. Q: How? By quantifying the uncertainty associated with all the investment alternatives. For this purpose, the fund manager needs a model for the returns of all the different investment alternatives. From the model, the fund manager gets expected returns, variances & covariances. Using these pieces of information, the f und manager builds a portfolio. This Lecture In the first part of the lecture, we review some of the concepts discu ssed in Lecture 1 (sample statistics, distributions, random variables, descript ive statistics, etc.). In the second part, we go over returns, yields and, then, we st art to apply statistical concepts to financial data. We also start to introduce R concepts and to write some R programs. Review - Population and Sample Definition: Population A population is the totality of the elements unde r study. We are interested in learning something about this population. Examples: Number of alligators in Texas, percentage of unemployed workers in cities in the U.S., the total return of all stocks in the U. S., the 10-year Japanese government bond yield from 1960-2023. \u00b6 A Random Variable (RV) X defined over a population is called the population RV. The population RV generates the data. We call the population RV the \" Data Generating Process ,\" or DGP. Usually, the population is large, making a comple te enumeration of all the values in the population impractical or impossible. Thus, the de scriptive statis tics describing th e population - i.e., the population parameters - will be considered unknown. Typical situation in statistic s: we want to make inferences about an unknown population parameter using a sample -i.e., a small collection of observations from the general population , , ..., . We summarize the information in the sample with a statistic , which is a function of the sample. That is, any statistic summarizes the data, or re duces the information in the sample to a single number. To make inferences, we use the informati on in the statistic instead of the entire sample. Definition: Sample The sample is a (manageable) subset of elements of the population. Example: The total returns of the stocks on the S&P 500 index. \u00b6 Samples are collected to learn about the population. The process of collecting information from a sample is referred to as sampling . Definition: Random Sample Get a statistic and Make inferences (\"learn\") Get a sample (size N or T) Population (DGP) Sample A random sample is a sample where the probability th at any individual member from the population being selected as part of the sample is exactly the same as any other individual member of the population. Example: The total returns of the stocks on the S&P 500 index is not a random sample. \u00b6 In mathematical terms, given a random variable X with distribution F, a random sample of length N is a set of N independent, identically distributed ( i.i.d.) random variables with distribution F. We will estimate population parameters using samp le analogues: mean, sample mean; variance, sample variance; , b; etc. In general, in finance and economics, we do not deal with random samples. The collected observations will have issues that make the sample not a truly random sample. Review - Samples and Types of Data The samples we collect to learn about the population by computing sample statistics are classified in three groups: - Time Series Data : Collected over time on one or more variables, with a particular frequency of observation. For example, we record for 10 y ears the monthly S&P 500 returns, or 10' IBM returns. Usual notation: x t, t = 1, 2, ..., T. - Cross-sectional Data : Collected on one or more variables collected at a single point in time. For example, today we record all closing re turns for the members of the S&P 500 index. Usual notation: xi, i = 1, 2, ..., N. - Panel Data : Cross-sectional Data collected over time. For example, the CRSP database collects daily prices of all U.S. traded stocks since 1962. Usual notation: x i,t, i = 1, 2, ..., N & t = 1, 2, ..., T. The different types of data will present different problems; for exam ple, autocorrelated data is a common problem in time series. Review - Sample Statistic A statistic (singular) is a single measur e of some attribute of a sample (for example, its arithmetic mean value). It is calculated by applyi ng a function (statistical algorithm) to the values of the items comprising the sample, which are known together as a set of data. Definition: Statistic A statistic is a function of the observable random variable(s), which does not contain any unknown parameters. Examples: Sample mean ( ), sample etc. \u00b6 Note: A statistic is distinct from a population parameter. A statistic will be used to estimate a population parameter. In this ca se, the statistic is called an estimator . Review - Population and Sample Sample Statistics are used to estimate population parameters. Example: is an estimate of the population mean, . A hat over the Greek letter ( ). Suppose we want to learn about the mean of IBM annual returns, IBM. From the population, we get a sample: {X1962, X1963, ..., Xn=2023}. Then, we compute a statistic, . As we will see later, on average is a good estimator of . The definition of a sample statistic is very general. For example, ( + )/2 is by definition a statistic; we could claim that it estimat es the population mean of the variable X. However, this is probably not a good estimate. We would like our estimators to ha ve certain desirable properties. Review - Sample Statistic Some simple properties for estimators: - An estimator is unbiased estimator of if E[] = . - An estimator is most efficient if the variance of the estimator is minimized. - An estimator is BUE, or Best Unbiased Estimate, if it is the estimator with the smallest variance among all unbiased estimates. - An estimator is consistent if as the sample size, n, increases to , n converges to . We write n . (A LLN is behind this result.) - An estimator is asymptotically normal if as the sample size, n, increases to , n, often standardized or transformed, converges in dist ribution to a Normal distribution. We write n N(, Var(n)). (A CLT is behind this result.) Calculate and infer IBM Get a sample (size N) IBM returns {X1962, X1963, ..., X2023} Review - PDF for a Continuous RV Definition : Suppose that X is a random variable. Let f(x) denote a function defined for - < x < with the following properties: 1. 0 2. 1. 3. Then, f(x) is called the probability density function (pdf) of X. The RV X is called continuous. We use the pdf to describe the behavior of X. Analogous definition applies for a discrete RV, where the notation uses p(x) instead and the summation sign replaces the integral. The pdf is non-negative and integrates to 1. Remark: We use the pdf to describe the beha vior of the RV (discr ete or continuous). Review - Popular PDFs: Normal Distribution A RV X is said to have a normal distribution with parameters (mean) and 2 (variance) if X is a continuous RV with pdf f(x): 1 2exp 2 Note: Described by two parameters: and 2. We write X ~ N(, 2) When = 0 and 2 = 1, we call the distribution standard normal. We write X ~ N(0, 1). This is the distribution that is tabulated. The normal distribution is often used to describe or approximate any variable that tends to cluster around the mean. It is the most assumed distributi on in economics and finance: rates of return, growth rates, IQ scores, observational errors, etc. The central limit theorem (CLT) provides a just ification for the normality assumption when the sample size, n, is large. Notation: PDF: X ~ N(, 2) C D F : (x) Review - Popular PDFs: Gamma Distribution Let the continuous RV X have density function): 10 00xxe xfx x where , > 0 and () is the gamma function evaluated at . Then, X is said to have a Gamma distribution with parameters and , denoted as X ~ Gamma(, ) or (). It is a family of distributions, with special cases: - Exponential Distribution, or Exp( ): = 1. - Chi-square Distribution, or : = /2 and = \u00bd. The Chi-square distribution, , will appear a lot in this class, since it is derived from a sum of independent square standard normals. It is the distribution of many popular test statistics. Below we plot the Chi-square di stribution with parameter , which we refer as degrees of freedom: Note: When is large, the converges to a N( , 2). Review - Popular PDFs: Other Distributions Other distributions that we will use in this cl ass: the t-distribution and the F-distribution. The t-distribution is the ratio of a standard normal and the square root of a Chi-squared distribution, divided by its de grees of freedom. That is , let Y ~ N(0, 1) and W ~ , then t = / ~ . The t-distribution is indexed by its degrees of freedom. It l ooks like a normal distribution, but with thicker tails. As increases, the t-distribution converges to a standard normal distribution. The F-distribution is the ratio of two independe nt Chi-squared distribut ions, divided by their degrees of freedom. That is, let ~ and ~ , then F = / / ~ , The F distribution is indexed by two degrees of fr eedom, informally referred as \"numerator and denominator degrees of freedom.\" We will use both distributions in the context of testing null hypothesis. Note: t 2 ~ ,. Review - CDF for a Continuous RV If X is a continuous random variable with probability density function, f(x), the cumulative distribution function (CDF) of X is given by: 00. 10. 2 048 1 2 1 6( = 4) ( = 5) ( = 6) Note: The FTC ( fundamental theorem of calculus ) implies: Review - The Empirical Distribution The empirical distribution (ED) of a dataset is simply the distributi on that we observe in the data. The ED is a discrete distribution that gives e qual weight to each data point, assigning a 1/ N probability to each of the original N observations. We form a cumulative distribution function, F*, that is a step function that jumps up by 1/ N at each of the N data point: F*(x) = , where I(.) is the indicator function: =1, if =0, if Example: We throw 100 times two dice and sum the results. The CDF is given below: In general, we use a histogram to describe the ED of a dataset. \u00b6 Important result: Let F be the true distribution of the data and F be the ED of the data. As N, the Law of large numbers (LLN) tells us that F becomes a good approximation of F. Review - Histogram of a RV Recall that a histogram is an approximate representation of the distribution of numerical data. Example: We use a histogram to estimate the distribution of a RV. Let X = Percentage changes in the CHF/USD exchange rate = e f and for MSCI USA Index returns . Data: For the CHF/USD exchange rate, we have monthly data from January 1971 to June 2020 (N=595 observations) and for the MSCI USA return s we have monthly data from January 1970 to June 2020 ( N=607). Note: We overlay a Normal density (blue line) over the histogram. \u00b6 Review - Moments of Random Variables The moments of a random variable X are used to describe the beha vior of the RV (discrete or continuous). Definition : K th Moment Let X be a RV (discrete or continuous), then the kth moment of X is: if is discrete if is continuous Definition: Central Moments Let X be a RV (discrete or continuous). Then, the kth central moment of X is defined to be: if is discrete if is continuous where = = E(X) = the first moment of X. The central moments describe how the probability distribution is distributed about the center of gravity, m. The first central moments is given by: The second central moment depends on the spread of the probability distribution of X about m. It is called the variance of X and is denoted by the symbol 2 = var(X): = var(X) = The square root of var(X) is called the standard deviation of X and is denoted by the symbol s = SD(X). We also refer to it as volatility : = Review - Moments of a RV: Skewness The third central moment: contains information about the skewness of a distribution. We use skewness as a gauge of symmetry. If 0 the distribution is symmetric; otherwise, asymmetric. A popular measure of skewness: Distribution according to skewness: 1) Symmetric distribution 2) Positively (right-) skewed distri bution (with mode < median < mean) 3) Negatively (left-) skewed distri bution (with mode > median > mean) 0,0 0,0 Skewness and Economics For changes in asset prices: - Zero skew means symmetrical gains and lo sses -i.e., extreme values tend to occur on both sides of the curve on similar proportions. - Positive skew suggests many small losses and fe w rich returns -i.e., extreme values tend to occur in the right tail - Negative skew indicates a lot of minor wins o ffset by rare major losses -i.e., extreme values tend to occur in the left tail. In financial markets, stock returns at the firm level show positive skewness, but at the aggregate (index) level show negative skewness. From horse race betting and from U.S. state lotteries there is evidence supporting the contention that gamblers are not necessarily risk-lovers but skewness-lovers: Long shots are overbet (positive skewness loved!). Review - Moments of a RV: Kurtosis The fourth central moment: It contains information about the shape of a distribution. The property of shape that is measured by this moment is called kurtosis, usually estimated by : = . Kurtosis measures how much weight there is in the tails of the distribution relative to the middle (we call this a measure of the \" fatness \" of the tails). We usually compare the kurtosis of a series relative to the kurtosis of a nor mal distribution, which is equal to 3. We measure the \"excess\" fatness of the tail over the normal curve. That is, the measure of (excess) kurtosis : 2 = 3 = 3 Distributions: -4 -3 -2 -1 0 1 20.0 0.1 0.2 0.3 0.4Density for Simulated Data with Negative Skew Simulated DataDensity0,0 1) Mesokurtic distribution 2) Platykurtic distribution 3) Leptokurtic distribution (usu al shape for asset returns) Positive excess kurtosis, 0, is the norm for financial returns. Below I simulate a series with =0, =1, zero skewness & kurtosis = 6 ( =3), overlaid with a standard normal distribution. Fat tails are seen on both sides of the distribution. 0 0 0 Review - Moments and Expected Values Note that moments are defined by expected values. We define the expected value of a function of a continuous RV X, , as If X is discrete with probability function p(x) Examples : g(x) = (x - )2 E[g(x)] = E[( x - )2] g(x) = (x - )k E[g(x)] = E[( x - )k]. \u00b6 We estimate expected values with sample averages. As we will see below, the Law of Large Numbers (LLN) tells us they are consistent estimators of expected values. Review - Estimating Moments We estimate expected values with sample averages. For example, the first moment, the mean, and the second central moment, th e variance, are estimated by: = ( N - 1 adjustment needed for E ) They are both unbiased estimators of their respective population moments (unbiased = \"on average, I get the population parameter\"). That is, E \" is the population parameter of interest\" E \" is population parameter of interest\" Review - Law of Large Numbers (LLN) Long history: Gerolamo Cardano (1501-1576) stat ed it without proof. Jacob Bernoulli published a rigorous proof in 1713. Theorem (Weak LLN) Let X1, ... , XN be N mutually independent random variables each having mean m and a finite s - i.e, the sequence { XN} is i.i.d. Let . Then, for any > 0 (no matter how small) P[| - | < ] = P[ - < < + ] 1, as N There are many versions of the LLN . It is a general result: A sample average as the sample size goes to infinite tends to its e xpected value. Also written as: N . (convergence in probability) Review - Central Limit Theorem (CLT) Let X1, X2, ..., XN be a finite mean m, and finite variance s2. Then as N increases, N, the sample mean, approaches th e normal distribution with mean and variance s2/N. This theorem is sometimes stated as _ 0,1 where means \"the limiting distribution (asymptotic distribution) is\" (or convergence in distribution ). Many versions of the CLT. This one is the Lindeberg-L\u00e9vy CLT . The CLT gives only an asymptotic distribution. We usually take it as an approximation for a finite number of observations. In these cases, the notation goes from to . Review - Sampling Distributions All statistics, T( X), are functions of RVs and, thus, th ey have a distribution. Depending on the sample, we can observe different values for T (X), thus, the finite sample distribution of T( X) is called the sampling distribution . For the sample mean, , if the Xi's are normally distributed, then the sampling distribution is normal with mean and variance 2/N. Or ~ N(, 2/N). Note: If the data is not normal, the CLT is us ed to approximate the sampling distribution by the asymptotic one, usually after some manipulations . Again, in those cases, the notation goes from to . The SD of the sampling di stribution is called the standard error (SE). Then, SE( ) = /sqrt( N). Example: We plot a Sampling Distribution for the sample mean, , of a normal population, as a function of the sample size ( N). For this purpose, we genera te 10,000 samples from a N(2, 4) population. We plot th e distribution of for three sizes of N = 10, 60 & 200: Note: As N , -i.e., the distribution becomes a spike at =2! \u00b6 For the sample variance 2, if the Xi's are normally distributed, th en the sampling distribution is derived from this result: (N - 1) s2/2 ~ . We use the properties of a to derive the mean & variance of : Property 1. Let Z ~ . Then, E[ Z] = . Property 2. Let Z . Then, Var[ Z] = 2. Application: 1 /2 ~ From Property 1 : E[ 1 /2] = 1 E From Property 2 : Var[ 1 /2 ] = 2* 1 Var 2/(N - 1). SE() = SD() = 2 * sqrt[2/( N - 1)]. Summary for : Sampling distribution: ( N - 1) /2 ~ . N = 10 N = 60 N = 200 M e a n : E Variance: Var 2/(N - 1). Note: If the data is not normal (& N is large), the CLT can be used to approximate the sampling distribution by the asymptotic one: N(2, 1/ ) where = (recall when data is normal, = 3). Example: We plot a Sampling Distribution for the sample variance, s2, of a normal population, as a function of the sample size ( N). Above, we generated 10,000 samples from a N(2, 4) population. Now, we plot the distribution of s2 for three sizes of N = 10, 60 & 200: Note: As N , the distribution of looks more Normal - the CLT at work! \u00b6 Review - Estimating Moments in R First, we need to import the data. In R, we use the read function, usually followed by the type of data we are importing. Below, we import a comma separated values (csv) file with monthly data for the S&P Composite Index (P), Dividends (D ), Earnings (E), CPI, Long interest rates (Long_i), and some transformations of the data (Real Prices, Real Dividends, Real Returns, etc). We use the read.csv function: Sh_da <- read.csv (\"https://www.bauer.uh.edu/rsusmel/4397/Shiller_2021_m_data.csv\", head=TRUE, sep=\",\") To check the names of the variables we imported, we use the names() function. It describes the headers of the file imported (41 10 N = 60 N = 200 The summary() function provides some st ats of variables imported: > summary(Sh_da) Date P D E CPI Min. :1871 Min. : 2.73 Min. : 0.180 Min. : 0.16 Min. : 6.28 1st Qu.:1908 1st Qu.: 7.89 1st Qu.: 0.420 1st 0.56 1st Qu.: 10.19 Median :1946 Median : 17.35 Median : 0.870 Median : 1.45 Median : 20.30 Mean :1946 Mean : 321.51 Mean : 6.732 Mean 15.15 :59.680 Max. :139.47 Max. :270.80 Second, we extract from the imported data, Sh_da, the column corresponding to the i_10 and for, later use, the S&P 500 Index (SP):That is , we extract from Sh_da, the column corresponding to the 10-year interest rate (Long_i) and, for later use, the S&P Composite Index (P): SP <- Sh_da$P # Extract P = S&P500 series i_10 <- # Extract Long_i = Interest rates N <- length(SP) # Length of data Then, we estimate the sample moments for. x <- i_10 # Series to be analyzed n <- length(x) # Number of observations ( ) m2 <- in denominator of both m3 <- sum((x-m1)^3)/n # For numerator of S m4 <- sum((x-m1)^4)/n # For numerator of K b1 # Sample Skewness ( SD > m1 # Sample mean (4.51% annual) [1] 4.509972 > s 2 # S a m p l e V a r i a n c e [1] 5.306247 > sd_s # Sample SD (2.30% > b1 # Sample # Sample Kurtosis [1] 6.751023 2.A presents a summary of the 2.A - 10-year Bond Rate (\"Long interest Rate\"): (1871: February - 2021: September) Statistic e f Mean 4.51 Median 3.82 Maximum 15.32 Minimum 0.62 Std. Dev. 2.30 Skewness 1.7951 Kurtosis 6.7510 Interest rates are right skewed and have kurtosi s greater than 3, pointing out to non-normality of data (\" fatter tails \"): 2 = - 3 = 3.7510. \u00b6 Returns Returns have better statistical properties than prices, as we will mention below, returns have a well-defined (long-run) mean and va riance, while asset prices, in general, do not. Thus, financial models tend to focus on returns. The return is the profit rate of hol ding an asset from time t 1 to t. We define net or simple (total) return, R t , as: 1 1 = capital gain + dividend yield where Pt = Stock price or Value of investment at time t D t = Dividend or payout of investment at time t Note: This is the return from time t-1 to time t. To be very explicit we can write this as ,. Then, the gross (total) return is given by: 1 1 In general, when the word \"total return \" is used in the definition, it means \" returns including dividends. \" Sometimes, total returns are also called \" overall returns .\" If D t = 0, the total return is just th e capital gain. In this situati on, it is common to just use the word returns . There is another commonly us ed definition of return, the log return , , defined as the log of the gross return: = log(1 + Rt) = log( + ) - log() Note: When the values are sma ll (-0.1 to +0.1), the two returns are approximately the same: Rt. In general -i.e., when returns are not small, r t < Rt . Derivation: Recall: ln(1) = 0, & . Now do a 1st-order Taylor expansion around x0 to get log( x) log() + | log(x0) + Thus, expanding around x0 = 1, we have for x 1: log( x) 0 + 1 1 Set x = (1 + R t) to get the result. The log return is also called continuously compounded return. When returns are small, say for daily or weekly data, the numerical differences between simple and compounded returns are very small. In this class, we will use log returns. Example: We estimate sample averages for e f = log returns for the CHF/USD . Note that there is no dividends or payouts for holding currency. Th at is, in this case, returns = capital gains. PPP_da <- read.csv(\"https://www.bauer.uh.edu/rsus mel/4397/ppp_2020_m.csv\",head=TRUE,sep=\",\") x_chf <- PPP_da$CHF_USD Now, we define e f = log returns ( % changes) for the CHF/USD . T <- length(x_chf) # Size of series read ( T or N notation is OK) e_chf <- log(x_chf[-1]/x_chf[-T]) # Log returns Then, we estimate the sample moments for. x <- e_chf # Series to be analyzed N <- length(x) # Number of observations ) m2 <- sum((x of both m3 <- sum((x - m1)^3)/N # For numerator of S m4 <- sum((x - m1)^4)/N # For numerator of K m3/m2^(3/2) # Sample Skewness ( ) [1] 4.621602 Summary of moments of e f = % changes in the CHF/US D exchange rate (1971:Jan - 2020:Jun): Statistic e f Mean -0.002551 negative skewness, kurtosi s greater than 3, pointing out to fatter tails: 2 = 3 = 1.62. \u00b6 Portfolio Returns For portfolios, the simple rate of return has an ad vantage: The rate of return on a portfolio is the portfolio of the rates of return. Let V P,t be the value of a portfolio at time t. , = , where Ni is the investment in asset i, which has a value , at time t. Then, the return , ,, , , = , where is the portfolio weight in asset i. This relationship does not hold fo r log returns because the log of a sum is not the sum of the logs. Multi-period holding return To simplify notation, include divi dends into prices. That is, Pd ,t +1 = Pt +1 + Dt return ,, , 1 , 1,1 Or 1, = 1 , 1, For small returns, we can use the log approximation: , , , The k-period gross holding return 1, = 1 , Or with the log approximation: , = , If the (expected) returns are equal, that is, ,. Then, the log approximation produces: , = , If the returns, , , are independent (covariance is 0) a nd with a constant variance equal to (a constant), then under the log approximation V a r ( , , Then, the SD is equal to SD(, Real returns We will deflate values by a Price Index, for example the CPI. Then, Real Price t Then, the real return becomes: 1 1 1 1 where is the inflation rate at time t. The log approximation (for small returns) produces Example : Below, we plot the long-run S&P Composite Index monthly data, nominal and real. Data taken from Robert Shiller's website = S&P Composite series D <- Sh_da$D # Extract D = S&P Dividends series CPI <- Sh_da$CPI # Extr act CPI = Price Index series R_SP <- Sh_da$Real_P Extract R_SP = # Define log returns lr_R = 3, col = c(\"blue\", \"red\")) R Note: A more elegant plot, with dates, can be done with the packag e ggplot2. You need to install it first with install.packages(\"ggpl ot2\"). Then, you need to call it, with: library(ggplot2) ggplot(data = Sh_da, aes(x = Date)) + SP, color = \"Nominal\")) + geom_line(aes(y = R_SP, color = \"Real\")) \"S&P Index: = \"Period: January 1871: September 2021\") Long-run S&P 500 monthly Robert Shiller's we bsite (1871:Jan N = # Define log (percent) changes D[-1]/12) - log(SP[-T] ) # log (total) returns, includes dividends I <- log(CPI[-1]/CPI[-T]) # Log Inflation rate Prices have a clear trend, returns do not. In statistics, we prefer to work with data with no trends, like returns; they have better properties, for example, a well defined long-run mean (expected value). Distribution of S&P 500 monthly log returns fat tail mean changes with time (& variance too) non-stationar y data mean seems constant over time (& variance too) stationary data Note: We observe slight negative skewness -left tail events are more common (steeper) than right tail events and fatter tails than the normal distribution. Thes e two features are part of the \"stylized facts \" for stock (index) returns. Example: Table 2.B reports univariate stat istics for Shiller's monthly S&P 500 returns (total, capital gains & real) and U.S. inflation . Table 2.B - S&P Composite Returns (Total, Ca pital Gains and Real) and Inflation Rate: (1871: February - 2021: September) Total Return Capital Gains Inflation Real Total Return 0.007378 are slightly (left-) skewed (& median > mean), and with \" fat tails \"-i.e., kurtosis is higher than 3. Check some results from log approximation: (1) (for small % changes) 0.005615 0.007378 0.001707 = 0.005671 (2) Multiperiod return: = 12 -i.e., from monthly to annual, - annual return = 0.007378 * 12 = 0.088531 (8.85%) - annual SD = 0.040455 * sqrt( 12) (14.01%) Note: Compounding (1 + 0.007378 )^12 - 1 = 0.0922. According to the annualized numbers from the above Table, since 1871, the average total stock market return has been 8.85% per year. \u00b6 Returns: Sample Moments - Changing Frequency Assuming independence of returns and constant mo ments, we can use the log returns to easily change frequencies for the mean and variance of returns. Suppose we have compounded data in base frequency b (say, monthly), but we are interested in compounded data in frequency q (say, annual) . The approximation formulas for mean and standard deviation (SD) are: q -frequency mean = b-freq mean * q/n = sqrt( q-freq SD) Example: Using the data from the previous table we calculate the weekly mean and standard deviation for returns ( b=30, q=7). - weekly return = 0.007378 * (7/30) 0.00172 (0.172%) - Returns: Sampling Distribution Recall that the sampling distribut ion of the sample mean is: ~ N(, 2/N) Example: Before, using monthly S&P 500 log returns ( N = 1805 ), we got: Estimated Monthly mean return = = 0.007378 Estimated Var( ) = s2/N = 0.0404552 /1805 = 9.067075e-07 The SD of the monthly mean (also called the Standard Error, SE): S.E.( ) = , .000952) Note: Compared to returns, expected returns, estimated by the sample mean, are more precisely estimated ( 0.1% vs 4.05% ). Not surprised, the sampling distribution of the mean shrinks towards the population mean as N increases. \u00b6 Yields Consider an nperiod discount bond. Time is measured in years. Today is t . Bond (asset) pays Ft +n dollars n years from now, at t + n. Ft +n = Face value (value at time t + n ). Pt = Market price of the bond. rn,t = Yield to maturity (YTM) at time t for a maturity of n years. n = Maturity of bond P t , Interpretation: If our initial capital, P t dollars, is invested toda y at the interest rate rn,t for n years compounded annually, then, at time t+n, the payoff is . YTM, r n,t , is a raw number. 4% at an annual rate is 0.04. Continuous Compounding More generally, suppose an investment is compounded m times per n years; where m is number of times return (yield) is compounded, for example, m = 4 for quarterly, m = 12 for monthly, m = for continuous compounding. Then, the market price of the bond is: As m , 1 where we used lim 1 = Then, Suppose the continuously compoun ded bond at maturity pays $1 (= ) and the remaining duration is D units of time. Then, P t $1 Then, the log return per year is: log( ) - log() = D ( where we ignore that D has one unit of time less at time t+1. That is, the da ily return of a bond is the change of yields multiplied by its duration. Now, suppose we invest in a bond with continuous co mpounding at an annual rate . Then, the value of the investment at year t is: V t The log return (log of gr oss return) per year is : log(V t+1) - log(V t) = The simple annual interest rate r quoted in the market is the annua l log return if the interest is compounded continuously. The effective annual in terest rate, r a, is simple the annual rate of return: 1 = 1 1 1 Example: Table 2.C reports descriptiv e statistics for monthly long-run S&P 500 (blue) returns and 10-year bond (red) rates , from Robert Shiller's website. Interest Returns are reported annually. Table 2.C - S&P Composit Returns & 10-year Bond Rates (1871-2021) Total Return 10-year S&P returns (blue) and 10-year Bond interest rates (red) data. Note: Interest rates have a lower monthly mean (0.003759 = 0.04511 /12), & lower volatility and kurtosis than stock returns. We plot the histogram for bond rates , with a normal curve (in blue) for comparison purposes Returns: Expected Returns & The Equity Risk Premium As mentioned above, returns are not very prec isely estimated. They have a large variance. Things get better for expected returns, which are estimated by the sample mean, since the S.E. of the sample mean gets smaller with N. That expected returns are bette r estimated that returns is a good property for financial models, since expect ed returns are a key component of every valuation model. The expected return on any investment can be written as the sum of a risk-free rate and a risk premium to compensate for the risk of the investment . A key element in equity valuation models is the risk premium that investors de mand to hold \"average (equity) risk\" or market (equity) risk, which in turn affects the prices of all risky investments. The difference between the expected risky market return and a risk-free rate is called the equity risk premium or ERP: ERP = E[ - )], where E[] is the expected return on a well- diversified market portfolio and is the risk-free rate. It is common to find that the ERP changes over time. In this case, we write the ERP at time , , as: = - ] where the subscript means the expectations for period are taken at time . In standard macroeconomic-finance models, th e ERP is determined by the aggregate risk aversion of investors and the volatil ity of equity markets. The ERP drives expectations of future equity market returns. It is an input in equi librium asset pricing models like the capital asset pricing model (CAPM) and multi-factor models, like the Fama-French 3-factor model. Returns: Expected Returns & The ERP - Components To calculate the ERP we need , and E[]. (1) The risk-free rate . The risk-free rate, , is approximated by the mean yield of government securities, typical examples, th e 3-month U.S. Treasury bill or the 10-year U.S. Treasury bond. In general, given the upward sloping terms struct ure, using the T-bill rate will give result in a higher E[ - )]. The risk-free rate used to compute the ERP has to be consistent with the risk-free rate used to compute expected returns. If we are estimating the co st of capital for a project that is expected to last 10 years, r f is approximated by the yield of 10-year government securities, for example, 10- year Treasury bonds. (2) Expected Market Return. To determine E[ ,] we need to determine the Market Portfolio. Q: What is the Market Portfolio? In theory, it re presents the universe of risky assets. In practice it represents the universe of trad ed equities, not just domestic, but in the world. Returns on this equity market portfolio should be me asured free of survivor bias. . In general, we approximate (\"proxy\") the Market Po rtfolio, with a well-div ersified equity index. Also, in general, market-weighted indexes ar e preferred. For example, the S&P 500 Index, the MSCI World Index, or the Weight ed Average of CRSP returns. Q: How do we calculate E[ ,]? There are three different ways to compute E[ ,]: 1) Surveys . Usually an average of ERPs provided by indivi dual investors, inst itutional investors, managers and, even, academics. 2) Historical data . Expectations are computed using pa st data. This is the most popular approach. For example, compute E[ ,] with sample averages of market returns, . As we have seen above, if we use this approach, it pays to use as much data as possible -more data, lower S.E. We think of E[ ,] as a long-run average of market returns. Robert Shiller's website has U.S. market equity returns from 1871, but if we think that the ERP is time varying, for example we consider the stock market in 1871 and in 2021 are very different, it may be better to use a shorter period, for example, 50 years or, may be shorter period, 20 years to compute the ERP. 3) Forward-looking data . We derive an (implied) ERP us ing market prices, for example, market indexes, options & futures on market inde xes, etc. Of course, we also need a model (a formula) that extracts the ERP from market prices. Returns: Expected Returns & The ERP - The Equity Risk Premium Puzzle Once we determine E[ ,] and the risk-free rate, we are ready to calculate the ERP. But, keep in mind that we make decisions along the way. For example, using Shiller's monthly data, with 1 50 years of data, we produce an estimate of the ERP = E[ - )]. Decisions made: comput ation of returns (log returns); method of computing ERP (Historical data); sample period (1871-2021) ; market portfolio (S&P Composite Index); risk-free rate (10-year U.S. bond rate). Then, Annualized Market return = 0.007378 * 12 = 0.088536 Annualized risk-free rate = = 0.088536 - 0.04511 = 0.043426 (4.34% ) Many economists would consider this estimate of the ERP \" too high. \" Why is 4.34% too high? Using standard macroeconomic (neoclassical) models, the degree of risk aversion to justify it is unreasonable high, between 30 and 50, while a reasona ble degree of risk aversion is no larger than 10. A too high (for economic models) ERP was first reporte d by Mehra and Prescott (1985), which they estimated around 6%. According to their calibration of their \"standard\" model, the ERP should be, at most, 1%. Mehra and Prescott (1985) labele d the incompatibility of theory & observed data the equity risk premium puzzle . There have been many attempts to explain the puzzle: statistical artifact (s urvivor bias); disaster insurance (peso problem/sample period), transa ction costs & taxation, model's preferences, behavioral issues (mainly, myopi c loss aversion & overreactions). Damodaran (2021), who produces an annual update of the literature and the numbers, said in overview of the possible explanations: \"It is true that histor ical risk premiums are higher than could be justified using conventional utility models for wealth. However, that may te ll us more about the dangers of using historical data and the failures of classic utility models than they do abo ut equity risk premiums.\" Returns: Expected Returns & The ERP - Wide Range Is it 4% or 6%? It turns out that even with 100+ years of data for developed markets there is no consensus on an ERP. Different choices in how to compute the ERP produce different estimates. For example, for the U.S. market, considered one of the best in terms of quality of data, Duarte and Rosa (2015) list over 20 approaches (\"models\") to estimate the ERP in the U.S. With 1 960-2013 data, D&R (2015) report estimates from -0.4% to 13.1% , with a 5.7% average for all models. A wide range! This wide range is consistent with the reported averages for developed markets that we presen t below in Tables 2.D and 2.E. In Table 2.D, using 50 years of monthly return data of log returns for some developed markets, we report mean equity annualized retu rns (in USD), the annualized standard deviation of returns (in USD) and the annualized ERP, wh ich, again, is defined as the expected market excess return over th e risk-free rate. There is a wide range of ERP estimates from 0.88% (Italy) to 11.56% (HK), using the sample mean return for the MSCI country index (in USD) and the average U.S. T-Bill rate for the period ( 4.50% ). The World Index, a weighted average of all equity markets, has an ERP equal to 3.17% , where the EAFE (Europe, Australia and the Fa r East) Index has a slightly lower ERP, 3.06%. Table 2.D - MSCI Index USD Equi ty Returns and ERP: (1970-2021) Market (N = 620) Equity Return Standard Deviation ERP U.S. 8.31 15.01 3.82 Canada 7.95 19.21 3.46 France 8.80 Kong 7.69 16.64 3.06 In Table 2.E, using 34 years of monthly return data or less , we report ERP (annualized) estimates for some emerging markets (in USD), wh ere we observe a big dispersion of estimates, with higher mean returns, but al so, higher standard deviations. Table 2.E - MSCI Index USD Equity Returns and ERP: (1987* - 2021) Market (N) Equity Return Standard Deviation ERP Argentina (404) 24.21 51.49 19.72 Brazil (404) 22.23 South 9.47 26.31 4.98 World (620) 7.66 14.54 3.17 EM Asia 8.85 23.13 4.36 We use the SE as a measure of precisi on of an estimate. For the sample mean, , we have: S.E.( ) = s Using the previous data, we calculate the S.E.( ) for several markets: big difference in precision betwee n Developed and Emerging Markets. Note: Notice the effect of N. Hong Kong has a larger SD than India and China, but more observations make the mean estimate more precise. Returns: Risk-Return - The Sharpe Ratio The most commonly cited statistics that provides a measure of th e risk-return trade-off for an asset is the Sharpe ratio (SR), the ratio of the excess expected return of an asset to its risk, measured by its return volatility (SD). We estimate the SR of asset i with = i - r f , where i is the sample mean ( ) return of asset i, is the risk-free rate and is the SD of the return of asset i. Interpretation: A 1% change in ris k, increases excess returns by SR%. The higher the SR, the better the risk-return trade- off. That is, if we compare assets, the asset with the higher SR provi des the better trade-off. Using the previous data, we calcula te the SR 0.2681. Review - Hypothesis Testing A statistical hypothesis test is a method of making decisions us ing experimental data. A result is called statistically significant if it is unlikely to have occurred by chance. These decisions are made using (null) hypothesis tests. A hypothesis can specify a particular value for a population parameter, say q=q 0. Then, the test can be used to answer a question like: Assuming q 0 is true, what is the probability of observing a value for the (test) statistic used that is at least as big as the value that was actually observed? Uses of hypothesis testing: - Check the validity of theories or models. - Check if new data can cast doubt on established facts. Testing involves the comparison between two competing hypothesi s (sometimes, they represent partitions of the world). - The null hypothesis, denoted H 0, is sometimes referred to as the maintained hypothesis. - The alternative hypothesis, denoted H1, is the hypothesis that will be considered if the null hypothesis is \"rejected.\" Idea: We collect a sample of data X = {X 1, X2, ... , XN}. We construct a statistic T( X) = f(X), called the test statistic . Now we have a decision rule: - If T(X) is contained in space R, we reject H0 (& we learn). - If T(X) is in the complement of R (RC), we fail to reject H0. Note: T(X), like any other statistic, is a RV. It has a distribution. We will use the distribution of T(X) to determine R. Example: Suppose we want to test if th e mean of IBM annual returns, IBM, is 10%. That is, H0: IBM = 10%. From the population, we get a sample: {X1962, X1963, ..., XN=2020 }, with N = 59. We use T(X) = , which is unbiased, cons istent, and, assuming X is normally distributed, we know its distribution, ~ N(, 2/N). Now, we need to determine the rejection region, R, such that if T(X) = [T LB, TUB] Reject H 0: IBM = 10%. That is, R = [ T LB, TUB ]. \u00b6 Q: How do we determine T LB and TUB and, thus, make a decision? As shown below, we use the distribution of , which is derived under H0. In practice, we try to avoid a true H 0. We set the probability of rejecting a true H 0 (= P[ R|H 0]) equal to a small number. Note: In the Graph below, the bl ue area gives us the associated probability with R. That is, the probability, under H0, that the observed T(X) = falls in the rejection re gion. The blue are is called the significance level , usually denoted by . Calculate T(X) = Get a {X1962, X1963, ..., X2020} Hypothesis Testing: p-value and steps We present the classical approach , a synthesized approach, known as significance testing . It relies on Fisher's p-value : p-value is the probability of observing a result at least as extreme as the test statistic, under H0. Example: Suppose T(X) ~ T(X) < 7.378 ] = 0.025 R Note: We compute the p-value using pchisq( q, df), which computes the CDF at value q of a Chi-square distribution with df degrees of freedom. Then, > pchisq( q = 7.378 , df p_val [1] 0.02499699. \u00b6 Using the distribution of the test statistic T( X) under the null hypothesis, Fisher's significance testing approach determines a rejection region, based on the significance level ( %). We follow these steps: 1. Identify H 0 & decide on a significance level (% = P[ R|H 0]) to compare your test results. 2. Determine the appropriate test statistic T( X) and its distribution unde r the assumption that H 0 is true. 3. Calculate T( X) from the data. 4. Rule: Reject H 0 if the p-value is \"sufficiently small ,\" then, we consider T( X) in R (we learn). Otherwise, we reach no conclusion (no learning). Note: In Step 4, setting % is equivalent to setting R. 2.5% 7.378 Q: What p-value is \"sufficiently small\" of H 0? Rule: If p-value < (say, 5%) test result is significant: Reject H0. If p-value > test results are \" not significant .\" No conclusions are reached (no learning here). Go back a nd gather more data or modify model. The father of this approach, R onald Fisher, favored 5% or 1%. Example: From the U.S. Jury System H 0: The defendant is not guilty. H 1: The defendant is guilty. \u00b6 In statistics, we learn when we reject. In this case, we learn a defendant is guilty when the jury finds the defendant guilty, by rejecting H 0. Example: From the U.S. Jury System 1. Identify H 0 & decide on a significance level (%) H 0: The defendant is not guilty H 1: The defendant is guilty Significance level = \"beyond reasonable doubt ,\" presumably, a small level. 2. After judge instructions, each juror forms an \"innocent index\" T( X)i. 3. Through deliberations, jury reaches a conclusion T( X) = T(X)i . 4. Rule: If p-value of T( X) < Reject H 0. That is, guilty! If p-value of T( X) > Fail to reject H 0. That is, non-guilty. Alternatively, we build a rejection region around H 0. \u00b6 Note: Mistakes are made. We want to quantify these mistakes. Failure to reject H 0 does not necessarily mean that the defendant is not guilty, or rejecting H0 does not mean necessarily the defendant is guilty. Type I error and Type II error give us an idea of both mistakes. Definition : Type I and T ype II errors A Type I error is the error of rejecting H0 when it is true. A Type II error is the error of \"accepting\" H0 when it is false (that is, when H1 is true). Notation: Probability of Type I error: = P[X R|H0] Probability of Type II error: = P[X RC|H1] We call 1 - the power of the test -i.e., the probab ility of rejecting a false null hypothesis. Example: From the U.S. Jury System Type I error is the error of finding an innocent defendant guilty. Type II error is the error of finding a guilty defendant not guilty. State of World Decision H0 true (\"not guilty\") H1 true (\"guilty\") Cannot reject H 0 Correct decision Type II error Reject H 0 Type I error Correct decision Note: We usually think that we learn when we reject H 0. Note that some \"learning\" comes from Type I error -i.e., from false positives . \u00b6 In general, we think Type I error is the worst of the two errors: We try to minimize the error of sending to jail an innocent person. Actually, we would like Type I error to be zero. However, the only way to do this (100% of innocent defendants are found not guilty) is to never reject H 0. Then, we maximize Type II error . There is a clear trade-off between both errors. Traditional view: Set Type I error equal to a small number (defined in the U.S. court system as \" beyond reasonable doubt \") and design a test that minimizes Type II error . The usual tests F-tests tests) incor porate this traditional view. Example: We want to test if the mean is equal to , against the alternative hypothesis of different than .Then, 1. H0: = . H 1: . Notice that we have a double side d alternative, which creates a rejection re gion on both sides of the distribution of T( X). 2. Appropriate T( X): t-test (based on unknown and estimated by s). Determine distribution of T( X) under H 0. Sampling distribution of , under H 0: ~ N(, 2/N). Then, distribution of T( X) under H 0: t = s ~ tN-1 -when N > 30, t ~ N(0, 1). 3. Compute t, t , using , , s, and N. Since it is a double side d t-test, we look at |t |. Then, get p- value (|t|). 4. Rule: Set level. If p-value (|t|) < Reject H 0: = . Alternatively, if | > tN-1,1-/2 (=1.96, if =.05) Reject H 0: = .\u00b6 Notice the alternative Rule; it sets a Rejection region: R = |t| > t N-1,1-/2] If = 5% and N > 30, then t N>30 , .025 = -1.96 & tN>30 , .975 = 1.96 ( 2). (The distribution is symmetric, that is, - tN>30 , .025 = tN>30 , .975 = 1.96). Technical Note 1: In step 2, the distributi on of the t-statistic, t, is exact if { } follows a normal distribution, otherwise, the di stribution is asymptotic (f or this we need a large ); that is t = s N(0, 1) . Technical Note 2: In step 2, we determine the di stribution of t, by using the sampling distribution of under H 0. If H 0 is not true, say = . then ~ N(, 2/N), thus, t is distributed N(0, 1) only under H 0, since only under H 0 the E[ ] = 0. Review - Hypothesis Testing: Examples Example 1: We want to test if the monthly mean to tal return of the S& P 500 is equal to zero using = .05. We use the S&P 500 monthly returns (1871-2021) with the following mean and variance: = 0.007378 , s = 0.04046 , = 0. ( 0) tN-1,/2 tN-1,1-/2 cc H 1: 0. (a two-sided alternative.) 2. t = s 3. t = 0.007378 0.040 46 = 7.7478 & p-value 9.325873e-15 < = Reject H = 0. Alternatively, |t = 7.7478 | > t1789 ,.025 = 1.96 Reject H 0: = 0. Conclusion: S&P 500 monthly mean total returns are not equal to zero. Computation in R, using our previo us extracted variables, SP & D: T <- length(SP) lr_t <- log(SP[-1] + D[-1]/12) - Define log total returns x <- lr_t # Series to be analyzed N <- length(x) # Number of observations pnorm(t1)) p-value of a two sided test, |t|. => Multiply by 2 > p_val [1] 9.325873e-15 find the t qt ( p, df), which gives the quantile of the t-distribution with df degrees of freedom . That is, > qt(.975, 1789) # = (-1)*qt(.025, 1789) by symmetry. [1] 1.961291 # Check result with Q: How do we calculate the p-value ? Recall, it is the probability of observing a result at least as extreme as the test statistic, under H 0. In this case, we know that under H 0: = 0, the t-stat is well approximated by a N(0,1) distribution (since N>30). Then, we use the R function pnorm to calculate the cumulative standard normal value up to 7.7478 , and then subtract it from 1: p_val_1 t_test > p_val_1 * 2 # Multiply by 2 since it is a double sided test [1] 9.325873e-15 The observed t (t = 7.7478 ) is outside the non-rejection region ( R C) built around H 0: (-1.96 1.96). Rejection region = (double-sided test multiply by 2). \u00b6 Example 2: We want to test if monthly S&P 500 total returns (1871-2021) follow a normal distribution using = .05. If the distribution is normal, sk ewness is zero and kurtosis is equal to 3 (or excess kurtosis equals 0). The estimated moments are: = -0.4705 , = (14.6105 - 3) = 11.6 105, & N = 1805 . 1. H 0 (Data is normal): = = 0 and = 3 = 0. H 1 (Data is not normal): 0 and/or 0. 2. Appropriate T( X): the Jarque-Bera test (JB), JB = + Under H 0, JB (chi-square distribution with 2 degrees of freedom = 10,204.9 ) 0 < = .05 Reject H 0. Alternatively, compare JB to the ,. value (,. = 5.991 ). That is, JB > ,. Reject H 0. (A strong rejection!) Conclusion: Monthly S&P 500 returns are not normally distributed. \u00b6 Conclusion: We st rongly reject H 0. That is, monthly S&P 500 returns are not normally distributed. \u00b6 Review - Confidence Intervals (C.I.) When we estimate parameters with an estimator, , we get a a point estimate for , meaning that is a single value in Rk. For example, in the previous example, we get = 0.003571. Broader concept: Estimate a set C n, a collection of values in Rk. For example, for = {0.00155, 0.00554}. It is common to focus on intervals C n = [L n; U n], called an interval estimate for . The goal of C n is to contain the true population value, . We want to see Cn, with high probability. Technical detail: Since C n is a function of the data, it is a RV and, thus, it has a pdf associated with it. The coverage probability of the interval C n= [L n; U n] is Prob[ Cn]. Intervals estimates C n provide an idea of the uncer tainty in the estimation of : The wider the interval C n, the more uncertain we are about our estimate, . JB = 10,204.9 RC: (0, 5.991), with 95% prob. Interval estimates C n are called confidence intervals (C.I.) as the goal is to set the coverage probability to equal a pre-specified target, usually 90% or 95%. C n is called a (1 - )% C.I. When we know the distribution for the point estima te, it is straightforward to construct a C.I. For example, if the distribution of is normal, then a (1 - )% C.I. is given by: C n= [ + z/2 * Estimated SE( ), + z1-/2 * Estimated SE( )], where z1-/2 is the 100*(1 - /2) percentile of the standard norm al distribution. (Recall that the normal distribution is symmetric, thus z/2 = - z1-/2 ). This C.I. is symmetric around . Its length is proportional to SE( ). If the data follows a Normal distribution, then for the sample mean a (1 - )% C.I. is given by: C n= [ - z1-/2 * SD(), + z1-/2 * SD()] The size of the symmetric C.I. depends on the SD (=SE). The higher SD, the wider the C.I. Example: Two 95% C.I. for the mean, with two di fferent SD (=1, 2), are plotted below. (Recall: z1-.05/2 = 1.96). Example: We estimate a 95% C.I. for the monthly total mean return of the S&P 500 . The sampling distribution of the sample mean (assuming normality) is ~ N(, 2/ ), then, a (1 - )% C.I. is given by: C n = [ - z1-/2 * SD(), + z1-/2 * SD()] The higher SD, the wider the C.I. Then, C n = [0.00738 - 1.96 * (0.04046 / ), By looking at the 95% C.I., we can reject the null hypothesis that monthly S&P 500 total returns are 0, since 0% is outside the 95% C.I. But, th e C.I is wide, even afte r 150 years of data. Conclusion: Reject H 0: = 0, since 0 is outside the observed 95% C.I. Note: Using the above confidence interval, we can also reject that monthly excess returns are equal to 0.0833% (= 1%/12). Reca ll that Mehra & Prescott (1983) reported that the ERP is too high since in their calculation the annualiz ed equilibrium ERP is equal to 1%. \u00b6 Example: We want to estimate a 95% C.I. for the variance of monthly S&P Composite mean total return . Assuming normality, the sample variance is distributed: ( - 1) /2 ~ . To derive a (1 - )% C.I. for the variance, we rewrite th e standard confidence interval for a chi- squared variable: P( ,/ < < , /) = P(,/ < 1 /2 < , /) = 1 - After some easy algebra, we derive: P[ 1 /, / < 2 < 1 /,/] = 1 - . Note: This C.I. is not symmetric. But, as the degrees of freedom get large, the starts to look like the normal distribution and, th us, CIs will look more symmetric. From the distribution, we get: ,. )] = .95 P[0.001535 < 2 < 0.001749] = .95 Taking square root above delivers a 95% C.I. for : , 4.182% ). The C.I. is quite compact around the sample point estimate. Compared to the mean, is measured with accuracy. Note: Usually is large ( > 30). We can use the normal approximation to calculate CIs for the population . For the S&P data, we estimate the S.E. for the sample SD: SE( s) = s/2 1 = 0.04046 /sqrt(2* 1804 ) = 0.000673 (or ED - The Bootstrap In the previous examples, we assumed that we knew the distribution of the data: Stock returns follow a normal distribution. What happens when the data follows an unknown distribution, F? We still can use the sample mean, , or the sample variance, s2, as estimates of and 2, since the LLN tell us that they are both consistent estimators. If we have a \"large\" dataset -i.e., large - we can use the CLT to justify a C.I based on a normal distribution. But, when we have an unknown distribution F and we do not have a large enough N or we suspect the normal approximation is not a good one, we still can bu ild a C.I. for any statistic using a new method: a bootstrap . Bootstrapping is the practice of estimating the propertie s of an estimator -say, its variance- by measuring those properties when sampling an approximating distribution (the bootstrap DGP ). 1688.2 1923.6 That is, it is necessary to estimate a bootstrap DGP from which to draw the simulated samples. The DGP that generated the orig inal data is unknown, and so it cannot be used to generate simulated data. The bootstrap DGP estimates the unknown true DGP. Idea: We use the data at hand -t he empirical distribution (ED)- to estimate the variation of statistics that are themselves computed from the same data. Recall that, for large samples, the ED approximates the CDF very well. The empirical bootstrap is a statistical technique, easy to im plement, that takes advantage of today's modern computers, by resampling from th e ED. Bootstrapping uses the ED -i.e., sample- as if it were the true CDF. Suppose we have a sample with N observations drawn from F(x): { , , ..., } From the ED, F*, we sample (\" resample \") with replacement N observations: { x = , x = , = , ... , = } This is an empirical bootstrap sample , which is a resample of the same size as the original data, drawn from F*. For any statistic computed from the original sample data, we can define a statistic * by the same formula, but computed instead using the resampled data. Then, { x = , x = , = , ... , = is computed by resampling the or iginal data; we can compute many * by resampling many times from F*. Say, we resample * B times. { x = , x = , = , ... , = } , = , ... , = , = , ... , = } We have *: { , , , ... , }. From this collection of *'s, we can compute the mean, the variance, skewness, draw a histogram, etc., and confidence inte rvals. From this collection of *'s, we learn about the behavior of statistic . samples ( B) 1. From the original sample, draw a random sample with size . 2. Compute statistic from the resample in 1: . 3. Repeat steps 1 & 2 B times Get B statistics: { , , , ... , } 4. Compute moments; draw hi stograms; etc. for these B statistics. Using the histogram or the sorted { , , , ... , }, we can build a 1 % C.I. Using the histogram, the lower bound leaves /2% of the * to the right and ( 1/2)% of the * to the left. Example : We construct a 95% C.I. for the variance of S&P 500 returns. (You need to install R package resample , q <- 0.975)) # Find q ci <- lr_var - 0.001376664 0.001909769 > Or, taking square roots above, we can get a 95% CI for : (3.71% , 4.37% ). \u00b6 Results principle): 1. With a large enough B, the LLN allows us to use the *'s to estimate the distribution of , F(). 2. The variation in is well approximated by the variation in *. Result 2 is the one that we use to estimate the size of a C.I. Technical Note: The bootstrap deli vers consistent results only. C.I. Application: The Bootstrap Percentile Method There are many ways to construct a C.I. using b ootstrapping. The easier one is the one described above. Just use the distribution of the *'s to compute directly a C.I. This is the bootstrap percentile method . The percentile method uses the distribution of * as an approximation to the distribution of . It is very simple, but not as appealing, since comparing differences tends to work better. Example : (Continuation of previous example.) We construct a 95% C.I. for the variance of S&P 500 returns. Using the boot .ci function, with type= perc, from boot package (install boot first, using the install.packages() function and then call libra ry(boot) before type = \" perc \") > boot.ci(boot.samps, type = \" perc\") BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS Based on 1000 bootstrap replicates CALL : boot.ci(boot.out = boot.samps, type = ( 0.0014 , 0.0020 ) Calculations and Intervals on Original Scale Draw a Histogram to Check Distribution of * new[25] [1] 0.001955096 Or for , taking square root of the above bounds, the 95% CI is given ( [1] 0.001955096 method uses the distribution of * as an approximation to the distribution of . It is very simple, but there are more appealing me thods. In general, a bootst rap based on comparing differences is sounder. This is the key to the empirical bootstrap. To build a C.I. for , we use , computed from the original sample. As in the previous C.I.'s, we want to know how far is from . For this, we would like to know the distribution of q = - . If we knew the distribution of q = - , we build a (1 - )% C.I., by finding the critical values q/2 & q(1- have: P r ( q/2 - q(1- /2) |) = 1 - Or, after some manipulations: P r ( - q/2 - q(1- /2) |) = 1 - , which gives a (1 - )% C.I.: C n = [ - q(1- /2), - q/2] We do not know the distribution of q, but we can use the bootstr ap to estimate it with q* = *- . and then to get q/ & q /: C n = [ - q /, - q/] This C.I. is called the pivotal C.I. Intuition: The distribution of is 'centered' at , while the distribution of * is centered at . If there is a significant separation between and , these two distributions will also differ significantly. On the other hand, the distribution of q = describes the variation of about its center. Similarly, the di stribution of q = * describes the variation of * about its center. Then, even if the centers are quite different, the two variations about the centers can be approximately equal. Example : (Continuation of previous example.) We wa nt to estimate a 95% C.I. for the variance of monthly returns of the S&P 500. (You need to install R package resample , using the install.packages() function.) sim_size <- 1000 # B = size of q <- c(0.025, ( 3.71% , 4.37% ). to the percentile bootstrap. \u00b6 Example : Now, we construct the same 95% C.I. for the variance of monthl y S&P 500 returns but using the R package boot. You need to install package first, using the install.packages() function. library( boot ) # function to obtain the variance from the data var_p <- boot.ci(boot.samps, type = \" basic \") # boot computes the CI with type= basic . > boot.ci(boot.samps, type = \" basic \") BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS Based on 1000 bootstrap replicates CALL : boot.ci(boot.out = boot.samps, type = \" basic \") Intervals : Level Basic 95% ( 0.0014 , 0.0019 ) Calculations and Intervals previ ous step-by-step process: # CI's Upper Bound [1] 0.001914674 We can transform this CI for th e variance into a CI for the SD: > , 4.38% ), wider than the CI assu ming a Normal distribution for returns. \u00b6 Note that we can also gauge th e uncertainty of the estimation of by computing the sample standard error, SE( *). (Recall we call the standard deviation of an estimator its standard error.): Steps 1. Computing the sample variance: V a r ( *) = *)2, where * = . 2. metric Bootstrap Method If we assume the data is from a parametric m odel (say, from a Normal or a Gamma distribution), we can use the parametric bootstrap to access the uncertainty (variance, C.I.) of the estimated parameter. In a parametric bootstrap, we generate bootstrap samples from the assumed distribution, based on moments computed from the sample . We do not use the ED. Suppose we have a sample with N observations drawn from F(x; ): { x1, x2, x3, bootstrap, we know F(x; ), the distribution of x, but we do not know its parameters. Suppose there is only one unknown parameter, (say, the variance). From the sample, we compute , the estimator of . Then, we bootstrap from F( x; ) and proceed as before to form a C.I.. Steps: 1. Draw B samples of size N from F( x; ). 2. For each bootstrap sample, { x , x, , ..., }, calculate *. Get B *. 3. Estimate a C.I. using the previous methods. Example : Suppose S&P 500 monthly returns follow a N(0, 2). We estimate 2 with s2 ( 3.94% , 4.20% ). Very close to the C. I.'s we obtained before assuming a Normal distribution fo r returns. Not a surprise! \u00b6 Note: In the previous example, to ga uge the uncertainty of the estimation of s2, we can also compute the sample standard error, SE( s2). Steps 1. Draw B samples of size N from a N(0, s2) Get B s2*. 2. Estimate the variability of s2 by computing the sample variance V a r ( s2) = 2, where = . 3. Estimate the S.E. of s2: SE(s2) = sqrt[Var( s2)]. Remark : An important difference between the nonparame tric and parametric bootstrap procedures is that in the nonparametric procedure, only values of the original sample appear in the bootstrap samples. In the parametric boots trap, the range of values in the bootstrap sample is the entire support of F(x; ). In the parametric bootstrap of the above example, the values in the bootstrap sample could be any value between negative and positive infinity. C.I. Application: Bootstrapping - Why? Question: Why do we a bootstrap? - Sample sizes are \"small\" and asymptotic assumptions do not apply - DGP assumptions are violated. - Distributions are complicated. Usually, we would not use a boot strap to compute C.I. 's for the mean; in general, the normal distribution works well, as long as is large enough. The bootst rap is used to generate standard errors for estim ates of other statistics where the normal distri bution is not a good approximation. A typical example is the median, where for non-normal underlying distributions the SE of the medi an is complicated to compute. Efron (1979) is the seminal paper. But, the rela ted literature is older. It became popular in the 1980's due to the explosion of computer power. Disadvantages and Advantages: - Disadvantage: Only consistent results, no finite sample results. - Advantage: Simplicity. C.I. Application: Value-at-Risk What is the most an investor can lose with a particular invest ment over a given time framework? Or, what is the worst case scenario? Value-at-Risk (VaR) provides one answer to this question: It gives a (lower) bound with a pr obability attached to it. So far, we have measured risk of an asset/investment w ith its volatility. Volatility is calculated including positive (right tail) and negative (left tail) returns. Investors, however, love the right tail of the return s distribution, but dislike the other tail. Value-at-Risk (VaR) focuses on the left tail. VaR gives a formal definition of \"worst case scenario\" for an asset over a period of time. VaR: Maximum expected amount (loss) in a given time interval within a ( one-sided ) (1 - )% C.I.: V a R ( 1 - ) = Amount exposed * (1 + worst % change scenario in C.I.) It is common to express the \" expected loss \" relative to today's expected value of asset/investment: VaR-mean(1 - ) = VaR - E[Amount exposed] There are different ways to compute the worst case scenario within a time interval. We go over two approaches: - Assuming a probability distri bution (normal, in our case). - Using the empirical distributi on (a bootstrap, using the past). Example : Let = .05 VaR = Amount exposed * (1 + worst change scenario in 97.5% C.I.). VaR-mean(97.5%) = VaR - E[Amount exposed]. C.I. Application: Range Estimates & Va R for Transaction Exposure - Normal VaR(97.5%): Minimum Amount within C.I. = 2.5 % Amount Exposedt VaR-mean(97.5%). \u00b6 When a company is involved with transactions denominated in foreign currency (FC), it is exposed to currency risk . Transaction exposure (TE) provides a simple measure of this exposure: T E t = Value of a fixed future transaction in FC * where S t is the exchange rate expressed as units of domestic currency (USD for us) per unit of FC (say, EUR). Example : A Swiss company, Swiss Cruises, sells packages in USD. Amount = USD 1 million . Payment: 30 days. = 0.92 CHF/USD TE * 0.92 CHF/USD = CHF 0.92M . If is described by a Random Walk, then TE t is a forecast of the value of the transaction in 30 days (TE t+30 ). \u00b6 Swiss Cruises wants a measure of the uncertainty related to the amount to receive in CHF in 30 days, since S t+30 is unknown. We can use a range to quantify this uncertainty; we want to say T E t+30 [TE LB, TE UB]. with high probability. To determine this range for TE, we assume that (log) changes in , ,, are normally distributed: , ~ N(, 2). Then, we build a (1- )% interval around the mean: [ z1-/2 * ]. Usual 's in |z.025| = 1.96 ( 2) ( , ) using (, s). \u00b6 Example : Range estimate based on a Normal distribution. Assume Swiss Cruises believes th at CHF/USD mont hly changes (e f,t) follow a normal distribution. Swiss Cruise s estimates the mean and variance using the last 15 years of data: = Monthly mean = -0.00152 -0.15% s2 Swiss Cruises constructs a 95% CI for CHF/USD monthly changes. Recall that a 95% C.I. for e f,t+30 (which applies to a ny t) is given by: , [-0.00152 1.96 * 0.03184 ] = [-0.06393 ; 0.06089 ]. Based on this range for e f,t, we build a 95% C.I. for S t+30 and, then, for 1M ] = [-0.06393 ; 0.06089 ]. Now, we derive a range for : (A) Upper bound , = * (1+ ef,t,UB) = 0.97602 CHF/USD (B) Lower bound , = * (1 + ,,)) = 0.92 CHF/USD * 0.86118 CHF/USD ; 0.97602 CHF/USD ]. Finally, we derive the bounds for the TE: (A) Upper bound (with , = \u00b6 The lower bound, for a receivable, represents the worst case scenar io within the interval in 30 days. That is, this is the VaR: VaR = Amount exposed * (1+ worst % change scenario in C.I.) = TE t * (1 + ,,) It is common to express the \" expected loss \" relative to today's expect ed value of transaction (or asset): VaR-mean = VaR - TE t = TE t * (1 + ,,) - TE t CHF 0.976M = CHF 0.92M = TE t * ,, Or just VaR-mean = Amount exposed * worst case scenario The minimum revenue to be received by SC in the next 30 days, within a 97.5% CI. = Interpretation of VaR: If SC expects to cover expenses with this USD 1M inflow, the maximum amount in CHF to cover, within a 97.5% one-sided CI, should be CHF 0.8612M . Relative to today's valuation (or expected valuation , according to RWM), the maximum expected loss in 30 within is a quantile , where a quantile is the fraction of observations that lie below a given value (i n this case the VaR). Example : In the previous example, the 0.025 quantile (or 2.5% quantile) for expected loses is CHF -0.0588M . 888 We could have used rent quantile -i.e. a different si gnificant level- to calculate the VaR, for example 1% ( = 2.33). Then, = CHF (A more conservative bound.) Relative today's valuation (or expected valuation , according to RWM), the maximum expected loss with a \"chance\" is CHF -0.0697M . Note: As the wider, Swiss Cr uises can spend less CHF on account of the USD 1M receivable. \u00b6 C.I. Application: Range Estimate s & VaR for TE - Bootstrap VaR is a statistic -a function of the data. We can do an empirical bootstrap to calculate the mean, SE (=SD), C.I., etc. Example: We want to calculate the average VaR(97.5 %) and its S.E., using all CHF/USD - <- CHF_USD column of the data T <- length(S) # Check total T (1971:1 to 2020:9) Tstart 1990:1 SP <- S[Tstart: T] # FX rate (1990:1 on) T <- length(SP) Val <- 1000000 # transaction in FC (in M) S_0 <- S[T] (Today's S_t) # <- length(e_f) alpha <- .05 # Specify alpha level # Obs corresponding to alpha/2*T_s culate Original # sort Original Original VaR > VaR_o [1] 860293 # function to obtain VaR from the data varisk <- function(data, i) { d <-data[i] TE <- Val*S_0*(1+d) # calculate R TE values STE <- sort(TE) TE > # boot computes the CI. BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS Based on 1000 bootstrap replicates CALL : boot.ci(boot.out = boot.samps, type Performance Evaluation In the 1990s, Bankers Trust evaluated traders based on a risk-adjusted performance measure called RAROC: Risk-adjus ted return on capital. RAROC = Profits/Capital-at-Risk RAROC adjusts profits taking into a ccount the exposure of the bank, called capital-at-risk . BT defined this exposure as the amount of capital ne eded to cover 99% of the maximum expected loss over a year. That is, capital-at-risk is the worst loss w ithin one-sided 99% C.I. We called this VaR- mean(99%). The rationale for this measure: BT needs to hold enough cash to cover 99% of possible losses. Example : Ranking two traders I and II, dea ling in different markets. Segment Profits (in USD, annualized) Position (in USD) Volatility (annualized) Trader I Futures stock indices 3.3 M 45 M 21% Trader II FX Market 3.0 M 58 M 14% To calculate RAROC, we calculate the VaR-mean (99%) -i.e., worst possible loss in a 99% CI. Assuming normality for profits with mean equal to zero (not important, since all traders are evaluated using the same mean). Then, (since =.01 z .01 = 2.33): VaR-mean(99%) = Amount exposed * worst case scenario = Position * z .01 * Volatility Since =.01 z.01 = 2.33. (1) Calculate VaR-mean (99%) for each trader (under normal distribution) Trader I : USD 45M * 2.33 * 0.21 = USD 22,018,500. Trader II : USD 58M * 2.33 * 0.14 = USD 18,919,600. (2) Calculate RAROC: Trader I : RAROC = USD 3.3M /USD 22,018,500 /USD 18,919,600 = .1586. Conclusion: Once adjusted for risk, Tr ader II provided a better return. \u00b6 Lecture 3 - Least Squares So far, we have focused on one RV at a ti me, say stock returns, and learning about its distribution, for example, using desc riptive statistics. In econometrics, we are more interested in describing or measuring the expected effect of a tax on consumption or th e expected effect of education on an employee's salary or a CEO's compensation. That is, we usually care about a functional relation between y, the dependent variable, and x, a set of explanatory variables. In this lecture, we linearly relate y to & an error term , : = + + , i = 1, 2, ...., N, where & are parameters to be estimated and , the error term or disturbance, has zero mean and constant variance, 2. That is, is a RV with E[ i] = 0 & Var[ i] = 2. We think of as the effect of individual variation th at have not been \"controlled for\" with x. The disturbance i is part of the model. Even if we know that the relation between y and x is linear and we also know & with certainty -i.e, no need to estimate them- we still would not be able to compute with 100% accuracy. We call the above equation the Data Generating Process (DGP), that is, the data we observe ( and ) are generated follo wing this equation. Given the always present uncerta inty, we focus on expected va lues. Then, under the assumption E[i] = 0, we have: E [ ] = + E[]. We have a linear relation between the expected value of and the expected value of . Example: The CAPM posits a relation betw een the excess return of asset i, ri,t - rf, and the excess return of the market, r m,t - rf. In equilibrium, the i E[(r m,t - rf)], where i is the sensitivity of asset i to market risk. The previous mathematical structure allows us to estimate IBM and also test the CAPM for asset i = IBM. Define = excess returns for IBM = excess returns for the ma rket (the \"Market\"). We express the underlying relation behind the CAPM as: = + + , t = 1, 2, ...., T, where & are parameters to be estimated and is the error term with E[] = 0 & Var[ ] = 2. Taking expectations: E [ ] = + E[]. Then, once we estimate & , we can compute the expected excess return for IBM. We can also test the CAPM for IBM, since according to the CAPM = 0. That is, we test: H 0: = 0 vs H 1: 0. \u00b6 To gain intuition and easy inte rpretation of the model, it is useful to think of x as given or predetermined (realized before ) variable. Then, we can e xpress the relation between & , in terms of the conditional expectation of , conditioning on the predetermined value of : E [ |xi] = + . (\" Regression equation\") The conditional expectation of is what we model; in general, based on finance theory or the experience of the practitioner. To be technically precise, for the regre ssion equation we require E[|] = 0. Note: We started with: = + + which can be converted to: = E[| ] + That is, is what we model plus so mething unexpected, a surprise. In the CAPM example above, we have that IBM excess returns are only related to (\"explained by\") the market excess returns. This is a one variable model . But, we could have used more explanatory variab les, for example the 3 factors in the standard Fama-French model: Excess market returns (marke t factor), SMB (size factor), and HML (book- to-market factor). This represents a multivariate model for IBM returns: = + 1 ,+ 2 , + 3 , + Though not necessary correct, we usually think of as the endogenous variable and as the exogenous variable, determined \" outside\" the model. (If is not endogenous, we have a lot of issues that will force us to change the model or the way we estimate the model.) The goal of this lecture is to learn how to estimate the population parameters & and, at the same time, learn the properties of estimators. Technical Note: We can st udy the joint distribution of & , f(, ) and describe the joint behavior in terms of expectations, conditional expectations, correlations, etc. For example, assuming joint normality for & , we can derive the conditional expectation of , given : E[| ] = + , which gives a functional (& linear) relation between & . Thus, in a joint normality context, we can study the effect of a change in on . Moreover, after a lot of manipulations and applying statistical definitions, we get a formula to estimate & in terms of moments of & . Then, why do we need other methods to estimate & ? Two things to consider: 1) In general, assuming joint normality is not realistic in economics and finance. 2) In many situations, we thi nk of the explanatory variable, , as control, not nece ssarily as RV. Remark: Without making any reference to a joint distribution, we will derive the formulas to estimate parameters in a linear relation. LS Estimation - OLS Old method: Gauss (1795, 1801) used it in astronomy. Idea: We relate a dependent variable to a set of k explanatory variables . This function depends on unknown parameters, , which we want to estimate. The relation between and is not exact. There is an error, . We have T observations of and . The model is: = f(,, ,, ..., ,; ) + , i = 1, 2, ...., T. If the functional form is known, we estimate the parameters by minimizing a sum of squared errors: m i n { S(,,,,...,,; ) = = ,,,,...,,; } The estimator obtained is called the Least Squares (LS) estimator. LS is a general estimation method. It can be applied to almost any function. The functional form, f( , ), is dictated by theory or experien ce. In this class, we mainly work with the linear case: f ( , ) = 1 , + + 3 x3,i + ... + k ,. Now, we estimate the vector = { 1, 2, ... , k} by minimizing S( , ) = = 1 ,2 , , In this case, we call this estimator the Ordinary Least Squares (OLS) estimator. (Ordinary = Linear functional form.) Notation: In lecture 2, we used ^ over the estimat or of the parameter of interest. For example, is the estimator of the parameter . Sometimes, to emphasize the method of estimation, we add to the estimated parameter the initials of the method used, say . For historical reasons, in the linear model, b is popularly used to de note the OLS estimator of . Example 1 : We want to study the effect of the tech boom ( ) on the San Francisco housing market (). We rely on a simple linear model, with only one explanatory va riable, the tech boom variable. That is, = + + . In this model, we are interested in estimating , our parameter of interest. measures the marginal effect of x on y. We can use the estimate of to check if the high tech boom has a positive effect on SF housing pri ces. In this case we test: H 0 (No or Negative effect): 0. H 1 (Positive effect): > 0. We have monthly data on SF Housing Prices and a Tech Indicator, developed by the Federal Reserve. We transform the data in percentage ch anges. Below, we plot the data: SF House Prices vs Tech Indicator (both in % changes). Example 2 : We want to study the eff ect of a CEO's education ( ) on a firm's CEO's compensation ( y). We build a CEO's compensation m odel including a CEO's education ( ) and other \" control variables \" (W: experience, gender, etc.), contro lling for other features that make one CEO's compensation different from another. That is, = f( , Wi, ) + , i = 1, 2, ...., T. The term i represents the effects of i ndividual variation that have not been controlled for with Wi or and is a vector of parameters. Usually, f(, ) is linear. Then, the co mpensation model becomes: = + + 1 W1,i + 2 W2,i + ... + Again, in this model, we are interested in the estimation of , our parameter of interest, which measures the effect of a CEO's network on a CEO's compensation. \u00b6 LS Estimation - General Functional Form We start with a general functional form, ,, where is a vector of k parameters. The general model: = , + We want to estimate k parameters. Objective function: S ( xi, ) = , ... + , We minimize S( x i, ) with respect to . That is, m i n { S(xi, ) = i i2 = f.o.c. 2,, 0 ,, 0 Suppose we have q elements in , the f.o.c.'s have set up a qxq system of equations. This system of equations is called the normal equations . The solution to the normal equation, , is the Least Squares estimator. We do not always can solve analytica lly the normal equations. Two cases: - When , is linear, we have an analytic , explicit solution, the OLS estimator, = b. - When , is non-linear , we do not have an explicit solution for . The system, however, can be solved numerically. In this case, the es timator is usually referred as Non-linear Least Squares estimator, . The estimator is a function of the data (y i, xi). OLS Estimation - One Variable Model One explanatory variable in a linear model: f (xi, ) = 1 + 2 xi Linear Model: = 1 + 2 + We have two parameters to estimate. Objective function: S( ; 1, 2) = = 12 = 12 + 12 + ... + 12 First, we take first derivatives: ( 1): 2 12 (-1) (2): 2 12 (-) Second, we set the f.o.c. and get the normal equations (2 equations, 2 unknowns): (1): 2 b1 2 (-1) = 0 b1 2 = 0 (1) (2): 2 b1 2 (-xi) = 0 1 2 = 0 (2) Now, we solve for b 1 & b 2, the OLS estimators: From (1): - b1 - b 2 = 0 - b1 - b 2 = 0 b1 = - b 2 From (2): - ( - b 2 ) - b 2 = 0 - - b 2 ( - )= 0 b2 = or, more elegantly, b 2 = , Note that we need 0 to get b 2. Interpretation of coefficients - b1 estimates the constant of the regression, the value of , when equals to 0. - b2 estimates the slope of the regression, the marginal eff ect -i.e., the firs t derivative of with respect to : = 2 That is, if increases by one un it (say, 1%), then, is estimated to increase by b 2 units (say, b2%). Conditional Prediction Suppose analysts estimate that will be z%, then, you estimate (or predict, given the z% value of ) yi: Predicted[ |=z%] = b 1 + b 2 * z. OLS Estimation - One Va riable Model: CAPM As mentioned in the introduction, a typical finance appli cation of a one variable linear model is the CAPM. Recall that the (Sharpe-Litn er) CAPM, in equilibrium, implies: E [ ( , - )] = i E[(, - )], where , = return on asset i at time t. = return of riskless asset at time t. , = return on the market portfolio at time t. i = asset i's sensitivity to market (systematic) risk. Note: The market portfolio in the CAPM represen ts wealth. All wealth. We need to include not only all stocks, but all bonds, r eal estate, privately held capita l, publicly held capital (roads, universities, etc.), and human capita l in the world. (Easy to state, but complicated to form.) In general, we proxy the Market Portfolio, with a well -diversified index, that only includes equities, like the S&P 500 Index, or the MSCI World Index. The CAPM is a particular case of wh at in financial theory we call \" factor models.\" Factors represent the systematic component that drives the cross-secti on of returns over time; they can be observed or unobserved. For example, a k-factor model for returns is given by: , = + 1 ,+ 2 , + ... + k , + , where , is the j (common) factor at time t, and constant over i, and , represents the idiosyncratic component of asset i. Thus, we think of returns as driven by comm on or systematic factor s (undiversifiable) and idiosyncratic factors (diversifiable in large portfolios.) Thus, in equilibrium, investors get compensated only for the syst ematic risk they take. The CAPM has only one factor : market excess returns (\" the market \"). The higher the exposure to this factor -i.e., i-, the higher the expected compensation. A linear data generating process ( DGP) consistent with the CAPM is: ( i - ) + ,, = 1, .., N & = 1, ..., T where i and i are the coefficients to be estimated by LS. Cov(,, ,) = 0 -i.e., market returns are exogenous . If i = 0, asset i is not exposed to market risk. Thus, the investor is not compensated with a higher return than r f. If i > 0, asset i is exposed to market risk & , , provided that E[( , - )] > 0. If i > 1 (i < 1), asset i is \"riskier\" (\"safer\" ) than the market. That is, the expected return for asset i is higher (lower) than the expe cted return for the market. For i > 1, we have E[( , - )] > i E[(, - )] -higher compensation for higher risk. If i > 0, then asset i has higher expected returns than what is expected in equi librium -i.e., what the CAPM implies. Then, in our linear model let represent IBM excess returns ( , - ) at time t and let represent Market excess returns (say, , - ) at time t. Then, b 2 estimates IBM's beta in the CAPM. Then, b 2 (= ) estimates the stock's beta in the CAPM; in the IBM case, we have : b 2 = , - , , - , - That is, the CAPM is the ratio of a covariance over a variance. Recall that the CAPM measures a stock's risk in relation to the risk (volatil ity) of the market. That is, we think of as a measure of the relative risk e xposure of holding a particular stock (IBM, in this case) in relation to the market. Interpretation of coefficients - b 1 estimates the constant of the regression: IBM excess re turns in excess of Market excess returns. In the CAPM, it should be 0 (= i). - b2 estimates the slope of the regression. In the CAPM: = estimated by b 2. That is, if Market excess returns increases by one 1% (unit), then we estimate that IBM excess returns are expected to increase by b 2% (b 2 units). The IBM also tells us if IBM is riskier ( IBM >1) or safer ( IBM <1) than the market. Conditional Prediction Suppose analysts estimate that Market excess returns will be 10%, then, we estimate (or predict, given the 10% value for Market excess returns): Predicted [IBM excess returns|(r Mkt - rf)=10% ] = b 1 + b 2 * .10. We will call the Predicted y i = i = fitted value. Example: We estimate the CAPM for IBM returns using lm R function. Import data with read function SFX_da <- read.csv(\"https://www.baue r.uh.edu/rsusmel/4397/Stocks_FX_1973.csv\", extract IBM price data x_Mkt_RF <- SFX_da$Mkt_RF # extrac t Market excess returns (in %) x_RF <- SFX_da$RF # extract Risk-free rate (in %) Define log returns & adjust size of variables accordingly T <- length(x_ibm) # sample size lr_ibm <- log(x_ibm[-1]/x_ibm[-T]) # crea IBM log returns (in decimal returns) Mkt_RF <- x_Mkt_RF[-1]/100 # Adjust sample size to ( T-1) by removing 1st obs RF <- x_RF[-1]/100 # Adjust sa mple size and use decimal returns. Define excess returns and estimate CAPM with lm function. Then, print results to screen with summary function: ibm_x <- lr_ibm - ~ Mkt_RF) lm (= package summary( fit_ibm_capm '**' 0.01 ' ' 1 Residual standard error: 0.05887 on 567 degrees of freedom Multiple R-squared: 0.3278, 0.3266 DF, p-value: < 2.2e-16 Interpretation of b 1 and b 2: b1 = constant. The additional IBM return, after excess market returns are incorporated, is 0.58% (under the CAPM, b 1 should be close to 0). b2 = slope. It is the marginal effect. If market excess returns increase by 1%, IBM excess returns increase by .90% . The estimate of the CAPM < 1, implying that IBM is less volatile (\"safer\") than the market. Unconditional (average) Expected IBM excess returns We observed an average excess market monthly return in the sample: 0.0056489 . Then, the expected IBM excess monthly return in the sample was: -0.005791 + 0.895774 * 0.0056489 = -0.000731 (0.07% ). Conditional prediction of IBM excess returns: Suppose market excess return s are expected to be 1% next month, then we predict next month IBM excess returns: -0.005791 + 0.895774 * .1 = According to the CAPM, IBM exce ss monthly returns should have been: - IBM excess returns (IBM) = 0.895774 * mean(Mkt_RF) = 0.895774 * 0.0056489 = 0.0050601 But, in the sample, we observed - IBM excess returns = mean(ibm_x) = -0.00073141 . That is, IBM underperformed relative to the CAPM. \u00b6 LS Estimation - Application 1: The CAPM & The Cost of Equity As mentioned in Chapter 2, the ERP is central to many financial theories, for example, as illustrated above the CAPM uses the ERP as an i nput to price assets. The CAPM states that, in equilibrium, the expected excess return for asset i is proportional to the expected market excess return or expected market risk premium (ERP), given by E[(, - )]. That is: E [ ( , - )] = i E[(, - )]. In equilibrium, the cost of equity , ke, is equal to the required (expect ed) rate of return a firm has to pay to investors/shareholders. Firms need to calculate the cost of equity to estimate the cost of capital, kc, which is used to discount the cash flows of a firm or a firm's project. According to the weighted average cost of capital (WACC) method, kc is given by: 1 where E represents total equity, D is to tal Debt, t is the effective tax rate and kd is the cost of debt. Firms routinely use expected returns to calculate the cost of equity. For example, using the CAPM we have: k e = + i E[(, - )]. Thus, in this case, in order to compute ke, a firm needs to determine i, the risk-free rate, r f, and the Market Portfolio, usually, a local market index, like the S&P 500 or the Nikkei 225, or a global index like the MSCI World Index. Example : Suppose IBM wants to determine its cost of equ ity. IBM decides to use the CAPM, with a U.S. ERP and U.S r f. Data: Estimated IBM = 0.895774 0.90 Risk-free rate, = = E[( )] = 0.0382 ke,IBM = + iBM E[(, - )] = 0.045 + 0.90 * 0.0382 = 0.07938 The required (or expected) rate of return for IBM investors is 7.938% . This is what IBM will use as ke to determine its cost of cap ital and, therefore, discount the cash flows associated with new projects. Note: If IBM decides to use the MSCI World I ndex as the benchmark fo r the Market Porfolio, then, ke,IBM = + iBM E[(, - )] = 0.045 + 0.90 * 0.0317 = 0.07353 a smaller number, which would produce a smaller co st of capital and, thus, increase the NPV of IBM or an IBM's project! \u00b6 Q: Which one should a firm use: a Domestic-based ERP or a World-based ERP? It depends on the view that a company has regarding capital markets. If capital markets are integr ated (or if the shareholders are world-wide diversified) the approp riate equity risk premium should re flect a world benchmark (say, MSCI World Index), ( , - )W. But, if markets are segmented (or if the shareholders hold domestic portfolios), then the appropriate equity risk premium should be based on a domestic benchmark (say, the MSCI US Index for U.S. companies), ( , - )D. The risk-free rate should also be adjusted accordingly. Then, using the CAPM we have: - World CAPM: ke = ke,W = ,W + W E[(, - )W] - Domestic CAPM: ke = ke,D = ,D + D E[(, - )D] The difference between these two models can be c onsiderable. In our previous example we have a 0.585% difference. According to Bruner et al. (2008), on average, there is a 5.55% absolute difference for emerging markets and a 3.58% absolute difference for developed markets. LS Estimation - Application 2: Hedging In the linear model, we can esti mate the optimal hedge ratio using a regression. To see this, we derive the optimal hedge ratio for a position in foreign currency (FC). Notation: St: Exchange rate at time t. We use direct quotat ions, that is, DC units per unit of FC, say S t = 1.30 USD/GBP. F t,T: Forward/Futures price at time t with a T maturity. ns: Number of units of foreign currency held. nf: Number of futures foreign excha nge units held (opposite position). h,t: (Uncertain) profit of the hedger at time t. X: Change in X (= X t - X t-1) h = hedge ratio =( nf/ns)= Number of futures per spot in position. We want to calculate h* (optimal h): We minimize the variability of St ns + Ft,T nf (Or, h,T/ns = St + h Ft,T.) We want to select h to minimize: Var( h,T/ns) = Var( ST) + h2 Var(Ft,T) + 2 h Covar(ST,Ft,T) = S2 + h2 F2 + 2 h SF f.o.c. 2 h* F2 + 2 * SF = 0 h* = -SF/F2 Note: A covariance over a variance. It can be estimated by LS: St = 1+ 2 Ft,T + t b2 estimates h*. Example: In March, we are long a GBP 1M po sition. We are uncertain about S t in the next 90 days. We hedge this position using June GBP fu tures (size of contract = GBP 62,500). We want to determine h*. Get Data (S t and F t,90-days , for say 10 years). Do a regression. St = 1 + 2 Ft,T + t Suppose we estimate this regression: S t = .001 + .82 Ft,T, h* = -.82. Now, we determine the number of June GBP futures contracts: nf/size of the contract = h * n s / size of the contract = = -.82 * 1,000,000 / 62,500 = -13.12 13 contracts sold! \u00b6 LS Estimation - Multivariate OLS The CAPM is a particular case of wh at in financial theory we call \" factor models.\" Factors represent the systematic component that drives the cross-secti on of returns over time; they can be observed or unobserved. For example, a k-factor model for returns is given by: , = + 1 ,+ 2 , + ... + k , + , where , is the j (common) factor at time t, and constant over i, and , represents the idiosyncratic component of asset i. Thus, we think of returns as driven by comm on or systematic factor s (undiversifiable) and idiosyncratic factors (diversifiable in large portfolios.) Thus, in equilibrium, investors get compensated only for the syst ematic risk they take. The CAPM has only one factor : market excess returns (\" the market \"). The higher the exposure to this factor -i.e., i-, the higher the expected compensation. LS is a general estima tion method. It allows any functiona l form for the relation between and . and it allows to be related to many explanatory variables, like the above mentioned multi- factor models for excess returns. In this lecture, we cover the case where f( , ) is linear . We assume a linear system with independent variables and observations. That is, = 1 + 2 + ... + k + i, = 1, 2, ...., The whole system (for all ) is: 1 2 ... k 1 2 ... k T It is cumbersome and complicated to write th e whole system. Using linear algebra, we can rewrite the system in a more compact and simplify derivations. For example, after some definitions, we can write the whole system as: y = f(X, ) + = X + Linear Algebra: Br ief Review - Matrix Life (& notation) becomes easier with linear Algebra. Concepts: Matrix. A matrix is a set of elements, organized into rows and columns a & d are the diagonal elements. b & c are the off-diagonal elements. Matrices are like plain numbers in many ways: they can be added, subtracted, and, in some cases, multiplied and inve rted (divided). Linear Algebra: Matrices and Vectors Examples : Dimensions of a matrix: numbers of ro ws by numbers of columns. The Matrix A is a 2x2 matrix, b is a 1x3 matrix. A matrix with only 1 column or only 1 row is called a vector . If a matrix has an equal numbers of rows and columns, it is called a square matrix. Matrix A, above, is a square matrix. Usual Notation: Upper case letters matrices Lower case vectors ; . Columns Rows Linear Algebra: Matrices - Information Information is described by data. A tool to orga nize the data is a list, which we call a vector. Lists of lists are called matrices. That is , we organize the data using matrices. We think of the elements of X as data points (\"data entries\", \"observations\"), in economics, we usually have numerical data. We store the data in rows. In a Txk matrix, X, over time we build a database: X Once the data is organized in matrices it can be easily manipulated: multiplied, added, etc. (This is what Excel does). Linear Algebra: Matrices in Econometrics In econometrics, we have a model y = f(x1, x2, ..., xk), which we want to estimate. We collect data, say T (or N) observations, on a dependent variable, y, and on k explanatory variables, X. Under the usual notation, vector s will be column vectors: y and x k are Tx1 vectors: & xj j = 1,..., k X is a Txk matrix: X Its columns are the k Tx1 vectors xj. It is common to treat x1 as vector of ones, . In general, we import matrices (information) to our programs. Example: In R, we use the read function, usually followed by the type of data we are importing. Below, we import a comma separated values (csv) file with monthly CPIs and exchange rates for 20 different countries, then we use the read.csv function: PPP_da <- read.csv(\"https://www.bauer.uh.edu/rsus mel/4397/ppp_2020_m.csv\",head=TRUE,sep=\",\") The names() function describes the headers of the file imported (41 variables imported: > summary(PPP_da) Date BG_CPI IT_CPI GER_CPI 1/15/1971: 67.30 Median : 75.30 1/15/1974: 1 Mean : 67.92 Mean 60.14 Max. :106.60 (Other) :588 We extract a variable from the matr ix by the name of file followed by $ and the header of variable: > x_chf <- PPP_da$CHF_USD # extr act CHF/USD exchange rate data We can transform the vector x_chf . For example, for % changes: T <- length(x_chf) of CHF/USD exchange rate data lr_chf <- log(x_chf[-1]/x_ch f[-T]) # create log returns (changes) for CHF/USD. \u00b6 Linear Algebra: Special Matrices Identity Matrix, I: A square matrix with 1's along the di agonal and 0's everywhere else. Similar to scalar \"1.\" A*I = A 100 010 001 Null matrix, 0: A matrix in which all elements are 0's. Similar to scalar \"0.\" A*0 = 0 000 000 000 Both are diagonal matrices off-diagonal elements are zero. Both are examples of symmetric and idempotent matrices. As we will see later: - Symmetric: A = AT - Idempotent: A = A2 = A3 = ... Linear Algebra: We multiply matrices: of matrices requires a conformability condition. Conformability condition: The column dimensions of the lead matrix A must be equal to the row dimension of the lag matrix B. If A is an ( mxn) and B an (nxp) matrix ( A has the same number of columns as B has rows), then we define the product of AB. AB = C is (mxp) matrix with its ik-th element is Q: What are the dimensions of the vector, matrix, and result? Dimensions: a(1x2), B(2x3) c(1x3) Example 1 : We want to multiply A (2x2) and B (2x2), where A has elements and B has elements . Recall the ikth element is A = 21 79 B = 10 23 C = 21 7910 2321122013 71927093 Dimensions: A(2x2), B(2x2) C(2x2), a square matrix. Example 2 : We want to multiply X (2x2) and b (2x1), where X has elements and has elements : X = & = We compute = X Recall the i-th element is = Then, = = Dimensions: X (2x2), (2x1) (2x1), a row vector. \u00b6 Linear Algebra: Transpose The transpose of a matrix A is another matrix AT (also written A) created by any one of the following equivalent actions: -write the rows (columns) of A as the columns (rows) of A T -reflect A by its main diagonal to obtain AT Example : 389 10 4 31 80 94. \u00b6 Some transpose results: If A is a m \u00d7 n matrix AT is a n \u00d7 m matrix. (A')' = A Conformability changes unless the matrix is square. (AB)' = B'A' Example : In econometrics, an important matrix is X'X. Recall X (usually, the matrix of k independent variables): X a ( Txk) matrix Then, X a ( kxT) matrix Linear Algebra: Math Operations Addition, Subtraction, Multiplication - Addition: Just add elements - Subtraction: Just subtract element - Multiplication: Multiply each row by each column and add - Scalar Multiplication: Multiply each element by the scalar, k Examples : Addition: 21 7931 0252 24 6114 12 34 18. \u00b6 Linear Algebra: Math Operations - X'X A special matrix in econometrics, XX (a kxk matrix). First, we look at this matrix for the simple case, with k = 2: X (Tx2) = & X' X' X (2x2) = = = For the general case, with k explanatory variables, we have XX (a kxk matrix): X (Txk) = & X' X'X (kxk) = = = = Linear Algebra: Math Operations - 'X Recall is a column vector of ones (in this case, a Tx1 vector): =1 1 1 Given X (Txk), then X a 1x k vector: X 1...1 = ... Note: If x1 is a vector of ones (representing a consta nt in the linear classical model), then: x1 = = 1 = T (dot product, \"\" ) Linear Algebra: Inverse of a Matrix Identity matrix: AI = A, where 10 0 01 0 00 1 Notation: Ij is a jxj identity matrix. Given A (mxn), the matrix B (nxm) is a right-inverse for A iff AB =Im Given A (mxn), the matrix C (nxm) is a left-inverse for A iff CA = In Theorem : If A (mxn), has both a right-inverse B and a left-inverse C, then C = B = A -1 Note: - If A has both a right and a left inve rse, it is a square matrix ( m=n). It is usually called invertible . We say \"the matrix A is non-singular .\" - This matrix, A-1, is unique. - If det( A) 0 A is non-singular. Linear Algebra: Symmetric Matrices Definition: If A' = A, then A is called a symmetric matrix. In many applications, matrices are often sy mmetric. For example, in statistics the correlation matrix and the variance covariance matrix . Symmetric matrices play the same role as real numbers do among the complex numbers. We can do calculations with symmetric matrices like with numbers: for example, we can solve B 2 = A for B if A is symmetric matrix (& B is square root of A.) This is not possible in general. X'X is symmetric. It plays a very important role in econometrics. Linear Algebra: Operations in R Many ways to create a vector (c, 2:7, seq, re p, etc) or a matrix (c, cbind, rbind). We use c(), the combine function : v1 <- c(1, 3, 8) # a (3x1) vector (vector s are usually treated as a column list) > v1 [1] 1 3 8 A <- matrix(c(1, 2, 3, 7, 8, 9), ncol = 3) # a (2x3) matrix > A [,1] [,2] [,3] [1,] 1 3 8 [2,] 2 7 9 B <- matrix(c(1, 3, 1, 1, 2, 0), nrow = 3) > B [,1] [,2] [1,] 1 1 [2,] 3 2 [3,] 1 0 Now, we use rbind to create A and cbind to create B v1 <- c(1, 3, 8) # 7, 9) A c(1, (3x2) matrix v3 v4 [1,] 1 1 [1,] 14 [2,] 64 32 Note: Usually, matrices will be data -i.e., read as input. Dot product \"\" is a function that takes pairs of vectors, with same lengt h, and produces a number. For vectors c & z, it is defined as: + + ... + Dot product with 2 vectors: v1 v2: sum of th e elementwise multiplied elements both vectors > t(v1) %*% v2 # v1 <- c( <- c(2, 7, 9) [,1] [1,] 95 Dot product with a vector itself: v1 v1: Sum of the square elements of vector > t(v1) %*% v1 [,1] [1,] 74 Dot product with (a vector of ones): sum of elements of vector i <- c(1,1,1) # vector of ones (iota) > v1 <- 2 7 9 [2,] 6 21 27 [3,] 16 56 72 Property of dot product: If the dot product of two v ectors is equal to zero, then the vectors are orthogonal (perpendicular or \"\") vectors. We interpret this result as \"the vectors are uncorrelated .\" Matrix transpose: t > t ( B ) # B [,2] [,3] [1,] 1 3 1 [2,] 1 2 0 X'X (a symmetric matrix) > t(B)%*%B # command cro ssprod(B) is more efficient [,1] [,2] [1,] 11 7 [2,] 7 5 Determinant: det (a symmetric matrix) > det(t(B)%*%B) # Matrix has to be square. If det( A)=0 A non-invertible [1] 6 (X'X) -1: Inverse: solve > solve(t(B)%*%B) # Matrix inside solve() [1,] 0.8333333 -1.166667 [2,] > diag(solve(t(B)%*%B)) [1] 0.8333333 1.833333 v3 v4 Example 1 - Linear DGP There is a functional form rela ting a dependent variable, y, and k explanatory variables, X. The functional form is linear, but it depends on k unknown parameters, . The relation between y and X is not exact. There is an error, . We have T observations of y and X. Then, the data is generated according to: = , + i = 1, 2, ...., T. Or using matrix notation: = X + where & are ( Tx1); X is (Txk); and is (kx1). We will call this relation data generating process (DGP). The goal of econometrics is to estimate the unknown vector . \u00b6 Example 2 - Linear System Assume an economic model as syst em of linear equations with: aij parameters, where i = 1,.., m rows, j = 1,.., columns endogenous variables ( n), exogenous variables and constants ( m). + + ... + = + = Using linear algebra, we have a system of linear equations: Ax = d ... = ... where A = (mxn) matrix of parameters x = column vector of endogenous variables ( nx1) d = column vector of exogenous variables and constants ( mx1). \u00b6 We want to solve for the solution of Ax = d, x*. Theorem: Given A ( mxn) invertible. Then, the equation Ax = d has one and only one solution for every d (mx1). That is, there is a unique x*. x* = A-1 d Example: In practice, we avoid computing A -1, we solve a system. A <- matrix(c(1, 1, 5, 7, 9, 11, 10, 10, 14), ncol = 3) # check det(A) for singularity -0.7777778 Linear Algebra: Linear Dependence and Rank A set of vectors is linearly dependent if any one of them can be expressed as a linear combination of the remaining vectors; otherwise, it is lin early independent. Formal definition: Linear independence (LI) The set { u 1, ..., uk} is called a linearly independent set of vectors iff c1 u1 + .... + c k uk = 0 c1 = c 2 = ... = c k,= 0. Notes: - Dependence prevents solvi ng a system of equations ( A is not invertible). More unknowns than independent equations. - The number of linearly independent rows or columns in a matrix is the rank of a matrix (rank( A)). Examples : (1) 51 2 10 24 51 0 12 24 2 1 (2) 2 7;1 8;4 5; 214 785 3262 121 6 45 32 2. \u00b6 Least Squares Estimation with Linear Al gebra - Rules for Vector Derivatives Below we present the tules for vector differentia tion of linear functions an d quadratic forms (for derivation of the rules, see Appendix at end of Lecture 4): (1) Linear function: = = + where and are -dimensional vectors and is a constant. Then, = (2) Quadratic form: q = = A where is x1 vector and A is a x matrix, with elements. Then, = A + A = (A + A) If A is symmetric, then = 2 A In the next section, we apply these rules to S( ; ) = = = ( - X) ( - X) = ( - X - X + XX) = ( - 2 X + XX) where we take derivatives with respect to the x1 vector . Least Squares Estimation with Linear Algebra Let's assume a linear system with k independent variables and T observations. That is, = 1 + 2 + ... + k + , = 1, 2, ...., The whole system (for all i) is: 1 2 ... k 1 1 Using linear algebra we can rewrite the system as: = X + Vectors will be column vectors: , xj, and are Tx1 vectors: xj xj = [ .... ] = [ .... ] X is a Txk matrix. X = [x1 x2 .... xk] X the linear assumption: f(X, ) = X , we can write the objective function as: S( ; ) = = = ( - X) ( - X) We want to minimize S(x i, ). After some simple algebra we have: S( ; ) = ( - X - X + XX) = ( c - d - d + A) = ( c - 2 d + A) ( A = XX is symmetric) First derivative w.r.t. : S; = (-2 d + 2 A ) ( x1 vector) = -2 ( X - XX ) F.o.c. (normal equations): ( X - XX b) = 0 Simple algebra ( XX) b = X Assuming ( XX) is non-singular -i.e., invertible-, we solve for b: b = (XX)-1 X Note: b is called the Ordinary Least Squares (OLS) estimator. ( Ordinary = f (X, ) is linear.) X is a Txk matrix. Its columns are the Tx1 vectors xj. It is common to treat x1 as vector of ones: x 1 x 1' = [1 1 .... 1] = ' This vector of ones represent th e usual constant in the model. Note: Recall the dot product: Post-multiplying a vector (1x T) xk by (or xk) produces a scalar, the sum of all the elements of vector xk: = = + + .... + = Least Squares Estimation with Linear Algebra: The Fama-French Model The CAPM is routinely rejected. A popular alternative model is th e empirically derived 3-Factor Fama-French (1993) Model, which adds two factor s to the market excess returns factor: a size factor, measured as small minus big (SMB), and a book-to-market factor (or value factor), measured as high minus low (HML). SMB accounts for companies with small market caps that generate higher returns, while HML accounts fo r value stocks with high book-to-market ratios that generate higher returns in comparison to the market. Then, a linear DGP generating this model is: , - = i + 1 (, - ) + 2 + 3 + ,, That is, according to this model, the main driv ers of expected returns are sensitivity to the market, sensitivity to size, and sensitivity to value stocks, as measured by the book-to-market ratio. The interpretation of 1 is the same as the interpretation in the CAPM, it measures the relation between asset i risk and market risk. 2 measures how tilted asset i is towards small stock (in general, 2 > 0 means that returns of asset i behaves like small stocks). Similarly, 3 measures how tilted asset i is towards value stock (in general, 3 > 0 means that returns of asset i behave like high book-to-market stocks). Like the CAPM, the 3-factor FF mode l produces expected excess returns: E[ , - ] = 1 E[, - ] + 2 E[+ 3 E[]. A significant constant would be evidence ag ainst this model: something is missing. In 2014, Fama and French added two additional f actors to their 3-fact or model: RMW & CMA. - RMW measures the return of the portfolio of most profitable firms (\"robust\") minus the portfolio least profitable (\"weak\"). - CMA measures the return of a por tfolio of firms that invest c onservatively minus a portfolio of firms that invest aggressively. Again, the 5-factor FF model produc es expected excess returns: E[ , - ] = 1 E[, - ] + 2 E[] + 3 E[] + 4 E[] + 5 E[] There is significant debate regarding the validi ty or usefulness of this extension, especially, outside the U.S. market. Example : Fama-French 3-factor x_Mkt_RF <- SFX_da$Mkt_RF # Read Factor data -Mkt_RF Factor (in %) x_SMB <- SFX_da$SMB # Read Factor data -SMB Factor (in %) x_HML <- SFX_da$HML # Read Factor data -HML Factor (in %) x_RF <- SFX_da$RF # Read Fact or data -Risk free rate (in %) T <- length(x_ibm) # Sample size lr_ibm <- log(x_ibm[-1]/x_ibm[-T]) # Log returns for IBM (lost one observation) Mkt_RF <- x_Mkt_RF[-1]/100 # Adjust size <- sample size (Original - 1 observation) x0 <- matrix(1,T,1) # Define vector of ones (the constant in X) x <- cbind(x0,x1,x2,x3) # Matrix X k the same numbers using R's lm (use summary (.) to print ' 1 Residual standard error: 0.05848 on 565 degrees p-value: 2.2e-16 OLS - Assumptions Typical OLS Assumptions (1) DGP: = 1 + 2 + ... + k + , = 1, 2, ...., functional form known, but is unknown. (2) E[i] = 0. expected value of the errors is 0. (3) Explanatory variables X 1, X2, ..., Xk, are given (& non random) no correlation with (Cov(, Xki) = 0.) for all k. (4) The k explanatory variables are independent. (5) Var[ ] = E[] = 2 < (homoscedasticity = same variance for all i) (6) Cov( , ) = E[ ] = 0. (no serial/cross correlation for all ij) These are the assumptions behind the classical linear regression model (CLM). Least Squares - Assumptions with Linear Algebra Notation We can rewrite the assumptions, conditioning on X, which allows X to be a random variable (though, once we condition, X becomes a matrix of numbers) . Using linear algebra: (A1) DGP: = f(X, ) correctly specified. E[|X] = 0 (A3) Var[|X] = 2 IT (A4) X has full column rank -rank( X)=k-, where T k. Assumption ( A1) is called correct specification . We know how the data is generated. We call = f(X, ) + the Data Generating Process (DGP). Note: The errors, , are called disturbances . They are not something we add to f(X, ) because we don't know precisely f(X, ). No. The errors are part of the DGP. Assumption ( A2) is called regression = f(X, is, the observed y will equal E[ |X] + random variation. (ii) Using rules of expectations and the law of iterated expecta tions (LIE), we get two results: (1) E[|X] = 0 E[] = 0 The conditional expectation = unconditional expectation (2) Cov(, X) = E[( - 0)(X - X)] = X] = E[X] - E[] = E[X] 0 (by LIE, E[ X] = E X[X E[|X]] = 0.) That is, E[ X] = 0 X. There is no information about in X and viceversa. Assumption ( A3) gives the model a constant variance for all errors and no relation between the errors at different measurements/times. That is, we have a diagonal variance-covariance matrix: Var[|X] = = 0 0 0 0 00 = 2 IT (kxk) matrix This assumption implies (i) homoscedasticity E[i2|X] = 2 for all i. (ii) no serial/cross correlation E[ i j |X] = 0 for i j. It can be shown using the law of total variance that Var[ |X] = = 0 0 0 0 00 = 2 IT From Assumption ( A4) the k independent variables in X are linearly independent. Then, the kxk matrix XX will also have full rank -i.e., rank( XX) = k. Thus, XX is invertible. We will need this result to solve a system of equations given by the 1st- order conditions of Least Squares Estimation. Note: To get asymptotic results we will need more assumptions about X. We assume a linear functional form for f(x, ) = X : (A1') DGP: y = X + CLM: OLS - Summary Classical linear regression model (CLM) - Assumptions: ( A1) DGP: = X + A2) E[|X] 0 ( A3) Var[|X] = ( A4) X has full column rank -rank( X) = k, where T k. Objective function: S( , ) = = = ( - X) ( - X) = ( - 2 X + XX) First order conditions: -2 ( X - XX b) = 0 Solving for b: b = (XX)-1 X (kx1) vector OLS Estimation: Second ... ... ... ... ... ... If there were a single b, we would require this to be positive, which it would be: 2 = 2 0. The matrix counterpa rt of a positive number is a positiv e definite (pd) matrix. We need XX to be pd. A square matrix (mxm) A \"takes the sign\" of the quadratic form, zA z, where z is a mx1 vector. Then, zA z is a scalar. A form is a polynomial expression in which each component term has a uniform degree. A quadratic form has a uniform 2 nd degree. Examples : 9 + 3 + 2 - 1st degree form. 6 2 + 2 + 2 2 - 2nd degree (quadratic) form. d2z = d2 + 2 d d + d2 - quadratic form. \u00b6 A quadratic form can be written in matrix notation as zA z , where A is (mxm) A and z is an mx1 vector. Then, zA z is a scalar. Example : The fiear quadratic form from the pr evious example can be written as q = 61 12 = 6 2 + 2 + 2 2 Once we know & , q is a number. Let q be a quadratic form. We say q is: - Positive definite if q is invariably positive (q > 0) - Positive semi-definite if q is invariably non-negative (q 0) - Negative semi-definite if q is invariably non-positive (q 0) - Negative definite if q is invariably nega tive (q < 0) Definition : Positive definite matrix A matrix A is positive definite (pd) if zA z >0 for any z (a kx1 vector). For some matrices, it is easy to check. Let A = XX (a kxk matrix). Then, zA z = zXX z = vv = > 0. ( v=Xz is an Nx1 vector) XX is pd b is a min! Technical notes: 1) In general, we need eigenvalues of A to check this. If all the ei genvalues are positive, then A is pd. 2) If A is pd, then A -1 is also pd. 3) In optimization problems in multivariate ca lculus, the second order condition requires the evaluation of the matrix of sec ond derivatives, the Hessian. If all the leading principal minors are positive, then the critical point obtained is a minimum. In our case, this means that the Hessian is pd. Loosely speaking, a matrix is positive definite if the diagonal elements are positive and the off- diagonal elements are not too large in absolute va lue relative to the diagonal elements. This is a very informal way of looking at a pd matrix, bu t, keep in mind for la ter, that the diagonal elements are positive. OLS Estimation - Properties of b The OLS estimator of in the CLM is b = (XX)-1X b is a (linear) function of the data (y i ,xi). b = (XX)-1X = (XX)-1 X(X + ) = + (XX)-1X b - = (XX)-1X Under the typical assumptions, we can establish properties for b. 1) E[ b|X] = E[|X] + E[( XX) -1X|X] = is ) 2) Var[ b|X] b matrix 3) Gauss-Markov Theorem b is BLUE ( Best Linear Unbiased Estimator ). No other linear & unbiased estimator has a lower variance. Proof : Let b* = C (linear we relate Var[ b|X] to Var[ b*|X]. Let D = C - (XX) -1 X (note DX = 0) Then, Var[ b*|X] = 2 (D + (XX)-1X) (D + X(XX)-1) = 2 DD + 2 (XX)-1 = Var[ b|X] + 2 DD. Since DD is positive definite Var[ b*|X] > Var[ b|X] 4) If |X ~ i.i.d. N(0, 2IT) we can derive the distribution of b. Since b = + (XX)-1X, we have that b is a linear combination of normal variables and, thus, follows a normal distribution: b|X ~ i.i.d. N(, 2 (XX)-1) sqrt(diagonal elements of 2 (XX)-1) Note: The marginal distribution of a multivariate normal distribution is also normal, then b k|X ~ N(k,vk2) Std Dev [b k Remark: With ( A5) we can do tests of hypothesis. 5) If ( A5) is not assumed, we still can obt ain a (limiting) distribution for b. Under additional assumptions -mainly, the matrix XX does not explode as T becomes large-, as T (i) b ( b is consistent) (ii) b N(, 2 (XX)-1) ( b is asymptotically normal) Properties (1)-(4) are called finite (or small) sample properties, they hold for every sample size. Properties (5.i) and ( 5.ii) in (5) are called asymptotic properties, they only hold when T is large (actually, as T tends to ). Property (5.ii) is very importan t: When the errors are not normally distributed we still can do testing about , but we rely on an \"appr oximate distribution.\" OLS Estimation - Fitted Values and Residuals OLS estimates with b. Now, we define fitted values as: = X b Now, we define the estimated error, e: e = - , it represents the unexplained part of , what the regression cannot explain. They are usually called residuals. Note that e is uncorrelated (orthogonal) with X X e = - Xb Xe = X ( - Xb) = X - XX (XX)-1 X = 0 Using e, we can define a measure of unexplained variation: Residual Sum of Squares (RSS) = e'e = OLS Estimation - Var[b|X] We use the variance to measure precision of estimates. For OLS: Var[b|X] = 2 (XX)-1 Example : One explanatory variable model. (A1') DGP: = 1+ 2 + Var[b|X] = 2 (XX)-1 2 11 1 = 2 = 2 Var[b 1|X] = 2 2 / Var[b 2|X] = 2 2 Covar[b 1, b2|X] = 2 . \u00b6 In general, we do not know 2. It needs to be estimated. We estimate 2 using the residual sum of squares (RSS): RSS = The natural estimator of 2 is = RSS/ T. Given the LLN, this is a consistent estimator of 2. However, this not unbiased. The unbiased estimator 2 is s2 s2 = RSS/ = /(T - k)/ To get E[ s2], we use a property of a RV with a distribution: E [ ] = Given that /2 ~ . Then, E [ /2|X] = E [ k)]|X] = 2 E[s2|X] = 2 Note: ( T-k) is referred as a degrees of freedom correction. Then, the estimator of Var[ b|X] = s2 (XX)-1 (a x matrix) This estimator gives us the standard errors (SE) of the individual coe fficients. For example, for the b k coefficient: S E [ |X] = sqrt[ s2(XX)-1]kk = sb,k OLS Estimation - Testing Only One Parameter We are interested in testing a hypothesis about one parameter in our linear model: y = X + 1. Set H 0 and H 1 (about only one parameter): H 0: k = H 1: k . 2. Appropriate T( X): t-statistic . To derive the distribu tion of the test under H 0, we will rely on assumption ( A5) |X ~ N( 0, 2IT) (otherwise, results ar e only asymptotic). Let = OLS estimator of SE[b Under |X ~ N( , sb,k2). Under H 0: = (b k - )/sb,k|X ~ . We measure distance in standard error units: = - )/sb,k Note: is an example of the Wald (normalized) distance measure . Most tests statistics in econometrics will use this measure. 3. Compute , t, using , , s, and ( X'X)-1. Get p-value (t). 4. Rule: Set an level. If p-value (t) < Reject H 0: k = Alternatively, if |t | > . / Reject H 0: k = . Special case: H 0: k = 0 H 1: k 0. Then, = /sqrt{ s2(X' X)kk-1} = b k/SE[b k] ~ . This case of tk is called the t-value or t-ratio (also refer as the \"t-stats\"). That is, the t- value is the ratio of the estimated coefficient and its SE. The t-value is routinely reporte d in all regression packages. In the lm() function, it is reported in the third column of numbers. Usually, = 5%, then if | | > 1.96 2, we say the coefficient b k is \"significant .\" OLS Estimation - Testing On ly One Parameter: The CAPM Recall that the CAPM states: E [ , - ,] = E[(, - ,)]. A linear data generating process ( DGP) consistent the CAPM is: - , + ,, i = 1, .., N & t = 1, ..., T Then, using the time series of stock returns, we test the CAPM for asset i by testing: H 0 (CAPM holds): = 0 H 1 (CAPM rejected): 0 Example : Testing the CAPM for IBM returns with time series. For IBM, we test the CAPM by testing: H 0 (CAPM holds): x_Mkt_RF <- SFX_da$Mkt_RF # Read Market excess returns (in %) T <- length(x_ibm) # Sample size lr_ibm <- log(x_ibm[-1]/x_ibm[-T]) # Log returns for IBM (lost one observation) Mkt_RF <- x_Mkt_RF[-1]/100 # Adjust size x_RF[-1]/100 ibm_x <- IBM fit_ibm_capm <- lm(ibm_x ~ Mkt_RF) # OLS estimation > summary( fit_ibm_capm ) Coefficients: 0.05 ' ' 1 Q: Is the intercept ( IBM) equal to 0? We use the t-value: t k = b k/Est. k] = 0.002487 | = -2.329 | > 1.96 Reject H 0 at 5% level Conclusion: The CAPM is rejected for IBM at the 5% level. Note: You can also reject H 0 by looking at the p-value of intercept ( 0.0202 < .05) Interpretation: Given that the intercept is signifi cant (& negative), IBM underperformed relative to what the CAPM expected: - IBM excess returns: mean(ibm_x) = -0.00073141 - excess returns (CAPM) \u00b6 Remark: Above we tested (& rejected) the CA PM for one asset only, IBM. But, the CAPM should apply to all assets. Suppose we have N assets. Then, a test for the CAPM involves testing N 's: H 0: 1 = 2 = .... = N = 0 H 0: at least one 0. This test is a joint test. It requires a simultaneous estimation of the N CAPM equations. Usually, since returns are estimated with a lot of noise, portfolios are used. Also, the estimation usually takes into account the possible change over time of beta coefficients. There are different ways to approach this si multaneous estimation, a common approach is a two- step estimation, popularly know n as Fama-MacBeth (1973). OLS Estimation - Testing On ly One Parameter: The CAPM (Cross-Section) The CAPM tells also a cross-section stor y for asset returns: Assets with higher i should get, on average, higher compensation. CAPM (cross-section): E[ , - ] = where , in equilibrium, is the market excess return (o r factor return). It is sometimes referred as the price of risk. If we have the for N assets, we can estimate the security market line (SML), where we show the effect of i on E[, - ]. Below we show the SML in red. All stocks on the SML are priced correctly, all above are underpriced securities, that is, th e return is higher than what is expected for a given level of risk ( i). Similarly, all stocks under the SML are overpriced. The SML answers this question: Assets with th e higher exposure to market risk -i.e., higher . A linear DGP consistent with the CAPM is: - = + + , i = 1, .., N Testing implication of the SML for th e cross-section of stock returns: H 0 (CAPM holds in the CS): = 0 and = E[, - ] H 1 (CAPM rejected in the CS): 0 and/or E[, - ]. Note: Fama and French (1992, 1993) estimated a variation of the DGP w ith more factors and found that beta was weakly significant or not signif icant (\"Beta is dead\") factor in explaining the cross-section of stock returns. The debate about beta and what factors should be included besides i and the precise number of fact ors to include continues. OLS Estimation - Testing Only One Parameter: The 3-Factor F-F Model As mentioned above, the CAPM is routinely rejected. A popular alte rnative model is the empirically derived 3-Factor Fama-French (1993) M odel, which adds two factors, related to firm characteristics, to the market excess returns factor: a) Size factor, measured as returns of a small size portfolio minus returns of a big size portfolio, or SMB (= long Small & short Big). b) Value factor or book-to-market factor, measured as returns of a hi gh B/M portfolio minus returns of a low B/M portfolio, or HML (= long High & short Low.) The three factors are, in theory, \" factor mimicking portfolios ,\" that is, portfolios with unit exposure to the factor in question (market, size, or value), and no exposure to any other factor. Any significant factor is considered a \"CAPM anomaly .\" Then, a linear DGP generating this model is: , - = + 1 (, - ) + 2 SMB t + 3 HML t + ,. That is, according to this model, the main driv ers of expected returns are sensitivity to the market, sensitivity to size, and sensitivity to value stocks, as measured by the book-to-market ratio. The estimated parameters in th e time-series regressions are called \"factor loadings,\" they measure the sensitivity of asset i to changes in the factor. Interpretation of coefficients: - 1 has the same as the interpretation in th e CAPM, it measures the relation between asset i risk and market risk. - 2 measures how tilted asset i is towards small stock (in general, 2 > 0 means that returns of asset i behaves like small stocks. - 3 measures how tilted asset i is towards value stock (in general, 3 > 0 means that returns of asset i behave like high book-to-market stocks). Like the CAPM, the 3-factor FF mode l produces expected excess returns: E[ , - ,] = i E[(, - ,)] + 2 E[SMB t] + 3 E[HML t]. A significant constant would be evidence against this model: something is missing in the model. Questions: - How where these factors found to be drivers of stock returns? - Are these 3 factors the definitive number of factor? In 2014, Fama and French added two additional factors to their 3-fact or model: RMW & CMA. - RMW measures the return of th e portfolio of most profitable firms (\"robust\") minus the returns of portfolio least pr ofitable (\"weak\"). - CMA measures the return of a portfolio of firms that invest conservatively minus the returns portfolio of firms that invest aggressively. Again, the 5-factor FF model produc es expected excess returns: E[ , - ,] = i E[(, - ,)] + 2 E[SMB t] + 3 + 4 E[RMW t] + 5 E[CMA t] There is debate regarding the validity of this extension, especially, outside the U.S. market. The portfolios SMB, HML, RMW and CMA are formed as follows: Step 1 . At the end of June of every year t, sort the stock returns by at tribute (size of B/M). Step 2 . Split the sorted assets by attribute into 3 equal-weighted/value-weighted portfolio (3 tercile portfolios). The split can be thinner, say using 5 quintile portfolios, or based on more complicated sorts, for example, using 6 portfo lios constructed by the in tersection of 2 size portfolios and 3 va lue portfolios. Step 3 . At the end of each month (week or day), from July of year t to June of year t+1, based on the portfolios constructed at end of June in Step 1 , compute the returns of each of the split portfolios. Step 4 . Form a \"hedge portfolio\", go long the top tercile and short the bottom tercile. Note: The portfolios for July of year t to June of t+1 include all NYSE, AMEX, and NASDAQ stocks for which there is data for June of t. Example : Testing individual parameters in the 3- Factor Fama-French Model for IBM. Using the time-series, we test if the factors in the 3-factor Fama-French model for IBM x_ibm x_Mkt_RF <- SFX_da$Mkt_RF # Read <- log(x_ibm[-1]/x_ibm[-T]) # Log returns for IBM (lost one observation) Mkt_RF <- x_Mkt_RF[-1]/100 # Adjust size ' 1 Residual standard error: 0.05848 on 565 degrees model, Mkt_RF, SMB and HML are drivers of the expected returns for IBM. The signs of 2 & 3 indicate that IBM behaves like a large & low B/M firm. Note: The constant is significant, that is, there is an \"extra\" component of expected returns not explained by the 3 F-F factors. Now, to gauge the behavior of the 3-factor m odel, we plot fitted IBM values and compare with actual IBM values. plot(y, type = \"l\", col = \"blue\", # Plot IBM returns main = \"IBM Returns: Actual Returns and Fitte = \"l\", col = \"red\") # Add fitted values to plot legend(\"topleft\", # Add legend to plot legend = c(\"Actual\", \"Fitted\"), col = c(\"blue\", \"red\"), lty = 1) We observe some periods with good fit -mainly ear ly and late periods- an d some periods with poor fit -mainly the middle period. \u00b6 Example : Testing IBM's in the 3-Factor Fama-French Model. In the context of the 3-factor Fama-French model, we test if IBM's risk has a one-to-one relation with the market risk. That is, we test if the market beta ( 1) is equal to 1. Formally, we test: H 0: 1 = 1 H 1: 1 1 Using the previous estimation, we have: = -)/Est. SE) 1 = (0.9082989 - -1.616674 | 1= -1.616674 | 1.96 Cannot reject H 0 at 5% level. Conclusion: IBM bears the same market risk as the market. R Note: You should get the same numbers using R's lm and extracting information from lm: b_ibm <- by [1] 0.1059487 . OLS Estimation Algebra Interpretation Disturbances and Residuals In the population: E[ X ] = 0. In the sample: Xe X(y - Xb) = Xy - XX(XX)-1Xy = 1/T( X e) = 0. We have two ways to look at y: y = E[ y|X] + = Conditional mean + disturbance y = Xb + e = Projection + residual OLS Estimation - Important Matrices: M Important Matrices (1) \"Residual maker\" M = I T - X(XX)-1X ( TxT matrix) M = - X(XX)-1X = - Xb = e (residuals) M X = (IT - X(XX)-1X) X = 0 - M is symmetric - M = M - M is idempotent - M*M = M - M is singular - M-1 does not exist. rank( M) = T - k - Special case: X = M 0 = I - ( )-1 = I - /T - since = T M0 = - ( )-1 = - - since /T = M0 - 1 1 1 Interpretation of M0: = Xb = (fitted values) Py = Projection of y into the column space (dimension k) of X. PX = (X(XX)-1X) X = X PX = Projection of X into X = X. PM = MP = 0 Note: M = IT - X(XX)-1X = IT - P - P is symmetric - P = P - P is idempotent - P*P = P - P is singular - P-1 does not exist. rank( P) = k Results when X Contains a Constant Term Let the first column of X be a column of ones. That is X = [, x2, ..., xK] Then, (1) Since Xe = 0 x1e = 0 -the residuals sum to zero. (2) Since = Xb + e = Xb + e = Xb yxb That is, the regression line passes through the means. Note: These results are only true if X contains a constant term! Goodness of Fit of the Regression After estimating the model ( A1), we would like to judge the ad equacy of the model. There are two ways to do this: - Visual: Plots of fitted values and residuals, histograms of residuals. - Numerical measures: R2, adjusted R2, AIC, BIC, etc. Numerical measures. In general, they are simple and easy to compute. We call them goodness- of-fit measures. Most popular: R 2. Definition: Variation In the context of a model, we consider the variation of a variable as the movement of the variable, usually associated with movement of another variable. Total variation = Total sum of squares (TSS) = We want to decompose TSS in two parts: one explained by the regressi on and one unexplained by the regression. TSS = = = + +2 = + Since 0 Or TSS = RSS + SSR RSS: Residual Sum of Squares (also called SSE: SS of errors) SSR: Regression Sum of S quares (also called ESS: explained SS) Goodness of Fit of the Reg ression - Linear Algebra Recall that we can use the de-meaning matrix M0 to write = M0y (Tx1 vector) where M0 = I - ( )-1 Using linear algebra we also get the decomposition of TSS. Now, TSS = = yM0 M0y = yM0M0y = yM0y. We want to decompose the total variation of y (assume X 1 = -a constant.) y = Xb + e, then, M 0y = M0Xb + M0e = M0Xb + e (deviations from means) M0 = b(X M0)(M0X)b + ee (sum of squared deviations from means) = bX M0Xb + ee. ( M0 is idempotent & e M0X = 0) TSS = SSR + RSS A Goodness of Fit Measure: R-squared We want to have a measure that describes the f it of a regression. Simplest measure: the standard error of the regression (SER) SER = sqrt{RSS/(T- k)} SER depends on units. Not good! R-squared (R2) 1 = SSR/TSS + RSS/TSS R2 = SSR/TSS = Regression variation/Total variation R2 = bXM0Xb/yM0y = 1 - ee/M0 = ( - ) ( - )/( - ) ( - ) = [ - T 2]/[-T2] As introduced here, R2 lies between 0 and 1 (& it is indepe ndent of units of measurement!). It measures how much of total va riation (TSS) is explained by regression (SSR): the higher R2, the better. Note: R 2 is bounded by zero and one only if: (a) There is a constant term in X -we need e' M0X=0! (b) The line is computed by linear least squares. Main problem with R 2: Adding regressors Given the above interpretation of R2, it seems an appropriate criter ia to select a model: If we have several models, the model with the higher R2 should be selected. However, R2 favors the addition of \" irrelevant \" explanatory variables. It can be shown that R 2 never falls when regressors (say z) are added to the regression. This occurs because RSS decreases with more information. Problem: Judging a model based on R 2 tends to over-fitting -i.e., in our linear model, including too many explanatory variables. Comparing Regressions When R 2 is used as a criteria for model select ion, make sure the denominator in R2 is the same - i.e., same left hand side variable. For example, R2 will not be an appropriate criteria to select between a linear vs. loglinear specifi cations of the dependent variable, . Loglinear will almost always appear to fit better becau se taking logs reduces variation. Linear Transformation of data does not change R 2. - Based on X, b = (XX)-1X. Suppose we work with X* = cX, instead ( c is a constant). P P same fit, same residuals, same R2! Adjusted R-squared To avoid over-fitting, R2 is modified with a penalty for number of parameters: Adjusted-R2 2 = 1 (T - 1) (T - k) (1 - R2) = 1 (T - 1) (T - k)RSS TSS = 1 s2 TSS/( T - 1) maximizing 2 <=> minimizing [RSS/ (T - k)]= s2 Degrees of freedom -i.e., ( T - k)- adjustment assumes something about \"unbiasedness.\" 2 includes a penalty for variables th at do not add much fit. Can fall when a variable is added to the equation. Technical note: 2 will rise when a variable, say z, is added to the regression if and only if the t- ratio on z is larger than one in absolute value. Theil (1957) shows that, under certain assumptions (an important one: the true model is being considered), if we consider two linear models: M 1: = X1 1 + 1 M 2: = X2 2 + 2 and choose the model with smaller s 2 (or, larger Adjusted R2), we will select the true model, M 1, on average. In this sense, we say that \"maximizing Adjusted R 2\" is an unbiased model-selection criterion. Other Goodness of Fit Measures There are other goodness-of-fit measures that also incorporate penalties for number of parameters (degrees of freedom). We minimize these measures. Information Criteria - Amemiya : [ee/(T - k)] * (1 + k/T) = s 2 * (1 + k/T) - Akaike Information Criterion (AIC) AIC = -2/ T(ln L - k ) L: Likelihood if normality = ln( e'e/T) + (2/T) k (+constants) - Bayes-Schwarz Information Criterion (BIC) BIC = -(2/ T ln L - [ln(T)/T] k) if normality AIC = ln( e'e/T) + [ln(T)/T] k (+constants) Example : 3 Factor for IBM returns: b <- solve(t(x)%*% x)%*% t(x)%*%y # b = (XX)-1X y (OLS regression) e <- y - x%*%b # regression residuals, e k + 2*k/T # under N(.,.) -i.e., under (A5) > R2 [1] 0.338985 The 3 factors explain 34% of the variability of IBM returns. > Adj_R2 [1] 0.3354752 > AIC [1] -5.671036. R Note: We can extract R2 and Adjusted Assume a particular di stribution with unknown paramete rs. Maximum likelihood (ML) estimation chooses the set of parameters that maximize the likelihood of drawing a particular sample. Consider a sample ( , ... , ) which is drawn from a pdf f(X|) where are parameters. If the Xi's are independent with pdf f(Xi|) the joint probability of the whole sample = -also written as L(X; )- is called the likelihood function . This function can be maximized with respect to to produce maximum likelihood estimates: . It is often convenien t to work with the Log of the likelihood function. That is, ln| = ln | The ML estimation approach is very general. We need a model and a pdf for the errors to apply ML. Now, if the model is not correctly specified, the estimates are sensitive to misspecification. A lot of applications in finance and economics: Time series, volatility (GARCH and stochastic volatility) models, factor models of the term structure, switchi ng models, option pricing, logistic models (mergers and acquisitions, de fault, etc.), trading models, etc. In general, we rely on numeri cal optimization to get MLEs. Maximum Likelihood Estimation: Properties ML estimators (MLE) have very appealing properties: (1) Efficiency. Under general conditions, they achieve lo west possible variance for an estimator. (2) Consistency . As the sample size increases, the MLE converges to the population parameter it is estimating: (3) Asymptotic Normality: As the sample size incr eases, the distribution of the MLE converges to the normal distribution. , where I() is the information matrix: (kxk matrix) (4) Invariance. The ML estimate is invariant under f unctional transformations. That is, if is the MLE of and if g() is a function of , then g() is the MLE of g(). (5) Sufficiency . If a single sufficient statistic exists for , the MLE of must be a function of it. That is, depends on the sample observations only th rough the value of a su fficient statistic. Maximum Likelihood Estimation: Example Let the sample be X = {5, 6, 7, 8, 9, 10} drawn from a Normal( , 1). The probabili ty of each of these points based on the unknown mean, , can be written as: 5|1 2exp5 2 6|1 2exp6 2 10|1 2exp10 2 Assume that the sample is independent. The n, the joint pdf function can be written as: | 5| * 6| * ... * 10| = The value of m that maximizes the likelihood f unction of the sample can then be defined by |. It easier, however, to maxi mize the Log likelihood, ln L(X| ). That is, 2 1st-derivative f.o.c. 56100 Then, the first order conditions: 5 6100 Solving for : 5678910 6 7.5 _ That is, the MLE estimator is equal to the sample mean. This is good for the sample mean: MLE has very good properties! Maximum Likelihood Estimation : Numerical Optimization We have a function | ln L(X|), with unknown parameters. We use numerical optimization to estimate . Numerical optimization are methods that search over the parameter space of looking for the values that optimize -i .e., maximize or minimize- the function |. In R, the functions optim & nlm do numerical optimization. Both minimize any non-linear function |. Recall that max | = min - |. Then, in practice, we numerically minimize the negative of the likelihood function, or ln L(X|) * (-1). Example : In Example I above, we numerically minimize ln L(X|) * (-1). Most common optimization problems are so lved using the Newton-Raphson method. The Newton-Raphson method is an iterative algorithm, where the j+1 iteration computes j+1 (or updates based on j: j+1 = j - |j , where = S() is the vector of first derivatives of the log likelihood, ln, evaluated at iteration j, with parameter j, Aj is the matrix of second derivatives of ln , evaluated at iteration j, with parameter j. The vector of first derivatives of is called the Score. The matrix of second derivatives is called the Hessian. To run optim or nlm, we need to specify: - Initial values for the parameters, 0. - Function to be minimized (in Example I, ln L(X|) * (-1)). - Data used. - Other optional inputs: Choice of method, hessian calculated, etc. More on this topic in Lecture 10. Example: For X = {5, 6, 7, 8, 9, 10} ~ N( , 1), code to get . mu <- 0 # assumed mean ( 0 = initial value for ) x_6 <- c(5, 6, 7, 8, 9, 10) # data dnorm(5, mu, sd=1) # probability of observing a 5, assuming a N(mu=0, sd=1) dnorm(x_6) # probability of observing each element in x_6 l_f <- prod(dnorm(x_6)) # Likelihood function log(l_f) # Log likelihood function sum(log(dnorm(x_6))) # Alternative cal culation of Log likelihood function # Step 1 - Create Likelihood function likelihood_n <- function(mu){ # Create a prob function with mu as an argument sum(log(dnorm(x_6, mu, sd=1))) } > likelihood_n(mu) # print likelihood [1] negative_likelihood_n <- function(mu){ # uses mu, sd=1))) * (-1) } > negative_likelihood_n(mu) [1] 183.0136 # Step 2 - Maximize (or Minimize ne gative Likelihood function) results_n <- nlm(negative_likelihood_n, mu, st epmax=2) results 14.26363 <= The is the max) $estimate [1] -4.736952e-12 <= Should be very cl ose zero if we're at a minimum mu_max <- results_n$estimate # Extract estimates > mu_max # Should be equal to mean [1] 7.5 > likelihood_n(mu_max) # Check max value at mu_max [1] -14.26363 . \u00b6 Now, we generalize the previous example to an i.i.d. sample X = {X 1, X2,..., XT} drawn from a Normal(, 2). Then, the joint pdf function is: | exp Then, taking logs, we have: ln 2ln 21 2 2ln 2 have: ln ln2 ln 2 ln Taking first derivatives: We can write the first deri vatives as a vector, the gradient, whose length is the number of unknown parameters in the likelihood -i.e., size of . In this case, a 2x2 vector: = In the case of a log likelihood function, the vector of first derivatives is called the Score. When we set the Score equal to 0, we have the set of first orde r conditions (f.o.c.).Then, we have the f.o.c. and jointly solve for the ML estimators: 1 ln 0 Note: The MLE of is the sample mean. Therefore, it is unbiased. 2 0 Note: The MLE of 2 is not s2. Therefore, it is biased! But, it is consistent. Example: Using X = {5, 6, 7, 8, 9, 10}, now drawn from a Normal( , ). s2 = . = 3.5 Note 2: The computation of MLE for the mean parameter is independent of the computation of the MLE for the variance . \u00b6 To obtain the variance of = [, we invert the information matrix for the whole sample |. Recall, , | where |) is the Information matrix for the whole sample. It is generally calculated as: | |, ( kxk matrix) where the matrix of second derivatives is the Hessian matrix, H: | = H In practice, we use numerical optimization packages (say, nlm in R), which minimize a function. Then, we minimize the negative log | and, thus, to get Var[ ] we do not need to multiply H by (-1). Example: For X = {5, 6, 7, 8, 9, 10} ~ N( , ), code to get MLEs. mu <- 0 # assumed mean (initial value) sig <- 1 # assumed sd (initial value) x_6 <- c(5, 6, 7, 8, 9, 10) # Step 1 - Create Likelihood function likelihood_lf <- function(x){ # Create a pr ob function with mu & sig as arguments mu negative_likelihood_lf(x) # Step 2 - Maximize Log Likelihood function (or Mi nimize negative Likelihood function) results_lf <- nlm(negative_lik <= minimum $code [1] 1 <= 1 if we program stopped at 34 <= Number Extract estimates > par_max # Should be equal to sample mean [1] 7.500000 1.707825 > likelihood_lf(par_max) # Check max value of likelihood function [1] -11.72496 # Step 3 - Standard Errors Maximum Likelihood Estimation : Linear Model Example We will work the previous example with matrix notation. Suppose we assume: , ~ 0, or , ~ 0, where x i is a kx1 vector of exogenous numbers and is a kx1 vector of unknown parameters. Then, the joint likeli hood function becomes: 2/ Taking logs, we have the log likelihood function: ln 2ln 21 2 2ln 21 2 The joint function becomes: ln ln 2 = ln 2 ln We take first derivatives of the log likelihood w.r.t. and 2: 2 / ln 21 2 1 2 Using the f.o.c., we jointly estimate and 2: ln 1 1 0 ln 1 2 0 Under ( A5) -i.e., normality for the errors-, we have that b. This is a good result for OLS b. ML estimators have very good properties: Efficiency, consistency, asymptotic normality and invariance. is biased, but given that it is an ML estimator, it is efficient, consistent and asymptotically normally distributed. Example: We estimate the 3 F-F Negative likelihood_lf <- function(theta, y ,X) likelihood_lf(theta,ibm_x,X) [,1] [1,] -599.0825 # Step 2 - Maximize (or function) estimates > to OLS results [1] -0.0005907974 0.8676052091 -0.6815947799 -0.2284249895 0.0557422421 > of likelihood function [,1] [1,] -835.3316 # Compare with OLS 0.05 '.' 0.1 ' ' 1 Maximum Likelihood Estimation: Score and Information Matrix Definition: Score (or efficient score) ; | | S(X; ) is called the score of the sample. It is the vector of pa rtial derivatives (the gradient), with respect to the parameter . If we have k parameters, the score will have a kx1 dimension. Definition: Fisher information for a single parameter for observation i: | I() is sometimes just called information . It measures the shape of the log f(X|). The concept of information can be generalized for the k-parameter case. In this case, for the whole sample: This is kxk matrix. If L is twice differentiable with respect to , and under certain regular ity conditions, then the information may also be written as )('| log log logT I )) (L(X-EL LE2 I() is called the information matrix (negative Hessian). It measur es the shape of the likelihood function. The inverse of the information matrix for the whole sample is the Variance of . That is, Var( ) = -1 Sometimes, the notation for the information matrix for the whole sample is |. Remark: In practice, we use the i nverse of the Hessian, evaluated at , as the estimator of the variance. R calculates the Hessian in all optimization packages (for example, nlm or optim ). In the previous example, we extracted the Hessian from the nlm function with assume: , ~ 0, or , ~ 0, Taking logs, we have the log likelihood function: ln 2ln 21 2 2ln 2 2ln1 2 The score function is -first de rivatives of log L w.r.t. = (, 2): 2 / Then, we take second deri vatives to calculate I( ): / - kxk matrix. 2 -scalar Using linear algebra ( k+1)x( matrix. \u00b6 Example: We continue the previous IBM exampl e, computing MLE SEs for linear model # Step 3 - Compute S.E. by inverting Hessian par_hess <- sqrt(diag(cov_lf)) # Compute se_lf [1] 0.002370939 0.054063912 0.080170161 0.080713227 0.001659791 # We can do testing. For example, H 0: Beta = 1. > \"If the data were perfect, co llected from well-designed random ized experiments, there would hardly be room for a separate field of econometrics .\" Zvi Griliches (1986, Handbook of Econometrics ) Three important data problems: (1) Missing Data - very common, especially in cross sections and long panels. (2) Outliers - unusually high/low observations. (3) Multicollinearity - there is perfect or hi gh correlation in the e xplanatory variables. In general, data problems are exogenous to the researcher. We ca nnot change the data or collect more data. Missing Data General Setup We have an indicator variable, si. If si = 1, we observe Yi, and if si = 0 we do not observe Yi. Note: We always observe the missing data indicator s i. Suppose we are interested in the population mean = E[Yi]. With a lot of information -large T-, we can learn p = E[s i] and 1 = E[ Yi| si = 1], but nothing about 0 = E[ Yi|si = 0]. We can write: = p 1 +(1 p) 0. Problem: Since even in large samples we learn nothing about 0, it follows that without additional information/assumptions there is no limit on the range of possible values for . Now, suppose the variable of interest is binary: Yi {0, 1}. We also have an explanatory variable of Yi, say Wi. Then, the natural (not data-inf ormed) lower and upper bounds for 0 are 0 and 1 respectively. This implies bounds on : [ LB, UB] = [p 1, p 1 + (1 p)]. These bounds are sharp , in the sense that without additional information we cannot improve on them. If from the variable W i we can infer something about the missing values, these bounds can be improved. Missing Data - CLM Now, suppose we have the CLM: yi = xi + i We use the selection indicator, s i , where si = 1 if we can use observation i. Then, b = + (i si xixi /T)-1 (i si xii /T) For unbiased (and consistent) results, we need E[ s i xii ] = 0, implied by E[ i|si xii] = 0 (*) In general, we find that when s i = h(xi), that is, the selection is a function of xi, we have an inconsistent OLS b. This situation is called selection bias . Example of Selection Bias: Determinants of Hedging. A researcher only observes companies that hedge . Estimating the determinants of hedging from this population will bi as the results! \u00b6 If missing observations are random ly (exogenously) \"selected,\" it is likely safe to ignore problem. Rubin (1976) ca lls this assumption \" missing completely at random \" (or MCAR). In general, MCAR is rare. In ge neral, it is more common to see \" missing at random,\" where missing data depends on observables (say, e ducation, sex) but one item for individual i is NA (Not Available). If in the regression we \"control\" for the observables that influence missing data (not easy), it is OK to delete the whole observation for i. Missing Data - Usual Solutions Otherwise, we can: a. Fill in the blanks -i.e., impute values to the missing data- with averages, interpolations, or values derived from a model. b. Use (inverse) probability weighted estim ation. Here, we inflat e or \"over-weight\" unrepresented subjects or observations. c. Heckman selection corr ection. We build a model for the selection function, h(x i). Outliers Many definitions: Atypical observations, extreme values, conditional unusual values, observations outside the e xpected relation, etc. In general, we call an outlier an observation that is numerically different from the data. But, is this observation a \"mistake,\" say a result of measurement error, or part of the (heavy-tailed) distribution? In the case of normally distributed data, roughly 1 in 370 data points will deviate from the mean by 3*SD. Suppose T=1,000 and we see 9 data points deviating from the mean by more than 3*SD indicates outliers. We expect 3 data points to deviate by more than 3*SD. Which of the 9 observations can be classified as an outlier? Problem with outliers: They can affect estimates . For example, with small data sets, one big outlier can seriously affect OLS estimates. Outliers: Identification Informal identification method: - Eyeball : Look at the observations away from a scatter plot. Example: Plot residuals for the 3 FF factor model for IBM returns e_ibm <- residuals( i): Check for errors that ar e 2*SD (or more) away from the expected value. - Leverage statistics : It measures the difference of an inde pendent data point from its mean. High leverage observations can be potential outliers. Le verage is measured by the diagonal values of the P matrix: h t = 1/T + (xt , i/SD(e i): Check for errors that ar e 2*SD (or more) away from the expected value. Example: Plot xlab=\"Date\", ylab=\"IBM residuals\") - Leverage statistic : It measures the difference of an inde pendent data point from its mean. High leverage observations can be potential outliers. Le verage is measured by the diagonal values of the P matrix: h t = 1/T + (xt - )/[(T - 1)sx2 ]. But, an observation can ha ve high leverage, but no influence . - Influence statistic: Dif beta . It measures how much an obs ervation influences a parameter estimate, say b j. Dif beta is calculated by removing an observation, say i, recalculating b j, say b j(- i), taking the difference in betas a nd standardizing it. Then, Dif beta j(-i) = [b j - b j(-i)]/SE[b j]. - Influence statistic: Distance D (as in Cook's D ). It measures the effect of deleting an observation on the fitted values, say j. Dj = j [j - j(-i)]/[k * MSE], where k is the number of parameters in the mode l and MSE is mean square error of the regression model ((MSE=RSS /T).. The identification statistics are usually compared to some ad-hoc cut-off values. For example, for Cook's D, if D i > 4/T observation i is considered a (potential ) highly influential point. Outliers? The analysis can also be carried out for groups of observations. In this case, we would be looking for blocks of highly influential observations. Outlier Identification: Leverage & Influence Deleting the observation in the uppe r right corner has a clear eff ect on the regression line. This observation has leverage and influence. Outliers: Summary of Rules of Thumb General rules of thumb (ad-hoc thres holds) used to identify outliers: Measure Value abs(stand resid) > 2 leverage > (2 k+2)/T abs(Dif Beta) > 2/sqrt( T) Cook's D > 4/ T In general, if we have 5% or less observations exceeding the ad-hoc thresholds, we tend to think that the data is OK. Example: Cook's D for IBM returns using the 3 FF line abline(h = 4*mean(cooksd, na.rm=T), col=\"red\") # add cutoff y=cooksd, labels =ifelse(cooksd>4*mean(cooksd, na.rm=T), names(cooksd),\"\"), col=\"red\") # add row numbers influential <- as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))]) # print observations. head( dat_xy [influential, ], n=10L) > are easier ways to plot Cook's D and identify the suspect outliers. The package olsrr can be used for this purpose too. \u00b6 Example: Different tools to check for outliers for residual in the FF model for IBM returns. We will use the package olsrr -- install it with install.packages() . install.packages(\"olsrr\") 2) # Rule of thumb count (5% count is OK) x_lev Rule of thumb count (5% count is OK) sum(cooksd > 4/T) # Rule of thumb count (5% count is OK) fit_ibm_ff3 ) D measure ) # Plot Difference in betas > sum(x_stand_resid > 2) [1] 13 # 5%? = 13/569 standardized residuals, get of outliers. Outliers: What to Do? Typical solutions: - Use a non-linear formulation or apply a transf ormation (log, square root, etc.) to the data. - Remove suspected observations. (Sometimes, ther e are theoretical reasons to remove suspect observations. Typical procedure in finance: remove public utilities or financial firms from the analysis.) - Winsorization of the data (cut an % of the highest and lowest observations of the sample). - Use dummy variables. - Use LAD (quantile) regr essions, which are less sensitive to outliers. - Weight observations by size of resi duals or variance (robust estimation). General rule: Present results with or without outliers. Multicollinearity The X matrix is singular (perfect collinearity) or near singular (multicollinearity ). - Perfect collinearity Not much we can do. OLS will not work X'X cannot be inverted. The model needs to be reformulated. - Multicollinearity . OLS will work. is still unbiased. The problem is in ( X'X) -1; that is, in the Var[ b|X]. Let's see the effect on the variance of particular coefficient, b k. Recall the estimated Var[b k|X] is the kth diagonal element of 2(X'X)-1. Let define R 2k. as the R2 in the regression of xk on the other regressors, X(-k). Then, we can show the estimated Var[b k|X] is V a r [ b k|X] = . . the higher R2k. -i.e., the fit between xk and the rest of the re gressors-, the higher Var[b k|X]. Multicollinearity: Signs Signs of Multicollinearity: - Small changes in X produce wild swings in b. - High R2, but b has low t-stats -i.e., high standard errors - \"Wrong signs\" or difficult to believe magnitudes in b. There is no cure for collinearity. Estimating somethi ng else is not helpful (transforming regressors, principal components, etc.). There are \"measures\" of multicollinearity, such as the - K# = Condition number = max(singular value)/min(singular value) - Variance inflation factor = VIF k = 1/(1 - R2k.). Rule of thumb for Condition number: If K# > 30 such matrix cannot be inverted reliably. Thus, X shows severe multicollinearity. Multicollinearity: VIF and Condition Index Belsley (1991) proposes to calculate th e VIF and the condition number, using R X, the correlation matrix of the standardized regressors: VIF k = diag(R X-1)k Condition Index = k = sqrt(1/ k) where 1> 2 > ... > p > ... are the ordered eigenvalues of R X. Belsley's (1991) rules of thumb for k: - below 10 good - from 10 to 30 concern - greater than 30 trouble - greater than 100 disaster. Another common rule of thumb: If VIF k > 5, concern. Best approach: Recognize the problem and understand its implications for estimation. Note: Unless we are very lucky, some degree of multicollinearity will always exist in the data. The issue is: when does it become a problem? Multicollinearity: Example Example: Check for be a problem. \u00b6 Lecture 4 - Appendix A: Rules for Vector Derivatives (1) Linear function Consider the linear function: y = = ' + where and are -dimensional vectors and is a constant. We derive the gradient in matrix notation as follows: 1. Convert to summation notation: 2. Take partial derivative w.r.t. element : 3. Put all the partial derivatives in a vector: 4. Convert to matrix notation: = (2) Quadratic form Consider a quadratic form: q = = ' A where is x1 vector and A is a x matrix, with elements. Steps: 1. Convert to summation notation: ' (we rewrite ) 2. Take partial derivative w.r.t. element : 2 Appendix B: Expectation of a RV and Rules of Expectations Let X denote a discrete RV with probability function p(x), then the expected value of X, E[X], is defined to be: E [ X] = and if X is continuous with probability density function f(x): E [ X] = For the continuous case, the expected value of g(X), E[g(X)], is: E [ X] = Note: The discrete case is a simple adaptation. Examples : = E[] = E[] = ( - )2 E[] = ( - )k E[] = E[( - )k] We derive the rules for the continuous case. That is, E [ X] = - Rule 1 . E[] = , where c is a constant. Proof: = Then, E[ ] = E[] = - Rule 2 . E[ + X] = + E[X], where & are constants. Proof: = + X Then, E[ ] = E[ + X] = = + E[X - Rule 3 . Var[X] = = - = Proof: Var[ X] = = = 2 = 2 + = 2 + = 2 + = Note: If =0, then Var[ X] = - Rule 4 . Var[ X + b ] = Var Proof: Do it yourself. Define X + b X + b Then, simplify b, apply square and use Rule 2. Suppose excess returns for asset i, , - , are driven by the following linear model (DGP behind the CAPM): (, - ) = i + (, - ) + i,t, where rm,t - rf = excess return on the market portfolio at time t. i = the sensitivity to market (systematic) risk. i,t = idiosyncratic error term, w ith mean 0 & unrelated to r m,t. Then, E[( , - )] = E[i] + i E[(, - )] + E[i,t] (by Rule 2 ) E[( , - )] = i + i E[(, - )] + E[i,t] (by Rule 1 ) E[( , - )] = i + i E[(, - )] -by assumption about mean 0 of i,t The CAPM implies that i = 0. Also, by Rule 4 , Var[( , - )] = (i)2 Var[(, - )] Var[i,t] Lecture 4 - OLS: Samplin g, and Bootstrapping OLS Estimation - Assumptions CLM Assumptions (A1) DGP: y = X + is correctly specified. (A2) E[|X] = 0 (A3) Var[|X] = 2 IT (A4) X has full column rank - rank( X)=k-, where T k. From assumptions ( A1), (A2), and ( A4) b = (XX)-1X y We define e = y - Xb Xe = X (y - Xb) X y - XX(XX)-1Xy = 0 Now, we will study the properties of b. Sampling Distribution of b Small sample = For all sample sizes -i.e., for all values of T (or N). b = +(XX)-1 X b is a vector of random variables. Properties (1) E[ b|X] = (2) Var[ b|X] = E[( b - )|X] 2 (XX)-1 (3) Gauss-Markov 2 ~ N(k, 2 (XX)kk -1 (Note: the last implication is derived from the fa ct that the marginal distributions of a multivariate normal are also normal.) Note: Under ( A5), b is also the MLE. Thus, it has all the nice MLE properties: efficiency, consistency, sufficiency and invariance! Sampling Distribution of b Recall that a sample statistic like b is a function of RVs. Then, it has a statistical distribution. In general, in finance, we observe only one sample mean (actually, our only sample). But, many sample means are possible from the DGP. A sampling distribution is a distribution of a statis tic over all possible samples. Let's generate some y i's using a DGP and, then, some b's. Using: b = +(XX)-1X = +i vii Set = .4; then, the DGP is: y = (.4) X + (1) Generate X (to be treated as numbers). Say X ~ N(2,4) x1 = 3.22, x2 = 2.18, x3 = -0.37, ......, x T = 1.71 (2) Generate ~ N(0,1) draws 1 = 0.52, 2 = -1.23 , 3 = 1.09, ....., T =-0.09 (3) Generate y = .4 X + y1 = .4 * 3.22 + 0.52 = 1.808 y 2 = .4 * 2.18 + (-1.23 ) = -0.358 y 3 = .4 * (-0.37) + 1.09 = 0.942 ... y T = .4 * 1.71 + (-0.09) = 0.594 (4) Generate b = (XX)-1Xy = i (xi - ) (yi - )/ i (xi - )2 We want to generate many b's. Steps (1) Generate X (to be treated as numbers). Say X ~ N(2,4) (2) Generate ~ N(0,1) (3) Generate y = .4 X + (4) Generate b = (XX)-1Xy = i (xi - ) (x i - )/ i (xi - )2 Conditioning on step (1), we can repeat (2)-(4) B times, say 1,000 times. Then, we are able to generate a sampling distribution for b. We can, obviously, play with T; say T=100; 1,000; 10,000. We can check: E[ b|X] = (1/ B) i bi = ? We can calculate the variance of Var[ b|X]. Sampling Distribution of b - Code in R Steps (1)-(4) in R to generate b, with a sample of size T=100: > T <- 100 # sample size > x <- rnorm(T,2,2) # generate x from a N(2, 2 2). > ep <- rnorm(T,0,1) # generate errors from a N(0, 1). > y <- .4*x + ep # generate y > b <-solve(t(x)%*% x)%*% t( x)%*%y # OLS regression We run these commands B (say, B=1,000) tim es to get the sampling distribution of b. Then, we can calculate means, variances, skew ness and kurtosis coefficients, etc. Script to generate samp ling distribution for B=1,000 & T=100: Allbs=NULL #Initialize vector that collects the b T <- 100 x <- rnorm(T,2,2) # generate x reps=1000 # number of repetitions (B) for (i in seq(1,reps,1)){ # \"for\" loop starts mb 0.022086 (Again!) Bootstrapping is the propertie s of an estimator -say, its variance- by measuring those properties when sampling from an approximating distribution (the bootstrap DGP ). Idea: We use the data at hand -t he empirical distribution (ED)- to estimate the variation of statistics that are themselves computed from th e same data. Recall that, for large samples drawn from F, the ED approximates the CDF of F very well. Thus, an easy choice for an approximating distributi on is the ED of the ob served data. That is, the ED becomes a \" fake population .\" John Fox (2005, UCLA): \"The population is to the sample as the sample is to the bootstrap samples.\" Bootstrapping: Empirical Bootstrap (Again!) Suppose we have a dataset with N i.i.d. from F: { x1, x2, x3, -\" fake population .\" From the ED, F*, we sample with replacement N observations: { x, x, , ..., } - a bootstrap sample This is an empirical bootstrap sample , which is a resample of the same size N as the original data, drawn from F*. For any statistic computed from the original sample data, we can define a statistic * by the same formula, but computed inst ead using the resampled data. * is computed by resampling the or iginal data; we can compute many * by resampling many times from F*. Say, we resample * B times: { , , , ..., }. From this collection *'s, we can compute moments, C.I.'s, etc. Bootstrap Steps: 1. From the original sample, draw random sample with size N. 2. Compute statistic from the resample in 1: . 3. Repeat steps 1 & 2 B times Get B statistics: { , , , ..., } 4. Compute moments, draw hi stograms, etc. for these B statistics. Results: 1. With a large enough B, the LLN allows us to use the *'s to estimate the distribution of , F(). 2. The variation in is well approximated by the variation in *. Result 2 is the one we used in Lecture 2 to estimate the size of a C.I. Bootstrapping: Variations If the ED is used for the draws, the method is usually called the nonparametric bootstrap . If a distribution is assumed, say a t- distribution, and we draw from this distribution, the method is called the parametric bootstrap. If the y's and the x's are sampled toge ther, this method is sometimes called the paired bootstrap - for example, in a regression. If blocks of data are sample together, the method is called block bootstrap - for example, in the presence of correlated data, typica l of time series or spatial data. Bootstrapping: Why? Question: Why do we need a bootstrap? - N is \"small,\" asymptotic assumptions do not apply. - DGP assumptions are violated. - Distributions are B) Compute * The main appeal is its simplicity and its consistent results. Bootstrapping in Econometrics Bootstrapping provides a very gene ral method to estimate a wide vari ety of statistics. It is most useful when: (1) Reliance on \"formulas\" is problematic b ecause the formula's assumptions are dubious. (2) A formula holds only as T , but our sample is not very big. (3) A formula is complicated or it ha s not even been worked out yet. The most common econometric applications are situ ations where you have a consistent estimator of a parameter of interest, but it is hard or impossible to calculat e its standard error or its C.I. Technical note: Bootstrapping is easiest to implement if the estimator is \"smooth,\" T-consistent, and based on an i.i.d. sample. In other situations, it is more complicated. Bootstrapping in Econometrics: Example You are interested in the rela tion between CEO's education ( X) and firm's long-term performance ( y). You have 1,500 observations on both vari ables. You estimate the correlation coefficient, , with its sample counterpart, r. You find the correlation to be very low. Q: How reliable is this result? The distribution of r is complicated. You decide to use a bootstrap to study the distribution of r. Randomly construct a sequence of B samples (all with T= 1,500). Say, B1 = {( x1,y1), (x1499,y1499)} rB We rely on the observed data. We take it as our \"fake population\" and we sample from it B times. We have a collection of bootstrap subsamples . The sample size of each bootstrap subsample is the same ( T). Thus, some elements are repeated. Now, we have a collection of estimators of i's: {r1, r2, r3, ..., rB}. We can do a and get an approximation of the probability distri bution. We can calculate its mean, variance, kurtosis, confidence intervals, etc. Bootstrapping in Econometrics: Estimating the mean Example: We bootstrap the mean returns of IB M, using monthly data 1973-2020, with B = 1,000. (You need to install R package boot.) sim_size = 1000 library(boot) # function to obtain the mean from the data mean_p <- function(data, i) { samples and compute mean > boot.samps # Print original mean, bias and SE of bootstraps ORDINARY NONPARAMETRIC BOOTSTRAP Call: [1] -0.0006985612 sd(boot.samps$t) #S We bootstrap the correlati on between the returns of IBM & the S&P 500, using monthly data 1973-2020, with B = 1,000 coefficient from the data cor_xy <- and compute mean > boot.samps # Print original , bias and SE of bootstraps sd(boot.samps$t) of the correlation using boot.ci(boot.samps, type = \" perc \") Empirical boostrap method C.I. > boot.ci(boot.samps, type=\" basic \") BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS Based on 1000 bootstrap replicates CALL : boot.ci(boot.out = boot.samps, type = \" Percentile 95% ( 0.5293 , Scale. \u00b6 Bootstrapping: How many bootstraps? It is not clear. There are many theorems on asym ptotic convergence, but there are no clear rules regarding B. There are some suggestions. Efron and Tibsharani's (1994) textbook recommends B=200 as enough. (Good results with B as low as 25!) Davidson and Mackinnon's (2001) te xtbook suggests steps to select B. In the D&M simulations, on average, B is between 300 and 2,400. Wilcox's (2010) textbook recommends \"599 [...] for general use.\" Rule of thumb: Start with B=100, then, try B=1,000, and see if your answers have changed by much. Increase bootstraps until you get stability in your answers. Example: We bootstrap the correlati on between IBM returns a nd S&P 500 returns, using B = 100. > # view bootstrap results > boot.samps ORDINARY on between IBM returns a nd S&P 500 returns, using B = 25. > # view bootstrap results > boot.samps ORDINARY change that much. \u00b6 Bootstrapping: Bias You can estimate the bias of the bootstrap of a parameter, say b: Bias (b) = (1/B)r b(r) b Note: In the OLS case, b is an unbiased estimator, but as an estimate, the bias can be non-zero. This estimate must be analyzed along the SE's. Example: In the previous bootstrapping correlat ions exercise, R displays the bias: \u00b6 Var[b] assumptions in the CLM ar e not reasonable -for example, homoscedasticity or no serial correlation, or if we assume ( A5), normality. If we assuming normality ( A5), we also assume the sampling distribution of b. But if data is not normal, the normality of the sampling distribution of b only apply for large N -i.e., asymptotic results. We can use a bootstrap to estimate the sampling distribution of b. It can give us a better idea of the small sample distribution. Then, we can estimate the Var[ b]. Monte Carlo (MC=repeated sampling) method: 1. Estimate model using full sample (of size T) we get b 2. Repeat B times: - D r a w T observations from the sample, with replacement - Estimate with b(r). 3. Estimate variance with V boot = (1/B) [ b(r) - b][b(r) - b]' In the case of one parameter, say b 1: Estimate variance with Var boot[b1]= (1/B)r [b1(r) - b1 ]2 You can also estimate Var[ b 1] as the variance of b1 in the Var = (1/B)r b1(r) Note: Obviously, this method for obtaining standard errors of parameters is most useful when no formula has been worked out for the standard e rror (SE), or the formula is complicated -for example, in some 2-step estimation procedures. Bootstrapping: Linear Model - Estimating Var[b] Example: We bootstrap the SE for b for IBM returns using the 3 FF Factor Model. We use the R package lmboot , which needs to be installed with the install.packages() function. library(lmboot) # need to run R data frame. We make one. ff3_b <- b mean(ff3_b$bootEstParam[,1]) # print mean of bootstrap samples for constant mean(ff3_b$bootEstParam[,2]) # print m ean of bootstrap samples for Mkt_RF mean(ff3_b$bootEstParam[,3]) # print mean of bootstrap samples for SMB mean(ff3_b$bootEstParam[,4]) # print mean of bootstrap samples for HML # Statistics for sampling distribution of b summary(ff3_b$bootEstParam) # distribution of b # SD of parameter vector b sd(ff3_b$bootEstParam[,1]) sd(ff3_b$bootEstParam[,2]) 1st Qu. :-0.006731 1st Qu. :0.8669 1st Qu. :-0.2890 Qu. :-0.003273 Qu. :0.9492 3r d Qu. :-0.1415 3rd Qu. :-0.1086 Max. : 0.002293 Max. :1.0854 the coefficients, a bit different for S.E. Usually, we rely on the bigger S.E., in this case , for inferences we'd rely on the Booststrap S.E. > ff3_b$bootEstParam[1:10,] # print the first compute variances and SD as usual. \u00b6 Bootstrapping: Some Remarks Question: How reliable is bootstrapping? - There is still no consensus on how far it can be applied, but for now nobody is going to dismiss your results for using it. - There is a general agreement th at for normal and close to norm al (and symmetric) distributions it works well. - Bootstrapping is more problem atic for skewed distributions. - It can be unreliable for situations where there are not a lot of observati ons. Typical example in finance: estimation of quantiles in the tails of returns distributions. Note: We presented two simple examples. Ther e are many bootstraps variations. We will not cover them. Lecture 5 - Testing in the CLM Review - OLS Assumptions CLM Assumptions (A1) DGP: y = X + is correctly specified. (A2) E[|X] = 0 (A3) Var[|X] = 2 IT (A4) X has full column rank -rank( X)=k-, where T k. Issues for this lecture: Q: What happens when we impose restrictions to the DGP ( A1)? Q: How do we test restrictions in the context of OLS estimation? OLS Subject to Linear Restrictions Restrictions: Theory imposes cer tain restrictions on parameters and provide the foundation of several tests. In this Lecture, we only consider linear restrictions, written as R = q. The dimension of R is Jxk, where J is the number of restrictions, and k is the number of parameters. , as usual, is a kx1 column vector. Then, q is a Jx1 column vector. Examples : (1) Dropping variables from the equati on. That is, certain coefficients in b forced to equal 0. For example, in the CAPM, we imposee that variables x 3=SMB and x4=HML are not part of the model. That is, we impose SMB = 0 and HML = 0.Using the above notation: R = q 0010 0001 = = 0 0 We have two restrictions (J=2): = 0 & = 0. We have k=4 parameters. R is a 2x4 matrix, is a 4x1 vector, and q is a 2x1 vector. Note: The restrcitions make the FF model into the traditional CAPM. (2) Adding up conditions: Sums of certain co efficients must equal fixed values. Adding up conditions in demand systems. In a CAPM setting, the sum of all cross-sectional i's should be equal to 1. For example, in the 3 Fama-French factor model, we force + = 1. R = q 0011 = = 1 We have one restrictions (J=1): + = 1. We have k=4 parameters. R is a 1x4 matrix (a row vector), is a 4x1 vector, and q is a scalar. (3) Equality restrictions: Certain coefficients must equal other coefficients. Using real vs. nominal variables in equations. For example, in the 3 Fama-French factor model, we force = . R = q 001 1 = 0. We have one restrictions (J = 1): + = 1. We have k = 4 parameters. R is a 1x4 matrix (a row vector), is a 4x1 vector, and q is a scalar. \u00b6 Common formulation: We minimize th e error sum of squares, subjec t to the linear restrictions. That is, Min b {S(x i, ) = i i2 = = (y - X) (y - X)} s.t. R = q In practice, restrictions can usually be imposed by solving them out. Suppose we have a model: + i (1) Dropping variables -i.e., force a coefficient to equal zero, say . Problem: Min .. 0 Min (2) Adding up. Suppose we impose: 1 + 2 + 3 = 1. Then, 3 = 1 - 1 - 2. Substituting in model: ( y - x 3) = 1 (x1 - x3) + 2 (x2 - x3) + e. Problem: Min (3) Equality. Suppose we impose: 2 = 3. Substituting in model: y = 1 x1 + 2 x2+ 2 x3 + e = 1 x1 + 2 (x2+x3) + e Problem: Min .. Min Before setting the general restricted LS problem , we look at the simplest case: one explanatory variable ( x) and one restriction ( r = q). Then, we set up the Lagrangean (recall values of Lagrange multiplier', , play no role): Min , , 2 - q We take first derivatives of , with respect to , : , 2 2 , 2 - q Then, the f.o.c. are: b* 0 b* b* - q equation: b (xx)-1 b* = b - r (xx)-1 Restricted OLS = OLS + \" correction \" Premultiplying both sides by r and then subtract q: rb* - q = rb - r 2 (xx)-1 - q 0 = - r2 (xx)-1 + (rb - q) Solving for = [r2 (xx)-1]-1 (rb - q) Substituting in b* b* = b - (xx)-1r [r2 (xx)-1]-1 (rb - q) This is the Restricted OLS estimator. Properties of Restricted OLS. Property 1. Taking expectations of b*: E[b*|X] = q) Implications: If the restriction is true -i.e., ( r = q) E[b*|X] = If the restriction is not true -i.e., ( r q) E[b*|X] Then, if theory imposes a co rrect restriction, then, b* is unbiased: E[b*|X] = In practice, if restrict ion is true, the restrict ed and unrestricted estimators should be similar. Note: If theory is correct, the expected shadow price is 0! E[|X] = [ r2 (xx)-1]-1 E[(rb - q)|X] = 0 That is, you would pay nothing to release the restriction, r = q. Property 2. We compute the Var[b*]. It can be shown that Var[b*|X] = Var[b|X] - (xx)-1 r (xx)-1]-1 r (xx)-1 > 0. The restricted OLS estimator is more efficient! Remark from Properties 1 and 2: It is comm on to select an estimator based on the MSE (=RSS /T). The one with the lowest MSE is said to be more \" precise .\" We can decompose the MSE of an estimator, , as: M S E [ ] = Variance[ ] + Squared bias ] For an unbiased estimator, like b MSE [ b] = Var[ b|X] Back to b*. Suppose the theory is incorrect b* is biased. There may be situations (small bias, but much lower variance) where b* is more \"precise\" (lower MSE) than b. It is possible that a practiti oner may prefer imposing a wrong H 0 to get a better MSE. For the general case, with k explanatory variables and J rest rictions, which we write as: R = q, we have a programming problem: Minimize wrt L * = (y - X)(y - X) s.t. R = q Quadratic programming problem: Minimize a quadr atic criterion subject to a set of linear restrictions. We solve this minimizations problem using the Lagrange multiplier method. We form the Lagrangean (the 2 is for convenience, since the value of is irrelevant for extrema): Min b, L* = ( y - X)(y - X) + 2 (R - q) f.o.c.: L*/b = -2 X(y - Xb*) + 2 R = 0 -X(y - Xb*) + R = 0 L*/ = 2( Rb* - q) = 0 (Rb* - q) = 0 where b* is the restricted OLS estimator. f.o.c.: - X(y - + R = 0 1 ) ( Rb* - q) = 0 ( 2 ) where b* is the restricted OLS estimator. Then, from the 1 st equation (and assuming full rank for X): - Xy + XXb* + R = 0 b* ( XX)-1Xy - (XX)-1R = b - (XX)-1R Premultiply both sides by R and then subtract q Rb * = Rb - R(XX)-1R Rb * - q = Rb - R(XX)-1R - q 0 = - R(XX)-1R + (Rb - q) Solving for = [R(XX)-1R]-1 (Rb - q) Substituting in b* b* = b - (XX)-1R[R(XX)-1R]-1(R b - q) Note: Restricted OLS = Unrestricted OLS + \"correction\" Restricted Least Squares Question: How do linear restrictions affect th e properties of the l east squares estimator? Restricted LS estimator: b* = b - (XX)-1R[R(XX)-1R]-1(Rb - q) Properties: 1. Unbiased? Yes, if Theory is correct! E[b*|X] = - (XX) -1R[R(XX)-1R]-1 E[(Rb - q)|X] = But, if Theory is Var[ b|X] 3. b* may be more \"precise,\" where pr ecision is measured by the MSE (=RSS/ T). We can decompose the MSE of an estimator, , as: M S E [ ] = Variance[ ] + Squared bias ] For an unbiased estimator, say b, then, MSE[ b] = Var[ b|X] Suppose the theory is incorrect. Then, b* is biased. There may be situations (small bias, but much lower variance) where b* is more \"precise\" (lower MSE) than b. A practitioner may prefer imposing a wrong H 0 to get a better MSE. Restricted Least Squares - Interpretation 1. b* = b - Cm, m = the \"discrepancy vector\" Rb - q. Note: If m = 0 b* = b. (Q: What does m = 0 mean?) 2. = [R(XX)-1R]-1(Rb - q) = [R(XX)-1R]-1m When does = 0? We usually think of as a \"shadow price.\" 3. Combining results: b* = b - (XX)-1R 4. We can show that RSS never decreases with restrictions: ee = (y - Xb)(y - Xb) e*e* = ( y - Xb*)(y - Xb*) Restrictions cannot increase R2 R2 R2* Two cases - Case 1: Theory is correct: R - q = 0 (restrictions hold). b * is unbiased & Var[ b*|X] Var[ b|X] - Case 2: Theory is incorrect: R - q 0 (restrictions do not hold). b * is biased & Var[ b*|X] Var[ b|X]. Interpretation - The theory gives us information. Bad information produces bias (away from \"the truth.\") Any information, good or bad, makes us more certain of our answer. In this context, any information reduces variance. Testing: Parameter vs Diagnostic So far, the tests discussed in Lectures 3 & 4, in volved parameters. We call these types of testing parameter tests. When we tests the assumptions behind the CLM, for example, ( A5), we perform a diagnostic tests. Parameter testing: We test economic H 0's. Example: Test k = 0 -say, there is no size effect on the expected return equation. \u00b6 Diagnostic testing: We test assumptions behind the model. In our case, assumptions ( A1)-(A5) in the CLM. Example: Test E[|X] = 0 -i.e., the residuals are zero -mean, white noise distributed errors. \u00b6 Review - Significance Testing Fisher's significance testing procedure relies on the p-value : the probability of observing a result at least as extreme as the test statistic, under H0. Fisher's Idea 1) Form H 0 & decide on a significance level (%) to compare your test results. 2) Find T(X). Know (or derive) the distribution of T(X) under H0. 3) Collect a sample of data X = {X1, ..., Xn}. Compute the test-statistics T(X) p-value . 4) Rule: If p-value < (say, 5%) test result is significant: Reject H0. If the results are \" not significant ,\" no conclusions are reached (no learning here). Go back gather more data or modify model. Review - Testing Only One Parameter We are interested in testing a hypothesis about one parameter in our linear model: y = X + 1. Set H 0 and H 1 (about only one parameter): H 0: = H 1: 2. Appropriate T( X): t-statistic . To derive the distribu tion of the test under H 0, we will rely on assumption ( A5) |X ~ N( 0, 2IT) (otherwise, results ar e only asymptotic). Let b k = OLS estimator of k SE[b = sqrt{[ s2(X'X)-1]kk} = b k|X ~ N(,vk2) Under 0: bk|X ~ N(, sb,k2). Under H 0: tb = (b k - )/sb,k|X ~ tT-k. 3. Compute tb, t, using b k, , s, and ( X'X)-1. Get p-value (t). 4. Rule: Set an level. If p-value (\u00a4t ) < Reject H 0: = Alternatively, if | t| > t , / Reject H 0: = . Review - Testing Only One Parameter: t-value Special case: H0: k = 0 H1: k 0. Then, t k = b k/sqrt{ s2(X' X)kk-1} = b k/SE[b k] tk case, we call tk the t-value or t-ratio. Usually, = 5%, then if | tk|> 1.96 2, we say the coefficient b k is \"significant .\" Review - Confidence Intervals The goal of the confidence intervals (C.I.) is to set the coverage probability to equal a (1 - )% pre-specified target. When we know the distribution of point estimate, it is easy to construct a C.I. Under the usual assumptions for b k we have: Cn= [b k - t , /* Estimated + t , /* Estimated SE(b k)] This C.I. is symmetric around b k: length is proportional to SE(b k). Usual levels and t , -when N > 30, (usual case) z(1-- /2)= 2.58. Testing: The Expectation Hypothesis (EH) Example : EH states that forward/futures prices ar e good predictors of fu ture spot rates: E t[St+T] = F t,T. Implication of EH: S t+T - F t,T = unpredictable. That is, E t[St+T - Ft,T] = E t[t] = 0! Empirical tests of the EH are based on a regression: (S t+T - F t,T)/St = + Zt + t, (where E[ t]=0) where Z t represents any economic variable th at might have power to explain S t, for example, (i d- if). Then, under EH, H 0: = 0 and = 0. vs H 1: 0 and/or 0. We will informally test EH using exchange rates (USD/GBP), 3-mo forward rates We do on & . > summary( fit_eh ) Call: 0.00339 ** slope is codes: 0 '***' 0.001 '**' 0.01 '*' ' 1 Residual standard error: 0.02661 on 361 degrees b: C [ b k - tk,/2 * Estimated SE(b k), b k + tk,/2 * Estimated SE(b k)] Then, C n [ -0.3591384 , -0.07236961] Since = 0 is not in C n with 95% confidence Reject H 0: 1 = 0 at 5% level. Note: The EH is a joint hypothesis, it should be tested with a joint test! \u00b6 The General Linear Hypothesis: Wald Statistic Most of our test statis tics, including joint tests, are Wald statistics. Wald = normalized distance measure. One parameter: t b = (b k - 0k)/sb,k = distance/unit More than one parameter. Let z = (random vector - hypothesized value) be the distance W = z [Var( z)] -1 z (a quadratic form) Distribution of W? We have a quadratic form. - If z is normal and 2 known, W ~ - If z is normal and 2 unknown, W ~ F - If z is not normal and 2 unknown, we rely on asymptotic theory, W The General Linear Hypothesis: H 0: R - q = 0 Suppose we are interested in testing J joint hypotheses. Example: We want to test that in th e 3 FF factor model that the SMB and HML factors have the same coefficients, SMB = HML = 0. We can write linear restrictions as H 0: R - q = 0, where R is a Jxk matrix and q a Jx1 vector. In the above example ( J=2), we write: 0010 0001 = Question: Is Rb - q close to 0? There are two different approaches to this question. Both have in common the property of unbiasedness for b. (1) We base the answer on the discrepancy vector: m = Rb - q. Then, we construct a Wald statistic: W = m (Var[ m|X]) -1 m to test if m is different from 0. (2) We base the answer on a model loss of fit when restrictions are imposed: RSS must increase and R 2 must go down. Then, we construct an F te st to check if the unrestricted RSS ( ) is different from the restricted RSS ( ). Approach (1) . To test H 0, we calculate the di screpancy vector: m = Rb - q. Then, we compute the Wald statistic: W = m (Var[ m|X])-1 m It can be shown that Var[ m|X] = R[2(XX)-1]R. Then, W = (Rb - q) {R[2(XX)-1]R}-1 (Rb - q) Under H 0 and assuming ( A5) & estimating 2 with s2 = ee/(T-k): W * = ( Rb - q) {R[s2(XX)-1]R}-1 (Rb - q) F = W*/J ~ , If (A5) is not assumed, the results are only asymptotic: J * F Example: We want to test that in the 3 FF factor model ( T=569) 1. H 0: SMB = 0.2 and HML = 0.6. H 1: SMB 0.2 and/or HML 0.6. J = 2 We define R (2x4) below and write m = R - q = 0: 0010 0001 = 0.2 0.6 2. Test-statistic: F = W*/ J = (Rb - q) {R[s2(XX)-1]R}-1 (Rb - q) Distribution under H 0: F = W*/2 ~ , (or asymptotic, 2*F ) 3. Get OLS results, compute F, . 4. Decision Rule: 0.05 level. We reject H 0 if p-value( \u00a4 ) < .05. Or, reject H 0, if > F J=2,T-4,. 05. J <- 2 # matrix of restrictions q <- c(.2, .6) # hypothesized values m <- R%*%b - q # m = Estimated <- # exact distribution normal p_val <- 1 - pf(F_t, df1=J, df1=J, df2=(T - # distribution (F-dist) if errors normal [1] 3.011644 F _ t > 3.011644 reject H level p_val <- 1 - pf(F_t, df1=J, df2=(T under errors normal > p_val [1] < 2.2e-16 ` reject H 0 at 5% F_t_asym H 5% level > p_val <- 1 - > p_val [1] < 2.2e-16 so low it is almost zero. Extremely low chance H 0 is true. Conclusion: We rej ect the restrctions: SMB = 0.2 and HML = 0.6. R Note: You can use the R package car to test linear restrictions (linear H 0). install.packages(\"car\") test Linear hypothesis test Hypothesis: SMB = 0.2 HML = 0.6 Model 1: restricted model Model 2: ibm_x ~ Mkt_RF + SMB + HML Res.Df 565 1.9324 2 0.33667 49.217 < 2.2e-16 0 '***' 0.001 ' ' 1. \u00b6 Example: Now, we do a joint test of the EH. H 0: = 0 and = 0. Using the previous program but with: J <- 2 # number of restriction R <- matrix(c(1,0,0,1), nrow=2) # matrix of restrictions q <- c(0,0) hypothesized values > F_t 4.1024 > > J, df2=(T - errors normal [1] 3.020661 F _ 3.020661 reject H level p_val <- 1 - pf(F_t, df1= J, df2=(T p-value(F_t) under errors normal > p_val [1] 0.01731 ` reject H 0 at 5% H 5% level > p_val <- 1 - p_val [1] 0.01653 ` reject H 0 at 5% level. The R package car can do the above too: > test, with F-distr Linear hypothesis test Hypothesis: (Intercept) = 0 x = 0 Model 1: restricted model Model 2: y ~ x Res.Df RSS Df Sum of Sq F ' J, df2=(T - k)) # exact errors normal [1] 3.020661 F _ > 3.020661 reject H 0 at 5% level Conclusion: We reject the joint restrctions: H 0: = 0 and = 0. \u00b6 The F Test: H 0: R - q = 0 Approach (2) . We know that imposing the restricti ons leads to a loss of fit. R2 must go down. Does it go down a lot? -i.e., significantly? Recall (i) e* = y - Xb* = e - X(b*- b) (ii) b* = b - (XX) -1R[R(XX)-1R]-1(Rb - q) e *e* = ee + (b* - b) XX (b*- b) Replacing ( b* - b) from (ii) in the above formula, we get: e*e* - ee = (Rb - q) [R(XX)-1R]-1(Rb - q) Note: e*e* - ee is a quadratic form, then we can use a lo t of results to derive its asymptotic distribution The F-distribution is a ratio of two independent and RV divided by their degrees of freedom F = ~ , Then, to get to the F-test, we rely on two results: - W = (Rb - q){R[2(XX)-1]R}-1(Rb - q) ~ (if 2 is known) - ee/ 2 ~ . F = (e*e* - ee)/J / [ee/(T - k )] ~ , We can write the F-test in terms of R2's. Let R2 = unrestricted model = 1 - RSS/TSS R*2 = restricted model fit = 1 - RSS*/TSS Then, dividing and multiplying F by TSS we get F = ((1 - R* 2) - (1 - R2))/J / [(1 - R2)/(T - k )] ~ , or F = ~ ,. The F Test: H 0: F-test of Goodness of Fit In the linear model y = X + = X1 1 + X2 2 +... + Xk k + We want to test if the slopes X 2, ... , Xk are equal to zero. That is, H 0: 0 H 1: at least one 0 J = k - 1 We can write H 0: R - q = 0 ... ............ ... ... We have J = k - 1. Then, F = ~ ,. For the restricted model, R*2 = 0. Then, F = ~ , Recall ESS/TSS is the definition of R2. RSS/TSS is equal to (1 - R2). F This test statistic is called the F-test of goodness of fit. It is reported in all regression packages as part of the regression output. In R, the lm function reports it as \"F-statistic.\" Example: We want to test if all the FF factors (Market, SMB, HML) are jointly significant (J=3), using monthly data 1973 96.58204 F_goodfit F 3,565,.05 = 2.62067 \u00b6 Conclusion: We strongly reject the restrctions: 0. \u00b6 The F Test: General Case - Example In the linear model y = X + = 1 + X2 2 + X3 3 + X4 4 + We want to test if the slopes X 3, X4 are equal to zero. That is, H 0: 3 = 4 = 0 H 1: 3 0 or 4 0 or both 3 and 4 0 We can use, F = (e*e* - ee)/J / [ee/(T - k)] ~ FJ,T-K. Define y = X + = 1 + X2 2 + (Restricted RSS = RSS R, with kR parameters) y = 1 + X2 2 + X3 3 + X4 4 + (Unrestricted RSS = RSS U, with kU parameters) Then, F = ~ ,, where J = , and T - = . The F Test: Are SMB and HML Priced Factors? Example: We want to test if the additional FF f actors (SMB, HML) are significant, using monthly data 1973 - 2020 (T=569). That is, we test H 0: 0. Unrestricted Model (Fama-Fr ench 3-factor model): (U) IBM Ret - rf = 0 + 1 (Mkt Ret - rf) + 2 SMB + 3 HML + Hypothesis: H 0: 2 = 3 = 0 H 1: 2 0 and/or 3 0 Then, the Restricted Model (CAPM): (R) IBM Ret - rf = 0 + 1 (Mkt Ret - rf) + Test: F = / / ~ FJ,T-K. with J = kU - kR = 4 - 2 = 2 The unrestricted model was already estimated in Lecture 3. For the restricted = 1.964844 # RSS R > J <- k - k2 # J = degrees of freedom of numerator > F_test <- is small Reject H 0. Conclusion: We strongly reject the restrctions: 0. \u00b6 R Note: There is package in R, lmtest, that performs this test, waldtest, (and many others, used in this class). You need to install it fi rst: install.packages(\"lmtest\"). For the waldtest , the default reports the F-test with the F distribution. Remark: The models need to be nested. Example: We test if the additional FF factors (SMB, HML) are significant, using monthly data 1973 2020 library(lmtest) fit_ibm_ff3 <- lm (y Model fit_ibm_capm fit_ibm_capm ) Wald Model 1: y ~ Mkt_RF + SMB + HML Model 2: y ~ Mkt_RF ** p-value H of Asymptotic Tests: LR, Wald, and LM In practice, we tend to rely on the asymptotic distribution of the Wald test. That is, W . There are two other popular tests th at are asymptotically equivale nt -i.e., they have the same asymptotic distribution: the Likelihood Ratio (L R) and the Lagrange Mu ltiplier (LM) tests. The LR is based on the (log) Likelihood. It needs two ML estimations: the unrestricted estimation, producing , and the restricted estimation, producing . Below we define the LR test: 2log log Note: MLE requires assuming a di stribution, usually, a normal. Technical note: The LR test is a consistent test . An asymptotic test which rejects H 0 with probability one when the H 1 is true is called a consistent test. That is, a consistent test has asymptotic power of 1. The LR test is a consistent test. Example: We use a likelihood ratio test to check if the additional FF factors (SMB, HML) are significant, using monthly p-value is small: Reject H \u00b6 The LM test needs only one estimati on: the restricted estimation, producing . If the restriction is true, then th e slope of the objective func tion (say, the Likelihood) at should be zero. The slope is called the Score, S( ). The LM test is based on a Wald test on S( ) = 0. It turns out that there is a much simpler formula tion for the LM test, based on the residuals of the restricted model. We will present this version of the test in Lecture 6. If the likelihood function were qua dratic then LR = LM = W. In general, however, W > LR > LM. Testing Remarks: Pre-testing A special case of omitted variables. - First, a researcher starts w ith an unrestricted model (U): y = X + . (U) - Then, based on (\"preliminary\") tests -say, an F-test - a researcher decides to use restricted estimator, b*. That is, y = X + . s.t. R = q (R) - We can think of the estimator we get from estimating R as: b PT = I{0, c}(F) b* + I {c, }(F) b, indicator function: I {c, }(F) =0, if F-stat in R {0, c}(F) =1 if F-stat in RC -say, F < c. c : critical value chosen for testing H 0: R = q , using the F-stat. The pre-test estimator is a rule, which chooses between the restricted estimator, b*, or the OLS estimator, b: b* + }(F) b, where b - (XX)-1R[R(XX)-1R]-1(Rb - q) Two \"negative\" situations: (1) H 0: R = q is true. The F-test will incorrectly reject H 0 % of the time. That is, in % of the repeated samples, we have \"irrelevant variables\" OLS b: No bias, but inefficient estimator. (2) H 0: R = q is false. The F-test will correctly reject H 0 a % of times equal to the power of the test. That is, (100 - )% of the time, R=q will be incorrectly imposed, we have \"omitted variables:\" OLS b*: bias, but small variance! The failure of the OLS estimator to have the properties under correct specification is called pre- test bias . Pre-testing (also called sequential estimation , data mining ) is common in practice. In general, it is ignored -and not even acknowledged. Main argument to ignore pre-testi ng: We need some assumptions to decide which variables are included in a model. Is the probability that pre-testing yields an incorrect set of X greater than the probability of selecting the \"correct\" assumption? David Hendry, a well known thinker of these met hodological issues, does not see pre-testing in the discovery stage as a probl em. For him, pre-testing at that stage is part of the process of discovery . Practical advise: Be aware of the problem. Do not rely solely on stats to select a model -use economic theory as well. Do not use same sample evidence to generate an H 0 and to test it! Example : The Fama-French factors have been \"d iscovered\" using the CRSP/Compustast database for a long, long time. Thus, testing th e Fama-French factors using the CRSP/Compustat is not advisable! (You can test them with another dataset, for example, get international data.) \u00b6 Testing Remarks: Significance level, So far, we have assumed that the dist ribution of the test statistic -say the F-statistic- under H 0 is known exactly, so that we have what is called an exact test . Technically, the size of a test is the supremum of the rejec tion probability over all DGPs that satisfy H 0. For an exact test, the size equals the nominal level , -i.e., the Prob[Type I error] = . Usually, the distribution of a te st is known only approximately (asymptotically ). In this case, we need to draw a distinction between the nominal level, (nominal size ), of the test & the actual rejection probability (empirical size ), which may differ greatly from the nominal level. Simulations are needed to gauge the empirical size of tests. Testing Remarks: A word about Ronald Fisher, before computers, tabulated di stributions. He used a .10, .05, and .01 percentiles. These tables were easy to use and, thus, t hose percentile became the de-facto standard for testing H 0. \"It is usual and convenient for experimenters to ta ke 5% as a standard le vel of significance.\" - Fisher (1934). Given that computers are pow erful and common, why is p = 0.051 unacceptable, but p = 0.049 is great? There is no published work that provides a theoretical basis for th e standard thresholds. Rosnow and Rosenthal (1989): \" ... surely God loves .06 nearly as much as .05.\" Practical advise: In the usual Fi sher's null hypothesis (significan ce) testing, significance levels, , are arbitrary. Make sure you pick one, say 5%, and stick to it thr oughout your analysis or paper. Report p-values , along with CI's. Search for economic significance . Questions: .10, .05, or .01 significance? Many tables will show *, **, and *** to show .10, .05, and .01 significance levels -for example, lm() in R. Throughout the paper, the authors will point out the different significance levels. In these papers, it is not clear what is the paper using for inference. We can think of these stars (or p-values ) as ways of giving weights to H 0 relative to H 1. Testing Remarks: A word about H 0 In applied work, we only learn when we reject H 0; say, when the p-value < . But, rejections are of two types: - Correct ones, driven by the power of the test - Incorrect ones, driven by Type I Error (\" statistical accident ,\" luck). It is important to realize that, however small the p-value, there is always a finite chance that the result is a pure accident. At the 5% level, th ere is 1 in 20 chances that the rejection of H 0 is just luck. Since negative results are difficult to publish ( publication bias ), there is an unknown but possibly large number of fals e claims taken as truths. Example: If 0.05, proportion of false H 0=10%, and = .50, 47.4% of rejections are true H 0 -i.e., \"false positives.\" \u00b6 Testing Remarks: Mass significance We have a model. We perform k different tests, say k t-tests, each with a nominal significance level of : = Prob (Rejecting for a given test |H 0 for this test is true) The overall significance of the test procedure is, however, given by * = Prob (Rejecting at l east one test | all H 0 are true). The probability of rejecting at least one H 0 is obviously greater than of rejecting a specific test. This is the problem of mass significance . Two cases (1) Independent tests * = 1 (1 ) k & = 1 (1 *)1/k (2) Dependent tests: * k & */k close to the \"independent\" values for small , but can differ for large . Example : repeated parametric testing (overall level 5%): - Only accept variables as important when their p-values are less than 0.001, preferably smaller - Maybe look for other ways of choosing variables, say AIC. In repeated diagnostic testing (overall level 20%), we should only accept there is no misspecification if - All p-values are greater than 0.05, or - Most p-values are greater than 0.10 with a few in the range 0.02 to 0.10 Non-nested Models and Tests So far, all our tests (t-, F- & Wald tests) have been based on nested models, where the R model is a restricted version of the U model. Example : Model U Y = X + W + (Unrestricted) Model R Y = X + (Restricted) Model U becomes Model R under H 0: = 0. \u00b6 Sometimes, we have two rival models to choos e between, where neither can be nested within the other -i.e., neither is a rest ricted version of the other. Example: Model 1 Y = X + W + Model 2 Y = X + Z + . \u00b6 If the dependent variable is the same in both m odels (as is the case here ), we can simply use Adjusted-R 2 to rank the models and select th e one with the largest Adjusted-R2. We can also use AIC and/or BIC. But, we can also use more sophisticated tes ting-based methods: Encomp assing test and J-test. Non-nested Models: Encompassing Test Alternative approach: Encompassing (1) Form a composite or encompassing model that nests both rival models say, Model 1 and Model 2. This is the unrestricted Model (ME). (2) Test the relevant restrictions of each rival model against ME . We do two F-tests, where the restricted models are Model 1 and Model 2. If we reject the restrictions against one Model, say Model 1, and we cannot reject the restrictions against the other, Model 2, we are done: We se lect the Model that the F test do not reject restrictions (Model 2). Assuming the restrictions cannot be rejected, we prefer the model with the lower F statistic for the test of re strictions. Note: We test a hybrid model. Also, multicollinearity may appear. Example : We have: Model 1 Y = X + W + Model 2 Y = X + Z + Then, the Encompassing Model (ME) is: ME: Y = X + W + Z + Now test, separately, the hypotheses (1) = 0 and (2) = 0. That is, F-test for H 0: = 0: ME (U Model) vs Model 1 (R Model). F-test for H 0: = 0: ME (U Model) vs Model 2 (R Model). If we reject H 0: = 0 Evidence against Model 1 (statistically different from ME). If we reject H 0: = 0 Evidence against Model 2 (statistically different from ME). \u00b6 Non-nested Models and Tests: IFE or PPP? Two of the main theories to explain the behaviour of exchange rates, S t, are the International Fisher Effect ( IFE) and the Purchasing Power Parity ( PPP). We use the direct notation for S t, that is, units of domestic currency (D C) per 1 unit of foreign currency (FC). IFE states that, in equilibrium, changes in excha nge rates (e) are driven by the interest rates differential between th e domestic currency, i d, and the foreign currency, i f:. A DGP consistent with IFE is: e = 1 + 1 (id - i f) + 1 PPP, in its Relative version, states that that, in equilibrium, e are driven by the inflation rates differential between the domestic Inflation rate, I d, and the foreign Inflation rate, I f. A GDP consistent with IFE is: e = 2 + 2 (Id - I f) + 2 Both theories are non-nested, thus, we need a non-nested method to select a model. Example : What drives log changes in excha nge rates for the USD/GBP (e): (i d - i f) or (I d - I f)? The USD is the DC; the GBP is the FC. Both non-nested models are: IFE Model : e = 1 + 1 (id - if) + 1 PPP Model : e = 2 + 2 (Id - If) + or \"Unrestricted Model\") e = + 1 (id - i f) + 2 (Id - I f) + 1 # Encompassing Model and Test fit_me <- lm(lr_usdgbp ~ int_dif + in summary( fit_me ) Coefficients: 0.3429106 2.171 0.0306 0 '***' 0.001 '**' 0.01 '*' ' ' 1 Residual standard error: 0.02662 on 360 degrees of freedom Multiple R-squared: 0.01316, encompasing test favors the PPP Model. Note: Two F-tests are needed, but for the one variable case, the t- tests are equivalent. R Note: The package in R, lmtest, performs this test, encomptest . Recall you need to install it first: install.packages(\"lmtest\"). The test reported is an F-test ~ ,, which, in this case with only one variable in each Model, is equal to ( )2. library(lmtest) fit_m1 <- lm(lr_usdgbp fit_m2 <- lm(lr_usdgbp ~ inf_dif) # Restricted Model 2 1: lr_usdgbp int_dif reject H 0: 2 0. Non-nested J-test We test for non-nested models, the Davidson-MacKinnon (1981)'s J- test. We start with two non-nested models. Say, Model 1 : Y = X + Model 2 : Y = Z + Idea: If Model 2 is true, then th e fitted values from the Model 1, when added to the 2nd equation, should be insignificant. Steps: (1) Estimate Model 1 obtain fitted values: Xb. (2) Add Xb to the list of regr essors in Model 2 Y = Z + Xb + (3) Do a t-test on . A significant t-value would be evidence against Model 2, favoring Model 1 . (4) Repeat the procedure for the models the other way round. (4.1) Estimate Model 2 obtain fitted values: Zc. (4.2) Add Zc to the list of regressors in Model 1: Y = X + Zc + (4.3) Do a t-test on . A significant t-value would be evidence against Model 1 and in favor of Model 2 . (5) Rank the models on the basis of this test. It is possible that we cannot reject both models . This is possible in small samples, even if one model, say Model 2, is true. It is also possible that both t-tests reject H 0 ( 0 & 0). This is not unusual. McAleer's (1995), in a survey, reports that out of 120 applications all m odels were rejected 43 times. Technical Note: As some of the regressors in step (3) are stochastic, Davidson and MacKinnon (1981) show that the t-test is asymptotically valid. One would also want to examine the diagnos tic test results when choosing between two models. Non-nested Models: J-test - IFE or PPP? Example : Now, we test IFE Model vs PPP Model 2, for changes in the USD/GBP exchange rate using the J-test. Model 1 (IFE): e = 1 + 1 (id - i f) + 1 Model 2 (PPP): e = 2 + 2 (Id - I f) + 2 y <- lr_usdgbp 0.001 '**' 0.01 ' ' 1 Residual standard error: 0.02662 on 360 degrees of freedom Multiple R-squared: 0.01316, '***' 0.001 '**' 0.01 ' ' 1 Residual standard error: 0.02662 on 360 degrees of freedom Multiple R-squared: 0.01316, J-test selects the Model 2 ). R Note: The lmtest package also performs this test, with the function jtest. Recall that you need to install Model 2: lr_usdgbp ~ inf_dif Estim to test H0: y = X + 0 (additive) vs H1: ln y = (ln X) + 1 (multiplicative) We look at the J-test - Step 1: OLS on H1: get OLS y = X + 1 exp{ln( X) } + t-test on 1 - Step 2: OLS on H0: get b OLS ln y = (ln X) + 0 Xb + t-test on 0 Situations: (1) Both OK: 1 = 0 and 0 = 0 get more data (2) Only 1 is OK: 1 0 and 0 = 0 (multiplicative is OK); 0 0 and 1 = 0 (additive is OK) (3) Both rejected: 1 0 and 0 0 new model is needed. Non-nested Models: J-test - Remarks The J-test was designed to test non-nested models (one model is the true model, the other is the false model), not for choosing competing models -the usual use of the test. The J-test is likely to over reject the true (model) hypothesis when one or more of the following features is present: i) A poor fit of the true model ii) A low/moderate correlation between the regressors of the 2 models iii) The false model includes more regressors than the correct model. Davidson and MacKinnon (2004) state that the J-test will over-reject, often quite severely in finite samples when the sample size is small or where conditions (i) or (i ii) above are obtained. Lecture 6 - Specification, Fo recasting & Mo del Selection OLS Estimation - Assumptions Brief Review of CLM Assumptions (A1) DGP: y = X + is correctly specified. (A2) E[|X] = 0 (A3) Var[|X] = 2 IT (A4) X has full column rank -rank( X)=k-, where T k. Question: What happens when ( A1) is not correctly specified? First, we look at ( A1), in the context of linearity. Are we omitting a relevant regressor? Are we including an irrelevant variab le? What happens when we impos e restrictions in the DGP? Second, in ( A1), we allow some non-lineariti es in its functional form. Specification Errors: Omitted Variables Omitting relevant variables: Suppos e the correct model (DGP) X22 + -the \"long with X1 & X2. But, we compute OLS omitting X 2. That is, y = X11 + -the \"short regression.\" We have two nested models: one model becomes the other, once a restriction is imposed. In the above case, the true model becomes \"the s hort regression\" by im posing the restriction 2 = 0. Question: What are the implications of usi ng the wrong model, with omitted variables? We already know the answer, we are imposing a wrong restriction: the restricted estimator, b*, is biased, but it is more efficient. Specification Errors: Omitted Variables Some easily proved results: E[b1|X] = E [( X1X1)-1X1 y] = E [( X1X1)-1X1 (X11 + X22 + )] = 1 + (X1X1)-1X1X22 1. Thus, unless X1X2 =0, b1 is biased . The bias can be huge . It can reverse the sign of a price coefficient in a \"demand equation.\" (2) Var[ where b1.2 is the OLS estimator of 1 in the long regression (the true model). Thus, we get a smaller variance when we omit X 2. Interpretation: Omitting X2 amounts to using extra information -i.e., 2 = 0. Even if the information is wrong, it reduces the variance. (3) Mean Squared Error (MSE = RSS/T) If we use MSE as precision criteri a for selecting an estimator, b 1 may be more \"precise.\" Precision = Mean squared error (MSE) = Variance + Squared bias. Smaller variance but positive bias. If bias is small, a practitioner may still favor the short regression. Note: Suppose X 1X2 = 0. Then the bias goes away. Inte rpretation, the information is not \"right,\" it is irrelevant: b1 is the same as b1.2. Example: We fit an ad-hoc model for U.S. short-term interest rates (i US,t) that includes inflation rate (I US,t), changes in the USD/EUR (e t), money growth rate (m US,t), and unemployment (u US,t), using monthly data from 1975:Jan-2020: Jul. That is, i US,t = 0 + 1 IUS,t + 2 et + 3 mUS,t + us_mg <- log(us_M1[-1]/us_M1[-T]) # US Money Growth: (Log) Changes in log(S_ger[-1]/S_ger[-T]) # (Log) Changes in USD/EUR us_i_1 sample size of untransformed us_u_1 <- us_u[-1] # Adjust sample size of untransformed data us_i_0 <- us_i[-T] # lagged interest 0.01 '*' ' ' 1 Residual standard error: 3.113 on 542 degrees of freedom Multiple R-squared: 0.2276, 0.2219 DF, p-value: us_mg, us_u_1, ~ xx_i) '***' 0.001 '**' 0.01 ' ' 1 Residual standard error: 3.113 on 542 degrees of freedom Multiple R-squared: 0.2276, R-squared: 0.2219 Note: Lagged i US (iUS, t-1 ) is very significant & changes significance of other variables. It may point out to a general misspecification in ( A1). \u00b6 Omitted Variables Example: Gasoline Demand We have a linear model for the demand for gasoli ne (G) as function of price (PG) and income (Y): G = PG 1 + Y 2 + , Q: What happens when you wr ongly exclude Income (Y)? E[b 1|X] = 1 + 2 In time series data, 1 < 0, 2 > 0 (usually) C o v [ Price , Income ] > 0 in time series data. The short regression will overestimate the price coefficient. In a simple regression of G (demand) on a constant and PG, the Price Coefficient ( 1) should be negative. Example: Estimation of Equation: Shoul dn't be Negative? Taken from Green's gra duate Econometrics textbook If a multiple regression is done, inco rporating income, Y, theory works! Ordinary least squares regression ............ LHS=G Mean = 226.09444 Standard deviation = 50.59182 Number of observs. = 36 Model size Parameters = 3 Degrees of freedom = 33 Residuals Sum of squares = 1472.79834 Standard error of e = 6.68059 Fit R-squared = .98356 Adjusted R-squared to identify a dema nd equation -i.e., with a negative slope for the price variable. \u00b6 Specification Errors: Irrelevant Variables Irrelevant variables. Suppose the correct model is y = X11 + -the \"short regression,\" with X1 But, X22 + -the \"long regression.\" Some easily proved results: Including irrelevant variables just reverse the omitted variables results: It increases variance -the cost of not using information-; but does not create biases. Since the variables in X 2 are truly irrelevant, then 2 = 0, so E[ b1.2|X] = 1. A simple example Suppose the correct model is: y = 1 + 2 X2 + But, we estimate: y = 1 + 2 X2 + 3 X3 + Results: - Unbiased: Given that 3 = 0 E[b 2|X] = 2 - Efficiency: | 1 1, where ,is the correlation coefficient between X 2 and X 3. Note: These are the results in general. Note that if X2 and X3 are uncorrelated, there will be no loss of efficiency after all. Testing Model Specification: Nested Models In both previous cases, we have two nested models, one is the restricted ve rsion of the other. For example, in the case of omitted variables: (U) y = X 1 + Z 2 + -the \"long regression,\" (R) y = X 1 + -the \"short regression.\" To test H 0 (No omitted variables): 2 = 0, we can use the F-test: F = / / ~ F J,T-K. Example : In the previous Lecture, we pe rformed this F-test to test if in the 3-factor FF model for IBM returns, SMB and HML were significant, whic h they were. That is, we showed that the usual CAPM formulation for IBM returns had omitted variables: SMB and HML. Testing Model Specification with an LM Test Note that the F-test requires two estimations: th e Unrestricted model and the Restricted model. There is another test of H 0: 2 = 0, that only uses the restricted model as the basis for testing: The Lagrange Multiplier (LM) test, which we introduced in Lecture 5. In this lecture, we present the simpler formulatio n of the LM test, which is based on the residuals of the restricted model. Simple intuition. Everything that is omitted from (& belongs to!) a model is in the residuals ( e R). The LM test is based on eR: We check if the omitted variables, Z, show up as drivers of e R. We use a simple regression of e R against Z to check for the misspecification. LM test steps: (1) Run restricted model ( y = X 1 + ). Get restricted residuals, eR. (2) (Auxiliary Regression). Run the regression of e R on all the m omitted m variables, Z, and the k included variables, X. In our case: e R,i = 0 + 1 xi,1 + ...+ k xi,k + 1 zi,1 + .... + m zi,m + v i Keep the R2 from this regression, . (3) Compute LM-statistic: L M = T * . Technical Note: We include the original variables in (2), X, in the auxiliary re gression to get the convenient form for the LM-test, as shown by Engle (1982). The LM Test is very general. It can be used in many settings, for example, to test for nonlinearities, intera ctions among variables, autocorrelation or hetero Asymptotically speaking, the LM Test, the LR Test and the Wald Test are equivalent -i.e, they have the same limiting distribution, . In small T, they can have different conclusions. In general, however, we find: W > LR > LM. That is, the LM test is more conservative (cannot reject more often) and the Wa ld test is more aggressive. Example : We use an LM test to check if the sta ndard CAPM for returns ~ Mkt_RF > summary( fit_lm ) ' 1 Residual standard error: 0.05848 on 565 degrees red (df=2) level p_val <- SMB & HML not in model. Conclusion: We strongly reject the CAPM (one factor model) , since the LM tests strongly suggests that SMB and HML should be in the model. \u00b6 Note: In Lecture 5 we performed the same test w ith the Wald test (using the F distribution), the p-value was 0.0091175 . (This almost exact coincidence is not always the case.) Functional Form: Linearity in Parameters Linear in variables and parameters: . So far, this is the linear model we have used. OLS estimates all parameters: , ,, & . Non-linear in variables, but linear parameters -i.e., intrinsic linear : log Define: , , & log Then, the non-linear model becomes a linear model: Again, OLS can be used to estimate all , ,, & . Suppose we have: The model allows for a quadratic relation between y and X2: y X 2 Let = , then, the model is intrinsic linear: Example: We want to test if a measure of market risk (Mkt Ret - rf)2 enters as an additional explanatory variable in the 3- f actor FF model for IBM returns. The model is non-linear in (Mkt Ret - rf), but still intrinsic linear: IBM Ret - rf = 0 + 1 (Mkt Ret - rf) + 2 SMB + 3 HML + 4 (Mkt Ret - rf)2 + We can do OLS, by redefining the variables: Let = (Mkt Ret - rf); H 0: =0. That is, there is no evidence that (Mkt Ret - rf)2 is an explanatory variable for IBM excess returns. Now, we can also check with an LM test if all variables squares ((Mkt Ret - rf)2, SMB2, and HML2) are omitted from > p_val [1] 0.4836944 p-value is higher than standard levels Cannot Reject H 0. \u00b6 Conclusion: The LM test cannot reject the 3-fact or F-F model, since all squared terms are not jointly significant. \u00b6 Nonlinear in parameters: This model is nonlinear in parameters since the coefficient of X4 is the product of the coefficients of X2 and X3. OLS cannot be used to estimate all parameters. Some nonlinearities in parameters can be lineari zed by appropriate transf ormations, but not this one. This is not an intrinsic line ar model. Different estimation tec hniques should be used in these cases. Intrinsic linear models can be estimated usi ng OLS. Sometimes, transformations are needed. Suppose we start with a power function: The errors enter in multiplicative form. Then, using logs: log log loglog log, or , where log, log,log, log Now, we have an intrinsic linear model: OLS can be used to estimate all the parameters. Similar intrinsic model can be obtained if Note: Recall that we can only use logs when has positive values. In general, we use logs when we believe the independent variab le has an exponential or powe r formulation, typical behavior for nominal variables, like sales, revenue or prices. Not all models are intrinsic linear. For example: log log We cannot linearize the model by taking loga rithms. There is no way of simplifying log( + ). We will have to use some nonlinear estimation t echnique for these situations. (ML can estimate this model.) Functional Form: Linear vs Log specifications Two popular models, especially in Co rporate Finance: linear or log? Model 1 - Linear model: Model 2 - (Semi-) Log model: log Box-Cox transformation: 1 when =1 log when 0 Putting = 0 gives the (semi-)log m odel (think about the limit of tends to zero.). The Box- Cox transformation is flexible. We can estimate to test if is equal to 0 or 1. It is possible that it is neither! Functional Form: Ra msey's RESET Test To test the specification of the functional form, Ramsey designed a simple test. We start with the fitted values from our ( A1) model: = Xb. Then, we add 2 to the regression specification: y = X + 2 + If 2 is added to the regression specification, it should pick up quadratic and interactive nonlinearity, if present, without necessarily being highly correlat ed with any of the X variables. We test H 0 (linear functional form): = 0 H 1 (non linear functional form): 0 t-test on the OLS estimator of . If the t-statistic for 2 is significant evidence of nonlinearity. The RESET test is intended to de tect nonlinearity, but not be spec ific about the most appropriate nonlinear model (no specific func tional form is specified in H 1). Example: We want to test the functi onal form of the 3 FF Factor Model for IBM returns, using monthly data 1973-2020. package performs this test, resettest, (and many others, used in this class, encompassing, jtest, waldtest, etc). You need to in stall it first: install.pa ckages(\"lmtest\"), then call the library(lmtest). Note: The test reported is an F-test ~ F 1,T-k, which is equal to (t T-k)2. The p-values should the same. (-0.379 )2 = 0.1434 . Conclusion: The RESET test does not find evid ence of non-lineari ties (or, in general, of misspecification) in the 3-factor F-F model, since the squared fitted values are not significant at the 5% level. \u00b6 Qualitative Variables and Functional Form Suppose that you want to model CEO compensation as a function of edu cation. You have data on annual total CEO compensation ( Comp ), annual returns, annual sales, CEO's age, CEO's previous experience, and the CEO's last degr ee (education). We have qualitative data. One approach to see the impact of educati on on the CEO compensation model is to run individual regressions for each last degr ee -i.e., BA/BS; MS/MA/MBA; Doctoral: degree Comp i = 0-u + 1-uzi + u,i Masters degree Comp i = 0-m + 1-mzi + m,i Doctoral degree Comp i = 0-d + 1-dzi + d,i where the zi is a vector of the CEO i's age and previous experience and his/her firm's annual returns and annual sales. We observe the impact of education through the different coefficients in each regression. A potential problem with this appr oach is that we may end up with three small samples (and imprecise estimations). An alternative approach that uses the whole sa mple in the estimation is to combine the three regressions in one. To do this, we use a \" dummy variable\" -also called, indicator variable -, which is a variable that points whether an observation belongs to a category or class or not. For example: D C,i = 1 if observation i belongs to category C (say, male.) = 0 otherwise. Simple process: First, we define dummy/indicat or variables for Master s & doctoral degrees: D m = 1 if at least Masters degree = 0 otherwise. D d = 1 if doctoral degree = 0 otherwise. Then, we introduce the dummy/indi cator variables in the model: Comp i = 0 + 1zi + 2Dm,i + 3Dd,i + 1zi Dm,i + 2zi Dd,i + i Not, this model uses all the sample to estimate the parameters. It is flexible: - Model for undergrads only (D m,i = 0 & D d,i = 0): Comp i = 0 + 1zi + i - Model for Masters degree only (D m,i = 1 & D d,i = 0): Comp i = (0 + 2) + (1 + 1)zi + i - Model for Doctoral degree only (D m,i = 1 & D d,i = 2): Comp i = (0 + 2 + 3) + (1 + 1 + 2)zi +i The parameters for the different categories are: - Constant: Constant for undergrad degree: 0 Constant for Masters degree: 0 + 2 Constant for Doctoral degree: 0 + 2 + 3 - Slopes: Slopes for undergrad degree: 1 Slopes for Masters degree: 1 + 1 Slopes for Doctoral degree: 1 + 1 + 2 We can test the effect of edu cation on CEO compensation: (1) H 0: No effect of grad degree: 3 = 2 = 0 & 1 = 2 = 0 F-test . (2) H 0: No effect of Masters degree on constant: 2 = 0 t-test . (3) H 0: No effect of doctoral degree: 3 = 0 & 2 = 0 F-test . (4) H 0: No effect of Dr degree on marginal effect: 2 = 0 F-test . We may have more than one qua litative category (last degree a bove) in our data that we may want to introduce in our model. Example : Suppose we also have data for CEO gradua te school. Now, we can create another qualitative category, \"quality of school\", define d as Top 20 school, to test if a Top 20 school provides \"more value.\" To do this, we use D T20 to define if any schooling is in the Top 20. D T20 = 1 if school is a Top 20 school = 0 otherwise. The model becomes: Comp i = 0 + 1zi + 2Dm,i + 3zi DT20,i + i In this setting, we can te st the effect of a Top20 e ducation on CEO compensation: (1) H 0: No effect of Top20 degree: 4 = 0 and 3 = 0 F-test . \u00b6 The omitted category is the reference or control category. - In our first example, with only educational degrees, the reference ca tegory is undergraduate degree. - In the second example, with educati onal degrees and quality of school (Top20 dummy), the reference category is undergraduat e degree with no Top 20 education. Dummy trap . If there is a constant, the numbers of dummy variables per qualitative variable should be equal to the number of categories minus 1. If you put the number of dummies variables equals the number of categories, you will create perfect multicollinearity. Dummy Variables as Seasonal Factors A popular use of dummy variables is in estimating seasonal effects. We may be interested in studying the January effect in stoc k returns or if the returns of oil companies (say, Exxon or BP) are affected by the seasons, since in the wi nter people drive less and in the summer more. In this case, we define dummy/i ndicator variables for Summer, Fall and Winter (the base case is, thus, Spring): D Sum,i = 1 if observation i occurs in Summer = 0 otherwise. D Fall,i = 1 if observation i o c c u r s i n F a l l = 0 otherwise. D Win,i = 1 if observation i occurs in Winter = 0 otherwise. Then, letting Z be the three FF factors, we have: XOM i = 0 + 1zi + 2DSum,i + 3DFall,i + 4 DWin,i + i Example: In the context of the 3-factor FF model, we test if Exxon (XOM) is affected by seasonal (quarters) factors: XOM i = 0 + 1zi + 2DSum,i + 3DFall,i + 4 DWin,i + i -0.007100 0.004944 -1.436 0.1515 Interpretation: In the Winter quarter, Exxon ex cess returns decrease, re lative to the Spring, by 1.13% . But since Spring's (& Fall's & Winter's) eff ect is non-significant, the decrease is in absolute terms. Conclusion: The t-value for the Winter dummy (Win t_1) is significant at th e 5% level. That is, we reject H 0: No seasonal effect on XOM excess returns. We can test if all quarters jointly matter. That is, H 0: 2 = 3 = 4 = 0. We do an F-test: fit_u <- reject joint seasonal effect. Conclusion: We cannot reje ct, at the 5% level, H 0: No joint seasonal effect. Suppose we are also interested in checking if the slopes -i.e., the marginal effects- are affected by the Winter quarter. Then, we fit: XOM i = 0 + 1zi + 2DSum,i + 3DFall,i + only factor in teracting significantly with Winter is the Market factor. Then, we have two significantly different slopes: - In the Winter, the Market slope is: 0.695762 + 0.208912 = 0.903674 - In all other quarters, the Market is: 0.695762 It looks like in the Winter, XOM behaves closer to the Market, wh ile in all other quarters, it is significantly less risky than the market. Now, we perform a joint test for interacting Winter effects in the model:: > f_test p-value < .05, then, we reject H 0 (joint Winter interactive effect): 1 = 0. Conclusion: We strongly reje ct, at the 5% level, H 0: No joint Winter interactive effect. \u00b6 Dummy Variables: Is There a January Effect? The January Effect is a hypothesis that states that the stock mark et has an unusually high return during the first month of the year. This result can be traced to an observation made in 1942 U.S. investment banker Sidney Wachte l. Wachtel noticed higher returns for small stocks than for large stock in January, a result later examined by Kiem (1983), who found th at the January return premium was evident for small stocks. In one of the earlier studies, Rozeff and Kinney (1976) found seasonal patterns in an equal-weighted index of NYSE prices over the period 1904-74. Specifically, the average monthly return in Ja nuary was 3.5%, while other months averaged 0.5% percent. A very strong result that shows a clearly predicta ble pattern, which goes against the Efficient Markets Hypothesis. Since then, a lot of work has been done: The evidence suggests that, in recent years, the January effect has dissipated. Example: We want to test the January effect on IBM stock returns, where because of tax reasons/window dressing, stocks go down in Decem ber and recover in January. The test can be done by adding a dummy variable to the 3-factor FF model: D J,i = 1 if observation i occurs in January = 0 otherwise. Then, we estimate the expanded model: (IBM Ret - rf)i = 0 + 1 (Mkt Ret - rf)i + 2 SMB i + 3 HML i + 4 DJ,i + i We test H 0(No January effect): 4 = 0 t-test . Alternatively, we can estimate do an LM test on the residuals of the 3-factor FF model and check if D J is significant. T <- length(ibm_x) Jan <- rep(c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) , (round(T)/12+1)) # Create January dummy Regression > summary( fit_Jan ) ' 1 Residual standard error: 0.058 on 565 degrees LM_test > p_val [1] 0.002578207 p-value is small Reject H 0. Given this result, we modify the 3-factor FF and add the January Dummy ' 1 Residual standard error: 0.058 on 565 degrees We have two Jensen's Feb - Dec: -0.7302% (significant). January: -0.7302% + 2.6966% = 1.9664% (significant). When the January dummy was not in the model, we had: -0.005191 , which is close to an average of the constants (= -0.007302 *11 + 0.019664 )/12 = IBM has an additional 2.6966% excess returns. This is a big number. Today, the evidence for the January eff ect is much weaker than in this case. \u00b6 Note: In the FF model we expect th e constant to be very small ( 0). In this case, it is not zero. Like in the case of the CAPM, a significant cons tant is evidence against the 3-factor model of Fama-French. Maybe we have a misspecified ( A1). Dummy Variable for One Observation We can use a dummy variable to isolate a single observation. D J = 1 for observation j. = 0 otherwise. Define d to be the dummy variable in question. Z = all other regressors. X = [Z, D J] Multiple regression of y on X. We know that X'e = 0 where e = the column vector of residuals. D J'e = 0 ej = 0 (perfect fit for observation j). This approach can be used to deal with (eliminate) outliers . Example: In Dec 1992, IBM reported record losses and ga ve a very bleak picture of its future. The stock tumbled -30.64% that month. We check the effect of that extreme observation, a potential outlier, on the 3-f actor FF model + January dummy: dec_1992 <- rep(0,T) # Define Dec 1992 dummy dec_1992[239] <- 1 # Define Dec 1992 dummy (=1 if 1992) fit_d92 <- of observation) Potential \"Outlier\" has no major effect on coefficients. \u00b6 Chow Test: Testing the effect of Categories on a Model It is common to have a qualitative variable w ith two categories, say e ducation (MS/MBA or not). Before modelling the data, we can check if onl y one regression (\"pooling\") model applies to both categories. We use the Chow Test (a n F-test) -Chow (1960, Econometrica ). Steps: (1) Run OLS with all the data , with no distinctio n between schools (Pooled regression or Restricted regression). Keep RSS R. (2) Run two separate OLS, one for each sc hool (Unrestricted regression). Keep RSS 1 and RSS 2 RSS U = RSS 1 + RSS 2. (Alternative, we can run just one regression with the dummy variable). (3) Run a standard F-test (testing Re stricted vs. Unrestricted models): / / / / 2 Example: Who visits doctors more: Men or Women? Data: German Health Care Usage Data, with 7,293 Individuals. Time Periods: Varying Number. Variables in the file are: Data downloaded from Journal of Applied Econom etrics Archive. This is an unbalanced panel with 7,293 individuals. Th ere are altogether 27,326 observations. The number of observations ranges from 1 to 7 per is: DOCVIS = number of visits to th e doctor in the observation period The explanatory variables are: HHNINC = household nominal monthly net income in German marks / 10000. (4 observations with income=0 were dropped) HHKIDS = children under age 16 in the household = 1; otherwise = 0 EDUC = years of schooling AGE = age in years MARRIED= marital status (1 = if married) WHITEC = 1 + x_hhinc) summary( fit_doc_vis ) OLS Estimation for Men only. Keep RSS M = 372,818.1 Coefficients: Estimate ' 1 Residual standard error: 5.118 on 14233 degrees 2.2e-16 OLS Estimation for Women only. Keep RSS W = 478,894.2 ## Run a regression with only Women data. Use Allgen to collect relevant data for women only.We will do a for loop and keep data if x_fem is greater than empty (to collect variables by one sex (f/m) only) i <- 1 T <- length(x_fem) k <- ncol(xx) for (i in 1:T) { (xx[i,1] > 0) { Allgen = rbi ' 1 Residual standard error: 6.052 on 13075 degrees 2.2e-16 OLS Estimation for Men only. Keep RSS M = 372,818.1 # Use above code, but change for loop ( now, keep data if x_fem less than 1) for (i in 1:T) { if (xx[i,1] < 1) { Allgen = rbi nd(Allgen, xx[i,2:k]) } } Coefficients: ' 1 Residual standard error: 5.118 on 14233 degrees for RSS ALL = 858,435 Coefficients: ' 1 Residual standard error: 5.606 on 27,315 degrees = 2.009925 reject H 0. Conclusion: There is strong evidence that men and women do not have the same behavior. \u00b6 Functional Form: Structural Change Suppose there is an event that we think had a bi g effect on the behaviour of our model. Suppose the event occurred at time TSB. We think that the before and after behaviour of the model is significantly different. For example, the pa rameters are different before and after TSB. That is, y i = 0 + 1 X1,i+ 2 X2,i + 3 X3,i + i for i TSB y i = 0 + 1 X1,i+ 2 X2,i + 3 X3,i + i for i > T SB The event caused structural change in the model. T SB separates the behaviour of the model in two regimes/categories (\" before \" & \" after \".) A Chow test can be used to check if one model applies to both regimes: y i = 0 + 1 X1,i+ 2 X2,i + 3 X3,i + i for all i Under H 0 (No structural change ), the parameters are the same for all i. We test H 0 (No structural change ): = = 0 = = 1 = = 2 = = 3 H 1 (structural change ): For at least (= 0, 1, 2, 3): What events may have this effect on a model? A financial crisis, a bi g recession, an oil shock, Covid-19, etc. Testing for structural change is th e more popular use of the Chow test. Chow tests have many interpretations: tests fo r structural breaks, pooling groups, parameter stability, predictive power, etc. One important consideration: T may not be large enough. For example, we may think that Covid- 19 had a structural effect on the behaviour of te ch companies. We may not have enough data to run an F-test. We structure the Chow test to test H 0 (No structural change ) as usual. Steps for Chow (Structural Change) Test: (1) Run OLS with all the data, wi th no distinction betw een regimes (Restricted or pooled model): Keep RSS R. (2) Run two separate OLS, one for each regime (Unrestricted model): Before Date T SB.. Keep RSS 1. After Date TSB.. Keep RSS 2. RSS U = RSS 1 + RSS 2. (3) Run a standard F-test (testing Re stricted vs. Unrestricted models): / / / / 2 Example: We test if the Oct 1973 oil shock in quarter ly GDP growth rates had an structural change on the GDP growth rate model. We model GDP the growth rate with an AR(1) m odel, that is, GDP growth rate depends only on its own lagged growth rate: y t = 0 + 1 ~ lr_gdp1) # e sum(e1^2) # RSS Regime 1 kk = t_s+1 F_test > p_val [1] 0.00824892 Reject H 0 (No structural change ). \u00b6 Example : 3 Factor Fama-French Mode l for IBM (continuation) Q: Did the dot.com bubble (end of 2001) affect the structure of the FF Model? Sample: Jan 1973 - June 2020 (T = 569). Pooled RSS = 1.9324 Jan 1973 - Dec 2001 RSS = RSS 1 = 1.3307 (T = 342) Jan 2002 - June 2020 RSS = RSS 2 = 0.5791 (T = 4,565,.05 = 2.39, we cannot reject H 0. Conclusion: We do not find eviden ce that the 3-factor F-F mode l IBM excess returns suffered a structural break in January 2002. \u00b6 Under the H 0 (No structural change ), we can pool the data into one model. That is, the parameters are the same under both regi mes. We fit the sa me model for all i, for example: y i = 0 + 1xi + i If the Chow test rejects H 0, we need to reformulate the model. A typical reformulation includes a dummy variable ( DSB,i). For example: y i = 0 + 1xi + 2 DSB,i + 1xi DSB,i + i where D SB,i = 1 if observation i occurred after TSB = 0 otherwise. . Example: We are interested in the e ffect of the October 1973 oil s hock in GDP growth rates. We can include a dummy variable in the model, say D 73: D73,i = 1 if observation i occurred after October 1973 = 0 otherwise. y i = 0 + 1xi + 2 D73,i + 1xi D73,i + i In the model, the oil shock affect ed the constant and the slopes. - Constant: Before oil shock (D 73 = 0): 0 After oil shock (D 73 = 1): 0 + 2 - Slopes: Before oil shock (D 73 = 0): 1 After oil shock (D 73 = 1): 1+ 1 We can estimate the above model and do an F-test to test if H 0 (No structural change ): 2 = 0 & 1 = 0. Example: We introduce an Oct 1973 dummy in the AR(1) GDP growth rate model. T1 <- T - 0.117346 -3.007 0.00287 ** significant effect codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Conclusion: After the oil shock, the slope significantly changed from 0.467457 to 0.11456 (= 0.467457 + (-0.352897) ). \u00b6 Chow Test: Structural Change - Unknown Break The previous example, computes the Chow test assuming that we know exactly when the break occurred -say, October 73 or Dec 2001. That is, the results are conditional on the assumed breaking point. In general, breaking points are unkno wn, we need to estimate them. One quick approach is to do a rol ling Chow test -that is we run th e Chow test for all dates in the sample- and pick the date that maximizes the F-test. This test was proposed by Quandt (1958): )( max } ,..., {max min T T F QLR The max (supremum) is taken over all potential breaks in ( min, max). For example, min= T*.15; max = T*.85; that is we trim 30% of the observations ( 0 = 15% in each side) to run the test. The problem with this approach is that the technical conditions under which the asymptotic distribution is derived are not met in this sett ing (the F-test are correlated, they are not independent). Andrews (1993) showed that under appropriate conditions, the QLR statistic, also known as SupLR statistic, has a non-standard limiting distribution (\"non-standard\" = no existing table; needs a new one). Andrews (1993) tabulated the non-standard distri bution for different number of parameters in model ( k), trimming values ( 0), & significance level ( ). Andrews' table is in the next slide. For example, for k=4, 0= min/T=(1- max/T)=.15, & =.05, the critical value is . Critical values of the QLR test Distri bution, taken from Andrews (1993). Note: p = # of parameters ( k), 0 = trimming value. (Ignore .) Example: We search for breaking points for IBM return s in the 3-factor FF model. Below, we plot all starting at T*15: Maximum F is 3.83 occurs in May 1993 (observation #243). Using Andrews' tabulated critical value of 16.45 , we have = 3.83 < 16.45 cannot reject H 0 at 5% level. Conclusion: We do not find eviden ce that the 3-factor F-F mode l IBM excess returns suffered a structural break during the sample. \u00b6 Chow Test: Structural Change - Script in R Chow Test for different break ing points, starting at = X' y (OLS regression) e <- y - x%*%b # regression residuals, e RSS_R <- as.numeric(t(e)%*%e) # structural change) T1 <- round(T * 1/5) # Trim .20 of data t <- T1 # t will be the counter for loop. Starts at T1. T2 <- round(T * 4/5) F-tests while (t <= T2) { # St art +1 # is an index that start at 1 All_F[kt] <- F # add F-test to All_F the maximum F-test (QLR) Chow Test: Structural Change - Remarks The results are conditional on the breaking point -s ay, October 73 or Dec 2001. The breaking point is usually unkno wn. It needs to be estimated. It can deal only with one struct ural break -i.e., two categories! The number of breaks is also unknown. Characteristics of the data (heteroscedasticity -for example, regimes in the variance- and unit roots (high persistence) complicate the test. In general, only asymptotic (c onsistent) results are available. There are many modern tests that take care of these issues, but usually also with non-standard distributions. Forecasting and Prediction Objective: Forecast Distinction: Ex post vs. Ex ante forecasting - Ex post: RHS data are observed - Ex ante (true forecasting): RHS data must be forecasted Prediction and Forecast - Prediction: Explaining an outcome, wh ich could be a future outcome. - Forecast: A particular predicti on, focusing in a future outcome. Example: Prediction: Given x 0 predict 0. Forecast: Given x0t+1 predict t+1. \u00b6 Two types of predictions: - In sample (prediction): The expected value of y (in-sample), given the estimates of the parameters. In sample predic tion produces fitted values, . - Out of sample (forecasting): The value of a future y that is not observed by the sample. Notation: Let T be the forecast origin and l is the forecast horizon. - Prediction for T made at T: . - Forecast for T+l made at T: , |, . - : l-step ahead forecast = Forecasted value YT+l at time T. Any prediction or forecast needs an information set, I T. This includes data, models and/or assumptions available at time T. The predictions and forecasts will be conditional on I T. For example, in-sample, I T = {x0} to predict y0. Or in a time series context, I T = {x0T-1, x0T-2, ..., x0T-q} to predict yt+l. Then, the forecast is just th e conditional expectation of Y T+l, given the observed sample: |,, ..., Example : If , then, the one-step ahead forecast is : |,, ...,. \u00b6 The conditional expectation of Y T+l is, in general, based on a model, the experience of the forecaster or a combination of both. Example : We base the conditional expect ation on the 3 FF factor model: E[(0 + 1 (Mkt Ret T+l + 3 HML T+l)|IT ] Note: The forecast of Y T+l also needs a forecast for the driving variables in the model. We need a forecast for E[(Mkt Ret general, we will need a model for . Things can get compli cated very quickly. Keep in mind that the forecasts are a random va riable. Technically speaking, they can be fully characterized by a pdf. In general, it is difficult to get the pdf for the fo recast. In practice, we ge t a point estimate (the forecast) and a C.I. Q: What is a good forecast? We need metrics to evaluate the forecasting performance of different models. In general, the eval uation of forecasts relies on MSE. Later in this class, when we cover time series (Brooks Chapter 6), we go deeper into forecasting. Forecasting and Prediction: Variance-bias Trade-off We start with general model = f(X, ) + . expectation: E[ |X, x0] = f(x0, ); which we estimate with 0 = f(x0, ). The realization 0 is just: 0 = f(x0, ) + 0 With 0 observed, we compute the prediction error: 0 - 0 and its associated expected squared error, which can be written as: E[ 0 - 02 ] = E[0 + [Bias( 0 )]2 + Var[] We want to minimize this squared error. Note th at there is nothing a fore caster can do regarding the last term, called the irreducible error . All efforts are devoted to minimize the sum of a variance and a squared bias (the MSE). This creates the variance-bias trade-off in forecasting. It is possible that biased forecast can produce a lo wer MSE than an unbiased one. In this lecture, we based our forecasts on OLS estimates, which under the CLM assumptions, produce unbiased forecast. Note: The variance-bias trade-off is always present in forecasting. In general, more flexible models have less bias and more variance. Th e key is to pick an \"optimal\" mix of both. Forecasting and Prediction: Point Estimate Prediction: Given x0 predict 0. 0 = x0 + 0 Note: The predictor includes an estimate of 0: 0 = b'x0 + estimate of 0. (Estimate of 0=0, but with variance.) Associated with the prediction (a poin t estimate), there is a forecast error: 0 - 0 = bx0 - x0 - 0 = (b - )x0 - 0 and a variance: Var[( 0 - 0)|x0] = E[(0 - 0) 0)|x0] = x0 Var[( b - )|x0] x0 + 2 Example : We have already estimated the 3 Factor Fama-French Model for IBM returns: > summary( fit_ibm_ff3 ) = [1.0000 -0.0189 -0.0142 -0.0027] Then, -0.0142 - 0.171500 * (-0.0027) = = -0.01877582 Suppose we observe y 0 = 0.1555214 . Then, the forecast error is 0 - y0 = -0.01877582 - and Prediction: Confidence Intervals How do we estimate the uncertainty behind the fo recast? Form a confidence interval. Two cases: (1) If x 0 is given -i.e., constants. Then, Var[ 0 - y0|x0] = x0 Var[ b|x0] x0 + 2 Form confidence interval as usual. Note: In out-of-sample forecasting, x 0 is unknown, it has to be estimated. (2) If x0 has to be estimated, then we use a random vari able. What is the vari ance of the product? One possibility: Use bootstrapping. Assuming x 0 is known, the variance of the forecast error is 2 + x0' Var[ b|x0]x0 = 2 + 2[x0' (X'X)-1x0] If the model contains a constant term, this is Var 11 (where Z is X without x1=). In terms squares and cross product s of deviations from means. Note: Large 2, small N, and large deviations from the mean s, decrease the precision of the forecasting error. Interpretation: Forecast variance is smallest in the middle of our \"experience\" and increases as we move outside it. Then, the (1 )% C.I. is given by: [ 0 \u00b1 tT-k,/2 * sqrt(Var As x 0 moves away from its mean, the C.I in creases, this is known as the \" butterfly effect .\" Example (continuation): We want to calculate the variance of the forecast error: for thee given x0 = [1.0000 -0.0189 -0.0142 -0.0027] Recall we got 0 b'x0 Var[ error if x 0 = colMeans(x)? # (1-alpha)% C.I. for prediction prediction: [ -0.13356 ; 0.09601 ] with 95% confidence. A wide interval, which makes clear the uncertainty surrounding the point forecast: 0 = -0.01877587 . \u00b6 Forecasting and Prediction - Model Validation Model validation refers to establishing the statistical adequacy of the assumptions behind the model -i.e., ( A1)-(A5) in this lecture. Predictive power can be used to do model validation. In the context of prediction and forecasting, mo del validation is done by fitting a model in- sample, but keeping a small part of the sample, the hold-out-sample , to check the accuracy of OOS forecasts. Hold out sample: We estimate the model using only a part of the sample (say, up to time T 1). The rest of the observations, the hold out sample, (T - T 1 observations) are used to check the predictive power of the model -i.e., the accuracy of predictions, by comparing 0 with actual 0. Steps to measure forecast accuracy: Step 1 . Select a (long) part of the sample (say, first T 1 observations) to estimate the parameters of the model. (Get in-sample forecasts, .) We call this sample, the estimation period . Step 2 . Keep a (short) part of the sample (say, (T - T 1) observations) to check the model's forecasting skills, This is the validation step . (Get OSS 0, but y0 is known.) Since y0 is known calculate true MSE or MAE. For example: 1 Step 3 . If happy with Step 2 , proceed to do out-of-sample forecasts. Note: In the Machine Learning literature, the terminology used fo r model validation is slightly different. Step 1 is called \" training ,\" the data used (first T 1 observations) are called training data/set . In this step, we estimate the parameters of the m odel, subject to the assumptions, for example, (A1)-(A4). Step 2 has the same name, validation (or \"single-split\" validation) . This step can be used to \"tune (hyper-)parameters. \" In our CLM, we can \"tune\" the model for departures of ( A1)-(A4), for example, by including more or different variables ( A1) and re-estimating the model accordingly using \"training data\" alone. We choose the model with lower MSE or MAE. Remark: The idea of this step is to simulate out-of-sample accuracy. But, the \"tuned\" parameters selected in Step 2 are fed back to Step 1. Step 3 tests the true out-of-sample forecast accuracy of model selected by Step 1 & Step 2 . This last part of the sample is called \" testing sample .\" Forecasting and Prediction - Cross Validation Step 2 is used as a testing ground of the model before performing OOS forecasting. There are many ways to approach the validation step. Instead of a single split, split the data in parts. This is called -fold cross-validation . For = 1, 2, ..., , use all folds but fold to estimate model; use fold to check model's forecasting skills by computing MSE, . The -fold CV estimate is an average of each fold MSE's: Usual choices for are 5 & 10. This is an arbitrary choice. Random and non-random splits of data can be us ed. The non-random splits are used for some special cases, such as qualitative data, to make sure the splits are \"representative.\" Use a single observation for validation. This is called leave-one-out cross-validation (LOOCV), which is a special case of -fold cross-validation with = T. That is, use (T - 1) observations for estimation, and, the n, use the observation left out, = 1, ..., T, to compute , which is just , where is the prediction for observation based on the full sample but observation . Then, compute: Instead of just one, it is possible to leav e p observations for validation. This is called leave-p-out cross-validation (LpOCV). Remark: In time series, since the order of the da ta matters, cross validation is more complicated. In general, rolling windows are used. Example : We do cross-validation on the 5-Factor Fama-French Model for folds fold.size <- nrow(dats)/n.folds remain <- 1:nrow(dats) # all obs for (i in 1:n.folds){ select <- sample(remain, fold.size, replace = FALSE) #randomly sample fold_size from remaining obs) folds[[i]] <- select # store indices ( write a special stat ement for last fold if 'leftover points') if rema ining indices to reflect what was taken out remain } results <- matrix(0,1,n.folds) # Vector to accumulate accuracy measures (MSE) for (i in 1:n.folds){ # fold i indis <- estim #split in to estimation (train) test <- dats[indis, ] lm.model <- lm(y[-indis] ~ ., da with estimation data pred <- predict(lm.model, newdata = te st) # predicted values for fold not used MSE <- mean((y[indis] - pred)^2) # MSE (any other evaluation measure can be used) results[[i]]<- MSE # Accumulate MSE in vector } return(results) Measures of Accuracy Summary measures of out-of-s ample forecast accuracy, after forecasts: Mean Error = Mean Absolute Error (MAE) = | | || Mean Squared Error (MSE) = Root Mean Square Error (RMSE) = Theil's U-stat = Theil's U statistics has the interpretation of an R2. But, it is not restricted to be smaller than 1. The lower the above criteria, say MSE, the be tter the forecasting ability of our model. An OOS R 2 can be computed as: = 1 - with = = where is the forecasting horizon. (See Goyal a nd Welch (2008) for a well-known finance application.) Again, cross-validation measures can be us ed to evaluate forecasting performance. Example : We want to check the forecast accuracy of the 3 FF Factor Model for IBM returns. We estimate the model using only 1973 to 2017 data (T=539), leaving 2018-2020 ( Regression from T0 to T1 b1 <- fit2$coefficients # Extract OLS coefficients from regression > summary( We condition on the observed data (no model to predict FF factors used) from 2018: Jan to = 0.003703207 MAE = 0.04518326 of actual IBM returns and forecasts. plot(y_f0, type=\"l\", col=\"red\", main = = c(\"blue\" , \"red\"), lty = 1) From the plot, some forecasts are very good, some are very bad. \u00b6 Evaluation of forecasts: Testing Accuracy Above, we have competing forecasting models and we computed measures of accuracy for each model. So far we have implicitly judged the model with the best (usually, the lower) measure of accuracy as the best forecasting model. But, meas ures of accuracy are RV, thus, in order to say one model forecasts better than other, we need a test. Suppose two competing forecasting proce dures produce a vector of errors: & . We decide to use the expected MSE as the criterion to judge the forecasting accu racy of a model. We want to test H 0: MSE(1) = MSE(2) H 1: MSE(1) MSE(2). Assumptions: forecast errors are unbiased, normal , and uncorrelated. If forecasts are unbiased, then MSE = Variance. Consider, the pair of RVs: ( ) & ( ). Now, That is, we test H 0 by testing that the two RVs are not correlated! Under H 0, 0. This idea is due to Morgan, Granger and Newbold (MGN, 1977). There is a simpler way to do the MGN test. Let, = = (1) Do a regression: = + (2) Test H 0: = 0 a simple t-test . The MGN test statistic is exactly the same as that for testing the null hypothesis that = 0 in this regression (recall: b = (XX) -1Xy). This is the approach take n by Harvey, Leybourne and Newbold (1997). If the assumptions are violate d, these tests have problems. A non-parametric HLN variation: Spearman' s rank test for zero correlation between x t and zt. Example : We produce IBM returns one-step-ahead forecasts for 2018-2020 using the 3 FF Factor Model for IBM returns: (IBM Ret - t = 0 + 1 (Mkt Ret - )t + 2 SMB t + 3 HML t + t Taking expectations at time t+1, conditioning on time t information set, I t ={(Mkt Ret - rf)t, SMB t, HML t} E[(IBM Ret - )t+1|It ] = 0 + 1 E[(Mkt Ret - )t+1|It ] + 2 E[SMB t+1|It ] + 3 E[HML t+1|It ] In order to produce forecast, we will make a naive assumption: The best forecast for the FF factors is the previous observation. Then, E[(IBM Ret - )t+1|It ] = 0 + 1 (Mkt Ret - )t + 2 SMB t + 3 HML t. Now, replacing the by the estimated b, we have our one-step-ahead forecasts. We produce one forecast at a time. We compare the forecast accuracy relative to a ra ndom walk model for IBM returns. That is, E[(IBM Ret - )t+1|It ] = (IBM Ret - )t Using R, we create the forecasting errors for both models and MSE: x_01 <- x[T1:(T-1),] # By a ssumption T1. is higher MSE. [1] 0.02031009 Now, we create = , & = . Then, regress: = + and test th at both MSE are equal MSE of RW is higher. \u00b6 Evaluation of forecasts: MSE/MAE? MSE and MAE are very popular criteria to judge the forecasting power of a model. However, it may not be the best measure for everybody. Richard Levich's textbook compares forecasting serv ices to the freely available forward rate. He finds that forecasting services may have some ability to predict dire ction (appreciation or depreciation). For some investors, the direction is what real ly matters, since direction determines potential profits, not the error. Example : Two forecasts: Forward Rate (F t,T) and Forecasting Service (FS) St = .7330 USD/CAD (Today's market spot rate.) rate.) EFS,t [St+1-month forecasts an appreciation of CAD. Investor's strategy: Buy CAD forward if FS for ecasts CAD appreciation, greater than the implied by the forward rate. Based on the FS forecast, Ms. Sterni n decides to buy CAD forward at F t,1-month . (A) Suppose that the CAD appreciates to St+1 .7390 USD/CAD .7390 - = USD/CAD. Investor makes a profit of .7390 - .7335 = USD .055 USD. (B) Suppose that the CAD depreciates to St+1 = .7315 .7342 = .0027 USD/CAD. smaller MAE! Investor takes .7315 - .7335 = USD -.0020 . Conclusion: A small forecast error is not that releva nt for investor, the direction of the error matter much more. \u00b6 Forecasting Application: Fundamental Approach There are two pure approaches to forecasting. Base d on how we select the \"driving\" variables X t, we have: - Fundamental (based on da ta considered fundamental) - Technical analysis (based on data that incorporates only past prices) Fundamental Approach to Forecast Exchange Rates, S t (USD/JPY) Based on an economic model, we generate E t[St+T] = E t[f(X t+T)] = g(X t), where X t is a dataset regarded as fundamental economic variables: - GNP growth rate, - Current Account, - Interest rates, - Inflation rates, etc. The economic model usually incorporates: - Statistical characteristics of da ta (seasonality, autocorrelation, etc.) - Experience of the forecaster (wha t information to use, lags, etc.) Mixture of art and science. The economic model provides the struct ure for the forecasts (also called structural model). The economic model is the starting point of the fundamental approach. Once we selected the economic model, we proceed to estimate the parameter on the model. We need to collect data and decide on how to estimate the model (OLS, MLE, etc.). Then, we test the model. We have to make sure that we have a good model. If the model survives the tests, then we use the model to forecast. We compare the economic model' s performance with the performa nce of a simpler model, the Random Walk model (RWM), which is found to be very good model for S t in the short-run. The forecasts for the RWM are given by: E t[St+1] = S t Steps Fundamental Forecasting: Steps (example: S t = USD/JPY) (1) Select a Model: Based on Theory (IFE, & Asset Approach) we model percentage changes in FX rates, e t: e t = 0 + 1 (iUS,t - i JAP,t) + 2 (yUS,t - y JAP,t) + 3 (m US,t - m JAP,t) + t E t[et+1] = 0 + 1 Et[iUS,t+1 - i JAP,t+1 ] + 2 E[y US,t+1 - y JAP,t+1 ] + 3 E[m US,t+1 - m JAP,t+1 ] Et[St+1] = SFt+1= S t * (1 + E t[et+1]) (2) Collect data: S t, Xt (Interest rates (i), GDP growth ra tes (y) and money growth (m) data needed.) (3) Estimation of Model (using estimation period ): OLS get . Model Data Estimation Forecast Evaluation Modify/Change Model Test Model Theory Pass? Practice Continue (4) Generate forecasts. Assumptions about X t are needed. E t[Xt+1 ] = 1 + 2 (Xt) -an AR(1) model. Et[et+1] = E t[Xt+1 ]' Et[St+1] = S t * (1 + Et[et+1]) (5) Evaluation of Forecasts: MSE (& compare with RW's MSE). Model's Forecast Error t+1 = Et[St+1] - St+1 Forecast Error t+1 = St - S t+1 Example: (1) & (2) Based on above model, I collect quart erly data (FX_USA_JAP.csv) from 1978:II - 2020:II. I read the data and transform it to estimate model: # Step (2) - US) data from FX_da us_i <- FX_da$US_I3M # Extract US 3-mo Interest rate (i US) data us_y <- FX_da$US_GDP_g # Extract US GDP growth (y US) data us_tb <- FX_da$US_CA_c # Extract US Current account change (tb US) data jp_I <- FX_da$JAP_INF # Extract Japan Inflation (I US) data jp_mg <- FX_da$JAP_MI_c # Extract Japan Money growth (m JP) data jp_i <- FX_da$JAP_I3M # Read Japan 3-mo Interest rate (i JP) data jp_y <- FX_da$JAP_GDP_g # Extract Japan GDP growth y JP) data jp_tb <- FX_da$JAP_CA_c # Extrac t Japan Current account change (tb JP) data e_f <- FX_da$JPY.USD_c # Extract changes in JPY/USD (e) # Step (2) - Transform variables (create differentials) inf_dif <- us_I - jp_I # int_dif <- us_i mg_dif <- us_mg - jp_mg # differential (mg_dif) y_dif <- us_y - jp_y # differential (y_dif) tb_dif <- us_tb - jp_tb for estimation period . e_f1 <- e_f[1:T_est] # Adjust sample size to <- xx[1:T_est,] # Adjust sample size to T_est # Step (3) - Estimation of model(using only estimation period (T=161): Get b. ' ' 1 Residual standard error: 6.293 on 157 degrees of freedom Multiple R-squared: 0.04673, DF, p-value: 0.05661 # Step (4) - Generate Forecasts. Need first to estimate model for X variables. (using estimation period data only) AR(1) for (i US,t - i JAP,t) <- int_dif[2:T_est] # Adjust sample fit_int <- lm(int_dif_lag0 ~ int_dif_ ' ' 1 Residual standard error: 1.045 on 158 degrees of freedom Multiple R-squared: 0.7732, 0.7718 fit_mg <- lm(mg_dif_lag0 ~ mg_dif_lag1) # 0.05 '.' 0.1 ' ' 1 Residual standard error: 2.74 on 158 degrees of freedom Multiple R-squared: 0.08766, Adjusted R-squared: y_dif[2:T_est] # Adjust above) fit_y <- lm(y_dif_lag0 ~ # '.' 0.1 ' ' 1 Residual standard error: 1.08 on 158 degrees of freedom Multiple R-squared: 8.263e-05, Adjusted R-squared: p-value: 0.9092 Now, we can do one-step-ahead forecast for X variables: T_val of Validation fit_int $coeff <- <- US,t - y JAP,t) Finally, we compute the one-step-ahead e: e_f_RW_0 <- rep(0,T-T_val+1) # RW t+T!) f_e_RW mse_e_RW [1] 3.381597 Lower MSE than Model. Not good for Model. Compare MSEs: The RW model ha s a better MSE (usual finding). A MGN test is usually done. But, we have only m=8 observations, we can do the test, but the results are very likely no t to be taken seriously. # Step (5) - Evaluation of Forecasts sample ). Residual standard error: 3.026 on 6 degrees of freedom very small df to make inferences. Multiple R-squared: 0.05322, Adjusted DF, p-value: 0.5826 Suppose you are happy with the M odel, you believe the difference in MSEs is not significant), now you generate out-of-sample forecasts. # Step (6) - Out-of-sample one-step-ahead forward forecast for S t: E t=2020:II [St+1=2020:III ] = S t=2020:II (1 + E t=2020:II [et+1=2020:II ]) We observe S t today (2020:II): S 2020:II = 100.77 JPY/USD, which we in vert since we work with direct quotes: S 2020:II = 0.009279 USD/JPY. We need to forecast the independent variables, based on AR(1) results, X t ={(i US,t - i JAP,t), (y US,t - y JAP,t), (m US,t - m JAP,t)} Forecasting (i US,t+1 - i JAP,t+1 [,1] [1,] 1.984401 2% depreciation S_p1 <- S*(1+e_f_p1/100) e_f_p1 /100) # e is in %, we divi de by 100 to put it decimal from > S_p1 # Print forecast for S t=2020:III [,1] [1,] 0.009463133 Model's forecast for S = E t=2020:II [St+1=2020:III ] = 0.009463133 USD/JPY . t=2020:II [St+1=2020:III = 105.6732 JPY/USD ). We can use the one-step-ahead forecasts to generate two-step-ahead forecasts. That is, we forecast E t=2020:II [St+1=2020:IV ] (=S_p2 below) <- cbind(1,mg_dif_p1)%*% fit_mg $coeff <- cbind(1,y_dif_p1)%*% fit_y $coeff 1.514363 1.11% can use the two-step-ahead forecast to generate three-step-ahead forecasts. Obviously, we can continue this process to generate l-step-ahead forecasts for S t (a simple do loop will do it). Eventually, we will collect m of out-of-sample forecasts ( m one-step-ahead forecasts, m two-step- ahead forecasts , m three-step-ahead forecasts, etc.) to ge t an MSE and run a MGN/HLN test on them. \u00b6 It is possible that one model is the best in the sh ort-term (say, up to 3 steps ahead); other is better in the medium-term (say, from 4 to 6 steps ahead ); and another is best for longer-term. For example, the RW model is very good (\" unbeatable \") up to 3 months ahea d. Then, other models start to produce better forecasts, especially after 6 months. Forecasting Application: Fundamental Approach Practical Issues in Fundamental Forecasting - Are we using the \"right model?\" - Estimation of the model (OLS, MLE, other methods). - Some explanatory variables (X t+T) are contemporaneous. We also need a model to forecast the X t+T variables. Does Forecasting Work? For many financial assets (stock prices, exchange ra tes), we expect forecas ting to be difficult. The Efficient Markets Hypothesis posits that financia l asset returns closely follows a \" Random Walk \" process, therefore forecasting asset return s is fruitless. Burton Malkiel in his book \" A Random Walk Down Wall Street ,\" first published in 1973, popularized this point. Example : For exchange rates, in the short-run, the Random Walk consistently models beat structural (and other) models, like PPP, IFE, Mone tary Approach: Lower MSE, MAE. That is bad news for the beaten models, since the RW forecast us es today's price to fore cast any future price. No model or estimation is needed. Note: Many argue that the structural models used to forecast exchange rates are not the \"right model.\" \u00b6 Model Selection Strategies Specifying the DGP in ( A1) is the most important step in ap plied work. We have assumed \"correct specification,\" which, in practice, is an unrealistic assu mption, since we do not really observed the true DGP. A bad model can create a lot of problems: bias es, wrong inferences, bad forecasts, etc. So far, we have implicitly used a simple strategy: (1) We started with a DGP, which we assumed to be true. (2) We tested some H 0 (from economic theory). (3) We used the model (restricted, if needed) for prediction & forecasting. Question: How do we propose and select a model (a DGP)? Potentially, we have a huge numb er of possible models. We can have models with different functional form: f(.), g(.), , or h(.), and/or different e xplanatory variables: X, Z, W and dummy variables, D. For example, we may have four diffe rent formulations to choose from: Model 1 Y = X + Model 2 Y = Z + Model 3 Y = (W) + Model 4 Y = exp(Z D ) + We want to select the best model, the one th at is closest to the true and unobserved DGP. In practice, we aim for a \"good\" model, a model that passes a barrage of sp ecification tests and has good forecasting power. Model Selection Strategies: Views A model is a simplification. There are many approaches to specify a model: - \"Pre-eminence of theory.\" Econom ic theory should drive a model. Data is only used to quantify theory. Econometric methods offer sophisticated ways 'to bring data into lin e' with a particular theory. - Purely data driven models. Success of ARIMA m odels (late 60s - early 70 s), discussed in Lecture 6: No theory, only exploiting the time-series ch aracteristics of the data to build models. - Modern (LSE) view. A compromise: theory and th e characteristics of the data are used to build a model. Theory and practice play a role in derivi ng a good model. David Hendry (2009) emphasizes: \"This implication is not a tract for mindless modeli ng of data in the absence of economic analysis, but instead suggests formulating more general in itial models that embed the available economic theory as a special case, consistent with our know ledge of the institutional framework, historical record, and the data properties.\" \"Applied econometrics cannot be co nducted without an economic th eoretical framework to guide its endeavours and help interpre t its findings. Nevertheless, since economic theory is not complete, correct, and immutable, and never will be, one also cannot justify an insistence on deriving empirical models from theory alone.\" Model Selection Strategies: A Good Model According to David Hendry, a good model should be: - Data admissible -i.e., modeled and observed y should have the same properties. - Theory consistent -our model should \"make sense\" - Predictive valid -w e should expect out-of-sample validation - Data coherent -all information should be in the model. Nothing left in the errors ( white noise errors ). - Encompassing -our model should explain earlier models. That is, we are searching for a statistical model that can generate the observed data ( y, X), this is usually referred as statistical adequacy , makes theoretical sense and can explain other findings. Model Selection Strategies: FAQ FAQ in practice: - Should I include all the variables in the da tabase in my model? - How many explanatory variab les do I need in my model? - How many models do I need to estimate? - What functional form should I be using? - Should the model allow for structural breaks? - Should I include dummies & interactive dummies? - Which regression model will work best and how do I arrive at it? Model Selection Strategies: Important Concepts Diagnostic testing: We test assumptions behind the model. In our case, assumptions ( A1)-(A5) in the CLM. Example : Test E[|X] = 0 -i.e., the residuals are zero -mean, uncorrelated with anything (that is, white noise dist ributed errors). In selecting a model, this is a ve ry important step. We run a lot of test to check the residuals are acceptable or the model is not misspecified: Ramsey 's reset test, tests fo r autocorrelation, etc. Parameter testing: We test economic H 0's. Example : Test k = 0 -say, there is no size effect on the expected return equation. Model Selection Strategies: Two Methods There are several model-selection methods . We will consider two: - Specific to General - General to Specific - Specific to General. Start with a small \"restric ted model,\" do some testi ng and make model bigger model in the direction indicated by the tests (for example, add variable x k when test reject H 0: k=0). - General to Specific. Start with a big \"general unrestricted model,\" do some testing and reduce model in the direction indi cated by the tests (for example, eliminate variable x k when test cannot reject H 0: k=0). Model Selection Strategie s: Specific to General Steps: (1) Begin with a small theoretical model - for example, the CAPM y = X + . (2) Estimate the model - say, using OLS. (3) Do some diagnostic testing - are re siduals white noise (uncorrelated)? If the assumptions do not hold, then use: - More advanced econometrics - GLS instead of OLS? - A more general model - More regressors? Lags? (4) Test economic H 0 on the parameters - Is SMB and HML significant? (5) Modify model in (1) in the di rection of rejections of H 0. This strategy is known as specific to general . In the machine learning literature, this strategy is also called forwards selection . Example: Specific-to-general strategy to model IBM returns: (1) We start with the 3-factor FF model for IBM: (IBM Ret - rf)t = 0 + 1 (Mkt Ret - rf)t + 2 SMB t + 3 HML t + t (2) Estimate 3-factor Multiple R-squared: 0.3393, 0.3358 (3) Diagnostic tests: Check t-values & R 2, F-test goodness of fit, etc. (4) LM Test to test if there is a January Effect (H 0: No January effect): > LM_test [1] 9.084247 LM_test > 3.84 Reject H 0. (5) Given this result, we modify the 3-factor FF and add the January Dummy can continue our search to see if an expansion of the specific model is needed. For example, we could have tested for a 2008 Fi nancial crisis dummy or Dot.com dummy. \u00b6 Some remarks based on the previous example: The specific-to-general method makes assumptions along the way. (1) Very likely the starting model is based on th eory and experience (HML is not significant at the usual 5% level). Not clear how to pro ceed from there to a more general model. (2) We tested for a January effect and then adde d to the model. However, we could have tested for a Dot.com effect or for an interactive Dot.com/ January effect with the 3 FF factors. Not clear when to stop the search. (3) Selection step uses a p-value to add variables to the model. In this case, we use the standard 5% for the tests. Model Selection Strategies: Specifi c to General - Stepwise Regression Note that in the previous example, we started with a model. What happens if we are skeptical regarding models? A popular implementation of the specifi c-to-general model selection is the stepwise regression , where we start with only a set of potential expl anatory variables and le t the data determine, starting from all potential one-variable models, wh ich variables to add. Overall structure of Stepwise Regression: - The method begins with a potential regressors. - Do one-variable regressions. Pick the one that shows the biggest t-stat or maximizes a goodness of fit measure, say, Adjusted-R 2, 2. Suppose is selected. - Then, do 1-variable regressions all with . Select the regressor (in addition to ) that has the highest t-stat or that maximizes 2. - Continue. But, when we start adding regresso rs, we usually check if the added regressor(s) change the significance of previous steps. (Note: at each step, we remove or add a regressor(s) based on t- or F-tests.) - Stop: Additional regressors do not have significant t-stats/increase 2. Decisions: We nned to select the initial variables, the level for tests ( = 5%, 10%, 30%?) and/or the goodness of fit statistic. Remark: Always keep in mind that the selected (fina l) model is not necessari ly better than others. Type I and Type II errors are likely to occur, thus the fina l model may have irrelevant and/or omitted variables. Example: Stepwise regression strategy to model IBM re turns. We start with the 5 FF factors as candidates for IBM. We use the function ols_step_forward_p in the olsrr package, which uses p- values to select variables. (You can also use other criteria to select the model, for example, ols_step_forward_aic uses aic to select variables.) The final output is long (details = TRUE), below we present the last two Entered method selects Market excess returns, SMB & RMW as the drivers of IBM excess returns. If we change the p-value to 0.1, RMW will be drop from final model. \u00b6 Technical Note: In general, the se lection of variables based on p- values is not advised, since the distribution of the OLS coefficien ts is affected. We mentioned th is above, when we discussed pre-testing. Model Selection Strategie s: General to Specific Begin with a general unrestricted model (GUM), which nests restrict ed models and, thus, allows any restrictions to be tested. Say: y = X + Z + W + (X * W) + (Z * D) + . Then, reduction of the GUM starts. Mainly using t-tests , and F-tests , we move from the GUM to a smaller, more parsimonious, specific model. If competing models are selected, encompassing tests or information criteria (AIC, BIC) can be used to select a final model. This is the discovery stage. After this reduction, we keep a final (restricted GUM) model: y = X + . Creativity is needed for the specification of a GU M. Theory and empirical evidence play a role in designing a GUM. Steps: Step 1 - First ensure that the GUM does not su ffer from any diagnostic problems. Check residuals in the GUM to ensure that they posse ss acceptable properties. (For example, test for white noise in residuals, incorrect f unctional form, autocorrelation, etc.). Step 2 - Test the restrictions implied by the specific model against the general model - either by exclusion tests or other te sts of linear restrictions. Step 3 - If the restricted model is ac cepted, test its residuals to ensure that this more specific model is still acceptable on diagnostic grounds. This strategy is called general to specifics (\"gets\"), LSE, TTT (Test, test, test). It was pioneered by Sargan (1964). The properties of gets are discussed in Hendy and Krolzig (2005, Economic Journal). The role of diagnostic testing is two-fold. - In the discovery steps (Steps 1 & 2), the tests are being used as design criteria. Testing plays the role of checking that the original GUM was a good starting point after the GUM has been simplified. - In the context of model evalua tion (Step 3), the role of testin g is clear cut. Suppose you use the model to produce forecasts. These forecasts can be evaluated with a test. This is the critical evaluation of the model. Example: General-to-specific strategy to model IBM returns: Step 1 - Start with a GUM: the 3-factor FF model for IBM + January Dummy + Dot.com Dummy + non-linear & in teractive effects: (IBM Ret - rf)t = 0 + 1 (Mkt Ret - rf)t + 2 SMB t + 3 HML t + 4 January t + 5 (Mkt Ret - rf)t2 + 6 SMB t2 + 7 HML t2 + 8 (Mkt Ret - rf)t*SMB t + 9 (Mkt Ret - rf)t*HML t + + 10 Dot.com t + 11 (Mkt Ret - rf)t * January t + 12 HML t * January t + 13 (Mkt Ret - rf)t* Dot.com t + 14 HML t * Dot.com + 15 SMB t * Dot.com + t Estimate GUM: t_sb <- 342 # Structural break date <- matrix(1, ' ' 1 Residual standard error: 0.05788 on 553 degrees of freedom Multiple R-squared: 0.3663, 0.3491 p-value: < 2.2e-16 Step 1 - Check GUM residuals for departures of ( A2)-(A3). A Ramsey's reset test can be done (using the resettest in the lmtest library). > resettest( 2, df2 = 552, p-value = 0.2832 Step 2 - Reduce Model with t-test and F-tests. Say, we keep all the variables with a p-value close to 10% (we still keep HML, using previous experience). We estimate a restricted GUM: ' ' 1 Residual standard error: 0.05761 on 562 degrees of freedom Multiple R-squared: 0.3618, 0.355 DF, p-value: < 2.2e-16 Step 2 - Test the restrictions implied by the specific model against the general model. Using an F-test, we test J=9 restrictions: H 0: 5= 6 = 8 = 9 = 10 = 11 = 12 = 14= 15. [1,] 0.919105 p-value is very high. No evidence for H 0. Step 2 - Further specification checks of Restricted GUM, for example, perform a Ramsey's reset test (using the resettest in the lmtest resettest( 561, p-value = 0.3218 Step 3 - Test if Restricted GUM residuals are accep table -i.e., do diagnostic tests (mainly, make sure they are white noise). If Restricted GUM passes all the diagnostic tests, it becomes the \"final model.\" Note: With the final model, we use it to just ify/explain financial theo ry and features, and do forecasting. \u00b6 Some remarks based on the previous example: The general-to-specific method makes assumptions along the way. (1) Select a p-value for the tests of si gnificance in the discovery stage (we use 10% ). Given that we performed 15 t-tests , we should not be surprised we re jected the GUM, since we had an overall significance, * = .79 [= 1 - (1 - .10)^15]. Mass significance is an issue. (2) Judgement calls are also made. (3) The reduction of the GUM involves \"pre-testi ng\" -i.e., data mining. We are likely rejecting a true H 0 (false positives) and not rejecting a true H 1, (false negatives) along the way. This increases the probability that the final model is not a good approximation. It is common to ignore (or not even acknowledge) pre-testing issues. Note: Similar to stepwise regression, we can remove use p-values to remove one step at a time variables from the GUM. R can do this using the function ols_step_backward_p in the olsrr package. Model Selection Strategies: Best Subset Begin with a big model, with regressors: y = X + . The idea is to select the \"best\" subset of the regressors in X, where \"best\" is defined by the researcher, say MSE, Adjusted-R2, etc. In theory, it requires 2regressions. It can take a while if is big ( < 40 is no problem). There are many tricks are used to re duce the number of regressions. In practice, we use best subset to reduce the number of models to consider. For example, from the regressions with one-variable, keep the best one-variable model, from the regression with two-variables, keep the best two-variable model, etc. Example : We want to select a model for IBM excess returns, using the =3 Fama-French factors: Market excess returns (Mkt _RF), SMB, & HML. have 8 1) 2) HML (the 3-factor F-F Model). We select the model with the lower MSE. Or, we can carry two or three models of the best models to do cross-validation and, then, pick the best model. \u00b6 Model Selection Strategies: Properties A modeling strategy is consistent if its probability of finding the true model tends to 1 as T -the sample size- increases. Properties for strategies (1) Specific to General - It is not consistent if the original model is incorrect. - It need not be predictive va lid, data coherent, & encompassing. - No clear stopping point for an unordered search. (2) General to Specific - It is consistent under some ci rcumstances. But, it needs a large T. - It uses data mining, which can lead to incorrect models for small T. - The significance levels are incorrect. This is the problem of mass significance . Lecture 7 - Departures from CL M Assumptions & the Generalized Regression Model Review of CLM Results Recall the CLM Assumptions (A1) DGP: y = X + is correctly specified. (A2) E[|X] = 0 (A3) Var[|X] = 2 IT (A4) X has full column rank -rank( X) = k-, where T k. OLS estimation: b = (XX)-1X y Var[ b|X] = 2 (XX)-1 (consistency, efficiency, invariance, etc). ( A5) gives us finite sample tests: t-test , F-test , Wald tests). CLM: Departures from the Assumptions So far, we have discussed some violations of CLM Assumptions: (1) (A1) - OLS can easily deal with so me non-linearities in the DGP. as long as we have intrinsic linearity, b keeps its nice properties. - Wald, F, & LM tests to check for misspecification (2) (A4) - Perfect multicollinearity means the model n eeds to be changed. Multicollinearity is a potential problem. In general, e xogenous to the researcher. We need to be aware of this problem. In this lecture, we examine assumptions ( A2), (A3) and ( A5). That is, we check (i) X is stochastic. That is , it has a distribution. (ii) Var[ |X] 2 IT. (iii) |X is not N( 0, 2IT). CLM: Departures from (A2) The traditional derivation of the CLM assumes X as non-stochastic. In our derivation, however, we allowed X to be stochastic, but we conditioned on obs erving its realizations (an elegant trick, but not very realistic). With stochastic X we need additional assumptions to ge t unbiasedness and consistency for the OLS b. - We need independence between X & : {x i, i} i=1, 2, ...., T is a sequence of independent observations. - We require that X have finite means and variances. Similar requirement for , but we also require E[] = 0. Then, E[b] = + E[( XX) -1X ] = + E[( XX)-1X] E[] = Technical Note: To get consistency (& asymptotic normality) for b, we need an additional (asymptotic) assumption regarding X: XX/T Q (Q a or plim ( Q Question: Why do we need this assump tion in terms of a ratio divided by T? Each element of XX matrix is a sum of T numbers. ... ... ... ... ... ... ... As T , these sums will become large. We divide by T so that the sums will not be too large. Note: This assumption is not a difficult one to make since the LLN s uggests that the each component of XX/T goes to the mean values of XX. We require that thes e values are finite. - Implicitly, we assume that there is not too much dependence in X. CLM: Departures from (A2) - Endogeneity If there is dependence between X & , OLS b is no longer unbiased or consistent. Easy to see the biased result: we cannot longer separate E[( XX)-1X ] into a product of two expectations: E[(XX)-1X] E[] Then, E[b] = + E[( XX)-1X ] Dependence between X & occurs when X is also an endogenous variable , like y. This is common, especially in Corporate Finance. For example, we study CEO compensation as function of size of firm, and Board compos ition. Board Composition an d size of firm are endogenous -i.e., determined by the fi rm, dependent on CEO's decisions. Inconsistency is a fatal flaw in an estimator. In these situations, we use different estimation methods. The most popular is Instrumental Variable (IV) estimation . CLM: Departures from (A2) - Asymptotics Now, we have a new set of assumptions in the CLM: (A1) DGP: y = X + . (A2') X stochastic, but E[ X ] = 0 and E[ ] = 0. (A3) Var[|X] = 2 IT (p.d. matrix with finite elements, rank= k) With these new assumptions and using propertie s of plims and the CLT, we can show the following asymptotic results: 1. b and s 2 are consistent. 2. T (b - ) N(0, 2Q-1) b N(, 2J CLM: Departures from (A5) Notice that asymptotic results 2 and 3 state the asymptotic distribution of b and the t-, F- and Wald test. All derived from the new set of assumptions and the CLT. ( A5) was not used. That is, we relax ( A5), but, now, we require large samples (T ). Note: In practice, we use the asymptotic distribut ion as an approximation to the finite sample - i.e., for any T- distribution. This is why we used the notation in: b N(, (2/T)Q-1) We should be aware that this approximati on may not be accurate in many situations. Two observations regarding relaxing ( A5) |X ~ i.i.d. N(0, 2IT): - Throwing away the normality for |X is not bad. In many econometric situations, normality is not a realistic assumption (daily, weekly, or monthly stock returns do not follow a normal). - Removing the i.i.d. assumption for |X is also not bad. In many econometric situations, identical distributions are not realistic, si nce different means and variances are common. Questions: - Do we need to throw away normality for |X? Not necessarily. We can test for normality on th e residuals using a Jarque-Bera test, though, for financial assets we usually reject normality is rejected, especially at the monthly, weekly, daily, and intra-daily frequencies. - Why are we interested in large sample propert ies, like consistency, when in practice we have finite samples? As a first approximation, the answer is that if we can show that an estimator has good large sample properties, then we may be optimistic about its finite sample properties. For example, if an estimator is inconsistent, we know that for finite samples it will definitely be biased. CLM: Departures from (A3) Now, we relax ( A3). The CLM assumes that errors are uncorrelated and all are drawn from a distribution with the same variance, 2. (A3) Var[|X] = 2IT Instead, we will assume: (A3') Var[|X] = (sometimes written= 2, where IT) = Two Leading Cases: - Pure heteroscedasticity: We model only the diagonal elements. - Pure autocorrelation: We model only the off-diagonal elements. CLM: Departures from (A 3) - Heteroscedasticity Pure heteroscedasticity: E[ ij|X] = ij = if i=j = 0 i f i j Var[i|X] = = 0 0 0 0 00 This type of variance-covariance structure is common in time series, where we observe the variance of the errors changing over time or subject to different regimes (say, bear and bull regimes). Relative to pure heteroscedasticity, LS gives each observation a weight of 1 /T. But, if the variances are not equal, then some observations (low variance ones) are more informative than others. CLM: Departures from (A3) - Cross-correlation Pure cross/auto-correlation: E[ ij|X] = ij if i j = 2 if i=j = This type of variance-covariance structure is co mmon in cross sections, where errors can show strong correlations, for example, when we model re turns, the errors of two firms in the same industry can be subject to common (industry) sh ocks. Also common in time series, where we observe clustering of shocks over time. Relative to pure cross/auto-correlation, LS is based on simple sums, so the information that one observation (today's) might provide about another (tom orrow's) is never used. Note: Heteroscedasticity and autocorrelation are different problems and generally occur with different types of data. But, the implications for OLS are the same. CLM: Departures from (A3) - Implications OLS b is still unbiased and consistent. (Proofs do not rely on ( A3). OLS b still follows an asymptotic normal distribution . It is - Easy to show this result for the pure hetero scedasticity case using a version of the CLT that assumes only independence ; - More complicated derivation -i.e., with ne w assumptions- for the auto-correlation case. But, OLS b is no longer BLUE. There are more efficient estimators; estimators that take into account the heteroscedas ticity in the data. Note: We used ( A3) to derive our test statis tics. A revision is needed! Finding Heteroscedasticity There are several theoretical reasons why the \u00a4 2 may be related to some variables z i and/or z i2: 1. Following the error-learning models , as people learn, their errors of behavior become smaller over time. Then, is expected to decrease. 2. As data collecting techniques improve, is likely to decrease. Companies with sophisticated data processing techniqu es are likely to commit fewer errors in forecasting customer's orders. 3. As incomes grow, people have more discretionary income and, thus, more choice about how to spend their income. Hence, is likely to increase with income. 4. Similarly, companies with larger profits are expected to show greate r variability in their dividend/buyback policies than companies with lower profits. Heteroscedasticity can also be the result of outliers (either very small or very large). The inclusion/exclusion of an outlier, especially if T is small, can affect the results of regressions. Violations of ( A1) -model is correctly specified -, can produce heteroscedasticity, due to omitted variables from the model or incorrect fu nctional form (e.g., linear vs log-linear models). Skewness in the distribution of one or more re gressors included in the model can induce heteroscedasticity. Exampl es are economic variables such as income, wealth, and education. Heteroscedasticity is usually modeled us ing one the following specifications: - H1 : is a function of past t2 and past (ARCH models). - H2 : increases monotonically with one monotonically with E(yt). - H4 : is the same within p subsets of the data but differs across the subsets ( grouped heteroscedasticity ). This specification allows for structural breaks. These are the usual alte rnatives hypothesis (H 1) in the heteroscedasticity tests. Visual test In a plot of residuals against dependent variable or other variable will often produce a fan shape. Testing for Heteroscedasticity Question: Why do we want to te st for heteroscedasticity if b is unbiased? OLS is no longer efficient. There is an estimator with lower asymptotic variance (the x2,..., x k) = E(2) = 2 H1 and the structure of the test depend on what we consider the drivers of \u00a4 2 - i.e., in the previous examples: H 1, H2, H3, H4, etc. The key is whether E[ 2] = is related to x and/or x i2. Suppose we suspect a particular independent variable, say Xj, is driving . Then, a simple test: Check the RSS for large values of Xj, and the RSS for small values of Xj. This is the Goldfeld-Quandt (GQ) test. Testing for Heteroscedasticity: GQ Test GQ tests H 0: = s2 H 1: = f(Xj) Easy to compute: - Step 1 . Arrange the data from small to large values of the independent va riable suspected of causing heteroscedasticity, X j. - Step 2 . Run two separate regression s, one for small values of Xj and one for large values of Xj, omitting d middle observations ( 20%). Get the RSS for each regression: RSS 1 for small values of Xj and RSS 2 for large Xj's. - Step 3 . Calculate the F ratio GQ = RSS 2/RSS 1, ~ F df,df - 2( k+1)]/2 ( A5 holds). 020406080100120140160180200 0 20 40 60 80 100 120Series1 If (A5) does not hold, the F df,df distribution becomes an approximation and other tests may be preferred. Note: When we suspect more than one variable is driving 2i, the GQ test is not very useful. But, the GQ test is a popular test for st ructural breaks (two regimes) in variance. Fo r these tests, we rewrite step 3 to allow for a different sample size in the sub-samples 1 and 2, since the breaking point does not have to be in the middle of the sample. - Step 3 . Calculate the F-test test using function gqtest . It splits the sample in the middle. You need to specify the d of middle observations not incl uded in test. Recall, you need to install the package before usi ng it: install.packages(\"lmtest\"). Example: We test if the 3-factor FF model for IB M and GE returns show s heteroscedasticity with a GQ test, using gqtest in package lmtest. IBM returns library(lmtest) > gqtest(ibm_x ~ Mkt_RF + = p-value = 0.2371 cannot reject H 0 at 5% level. alternative hypothesis: variance increases from segment 1 to 2 GE returns gqtest(ge_x ~ Mkt_RF + SMB + HML, fraction p-value < reject H 0 at 5% level. alternative hypothesis: variance in creases from segment 1 to 2. Heteroscedasticity: LM Tests Popular heteroscedasticity LM tests: - Breusch and Pagan (1979 )'s LM test (BP). - White (1980)'s general test. Both tests are based on OLS residuals, e, and calculated under H 0 (No heteroscedasticity): s2. The squared residuals are used to estimate . The BP test is an LM test, derived under normality -i.e., ( A5). It is a general tests designed to detect any linear forms of heterosced asticity, driven by some variables, z. That is, the BP tests: H 0: = s2 H1: = f(zi) The White test is an asymptotic Wald-type test , where normality is not needed. It allows for nonlinearities by using squares a nd cross-products of all the x's in the auxiliary regression -i.e., as the drivers of . That is, the White tests: H 0: ,...) Testing for Heteroscedasticity: BP Test The derivation of the BP test is complicate d, it relies on the likelihood function, which is constructed under normality, and its first derivative , the score. However, the implementation of the BP test is simple, based on the squared OLS residuals, e i2. Calculation of the Breusch-Pagan test - Step 1 . Run OLS on DGP: y = X + . -Keep e i and compute R2 = RSS/ T - Step 2 . (Auxiliary Regression). Run the regression of e i2/R2 on the m explanatory variables, z. In our example, e i2/R2 = 0 + z i,1' 1 + .... + z i,m' m + v i -Keep RSS 3 . With the RSS ( ) from Step 2 regression and the Total Sum of Squares (TSS) also from Step 2, compute: LM = (TSS - /2 . There is version of the BP, which is robus t to departures from normality. It is the \" studentized \" version of Koenker (1981). The BP test is asymptotically equivalent to a T*R2 test, where R2 is calculated from a regression of e i2/R2 on the variables Z. (Omitting R2 from the denominator is OK.) We have different Steps 2 & 3: - Step 2 . (Auxiliary Regression). Run the regression of e i2 on the m explanatory variables, z. In our example, e i2 = 0 + z i,1' 1 + .... + z i,m' m + v i -Keep R2 ( ) - Step 3 . Using the R2 from Step 2. Let's call it . Compute LM = T . Example: We suspect that squared Mkt_RF (x1) -a measure of the overall market's variance- drives heteroscedasticity. We do a studentized - OLS in DGP e <- fit_ibm_ff3 $residuals # Step 1 - keep <- regression Re_2 <- summary( fit_BP )$r.squared # Step 2 LM_BP_test <- Re2 * T > LM_BP_test # Step LM-BP test: R2 [1] 0.25038 in the lmtest package performs a studenti zed LM-BP test for the same variables used in the model (Mkt, SMB a nd HML). For Mkt_RF + ); Using the Breusch-Paga n test with Mkt_RF^2 as the driver of heteroscedasticity, we cannot reject H 0. That is, we cannot reject homocedasticity for the residuals of the 3-factor FF model for IBM excess returns . \u00b6 Note: Heteroscedasticity in financial time series is very common. In general, it is driven by squared market returns or squared past errors, thus the default setup of R's bptest is not very useful. Example: We suspect that squared Market returns drive heteroscedastici ty. We do an LM-BP (studentized) test for Disney : lr_dis <- log(x_dis[-1]/x_dis[-T ]) Log returns for DIS dis_x RF ~ Mkt_RF + SMB FF model) e_dis lm (e2 ~ Mkt_RF_2) regression Re_2 <- summary( fit_dis_BP )$r.squared # Step 2 - Keep R^2 from Auxiliary reg LM_BP_test <- Re_2 * T # Step 3 - * T with a p-value = .0001 . We do the same test 2[1],.05 3.84); with a p-value = .006. Conclusion: Using the Breusch-Pa gan test, we reject homocedastic ity for the residuals of the 3- factor FF model for Disney excess returns . If we do use the lmtest package, we get: bptest(dis_x = LM-BP H 0 5% 7.815 ); with a p-value = .07211 . Note: Again, in general, you need squared values when model heterosced asticity in financial assets. \u00b6 Example: We suspect that squared interest rate di fferentials drive heteroscedasticity for residuals in the encompassing (IFE + PPP) model for changes in the USD/GBP . We do an LM- BP (studentized) test, considering the square s of interest different ials as drivers of heteroscedasaticity: y .00001 ). Conclusion: Using the BP LM te st, we have a strong rejectio n of homocedasticity for the residuals of the encompassing (PPP + IFE) model for changes in the USD/GBP . \u00b6 Testing for Heteroscedasticity: White Test The White test derivation is also complicated, but, the usual calcu lation of the White test is a known one for us: - Step 1 . (Same as BP's Step 1). Run OLS on DGP: y = X + . Keep residuals, e i. - Step 2 . (Auxiliary Regression). Regress e2 on all the explan atory variables (X j), their squares (Xj2), and all their cross products. For example, when the model contains k = 2 explanatory variables, the test is based on: e i2 = 0 + 1 x1,i + 2 x2,i + 3 x1,i2 + 4 x2,i2 + 5 x1x2,i + v i Let m be the number of regressors in auxi liary regression (in the above example, m=5). Keep R2, say . - Step 3 . Compute the statistic: LM = T . Example: White Test for the 3-factor F-F model for IBM excess returns (T=569). We also run the White Test for DIS and GE excess returns . In the case of IBM excess returns we have: IBM Ret - rf = 0 + 1 (Mkt Ret - rf) + 2 SMB xx2) # Not including origin al variables in Aux Reg (Mkt_RF,SMB & HML) is OK r2_e2 <- summary( 10.93 ) cannot reject H 0 at 5% level for the residuals of the 3-factor F- F model. (LM-White Test < 2[6],.05 12.59 ). Now, we do a White Test for the 3 factor F-F model for DIS and GE excess returns (T=569). - 5% level. Conclusion: Using the White test , we strongly reject homoscedasti city for the errors of the 3- factor FF model for DIS returns and GE returns . \u00b6 Example: We do a White Test for the residuals in the encompassing (IFE + PPP) model for changes > 0.001458139 reject H 0 at 5% level Conclusion: Using the White test , we strongly reject homoscedasti city for the residuals of the encompassing (PPP + IFE) for changes in the USD/GBP. \u00b6 Testing for Heteroscedasticity: LR Test We define the likelihood function, assuming normality -i.e. ( A5)-, for a general case, where we have g different variances: ) () (1 21ln22ln2ln 12 12 i i i ig i ig iiiX y X yT TL We have two models: (R) Restricted under H 0: i2 = 2. From this model, we calculate ln L )ln(2]1)2[ln(2ln2T TLR (U) Unrestricted. From this mode l, we calculate the log likelihood. ) () ( ;ln2]1)2[ln(2ln1 2 12bXybXyT TLi i i i T ig iii Ui Now, we can estimate the Likelihood Ratio (LR) test: 2 12 12ln ln ) ln (ln2 ga ig ii R U T T L L LR Under the usual regularity condi tions, LR is approximated by a . Testing for Heteroscedasticity: Remarks Drawbacks of the Breusch-Pagan test: - It is sensitive to violations of the normality assumption. The studentized version of Koenker is more robust and, then, more used. Drawbacks of the White test - If a model has several regressors, th e test can consume a lot of df's. - In cases where the White test statistic is stat istically significant, hete roscedasticity may not necessarily be the cause, but model specification errors. - It is general. It does not give us a clue about how to m odel heteroscedasticity to do FGLS. The BP test points us in a direction. - In simulations, it does not perform well relati ve to others, especi ally, for time-varying heteroscedasticity, typical of financial time series. Finding Auto-correlation In general, we find autocorrelation (o r serial correlation) in time se ries, shocks are persistent over time: It takes time to absorb a shock. The shocks can also be correlated over the cr oss-section, causing cross-correlation. For example, if an unexpected new tax is imposed on the techno logy sector, all the compan ies in the sector are going to share this shock. Usually, we model autocorrelation using two mode ls: autoregressive (AR) and moving averages (MA). In an AR model, the errors, t, show a correlation over time. In an MA model, the errors, t, are a function (similar to a weighted average) of previous errors, now denoted u t's. Examples: autocorrelation: MA(3) = + 123 Note: The last example is described as thir d-order moving average autocorrelation, denoted MA(3), because it depends on th e three previous innovations as well as the current one. Finding Auto-correlation - Visual Check Plot data, usually residuals from a regr ession, to see if there is a pattern: - Positive autocorrelation: A pos itive (negative) observation te nds to be followed by a positive (negative) observation. We tend to see continuation in the series. - Negative autocorrelation: A positive (negative) observation tends to be followed by a negative (positive) observation. We tend to see reversals. - No autocorrelation: A positive (negative) observation has the same probability of being followed by a negative or positive (positive or negative) observation. We tend to no pattern. Example: I simulate a series, with N(0,1) errors: = Three cases: (1) Positive autocorrelation: (2) Negative (3) No correlation: code for T_sim <- 200 u errors y_sim <- matrix(0,T_sim,1) rho <- .7 # Change to create different correlation patterns a <- 2 # Time index for observations while (a <= T_sim) { y_sim[a] = rho * y_sim[a-1] + u[a] # y_sim simulated autocorrelated values Negative autocorrelation r1 = -.70 (3) No autocorrelation: r1 = 0 Example: Residual plot for the 3 factor F-F model for IBM returns and GE returns : Conclusion: It looks like a small , but not very clear pattern from the graphs. \u00b6 Testing for Autocorrelation: LM Test There are several autocorrelation tests. The AR( ) model to be tested is: = Under the null hypothesis of no autocorrelation of order , we have H 0p = 0. Under H 1, at least one i 0, for i = 1, 2, ..., p Under H 0, we can use OLS residuals. Breusch-Godfrey (1978) LM test . Similar to the BP test: - Step 1 . (Same as BP's Step 1). Run OLS on DGP: y = X + . - Keep residuals, e i. - Step 2 . (Auxiliary Regression). Run the regression of e i on all the explanatory variables, X:, and lags of residuals, e i e t = Xt' + 1 et-1 + .... + p et-p + v t - Keep R2 () - Step 3 . Keep the R2 from this regression. Let's call it . Then, calculate: LM = ( T- ) . - ( T- p) = we lost p observation by taking lags of e. Example: LM-AR Test for the 3 factor F-F model for IBM excess returns (p=12 lags fit_ibm_ff3 $residuals # OLS residuals p_lag <- 12 # Select # of lags for test (set p) e_lag <- matrix(0,T-p_lag,p_lag) # Matrix to collect lagged residuals a <- 1 while (a<=p_lag) { # Do loop cr eates <- summary( fit_lm1 # degrees of 1 - pchisq(lm_t,df) [1] 0.1560063 LM-AR( 12) 5% level ( p-value > .05). If I run the test with p = 4 lags , I get LM-AR( 4) Test: 2.9747 (p-value = 0.56) cannot reject at 5% level ( p-value > .05). Conclusion: No evidence for th e residuals of the 3-factor FF model for IBM excess returns at the 5% level. \u00b6 R Note: The package lmtest, performs this test, bgtest, (and many others, used in this class, encompassing, jtest, waldtest, etc). Recall that you need to install it first: install.packages(\"lmtest\"), then call the library(lmtest). SMB + HML, order= 12) Breusch-Godfrey test for serial correlation of order up to 12 data: lr_ibm ~ Mkt_RF = 12, p-value = 0.1797 (minor difference with th e previous test, likely due to multiplication by T. Results do not change much) Note: If you do not include in the Auxiliary Regr ession the original regressors (Mkt_RF, SMB, HML) the test not change much. You get LM-AR( 12) Test: 16.83253 very similar. Not entirely correct, but it works well. \u00b6 Example: Autocorrelation is very co mmon. If I run the test for Disney , CNP , or GE, instead, we get significant DIS: HML, order= 4) Breusch-Godfrey test fo r serial correlation of order up to 4 data: dis_x ~ Mkt_RF = p-value 5% order= 12) Breusch-Godfrey test for serial correlation of order up to 12 data: dis_x ~ Mkt_RF at 5% level ( p-value < .05) - For HML, order= 4) Breusch-Godfrey test for serial correlation of order up to 4 data: ge_x ~ Mkt_RF = p-value at 5% level ( p-value >.05) - HML, order= 12) Breusch-Godfrey test for serial correlation of order up to 12 data: cnp_x ~ Mkt_RF 5% level ( p-value < .05) Conclusion: Significant evidence for the residua ls of the 3-factor F-F model for DIS, GE & CNP excess returns . \u00b6 Question: How many lags are needed in the test? Enough to make sure there is no auto-correlation left in the residuals. There are some popular rule of thumbs: for daily data, 5 or 20 lags; for weekly, 4 or 12 lags; for monthly data, 12 lags; for quarterly data, 4 lags. Testing for Autocorrelation: Durbin-Watson The Durbin-Watson (1950) (DW) test against H10. Based on simple correlations of e. T ttT tt t ee e d 1222 1) ( It is easy to show that when T , d 2(1 - ). is estimated by the sample correlation r. Under H 0, = 0. Then, d should be distributed randomly around 2. Small values (close to 0) or Big values (close to 4) of d lead to rejection of H 0. The distribution depends on X. Durbin-Watson derived bounds for the te st. Since there are better tests, in practice, the DW is used \"visually,\" that is, without checking the bounds. The R function dwtest from the lmtest package produces also a p-value . Example: DW Test for the 3 factor IBM returns fit_ibm_ff3 <- lm(ibm_x ~ <- fit_ibm_ff3 $residuals # OLS residuals R S S < - s u m ( e _ i b m ^ 2 ) # R S S DW > statistic 2 No evidence for autocorrelation of order 1. > 2*(1-cor(e[1:(T-1)],e[2:T])) # approximate DW stat [1] 2.048281 [,1] 2.1609 DW statistic 2 But, DIS suffers from autocorrelation! This is why DW are not that informative. They only test for AR(1) in residuals. R Note: The package lmtest performs this test too, dwtest : > dwtest(y in the resi duals of the IBM . \u00b6 Test for the residuals of the enco mpassing model (IFE + PPP) for 5% level. alternative hypothesis: true au tocorrelation is greater than 0 Conclusion: No evidence of fi rst order autocorrelation in the residuals of the encompassing model (IFE + PPP) for changes in USD/GBP . \u00b6 Testing for Autocorrelation: Portmanteu tests Portmanteu tests are test s with a well-defined H 0, but not specific, or loosely defined, H 1. We present two Portmanteu test fo r autocorrelation: the Box-Pierce (1970) test and its modification, the Ljung-Box (1978) test. - Box-Pierce (1970) test (Q test). For a series , it tests H 0 p = 0 using the sample correlation: = where, using time series notation, we have: = sample covariance between and = = is = sample variance. In the case of analyzing residuals of a regression, , we compute rj as: rj: = Then, under H0: Q = T . - Ljung-Box (1978) test (LB test). A variation of the Box-Pierce test. It has a small sample correction, which improves the performance of the test: L B = T * (T - 2) * . Both statistics test whether a group of autocorrela tions are different from zero. Both are general tests, not testing zero-autocorrelation at each lag. Technical note: The asymptotic distribution is ba sed on the fact that, unde r the null hypothesis of independent data, N(0, I). Both tests are widely used, especially the LB test. But, the Breusch-Godfrey (1978) LM tests conditions on X. Thus, LM tests are more powerful. Example: Q and LB tests with p = 12 lags for the residuals in th e 3-factor FF model for IBM a <- a + 1 } Q > Q [1] 16.39559 (p-value = 5% level. R Note: The Box.test function computes Note: There is a minor difference between the prev ious code and the code in Box.test. They are based on how the correlations of e are comput ed (centered around the mean, or assumed zero mean). Conclusion: Using the Q and LB tests, with diffe rent lags, we find no evidence of autocorrelation in the residuals of the 3-factor F-F model for IBM excess returns . Same tests (p = 12 lags ) & same model: for Disney & GE. - For DIS (e_dis), we fit_dis_ff3 <- lm(ge_x (p-value = 0.01117 ) H 0 - For GE (e_ge), we get fit_ge_ff3 <- lm(ge_x ~ (p-value H 5% level. Conclusion: Using the Q and LB te st, we find evidence of autocorre lation in the residuals of the 3-factor F-F model for DIS & GE excess returns . \u00b6 Autocorrelation in financial asset returns is a us ual finding in monthly, weekly and daily data. Example: Same Q and LB tests (p=12 lags) for the USD/GBP residuals in IFE) model: 0.0753 cannot reject H 0 at 5% level, but close. > Box.test(e_gbp, lag = 0.06649 cannot reject H 0 at 5% level, but close. Conclusion: Using the Q and LB tests, with diffe rent lags, we find no evidence of autocorrelation in the residuals of the encompassing (PPP + IFE) model for changes in the USD/GBP . \u00b6 Above, we mentioned that the Q & LB tests ar e widely used. But, they present two main limitations: (1) The test was developed under the independence assumption. If there is dependence in the data, such as heteroscedasticity, the asymptotic variance of is no longer I, but a non-diagonal matrix. There are several proposals to \"robustify\" both Q & LB tests, see Diebold (1986), Robinson (1991), Lobato et al. (2001). The \"ro bustified\" Portmanteau statistic uses instead of : = = Thus, for Q we have: Q* = T . (2) The selection of the numb er of autocorrelations is arbitrary. The traditional approach is to try different values, say 3, 6 & 12. Another popular approach is to let the data \"select\" , for example, using AIC or BIC, an approach sometimes referred as \"automatic selection .\" Escanciano and Lobato (2009) propose combin ing BIC's and AIC's penalties to select in Q* (BIC for small r and AIC for bigger r). The Auto.Q function in the R package vrtest computes Q* with this automatic selection of . It is possible to reach very di fferent conclusion from Q and Q*. Example: Q* tests with automatic selection of p for the residuals in the 3-factor FF model for IBM, DIS and GE excess returns : - For IBM (e_ibm), we get: > library(vrtest) > Auto.Q(e_ibm, Q* test, with auto matic lag selection, we find no evidence of autocorrelation in the residuals of the 3-factor F-F model for IBM excess returns . Same conclusion we reached with the Q & LB tests above. But, for DIS residuals and GE residuals we get a different conclusion. Now, once we take in to consideration heteroscedasticity, we cannot reject at the 5% level the null hypothesis of no autocorrelation, \u00b6 Time-varying volatility is very common in fi nancial time series. We can use the tests for autocorrelation to check for autocorrelation in squared returns, e , which based on White's idea, we use to estimate . Testing for Autocorrelation: Heteroscedasticity We can use a Portmanteu test on the squared resi duals to check for this particular kind of = s2 H1: = f(e , e, ...., e) Of course, an LM-BP test can also be used, using lagged squared residu als as the drivers of heteroscedasticity (more on this topic in Lecture 10). Example: Q and LB tests with p=12 lags for the squa red residuals and tests with p=12 lags for the squa red residuals in the 3-factor FF model for DIS & GE returns: - For DIS (dis_x), we get > Box.test(e_dis2, lag - For GE (ge_x), we get > lag Using Q and LB tests for square d residuals, we find strong evidence for time- varying heteroscedasticity in the residuals of th e 3-factor F-F model for IBM & DIS excess returns . \u00b6 Generalized Regression Model (GRM) Now, we go back to the CLM Assumptions: (A1) DGP: y = X + is correctly specified. (A2) or ( A2') (A3') Var[|X] = ( TxT) symmetric matrix (A4) or ( A4') This is the generalized regression model (GRM), which allows the variances to differ across observations and allows corre lation across observations. OLS is still unbiased. Can we still use OLS? GR Model: True Variance for b From ( = 2 IT we have ( A3') = The true of A3') should be: Var T[b|X] We compute the true variance for the simplest case, a regression with only one explanatory variable and uncorrelated error term: y = X + Then, Var T[b|X] = . If we compute the OLS variance, we see how both estimators differ: Var[ b|X] = Var T[b|X] Note: In the special case that is independent of (u ncorrelated with) of , then both variances are (asymptotically) the same since . (XX)-1- is biased . If we want to use OLS for inferences (say, with t-test or F-test ),, we need to estimate Var T[b|X]. That is, we need to estimate the unknown . But, it has T*(T+1)/2 parameters. Too many parameters to estimate with only T observations! GR Model: Robust Covariance Matrix We will not be estimating . Impossible with T data points. We will estimate XX = ij , a (x) matrix. That is, we are estimating [*(+1)]/2 elements. This distinction is very important in modern applied econometrics: - The White estimator - The Newey-West estimator Both estimators produce a consistent estimator of Var T[b|X]. Since b consistently estimates , the OLS residuals, e, are also consistent estimators of . We use e to consistently estimate XX. Covariance Matrix: The White Estimator The White estimator simplifies the estimation since it only assumes heteroscedasticity. Then, is a diagonal matrix, with elements i2. = 0 0 0 0 00 Thus, we need to estimate: Q* = (1/ T) XX where X ' X = Question: How do we estimate ? We need to estimate: Q* = (1/ T) XX = (1/ T) The OLS residuals, , are consistent estimators of . This suggests using to estimate . That is, we estimate Q* = (1/ T) with S 0 = (1/ T) Example: Back to the simplest case, a regression with only one explanatory variable, but now with a heteroscedastic error term , we have that the variance of b is given by: Var[ b|X] = which we estimate using OLS residuals, : Est Var[ b|X] = . \u00b6 White that a c onsistent estimator of Var[ b|X] is obtained if is used as an estimator of . Taking the square root, we get a heteroscedasticity-consistent (HC) standard errors. (A3') was not specified. That is, the White estimator is robust to a potential misspecifications of heteroscedasticity in ( A3'). The White estimator allows us to make inferences using the OLS estimator b in situations where heteroscedasticity is suspect ed, but we do not know enough to identify its nature. Note: The estimator is also called the sandwich estimator or the White estimator (also known as Eiker-Huber-White estimator) . Remarks: (1) Since there are many refinements of the Wh ite estimator, the White estimator is usually referred as (or \"HC\"): HC0 = (X'X)-1 X' Diag[e i2] (X'X)-1 samples, SEs, t-tests and F-tests are asympt otically valid. (3) The OLS estimator remains inefficient. Bu t inferences are asymptotically correct. (4) The HC standard errors can be larger or smaller than the OL S ones. It can make a difference to the tests. (5) It is used, along the Newey-West estimator, in almost all finance papers. Included in all the packaged software programs (6) White SEs are easy to program: # White SE in R White_f <- Create kxk matrix # X' X F <- t(X)%*%X The library \" sandwich\" calculates White SEs. Remember to install it first and, then, call the library before you use it. Example 1: We estimate t-values using OLS and White SE, for the 3 factor F-F model for IBM returns : I B M Ret - rf = 0 + 1 (Mkt Ret - rf) + 2 SMB + ~ , White SEs make a diff erence in the test results. HML is not longer significant at the 10% level, once we adjust the SEs for heteroscedasticity. \u00b6 Example 2: We estimate Mexican interest rates ( iMX) with a linear model including US interest rates, changes in exchange rates (MXN/USD) , Mexican inflation a nd Mexican GDP growth, using quarterly data 197 8:II - 2020:II (T=166): i MX,t = 0 + 1 iUS,t + 2 et + 3 mx_I t + 4 FMX_da$MX_CPI # Mexican CPI mx_M1 Supply (M1) mx_i <- Mexican short-term int rates (i FMX_da$MX_ GDP # Mexican GDP S_mx <- FMX_da$MXN_USD length(mx_CPI) mx_I <- <- log(mx_GDP[-1]/mx_G DP[-T]) # grow th: GDP mx_mg <- log(mx_M1[-1]/mx_M1[-T]) # Money growth: Log changes in M1 e_mx <- log(S_mx[-1]/S_mx[-T]) # Log changes in S t. coefficients White fit_i, type at 10% level. Conclusion: Again, White SEs make a difference in the test results. U.S. interest rates are not a significant driver (& bi g drop in t-value!), once we adjust the SE for heteroscedasticity. \u00b6 Newey-West Estimator Now, we also have autocorrelation. We need to estimate Q* = (1/T) XX = (1/ T) ij Newey and West (1987) follow Wh ite (1980) to a HAC ( Heteroscedasticity and to estimate ij . The natural estimator of Q * becomes: ST = (1/ T) Or using time series notation, estimator of Q*: ST = (1/ T) There are some restrictions that need to be imposed: - Q* needs to be a pd matrix use a quadratic form. - The double sum cannot explode use decaying waits to cut the sum short. Two components for the NW HAC estimator: (1) Start with Heteroscedasticity Component: S 0 = (1/T) xi xi - the White estimator. (2) Add the Autocorrelation Component S T = S 0 + (1/T) ( + ) where || -decaying weights ( Bartlett kernel ) L is the cut-off lag, which is a function of T. (More data, longer L). The weights are linearly decaying, suppose L=30. Then, k(1) = = S T = S 0 + (1/ T) ( + ) Then, ST (X'X/T)-1 -NW's HAC Under suitable conditions, as L, T , and L/T 0, S T Q*. Asymptotic inferences can be based on OLS b, with t-tests and Wald tests using N(0,1) and 2 critical values, respectively. There are many refinements of the NW estimat ors. Today, all HAC estimators are usually referred as NW estimators, regardless of the weights ( kernel ) used if they produce a positive (semi-) definite covariance matrix. All econometric packages (SAS, SPSS, Eviews, etc.) calculate NW SE. R Note: You can use the library \" sandwich ,\" to calculate NW SEs: library(sandwich) > NeweyWest( x, lag = NULL, order.by = NULL, prew hite = TRUE, adjust = FALSE, diagnostics = FALSE, sandwich = TRUE, ar.method = \"ols\", data = list (), verbose = FALSE) You need to install the package sandwich and then call the library(sandwich). Example : # fit the 3 factor Fama French Model for IBM returns: fit_ibm_ff3 computes the NW SEs. It lags= L & suppression of prewhitening NeweyWest( fit_ibm_ff3 , lag = 4, prewhite = FALSE) Note: It is usually found that the NW SEs are downward biased. You can also program the NW SEs yourself. In R: NW_f <- function(y, X, while (a <= lag) { 1 : We estimate the 3 factor F-F model for IBM returns with NW SE with 4 lags : > , lag = 4, prewhite = FALSE) # NW Var Matrix with 4 lags SE_NW <- diag(sqrt(abs(NW))) # NW SE with at 5% level If we add more lags in the NW function ( lag = 8) NW <- NeweyWest( fit_ibm_ff3 Conclusion: Newey-West SEs make a difference in the test results. Now, SMB is not longer significant at the 5% level, though borderli ne, once we adjust the SEs not only for heteroscedasticity and autocorrelation. \u00b6 Example 2: Mexican short-term interest rates with NW SE with 4 lags & 8 lags .. For comparison we reproduced the regression (with OLS t-varlues and the White t-values): fit_i If we add more lags in the text ( lag = 8) NW <- NeweyWest( fit_i, SEs make a difference in the test results, but in this case, the results are not that different from the White SEs. \u00b6 There are many estimators of Q * based on a specific parametric model for . Thus, they are not robust to misspecification of ( A3'). This is the appeal of White & NW. NW SEs are used almost universally in academia. However: - NW SEs perform poorly in Monte Carlo simulations: - NW SEs tend to be downward biased. - The finite-sample performance of tests us ing NW SE is not well approximated by the asymptotic theory. - Tests have size distortions. Question: What happens if we know the specific form of ( A3')? We can do much better than using OLS with NW SEs. In this case, we can do Generalized LS (GLS), a method that delivers the most efficient estimators. Generalized Least Squares (GLS) GRM: Assumptions ( A1), (A2), (A3') & ( A4) hold. That is, (A1) DGP: y = X + is correctly specified. (A2) E[|X] = 0 (A3') Var[|X] = = 2 ( is symmetric T'T= ) (A4) X has full column rank -i.e., rank( X)=k-, where T k. Question: What happens if we know the specific form of ( A3')? We can use this information to gain efficiency. When we know ( A3'), we transform the y and X in such a way, that we can do again OLS with the transformed data. To do this transformation, we exploit a propert y of symmetric matrices, like the variance- covariance matrix, : is symmetric exists T T'T= T'-1 T-1= I Note: can be decomposed as = T'T (think of T as 1/2) T'-1 T-1= I We transform the linear model in ( A1) using P = -1/2. P = -1/2 P'P = -1 Py = PX + P or y * = X* + *. E[**'|X*] = P E['|X* ]P' to ( A3) The transformed model is homoscedastic: We have the CLM framework back we can use OLS! b PE['|X*]P' We have the CLM framework back: We do OL S with the transformed model, we call this OLS estimator, the GLS estimator: assumption: is known, and, thus, P is also known; otherwise we cannot transformed the model. Big Question: Is The GLS b. bGLS is BLUE by construction, b is not. Check unbiasedness: b GLS = ( can be derived from Var[ bGLS|X] = 2 (X*'X*)-1 = 2 (X'-1X)-1 Then, the usual OLS variance for b is biased and inefficient! Note II: Both unbiased and consistent. In practice, bot h estimators will be different, but not that different. If they are very diffe rent, something is not kosher. Steps for GLS: Step 1 . Find transformation matrix P = -1/2 (in the case of heteroscedasticity, P is a diagonal matrix). Step 2 . Transform the model: X* = PX & y* = Py. Step 3 . Do GLS; that is, OLS with the transformed variables. Key step to do GLS: Step 1 , getting the transformation matrix: P = -1/2. Technical detail: If we relax the CLM assumptions ( A2) and ( A4), as we did in Lecture 7-a, we only have asymptotic properties for GLS: - Consistency - \"well behaved data.\" - Asymptotic distribution under usual assumptions. (easy for heteroscedasticity, complicated for autocorrelation.) - Wald tests and F-tests with usual asymptotic 2 distributions. (Weighted) GLS: Pure Heteroscedasticity Find the transformation matrix P = -1/2 for: (A3') 0. . .0 . . -1/2 1/ . 0 00 . . . transform y & X: 1/ 0. . .0 01 / . 0 00 y y =y/ y/ y/ Each observation of y, y i, is divided by . Similar transformation occurs with X: 1/ 0. . .0 01 /... . 0 00 . . 1 = 1//.../ 1//.../ ... 1//.../ Now, we can do OLS with variables: b GLS = b* = ( the case of heteroscedasticity, GLS is also called Weighted Least Squares (WLS): Think of 1/ . as weights. The GLS estimator is: y Observations with lower (bigger) variances -i.e., lower (bigger) i- are given higher (lower) weights in the sums: More prec ise observations, more weight! The GLS variance is given by: Example : Last Lecture, we found that squared ma rket returns (Mkt_RF^2) influence the heteroscedasticity in DIS re turns. Suppose we assume: (A3') = (Mkt_RT i)2. Steps for GLS: Step 1 . Find transformation matrix, P, with ith diagonal element: 1/ Step 2 . Transform model: Each y i and x i is divided (\"weighted\") by sqrt[(Mkt_RT i)2]. Step 3 . Do GLS, that ' ' 1 Residual standard error: 7.984 on 566 degrees of freedom Multiple R-squared: 0.09078, Conclusion: Quite different results, includi ng a change in sign in HML, from positive & significant at the 10% level (O LS) to negative & not signifi cant (GLS) and change in significance in SMB, from not significant (OLS) to very significant (GLS). \u00b6 GLS: First-order Autocorrelation Case We assume an AR(1) process for the t: t = t-1 + u t, u t: non-autocorrelated error ~ D(0, ) We need to find matrix P = -1/2 for: (A3') ... ... ..., where is and j ( = E[i j] = = ). and = = Var[t] (homoscedasticity, constant for all t.) Notation: We use to denote a covariance between two obs ervations --in this case, errors-- separated by periods. For example, = = = ... = = Cov[t, t-1] = E[t t-1] -Covariance between two errors separated by one period. Steps for GLS: Step 1 . To find the transformation matrix P, we need to derive the implied ( A3') based on the AR(1) process for t: (1) Find diagonal elements of : Var[t] = = t = t-1 + u t (the autoregressive form) Var[t] -we need to assume | |< 1. Using the above notation, we have = E[t t] = Var[t]. (2) Find the off-diagonal elements of : = : = = Cov[i, j] = E[i j] l = j - i = , = = Var = = , = = , = = , = = , = = = , = If we define = = /(1 - 2), then we generalize the autocovariance function , : = Now, we get ( A3') = 2 . (A3') 1 1 1 1 00 . . . 0 10 ... 00 0 0 Step 2 . With P = -1/2, we transform the data ( y & X) to do GLS: / 1 00 . . . 0 = 1 ... Step 3 . GLS is done with transformed data. In ( A3') we assume known. In practice, we need to estimate it. GLS: The Autoregressive Transformation With AR models, sometimes it is easi er to transform the data by taking pseudo differences. For the AR(1) model, we multiply the DGP by and subtract it from it. That is, , + + Now, we have the errors, , which are uncorrelated. We can do OLS with pseudo differences . Note: & are pseudo differences . FGLS: Unknown The problem with GLS is that is unknown. For example, in the AR(1) case, is unknown. Solution: Estimate . Feasible GLS (FGLS) . In general, there are two approaches for GLS (1) Two-step, or Feasible estimation : - First, estimate first. - Second, do GLS. Similar logic to HAC procedures: We do not need to estimate , difficult with T observations. We estimate (1/ T)X-1X. -Nice asymptotic properties for FG LS estimator. Not longer BLUE (2) ML estimation of , 2, and at the same time (joint estimation of all parameters). With some exceptions, rare in practice. FGLS: Specification of must be specified first. is generally specified (modeled) in terms of a few parameters. Thus, = () for some small parameter vector . Then, we need to estimate . Examples : (1) Var[ i|X] = 2 f(zi). Variance a function of and some variable zi (say, market volatility, firm size, country dum my, etc). In general, f is an exponential to make sure the variance is positive. (2) i with AR(1) process. We have already derived 2 as a function of . Technical note: To achieve full efficiency, we do not need an efficient estimate of the parameters in , only a consistent one. FGLS: Estimation - Steps Steps for FGLS: Step 1 . Estimate the model proposed in ( A3'). Get & Step 2 . Find transformation matrix, P, using the estimated & . Step 3 . Using P from Step 2, transform model: X*= PX and y*= Py. Step 4 . that is, OLS with X* & y*. Example: In the pure heteroscedasticity case ( P is diagonal): 1. Estimate the model proposed in ( A3'). Get . 2. Find transformation matrix, P, with ith diagonal element: 1/ 3. Transform model: Each y i and x i is divided (\"weighted\") by . 4. Do FGLS, that is, OLS with transformed variables. FGLS Estimation: Heteroscesdasticity Example : Last lecture, we found that Mkt_ RF^2 and SMB^2 are drivers of the heteroscedasticity in returns: Suppose assume: (A3') i2 3 i2 Steps vector, with elements . 2. Find transformation matrix, P, with i th diagonal element: 1/ w_fgls <- sqrt(var_dis2) # 1/ 3. Transform model: Each y i and x i is divided (\"weighted\") by . # transformed X 4. that is, OLS with transformed variables. fit_dis_fgls <- lm(y_fw ~ xx_fw - 1) > 0.001 '**' 0.01 ' ' 1 Residual standard error: 0.9998 on 566 degrees of freedom Multiple R-squared: 0.3413, 0.3366 OLS, GLS & FGLS estimates are quite different than OLS estimates (remember OLS is unbiased and consistent). Very likely the assumed functional form in ( A3') was not a good one. - The FGLS results are similar to the OLS, as e xpected, if model is OK. FGLS is likely a more precise estimator (HML is not longer significant at 10%). AR(1) Case - Cochrane-Orcutt In the AR(1) case, it is easi er to estimate the model in pseudo differences : yt* = Xt* + u t yt - yt-1 = - + yt = yt-1 + Xt' - Xt-1' + u t We have a linear model, but it is nonlinear in parameters. OLS is not possible, but non-linear estimation is possible. Before today's computer power, Cochrane-Orcutt' s (1949) iterative proce dure was an ingenious way to do this estimation. Steps for Cocharne-Orcutt: (1) Do OLS in ( A1) model: y = X + . Get residuals, e, & RSS. (2) Estimate with a regression of e t against et-1 get (the estimator of ). (3) FGLS Step. Use transform the model to get y* and X*. Do OLS with y* and X* get b to estimate . Get residuals, e* = y - X b, and new RSS. Go back to (1). (4) Iterate until convergence, usua lly achieved when the difference in RSS of two consecutive iterations is lower than some tolera nce level, say .0001. Then, stop when RSS i - RSS i-1 < .0001. Example : Cochrane-Orcutt in R # C.O. function requires Y, X (with constant), OLS b. c.o.proc <- function(Y,X,b_0,tol) { T <- rss # RSS_1 will be used to reset after each iteration d_rss = rss # e2 size for e t e3 <- e[-T] # adjust sample size for e t-1 ols_e0 <- lm(e2 ~ e3 - 1) # OLS to estimate rho rho <- ols_e0$coeff[1] # initial value for rho, 0 i<-1 while (d_rss > tol) { # toleran ce of do loop. Stop when diff in <- X[2:T, ] - ] # pseudo-diff X ols_yx <- lm(YY ~ XX - 1) # ad just if constant included in X b <- ols_yx$coef # updated OLS b at iteration i # b[1] <- b[1]/(1-rho) # If cons tant not pseudo-differenced e1 <- Y - ted residuals at iteration i e2 <- e1[-1] # adjust sample size for updated e t e3 <- e1[-T] # adjust sample size for updated e_t-1 (lagged e t) ols_e1 <- lm(e2~e3-1) # updated regression to value fo r rho at iteration i rho <- ols_e1$coeff[1] # updated value of rho at iteration i, i rss_1 <- sum(e1^2) # updated value of RSS } Example : In the model for Mexican interest rates (i MX), we suspect an AR(1) in the residuals: i MX,t = 0 + 1 iUS,t + 2 et + 3 mx_I t + 4 mx_y t + ' ' 1 Residual standard error: 0.09678 on 160 degrees of freedom Multiple R-squared: 0.1082, 0.1663884 Constant corrected if X does not include a constant $Number.Iteractions [1] 10 algorithm converged in 10 iterations. Conclusion: Quite high the auto correlation in th e residuals ( =0.8830857 ), which has a big effect on the results. Once we account for the auto correlation in the residuals, U.S. interest rates and Mexican Inflation rates are not longer significant drivers of Mexican interest rates. The model needs to be reformulated. \u00b6 GLS: General Remarks GLS is great (BLUE) if we know . Very rare situation. It needs the specification of -i.e., the functional fo rm of autocorrelation and heteroscedasticity. If the specification is bad estimates are biased. In general, GLS is used for larger samples, because more parameters need to be estimated. Feasible GLS is not BLUE (unlike GLS); but, it is consistent and asymptotically more efficient than OLS. We use GLS for inference and/or efficienc y. OLS is still unbiased and consistent. OLS and GLS estimates will be different due to sampling error. But, if they are very different, then it is likely that some ot her CLM assumption is violated. Lecture 8 - Time Series Time Series: Introduction A time series is a (stochastic) process obs erved in sequence over time, t = 1, ...., T Yt = {, , , ..., }. Examples : IBM monthly stock prices from 1973:January to 2020:September (plot below); or USD/GBP daily exchange rates from February 15, 1923 to March 19, 1938. R Note: There are different ways to do the above plot in R: - Using plot.ts, creating a timeseries object in R: <- ts(x_ibm,start=c(1973,1),frequency=12) # the function ts creates = starting year, plot.ts(ts_ibm,xlab=\"Time\",yl ab=\"IBM price\", main=\"Time = \"IBM IBM \", subtitle = \"Monthly: 1973-2020\") Time Series: Introduction - Types Usually, time series models are separated into two categories: - Univariate ( R, it is a scalar) Example: We are interested in the behavior of IB M stock prices as f unction of its past. Primary model: Autoregressions (ARs). - Multivariate ( Rm, it is a vector-valued) Example: We are interested in the joint behavior of IBM stock and bond prices as function of their joint past. Primary model: Vector autoregressions (VARs). \u00b6 Time Series: Introduction - Dependence Given the sequential nature of Y t, we expect y t & y t-1 to be dependent. This is the main feature of time series: dependence. It cr eates statistical problems. In classical statistics, we usua lly assume we observe several i.i.d. realizations of Y t. We use to estimate the mean. With several independent realizations we are able to sample over the entire probability space and obtain a \"good\" -i.e., consistent or close to the population me an- estimator of the mean. But, if the samples are highly dependent, then it is likely that Y t is concentrated over a small part of the probability space. Then, the sample mean will not converge to the mean as the sample size grows. Technical note: With dependent observations, th e classical results (bas ed on LLN & CLT) are not to valid. New assumptions a nd tools are needed: moments Y t; ergodicity requires that the dependence is short-lived, eventually y t has only a small influence on y t+k, when k is relatively large. The amount of dependence in Y t determines the 'quality' of the estimator. There are several ways to measure the dependence. Th e most common measure: Covariance. Cov, Note: When =0, then Cov, Time Series: Introduction - Forecasting In a time series model, we describe how depends on past 's. That is, the information set is It = {, , , ....} The purpose of building a time series model: Forecasting. We estimate time series models to forecast out-of-sample. For example, the l-step ahead forecast: y Et[|] In the 1970s it was found that very simple time se ries models out-forecast ed very sophisticated (big) economic models. This finding represented a big shock to the big multivariate models that were very popular then. It forced a re-evaluation of these big models. Time Series: Introduction - White Noise In general, we assume the error term, t, is uncorrelated with everything, with mean 0 and constant variance, 2. We call a process like this a white noise (WN) process . We denote a WN process as ~ WN(0, 2) The WN is a very simple example of a stochastic process. We think of a white noise process as the basic building block of all tim e series. It can be written as: z = u, u ~ i.i.d (0, 1) z ~ WN(0, 2) The z's are random shocks, with no dependence over time, representing unpredictable events. It represents a model of news. Technical note: There may be depend ence in the higher order moments of . For example, ] = ] * ]. If we assume is i.i.d., this dependence is excluded. Time Series: Introduction - Conditionality We make a key distinction: Conditional vs Unconditional moments. In time series we model the conditional mean as a function of its past, for exampl e in an AR(1) process, we have: = + + . Then, the conditional mean forecast at time t, conditioning on information at time It-1, is: E t[|] = E t[] = + Notice that the unconditional mean is given by: E [ ] = + E[] = /(1 - ) = constant The conditional mean is time vary ing; the unconditional mean is not! Key distinction: Conditional vs. Unconditional moments. Time Series: Introduction - AR and MA models Two popular models for E t[|]: - An autoregressive (AR) process models E t[|] with lagged dependent t[|] = f(, = + + .. \u00b6 - A moving average (MA) process models E t[|]:with lagged errors, t: E t[|] = f(, , = + 1 + . \u00b6 - There is a third model, ARMA, that combines lagged dependent variables and lagged errors. We want to select an appropriate time series model to forecast y t. In this class, we will use linear model, with choices: AR (p), MA(q) or ARMA(p, q). Steps for forecasting: (1) Identify the appropriate mode l. That is, determine p, q. (2) Estimate the model. (3) Test the model. (4) Forecast. Time Series: CLM Revisited & New Assumptions With autocorrelated data, we get dependent obs ervations. For example, with autocorrelated errors: = + u, with u ~ WN(0, 2), the independence assumption (A2) is violated. Th e LLN and the CLT cannot be easily applied in this context. We need new tools. We introduce the concepts of stationarity and ergodicity. The ergodic theorem will give us a counterpart to the LLN. To get asymptotic distributions, we also n eed a CLT for dependent variables, using new technical concepts: mixing a nd stationarity. Or we can rely on a new CLT: The martingale difference sequence CLT . We will not cover these technical points in detail. Time Series - Stationarity Consider the joint probability distribution of the collection of RVs: , ,..., ,,..., To do statistical analysis with dependent obser vations, we need some extra assumptions. We need some form of invariance on th e structure of the time series. If the distribution F is changi ng with every observation, estimati on and inference become very difficult. Stationarity is an invariant prope rty: the statistical characteristics of the time series do not change over time. There different definitions of stationarity, they differ in how strong is the invariance of the distribution over time. We say that a process is stationary of 1 st order if for any , k 2nd order if , , for any k Nth-order stationarity is a strong assumption (& difficult to verify in practice). 2nd order stationarity is weaker: only consider mean a nd covariance (easier to verify in practice). Moments describe a distribution. We calculate moments as usual: E Var Cov, = 1 -2 Time Series - Stationarity, A utocovariances & Autocorrelations Cov, = 1 -2 is called the auto-covariance function. Notes: is a function of k = (0) is the variance. The autocovariance function is symmetric. That is, Cov, = Cov, = Autocovariances are unit dependent. We will ha ve different values if we calculate the autocovariance for IBM returns in % terms or in decimal terms. Remark: The autocovariance measures th e (linear) dependence between the Y t's separated by k periods. From the autocovariances, we derive the autocorrelations: Corr , , 1 -2 1 -2 (0) the last step takes assumes: 0 Corr, , is called the auto-correlation function (ACF), -think of it as a function of k = t 1 - t2. The ACF is also symmetric. Unlike autocovoriances, autocorrelations are no t unit dependent. It is easier to compare dependencies across different time series. Stationarity requires all these moments to be independent of time. If the moments are time dependent, we say the series is non-stationary. For strictly stationary process (constant moments), we need: because , , Cov,= Cov, ,, Let & , ,, = = The correlation between any two RVs depends on the time difference. Given the symmetry, we have = . Time Series - Weak Stationarity A Covariance stationary process (or 2nd -order weakly stationary ) has: - constant mean - constant variance - covariance function depends on time difference between RVs. That is, Z t is covariance stationary if: E = constant = Var = constant = Cov, E[()( )] = - = - Remark: Covariance stationarity is only concer ned with the covariance of a process, only the mean, variance and covariance are time-invariant. Nth-order stationarity is stronger and assumes that the whole distributio n is invariant over time. Example : Stationary time series. Assume ~ WN(0, 2). = + (AR(1) process) Mean Taking expectations on both side: E[ ] = E[] + E[] = + 0 E[] = = 0 (assuming 1) Variance Applying the variance on both side: Var[ ] = 0 2 Var[] + Var[] 0 = 2/(1 - 2) (assuming | |< 1) Autocovariances 1 Cov, E E = E[2] = Var[] = 0 = [2/(1- 2)] 2 = Cov[, ] = E[ ] = E[( + ) ] = E[-1 ] = Cov[, ] = 0 = 2 0 = 2 [2/(1- 2)] ... = Cov[, ] = k 0 If | |< 1, the process is covarian ce stationary: mean, variance and covariance are constant. Note: From the autocovariance function, we can derive th e auto-correlation function: (0) (0). \u00b6 Example : Non-stationary time series. Assume t ~ WN(0, 2). = + + (Random Walk with drift process) Doing backward substitution: = + ( + + ) + = 2* + + + = 2* + ( + + ) + + = 3* + + + + = t + + y 0 Mean & Variance E [ ] = t + y 0 V a r [ ] = 0 = 2 = 2 t the process is non-stationary; that is, moments are time dependent. \u00b6 Stationary Series - Examples Examples : Assume t ~ WN(0, 2). 0.080.4 - MA(1) process 0.13 - AR(1) process Changes in the JPY/USD exchange rate (e f,t) is an example of a stationary series. \u00b6 Non-Stationary Series - Examples Examples : Assume t ~ WN(0, 2). - AR(2) with deterministic trend - Random Walk with drift The level of the JPY/USD exchange rate (S t) is an example of a non-stationary series. \u00b6 Time Series - Stationarity: Remarks The main characteristic of time series is that observations are dependent. To analyze time series, however, we need to a ssume that some features of the series are not changing. If we have non-stationa ry series (say, mean or va riance are changing with each observation), it is not possible to make inferences. Stationarity is an invariant prope rty: the statistical characteristics of the time series do not vary over time. If IBM is weak stationary, then, the returns of IBM may change m onth to month or year to year, but the average return and the variance in two equal lengths time intervals will be more or less the same. In the long run, say 100-200 years, the stationarity assumption may not be realistic. After all, technological change has affected the return of IBM over the long run. But, in the short-run, stationarity seems likely to hold. In general, time series analysis is done under the stati onarity assumption. Time Series - Ergodicity We want to estimate the mean of the process {Z t}, (Zt). But, we need to distinguishing between ensemble average (with m observations) and time average (with T observations): - Ensemble Average: - Time Series Average: Question: Which estimator is the most appropriate? A: Ensemble Average. But, it is impossi ble to calculate. We only observe one Z t, with dependent observations. Question: Under which circumstances we can use the time average (with only one realization of {Z t})? Is the time average an unbiased and consistent estimator of the mean? The Ergodic Theorem gives us the answer. Intuition behind Ergodicity: We go to a casino to play a game with 20% re turn, but on average, one gambler out of 100 goes bankrupt. If 100 gamblers play the game, there is a 99% chance of wi nning and getting a 20% return. This is the ensemble scenario. Suppose that gambler 35 is the one that goes bankrupt. Gambler 36 is not affected by the bankruptcy of gamble 35. Suppose now that instead of 100 gamblers you play the game 100 times. This is the time series scenario. You win 20% every day until day 35 when you go bankrupt. There is no day 36 for you (dependence at work!). Result: The probability of success from the group (ensemble scenario) does not apply to one person (time series scenario). Ergodicity describes a situation where the ensemble scenario outcome applies to the time series scenario. With dependent observation, we cannot use the LLN used before. The ergodicity theorem plays the role of the LLN with dependent observations. The formal definition of ergodicity is complex and is seldom used in time series analysis. One consequence of ergodicity is the ergodic theorem, which is extremely useful in time series. It states that if Z t is an ergodic stochastic process then . E[ ] for any function g(.). And, for any time shift k ,,..., . E[,,...,] where a.s. means almost sure convergence , a strong form of convergence. Definition : A covariance-stationary process is ergodic for the mean if E[Z t] = This result needs the variance of to collapse to 0. It can be shown that the var[ ] can be written as a function of the autocorrelations, : var var / 1|| Theorem : A sufficient condition for ergodicity for th e mean is that the autocorrelations k between two observations, say ( ,), , = , go to zero as & grow further apart. Condition for ergodicity: 0, as k Time Series - Lag Operator Define the operator L as L = . It is usually called Lag operator. But it can produce lagged or forw ard variables (for negative values of k). For example: L = . Also note that if c is a constant L c = c. Sometimes the notation for L when working as a lag operator is B (backshift operator), and when working as a forward operator is F. Important application: Differencing = (1 - L) = . 1 L 2. Time Series - Useful Result: Geometric Series The function 1 can be written as an infinite ge ometric series (use a Maclaurin series around c=0): 1... If we multiply by a constant, a: 1 Example: In Finance we have many applications of the above results. - A stock price, P, equals the discounted some of all fu tures dividends. Assume dividends are constant, d, and the discount rate is r. Then: = 1) = 1) = where . \u00b6 We will use this geometric series result whe n, under certain conditions, we invert a lag polynomial (say, (L)) to convert an AR (MA) process in to an infinite MA (AR) process. Example: Suppose we have an MA(1) process: = + 1 + = + (L) , with (L) = (1 + 1 L) ( (L): lag polynomial) Recall, 1... Let x = Then, (L)-1 = (-1L)1(-1L)(-1L) (-1L) (-1L) ... (-1 L = 11 L1L1L1L That is, we get an AR( ), by multiplying both sides by (L) -1: (L)-1 = (L)-1 + = * + Or (L)-1 = 1 111 * + Solving for y t: * + 1 1 1 1 + . \u00b6 Moving Average Process An MA process models E[ |It-1] with lagged error terms. An MA( q) model involves q lags. We keep the white noise assumption for t: t ~ WN(0, 2) Example : A linear MA( q) model: + 1 + 2 + ... + q + = + (L) , where (L) = 11L2L3L...qL. \u00b6 In time series, the constant does not affect the properties of AR and MA process. It is usually removed (think of the data analyze as demeaned). Thus, in this situation we say \"without loss of generalization\", we assume =0. Moving Average Process - Stationarity To check if an MA( q) process is stationary, we check the moments (assume = 0). + 1 + 2 + ... + q = + 1 + 2 + ... + q-1 + q Mean E[ ] = E[] + 1 E[] + 2 E[] + ... + q E[] = 0 Variance Var[ ] = Var[] + + 22 Var[] + ... + q2 Var[] = (1 + 12 + 22 + ... + q2) 2. To get a positive variance, we require (1 + 12 + 22 + ... + q2) > 0. Autocovariances = + 1 + 2 + 3 + ... + q = + 1 + 2 + ... + q + q 1 = Cov[ , E[ ] 1 + +...+ q )] = E[ ] + 1 E[ ] + 2 E[ ] + .... + 1 E[ ] + 12 E[ ] + 1 2 E[ ] + ... + 2 E[ ] + 2 1 E[ ] + 2 1 E[ ] + ... ... + q E[] + q + q q-1 E[] + q2 E[ ] = 1 2 + 2 1 2 + 3 2 2 + ... + q q-1 2 + 0 = 2 j j-1 (where 0 = 1) We can also derive 1 without computing the expectation of the cross products of errors. It is easier to look at the sum of E[ t-j]'s: 1 = E[ ] = E[ * ( + 1 + 2 + ... + q-1 + q )] = E[ ] + 1 E[ ] + 2 E[ ... + q-1 E[ ]+ q E[ t-q-1] = 1 2 + 2 1 2 + 3 2 2 + ... + q q-1 2 + 0 = 2 j j-1 ( w h e r e 0 = 1) We continue with the derivations of the function. It is easier to derive it by rewriting y t & yt-2 : = t + 1 + 2 + 3 + ... + q = + 1 + 2 + 3 + ... + q 2 = Cov[, ] = E[ ] = E [ ] + 1 E[ ] + 2 E[ ] + ... + q E[ ] = 2 2 + 3 1 2 + 4 2 2 + ... + q q-2 2 + 0 = 2 j j-2 (where 0 = 1) = E[ ]] = = E [ ] + 1 E[ ] + 2 E[ ] + ... + q E[ ] = q 2 = 2 j j-q (where 0 = 1) In general, for the k autocovariance: = 2 j j-k for | k| q = 0 for | k| > q Remark: After lag q , the autocovariances are 0. Autocorrelations From the autocovariances, we define the auto correlations, by dividing the autocorrelations by 0: = 2 jj-q (1 + 12 + 22 + ... + q2) 2 = jj-q (1 + 12 + 22 + ... + q2) (0=1) In general, for the k autocorrela tion function (ACF): = jj-q (1 + 12 + 22 + ... + q2) for | k| q = 0 for | k| > q Remark: After lag q , the ACF is 0. It can be shown that for t with same distribution (say, nor mal) the ACF are non-unique. For example, for the MA(1) processes: = + 0.5 1 1/(1+ 12) = 0.4 = + 2 1 1/(1+ 12) = 0.4 It is easy to verify that the sums jj-k are finite. Then, mean, variance and covariance are constant. MA( q) is always stationary. Moving Average Process - Invertibility As mentioned above, it is possibl e that different time-series pr ocesses produce the same ACF. Example : Two MA(1) produce the same : = + .2 , ~ i.i.d. N(0, 25) = + 5 , ~ i.i.d. N(0, 1) We only observe the time series, or , and not the noise, or , thus, we cannot distinguish between the models. Thus, we select only one of them. Which one? We select the model with an AR() representation. Assuming (L) 0, we can invert (L). Then, by inverting (L), an MA (q) process generates an AR process: = + (L) (L)-1 = (L) = * + . Then, we have an infinite sum polynomial on L. (Recall the geometric series result.) That is, we convert an MA (q) into an AR( ): = We need to make sure that (L) = (L)-1 is defined: We require (L) 0. When this condition is met, we can write t as a causal function of y t. We say the MA is invertible. For this to hold, we require: || Technical note: An invertible MA( q ) is typically required to have roots of the lag polynomial equation (z) = 0 greater than one in absolute value (\" outside the unit circle \"). In the MA(1) case, we require | 1 | < 1. In the previous example, we select the model with 1 = .20. Moving Average Process - MA(1) Example : = 1 + = + (L) , with (L) = (1 + 1 L) Moments E[ ] = 0 Var[ ] = 0 = 2 + 12 2 = 2 (1+ 12) Cov[ , ] = 1 = E[ ] = E[( 1 + ) * (1 + )] = 1 2 Cov[ , ] = 2 E[ ] = E[( 1 + ) * (1 + )] = 0 = E[ ] = E[(1 + )) * (1 +)])] = 0 (for k > 1) That is, for | k| > 1, = 0. To get the ACF, we divide the autocovariances by 0. Then, the autocorrelation function (ACF): 1 1/0 = 1 2 /2 (1 + 12) = 1 / (1 + 12) /0 = 0 (for k > 1) Remark: The autocovariance function is zero after lag 1. Similarly, th e ACF is also zero after lag 1. Note that |1| 0.5. When 1 = 0.5 1 = 0.4. 1 = -0.9 1 = -0.497238. 1 = -2 1 = -0.4. (same 1 for 1 & 1/1) Moving Average Process - MA(1): Simulations We simulate and plot three MA(1) processes, with standard normal -i.e., =1: = + 0.5 = - 0.9 = - 2 R Note: We use the arima.sim function to simulate the behavior of different ARIMA models (or ARMA, by setting the orden of Integration (I) equal to 0). Below we plot differen MA(1) process. On the first panel, we use the script below to plot = + 0.5 with 100 simulations. The other panels are straightforward to get. > plot(arima.sim(list(order=c(0,0,1), ma= 0.5), n=100), ylab=\"ACF\", main=(expression(MA(1)~~~theta==+.5))) Note: The process 1 > 0 is smoother than the ones with 1 < 0. Below, we compute and plot the ACF fo r the 3 simulated process, using the acf R function. 1) = + 'sim_ma1_6', by lag 0 1 2 3 4 5 6 7 8 9 10 11 12 'sim_ma1_9', by lag 0 1 2 3 4 5 6 7 8 9 10 11 12 'sim_ma1_2', by lag 0 1 2 3 4 5 6 7 8 9 10 11 12 1, we can write (1 + 1 L)-1 + * = t Or expanding 1122 1 1 )( * * ...) ... 1( it t i tjjyL y L L L That is, i = (-1)i. The simulated process with 1 = -2 is non-invertible , the infinite sum of i would explode. We would select the MA(1) with 1 = -.5. Moving Average Process - MA(2) Example : y t = + 2 t-2 + 1 t-1 + t = + (L) t, with (L) =(1+ 1 L + 2 L2). Moments 2 ,02 ,1 , 10 , 1 2 222 12 22 12 kkkkYE kt Remark: The autocovariance function is zero after lag 2. Similarly, th e ACF is also zero after lag 2. - Invertibility: The roots of - 1 - 2 = 0 all lie inside the unit circle. It can be shown the invertibility condition for an MA(2) process is: 1 + 2 < 1 1 - 2 < 1 -1 < 2 <1. Moving Average Process - Estimation MA processes are more complicated to estimate. In particular, there are n onlinearities. Consider an MA(1): = + , ~ WN. We cannot do OLS, since we do not observe . But, based on the ACF, we can estimate . The auto-correlation is 1 = /(1+2). Then, we can use the me thod of moments (MM), which sets the theoretical moments e qual to the sample moments and, then, solve for parameters of interest. In the MA(1) case, the theoretical formula for 1 is: 1 /(1 + 2) . Then, we use the estimated 1, , to estimate : A nonlinear solution and difficult to solve. Alternatively, if | |< 1, we can invert the MA(1) process. Then, based on the AR representation, we can try finding a (-1; 1), t ... and look (numerically) for th e least-square estimator })( )( min{arg 12 T t t T a a aS The Wold Decomposition Theorem - Wold (1938). Any order, movi ng-average representation: + , where is a deterministic term -i.e., completely predictable. For example, = or a linear combination of past (known) values of . = (= , with = infinite lag polynomial) < (assumption for the stability of polynomial, \" square summability \") only depend on j (weights of innovations are not time dependent) = 1 (a convenient assumption) t ~ WN(0, 2) ( t independent and uncorrelated with ) Thus, y t is a linear combination of innovations over time plus a deterministic part. A stationary process can be decomposed into a sum of two parts, one represented as an MA( ) and the other a deterministic \"trend.\" Example : Let x t = y t - . (x t = MA() part) Then, check moments: 022 2 2 1 122 2 1 1 2 2 1 102 2 02 2 20 ...) (...) ...)( [( ] ,[. ] [ ] [.0] [ ] [ ][ kk k j j jjt jt jt t t t jt tjjjjt j t t E xxEE xE Xt is a covariance stationary process. \u00b6 Remark: This old theorem is the backbone of time series analysis. We will approximate the Wold infinite lag polynomial with a ratio of two finite lag pol ynomials. This approximation is the basis of ARMA modeling. Autoregressive (AR) Process We model the condition al expectation of , E[|It-1], as a function of its past history. We assume follows a WN(0, 2). The most common models are AR models. An AR(1) model involves a single lag, while an AR(p) model involves p lags. Then, the AR( p) process is given by: = + 1 + 2 +... +p + , ~ WN. Using the lag operator, we write the AR( p) process: L with (L) = 1 - 1 L - 2 L2 - ... - p Lp We can look at an AR( p) process: = + 1 + 2 +... +p + , as a stochastic (linear) difference equation (SDE). With difference equations we try to get a solution -i.e., given some initial cond itions/history, we know the value of for any - and, then, we study its characteristics (stability, long-run value, etc.). The solution to a difference equation can be written as a sum of two solutions: 1) Homogeneous equation (the part that only depends on the 's): = 1 + 2 +... +p (set + = 0) 2) A particular solution to the equation. Once we get a solution, we study its st ability. We want a stable one. We get a solution to the simple case, the AR(1) process. = + 1 + , ~ WN. We use the backward substitution method: = + 1 ( + 1 + ) + = (1 + 1) + + + 1 = (1 + 1) + ( + 1 + ) + + 1 = (1 + 1 + ) + + + 1 + = (1 + 1 + +... + ) + + The solution is a function of , the whole sequence , , ..., and the initial condition . The effect of \"dies out\" if | 1|< 1. The stability of the solution is crucial. With a stable solution, Y t does not explode. This is good: We need well defined moments. It turns out that the st ability of the equation depends on th e solution to the homogenous equation. In the AR(1) case: = 1 with solution = If |1|< 1, never explodes, as . In this case, in the solution to the AR(1) process, the effect of \"dies out\" as . We can analyze the stability fr om the point of view of the roots , z, of the characteristic equation of the AR( p) process, (L) = 0. For the AR(1) process (z) = 1 - 1 z = 0 |z| = 1/|1| > 1. That is, the AR(1) process is stable if the root of (z) is greater than one (also said as \" the roots lie outside the unit circle \"). This result generalizes to AR( p) process. For example, for the AR(3) process = 1 + 2 + 3 + , (z) = 1 - 1 z - 2 z2 - 3 z3 the roots, z1, z2 & z3, should lie outside the unit circle . For an AR (p), we need the roots of (z) to be outside the unit circle. For the AR(2), = 1 + 2 . We need the roots of (z) to be outside the unit circle. The characteristic polynomial of the AR(2) can be written as: (z) = 1 - ( 1+ 2) z - 1 2 z2 = (1 - 1 z) (1 - 2 z) = 0 where 1 = 1+ 2, and 2 = 1 2. (1 & 2 are eigenvalues or characteristic roots .) If |1| < 1, and | 2| < 1, the roots lie outside the unit root stationary. Then, some implications for 1 & 2: | 1 + 2 |< 2 |1| < 2 | 1 2| < 1 |2| < 1 Summary: We say the process is globally (asymptotically ) stable if the solution of the associated homogenous equation tends to 0, as t . Theorem A necessary and sufficient condition for global asymptotical stability of a p th order deterministic difference equation with constant coefficients is that all roots of the associated lag polynomial equation (z)=0 have moduli strictly more than 1. For the case of real roots, m oduli means \"absolute values.\" AR(1) Process = + , ~ WN. Recall that in a previous example, under the stationarity condition | | < 1, we derived the moments: E[ ] = = 0 (assuming 1) Var[ ] = 0 = 2/(1 - 12) (assuming | | < 1) 1 = E[ ] = E[( + ) * ] = 0 2 = E[ ] = E[( + ) * ] = E[ ] = 1 = 2 0 3 = E[ ] = E[(1 + ) * ] = E[ ] = 1 2 = 13 0 = 1 = 1k 0 Now, we derive the autocorrelation: , 1 - 2 If the process is stationary ( 0 1, 1 = 1 2 Remark: The ACF decays with k. When we plot against k, we plot also 0 which is 1. Note that when 1 = 1, the AR(1) is non-stationary, 1, for all k. The present and the past are always correlated! Again, when | |< 1, the autocorrelations do not explode as k increases. There is an exponential decay towards zero. Note: - when 0 < < 1 All autocorrelations are positive. - when 1 < < 0 The sign of shows an alternating pa ttern beginning a negative value. AR(1) Process - Stationar ity & ACF: Simulations We simulate and plot three MA(1) processes, with standard normal -i.e., =1: = 0.5 + = -0.9 + = 2 + Below using the arima.sim R function we 3 AR process. We start with the plot of = 0.5 + with 200 simulations. > plot(arima.sim(list(order = c(1,0,0), ar = 0.5), n=200), ylab=\"Simulated Series\", main=(expression(AR(1)~~~phi==+.5))) Note: The process 1 > 0 is smoother than the ones with 1 < 0. The process with | 1| > 1, explodes! Below, we compute and plot the ACF for the the two stable simulated process. 1) = 0.5 + sim_ar1_5 <- arima.sim(lis 'sim_ma1_5', by lag 0 1 2 3 4 5 6 7 8 9 10 11 12 'sim_ma1_9', by lag 0 1 2 3 4 5 6 7 8 9 10 11 12 Real Data Example: A process with | 1|< 1 (actually, 0.065) is the monthly changes in the USD/GBP exchange rate. Below we plot its corresponding ACF: Below we plot the monthly changes in the USD/ GBP exchange rate. Stationary series do not look smooth: Example: A process with 1 (actually, 0.99) is the nominal USD/GBP exchange rate. Below, we plot the ACF, it is not 1 all the time, but its d ecay is very slow (after 30 months, it is still .40 correlated!): Below we plot the nominal USD/GBP exchange rate. Stationary series look smooth, smooth enough that you can clearly spot trends: AR(1) Process - Stationarity & ACF t~ Moments: E[ ] = / (1 - - ) = 0 (assuming + 2 1) Var[ ] = 2/(1 - 12 - 22) (assuming 12+ 22 < 1) Autocovariance function = Cov[ , ] = E[( + 2 + ) ] = E[ ]+ 2 E[ ] + E[ ] = 1 + 2 2 + E[ ] We have a recursive formula: (k=0) 0 = 1 + 2 2 + E[ ] = 1 + 2 2 + 2 (k=1) 1 = 1 0 + 2 1 + E[ ] = 0 + 2 1 1 = [/(1 - 2)] 0 (k=2) 2 = 1 + 2 0 + E[ ] = 1 + 2 0 2 = [12 0/(1 - 2)] + 2 0 = [ 12/(1 - 2) + 2] 0 Replacing 1 and 2 back to 0: 0 = [12 /(1 - 2)] 0 + [2 12/(1 - 2) + 22] 0 + 2 = 2(1 - 2) (1 - 2) 12(1 + 2) + 22(1 - 2) |2|<1 Dividing the previous formulas by 0, we get the ACF: / 0 = 1 1 + 2 2 + E[ ]/ 0 (k=0) 0 = 1 (k=1) 1 = /(1 - 2) (k=2) 2 = 1 + 2 0 = 12/(1 - 2) + 2 (k=3) 3 = 2 + 2 1 = = 13/(1 - 2) + 1 2 + 21 /(1 - 2) Remark: Again, we see expone ntial decay in the ACF. From the work above, we need: 1 + 2 1. 12+ 22 < 1. | 2|< 1. AR(p) Process - VAR(1) Representation With AR process with more lags than the AR (1) process, it is complicated to determine stationarity by looking at the i's coefficients. Stationarity conditions can be derived in a si mplified way by rewriting an AR(p) as AR(1) process. For example, the AR(2) process: 11 can be written in matrix form as an AR(1): 0 10 0 The AR(2) in matrix AR(1) form is called Vector AR( 1) or VAR(1). We can derived a matrix lag polynomial A(L): . AR(2) Process - VAR(1) & Stationarity If we can write an MA( ) representation: Note: Recall the expansion: Checking that [ I - AL] is not singular, same as checking that A j does not explode. The stability of the system (solution) can be determined by the eigenvalues of A. That is, get the i's and check if |i|<1 for all i. 12 10 | |det1 2 1 -(1 2 2 1 Solution to quadratic equation: i = 1 12 Stability and stationary: | i |< 1. roots of (z) outside unit circle. For the AR(2) process, we have alre ady derived some relations between i's and i's: 2 |||2|1 1 |||1 |2 We derived auto covariance function, (k), before, getting a recursiv e formula. Let's write the first autocovariances: (k=0) 0 = 1 + 2 2 + 2 (k=1) 1 = [/(1 - 2)] 0 (k=2) 2 = [12/(1 - 2) + 2] 0 With |2|< 1, we get well defined (1), (2) & (0). The VAR(1) has a nice property: The VAR(1) is Markov -i.e., forecasts depend only on today's data. It looks complicated, but it is straightforwar d to apply the VAR formulation to any AR( p) processes. We can also use the same eigenvalu e conditions to check th e stationarity of AR( p) processes. AR Process -Stationarity & Ergodicity Theorem : The linear AR(p) process is strictly stati onary and ergodic if and only if the roots of (L) are |r j|>1 for all j, where |r j| is the modulus of the complex number r j. Note: If one of the r j's equals 1, (L) (& ) has a unit root -i.e., (1)=0. This is a special case of non-stationarity . Recall (L)-1 produces an infinite sum on the t-j's. If this sum does not explode, we say the process is stable . AR Process - Dynamic Multiplier & IRF If the process is stable, we can calculate /. / = How much is affected today by an innovation ( ) t j periods ago. When expressed as a function of j, we call this dynamic multiplier . Accumulated over time it is the impulse response function (IRF). The dynamic multiplier measurers the effect of an innovation, , (economist like to call the 's, \"shocks \") on subsequent values of : That is, the first derivative on the \"Wold representation\" - i.e., a stationary process repr esented as an MA process: / = / = j. where j's are the coefficient of MA representation. For an AR(1) process: / = / = j. That is, the dynamic multiplier fo r any linear stochastic difference equation (SDE) depends only on the length of time j, not on time t. The impulse-response function (IRF) is an accumulation of the sequence of dynamic multipliers, as a function of time from the one time change in the innovation, . Usually, IRFs are represented with a graph, th at measures the effect of the innovation, , on over time: / + / + / +...= j + j+1+ j+2+... Once we estimate the AR, MA or ARMA coefficients, we draw an IRF. Example : AR(1) process: = + + , ~ WN. The AR(1) is stable if | | < 1 stationarity condition. We invert the AR(1) to get an MA( ): 1/11 Then, * + + . Under the stationarity condition, we calculate the dynamic multiplier: / = 1 j Accumulated over time, after J pe riods, the effect of shock t at t+J is: IRF(at t+J) = 1 Suppose 1 = 0.40. Then, / = = 0.40 / = = 0.402 / = = 0.40J After 5 periods, the accumulated effect of a shock today is: IRF(at t+5) = 0.40 0.405 = 0.65984. \u00b6 AR Process - Causality The AR( p) model: , ~ WN. where 1.... Then, , an MA() process! But, we need to make sure that we can invert the polynomial (L). When (L) 0, we say the process y t is causal (strictly speaking, a causal function of {t}). Definition: A linear process {y t} is causal if there is a Definition: A linear process {y t} is causal if there is a 1 with with . Example : AR(1) process: , where 11 Then, y t is causal if and only if: |1| < 1 (same condition as stationarity) or the root r 1 of the polynomial (z) = 1 1 z satisfies |r 1|>1. Question: How do we calculate the 's an AR( p)? A: Matching coefficients ( =0): 1 111 1, 0 AR Process - Estimation and Properties We go back to the general AR (p). Define 1.... .... Then the model can be written as The OLS estimator is Properties: - Using the Ergodic Theorem, OL S estimator is consistent. - Using the CLT, OLS estimat or is asymptotically normal. asymptotic inference is the same. The asymptotic covariance matrix is estimated ju st as in the cross-s ection case: The sandwich estimator. ARMA Process A combination of AR( p) and MA( q) processes produces an ARMA( p, q) process: . Usually, we insist that (L) 0, (L) 0 & that the polynomials (L), (L) have no common factors . This implies it is not a lower order ARMA model. ARMA Process - Common Factors It is possible to reduce the orde r of an ARMA st ructure if the (L) and (L) lag polynomials have common factors . Example : Suppose we have the following ARMA(2, 3) model with 1.6 .3 11.4 .9.31.6 .31 This model simplifies to: 1 an MA(1) process. \u00b6 ARMA Process - Representation An ARMA process can be rewritten as: - Pure AR Representation: - Pure MA Representation: Special ARMA( p, q) cases: - p = 0: MA( q) - q = 0: AR( p). ARMA: Stationarity, Causality and Invertibility Theorem: If (L) and (L) have no common factors, a (unique) stationary solution to exists if and only if ||1 1 ...0. This ARMA( p, q) model is causal if and only if ||1 1... 0. This ARMA( p, q) model is invertib le if and only if ||1 1...0. Note: Real data cannot be exactly modeled using a finite number of parameters. We choose p, q to create a good approximated model. Lecture 9 - ARIMA Models - Identification & Estimation ARMA Process We defined the ARMA( p, q) model: The mean does not affect the order of the ARMA. Then, if 0 , we demean the data: . Then, xt is a demeaned ARMA process. In this lecture, we will study: - Identification of p, q. - Estimation of ARMA( p, q) - Non-stationarity of x t. - Differentiation issues - ARIMA( p, d, q ) - Seasonal behavior - SARIMA( p, d, q )S Autocovariance Function (Again) We define the autocovariance function: For an AR( p) process, WLOG with =0 (or demeaned y t), we get: . 12.... Notation: (k) or k are commonly used. Sometimes, (k) is referred as \" covariance at lag k.\" The (t-j) determine a system of equations: 0 123 . 1 012 . 1 2 101 .... 2 This a pxp system of equations. Using linear al gebra, we can write the system as: = where is a pxp matrix of autocovariances, with 0 on the diagonal; is the px1 vector of AR(p) coefficients; and is the px1 vector of autocovariances Example : AR(1) model: = 1 + , ~ WN. Then, the autocovariance function is: 0 = = Var[] = 2 /(1- 12) 1 = = E[( + ) * ] = 1 0 2 = = E[( + ) * ] = 1 1 = 12 0 3 = = E[( + ) * ] = 1 E[ ]] = 13 0 .... = 1 = 1k 0 If || < 1, exponential decay. Under stationarity, moments are constant. That is, Var[ ]Var[ 0. \u00b6 Example : MA(1) process: = , ~ WN. Then, the autocovariance function is: 0 = 2 + 12 2 = 2 (1+ 12) 1 = E[ ] = E[(1 + ) (1 + )] = 1 2 ..... = E[ ] = E[(1 + ) (1 + )] = 0 (for k>1) That is, for | k| > 1, = 0. \u00b6 Example : AR MA(1,1) process: = 1 + + 1 , ~ WN. 11 1 1 1 11 0 11 1 1 1 1111 1 11 11111 Similarly: 1 1 0 1 1 1 01 Two equations for 0 and 1. Solving for 0: 0 011 1 11 01 1 1 1 01 1 1 1 1 1 1 Continuing the process: 2 11 1 1 In general: 1 11, 1 If | |<1, exponential decay. \u00b6 Note: If stationary, ARMA(1,1) and AR(1) show exponential decay. Difficult to distinguish one from the other by looking at the autocovariance functions. Autocorrelation Function (ACF) Now, we define the auto correlation function (ACF): () (0)covariance at lag variance The ACF lies between -1 and +1, with 0 1. Dividing the autocovariance system by (0), we get: 0 1 1 1 0 2 1 2 0 1 2 Or using linear algebra: = These are \"Yule-Walker\" equations, which can be solved numerically. Autocorrelation Function (ACF) - Estimation & Correlogram Estimation : Easy: Use sample moments to estimate (k) and plug in formula: Then, we plug the in the Yule-Walker equations and solve for : = The sample correlogram is the plot of the ACF against k. As the ACF lies between -1 and +1, the correlogram also lies between these values. Distribution : For a linear, stationary process, with large T, the distribution of the sample ACF, is approximately normal with: r N(, V/T), V is the covariance matrix. Under H 0: k = 0 for all k > 1. r N(0, I/T) Var[r( k)] = 1/ T. Under H 0: k = 0 for all k., the SE = 1/ 95% C.I.: 0 \u00b1 1.96 * 1/ Then, for a white noise sequence, approximately 95% of the sample ACFs should be within the above C.I. limits. Note: The SE = 1/ are sometimes referred as Bartlett's SE . Example : Sample ACF for an AR(1) process: Under stationarity (constant moments, in particular, Var[ ] = Var[] = 0): = 0, 1, 2, ... If | 1 |< 1, the ACF will show exponential decay. Suppose 1 = 0.4. Then, 0 = 1 1 = 0.4 2 = 0.42 = 0.16 3 = 0.43 = 0.064 4 = 0.44 = 0.0256 0.4k We simulate an AR(1 ) series with with using ACF for an MA(1) process: (0) = 1 () = 1/(1+ 12), for = 1, -1 () = 0 for | | > 1. After k = 1 -i.e., one lag- the ACF dies out. Suppose 1 = 0.5. Then, 0 = 1 1 = 0.4 = 0 for | | > 1. We simulate an MA(1) series sim series The goal of these criteria is to provide us w ith an easy way of comparing alternative model specifications, by ranking them. General Rule: The lower the IC, the better the mode l. For the previous IC's, then choose model to AIC J, BIC J, or HQIC. Some remarks about IC's: - IC's are not test statistics. They do not test a model.\\ Example : Sample ACF for an MA( q) process: (0) = E[ ]] = 2 (1 + 12 + 22 +...+ q2) (1) = E[ ] = 2 (1 + 2 1 + 3 2 +...+ q q-1) (2) = E[ ]] = 2 (2 + 3 1 +...+ q q-2) (q) = q In general, with 1. 0 otherwise. Then, 0 otherwise. For an MA(3): Then, 0 = 1 1 = 2 = 3 = = 0 f o r | | > 3. Suppose 1 = 0.5; 2 = 0.4; 3 = 0.2. Then, 0 = 1 1 = (0.5+0.4*0.5+0.1*0.4)/(1 + 0.5 2+ 0.42+ 0.12) = 0.5211 +0.52 +0.42+ 0.12) = 0.0704 = 0 f o r | | > 3. Plot of simulated series and ACF > sim_ma3_05 <- arima.sim(list(order=c(0,0,3), ma=c(0.5, 0.4, 0.1)), n=200) : Sample ACF for an ARMA(1,1) process: 1 From the autocovariances, we get 0 1 1 1 1 1 1 1 1 1 1 Then, 1 If | | < 1, exponential decay. Sim ilar pattern to AR(1). The ACF for an ARMA(1,1): 1 Suppose = 0.4, 1 = 0.5. Then, 0 = 1 1 . . . ACF > acf_p Autocorrelations of series 'lr_p', by lag 0 1 2 3 4 5 6 7 8 9 10 14 15 16 17 18 19 20 21 22 first correlati on, correlations are small. However, many are significant, not strange result when T is large. \u00b6 Example : US Monthly Changes in Stock Dividends (1871 - 2020, T=1,795) > acf_d Autocorrelations of series 'lr_d', by lag 0 1 2 3 4 5 6 7 8 9 10 14 15 16 17 18 19 20 21 22 32 months! Note: Correlations are positive for almost 1.5 ye ars, then correlations become negative. \u00b6 ACF - Joint Significance Tests Recall the Ljung-Box (LB) statistic as: 2 The LB test can be used to determine if the first m sample ACFs are jointly equal to zero. Under H0: 1= 2=...= 0, the distribution . Example : LB test with 20 lags for US Monthly Returns and Changes in Dividends (1871 - 2020) > 2.2e-16 Reject H 0 at 5% level. Joint significant first 20 correlations. 2.2e-16 Reject H 0 at 5% level. Joint significant first 20 correlations. \u00b6 Partial ACF (PACF) The ACF gives us a lot of information about th e order of the dependence when the series we analyze follows a MA process: The ACF is zero after q lags for an MA( q) process. If the series we analyze, however, follows an ARMA or AR, the ACF alone tells us little about the orders of dependence: We onl y observe an exponential decay. We introduce a new function that behaves like the ACF of MA models, but for AR models, namely, the partial autocorrelation function (PACF). The PACF is similar to the ACF. It measur es correlation between observations that are k time periods apart, after controlling for correlations at intermediate lags. Intuition: Suppose we have an AR(1): . Then, 2 = 2 0 The correlation between y t and y t-2 is not zero, as it would be for an MA(1), because y t is dependent on y t-2 through y t-1. Suppose we break this chain of dependence by removing (\"partiali ng out\") the effect . Then, we consider the correlation between [ - 1 ] and [ - 1 ] -i.e, the correlation between y t and y t-2 with the linear dependence of each on y t-1 removed: 2 = Cov( - , - 1 ) Cov(, - ) = 0. Similarly, = Cov( , - ) = 0 for all > 1. Definition: The PACF of a stationary time series { } is 11 = Corr(, ) = (1) hh = Corr( - E[y t|It-1], - E[|It-1]) for h = 2, 3, .... This removes the linear effects of , , .... . The PACF hh is also the last coefficient in the best linear prediction of given , , .... . Estimation by Yule-Walker equation, using sample estimates: a recursive system, where h = (h1, h2 , ..., hh) and is matrix. A recursive algorithm, Durb in-Levinson, can be used. Also OLS can be used. Partial ACF - AR(p) Example : AR( p) process: ... | ... | ... Then, hh = h if 1 h p = 0 otherwise After the pth PACF, all remaining PACF are 0 for AR( p) processes. \u00b6 The plot of the PACF is called the partial correlogram . R Note: The R function pacf computes 1 2 3 4 5 6 7 8 9 14 15 16 17 18 19 20 21 22 be calculated by h regressions, each one with h lags. The hh coefficient is the hth order PACF. Partial ACF - MA(q) Following the analogy that PACF for AR processes behaves like an ACF for MA processes, we will see exponential decay (\" tails off \") in the partial correlogram fo r MA process. Similar pattern will also occur for ARMA(p, q) process. Example : We simulate an MA(1) process with = 0.5. sim_ma1 For an ARMA processes, we will see exponential decay (\" tails off \") in the partia l correlogram. Example : We simulate an ARMA(1) process with 1= Monthly Returns (1871 - 2019, T=1,795) pacf_p <- acf(lr_p) # pacf: that PACF > pacf_p Partial autocorrelations of series 'lr_p', by lag 1 2 3 4 5 6 7 8 9 10 22 pacf(lr_p) the exception of th e first partial correla tion, partial correlations are small, though, again, some are significant. \u00b6 Example : US Monthly Stock Dividends (1871 - 2020, T = 1,795) pacf_d <- pacf(lr_d) > pacf_d Partial autocorrelations of series 'lr_d', by lag 1 2 3 4 5 6 7 8 9 10 13 14 15 16 17 18 19 returns. > pacf(lr_d) Note: Partial correlations are positive for almost 6 lags, then become small. \u00b6 Non-Stationary Time Series Models The ACF is as a rough indicator of whether a trend is present in a series. A slow decay in ACF is indicative of highly correlated data, which suggests a true unit root process, or a trend stationary process. Formal tests can help to determine whether a sy stem contains a trend and whether the trend is deterministic or stochastic (unit root). We will analyze two situations faced in AR MA models: (1) Deterministic trend - Simple model: y t = + t + t. - Solution: Detrending -i.e., regress y t on t. Then, keep residuals for further modeling. (2) Stochastic trend - Simple model: y t = c + y t1 + t. - Solution: Differencing -i.e., apply = (1 - L) operator to y t . Then, use yt for further modeling. Example : Plot of US Monthly Prices and Dividends (1871 - 2020) Non-Stationary Time Series Mo dels - Deterministic Trend Suppose we have the following model: = + t + . = - = t - (t - 1) + - = + - E[] = {yt} will show only temporary departures from the trend line + t. This type of model is called a trend stationary (TS) model. If a series has a deterministic time trend, then we simply regress on an intercept and a time trend ( t = 1, 2, ..., T ) and save the residuals. The residuals are the detrended yt series (= without the influence of t). If is stochastic, we do not necessarily get stationary series, by detrending. Many economic series exhibit \"e xponential trend/growth\". They grow over time like an exponential function over time instea d of a linear function. In this cases, it is common to work with logs ln( ) = + t + . The average growth rate is: E[ ln()] = We can have a more general model: Estimation: - OLS. - Frish-Waugh method (a 2-step method): (1) Detrend , get the residuals (= without the influence of t) (2) Use residuals to estimate the AR( p) model. Example: We detrend U.S. Stock Prices T <- length(x_P) # length of series trend <- c(1:T) # create trend det_P <- lm(x_P ~ Not very appealing series. We still see trends . Now, we detrend U.S. Stock Prices adding a squared trend. trend2 det_P2 <- lm(x_P with linear and quadratic trends\") Still, trends are still observed. We detrend Log U.S. Stock Prices adding a square trend l_P <- log(x_P) det_lP <- lm(l_P with quadratic trends\") Remark: The second detrended series , with linear and quadr atic trends looks be tter, but we still see trends in the graph. \u00b6 Non-Stationary Time Series Models - Stochastic Trend The more modern approach is to consider trends in time series as a variable. A variable trend exists when a trend changes in an unpredictable way. Therefore, it is considered as stochastic . Recall the AR(1) model: = c + 1 + . As long as | | < 1, everything is fine, we have a stati onary AR(1) process: OLS is consistent, t- stats are asymptotically normal, etc. Now consider the extreme case where 1 = 1, = c + + . Where is the (stochastic) trend? No t term. Let us replace recursively the lag of y t on the right-hand side: = + + = + ( + + ) + ... = y 0 + t + A constant (y0), a determinist trend (t ) and an accumulation of errors over time ( ) appear in the recursive formula tion. This is what we call a \" random walk with drift \". The series grows with t. Each t shock represents a shift in the intercept. All values of {t} have a 1 as coefficient each shock never vanishes (permanent). We remove the trend by differencing = (1 - L) = + Note: Applying the (1 L) operator to a time series is called differencing Example: We difference U.S. Stock Prices , U.S. Stock Prices\") Remark: The trend is gone from the graph. \u00b6 y t is said to have a stochastic trend (ST), since each t shock gives a permanent and random change in the conditional mean of the series. For these situations, we use Autoregressive Integrated Moving Average (ARIMA) models. Question: Deterministic or Stochastic Trend? They appear similar: Both lead to growth over time. The difference is how we think of t. Should a shock today affect ? - TS: = c + (t+1) + does not affect . - ST: = c + yt + = c + [c + + ] + affects . (In fact, the shock will have a permanent impact.) ARIMA(p,d,q) Models For p, d, q 0, we say that a time series {y t} is an ARIMA (p,d,q) process if w dyt = (1 L)d yt is ARMA( p,q). That is, Applying the (1 L) operator to a time series is called differencing . Notation: If y t is non-stationary, but dyt is stationary, then y t is integrated of order d, or I (d). A time series with unit root is I(1). A stationary ti me series is I(0). Examples : Example 1: RW: = + . yt is non-stationary, but (1 L) = white noise! Now, ~ ARIMA(0,1,0). Example 2 : AR(1) with time trend: = + t + 1 + . is non-stationary, but = (1 L) = + t + 1 + - ( + (t-1) + 1 + = + 1 + - Now, ~ ARIMA(1,1,1). We call both process first difference stationary. \u00b6 Note: Example 1 : Differencing a series with a unit root in the AR part of the model reduces the AR order. Example 2: Differencing can introduce an extra MA st ructure. We introduced non-invertibility. This happens when we difference a TS series. Detrending should be used in these cases. In practice: A root near 1 of the AR polynomial differencing A root near 1 of the MA polynomial over-differencing In general, we have the following results: - Too little differenci ng: not stationary. - Too much differencing: ex tra dependence introduced. Finding the right d is crucial. For identifying preliminary values of d: - Use a time plot. - Check for slowly decaying (persistent) ACF/PACF. ARIMA Models: Unit Roots 1? Example levels (1871-2020) acf_P <- acf(x_P) > acf_P Autocorrelations of series 'x_p', by lag 0 1 2 3 4 5 6 7 8 9 10 14 15 16 17 18 19 20 21 22 like 1 1. \u00b6 Example 2 : Monthly Interest Rates (1871-2020) acf_i <- acf(x_i) > acf_i Autocorrelations of series 'x_i', by lag 0 1 2 3 4 5 6 7 8 9 10 14 15 16 17 18 19 20 21 22 1 1. \u00b6 ARIMA Models - Random Walk A random walk (RW) is defined as a process where the cu rrent value of a vari able is composed of the past value plus an error term defined as a white noise (a normal variable with zero mean and variance one). A Random Walk is an ARIMA(0,1,0) process Popular model. Used to explain the behavior of financial assets, unpredictable movements (Brownian motions, drunk persons). It is a special case (limiting) of an AR(1) process: a unit-root process. Implication: E[y t+1|It] = y yt is absolutely random. Thus, a RW is nonstationary, a nd its variance increases with t. Examples : Two simulated RW T_sim <- 200 # Sample size for simulation u <- rnorm(200) # Draw T_sim normally distributed errors y_sim <- matrix(0,T_sim,1) # V ector to collect simulated data phi <- 1 # Change to crea te different correlation patterns a <- 2 # Time index for observations mu <- 0 # RW Drift (mu = 0, no drift) while (a <= T_sim) { y_sim[a] = mu + rho * y_sim[a-1] + u[a] # y_sim simulated autocorrelated values ) are clear in both graphs. \u00b6 ARIMA Models - Random Walk with Drift Change in is partially deterministic ( ) and partially stochastic. It can also be written as = + t + t has a permanent effect on the mean of y t . Recall the difference between condi tional and unconditional forecasts: E[ ] = + t (Unconditional forecast) E[y t+s |] = + s (Conditional forecast) ARIMA Models: Box-Jenkins An effective procedure for build ing empirical time series models is the Box-Jenkins approach, which consists of three stages: (1) Model specification or iden tification (of ARIMA order) (2) Estimation (3) Diagnostics testing. Two main approaches to (1) Identification. - Correlation approach , mainly based on ACF & PACF. - Information criteria , based on the maximized likelihood (x2) plus a penalty function. For example, model select ion based on the AIC. Question: We have a family of ARIMA models, indexed by p, q, and d. How do we select one? Box-Jenkins Approach provides a method to answer the question. 1) Make sure data is stationary -c heck a time plot. If not, differentiate. 2) Using ACF & PACF, guess small values for p & q. 3) Estimate order p, q. 4) Run diagnostic tests on residuals. Are they white noise? If not, add lags ( p or q, or both). If order choice not clear, use AIC, AIC Correct ed (AICc), BIC, or HQC (Hannan and Quinn (1979)). Value parsimony. When in doubt , keep it simple (KISS). ARIMA Models: Identification - Correlations Correlation approach. Basic tools: sample ACF and sample PACF. - ACF identifies orde r of MA: Non-zero at lag q; zero for lags > q. - PACF identifies orde r of AR: Non-zero at lag p; zero for lags > p. - All other cases, try ARMA( p, q) with p > 0 and q > 0. Summary: For p>0 and q>0. AR(p) MA(q) ARMA( p, q) ACF Tails off 0 after lag q Tails off PACF 0 after lag p Tails off Tails off Note: Ideally, \"Tails off\" is exponential decay. In practice, in these cases, we may see a lot of non-zero values for the ACF and PACF. ARIMA Models: Identification - Examples Example 1 : Monthly US Returns (1871 - 2020). Note: ARMA(1,1), MA(1), AR(2)? Example 2 : Monthly US Dividend Changes (1871 - 2020). Note: Not clear: Maybe long a ARMA(p,q) or needs differencing? \u00b6 Example 3 : Monthly Log Changes in Oil Prices (1973 - 2020). Note: MA(1), AR(4)? \u00b6 Example 4 : Monthly Log Changes in Gold (1973 - 2020). Note: No clear ARMA structure. \u00b6 ARIMA Model: Identification - IC It is difficult to identify an ARMA model us ing the ACF and PACF. It is common to rely on information criteria (IC). IC's are equal to the estimated variance or the log-like lihood function plus a penalty factor, that depends on k. Many IC's: - Akaike Information Criterion (AIC) AIC = -2 * (ln L - k ) = -2 ln L + 2 * k if normality AIC = T * ln(e'e/T) + 2* k (+constants) - Bayes-Schwarz Informati on Criterion (BIC or SBIC) BIC = -2 * ln L - ln(T) * k if normality AIC = T * ln(e'e/T) + ln(T) * k (+constants) - -2*(ln L - k [ln(ln( T))] if normality AIC = T * ln(e'e/T) + 2 k [ln(ln( T))] (+constants) It is very common to compute the IC's under norm ality (it is the default setting in R and almost all other packages). Recall that under nor mality, we write the Likelihood function ,, 2ln21 constant Since we compare different ARIMA models, using th e same data, the constants play no role in our decision. They can be ignored. Then, - AIC = T * ln() + 2 * k - BIC = T * ln() + ln(T) * k - HQIC = T * ln() + 2 * k * [ln(ln( T))] The goal of these criteria is to provide us w ith an easy way of comparing alternative model specifications, by ranking them. General Rule: The lower the IC, the better the mode l. For the previous IC's, then choose model to AIC J, BIC J, or HQIC. ARIMA Model: Identification - Remarks Some remarks about IC's: - IC's are not test statistics . They do not test a model. - They are used for ranking. The raw value tends to be ignored. - They have two components: a goodness of fit component -based on lnL- and a model complexity component -the penalty based on k. - Different penalties, different IC's. - Some authors scale the IC's by T. Since raw values tend to be irre levant, this is not an issue. We would like these statistics -i.e., the IC's- to have good properties. For example, if the true model is being considered among many, we want the IC to select it. This can be done on average (unbiased) or as T increases (consistent). Some results regarding AIC and BIC. - AIC and Adjusted R 2 are not consistent. - AIC is conservative -i.e., it tends to over-fit: kAIC too large models. - In time series, AIC selects the model that mi nimizes the out-of-sample one-step ahead forecast MSE. - BIC is more parsimonious than AIC. It pe nalizes the inclusion of parameters more ( k BIC kAIC). - BIC is consistent in autoregressive models. - No agreement which criteria is better. ARIMA Model: Identification - Small Sample Modifications There are modifications of IC to get better finite sample behavior , a popular one is AIC corrected, AICc, statistic: ln AICc converges to AIC as T gets large. Using AI Cc is not a bad idea. For AR( p) models, other AR-specific criteria are possi ble: Akaike's =Minimum IC): Calculate the BIC for different p's (estimated first) and different q's. Select the best mode l -i.e., lowest BIC. Note: Box, Jenkins, and Reinsel (199 4) proposed using the AIC above. ARIMA Model: Identification - In practice Example : We compute, for monthly US Returns (1871 - 2020), annan and Rissannen (1982)'s minic, based on AIC. Minimum Information Criterion Lags MA 0 MA 1 MA 2 MA 3 MA 4 MA 5 AR 0 -6403.59 that se lect automatically the \"best\" ARIMA package forecast ) minimizes AIC, AICc (default) or BIC. > armaselect(lr_p) # shows th e best 10 models according to BIC p q sbc [1,] 2 0 -11644.79 [2,] 1 0 ic=\"bic\", trace=TRUE) function approximates models. Fitting models using approxima tions to speed things up... ARIMA(2,0,2) with non-zero ARIMA(0,0,0) with with with with with -6523.415 ARIMA(0,0,1) with zero -6534.284 re-fitting the model(s) without The function auto.arima does not try a lot of models, it tries to keep the p+q 5. \u00b6 Remark: Do not take the results from auto.arima or armaselect or minic as the final model. We still need to check th e residuals are WN. Script in R to select model using arima function. p <- 6 # set max order for AR part: p-1 q <- 6 # set p, q, AIC: AIC in last column j <- 0 k <- 1 while (j < p) { i <- 0 while (i < q) { mod_j # extract aic from arima fit model i <- i + 1 k <- k + 1 } j <- j + 1 } aic_m # Print all the results AR(i), MA(j), AIC min_aic <- min(aic_m[,3]) arr.i nd=TRUE) # Prints the row ARIMA Model: Identification - Final Remarks There is no agreement on which criteria is best. Th e AIC is the most popular, but others are also used. Asymptotically, the BIC is consistent -i.e., it selects the true model if , among other assumptions, the true model is among the candidate models considered. The AIC is not consistent, gene rally producing too large a model, but is more efficient -i.e., when the true model is not in the candidate m odel set the AIC asymptotically chooses whichever model minimizes the MSE/MSPE. ARIMA Process - Estimation We assume: - The model order ( d, p and q) is known. Make sure y t is I(0). - The data has zero mean ( =0). If this is not reasonable, demean y . Fit a zero-mean ARMA m odel to the demeaned y t: Several ways to estimate an ARMA( p, q) model: 1) Maximun Likelihood Esimation (MLE). Assume a distribution, usually a normal distribution, and, then, do ML. 2) Yule-Walker for ARMA (p,q). Method of moments. Not efficient. 3) Innovations algorithm for MA(q). 4) Hannan-Rissanen algorithm for ARMA( p, q). ARIMA Process - Estimation: MLE Steps: 1) Assume a distribution for the errors. Typically, .i.i.d. normal, say: ~ i.i.d. N(0,2) pdf: exp 2) Write down the joint pdf for : f(, ..., ) = f() ... f() Note: We are not writing the joint pdf in terms of the 's, as a multiplication of the marginal pdfs because of the dependency in . 3) Get t. For the general stationary ARMA( p,q) model: 1 (if 0, demean .) { . ..., ,... , ). With an AR( p, q) model, we need p and q initial lags for and . We assume that initial conditions Y* =(, , ... , )' and = (,, )' are known. 6) The conditional log-like lihood function is given by L = lnL,,, ln2,, where ,,,,|,, is the conditional sum of squares (SS). Note: Usual Initial conditions: and 0. Numerical optimization problem, where initial values ( y*) matter. Example : AR(1) process: 1 , ~...0,. - Write down the joint likelihood for t L ,,2/exp First, we need to solve for t: 1 Let's take 0 1 1 1 1 1 1 Technical note: The joint likelihood is in terms of . We want to change the joint from t to yt, for this, we need the Jacobian |J|. || 10 0 11 0 00 11 Then, ,,|,,||,, - Then, the likelihood function can be written as L 1,,,,,|,, / / 1 , where ~ 0, 1 1. Then, L1,1 1 2/exp1 2 1 1 1 - Then, the log lik elihood function: L1, 2 1 1 1 11 1 1 where S*(1) is the conditional SS and S(1) is the unconditional SS. F.o.c.'s: 1, 1 = 0 1, = 0 Note: - If we neglect ln(1 12), then MLE = Conditional LSE. max 1, min1. - If we neglect both ln(1 12) and 1 1, then max 1, min 1. \u00b6 ARIMA Process - Estimation: Yule-Walker Yule-Walker q): Method of moments. Not efficient. Example : For an AR( p), we the Yule-Walker equations are 0 1 1 1 0 2 1 2 0 1 2 Method of Moments (MM) Estimation: Equate sample moments to population moments, and solve the equation. In this case, we use: & Then, the Yule-Walker estimator for is given by solving 1 1 1 1 1 2 1 2 1 1 2 \u00b6 Note: If 0, then, is nonsingular. If { Yt} is an AR( p) process, , 0, for kp. Thus, we can use the sample PACF to test fo r AR order, and we can calculate approximated C.I. for . Distribution: If is an AR( p) process, and T is large, ~ .0, 100(1)% approximate C.I. for j is / / Note: The Yule-Walker algorithm requires -1. For AR( p). The Levinson-Durbin (LD) algorithm avoids -1. It is a recursive linear algebra prediction algorithm. It takes advantage that is a symmetric matrix, wi th a constant diagonal (Toeplitz matrix). Use LD replacing with . Side effect of LD: automatic calculation of PACF and MSPE. Example 1 : AR(1) (MM) estimation: It is known that 1 = . Then, the MME of is Also, 2 is unknown: 11. \u00b6 Example 2 : Suppose we suspect an AR(3). We have estimated , , and . Then, 1 1 1 Suppose we get: = 0.5, = 0.4, and = -0.3. Then, solving for : = 10 . 5 0 . 4 0.5 1 0.5 0.4 0.5 1 0.5 0.4 0.30.555 0.511 0.777 Solving system with R: Rho <- matrix(c(1, 0.5, 0.4, 0.5, 1, 0.5, 0.4, 1), nrow=3) r <- c(.5, 0.4, -0.3) solve(Rho)%*%r. \u00b6 Example : process with MM estimation: 1 Again using the autocorrelati on of the series at lag 1, 1 1 110 , Choose the root satisfying the invert ibility condition. For real roots: 140 0.25 0.50.5 If 0.5, unique real roots but non-invertible. If ||0.5, unique real roots and invertible. We keep this one. \u00b6 Remarks - The MMEs for MA and ARMA models are complicated. - In general, regardless of AR, MA or ARMA models, the MMEs are sensitive to rounding errors. They are usually used to provide initial estimates needed for a more efficient nonlinear estimation method. - The moment estimators are not recommended for final estimation results and should not be used if the process is close to be ing nonstationary or noninvertible. ARIMA Process - Estimation: Yule-Walker - Remarks The MM estimations for MA and ARMA models are complicated. In general, regardless of AR, MA or ARMA models, th e MMEs are sensitive to rounding errors. They are usually used to provi de initial estimates needed for a more efficient nonlinear estimation method. The moment estimators are not recommended for fi nal estimation results and should not be used if the process is close to bei ng nonstationary or 1. Estimate high-order AR. 2. Use Step (1) to estimate (unobserved) noise t 3. Regress y t against , , ..., , , ... , 4. Get new estimates of t. Repeat Step (3). Example : We estimate a ARIMA(0,0,1) model for S&P 500 historical returns , using the arima function, part of the R forecast package. > arima(lr_p, method=\" ML\") aic = -6552.94. \u00b6 Note: Model was selected by AC F/PACF and confirmed with auto.arima function. Not a lot of structure in stock returns. Example : We use auto.arima function to estimate a model for DIS, GE, and IBM returns. > For both DIS & GE returns , we observe low AR(1) coeffici ent, aic = -6552.94. Note: Unpredictable! In gene ral, we do not find a lot of structure in stock returns; autocorrelations die out very quickly. This result is expecte d, given the Efficient Markets Hypothesis. \u00b6 Example : We use auto.arima function to estimate a model for changes in oil prices . > auto.arima(lr_oil) changes in oil prices, but mainly decaying at .30. Example : We use auto.arima function to estimate a model for monthly U.S. interest long rates (1871 - Note: We need to differentiate interest rates to get a stationary MA(2) model. \u00b6 ARIMA Process - Diagnostic Tests Once the model is estimated, we r un diagnostic tests. Usually, we check for extra-AR structure in the mean. We check visual plot s of residuals, ACFs, and the di stribution of residuals. More formally, we compute the LB test on the residuals . If we find extra-AR structure, we increase p and/or q. R Note: If we use arima () or auto.arima () functions, we can use the function checkresiduals () to do the plots and testing for us. Example: We check the MA(1) model for U.S. historical stock = 8, p-value = 0.01728 There seems to be more AR structure Model df: 2. Total lags used: 10 R Note: We check stationarity/invert ibility too -i.e., if the roots are inside the unit circle. In this case, an MA model, stationarity is not an issue (MA are stationary ), but invertibility is. We use the R function autoplot , part of the forecast package. Be aw are that autoplot plots the inverse roots, not the roots; in this case, a stationary AR (or invertible MA) process will have the inverse roots inside the unit circle. > autoplot(fit_arima_lr_p) Note: The inverse root is inside the and are real: invertible MA(1). \u00b6 Example: We change the model for U.S. stock returns . We estimate an ARIMA(1,0,5). -6540.4 p-value = 0.6359 The joint 10 lag autocorrelation not significant. Model df: 7. Total lags used: 10 Note: We still see some small autocorrelations different from 0. We check the stationarity and i nvertibility of ARIMA(1,0,5) model > autoplot(fit_arima_lr_p15) Note: All inverse roots inside the unit circle: stati onary and invertible. Notice that we have some roots on the MA part that are imaginary. \u00b6 Example: We check the fit of the ARIMA model for U.S. long interest p-value = 4.014e-05 Again, more AR or MA structure needed Model df: 2. Total lags used: 10 Note: We still see some large autocorrelations. change model (usually, increase p and/or q). But, we may in the presence of a series with regime change. We may need to focus on 2nd regime (post 1950s). We check the invertibility of ARIMA(0,1,2) model > autoplot(fit_arima_i) Note: All inverse roots are inside the unit circle. MA process is invertible. Notice that all roots are real. \u00b6 Example: We check the fit of the ARIMA(4,0,0) model selected by auto.arima for changes in Oil Prices. fit_arima_oil<- auto.arima(lr_oil) zero Q* = 2.72, df = 9, p-value = 0.84 No significant joint AR structure Model df: 4. Total lags used: 10 Note: Nothing significant. Happy with fit. Ready to forecast. We check the stationa rity of AR(4) model > autoplot(fit_arima_oil) Note: All (inverse) roots inside the un it circle -we have imaginary roots. \u00b6 Non-Stationarity in Variance Stationarity in does not imply in variance. However, non- stationarity in mean implies non-stationarity in variance. If the mean function is time dependent: 1. The variance, Var(y t) is time dependent. 2. Var[y t] is unbounded as t . 3. Autocovariance functions and ACFs are also time dependent. 4. If t is large with respect to the initial value y 0, then k 1. It is common to use variance stabilizing transformations: Find a function G(.) so that the transformed series G(y t) has a constant variance. Very popular transformation: 1) Log transformation: log Example : We log transform the monthly variable To tal U.S. Vehicle Sales data (1976: Note: The volatility is significantl y reduced by the log transformation. \u00b6 2) Box-Cox transformation: G where > 0, usually between 0 and 2 ( it can be estimated too). When =1, we have a linear ; when 0, we have a log transformation for . Example : We do a Box-Cox transformation of the mont hly variable Total U.S. Vehicle Sales data (1976: Jan - 2020: Sep), setting = 0.75: U.S. Vehicle Sales\") Note: Again, we see a reduced volatility. But, different s will have a different impact on volatility. \u00b6 Remarks: - Variance stabilizing transforma tion is only done for positive series , usually for nominal series (say, in USD total retail sales or units, like Total U.S. vehicle sales). - If a series has negative values, then, we need to add each value with a positive number so that all the values in the series are positive. - Then, we can search for any need for transformation. - It should be performed before any other analysis, such as differencing. - Not only stabilize the variance, but we tend to find that it al so improves the approximation of the distribution by the Normal distribution. Seasonal Time Series In time series, seasonal patterns (\" seasonalities \") can show up in tw o forms: additive and multiplicative. - Additive: The seasonal variation is independent of the level. - Multiplicative: The seasonal variat ion is a function of the level. Note: In the multiplicative case, the amplitude of the seasonal pattern is changing over time, while in the additive th e amplitude is constant. Examples: We simulate the two seasonal patterns, addi tive and multiplicative, with trend and no trend. A. With trend B. With no trend In the presence of seasonal patterns, we pro ceed to do seasonal adjust ments to remove these predictable influences, which can blur both the tr ue underlying movement in the series, as well as certain non-seasonal characteristics wh ich may be of interest to analysts. The type of adjustment depends on how we vi ew the seasonal pattern: Deterministic or Stochastic. Similar to the situation where the series had a tr end, once we determine the nature of the seasonal pattern, we filter the series -i.e., we remove the seasonal patter- to conduct further ARIMA modeling. When we work with a nominal series (not changes, say, USD total retail sales or total units sold), it is common to first apply a variance stabilizi ng transformation to the da ta, usually using logs. Seasonal Time Series - Types Two types of seasonal behavior: - Deterministic - Usual treatment: Build a deterministic function, , 0,1,2, We can include seasonal (means) dummies, for exam ple, monthly or quarte rly dummies. (This is the approach in Brooks' Chapter 10). Instead of dummies, trigonometric functions (sum of cosine curv es) can be used. A linear time trend is often included in both cases. -Stochastic - Usual treatment: SARIMA model. For example: or 1 1 where s the seasonal periodicity -associ ated with the frequency- of . For quarterly data, s = 4; monthly, s= 12; daily, s = 7, etc. Seasonal Time Series - Finding Seasonality with Visual Patterns The raw series along with the ACF and PACF ca n be used to discover seasonal patterns. Signs: Periodic repetitive wave pattern in AC F, repetition of signifi cant ACFs, PACFs after s periods. We simulate an ARMA(1,1) with a December s easonal pattern, typical of retail sales with a significant Christmas spike. Seasonal Time Series - Deterministic We use seasonal dummy variables, say monthly, in a linear model to capture seasonal patterns. Depending on the pattern, we have different specifications to re move the pattern. Suppose has monthly frequency and we suspect that in every December increases. - For the additive model, we can regress against a constant and a December dummy, Dt: For the multiplicative model, we can regress against a constant and a December dummy, Dt:, interacting with a trend: The residuals of this regressions, -i.e., = filtered , free of \"monthly seasonal effects\"- are used for further ARMA modeling. Example: We simulate an AR(1) series, with a multiplicative December seasonal behavior. Seas_12 <- rep(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1) , (length(y_sim)/12+1)) # Create Oct dummy T_sim <- 500 u <- rnorm(T_sim, sd=0.75) normally distributed errors y_sim <- matrix(0,T_sim,1) # vector to accumulate simulated data phi1 <- 0.2 # Change to create different correlation patterns k <- 12 # Seasonal Periodicity a <- k+1 # Time index for observations mu <- 0.2 mu_s <- .02 while (a <= T_sim) + phi1 * y_sim[a-1] Seas_12[a] * mu_s * a + u[a] # y_sim simulated autocorrelated values plot(y_seas, series, ACF, & PACF. We detrend (\" filter \" 0.01 ' ' 1 Residual standard error: 0.8209 on 484 degrees of freedom Multiple R-squared: 0.7929, 0.7917 p-value: < 2.2e-16 We plot the detrended simulated series, along with the ACF and PACF. The strong December seasonal pattern is gone from the detrended series. We run sigma^2 estimated residuals. \u00b6 Example: We model log changes in real estate prices in the LA market , . First, we run a regression to remove ( filter ) the monthly effects from . Then, we model as an ARMA( Estate Prices in LA\") We look at the ACF & PACF for LA > acf(x_la) > pacf(x_la) Note: ACF shows highly autocorrelated data, with some seasonal pattern (there is a periodic decreasing wave). We define monthly dummies. Then, we regress x_la against the monthly dummies. Feb1 <- rep(c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), (length(zz)/12+1)) # Create January dummy Mar1 <- rep(c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) , (length(zz)/12+1)) # Create March dummy Apr1 <- rep(c(0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0) , (length(zz)/12+1)) # Create April dummy May1 <- rep(c(0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0) , (length(zz)/12+1)) # Create May dummy Jun1 <- rep(c(0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), (length(zz)/12+1)) # Create June dummy Jul1 <- rep(c(0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0) , (length(zz)/12+1)) # Create Jul dummy Aug1 <- rep(c(0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0) , (length(zz)/12+1)) # Create Aug dummy Sep1 <- rep(c(0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0) , (length(zz)/12+1)) # Create Sep dummy Oct1 <- rep(c(0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0) , (length(zz)/12+1)) # Create Oct dummy Nov1 <- rep(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0) , (length(zz)/12+1)) # Create Oct dummy Dec1 <- rep(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0) , (length(zz)/12+1)) # + seasonal dummies > summary(x_la_fit_sea) Coefficients: ' 1 Note: Returns -i.e., home prices- are higher from April to August. Now, we model e t, the filtered LA series x_la_filt <- x_la_fit_sea$residuals # residuals, <- auto.ari ma(x_la_filt) # use auto.arima good model > fit_ar_la_filt zero Q* = 13.5, df = 7, p-value = 0.06083 Reject H 0 at 5% lever. But, judgement call is OK. Model df: 3. Total lags used: 10 We check residual plots. Note: ACF shows some small, but significant auto correlations, but the seas onal (wave) pattern is no longer there. Finally, we check the stati onarity & the invert ibility of the ARIMA(2,0,1) process. Note: All inverse roots inside the unit circle (& real): stationarity and invertibility. \u00b6 Seasonal Time Series - SARIMA For stochastic seasonality, we use the Seasonal ARIMA model. In general, we have the SARIMA (P, D, Q) s: 1 where 0 is constant and 1 1 Example 1 : SARIMA(0,0,1) 12= SMA(1) 12 - Invertibility 0, otherwise ACF non-zero at s easonal lags 12, 24,... Example 2 : SARIMA(1,0,0) 12 = SAR(1) 12 1 The process is - This is a simple seasonal AR model. - Stationarity Condition: || < 1. : , 0, 1, 2, When = 1, the series is non-stationary. \u00b6 Now, we put together the s easonal behavior and the ARMA be havior. That is, we have the multiplicative SARIMA model ( p,d,q) x (P,D,Q) s Example 1 : ARIMA(0,0,1) x (0,0,1) 12 (usually, with monthly data): 11 Then, the process is . \u00b6 Example 2 : Suppose p = Q = 1 and P = q = 0, with s=4, then, we have an ARIMA(1,0,0) x (0,0,1) 4 (usually, with quarterly data): 1 1 Then, the process is . \u00b6 In general, we the multiplicative SARIMA model ( p,d,q) x (P,D,Q) s is written as: where is the AR lag polynomial, is the MA lag polynomial, is the seasonal AR lag polynomial, and is the seasonal MA lag polynomial. Example : We model with a SARIMA model for U.S. vehicle sales . First, we look autocorrelated data , with some clear seasonal wave pattern. Then, we plot the data and, then, log transform Note: R has a function, decompose , that decomposes the data in trend, seasonal and random (unexplained): comp_lcar <- decompose(l_car) > plot(comp_lcar) Question: Should we try terministic seasonalities? No clear trend in data. We things up... ARIMA(2,0,2)(1,1,1)[12] with dr ift : -1049.585 ARIMA(0,0,0)(0,1,0)[12] with dr ift : -609.8308 ARIMA(1,0,0)(1,1,0)[12] with dr ift : -928.3348 ARIMA(0,0,1)(0,1,1)[12] with dr : -780.978 ... ARIMA(2,0,2)(0,1,2)[12] with : -1072.605 ARIMA(2,0,2)(1,1,2)[12] with dr : -1055.059 ARIMA(1,0,2)(0,1,2)[12] with dr ift : -1080.563 ARIMA(0,0,2)(0,1,2)[12] with dr ift : -905.0785 ARIMA(1,0,1)(0,1,2)[12] with drif -1081.598 Best model: ARIMA(1,0,1)(0,1,2)[12] Check Model df: Total lags used: 24 Finally, we check residuals, ACF and distribution. Note: ACF shows small and significant autocorrel ation, but the seasonal pa ttern is gone. More lags maybe needed. \u00b6 Forecasting One of the most important objectives in time series analysis is to forecast its future values. It is the primary objective of ARIMA modeling. Two types of forecasts. - In sample (prediction): The expected value of the RV (in-sample), given the estimates of the parameters. - Out of sample (forecasting): The value of a future RV that is not observed by the sample. To evaluate forecasts, we can use in-sample estimation to learn about the order of the ARMA( p,q) model and then use the model to forecast. We do the in-sample estimation keeping a hold-out sample. We use the hold-out sample to validate the selected ARMA model. Any forecasts needs an information set, I T. This includes data, models and/or assumptions available at time T. The forecasts will be conditional on IT. The variable to forecast is a RV. It can be fully characterized by a pdf. In general, it is difficult to get the pdf for the fo recast. In practice, we ge t a point estimate (the forecast) and a C.I. Notation: - Forecast for T+ made at T: , |, . - T+ forecast error: - Mean squared error (MSE): ]2 To get a point estimate, \u00a4\u00a4 , we need a cost function to judge various alternatives. This cost function is call loss function. Since we are working with forecast, we work with a expected loss function. A popular loss functions is the MSE, which is qua dratic and symmetric. We can use asymmetric functions, for example, functions that penalize positive errors more than negative errors. If we use the MSE as the loss function, we look for , which minimizes it. That is, min 2 Then, f.o.c. implies: 22 0 . The optimal point forecast under MSE is the (conditional) mean: E| Different loss functions lead to different optimal forecast. For example, for the MAE, the optimal point forecast is the median. The computation of E[ |IT] depends on the distribution of { }. If {} ~ WN, then E[|IT] = 0, which greatly simplifies computati ons, especially in the linear model. Then, for an ARMA( p, q) stationary process (with a Wold representation), the minimum MSE linear forecast (best linear predictor) of , conditioning on I T is: Forecasting Steps for ARMA Models The usual process has the following steps: - ARIMA model: - Estimation Estimate of Prediction (Evaluation in-sample) - Forecast Y Forecast (Evaluation out-of-sample) We observe the time series: I T = {Y1, Y2,...,Y T}. - At time T, we want to forecast: YT+1, YT+2,..., . - T : The forecast origin. - : Forecast horizon - : -step ahead forecast = Forecasted value Use the conditional expectation of , given the observed sample. |,, ..., Example: One-step ahead forecast : |,, ...,. \u00b6 Forecast accuracy to be measured by MSE conditional expectation, best forecast. Forecasting From MA(q) Models The stationary MA( q) model for Yt is Then, assuming we have the data up to T T l-step ahead forecasts using: ( l > 2) Now, we take conditional expectations: | = E|ITE | E | Note: The forecasts are a linear combin ation of forecast and past errors. Some of the errors are know at time T: 1, 2 , ..., T , the rest are unknown. Thus, E [ | = 0 for l > 1. Example: For an MA(2) we have: = E|E|E| = E|E|E| = E|E|E| At time T=t, we know and . Set E|It=0 for j > 1. Then, = E|ItE|I = = E|I = = = f o r l > 2 MA(2) memory of 2 periods. For l > 2, all forecast are constant (= ). \u00b6 The example generalizes: An MA( q) process has a memory of only q periods. All forecasts beyond q revert to the unconditional mean, . Example: An industrial firm uses an MA(2) to for ecast sales. The estimated MA(2) model is: = 2.20.4 0.2 At time T=t, the firms know = 1.42 and = -0.91 . Then, the first three forecast are: = 2.20.4.0.2. = 2.586 = 2.20.2. = 2.484 = 2.2 ( = 2.2 for l > 3.) Later, the firm observes: = 4.77, = 3.15 & = 1.85. Then, the MSE: MSE = * [(4.77 - 2.586 )2 + (3.15 - 2.484 - = 1.779 . \u00b6 Example: We fit an 3275.83, aic = -6545.67 > fcast_p Point Forecast Lo 80 Hi 80 Lo 95 Hi MA(1) process generates constant forecasts. \u00b6 Forecasting From AR(p) Models The stationary AR( p) model for Yt is Then, assuming we have the data up to time T (, , ..., ) and parameter constancy, T l-step ahead forecasts using: ( l > 2) Now, we take conditional expectations: |IT = E |ITE |IT E |IT Note that E |IT are also forecasts. The forecasts is a linear combination of past forecast. Example: AR(2) model for Y t+l is Then, taking conditional expectations at time T=t, we get the forecasts: AR-based forecasts are autocorrelated, they have long memory! \u00b6 Example: An industrial firm uses an AR(2) to for ecast sales. The estimat ed AR(2) model is: = 0.70.51 0.1 At time T=t, the firms know = 3 and = 3.52. Then, the first three forecast are: = 0.70.51 0.1 * 3.52 = 2.582 = 0.70.51 2.582 0.1 * 3 = 2.317 = 0.70.51 2.31 7 0.1 * 2.582 = 2.140 Later, the firm observes: = 4.77, = 3.15 & = 1.85. Then, the MSE: MSE = * [(4.77 - 2.582 )2 + (3.15 - 2.317 = 344.57, aic = -677.14 > fcast_oil Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 365 the point forecasts from the forecast function using $mean. That is, fcast_oil$mean extracts the w hole vector of forecasts. We plot the 12 forecasts: > plot(fcast_oil) Remark: Different from the MA(1) forecasts , AR(1) process generates non-constant forecasts. \u00b6 Forecasting From ARMA(p,q) Models The stationary ARMA model for Yt is Assume that we have data Y1, Y2, ... , Y T ; 1, 2, ..., T. We want to forecast . Then, Taking expectations: |IE |I E |I Remark: An ARMA forecasting is a combination of past forecasts and observed past . Example: An industrial firm uses an ARMA(1,2) to forecast sales. The estimated ARMA(1,2) model is: = 1.90.32 0.25 0.1 At time T=t, the firm knows: = 3, = 0.81, & = 0.47. Then, the first three forecast are: = 1.90.320.25. - 0.1 * 0.47 = 2.981 = 1.90.32. - 0.1 * 0.81 = 2.770 = 1.90.32. = 2.786 Later, the firm observes: = 4.77, = 3.15, & = 1.85. Then, the MSE: MSE = * [(4.77 - 2.981 )2 + (3.15 - 2.770 - 2.786 )2] = 1.407 . Alternatively, we can forecast idering the Wold representation: Taking the expectation of , we have |,,, = where |,,0, 0 , 0 Then, we define the forecast error: The forecast error is: Note: The expectation of the forecast error: E[ ] = 0 we say the forecast is unbiased . The variance of the forecast error: Example 1: One-step ahead forecast ( l = 1). 1 1 . \u00b6 Example 2: One-step ahead forecast ( = 2). = 2 21 Note: lm lim Recall that the Wold representation depends on an infinite number of parameters, but, in practice, they decay rapidly. Then, as we forecas t into the future, the forecasts are not very interesting (unconditional forecasts!). That is why ARMA (or ARIMA) forecasting is useful only for short-term forecasting. \u00b6 Forecasting From ARMA(p,q) Models: C.I. A 100(1- 95% C.I. for the 2-step-ahead forecast: 2 1.96 1 When computing prediction intervals from data, we substitute estimates for parameters, giving approximate prediction intervals. \u00b6 Note: Since s are RV, MSE[ T+] = MSE[ eT+] = Example: We fit an ARMA(4, 5), as selected by the function auto.arima , to changes in monthly U.S. h=20) # h=number of step-ahead forecasts > fcast_e Point Forecast Lo 80 Hi 80 Lo 95 the point forecasts from the forecast function using $mean. That is, fcast_e$mean extracts the whole vector of forecasts. We plot the forecast and the C.I. > plot(fcast_e, type=\"l\", incl ude = 24, in Earings: Forecast 2020:Oct - 2021:Jun\") #We include the last 24 observations along the forecast. Forecasting From ARMA(p,q) Models - Updating Suppose we have T observations at time t=T. We have a good ARMA model for Yt. We obtain the forecast for YT+1, YT+2, etc. At t = T + 1, we observe Y T+1. Now, we update our forecasts using the original value of YT+1 and the forecasted value of it. The forecast error is: The forecast error associated with 1 is: 1 1 Then, 1 1 1 11 11 Example : = 1, T = 100. 121 . \u00b6 Forecasting From ARMA(p,q) Models - Remarks In general, we need a large T. Better estimates and it is possible to check for model stability and check forecasting ability of model by withholding data. Seasonal patterns also need large T. Usually, you need 4 to 5 seasons to get reasonable estimates. Parsimonious models are very important. Easier to compute and interpre t models and forecasts. Forecasts are less sensitive to deviations between parameters and estimates. Forecasting From Simple Models: ES Industrial companies, with a lot of inputs a nd outputs, want quick and inexpensive forecasts. Easy to fully automate. In general, they only use past observations of the series to forecat. That is, we use past to forecast future , which in the literature is usually referred as the \" level's forecasts .\" Exponential Smoothing Models (ES) fulfill these requirements. In general, these models are limited and not optimal, especially compared with Box-Jenkins methods. Goal of these models: Suppress the short-run fluctuation by smoothing the series. For this purpose, a weighted average of a ll previous values works well. There are many ES models. We will go over the Simple Exponential Smoothing (SES) and Holt-Winter's Exponential Smoothing (HW ES). Simple Exponential Smoothing: SES We \" smooth \" the series to produce a quick forecast, called the \" level's forecast. \" Smooth? The graph of is less jagged than the graph of original series . Observed time series at time T: , , ..., . The SES Model has only one equa tion, we only forecast the level: 1 where - : The smoothing parameter, 0 1. - Yt: Value of the observation at time t. - St: Value of the smoothed observation at time t -i.e., the forecast. The equation can also be written as an updating equation : past forecast erro Note: The updating form of the SES model looks like an MA(1) model. SES: Forecast and Updating From the updating equation for : we compute the forecast: 1 That is, a simple updating forecast: last period forecast + adjustment. For the next period, we have: 11= Then the -step ahead forecast is: A naive forecast! Note: Similar to an MA(1) process, SES forecasts are not very interesting after > 1. Example: An industrial firm uses SES to forecast sales: The firm estimates = 0.25. The firm observes = 5 and, last period's forecast, = 3. Then, the forecast for time t+1 is: 3 + 0.25 * (5 - 3) = 3.50 The forecast for time t+1 and any period after time t+1, we have 3.50 for > 1. Later, the firm observes: = 4.77, = 3.15, & = 1.85. Then, the MSE: MSE = * [(4.77 - 3.50)2 + (3.15 + (1.85 - 3.50)2] = 1.486 . Note: If = 0.75, then 3 + 0.75 * ( 5 - 3) = 4.50 A bigger gives more weight to the more recent observation -i.e., . \u00b6 SES: Exponential? Question: Why time series { Y1,Y2,...,Y T, YT+1}, using backward substitution, 1 can be expressed as a weighted sum of previous observations: 1 11 11 1 where ci's are the weights, with 1; 0, 1, . . . ; 0 1. We have decreasing weights, by a constant ratio for every unit increase in lag. Then, 1111 111 Let's look at the weights: 1; 0, 1, . . . ; 0 1. = 0.25 = 0.75 0.25 0.75 0.25 * 0.75 = 0.1875 0.75 * 0.25 = 0.1875 .25 * 0.752 = 0.140625 0.75 * 0.252 = 0.046875 .25 * 0.753 = 0.1054688 0.75 * 0.253 = 0.01171875 .25 * 0.754 = 0.07910156 0.75 * 0.254 = 0.002929688 .25 = 0.007919088 0.75 * 0.2512 = 4.470348e-08 Decaying weights. Faster decay with greater , associated with faster learning: we give more weight to more recent observations. We do not know ; we need to estimate it. SES: Selecting Choose between 0 and 1. - If = 1, it becomes a naive model; if 1, more weights are put on recent values. The model fully utilizes forecast errors. - If is close to 0, distant values are given weights comparable to recent values. Set 0 when there are big random variations in Y t. - is often selected as to minimize the MSE. In empirical work, 0.05 0.3 are used ( 1 is used rarely). Numerical Minimization Process: - Take different values ranging between 0 and 1. - Calculate 1-step-ahead forecast errors for each , where the forecast error is . - Calculate MSE for each case. - Then, choose the which produces the minimum MSE: Example: Time Yt St+1 ( = 0.10) (YtSt)2 1 5 - - 2 7 (0.1) 5 +(0.9)5 = 5 4 3 6 (0.1) 7 + (0.9) 5 = 5.2 0.64 4 3 (0.1)6 + (0.9) 5.2 = 5.28 5.1984 5 4 (0.1)3 + (0.9) 5.28 = 5.052 1.107 TOTAL 10.945 12.74 Calculate this for = 0.2, 0.3,..., 0.9, 1 and compare the MSEs. Choose with minimum MSE. Note: Y t=1 = 5 is set as the initial value for the recursive equation . \u00b6 SES: Initial Values We start forecasting at time 2. Since we have a recursive equation, we need an initial value for S1 (or Y0). Approaches: - Set S 1 to Y1 is one method of initialization. Then, S2 = Y1. - Take the average of the first p obser vations, say first 4 or 5 observations: Use this average as an initial value S1 = Y0. Obviously, in ths case our first prediction will be for time (p+1), which becomes: . - Estimate S1 (similar to the estimation of ). SES: Forecasting Examples Example 1: We want to forecast log changes in U.S. monthly dividends (T=1796) using SES. First, we estimate the model using the R function HoltWinters (), which has as a special case SES: set beta=FALSE, gamma=FAL SE. without seasonal component. Call: HoltWinters(x = lr_d[1:1750], beta = FALSE, gamma = FALSE) Smoothing parameters: alpha: 0.289268 Estimated beta : FALSE gamma: [,1] a 0.004666795 Forecast Now, <- nrow(mod1$fitted) # number of # forecast horizon ses_f <- matrix(0,h,1) # Vector to collect 1 # Start of forecasts a <- T1 # index for while loop sm[a-1] <- mod1$fitted[T_last] # last in-sample forecast while (a <= T) { sm[a] = alpha * y[a-1] + (1-alpha) * sm[a-1] wider (as expected) with h . \u00b6 Example 2: We want to forecast log monthly U.S. vehicles (1976-2020, T=537) without seasonal component. Call: HoltWinters(x = l_car[1:512], be ta = FALSE, gamma = FALSE) Smoothing parameters: alpha: 0.4888382 Estimated beta : FALSE programs auto matically select the optimal using a line search method or non- linear optimizati on techniques. We have a recursive equation, we need initial values for S1. This model ignores trends or seasonalities. No t very realistic, especially for manufacturing facilities, retail sector , and warehouses. But, deterministic components, D t, can be easily incorporated. The model that incorporates both features is called Holt-Winter's ES. Holt-Winters (HW) Exponential Smoothing ) & seasonality ( ) factors. Since we also produce smooth forecasts for & , this method is also called triple exponential smoothing . The h-step ahead forecast is a combination of the smooth forecasts of (Level), (Trend) & (Seasonal). Both, & , can be included as additively or multiplicatively factors. In this class, we consider an additive trend and the seasonal factor as additive or multiplicative, see Figure 9.1. We produce h-step ahead For the additive model: For the multiplicative model: Note: Seasonal factor is multiplied in the h-step ahead forecast. Figure 9.1 - Different Seasonality Models with Trend Holt-Winters ahead forecast: where s is the number of periods in seasonal cycles ( s =4 for quarters). Components: - The level, , is a weighted average of seasonal adjusted and the non-seasonal forecast : 1 - The trend, , is a weighted average of and the change in . 1 Additive Model: variability with an additive trend. - The seasonality is also a weight ed average of seasonal index of s last year, , and the current seasonal index : 1 Summary: The additive model produces the following h-step ahead forecast: We use three equations: 1 1 1 We have only three smoothing parameters: = level multiplicative case (with an additive trend), we have the h-step ahead forecast: Details for multiplicative seasonality -i.e., Y t/It- and additive trend - The forecast, , now shows the average adjusted ( ). - The trend, T t, is a weighted average of T t-1 and the change in S t. - The seasonality is also a weighted average of I t-S and the Y t/St Then, the model has three equations: 1 1 1 We think of ( Yt /St) as capturing seasonal effects , where s represents the number of periods in the seasonal cycles. For example, s = 4, for quarterly data; s = 12, for monthly data; Again We have only three parameters: = smoothing parameter = trend coefficient = seasonality coefficient determine these 3 parameters? - Ad-hoc method: , and can be , , <0.2 - Optimal method: Minimization of the MSE, as in SES. Example: An industrial firm uses HW ES to forecast sales next three quarters ( = 1, 2 & 3; with = 4): with , , & factors given by: 1 1 1 The firm estimates: = 0.25; = 0.1; and = 0.4. The firm observes = 5; last quarter's smoothed forecasts: = 3, & = 1.2; and last year's seasonal factors: = 1.1 , = 0.7 & = 1.2, & = 0.8. Components forecasts: 0.25 5 1.1 10.25 31.2 = 4.2864 0.1 4.28643 10.1 * 1.2= 1.2086 0.4 5 4.2864 10.4 1.1 = 1.1266 The forecast for = 1 (next quarter) is: 4.28641.20860. = 4.8125 The forecast for = 2 4.2864 Values We have three recursive equations. That is, we need initial values for S 0, T0 and It-s. To calculate initial values for the algorithm, we need at l east one complete season of data to determine the initial estimates. Like in the SES model, there ar e different approaches. Below, we present one approach for the multiplicative model: - Initial values for S 0 and T 0: T or T / / / - Initial values for It-s.: Assume we have T observation and quarterly seasonality (s=4): (1) Compute the averages of each of T years. ,/4 , 1, 2,,6 yearly averages (2) Divide the observations by th e appropriate yearly mean: Y t,i/At. (3) I s is formed by computing the average Y t,i/At per year: ,/ 1, 2, 3, 4 Holt-Winters (HW) ES: Damped Model We can damp the trend as the foreca st horizon increases, using a parameter . For the multiplicative model, see Figure 9.2, we have: 1 1 1 Then, the h-step ahead forecast: 1 Figure 9.2 - Damped Additive Trend with Multiplica tive Seasonality This model is based on practice: It seems to work well for industrial outputs. Not a lot of theory or clear justification behind the damped trend. HW ES Models - Different Types We have many variations: 1. No trend and additive seasonal variability. 2. Additive seasonal variability with an additive trend. 3. Multiplicative seasonal variability with an additive trend. 4. Multiplicative seasonal variabil ity with a multip licative trend. 5. Dampened trend with addi tive seasonal variability. 6. Multiplicative seasonal variab ility and dampened trend. Q: Which model should be used? A: Select the type of model to fit based on the presence of - Trend - additive or multiplicative, dampened or not - Seasonal variability - additive or multiplicative HW ES: Example - Log U.S. Vehicles Sales Example: We want to forecast log U.S. monthly vehi cle sales with HW. We use lr_d Estimated seasonal parameter > hw_d_car Coefficients: [,1] a 7.177857555 for level b 0.0001100345 forecast for trend s1 -0.075314457 forecast for seasonal month 1 s2 -0.084468361 forecast * y[a-1] + (1-alpha) * sm[a-1] Tr[a] = beta * (sm[a] - sm[a-1]) + (1 - beta) * Tr[a-1] I[a] = gamma * (y[a] - sm[a ]) + (1 - gamma) * I[a - 12] a <- a computer program selects = 0 = , it has a lack of trend or seas onality. It implies a constant (deterministic) component. In this case, an AR IMA model with deterministic trend may be a more appropriate model. - For HW ES, a seasonal weight near one im plies that a non-seasonal model may be more appropriate. We can model seasonalities as multiplicative or additive: Multiplicative seasonality: Forecast t = S t * I t-s. Additive seasonality: Forecast t = S t + I t-s. Evaluation of forecasts - Accuracy measures The mean squared error ( MSE ) and mean absolute error ( MAE ) are the most popular accuracy measures: MSE = MAE = | | || where m is the number of out-of-sample forecasts. But other measures are routinely used: - Mean absolute percentage error ( MAPE ) = | | - Absolute MAPE (AMAPE ) = | | Remark: There is an asymmetry in MAPE, the level matters. - % correct sign predictions (PCSP) = where = 1 if ) > 0 = 0, otherwise. - % correct direction cha nge predictions (PCDP)= where = 1 if ) * () >0 = 0, otherwise. Remark: We value forecasts with the right direct ion (sign) or forecast that can predict turning points. For stock investors, the sign matters! MSE penalizes large errors more heavily than sm all errors, the sign prediction criterion, like MAE, does not penalize large errors more. Example: We compute MSE and the % of correct dir ection change (PCDC) predictions for the one-step forecasts for U.S. monthly vehicles sales based on the SES and HW ES models. > MSE_ses [1] 0.027889 > MSE_hw true pcdc_hw <- sum(indicator_hw)/h > indicator_hw [1] 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 > pcdc_hw [1] [1] 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 > pcdc_s [1] 0.76 . Note: Same percentage of correct direction ch ange (PCDC) predictions, but the sequence of correct predictions is not the same. \u00b6 Evaluation of forecasts - DM Test To determine if one model predicts better than an other, we define the loss differential between two forecasts: d t = g(e tM1) - g(e tM2) where g(.) is the forecasting lo ss function. M1 and M2 are two competing sets of forecasts - could be from models or something else. We only need {e tM1} & {e tM2}, not the structure of M1 or M2. In this sense, this approach is \"model-free .\" Typical (symmetric) lo ss functions: g(e t) = e t2 & g(e exp(et2 ) - et2 (>0). Then, we test the null hypotheses of equal predictive accuracy: H 0: E[d t] = 0 H 1: E[d t] = 0. - Diebold and Mariano tM1} & {e conditions (finite Var[d t], independence of forecasts after periods) needed to apply CLT. Then, / 0,1, 1 Then, under H 0, the DM test is a simple z-test : / 0,1 where is a consistent estimator of the varian ce, usually based on sample autocovariances of dt: 02 There are some suggestion to calculate small samp le modification of the DM test. For example, : DM* = DM/{[ T + 1 - 2 + ( - 1)/ T]/T}1/2 ~ tT-1. where -step ahead forecast. If A RCH is suspected, replace with [0.5 (T)] + . perfectly correlated, the numerat or and denominator of the DM test are both converging to 0 as T . Avoid DM test when this situation is suspec ted (say, two nested models.) Though, in small samples, it is OK. Code in R dm.test <- function (e1, e2, h = 1, power sqrt(dv) else if(h==1) stop(\"Variance of DM statistic is zero\") else { warning(\"Variance is negative, using horizon h=1\") return(dm.test(e 1,e2,alternative,h=1,power)) } n <- length(d) k - 2*h + (h-1))/n)^(1/2) STATISTIC <- STATISTIC * k names(STATISTIC) <- \"DM\" } Example : We compare the SES and HW forecasts for th e log of U.S. monthl y vehicle sales. We use the dm.test function, part of the forecast package. library(forecast) > 1.6756 , Forecast horizon = 1, Loss function power = 2, p-value = 0.1068 DM = 1.94, Forecast horizon = 1, Loss function power = 1, p-value = 0.064 alternative hypothesis: two.sided Note: Cannot reject H MSE SES = MSE HW at 5% level. \u00b6 Evaluation of forecasts - DM Test: Remarks The DM tests is routinely used. Its \"model- free\" approach has appeal. There are model- dependent tests, with more comp licated asymptotic distributions. The loss function does not need to be symmetric (like MSE). The DM test is based on the notion of unconditi onal -i.e., on average over the whole sample- expected loss. Following Morgan, Granger and Newbold (1977) , the DM statistic can be calculated by regression of d t, on an intercept, using NW SE. But, we can also condition on variables that may explain d t. We move from an unconditional to a conditional expected loss perspective. Combination of Forecasts Idea - from Bates & Granger ( Operations Research Quarterly, 1969): - We have different forecasts from R models: ,,.... Question: Why not combine them? .... Very common practice in economics, finance and politics, reported by th e press as \"consensus forecast.\" Usually, as a simple average. Question: Advantage? Lower forecast variance. Diversification argument. Intuition: Individual forecasts are each based on partial in formation sets (say, private information) or models. The variance of the forecasts is: 2 var Note: Ideally, we would like to ha ve negatively correlated forecasts. Assuming unbiased forecasts and uncorrelated j=1/R. Then, We combine the of - Optimal & Regression Weights We can derived optimal weights -i,e., j's that minimize the variance of the forecast. Under the uncorrelated assumption: The j*'s are inversely proportiona l to their variances. In general, forecasts are biased and correlated. Th e correlations will appear in the above formula for the optimal weights. For the two forecasts case: 2 2 Ideally, we would like to have negatively correlated forecasts. Granger and Ramanathan (1984) used a regression method to combine forecasts. - Regress the actual value on the forecasts. The estimated coefficients are the weights. .... Should use a constrained regression - Omit the constant - Enforce non-negati ve coefficients. - Constrain coefficients to sum to one Example: We regress the SES and HW forecasts against the observed car sales to obtain optimal weights. We omit the constant > lm(y[T1:T] ~ ses_f_c + car_f_hw - 1) Call: lm(formula Coefficients (weights) add up to 1. But, we see negative weights. In general, we use a constrained regression, forcing parameters to be between 0 and 1 (& non-negative). But, h=25 delivers not a lot of observati ons to do non-linear estimation. \u00b6 Remarks: - To get weights, we do not include a constant. Here, we are assuming unbiased forecasts. If the forecasts are biased, we include a constant. - To account for potential correla tion of errors, we can allow fo r ARMA residuals or include y T+l- 1 in the regression. - Time varying weights are also possible. Question: Should weights matter? Two views: - Simple averages outperform more complicated combination techniques. - Sampling variability may affect weight estimates to the extent th at the combination has a larger MSE. Combination of Forecasts: Final Remarks Since Bates and Granger (1969) and Granger and Ramanathan (1984), combination weights have generally been chosen to minimize a symmetric, squared-error loss function. But, asymmetric loss functions can also be used. Elliot and Timmermann (2004) allow for general loss functions (and distributions). They find that the optimal weights depend on higher order moments, such a skewness. It is also possible to forecast quantiles and co mbine them. We will not explore these issues in more detail in this class. Lecture 10 - Efficient Market s Hypothesis & Predictability Efficient Markets Hypothesis (EMH) Questions: Can we predict stock retu rns? Can past information be us ed to build profitable trading rules in financial markets? In particular, can past re turn realizations te ll us anything about expected future returns? Very old questions. The efficient markets hypothesis (EMH) is a first attempt to a ddress the predictability issue. Earliest known version: \"When shares become publicly known in an ope n market, the value which they acquire there may be regarded as the judgement of the best intelligence concerning them.\" - George Gibson, The Stock Exchanges of London, Paris and New York , G. P. Putnman & Sons, New York, 1889. In 1900, Louis Bachelier, a French PhD student at the time, was the first to propose the \"Random Walk Model\" for security prices. Samuelson (1965) \"In an informationally efficient market, price changes must be unforecastable.\" Fama (1970) \"A market in which prices always fully reflect available information is ' efficient '.\" If we have new information (a new earnings an nouncement) prices will adjust immediately (or very fast). Prices (significantly) jump with relevant information. But, they have to jump a proper amount, not too much (over-reaction) or not too little (under-reaction)! Grossman and Stiglitz (1980) \"There must be sufficient profit opportunities, i.e. inefficiencies, to compensate investors for the cost of trading and in formation-gathering. \" The, under a frictionless world, it is impossible to have efficient prices (& EM). Only when all information gathering & trading costs are zero we can expect prices to fully reflect all available information. But, if prices reflect fully and instantly al l available information, who is going to gather information? Malkiel (1992) \"The market is said to be efficient with resp ect to some information set... implies that it is impossible to make economic profits by trading on the basis of [the information in that set].\" The first sentence of Malkiel's definition expa nds Fama's definition a nd suggests a test for efficiency useful in a laboratory. The second sentence suggests a way to judge effici ency that can be used in empirical work. This is what is usually done in the finance literature. Example : If Fund managers outperform the market consistently, th en prices are not efficient with respect to their information set. Many examples of \" inefficiencies \" with respect to some information sets. The behavioral finance field ha s found that investors often show predictable and financially ruinous behavior (irrational? ). Different causes: overreact ion, overconfidence, loss aversion, herding, psychological accounting, miscalibra tion of probabilities, regret, etc. Examples: Momentum strategies (buying past winners and selling past losers, under-reaction?) and Contrarian strategies (buying past losers and selling past winners, over-reaction?) achieve abnormal returns. Lo (2004) \"... much of what behavioralists cite as counterexamples to economi c rationality [...] are, in fact, consistent with an evolutionary model of i ndividuals adapting to a changing environment.\" There is a time dimension. It takes time to adapt to new circumstances. EMH: Versions Efficiency can only be defined with reference to a specific type of information. Fama (1970) defined three classes of information sets: (a) Historical sequence of prices . This set gives Weak form EMH. (b) Public records of companies and public forecasts regarding the future performance and possible actions. Sets (a) & (b) create the Semi-strong form EMH. (c) Private or inside information. Sets (a ), (b) & (c) deliver the Strong form EMH. Violations: - Technical traders devising prof itable strategies (weak EMH) - Reading a newspaper and devising a prof itable trading strategy (semi-strong EMH) - Corporate insiders making profitable trades (strong EMH). Question: Can markets really be strong-form effi cient? Very unlikely, plenty of examples of successful trading with private information: Jeffrey Skilling (Enron), Ivan Boesky & Michael Milken (junk bonds), Eugene Pl otkin and David Pajcin (from Goldman Sachs, trading on M&A inside information), James McDermott Jr (K eefe, Bruytee & Woods, passed M&A tips to his mistress), Raj Rajaratnam (Galleon Group), Sco tt London (KPMG, passed tips from clients to a friend). Perfectly rational factors may account for violations of EMH: - Microstructure issu es and trading costs. - Rewarding investors for bearing certain dynamic risks. - Time-varying expected returns due to cha nging conditions can gene rate predictability. EMH: Joint Tests We are talking about economic pr ofits, adjusting for risk and costs. Thus, a model for risk adjustment is needed. Results will be condi tional on the underlying asset pricing model. Fama (1991) remarks that tests of efficiency are joint tests of efficiency and some asset pricing model, or benchmark. Example : Many benchmarks assume constant \"normal\" re turns. This is easier to implement, but may not be correct. Thus, rejections of efficiency could be due to rejections of the benchmark. Most tests suggest that if the security return (beyond the mean) cannot be forecasted, then market efficiency is not rejected. Example : A wrong asset pricing model may reject e fficiency. It would be easy to find (demeaned) returns to be forecastable if we use the wrong mean. EMH: Expectations and Information Set The conditional expectation of the stochastic process X t+1, conditioned on information set I t, can be written as: E [ X t+1|It] = E t [X t+1] Information set, I t: It describes what we know at time t. The usual assumption is that we do not forget anything. Over time, th e information set increases: I t is contained in I t+1 ; It+1 is contained in I t+2 , etc. That is, we have a sequence I 0 I1 I2 ... It. In stochastic processes this sequence is called a \" filtration, \" with notation { t}. Technical note: We say a stochastic process {X t} is adapted to a filtration { t} if X t is measurable t for all t. Measurable? The event of interest is in t. EMH: Random Prices Efficient market: A market where prices are random with respect to an information set (\"filtration\" ), It. Let the price of a security at time be given by the expectation of some \" fundamental value, \" V*, conditional on I t: P t = E[V*|I t] = E t[V*] The same equation holds one period ahead so that: P t+1 = E[V*|I t+1] = E t+1[V*] The expectation of the price ch ange over the next period is: E t[Pt+1 - Pt] = E t[Et+1[V*] - E t[V*]] = 0 since I t is contained in I t+1 Et[Et+1[V*]] = E t[V*] (by the law of IE). Remark: Under efficiency, financia l asset prices ar e unpredictable. EMH: Martingale & Fair Games Martingale: A stochastic process P t is a martingale if: E [ P t+1 | t] = P t (or E t[Pt+1] = P t) where the information set is t (what we know at time t, includes P t). Submartingale: If E[P t+1 | t] Pt. -P t is a lower bound for E t[Pt+1] Supermartingale: If E[P t+1 | t] Pt -P t is an upper bound for E t[Pt+1] Fair game model : A stochastic process is a fair game if: E [ | t] = 0 if P t is a martingale or pure random walk, (P t+1 - P t) is a fair game. Note: Only referring to expected values! The Martingale process can be setup as a special case of an AR(1) process: = + + with = 1, = 0, & E t[] = 0. A non-stationary process. Technical detail: Martingale condition is neit her a necessary nor a sufficient condition for rational expectations models of asset prices (L eRoy (1973), Lucas (1978)). According to Lucas (1978), in markets where all investors have rational expectations, prices do fully reflect all available information and marginal-utility weighted prices follow martingales. But, we consider the martingale as an important starting point. EMH: The Random Walk Hypothesis (RWH) Definition: Random Walk (RW) A stochastic process is a RW if: = + + -where = ln(P t) = + = Assumptions about : Uncorrelated with past information, with constant mean (=0) & variance (2). That is, t D (0, 2), with E t[] = 0, E t[2t+1] = 2 If 0, the process is called a RW with a drift . A RW with no drift is a martingale with structure for the error term, t, uncorrelated, zero mean and constant variance. We start testing the EMH by assuming log returns, , follow a RW with a drift. We called this \"Random Walk Model\": = + = where t D (0, 2). Different specifications for t produce different testable hypoth esis for the EMH-RW Model: and identi cally (i.i.d.) ~ D(0, 2). Not realistic. (Old tests: Cowles and Jones (1937)). - RW2 : is independent (allows for heteroskedasticity). Te st using filter rules, technical analysis. (Alexander (1961, 1964), Fama (1965)). - RW3 : is uncorrelated (allows for dependence in higher moments). Test using autocorrelations, variance rati os, long horizon regressions. The is ergodic. Then = cov(, ) - Auto-covariance between times & = / . - Var[] = are not time dependent. We es timate both statistics with and . Under RW1 Hypothesis (and some assumptions) N(0, 1) SE[ ] = 1/ Technical Note: The sample correlation coefficients, , are negatively biased in finite samples. See Fuller (1976). To check autocorrelations up to order , we use the ACF for . Confidence Intervals can be easily approximated by 2/ . Example: ACF with = 24 lags for the monthly Equal- and Value-weighted (EW & VW, respectively) CRSP index returns distributions) lr_ew <- EMH_da$ewretd # Equal weight => significant > SE_rho [1] 0.02942449 # |rho| 'lr_vw', by lag 0 1 2 3 4 5 6 7 8 9 10 11 12 15 16 17 18 19 20 21 22 23 returns: There are a few significant autocorrelations (3 rd, 4th, and 8th), all smaller than 0.2 in absolute value. ## 'lr_vw', by lag 0 1 2 3 4 5 6 7 8 9 10 11 15 16 17 18 19 20 21 22 23 returns: Again, a few significant autocorrelations, but, small in size. \u00b6 Example: ACF with = 24 lags for the daily Equal- and Value-we ighted (EW & VW, respectively) CRSP index returns from 1926:Jan 1 - Asymptotic SE => significant > SE_rho [1] 0.006279628 # |rho| 'lr_vw_d', by lag 0 1 2 3 4 5 6 7 8 9 10 11 15 16 17 18 19 20 21 22 23 returns: There are many significant autocorrelations, with the exception of the first one, all very small. ## 'lr_ew_d', by lag 0 1 2 3 4 5 6 7 8 9 10 11 15 16 17 18 19 20 21 22 23 24 rturns: Lots of significan t autocorrelations, but, in general, small. \u00b6 The RWH: Autocorrelations Joint Tests We already know two tests to check for zero au tocorrelation in a time series: Box-Pierce Q and Ljung-Box tests. We usually re ly on the Ljung-Box (1978), LB, test, since it has better small sample properties. -The Q & LB statistics test a joint hypothesis that the first autocorrelations are zero: H 0: r1 = ... = rp = 0 Under RW1 and using the asymptotic distribution of : Q = T r . L B = T * (T - 2) * r . Q & LB tests are widely use, but they have two main limitations: (1) The test was developed under the independence ( RW1 ) assumption. If shows dependence, such as heterosced asticity, the asymptotic variance of is no longer I, but a non-diagonal matrix. There are several proposals to \" robustify \" both Q & LB tests, s ee Diebold (1986), Robinson (1991), Lobato et al. (2001). The \"ro bustified\" Portmanteau statistic uses instead of : = = Thus, for Q we have: Q * = T . (2) The selection of the numb er of autocorrelations is arbitrary. The traditional approach is to try different values, say 3, 6 & 12. Another popular approach is to let the data \"select\" , for example, using AIC or BIC, an approach sometimes referred as \"automatic selection .\" Escanciano and Lobato (2009) propose combin ing BIC's and AIC's penalties to select in Q* (BIC for small r and AIC for bigger r). Note: It is common to reach diffe rent conclusion from Q and Q*. Example: Q and LB tests with = 3 & 12 lags for the monthly EW & VW CRSP index returns from 1926:Jan - 2022:March ( T = 1155): Q test for monthly VW > Box.test(lr_vw, lag = LB tests monthly VW > 0.0004912 Q* tests with automatic lag selection. In R, the package vrtest has the Auto.Q function that computes this test. As always, you need to install vrtest first. Q* test for monthly $Pvalue [1] 0.08026232 Conclusion for VW Once we take into consideration potential heteroscedasticity in , there is weak evidence for autocorr elation in monthly Value-weighted CRSP index returns. Q test for monthly EW > Box.test(lr_ew, lag = 4, 12, LB tests monthly EW > = type=\"Ljung-Box\") X-squared = idence for autocorrelati on in monthly EW CRSP returns (the evidence was weaker, once we take into consideration potential heteroscedasticity in , for monthly VW CRSP returns). That is, we reject the RW hypothesis for monthly EW CRSP returns. \u00b6 Example: Q and LB tests with = 5 & 20 lags for the daily Equal- and Value-weighted (EW & VW, respectively) CRSP index returns from 1926: Jan 1 - lr_ew_d <- EMH_d_da$ewretd # Equal wei ghted CRSP returns (with distributions) T <- length(lr_ew_d) Q tests for daily VW > Box.test(lr_vw_d, type=\"Box-Pierce\") Q* test for daily VW (continuation) > Auto.Q(y, 20) # Q* test automatic selection of p $Stat [1] 11.73454 $Pvalue [1] 0.0006135076 Q tests type=\"Box-Pierce\") Q* test for daily EW (continuation) > Auto.Q(y, 40) # Q* test automatic selection of p $Stat [1] 235.7106 $Pvalue [1] 0 Conclusion: Strong evidence for au tocorrelation in daily VW & EW CRSP returns. That is, we reject the uncorrelated returns hypothesis as implied by the RW hypothesis for daily VW & EW CRSP returns. \u00b6 The RWH: Variance Ratio (VR) Test all 3 RW hypotheses, the variance of RW increments is linear in the time interval. If the interval is twice as long, the variance must be twice as big. That is, the variance of monthly data should be 4 times bigger th an the variance of weekly data . (Recall the log approximation rules for i.i.d. returns.) If r t is a covariance stationary process (constant first two moment, and covariance independent of time), then for the variance ratio of 2- period versus 1-period returns, VR(2): Var[] + 2 Cov[ ,] 2*Var[ = 2 2 + 2 22 = 1 + where (2) = + Three cases: = 0 VR(2) = 1 (True under RW1 , random walk) > 0 VR(2) > 1 (mean aversion) < 0 VR(2) < 1 (mean reversion) The intuition generalizes to longer horizons: VR( q) = Var[(q)] q *Var[] = 1 + 2 * 1 . The VR( q) is a particular linear combination of the 1st (q - 1) autocorrelation coefficients (with linearly declining weights). Under RW1 , we have H 0: VR( q) = 1. H 1: VR( q) 1. Technical Note: Under RW2 and RW3 , VR( q) = 1 provided 1 / T t Var[] > 0 we need this assumption, since some \"fat-tailed\" distributions do not ha ve a well-defined second moment. To do any testing we need the sampling distri bution of the VRs (estimated variance ratios) under H 0: VR( q) = 1. We use the statistic: VR(q) - 1) N(0, 1) This is Cochrane's (1988) VR test. The test rejects H 0 -i.e., the RWH - if the above statistic is greater in absolute value than 1.96. For the special case of q = 2, we use (VR(2) - 1) N(0, 1) Var[(q)] is computed using the MLE formulation , that is, dividing by T, not by ( T - 1) (or T minus degrees of freedom). Example : We have monthly data fro m Jan 1973. Then, we compute Var[ ] = Var[ (2)] = . Note: Since the tests are asymptotic tests, in this case, relying on the Normal distribution, dividing by T or by ( T - ) does not make any difference. \u00b6 Var[(q)] is computed using non-overlapping returns . Example : We compute non-overlapping bi-monthly returns , using monthly data from Jan 1973. (1) monthly returns: is computed as usual. For the first return: = ln , ln , (2) bi-monthly returns. The first three (2) are computed as: (2) = (2) = r (2) = r Note: We have \"clean data,\" with no introduced se rial correlation. But, we lose observations. If we have 1,000 monthly returns, using non-overla pping bi-monthly returns we end up with only 500 observations. \u00b6 Example: We check the RW Hypothesi s, under RW3, for the monthly CRSP EW and VW Index returns. In R, the package vrtest has functions to compute the above mentioned VR tests. VR tests for monthly VW library(vrtest) <- VR.minus.1(y, kvec) # St at be close to 0 if RW > vr_1 $VR.auto # [1] 2 3 > vr_1 <- VR.minus.1(y, kvec) # St at be close to 0 if RW > vr_1 $VR.auto # [1] 2 3 Conclusion: Using the VR test (with q = 2, 3, 12), we reject the RW Hypothesis tests are greater in absolute value than 1.96. \u00b6 The RWH: Variance Rati o (VR) Test - Issues Several issues has been raised regarding the VR's tests. The main issues are: (1) Choice of q. In the previous examples, we have arbitrarily selected q. Similar to the situation with the Q and LB tests, there are suggestions to automatically (or \"optimally,\" according to some loss function) select q. Choi (1999) is one exampl e of this approach, (the vrtest R package uses this approach in the Auto.VR test). (2) Poor asymptotic approximation . In simulations, it is found that the asymptotic Normal distribution is a poor approximation to the small- sample distribution of the VR statistic. The usual solution is to use a bootstrap (Kim's (2009) bootstrap gives the p-value of the automatic VR test in the Auto.VR function). Example: We use VR tests with automatic selection and a bootstrap to check the RW Hypothesis for the monthly CRSP EW and VW Index returns. Again, we use AutoBoot.test function in R package vrtest . Automatic VR tests (Automatic variance statistic as in Choi (1999)) [1] 2.509324 $VRsum (1+ weighted sum of auto (Automatic variance statistic as in Choi (1999)) [1] 4.173898 $VRsum (1+ weighted sum of auto test and a bootstrap, we have strong evidence against the RW Hypothesis for EW, but weak for VW. \u00b6 The RWH: VR Tests - LM's Modifications Lo & MacKinlay (LM, 1988, 1989) propos e modifications to the test: - Allow for overlapping returns , and, thus, using more observati ons. But, overlapping returns will be autocorrelated, even if underlying process is not. We need to adjust for this feature. - Use unbiased estimators of variances -i.e., divide by ( T - df ). q) - 1) N(0, 1), where (q) is the VR statistic comput ed using overlapping returns. - Allow for possible heteroscedasticity of returns (more realistic) q) - 1) N(0, 1), where = }. Example: We check the RW Hypothesi s, under RW3, for the monthly CRSP EW and VW Index returns using the LM's tests: M1 and M2. Again, we use the R package vrtest . Automatic VR tests for monthly VW LM's tests (RW Model) using M1 for q = 2, 3; but, once we allow for heteroscedasticity (M2 test s), we cannot reject H 0. Automatic VR tests for monthly EW y <- lr_ew > Lo.Mac(y, kvec) # LM's tests using M1, especially for q = 2, 3; but, using M2 test with q = 12 , we cannot reject the RW Hypothesis . Consistent with previous result, stronger evidence for EW returns than for VW returns. \u00b6 The RWH: VR & LM Tests - Issues Several issues has been raised regarding the LM's tests: (1) Poor asymptotic approximation . The asymptotic standard normal distribution provides a poor approximation to the small-sample distribution of the VR statistic. LM 's tests tend to be biased and right-skewed, in finite samples. Proposed solutions: - Alternative asymptotic distributions , as in Richardson and Stock (1989) or Chen and Deo (2006). - Bootstrapping , as in Kim (2006) or Malliaro pulos and Priestley (1999). (2) Joint tests . The LM's tests are in dividual tests, where H 0 is tested for a specific value of q. But, under H 0, VR( q) = 1, for all q. LM's tests ignore the joint nature of testing for the RW Hypothesis. Proposed solutions: - RS statistic , a Wald Test, as proposed by Richardson and Smith (1993): RS( q) = T (VR ) -1 q. where ( q\u00d71) vector of q sample variance ratios, is the ( q\u00d71) unit vector, and is the covariance matrix of VR. - QP statistic , a Wald Test based on a \"power transformed\" VR statistic, as proposed by Chen and Deo (2006). QP asymptotically follows a q distribution. This test is a one-sided test (H 1: VR(q) < 1 for all q.) - CD statistic , a join test, as proposed by Chow and Denning (1993): CD = max || which follows a complex distribution, the stud entized maximum modulus [SMM] distribution with m and T degrees of freedom ( m is the number of k values). This SMM distribution is tabulated in Hahn and Hendrickson ( 1971) and Stoline and Ury (1979). In general, we use the simulated critical valu es obtained by simulations as done by Chow and Denning themselves or a bootstrap as in Kim (2006). Example: We check the monthly LM test results usi ng a bootstrap instead of the asymptotic distribution. We use the Boot.test function in the R package vrtest , which provides two bootstrapped p-values: one for the LM statisti c and the other one for the CD statistic. VR tests for monthly VW > y <- lr_vw > Lo.Mac(y, kvec) # LM's lr_ew > Lo.Mac(y, kvec) # LM's result, solid evidence for the RW for EW returns, but weak evidence (only the Wald test rejects H 0) for VW returns. \u00b6 Example: We check the RW Hypothesis, under RW3, for the daily CRSP EW and VW Index returns. VR tests for daily VW kvec <- c(5, 20, vr_1 <- VR.minus.1(y, kvec) # Stat 0 if RW > vr_1 $VR.auto (value of VR-1 with auto matic selection of holding vectors) [1] 0.08049192 $Holding.Periods [1] 5 20 60 $VR.kvec (the values for the chosen holding periods) [1] 0.06015875 0.11155693 0.16958754 sqrt(T*kvec)/sqrt(2*(kvec-1))*vr_1$VR.kvec Implications Tests results are based on CRSP value-weighted (VW) and equal weighted (EW) indices from 1925 & individual securities from 1962 . Daily, weekly and monthly returns from VW a nd EW indices show significant (positive) autocorrelation. VR(q) > 1 statistics reject RW3 fo r EW index but not VW index. Market capitalization or size may be playing a role. Rejection of RW stronger for smaller firms. Their returns more serially correlated. For individual securities, VR( q) < 1, suggesting small and ne gative correlations (and not significant). VR tests in other countries and financial market s. Tests also tend to reject the RWH, with stronger rejections for smaller ma rkets and less liquid markets. The rejection of the RWH does not necessa rily imply a violation of the EMH. Main implication: Theoretical pr icing models should be able to explain the pattern of serial correlation. Side Question: How can portfolios show VR( q) > 1 when individual securities show VR( q) < 1? Predictability Traditional view pre 1980: - CAPM is a good measure of risk - Usual findings: (a) Stock, bond and foreign exchan ge changes are not predictable (b) Constant equity premium - Market volatility does not change much through time - Professional managers do not reliably outperf orm simple indices and passive portfolios once one corrects for risk Summary of State of the Ar t, late 1970s (Jensen, 1978): \"I believe there is no other pr oposition in economics which has more solid evidence supporting it than the Efficient Markets Hypothesis.\" Modern view post-1980: - Rejection of the RW Hypothesis. - Stock returns are predictable. Valuation ratios (D/P, E/P, B/M ratios) Interest rates (term spread, short-long T-bill rates, etc.) Decision of market participants (corporate financ ing, consumption). Cross-sectional equity pricing. Bond and foreign exchange re turns are also predictable. - Some funds seem to outperform simple indices, even after controlling for risk through market betas. - New equilibrium (theory) models with time-varying equity premium. Predictive Regressions Motivation: 1. Mounting evidence that stock and bond returns ar e predictable. 2. Q: Market inefficiency vs Rati onal variation in expected returns? Economic questions: 1. Do the expected returns on bonds and stocks move together? 2. Do the same variables forecast bond and stock returns? 3. Is the variation in expected returns related to business cycles? Setup: Regress future returns, , on variables known at time . ( 1 ) where can be one month, one quarter, and one to four years. Predictive Regressions - Fama-French (1989) One of the first papers to show a predictive pattern at differen t horizons. The setup of Fama and French (JFE, 1989): - : value- & equal-weighted market portfolio s of NYSE; value-wei ghted corporate bond portfolios. - variables: - Dividend yields, /: Add monthly dividends for the year preceding time divided by the value of the portfolio at time - Term Premium, : Long term government bond yield minus treasuries -see, Keim and Stambaugh (1986). - Default premium, : AAA bond yields minus BAA bond yields -see, Keim and Stambaugh (1986). Sample: & annual ( T=61) data. For longer horizons (bi-annual+), overlapping observations. Findings: - variables work, especially / with high t-stats & high R2 for forecast horizons beyond 1 year. - (Conditional) Expected returns move with the predictors, : [ That is, even with = , expected future returns are time-varying ! - Regression coefficients and R2 increase with the forecast horizon. \u00b6 Interpretation of the Fama-French's slope estimate for / (similar of other financial ratios with in the denominator): - There is a positive relation between / and . A high (low) / forecasts high (low) subsequent returns (higher !). Since we tend to observe high / when is low, we have evidence for mean reversion in stock prices. - Let's look at the one-year / EW slope coefficient: 5.75. Then, a 1% increase in dividend increase expected (total) retu rns by 5.75% (an investor gets 1% dividend plus 4.75% extra return). Big number ! - Using the above 5.75 slope, we can derive an informal range for the expected 1-year return: In the past 40 years / ranged from 1% to 6%, ignoring , the range for E t[ is {5.75% - 34%}. Very big! Interpretation of Fama-French's R2 for / (again, similar interpreta tion for other ratios with in the denominator): - R2 are small, but they start to be worth paying attention to for horizon s of 1-year ahead or longer. \"Small\" and \"big\" are relative term, remember that according to the RW the R2 should be 0! Then, any R2 > 0 is \"interesting.\" - For the EW returns, / predicts 7% of the variability of one -year ahead returns and 23% of the variability of 4-year ahead returns. These ar e results that, on averag e, can produce profitable investment strategies. Rational explanations for time- variation of expected return: - Time-varying risk aversion - Time-varying amount of risk - Parallel behavior explan ation (investor sentiment). Remark: We expect low prices -relative to , , - to be followed by high returns (high prices). Going back to the EMH, can we profit from this predictability? Note: Another well-cited paper is Lamont (JF, 1998), who finds that other financial ratios also work as predictors: dividends yield & earnings yield . Lamont also find th at the dividend payout ratio has cross-sectiona l predictive power. Example: We use Shiller's data ( 1871:Jan - 2021:Dec ) predictive regressions of Fama-French (see FEc_pr og_Pred for code and links to data). Independent Variable: Excess Returns at +1 With the exception of lagged excess returns and the default yield spread (AAA yield - BBB yield) nothing is significant. \u00b6 Example: We use Shiller's data ( 1925:Jan - predictive regressions of Fama-French (see FEc_pr og_Pred for code and links to data). Independent Variable: Excess Returns at +1 excess returns are significant. We just see \"momentum\" at work at the monthly level. \u00b6 Predictive Regressions: Methodological Issues Data snooping . Are /, , Payout Ratios the only variables used in those regressions? The standard finance and economic databases used in academic and industry research (CRSP, Compustat, Refinivit) have thousands of potential predictors . Recall Type I error: If we use 100 regresso rs, 5 will be significant at the 5% level! Peso problem. In the sample, we do not observe a \"crash,\" which are very low probability events, but agents do compute that probability in the exp ectation. Then, on average, the sample average is biased! Regime Change . Always a potential problem. Maybe coefficients change with the business cycle, Fed policy, bull/bear markets, etc. Endogeneity. Regressors are only predetermined, but not exogenous . OLS slopes have a small bias (Stambaugh, 1986). Traditional OLS S.E. are likely not appropriate (Hodrick, 1992). Persistence of Financial Ratios. Valuation ratios are persistent and their innovations are correlated with returns, causing - biased predictive coefficients: Stambaugh (1999) - over-rejection by standard t-test: Cavanagh-Elliott-Stock (1995) Note: These issues are less relevant for interes t rates & recently proposed predictor variables (persistent, but less correlated with ). Predictive Regressions: Valu ation Ratios - Persistence / is persistent, / stays \"high\" or \"low\" for a long time . It moves around a constant mean (in red) & has no trend (stationary?). There is some evidence for mean reversion , but it can take many years (decades?) to get back to the mean. Given the persistence in /, the Fama-French results imply that we should also have persistence in the forecast of expected returns. That is, we have high (low) expected returns for a long time (decades?)! Issue: How persistent is /? - / is likely to be persistent: it reflects long-run expectations. - But, is / stationary? unit root? explosive? To answer the above question, we compute the ACF for /. (Recall that a persistent series will show a slow decay in the ACF.) The first order autocorrelation is 0.882 . Very persistent series! That is, next period dividend yield is very likely to be similar to this period. There seems to be a rela tion (non-linear?) between / & the business cycle. We see big spikes in / when there is a recession (clear spike in the 1930s and in 2008-2009). Though these spikes are relatively short-lived (years, not decades). Thus, expected returns vary with the business cycle (not a surprise): A big increase when there is a recession (risk is higher). Potential Problem with : \"too smooth \" (measurement error?). The observed data may not be the \"true\" series of interest. Subtle point: Since is too smooth, all the predictability comes from . What news affect more future stock prices (& return s): \"Cash Flows news or Disc ount Rates news\"? Discount rates news. Predictive Regressions: Stambaugh Bias One econometric issue in Fama and French (1989 ): Regressors are only predetermined, but not exogenous. Start with predictive regression for returns, : = + : / -i.e., the dividend price ratio Note: depends on the price at the beginning of , the change of at the end of +1 reflects changes in price from to +1 , as does ; E[|, ] 0, more generally, E[ |, ] 0, < < . Assumption ( A2) is violated! In addition, is persistent. It can be modeled with an ARMA. Stambaugh (1999) assumes that follows an AR(1) = + + (2) where & follow a multivariate N(0, ), independent across . Results: b (OLS estimate) is biased upward, positively skewed, and has higher variance and kurtosis than the normal sampling distribution of the OLS estimator. Stambaugh bias: E [ b - ] = (/ ) E[ - ] It turns out has a downward bias and is negative b shows an upward bias. Conve ntional t-tests are misleading. Finding: Correcting the bias weaken s the predictability evidence. Since conventional t-tests are misleading, th ere are many suggestions to check if the predictability of the very pers istent valuation ratios remains after correcting for the bias. One approach is Lewellen (2004): Adjust the OLS estimator under worst case scenario for persistence ( = 1): b adj = b - (/ ) E[ - 1] In practice, the estimated pers istence is very close to one. The bias correction is small. Predictability survives: - / predicts market returns from 1946-2000 and sub-samples. - B/M and / predict returns during the shorter sample 1963-2000. Interesting Result: In a (1)-(2) fr amework NW SE are not reliable in small samples. Result from Hodrick (1992) & Kim and Nelson (1993). Predictive Regressions: Lo ng Horizon Returns (Aside) / and other ratios forecast ex cess returns on stocks. Regre ssion coefficients and R2 rise with the forecast horizon. This is a result of the fact that the forecasting variable is persistent . Model (1)-(2), assuming = = 0. = + (1) = + + (2) Now, we compound 2-period returns (w ith log returns, we add them): 2 = + = ( = ( + = (1 + + = > . The previous result generalizes: = + = ( = (1 + ... + = > . The coefficient of the persistent ratio is in creasing with the horiz on of compounding returns. Note: A more complicated derivation is needed for the increase in R 2. Predictive Regression s: More Predictors Lots of variables have been propos ed as predictors. A short list: - Book-to-market (b/m t-1), equity share in new issues (S, equis t-1), and lagged returns , as in Baker and Wurgler (2000) ( B-W , next slide). - Cross-sectional premium (csp): The relative valuations of hi gh- and low-beta stocks, as in Polk, Thompson, and Vuolteenaho (2006). - Net Equity Expansion (ntis): The ratio of 12-month moving sums of net issues by NYSE listed stocks divided by the total end-of-year mark et capitalization, as in Boudoukh, et al. (2007). - Long Term Yield (lty): Long-term government bond yields. - Investment to Capital Ratio (i/k): The ratio of aggregate (p rivate nonresidential fixed) investment to aggregate capital for the whole economy, as in Cochrane (1991). - Consumption, wealth, income ratio (cay): Estimated from an equation from a model proposed by Lettau and Ludvigson (2001). Example: We use the expanded Goyal and Welch data ( 1927 - 2021 ) to redo the annual predictive regressions of Baker-Wurgler, using S&P excess returns (see FEc_prog_Pred for code & pr evious table for VW returns, equity share in new equity is significant. We also run pred ictive regressions for the othe r variables mentioned above. Investment-to-capital (ik, starting in 1947 ) was very significant, with very high R2. (Note: cay (starting in 1944 ) & csp (starting in 1937 ) were not significant). \u00b6 Predictive Regressions: Way More Predictors With the advances in computer power, the succe ss of finding predictors of future returns has continued almost exponentially. For example, using Machine Learning models (Neural Networks) we have: - Gu, Kelly and Xiu (2020): 176 predictors , grouped in 94 stock-level predictive characteristics (Green et al. (2017)); 8 macroeconomic & financial variables predictors (Welch and Goyal (2008)); and 74 industry dummies (& even 94 * 8 inte raction terms!). - Bianchi, Buchner and Tamoni (2021): 128 monthly macroeconomic and financial variables (McCracken and Ng (2015)). Always keep in mind that the standard fina nce databases for resear ch (CRSP & 1,000 potential predictors (without counting inte ractions). It is always possible to find more predictors! Question: Why not use them all? Predictive Regressions: I n-sample vs Out-of-sample In a very well know paper, Goyal and Welch (2008) argue that the in-sam ple (IS) predictability seen in predictive regression, once evaluated out-o f-sample (OOS), becomes very weak or just disappears. Setup of OOS Evaluation (1) Perform Q -step-ahead forecasts using: - Rolling predictive regressions, adding one observa tion at a time. That is, we obtain Q forecasts, . - Use the mean of the rolling period at time as the forecast. That is, we obtain Q forecasts, . (2) Get Q rolling forecast errors, , & Q mean forecasts, . (3) Compute & . (4) Evaluate MSEs using the Diebold-Mariano test. An OOS R2 can be computed as: = 1 - with = = Note: Goyal and Welch (2008) evaluate the MS Es using other tests, proposed by Clark and McCracken (2001) and McCrack en's (2004) variation of the Diebold-Mariano test. Findings: Very difficult to identify any robust pr edictor of excess stock re turns. There are short time intervals of significant OOS predictability, but these \" pockets of predictability\" are surrounded by long periods of little or no pred ictability, see Lansi ng, LeRoy & Ma (2022). Example: We use the expanded Goyal and Welch data ( 1927 - 2021 ) to compute their annual OOS R2, using rolling regressions starting in 1967 , and perform Diebold-Ma riano (DM) tests for significant differences of the forecasts (R script below for ik). Findings: Consistent with the results of G oyal and Welch (2008), we do not find a lot of consistent predictability out of sample. In general, DM tests fail to reject H 0 that the predictors do better than the unconditional mean in forecasting next year excess returns. R Code for ik (OOS rolling regressions) yy <- y_a_ik # (y_t+1) xx <- ik_a # Independent variable x_t Alles = NULL # Initialize empty (a space to put forecasts errors) k_for <- 40 # Start of Rolling Sample i <- k_for # Counter for while loop <- mean(y_tp1) <- c(f_e_a, f_e_n) # Co mbine both forecast errors in a vector Alles = rbind(Alles,f_2e) # accumula te forecast errors in rows (two columns) i <- i+1 } # Checking accuracy of forecasts with OOS R^2 mse <- [1] 0.02177127 Relative to IS results, big reduction in R 2. # Testing accuracy of forecas ts , Forecast horizon = 1, Loss f unction power = 2, p-value = 0.8975 -0.23874 , Forecast horizon = 1, Loss f unction power = 1, p-value = 0.8128 alternative hypothesis: two.sided. \u00b6 Predictive Regressions: Final Remarks There is a big and active literatur e on the predictability of stoc k returns, lately using ML/AI models. It has found lots of poten tial predictors of excess stock re turns, for example, Gu, Kelly and Xiu (2020) use Neural Networks to discover 176 predictors (with interaction terms, they almost use almost 1,000 predictors!) Given the usual data mining results in large data sets, many of the discovered predictors are not \"true predictors,\" but \"false positive lot of FP predictors will increase C.I. for forecasts. We have a typical model selection problem. If we use the General-to-specific approach, the question is: How to reduce the GUM? Several proposals: optimize , OOS SR, minimize FP predictors, etc. Old question: Can we make money from these predictors? Not clear. Lecture 11 - Volatility Models Linear and Non-linear Models So far, we have focused on linear mo dels. We have relied on Assumption ( A1), where the relation between & is given by: + , ~ ... D(0, 2) There are, however, many relationships in fina nce that are intrinsically non-linear: The payoffs to options are non-linear in some of the input variables, for example, S t; investors' willingness to trade off returns and risks are also non-linear; CEO compensation that de pends on thresholds and with a big option compone nt are also non-linear. The textbook of Campbell et al. (1997) defines a non-linear data generating process as one where the current value of is related non-linearly to current a nd previous values of the error term, : = f(, , , . . . ) where is i.i.d. and f is a non-linear function. A friendlier and slightly more specific definiti on of a non-linear model is given by the equation = g(, where g is a function of past error terms only, and 2 can be interpreted as a variance term, since it is multiplied by the current value of the error. Cases - Non-linear in mean only: g () = non-linear & 2() = 2 - Non-linear in variance only: g () = linear & 2() non-linear g () - Non-linear in mean and variance: both g () & 2() are non-linear. Most popular non-linear models in finance: The ARCH models, where we model a time-varying variance as a function of past 's. ARCH Models Until the early 1980s econometrics had focused almost solely on modeling the conditional means of time series, conditioning on information set at time t, It : = E[| It] + , ~ D(0, 2) Suppose we have an AR(1) process: = + + . The conditional mean is: E[| It] = Et[] = + The unconditional mean and variance are: E [ ] = /(1 - ) = constant V a r [ ] = 2/(1 - 2) = constant Note: Conditional mean is time varying; unconditional mean is not! Similar idea for the variance. For the AR(1) process, we have: - Conditional variance: V a r [ |It] = Et[( - Et[|It])2] Et[] = - E[])2] = - 2) unconditional variance measures overall uncerta inty. In the AR(1) example, the information available at time t, I t , plays no role: V a r [ ] = 2/(1 - 2) . The conditional variance, Var[ |It], is a better measure of uncertainty at time t. It is a function of information at time t, It. Notation: = | It Summary: - Unconditional variance measur es the overall uncertainty. - Conditional variance measures uncertainty at time t. Remark: Conditional moments are time va rying; unconditional moments are not! ARCH Models: Stylized Facts of Asset Returns (1) Thick tails : Leptokurtic (thicker tails than Normal). (2) Volatility clustering : \"Large changes tend to be followed by large changes of either sign.\" (3) Leverage Effects : Tendency for changes in stock prices to be negatively correlated with changes in volatility. (4) Non-trading Effects, Weekend Effects : When a market is closed, information accumulates at a different rate to when it is open -for example, the weekend effect, where stock price volatility on Monday is not three times the volatility on Friday. (5) Expected events : Volatility is high at regular times such as news announcements or other expected events, or even at certain times of day -for example, less volatile in th e early afternoon. (6) Volatility and se rial correlation : Inverse relationship between the two. (7) Co-movements in volatility : Volatility is positively correlated across markets/assets. We need a model that accommoda tes all these (non-linear) facts. Stylized facts (1) and (2) form th e basis of Volatility (ARCH) Models. Easy to check leptokurtosis (Stylized Fact #1). Descriptive Statistics and Distri bution for Monthly S&P500 Returns Statistic Mean (%) 0.0004) greater than 0! Heavy tail s are very common in fi nancial time series. Easy to check Volatility Clustering (Stylized Fact #2) Note: Periods with low changes, usually long, and pe riods of high changes, usually short. That is, volatility shows au tocorrelation. ARCH Models: Engle (1982) We start with assumptions ( A1) to ( A5), but with a specific ( A3'): , ~ 0, (A3') which we can write, using the L operator, as: We can write the model in terms of an AR(q) for . Define , -an error term for the variance. Then, Correlated 's: High (low) past 's produce a high (low) today. The model is an AR(q) model for squared innovations, . We have the ARCH model: Auto-Regressive Conditional Heteroskedasticity . ) variance. Non-negative constraints: Since we are dea ling with a variance, we usually impose > 0 and i > 0 for all i. Notation: = ||It-1 Useful result: Since E[ ] = 0, then [] = ARCH Models: Unconditional Variance The unconditional variance is determined by: That is, To obtain a positive 2, we impose another restriction: ( 1 ) > 0 Example: ARCH(1) , ~0, We need to impose restrictions: > 0, 1 > 0, & (1 - 1) > 0. \u00b6 ARCH Models: Leptokurtosis Errors may be serially uncorrelated, but they are not independent: There will be volatility clustering, which produces fat tails. We want to calculate the kurtosis of the errors: / We define standardized errors: They have conditional mean zero and a time invari ant conditional variance equal to 1. That is, z t ~ N(0, 1). From the definition of we have: = Now, we compute the four th (also central, since E[ ]=0) moment: Then, using Jensen's inequality: 3 /3. where we have used the fact that since E[ ] = 0, then E[ ] = E[]. Technical point: It can be show n that for an ARCH(1), the 4 th moment for an ARCH(1): 31 13 i f 3 1. More convenient, but less intuitive, presentation of the ARCH(1) model: , ~ 0, 1 that is, is i.i.d. with mean 0, and Var[ ]=1. Since is i.i.d., then: which delivers the AR(1) representation for . Also, if we assume is normally distributed, then ~ 0,. GARCH Model: Bollerslev (1986) An early technique to determine q was to look at the ACF/PACF for squared returns, , which usually determined a very large q. Example: We calculate the ACF and PA CF for the squared of the U.S. monthly stock returns (1871-2020). Note: Highly autocorrelated squared returns. To accommodate the long autocorrelations, we use large q. This result is not surprising, is a very persistent process. Pe rsistent processes can be captured with an AR( p), where p is large. This is not efficient. Following the idea of an ARMA process, we can use a more parsimonious representation of the ARCH model: The Generalized ARCH model or GARCH(q, p): which can be shown it is an ARMA(max( p,q), p) model for the squared innovations. Popular GARCH model: GARCH(1,1): with an unconditional variance: Var[ ] = 2 = /(1 - 1 - 1). Restrictions: > 0, 1 > 0, 1 > 0; (1 - 1 - 1) > 0. Technical details: This is covariance stationary if all the roots of (L) + (L) = 1 lie outside the unit circle. For the GARCH(1,1) this amounts to 1 + 1 < 1. Question: What should be the order of the GARCH Model? We should use enough lags to make sure the resi duals do not have any more autocorrelation in the square residuals. If the order of GARCH process is well determined, the ACF/PACF for should show no significant autocorrelations. We can add lags until the tests for ARCH structur e in the squared residuals, discussed later, are not longer significant. A GARCH(1,1) is a ve ry good starting point. GARCH-X In the GARCH-X model, exogenous variables are added to the conditional variance equation. Consider the GARCH(1,1)-X model: + f(Xt-1, ), where f(Xt, ) is strictly positive for all t. Usually, Xt, is an observed economic variable or indicator, for example, a liquidity index, and f(.) is a non-linear transformation, which should be non-negative. Examples : We can use 3-mo T-bill rates for modeling stock return volatility, or interest rate differentials between countries to model FX return volatility. The US congressional budget office uses infla tion in an ARCH(1) model for interest rate spreads. \u00b6 ARCH Estimation: MLE All of these models can be estimated by maximu m likelihood. First we need to construct the sample likelihood. Since we are dealing with dependent variables, we use the conditioning trick to get the joint distribution: ,,...,;|;|,,;|,,,,; * log|,...,,,...,; log|,; We maximize this with respect to the k mean parameters ( ) and the m variance parameters ) Variance equation: We write the for the normal distribution, |, , exp = exp We form the likelihood L (the joint pdf): L exp 2/ exp We take logs to form the log likelihood, = log L: log 2log21 2log1 2 / Then, we maximize with respect to = (, , ) the function . 2log21 2log 1 2 / Taking derivatives with respect to = (, 1, ), where is a vector of k 1/2 1/ 1/2/ 1/2/ 1/2/ / ( kx1 vector derivatives) We form the f.o.c.; that is, we wr ite the first derivative vectors as and, then, set it equal to 0: = S(y t, ) = 0 -a ( k+2) system of equations. The vector of first derivatives is called the score vector, S(y t, ). Take the last f.o.c., the kx1 vector, 0: /, = /, = 0 , , , = 0 The last equation shows that MLE is GLS for the mean parameters, : each observation is weighted by the inverse of ,. We have a ( k+2) system. It is a non-linear system. The system is solved using numerical optimization (usually, with the Newton-Raphson method). \u00b6 Technical Note: If the conditional density for is well specified and 0 (the true parameter) belongs to the parameter space, , then ,, where , A0 is the matrix of second derivatives of the log likelihood, . It is called the Hessian . In general, it is difficult to numerically compute and make sure it is positive definite (so it can be inverted), especially when the dimensions are big. There a lot of computational tricks to comput e a Hessian that is invertible, the most popular algorithm is the Broyden-Fletcher-Goldfarb-Shanno, or \" BFGS .\" ARCH Estimation: MLE - Standard Errors Under the correct specification assumption, A 0 = B 0, where ,,, We estimate A 0 and B 0 by replacing 0 by its estimated MLE value, MLE . The estimator B 0 has a computational advantage over A 0.: Only first derivatives are needed. But A0 = B 0 only if the distribution is corr ectly specified. This is very difficult to know in practice. Common practice in empirical studies: Assume the necessary regularity conditions are satisfied. ARCH Estimation: Nume rical Optimization In general, we have a ( k+m x k+m) system; k mean parameters and m variance parameters. But, it is a non-linear system. We use numerical optimization, which are methods that search over the parameter space looking for the values th at maximize the log likelihood function. In R, the function optim does numerical optimization. It mi nimizes any non-linear function. It needs as inputs: - Initial values for the parameters, 0. - Function to be minimized (includes the GARCH process). - Data used. - Other optional inputs: Choice of method, hessian = function to be minimized. \u00b6 Initial values: - Numerical optimization needs initial values for , say 0. It is very common to find that the optimization is sensitive to the in itial values. It is a good practice to try different sets of initial values. We want to avoid selecting a local maximum: Initial values (continuation): - Numerical optimization needs initial values for , say 0. It is very common to find that the optimization is sensitive to the in itial values. It is a good practice to try different sets of initial values. We want to avoid selecting a local maximum: - Given the autoregressive structure in , and sometimes we have AR(p) in the mean, we need to make assumptions about 0 and the 0, ..., q (and 0, 1 , ..., p if we assume an AR(p) process for the mean). Usual assumptions: 0 = unconditional SD; 0 = 1 = ...= p= 0. - Alternatively, we can take 0 (and 0, 1, ..., p) as parameters to be estimated (it can be computationally more intensive and estimation can lose power.) ARCH Estimation: MLE - Example (in R) Log likelihood of AR(1)-GARCH(1,1) 2:n) {u[t] = r[t] - mu - rho1*r[t-1]} # this setup allows for ARMA in mean vector(lengt h=n); h <- ts(h) = omega/chk0 initial value for h[t] series if (chk0==0) {h[1]=.000001} # check to avoid dividing by 0 for (t in 2:n) {h[t] = abs(omega + alpha 1*(u[t-1]^2) + beta1*h[t-1]) {h[t]=.00001} } # check to avoid log(0) return(-1*sum(- 0.5 * log(abs(h[2:n])) - 0.5 * (u[2:n]^2)/abs(h[2:n]))) } # I use optim to minimize a function, to maximize multiply by -1 Example 1 : GARCH(1,1) model for changes in CHF/USD . We will use R function optim (mln can also be used) to maximize # estimated parameters I_Var_m2 quency=12, plot.ts(chf_usd) # time series plot of data > logL_g11 # Log likelihood value [1] -1745.197 > ml_2$par of 2nd derivatives) > eigen(I_Var_m2) # Check if Hessian is pd to log(S t-1)] , It ~ N(0, ) T: 562 (January 1971 - July 2020, monthly). The estimated model for e f,t is given + \u00df 1 = .90 < 1. (Persistent.) \u00b6 Example 2 : Using Robert Shiller's monthly data set for the S&P 500 (1871:Jan - 2020:Aug, T=1,795), we estimate an AR(1)-GARCH(1,1) model: = t) - , It-1 ~ N(0, ) The estimated model for s t is given by: rt = 0.338 + Note: 1 + \u00df 1 = .952 < 1. (Very persistent.) Above, we plot the time-varying va riance. Certain events are clea rly different, for example, the 1930 great depression, with a peak variance of 282 (18 times unconditional variance!). The covid-19 volatility similar to the 2008-2009 financial crisis recession. \u00b6 GARCH: Forecasting and Persistence Consider the forecast in a GARCH(1,1) model: Taking expectation at time t 1 Then, by repeated substitutions: Assuming ( ) < 1, as j , the forecast reverts to the unconditional variance: 2 = /(1 - 1 - 1). When 1 + 1 = 1, today's volatility affect future forecasts forever: Example 1 : We want to forecast next month (September 2020) variance for CHF/USD changes. Recall we estimated : = 0.00012 + 0.19003 + 0.71007 . getting : : = sqrt( 0.00367 ) = 6.1%) We based the : forecast on: Then, = 0.190 + 0.710 = 0.900 : : 0.00012 0.00367 0.9) = 0.003423 also : {1+ (0.9)+ (0.9)2} + 0.00367 * (0.9)3 for March 2021: : :0.00012 * {1 + ( * (0.9)6 = 0.002512659 Remark: We observe that as the forecast horizon increases (j ), the forecast reverts to the unconditional variance: /(1 - 1 - 1) = 0.00012 /(1 - 0.9) = 0.0012 = sqrt(0.0012) = close to sample SD = 3.36% ). \u00b6 Example 2 : On August 2020, we forecast the December's variance for the S&P500 changes. Recall we estimated : = 0.756 + 0.125 + 0.826 , getting : = 43.037841 We based the : forecast on: Then, s * (0.952 )4 = 38.02797 Lower variance forecasted for the end of the year, but still far from the unconditional variance of 15.4. \u00b6 GARCH: Forecasting - Application to VaR Example : In September 2020, Swiss Cruises wants to construct a VaR-mean for the USD 1 M receivable in 30 days (October). Data Receivable: USD 1 M S t=2020:9 = 1.45 CHF/USD ef,t=2020:9 = 0.01934126 : - 2.33 * sqrt( : :)} :, : = -0.00211 + 0.026 * -0.1999941 M Interpretation of VaR-mean: Relative to today's valuation (or expected valuation , according to RWM), the maximum expected loss with a 99% \"chance\" is CHF -0.20 M . We also derive this value, using the sample mean and sample SD: sample mean = -0.00259 sample SD = 0.033357 VaR-mean(.99) = CHF 1.45M * (-0.00259 = CHF - 0.1164491 Remark: The GARCH forecast reflects the highe r than average uncertainty in 2020:9 (Covid-19, presidential elections). \u00b6 GARCH: Rugarch Package GARCH estimation requires numerical optimization, which is dependent on initial values. The R package does a good job at estima ting ARMA-GARCH models, allo wing for different models and performing a lot of specification tests. You need to specify the model (\" specs \") first, for example, you want to estimate an AR(1)- GARCH(1,1) with a constant in the mea n. Then, you estimate the model with the ugarchfit command. Example: We estimate an AR(1)-GARCH(1,1) for the historical U.S. monthly returns (1871 - 2020, T = 1,797). x <- lr_p # SP500 long run monthly returns library(rugarch) # You need to install package first! mod_gar norm Optimal Parameters ------------- ------------- lags in mean. d.o.f=1 H0 : No serial correlation Weighted Ljung-Box Test IGARCH Recall the technical detail: The standard GARCH model: is covariance stationary if (1) + (1) < 1. But strict stationarity does not re quire such a stringent restriction In the GARCH(1,1) model, if 1 + 1 =1, we have the Integrat ed GARCH (IGARCH) model. In the IGARCH model, the autoregressive pol ynomial in the ARMA re presentation has a unit root: a shock to the conditional variance is \" persistent .\" Variance forecasts are generated with: today's variance remains important for all future forecasts. This is persistence! Variance forecasts are generated with: That is, today's variance remains importan t for future forecasts of all horizons. In practice (see previous Ex ample 2 for the S&P 500 data), it is often found that 1 + 1 are close to 1. GARCH: Variations - GARCH-in-mean The time-varying a dynamic mean-variance relations. It desc ribes a specific form of the risk-return trade- off. Finance intuition says that has to be positive and significant . However, in empirical work, it does not work well: is not significant or negative. GARCH: Variations - Asymmetric 1993): where = 1 if < 0; = 0 otherwise. Using the indicator variable , this model captures sign (asymmetric) effects in volatility: Negative news ( < 0) increase the conditional volatility ( leverage effect ). The GARCH(1,1) version: where = 1 if < 0; = 0 otherwise. When < 0 > 0 This is a very popular variation of the GARCH models. The leverage effect is significant. There is another variation, th e Exponential GARCH, or EGARCH , that also captures the asymmetric effect of negative news on the conditional variance. GARCH: Variations - NARCH Non-linear ARCH model NARCH - Higgins and Bera (199 2) and Hentschel (1995). These models apply the Box-Cox-type tr ansformation to the conditional variance: || Special case: = 2 (standard GARCH model). Note: The variance depends on both the size and the sign of the va riance which helps to capture leverage type (asymmetric) effects. ARCH Estimation: MLE - Regularity Conditions Technical Note: The appeal of MLE is the optimal properties of the resulting estimators under ideal conditions. However, these id eal conditions, which are called \" regularity conditions ,\" are difficult to verify for ARCH models Block-diagonality In many applications of ARCH mo dels, the parameters can be partitioned into mean parameters, 1, and variance parameters, 2. Thus, the Information matrix ( Hessian) is block-diagonal. Not a bad result: - Regression can be consistently done with OLS. - Asymptotically efficient estimates for the AR CH parameters can be obtained on the basis of the OLS residuals. But: - Conventional OLS standard er rors could be terrible. - When testing for autocorrelation, in the pres ence of ARCH, the conven tional Bartlett s.e. - T - 1/2- could seriously underestimate the true standard errors. ARCH Estimation: Non-Normality The basic GARCH model allows a certain amount of leptokurtosis. It is often insufficient to explain real world data. Solution: Assume a distribution, other than th e normal, that can produ ce fatter tails in the distribution. t Distribution - Bollerslev (1987) The t distribution has a degrees of freedom pa rameter which allows greater kurtosis. The likelihood function for observation t is: ) ln(5.0) 1()2()5.0())1(5.0(ln(2 2/)1(1 2/1 1 tv t t vz vv v l where is the gamma function and v is the degrees of freedom. As , this tends to the normal distribution. ARCH: Testing Standard BP test, where we test H 0: 1 = 2 = ... = q= 0. Steps: - Step 1 . (Same as BP's Step 1). Run OLS on DGP: y = X + . Keep residuals, e t. - Step 2 . (Auxiliary Regression). Regress e t2 on e t-12, ...., e t-q2 e t2 = 0 + 1 et-12 + .... + m et-q2 + v t. Keep R2, say . - Step 3 . Compute the statistic: LM = ( T - q ) . Example: We do an ARCH Test with 4 lags, for th e AR(1) residuals of log changes in the lm_t [1] 17.08195 Reject H 0 a p-value of 0.001 . \u00b6 ARCH: Testing - Ignoring ARCH In ARCH Models, testing as usual: LR, Wald, and LM tests. Suppose ARCH is detected, but ARCH is ignored. What are the consequences of ignoring ARCH? Ignoring ARCH - Suppose y t has an AR structure: y t = 0 + 1 yt-1 + t, tIt-1 ~ N(0, 2t). with ARCH structure in the e rror term, but ARCH is ignored. Then, we fit the AR(1) model using OLS. - Simulations find that OLS t-test with no correction for ARCH spuriously reject H 0: 1 = 0 with arbitrarily high probability for sufficiently large T. - If White's (1980) SE are used, the re sults are better. NW SE help less. Figure. From Hamilton (2008). Fraction of samples in which OLS t-test leads to rejection of H 0: 1 = 0 as a function of T for regression with Normal errors (solid blue line) and Student's t errors (dashed green line). Note: H 0 is actually true & the t-test is evaluated at the 5% level. ARCH: Which Model to Use Questions 1) Lots of ARCH models. Which one to use? 2) Choice of p and q. How many lags to use? Hansen and Lunde (2004) compared lots of ARCH models: - It turns out that the GARCH(1, 1) is a great starting model. - Add a leverage effect for financ ial series and it's even better. - A t-distribution is also a good addition. RV Models: Intuition The idea of realized volatility is to estimate the latent (unobserve d) variance using the realized data, without any modeling. Recall th e definition of sample variance: Suppose we want to calculate the daily variance for stock returns. We know how to compute it: we use daily information, for T days, and apply the above definition. Alternatively, we use hourly data for the whole day (with k hours). Since hourly returns are very small, ignoring seems OK. We use , as the ith hourly variance on day t. Then, we add , over the day: , In more general terms, we use higher frequenc y data to estimate a lower frequency variance: , where rt,i is the realized returns in (higher frequency) interval i of the (lower frequency) period t. We estimate the t-frequency variance, using k i-intervals . If we have daily returns and we want to estimate the monthly variance, then, k is equal to the number of days in a month. It can be shown that as the interval i becomes smaller ( i 0), Return Variation [ t - 1, t]. That is, with an increasing number of observati ons we get an accurate measure of the latent variance. RV Models: High Frequency Note that RV is a model-free measure of variation -i.e., no n eed for ARCH-family specifications. The measure is called realized variance (RV). The square root of the realized variance is the realized volatility (RVol, RealVol): Given the previous theoretical result, RV is commonly used with intra-daily data, called high frequency (HF) data. It lead to a revolution in the field of volatility, creating new m odels and new ways of thinking about volatility and how to model it. We usually associate realized volatility with an observable proxy of the unobserved volatility. RV Models: High Fre quency - Tick Data As mentioned above, the theory behind realized variation measures dictates that the sampling frequency, or k in the RV t formula above, goes to . Then, use the highest frequency available, say millisecond to millisecond returns. Intra-daily data applications are the most common. But, when using intra-daily data, RV calculations are affected by microstructure effect s: bid-ask bounce, infreq uent trading, calendar rt,i does not look uncorrelated. Example: The bid-ask bounce induces serial correlati on in intra-day returns, which biases RV t. The usual solutions: (1) Filter data using an ARMA model to get rid of the autocorrelations and/or dummy variables to get rid of calendar effects. Then, used the filtered data to compute RV t. (2) Sample at frequencies where the impact of microstructure effects is minimized and/or eliminated. We will follow solution (2). RV Models: High F requency - Practice In intra-daily RV estimation, it is common to use 10' intervals. They have good properties. However, there are estimations with 1' intervals. Some studies suggest using an optimal frequency, where optimal frequency is the one that minimizes the MSE. Hansen and Lunde (2006) find that for very liquid assets, such as the S&P 500 index, a 5' sampling frequency provides a reasonable choice. Thus, to calculate daily RV, we need to add 78 five-minute intervals. Example: Based on TAQ ( Trade and Quote ) NYSE data, we use 5' realized returns to calculate 30' variances -i.e., during the jth interval on the half hour t. The n, we calculate 30' variances for the whole day -i.e., we calculate 13 variances, si nce the trading day goes from 9:30 AM to 4:00 PM. The Realized Volatility, RVol, is: Example: Below, we show the first transaction of the SPY TAQ (Trade and Quote ) data (tick- by-tick trade data) on January 2, 2014 . SYMBOL DATE TIME PRICE SIZE SPY 20140102 9:30:00 183.97 400 Example: we show the first transaction of the AAPL TAQ (Trade and Quote ) data (tick- by-tick quote data) on January 2, 2014: 4 AM SYMBOL DATE TIME BID OFR BIDSIZ OFRSIZ MODE EX AAPL T 1 1 12 P AAPL 1 1 12 P 1 12 T 1 1 12 P 1 1 12 T 1 1 12 T 1 2 12 P 1 1 12 T 558.83 5 2 12 P RV Models: High Frequency - Working with Tick Data Example: We read SPY trade data for read.csv(\"https://www.bauer .uh.edu/rsusmel/4397/SPY_2014.csv\", Min. :20140102 9:30:00 : 21436 Min. 1st Qu.:20140110 16:00:00: 11352 1st Qu.:178.9 1st :20140121 9:30:01 : 5922 Median :182.6 Median 100 Median :0 Mean :20140119 15:59:59: 4090 Mean :181.4 Mean : 337 15:59:55: 3198 EX :0.0e+00 :3351783 18057 :1062382 Qu.:0.0e+00 4 : 9098 K : Max. 8142 J : 356539 (Other): 1194 (Other): 777625 Now, we calculate using 5'-returns a daily real ized volatilitiy for the first 4 days in 2014 (2014:01:02 - 2014:01:07). Originally, we have # De fine a specific time series data set # pt pastes together DATE and Time. spy_p <- as.numeric(hf_1$PRICE) # Read mean(spy_ret) sd(spy_ret) Very noisy data, with lots of \"jumps\": tick by tic k return: -3.7365e-09 Tick-by-tick SD: 6.3163e-05 For -4.796933e-09 > sd(spy_ret) the TAQ SPY data: Autocorrelations of series 'spy_ret', by lag 0 1 2 3 4 5 6 7 8 9 1 st-order autocorrelation: -0.459. We aggregate the tick-by-tick data in 5' intervals using the function aggregateTrades in the R package highfrequency . It needs as an input an xts object (hf_1, for us). library(highfrequency) spy_5 <- aggregateTrades( hf_1, on = \"minutes\", # you can use al so seconds, days, weeks, etc. k = 5, # number of units in for \"on\" acf_spy_5 Autocorrelations of series 'spy_ret', by lag 0 1 2 3 4 5 6 7 8 9 -0.105 , thought not significant. However, the autocorrelation of order 5 is significant. We plot the 10-minute TAQ SPY return data. the 10' TAQ SPY return data: Note: Now, none of the autocorr elations is significant. The 10-minute returns look independent. \u00b6 RV Models: High Freque ncy - TAQ In Practice In practice, 10' returns are common. To form a daily measure for RV, we have 39 10-minute returns plus one overnite return (f rom 16:00 PM to next day 9:30 AM) We have some technical issues working with tick data: - Not all days the stock market is open from 9: 30 AM to 16:00 PM, NYSE closes early on certain days (Christmas Eve, Thanksgiving). - For many stocks, we do have lapses in trading. For these stocks, using 5' or 10' intervals may not work well. - There are many suggested solutions to the pr oblem of infrequent trading. Usual solution: interpolation from quote data. - We have a lot of (discrete) jumps in the data. Example: R script to compute monthly realized volatility for MSCI MSCI USA Index T <- length(x) rvs=NULL # create vector to fill with RV i <- 1 k <- 21 # k: observations per period (78 for 5' data) while (i < T - k) { s2 <- sum(x[i:(i+k)]^2) # realized variance i rbind(rvs,s2) } <- sd(rvol) # variance Example: Using daily MSCI USA data we calculate 1-mo Realized Volatility ( k=21 days) for log returns for the USA MSCI (1970: Jan - 2020: Oct). > mean(rvol) # average monthly Rvol in the sample [1] 0.04326531 very close to monthly S&P Volatility: 4.49% > sd(rvol) # standard deviation of monthly Rvol in the sample [1] 0.02592653 dividing by sqrt( T) we get the SE = 0.001 (very small). \u00b6 Technical computing points: We use k=21 days, which is an average of the tradi ng days per month. Of course, not all months have the same amount of trading days. In 2019, Fe bruary had the fewest (19) and October the most (23), but, in 2018, February and September (18) had the fewest and August the most (23). For us, k=21 days is an approximation. To be precise, if we use daily da ta to calculate a monthly variance , we need to use an exact index of trading days, say, K=[ k 1, k2, k3, ... k J] where ki is the exact number of trading days in month- year i. In addition, for daily data, we should not i gnore the mean in the computation of RV. Example : Below, the while loop in R is modified to incorporate the vector K (c1) of exact trading days for el/4397/MSCI_d_count_days.t xt\",header=FALSE) c1 <- MSCI_cd[,1] in a month n_c1 <- length(c1) # Total number of days in sample rvs=NULL # Initialize empty vector to place RVs t <- 1 # index for the days for while loop tj <- 1 # index for the months for while loop x_m = mean(x) while (tj <= n_c1) { mj <- c1[tj] # reading ex act number of days for month tj xx <- x[t:(t+mj-1)] - x_m # daily returns (in deviation from mean) per month tj s2 <- sum(xx^2) # RV for month tj t <- t + mj tj <- tj + 1 rvs <- rbind(rvs,s2) # add RV for month tj to vector rvs } rvol <- sqrt(rvs) # realized volatility > The results (mean, SD and shape of RV) are ve ry similar, but if used to compare to other monthly volatility estimates, these are the correct monthly RVol estimates. \u00b6 RV Models: Log Approximation Rules The log approximations rules for the variance and SD are used to change frequencies for the RV and RVol. For example, suppose we are calculating RV based on frequency j, RV t=j; but we are interested in the J-period RV t=J. Then, the J-period (with j inte rvals) realized variance and realized volatility can be calculated as Example: We calculate using 5' data the daily realized variance, RV t=daily . Then, the annual variance can be calculated as 260 where 260 is the number of trading days in the ye ar. The annualized RVOL is the squared root of : 260 Example: Using daily data we calculate 3-mo Realized Volatility ( k=66 days) for log returns for the MSCI (1970: March - 2020: Oct). > mean(rvol) # average Rvol in the sample [1] sd(rvol) # standard deviat in the sample [1] 0.02592653. \u00b6 RV Models: Properties Under some conditions (bounded kurtosis and auto correlation of squared returns less than 1), RV t is consistent. Realized volatility is a meas ure. It has a distribution. For returns, the distribution of RV is non-normal (as expected). It tends to be skewed right and leptokurtic. Daily returns standardized by RVol measures are nearly Gaussian. RV is highly persistent. (Check with a LB test.) Daily RV calculate with in tra-daily data, it is found to be mo re robust than measures using daily data, like GARCH. RV Models: ACF and Persistence Like all volatility measures, R VOL is highly autocorrelated. Example: We plot the ACF and PACF for the 1-mo R ealized Volatility, based on daily data for the monthly USA MSCI data. Model: AR(2)? RV Models: Forecasting We can fit ARMA models to the R VOL series to generate forecasts. Example: Based on the ACF and PACF, we fit an AR(2) model for the monthly RVOL, calculated from -3128.92 df: 3. Total lags used: 10 AR(2) model seems to pass diagnostic tests. Now, we forecast RVOL. fcast_rvol <- forecast(fit_rvol_a forecasts > fcast_rvol Point index (\" fear index \") is a forecast for the next 30-day volatility, derived from S&P 500 options. The VIX on Sep 30, 2020 was 26.37, that is, the volatility at the end of October is expected to be 26.37% annualized or 7.61% mont hly, higher than 5.20%, but, well within the 95% C.I. (More on this later.) RV Models: Forecasting - Using VIX Empirical work uses the VIX to calculate the implied volatility, IV t, for the S&P500. The VIX index is based on the S&P500 index options (on a panel of S&P 500 option prices), using the \"model-free\" approach tailored to replicate the (a nnualized) risk-neutral vol atility of a fixed 30- day maturity. Example: We use VIX to forecast monthly RV based on daily data (1990:May - 2020:Sep). We regress R V t+1 = move first observation (RV t+1) VIX/sqrt(12) # VIX > 0.01 ' ' 1 Residual standard error: 1.967 on 363 degrees of freedom Multiple R-squared: 0.5358 , Adjusted R-squared: 0.5345 DF, p-value: < 2.2e-16 Note: In sample, a strong positive predictive relation. Note: There is good match between the two series. RVOL shocks (Financial crisis, Covid) are unexpected by IV. We also check the contemporaneous relation between RVOL and VIX. ' ' 1 Residual standard error: 1.436 on 363 degrees of Risk Premium (VRP) The implied volatility of an option, calculated today, or IV t, is a measure of the (\"ex ante\") expected variance over the re maining life of the option. The Black-Scholes (BS) and similar models for option prices produce the same option prices as would be seen under modified proba bilities in a world of investors who were indifferent to risk (risk neutral ). IV & other parameters extracted from opti ons market prices embed these modified \"risk neutral\" probabilities, that combine investors' objec tive predictions of the real world returns distribution with their risk preferences. Under BS assumptions, IV and market volatility are the same. But, BS assumptions do not hold. The VRP uses this disparity. We define the variance risk premium (VRP ) as the difference between the \"ex-ante\" risk neutral expectation at time t of the future return variation over the period [ t, t+1 ] time interval and the ex-post realized return variation over the [ t - 1, t]: V R P t = IV t RV t. It is an ad-hoc definition , we could have defined VRP t based on the expectation at time t for RV t+1, in this case E t[RV t+1]. The one-step-ahead forecast can be obtained using an ARMA process for RV t. In practice, using E t[RV t+1] or RV t, does not affect VRP t that much. The are many ways to calculate IV: based on models, like the BS, or \" model free ,\" similar to how we calculated IV, in this case, using changes in option prices for different strike prices and computing an average. Example: We plot IV t(=VIX), RV t & VRP t for the S&P500 Index (shaded blue area are U.S. recessions). Data: Monthly 1990-2008. Bollerslev et al. (2009) use 5' intervals to calculate RV t find that VRP t is a predictor of stock market excess returns at different horizons ( t+h). That is, they regress: r t+h - = [log(P t) log(P t-1)] = + VRP t + They find that is positive and has a t-stat =1.76 for monthly data ( h=1) and a t-stat = 2.86 for quarterly data ( h=3). The R2 is 1.07% for monthly data and 6. 82% for quarterly data. For annual data the t-stat is not significant. Example: We regress excess next-month returns, usi ng the FF vrp) # Predictive regression > summary(pred_vrp) 0.01 '*' 0.05 ' ' 1 Residual standard error: 0. 04346 on 363 degrees of freedom Multiple t)]2 /(4ln(2) T)}, where H t is the highest price and L t is the lowest price. There is an RV counterpart, using HF data: Realized Range (RR): RR t,j and L t,j are the highest and lowest price in the jth interval. These \"range\" estimators are very good and very efficient. These estimators can be applied to intra-da ily data. The Realized Range works well with combined with other models. Stochastic volatility (SV/SVOL) models Now, instead of a known volatility at time t, like ARCH models, we allo w for a stochastic shock to t, t or t: + , ~ 0, Or using logs: log log + , ~ 0, The difference with ARCH models: The shocks th at govern the volatility are not necessarily the shocks to the mean process, t's. Usually, the standard model centers log volatility around : log log - ) + , Then, E[log( t)] = Var[log( t)] = 2 = 2/(1 - 2). Unconditional distribution: log( t) ~ N(, 2) Like ARCH models, SV mode ls produce returns with kur tosis > 3 (and, also, positive autocorrelations between squared excess returns). We have 3 SVOL parameters to estimate: = (, , ). Estimation: The modern approach uses Bayesian methods (MCMC), which are advanced for this class. Brooks discusses the "}