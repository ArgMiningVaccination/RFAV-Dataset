{"title": "At what cost (overzealous) reactions?", "author": "Peter Kissinger", "url": "https://www.drugdiscoverynews.com/at-what-cost-overzealous-reactions-2399", "hostname": "drugdiscoverynews.com", "description": "Franklin Roosevelt suggested that \"the only thing we have to fear is fear itself.\" There are many other things to fear, and one of them is poorly reported medical data.", "sitename": "Drug Discovery News Magazine", "date": "2008-09-18", "cleaned_text": "At what cost (overzealous) reactions? Franklin Roosevelt suggested that \"the only thing we have to fear is fear itself.\" There are many other things to fear, and one of them is poorly reported medical data. \"Oh what a tangled web we weave, when first we practice to deceive.\" \u2014 Sir Walter Scott I've always liked this quote given that it reminds me of the wooden puppet Pinocchio, whose nose grows longer every time he tells a lie. It also conjures up the image of public figures who've had troubles with their peccadilloes evolving into major events in response to their own denials. Is information left out of a press report a lie when an intentional oversight, a mistake when innocently forgotten, or a sign of ignorance in the hands of a non-expert? I propose a higher journalism standard that would help the credibility of the life sciences and protect patients and market caps alike. A few weeks back, I noticed multiple reports of a problem with a drug whereby six people encountered a serious side effect and two of them died from it. I read about a half dozen of these reports in various media. There was the suggestion that it made sense to consider a change in the product insert to alert physicians and patients to this deadly possibility. I can't see any reason to disagree with being cautious and staying alert. On the other hand, not a single report told me the number of patients who had been prescribed the drug and for how long. How many others prescribed the drug had died of automobile accidents or fell out of a tree? How many died of other causes? One should never take adverse events lightly, including idiosyncratic side effects that are not understood. Such information can be of great help in eventually sorting out very complex biological science. On the other hand, why can't we present such information in a way that affords the general public a meaningful risk assessment? It is more important for the public to be educated rather than frightened. The same is true for the investing public. We can't fairly assess investment risk from such meaningless babble. Or can we? We do assess the risk of panic whereby those who don't know what the report really means sell shares in fright. We may then join the herd and do the same. I discussed this with a physician and vice president of a major pharma company who concurred that the denominator in a fraction is important. It is odd how often it is left out. We teach kids fractions, but writers don't remember that 6/100 and 6/100,000 are different concepts than just 6. Perhaps we can work toward improvements. Much of what we read in both the popular and business press is subject to misinterpretation because it does not fairly cover important details. A headline that suggests a \"50 percent increase in cardiovascular side effects\" for a new drug versus an old drug doesn't tell us much of interest unless we know the inclusion-exclusion criteria for the study, the number of subjects and the side effect rate for the older drug. An increase from four to six heart attacks out of 10,000 subjects who were males between 25 and 50 is quite a different result from a study of 500 men and women over 70 where the adverse events increased from 20 to 30. There also should be indications of the expected events for a comparable group receiving no treatment at all. It strikes me as odd that for political polls, we learn the size of the survey and also a margin of error. We rarely see either in public announcements of clinical results. As scientists, it's our duty to insist that clinical observations not be imbued with either a positive or negative bias. Human nature makes perfection in this respect impossible. Skepticism and enthusiasm make us human, but let's at least not be caught omitting important numbers. Admittedly, the scientific literature is highly biased by the fact that positive results get published and negative results most often do not. At least in the case of clinical trials there is now transparency in that the initiation and results of all drug trials are reported on the Web (see for example Disease is a serious matter. Direct marketing of drugs to consumers is not all bad, but seeing smiling faces on actors while side effects are rattled off in a meaningless fashion has become one more joke for late-night TV, one more unintended consequence. My stress level has just gone up a notch as I put this column to bed. The Wall Street Journal (Sept. 3) reports that \"Elevated Rate of Teen Suicide Stirs Concern\" and that the \"trend is linked to drop in use of antidepressants after FDA raised worries about risks\" and made a decision in 2004 to add black box warnings on labels for SSRI antidepressants. Parents were frightened by a lack of understanding and children may have died as a result. Franklin Roosevelt suggested that \"the only thing we have to fear is fear itself.\" There are many other things to fear, and one of them is poorly reported medical data. DDN I've always liked this quote given that it reminds me of the wooden puppet Pinocchio, whose nose grows longer every time he tells a lie. It also conjures up the image of public figures who've had troubles with their peccadilloes evolving into major events in response to their own denials. Is information left out of a press report a lie when an intentional oversight, a mistake when innocently forgotten, or a sign of ignorance in the hands of a non-expert? I propose a higher journalism standard that would help the credibility of the life sciences and protect patients and market caps alike. A few weeks back, I noticed multiple reports of a problem with a drug whereby six people encountered a serious side effect and two of them died from it. I read about a half dozen of these reports in various media. There was the suggestion that it made sense to consider a change in the product insert to alert physicians and patients to this deadly possibility. I can't see any reason to disagree with being cautious and staying alert. On the other hand, not a single report told me the number of patients who had been prescribed the drug and for how long. How many others prescribed the drug had died of automobile accidents or fell out of a tree? How many died of other causes? One should never take adverse events lightly, including idiosyncratic side effects that are not understood. Such information can be of great help in eventually sorting out very complex biological science. On the other hand, why can't we present such information in a way that affords the general public a meaningful risk assessment? It is more important for the public to be educated rather than frightened. The same is true for the investing public. We can't fairly assess investment risk from such meaningless babble. Or can we? We do assess the risk of panic whereby those who don't know what the report really means sell shares in fright. We may then join the herd and do the same. I discussed this with a physician and vice president of a major pharma company who concurred that the denominator in a fraction is important. It is odd how often it is left out. We teach kids fractions, but writers don't remember that 6/100 and 6/100,000 are different concepts than just 6. Perhaps we can work toward improvements. Much of what we read in both the popular and business press is subject to misinterpretation because it does not fairly cover important details. A headline that suggests a \"50 percent increase in cardiovascular side effects\" for a new drug versus an old drug doesn't tell us much of interest unless we know the inclusion-exclusion criteria for the study, the number of subjects and the side effect rate for the older drug. An increase from four to six heart attacks out of 10,000 subjects who were males between 25 and 50 is quite a different result from a study of 500 men and women over 70 where the adverse events increased from 20 to 30. There also should be indications of the expected events for a comparable group receiving no treatment at all. It strikes me as odd that for political polls, we learn the size of the survey and also a margin of error. We rarely see either in public announcements of clinical results. As scientists, it's our duty to insist that clinical observations not be imbued with either a positive or negative bias. Human nature makes perfection in this respect impossible. Skepticism and enthusiasm make us human, but let's at least not be caught omitting important numbers. Admittedly, the scientific literature is highly biased by the fact that positive results get published and negative results most often do not. At least in the case of clinical trials there is now transparency in that the initiation and results of all drug trials are reported on the Web (see for example [http://www.clinicaltrials.gov/](http://www.clinicaltrials.gov/)). Disease is a serious matter. Direct marketing of drugs to consumers is not all bad, but seeing smiling faces on actors while side effects are rattled off in a meaningless fashion has become one more joke for late-night TV, one more unintended consequence. My stress level has just gone up a notch as I put this column to bed. The Wall Street Journal (Sept. 3) reports that \"Elevated Rate of Teen Suicide Stirs Concern\" and that the \"trend is linked to drop in use of antidepressants after FDA raised worries about risks\" and made a decision in 2004 to add black box warnings on labels for SSRI antidepressants. Parents were frightened by a lack of understanding and children may have died as a result. Franklin Roosevelt suggested that \"the only thing we have to fear is fear itself.\" There are many other things to fear, and one of them is poorly reported medical "}