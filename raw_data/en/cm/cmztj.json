{"title": "PDF", "author": "PDF", "url": "https://www.massgeneral.org/assets/MGH/pdf/Research/dcr/DCR-ClinicalResearchDay-2018.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "16thAnnual Celebration of Clinical Research October 4, 2018 8 am -1 pm O'Keeffe Auditorium & Bulfinch Tents i Welcome to Clinical Research Day 201 8 Clinical Research Day is an annual celebration of clinical and translational investigators and their accomplishments over the past twelve months at the Massachusetts General Hospital (MGH) . Every year the clinical research community at MGH becomes more vibrant and robust with this year being no exception. This year, we had a remarkable 329 abstract submissions reflecting the exciting work being conducted by clinical research teams here at MGH . Clinical Research Day is a great opportunity for interaction between clinical researchers and for learning more about the different types of research being conducted at our Institution. It is also an opportunity for sharing ideas that may contribute to s trategic thinking and future planning. The keynote speaker for Clinical Research Day 2018 is Carl June , M.D. Dr. June is the Director of the Center for Cellular Immunotherapies and Director of the Parker Institute for Cancer Immunotherapy in the Perelman School of Medicine at the University of Pennsylvania . Dr. June will present on \"The Role of Immunotherapies in Medicine \". Following the Keynote Address by Dr. June , there will be two poster s ession s and a panel discussion: 9 to 11 am Poster Session I Bulfinch Tents 11 to noon Panel Discussion O'Keeffe Advances in T -Cell Engineering Auditorium Moderator: Maurizio Fava, M.D., Director, Division of Clinical Research, MGH Research Institute Panel Members: Panel Members: Carl June, M.D., Director of the Center for Cellular Immunothera pies at the Perelman School of Medicine, Director of the Parker Instit ute for Cancer Immu notherapy at the University of Pennsylvania Tomas Neilan, M.D., MPH , Dire ctor, Cardio -Oncology Program, Co-Director, Cardiac MR PET CT Program, MGH Glenn Dranoff, M.D., Head of Immuno -Onco at Institutes fo r Biomedical Maus, M.D., Ph.D., Director, Cellular Immunotherapy Program, MGH Alfred Sandrock, M.D., Ph.D., Chief Medical Officer, Biogen 11:30am to 1pm Poster Session II Bulfinch Tents ii Acknowledgements Clinical R esearch Day is a collaborative effort amongst the clinical research community. We would like to acknowledge investigators who submitted abstracts, mentors who encouraged them and departmental leaders who continuously nurtur e clinical investigation at MGH. They deserve our collective gratitude for doing this important work. We would also like to thank the chiefs for their sponsorship of departmental awards : Jeanine P. Wiener -Kronish, M.D., Anesthesia David E. Fisher, M.D., Ph.D., Dermatology David F .M. Brown , M.D., Emergency Medicine Katrina A. Armstrong , M.D., Medicine Merit E. Cudkowicz , M.D., Neurology Jeffrey L. Ecker, M.D., Ob/Gyn David N. Louis , M.D., Pathology Allan J. Goldstein, M.D., Pediatric Surgery Ronald E. Klei nman, M.D., Pediatrics Timothy G. Ferris, M.D., MGPO Jerrold F. Rosenbaum , M.D. , Psychiatry James M.D., Radiology Keith D. Lillemoe , M.D. , Surgery Faculty and staff of the MGH Division of Clinical Research contributed valuable ideas, time and energy to make this day possible. We thank you all for your help and support. Special thanks are due to: Andrew Nierenberg, M.D., and Karen Miller, M.D., Co -Director s, Training and Education, MGH Research Institute Kelsey Gay, Stacey Grabert, Tatiana Kore tskaia A very special thank you to Tiereny Morrison -Rohlfs for her leadership and hard wo rk to make Clinical Research Day a success. iii Keynote Speaker Carl H. June , MD, Richard W. Vague Professor in Immunotherapy, Director of the Center for Cellular Immunotherapies and Director of the Parker Institute for Cancer Immunotherapy in the Perelman School of Medicine at the University o f Pennsylvania. Hospital -Wide Awards Poster # $1,500 Team Science Awards (3) 17 Preemptive Pan -genotypic Direct Acting Antiviral Therapy in Donor HCV -positive to Recipient HCV -negative Cardiac Transplantation: A Novel to Enhance Donor Organ Supply Bethea, Emily; Gaj, ; Ibrahim, Nasrien ; Carlson, William ; Thomas, Sunu ; Newton -Cheh, Christopher ; Shah, Ravi Nahel ; Yeh, Heidi ; Bhan, Irun ; Pratt, Daniel ; Andersson, Karin ; Sise, Meghan ; Wojciechowski, -Theoduloz, Mauricio; D'Alessandro, David ; Lewis, Gregory; Chung, Raymond Medicine - Gastroenterology 130 A Clinical, Prote omics and Artificial Intelligence -Driven Model to Predict Acute Kidney Injury in Patients Undergoing Coronary Angiography -Results from the Catheter Sampled Blood Archive in Cardiovascular Diseases Study Ibrahim, Nasrien James L. Medicine - Cardiology 76 Prenatal Gene Expression and Folic Acid Exposure Interact to Influence Brain Development in Late Adolescence Dow ling, Kevin Francis ; Soare, Thomas W; Eryilamz, Hamdi; Roffman, Joshua L Psychiatry $1,500 Translational Science Award (3) 253 Circulating Tumor DNA as a Tool for Predicting Response to Targeted Therapy in - Hematology/Oncology 3 [11C] -PBR28 PET Correlates with Clinical Outcomes in Amyotrophic Lateral Sclerosis Alshikho, Mohamad 163 The efficacy of VVZ -149 in the treatment of the affective Care and Pain Medicine $500 Clinical and Outcome s Research Award (3) 22 Risk of methicillin -resistant Staph ylococcus aureus and Clostridium difficile in patients with a documented penicillin allergy: A population -based matched cohort study Blumenthal, Kimberly G; Lu, Na; Zhang, Yuqing; Li, Minimally Invasive Radical Hysterectomy f or Early -Stage Cervical Cancer Melamed, Alexander; Chen, Ling; Keating, Nancy Carmen, Wright, Jason; Rauh -Hain, J. Alejandro OB/GYN 80 Risks and benefits of dolutegravir -based ART for women with HIV of childbearing age in South Africa: A model -based analysis Dugdale, Caitlin M .; Ciaranello, Andrea L.; Bekker, A.; Walensky, P. Medicine - Infectious vi Poster number Anesthesia The efficacy of VVZ -149 in the treatment of the affective component of pain after laparoscopic colorectal surgeries Sheldon Bao $1,000 163 A comparison of length of stay and perioperative outcomes in epidurals versus TAP catheters for cystectomy Rachel Steinhorn $500 279 Impella Placement Guided by Echocardiography Can be Used as a Strategy to Unload the Left Ventricle During Peripher al Venoarterial ECMO Adam Dalia $500 58 Ventilation/Perfusion Matching in Severely Obese Patients Measured by Electrical Impedance Tomography: The Effect of Lung Rec ruitment Maneuvers and Mechan ics-Based PEEP Roberta Santiago $250 251 Dermatology Novel point of care diagnostic strategies for HIV -associated malignancies: portable confocal m icroscopy for Kaposi's sarcoma Esther $500 104 Emergency Medicine Emergency providers' attitudes towards opioid use disorder and emergency department -initiated buprenorphine treatment: a mixed -methods study Dana Im $750 133 Pulmonary Hypertension in the MGH ED Jason Bowman $500 23 Medicine Preemptive Pan -genotypic Direct Acting Antiviral Therapy in Donor HCV -positive to Recipient HCV -negative Cardiac Transplantation: A Novel Strategy to Enhance Donor Organ Supply Emily Bethea $1,000 17 Alcohol Consumption and the Risk of Coronary Heart Disease and Mortality in Patients with Rheumatoid Arthritis Isaac Smith $500 275 Neurology Brain Age Index Predicts Mortality 216 Luis De Carvalho Paixao $500 Clinical and Neuroimaging Biomarkers Acquired During the Acute Stroke Admission to Predict Upper Extremity Motor Recovery Aft er Stroke 177 David Lin $500 Ob/Gyn Barriers to Influenza Vaccination in Pregnancy During the 2017 -2018 Flu Season Daniela Del Campo $500 64 Minimally Invasive Radical Hysterectomy for Early -Stage Cervical Cancer Alexander Melamed $500 192 vii Pathology The development of a laboratory screening algorithm for Anaplasma phagocytophilum polymerase chain -Cas12a DNA Detectio n Benjamin Kleinstiver $250 154 Pediatric Surgery How Do Children Compare to Adults as Resea rch Subjects? Carla For tes-Monteiro $500 102 Pediatrics MRI Analysis and Machine Learning for Brain Lesion Detection and Neurocognitive Outcome Prediction in Survivors of Neonatal Hypoxic Ischemic Encephalopathy Sara Bates $500 14 Performance Analysis & Improvement/Practice Improvement Direct Scheduling: Does It Contribute to Lower No Shows Rates? Craig Cochran $1,000 51 Psychiatry Misconceptions of Medical Marijuana: No Improvement in Clinical Outcomes After Three Months of Treatment Wilson Ho $250 126 Assessment and Prediction of (Post)Traumatic Stress in Operational Firefighting: Pilot Studies Using Multimodality Brain and Psychophysiological Monitori ng Vladimir Ivkovic $250 134 Integrated Diabetes Management for Individuals with Serious Mental Illness - Results from Year 1 Krsitina Cieslak $250 48 Sustained Effects of Resilience Training in At -Risk College Students Wisteria Deng $250 66 Genes, exposure to adversity, and sensitive periods in risk for depression Erin Dunn $250 81 Prenatal Gene Expression and Folic Acid Exposure Interact to Influence Brain Development in Late Adolescence Kevin Dowling $250 76 Visualization of Biomarkers of Aversion Reward Conflict in EEG Alexander Rockhill $100 238 Nightmare and bad dream recall rates correlate with hyperarousal in individuals exposed to psychological trauma Ma Cherrysse Ulsa $250 208 viii Using Mendelian Randomization to Test Causal Bidirectional Influences Between Physical Activity and Depression Karmel Choi $100 44 The Psychophysiology of Cognitive Control in Emotional Regulation: The Preliminary Relationship Between Depression Severity a nd Heart Rate Change Regina Roberg $100 236 Pilot Study of a Transdiagnostic, Emotion -Focused Group Intervention for Young Adults with Substance Use Disorders: Sample Characteristics and Preliminary Results Kelsey Lowman Agents for Tissue -Specific Imaging Yuanyuan Ji $500 43 Surgery Clinical Impacts of the Immune Environment in Pancreatic Neuroendocrine Tumors (PanNET) Lei Cai $250 31 Goals of Care Concordance Among Patients and Health Surrogates in the Perioperative Setting Brooks Van Udelsman $250 296 Division of Clinical Research (DCR) Mission: Increase quantity, quality and efficiency of translating basic science advances into improved care for our patients . Consultations with DCR faculty: Bioinformatics Biostatistics Biostatistics for K Awar dees Community Access, Recruitment, and Engagement Drug Discovery Rounds Global Health Research Imaging Biomarkers K Grant Writing Mentoring Corner Nursing Research Omics Research Patient -Centered Outcomes Research Philanthropy Education Project Management Qualitative Research Quantitative Health Research Research Ethics Consultation Study Hypothesis and Design Survey Research Translational Medicine Trial Innovation To learn more about DCR and request Consultations and Services, please, go to: http://www.massgeneral.org/dcr 11 Mariana Almeida, BA, Medicine - Gastroenterology Gastrointestinal symptoms frequently predate the diagnosis among many multiple sclerosis patients: results from a 14-year cohort study M. Almeida, C. Silvernale, B. Kuo and K. Staller Gastrointestinal, Massachusetts General Hospital, Camrbridge, MA, USA Introduction: Multiple Sclerosis (MS) is an autoimmune, neurologic disease with common GI manifestations. Most MS initially manifests with a Clinically Isolated Syndrome (CIS), also known as a first demyelination event, which can lead to an MS diagnosis if accompanied by a plaque on MRI, or if there is a second CIS clinical presentation. Although up to 50% of patients with MS will experience bowel dysfunction as part of their illness post diagnosis, pre-diagnosis gastrointestinal symptoms are not currently factored into most clinical decision algorithms as an indicator of possible MS. We examined the prevalence and possible predictors of GI symptoms preceding a CIS in MS.Methods: We constructed a retrospective cohort analysis of all MS patients reporting GI symptoms from 2003-2017 at two tertiary care centers and compared the date of onset of GI symptoms to the date of first CIS prior to MS diagnosis. Additionally, we collected demographics and information on other MS-related symptom diagnoses. We calculated the overall prevalence of broadly-grouped GI symptom complexes predating the first CIS, along with the prevalence within 1, 2, 3 and >3 years. The GI symptoms assessed included constipation, diarrhea, dyspepsia, dysphagia, esophageal reflux, fecal inconti - nence, flatulence, nausea/vomiting, and irritable bowel syndrome.Results: There were 164 MS patients (83.5% female, 83.5% white, mean age 43 years at CIS presentation) reporting GI symptoms with an identifiable CIS prior to their MS diagnosis. Of these patients, 74 (45.1%) reported GI symptoms prior to their CIS (Table 1). Overall, heartburn/gastroesophageal reflux was the most common symptom (22.6%), followed by constipation (18.3%)(Figure 1). There was no significant difference in age (p=0.38) and sex (p=0.36) between those with antecedent GI symptoms and those with post-diagnosis GI symptoms. The mean time between onset of GI symptoms and CIS was 4.37 years. In a multivariable logistic model including age, sex, race, pre-CIS urinary dysfunction, pre-CIS fatigue and pre-CIS sensory disturbances, pre-CIS fatigue (OR of 6.6, 95% confidence interval 2.76-17.23; p<.0001) and pre-CIS sensory disturbances (OR of 2.5, 95% confidence interval 1.12-5.7; p<.05) were associated with increased likelihood of pre-CIS GI symptoms.Conclusion: Almost half of MS patients developed GI symptoms severe enough to seek medical care prior to their first demyelination event with gastroesophageal reflux and constipation being the most common complaints. Despite their lack of inclusion in current diagnostic guidelines, GI symptoms may herald a prodromal phase in MS patients. Further studies are needed in order to characterize the natural history of these symptoms in MS patients to facilitate earlier identification and treatment. The prevalence of GI symptoms preceding the first demyelination event overall and within 1, 2, 3, and >3 years Figure 1. The overall prevalence of each GI symptom preceding a CIS22 Marianna Almpani, MD, Surgery Do Trauma Severity Scores Successfully Predict Hypersusceptibility To Infections In Trauma Patients? Hospital, Boston, MA, USA, 2Shriners Hospitals for Children, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: Determine whether commonly employed injury severity scores can successfully predict hypersusceptibility to multiple independent infection episodes (MIIEs) in trauma patients.Methods: Secondary retrospective analysis of data from the \"Infection and the host response to injury\" (\"Glue Grant\") prospective longitudinal cohort study. Multivariate logistic regression was performed to measure the odds ratio of five commonly employed trauma severity scores [Denver, Marshall, Acute Physiology and Chronic Health Evaluation II (APACHE II), Injury Severity Score (ISS) and New Injury Severity Score (NISS)] in predicting hypersusceptibility to MIIEs. The latter was defined as 2 or more independent infection episodes during the recovery period. Setting: Four Level 1 trauma centers in the United States. Patients: 1665 trauma patients older than 16 years. Interventions: None. Main Outcome Measures: The correlation between trauma severity scores and hypersusceptibility to MIIEs. Results: 20.8% of the population was found to be hypersusceptible to MIIEs. Denver and Marshall scores were highly predictive of the MIIE status. For every unit increase of either the Denver or the Marshall score, there was a 15% increase in the odds of MIIEs occurrence. APACHE II, ISS and NISS were not independent predictors of MIIEs. Conclusion: Denver and Marshall scores can reliably predict which trauma patients are prone to multiple independent infections during their hospital stay, before any signs of infection become apparent. Early identification of such individuals would potentially allow rapid, personalized, preventative measures, thus improving patient outcomes and reducing healthcare costs. 3 Mohamad J. Alshikho, MD, Neurology [11C]-PBR28 PET Correlates with Clinical Outcomes Alshikho1, N. Atassi1 1Neurology, Neurological Clinical Research Institute (NCRI), Department of Neurology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA, Boston, MA, USA and 2Radiology, A. A. Martinos Center for Biomedical Imaging, Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Charlestown, MA, USA, Boston, MA, USA Introduction: [11C]-PBR28 is a second generation radioligand binds translocator protein 18 kDa (TSPO) to detect glial activation. We have shown that [11C]-PBR28 is able to discriminate between amyotrophic lateral sclerosis (ALS) patients and healthy controls (HC) based on increased glial activation in the motor cortices in ALS. In addition, [11C]-PBR28 PET signal colocalizes with structural cortical changes of cortical thinning and reduced Fractional anisotropy. Here, we evaluate the relationship between [11C]-PBR28 PET and the clinical outcome measures in a larger cohort of ALS and HC participants. Methods: Eighty-five ALS participants and thirty-three HC were enrolled in this study. All individuals were genotyped for the Ala147Thr TSPO polymorphism and those carrying the low TSPO affinity binding thr/thr genotype were excluded. Patients met the diagnostic criteria for ALS based on the El Escorial revised, and clinically assessed using the upper motor neuron burden (UMNB), and the revised ALS functional rating scale (ALSFRS-R). The rate of disease progression (RDP) was calculated using the formula: [(48-ALSFRS-R at scan-visit/disease duration]. Eligible individuals underwent positron emission tomography (PET) imaging with [ 11C]-PBR28. PET signal was quantified as standardized uptake value (SUV) from the 60-90 minutes post radio tracer injection and normalized by whole brain mean (SUVR). Surface -based analyses were performed in Freesurfer (v6.0) to compare SUVR between the groups and to correlate [ 11C]-PBR28 PET signal with the clinical outcome measures. All analyses were set to keep clusters that have voxel-wise/cluster forming threshold Z=2 (p cw0.01), and cluster-wise p values (pcw0.01). Age, gender and [11C]-PBR28 binding affinity were modeled as covariates of no interest.Results: Surface-based analyses confirm increased [ 11C]-PBR28 in ALS compared with HC (Figure 1A; pcw0.01), and show significant correlations with increased upper motor neuron dysfunction measured by UMNB (Figure 1B; pcw 0.01), decreased functional status measured by ALSFRS-R (Figure 1C; pcw0.01), and faster disease progression rate measured by RDP (Figure 1D; pcw 0.01). All these correlations were seen in the motor cortices.3Conclusion: [11C]-PBR28 PET is increased in people with ALS and correlates with the clinical outcome measures. [11C]- PBR28 PET is a molecular imaging biomarker of microglia activation that can be implemented in early phases of ALS clinical trials. Whole brain surface-based analyses: [11C]-PBR28 PET groups comparison and correlation analyses 4 Jeremy S. Altman, B.A., Anesthesia, Critical Care and Pain Medicine Prospective Study to Evaluate the Clinical Utility of Pharmacogenomic Testing in Chronic Pain Management J.S. Altman1,2, C. Copacino1,2, Narang1,2, and S. Nedelkovic2,1 1Anesthesiology, Massachusetts General Hospital, Chestnut Hill, MA, USA and 2Anesthesiology, Brigham & Women's Hospital, Boston, MA, USA Introduction: Patients who suffer from chronic pain frequently rely on taking an array of opioid and/or analgesic medica - tions for pain relief. Additionally, patients with chronic pain may have psychiatric co-morbidities that lead to consuming benzodiazepines, antidepressants, and antipsychotics. CYP450 is a gene family that contains multiple isoenzymes that are important in the metabolism of 90% of commonly prescribed drugs, including many analgesic and psychiatric medications. Concurrently taking these types of medications can lead to adverse events. In some cases, drug-drug interactions can lead to lack of efficacy, whereas in other cases these interactions can cause an exaggerated response to one or more of the drugs being given. Clinicians frequently modify medications and dosages on a trial-and-error basis to achieve a satisfactory result. Pharmacogenomic testing (PGx), utilizes the genotyping of specific regions of genes to predict the responses a patient could have to certain medications. PGx allows the evaluation of inherited differences in drug metabolism. PGx has the potential to serve as a key facet of personalized medicine while expanding the knowledge of relationships between an individual's genome to pharmacotherapy. This study aims to identify the frequencies of genetic risks for common medications used in a chronic pain management setting, based on mutations of their respective genes. The study also aims to determine if PGx has clinical utility, and will evaluate if its implementation leads to significantly better clinical outcomes and less medication consumption. It is hypothesized that there will be moderate-to-high presence of gene mutations related to metabolism for common medications used for chronic pain.Methods: The study aims to recruit 500 subjects who are are being treated for chronic pain in a pain management clinic. Subjects will be eligible if they are either taking 4 or more medications or if the subject is prescribed one of the medication categories in an inclusion list. Upon a patient giving consent, a buccal swab sample will be collected and sent for PGx testing. Within 7 days, a report of the PGx testing will be provided to the pain management physician, who will then be able to review a comprehensive genetic profile of the patient as it relates to drug metabolism and drug-drug interactions. The profile 4consists of factors that may affect responses to analgesics, potentials for drug-drug interactions, anticholinergic burden, lifestyle factors, known FDA warnings that may impact the subject, and relevant Beers Criteria for inappropriate medication use. The clinician will respond to a questionnaire regarding the perceived clinical utility of PGx testing and will be asked if the testing influences their management of the subject. The subject will be assessed approximately 30 days after enrollment to review if there have been any drug-drug interactions, other adverse events, analgesic/opioid consumption, hospital and emergency room visits, and any unscheduled phone calls and office visits since their consent. The subject will also complete a written assessment of function, behavior, side effects, and overall satisfaction with care. Outcomes in patients for whom PGx testing was felt to have clinical utility will be compared to outcomes in patients for whom PGx testing was not felt to be useful by their treating pain management clinician. Subjects will be compared using propensity-based matching using logistic regression models evaluating the similarities of the subjects' characteristics and treatment interventions. The scores will be matched within a window such that two groups will be evaluated taking into account the amount of variation in propensity score distributions. This method is useful in determining important baseline traits for subjects so that they can be matched and compared to identify significant differences in outcomes between the two study arms from pain management interventions. Results: It is expected that there will be a high frequency (approximately 70% of subjects) of mutations in genes for the metabolism of pain management drugs, such as opioids, sedatives, hypnotics, antipsychotics, antidepressants, and NSAIDs. It is also expected that through the use of PGx testing, clinicians will find the genomic reports to have significant clinical utility. These findings will suggest that using PGx can personalize the care of patients in a pain management center environ- ment and improve clinical outcomes and pain medication usage for chronic pain patients.Conclusion: Evaluating the clinical significance of PGx in the chronic pain setting has the potential to reduce the drug-drug interactions and side effects of opioids and other analgesics. Using the personal genetic profile of patients will provide clinicians with a stronger understanding of how drugs may affect each individual patient. It is anticipated that PGx testing will result in a decrease in the amount of unwanted side effects and enable a better quality of care for patients who have chronic pain. 5 Selen Amado, Bachelor of Arts, Psychiatry Cross-national differences in depression, suicidal ideation and Massachusetts General Hospital, Brookline, MA, USA, 2T.H. Chan School of Public Health, Harvard University, Cambridge, MA, USA and 3Department of Psychiatry, Harvard Medical School, Cambridge, MA, USA Introduction: The rates of mental health problems vary widely across countries. Previous studies have found cross-national differences in suicidal ideation, depression, mania and well-being (Bernal et. al. 2007, Weissman et. al. 1996, Mikolajczyk et. al. 2008). The purpose of the study was to investigate differences in depression, suicidal ideation, mania, disability related to mental health burden and well-being across countries. Methods: Moodnetwork is an online platform through the National Patient-Centered Clinical Research Network (PCORnet) designed for patients and families of those with mood disorders. PCORnet is a part of the Patient-Centered Outcomes Research Institute (PCORI) that allows patients to actively participate in every aspect of the research process. It serves as both a platform for research studies as well as a resource for individuals with mood disorders, allowing patients the opportu- nity to track their mood and symptoms, participate in online forums, ask questions, and identify their research priorities through surveys and other online tools. After signing informed consent to register for the network, participants complete a demographics form that includes a question on the country that they currently live in. Once registered, participants have access to a variety of questionnaires. They can complete these questionnaires any time after they have registered and there is no limit on how many times they can complete them. These questionnaires include Quick Inventory of Depressive Symptom- atology (QIDS-SR16), World Health Organization-Five Well-Being Index (WHO-5) Survey, Altman Self-Rated Mania Scale (ASRM) and Sheehan Disability Scale. For this analysis, we used the first questionnaires completed by the participants. We examined the differences in mean scores in these questionnaires across countries to measure the differences in rates of depression, suicidal ideation, mania, disability associated with mental health burden and well-being using t-tests. We used the Satterthaite-Welch adjustment for the degrees of freedom because the results to account for unequal variances. Results: The MoodNetwork has participants from 58 different countries. The four regions that we have the most participants from were selected for the analysis; USA (N= 1078), Canada (N= 34), Great Britain (N= 44) and Australia/New Zealand (N= 34). We examined differences in mood, suicidal ideation and disability in the participants who registered for MoodNet- work from these four regions. The analysis revealed that there was a significant difference in depression ratings on the Quick Inventory of Depressive Symptomatology (QIDS-SR16) between participants from Great Britain (M=18.2 SD=4.0) and from the US (M=15.7, SD= 5.2); t(20.45)= 2.6, p= 0.01. There were no significant differences in depression 5between other groups. Participants from Great Britain had a significantly lower mean score on the World Health Organiza - tion-Five Well-Being Index (WHO-5) (M=4.2, SD=3.84) than the US (M=7.3, SD=5.2); t(20.68)= -3.6, p= 0.001. There were no significant differences in mania ratings on the Altman Self- Rated Mania Scale (ASRM) between the groups (p> .05). There were no significant differences in disability ratings on the Sheehan Disability Scale between the groups (p> .05). The suicidal ideation ratings on the Quick Inventory of Depressive Symptomatology were significantly different between groups. The mean suicidal ideation score of participants from Great Britain (M=1.6, SD= 1.0) was significantly higher than that of participants from the US (M=1.1, SD=0.9); t(19.29)= 2.4, p = 0.027. The mean suicidal ideation score of participants from Great Britain was significantly higher than that of participants from Australia/ New Zealand (M= 0.8, SD= 0.7); t(30.91) = -2.8, p = 0.007. The mean score of participants from Great Britain were also significantly higher than participants from Canada (M= 16.3, SD= 3.7); t(30.99)= -2.3, p = 0.027. The suicidal ideation ratings of other regions were not significantly different from each other.Conclusion: These findings support the hypothesis that there are differences across countries in depression, well-being and suicidal ideation for MoodNetwork participants. The results indicate that Great Britain has a higher mean score on depression and suicidal ideation than the US as well as a lower mean score on well-being. Additionally, Great Britain has higher suicidal ideation rates than Canada and Australia/New Zealand. However, the sample sizes from the countries except the US are low, therefore our sample may be affected by a self-selection bias. People who are more in need of help may be the ones who are registering for the network. Further research should address these variables to enhance understanding on the reasons behind the differences in mental health across countries. 6 Juliana Arenas, Medicine - Endocrine-Diabetes Early Pregnancy Post-Load Glucose is Associated with Adverse Perinatal Outcomes J. Arenas1, M. Cayford1, J. Tangren2, S. Bernstein3 and C. Powe1 1Diabetes Unit, Massachusetts General Hospital, Boston, MA, USA, 2Nephrology Division, Massachusetts General Hospital, Boston, MA, USA and 3Maternal Fetal Medicine Division, Massachusetts General Hospital, Boston, MA, USA Introduction: Gestational diabetes mellitus (GDM) is associated with perinatal complications including fetal overgrowth, preeclampsia, and neonatal hypoglycemia. GDM is typically diagnosed in the late 2nd trimester, but previous studies have suggested worse outcomes in those diagnosed earlier in pregnancy. There is confusion about diagnostic criteria for abnormal glucose in early pregnancy and limited knowledge of the effects of early pregnancy glycemia on perinatal outcomes. Methods: The Study of Pregnancy Regulation of Insulin and Glucose (SPRING) is a longitudinal cohort study of glucose metabolism in pregnancy. Pregnant SPRING participants are women, ages 18-44, with risk factors for GDM (e.g. GDM in a previous pregnancy, family history of diabetes mellitus (DM), body mass index (BMI) 25 kg/m 2 plus one other risk factor). Women with known pre-existing DM, with a hemoglobin A1C 6.5% at the first study visit, or using medications known to affect glucose metabolism are excluded. In the 1st trimester, women undergo a fasting two-hour 75-gram oral glucose tolerance test (OGTT) with blood sampling at baseline, 30 minutes, 60 minutes, and 120 minutes for glucose and insulin levels. Adverse pregnancy outcomes are assessed by chart review. In this analysis, we included women in whom delivery records are presently available. We compared 1st trimester glucose levels and hemoglobin A1c of women who developed hyperglycemia-associated outcomes during their pregnancy (preeclampsia, shoulder dystocia, infant hypoglycemia, and the delivery of a large for gestational age infant) to those who did not. Glucose and hemoglobin A1c values are given as median [IQR]. Continuous variables were compared using Wilcoxon Rank-Sum tests and categorical variables were compared using the Fisher's exact test. Statistical analyses were conducted in STATA version 14 (StataCorp, College Station, TX).Results: Forty pregnant subjects were studied. Of the these, 11 (27.5%) developed hyperglycemia-associated adverse perinatal outcomes. Participant characteristics including age, nulliparity, family history of diabetes, education level, household income level, first trimester BMI, and gestational age at the time of study were similar between the two groups. First trimester sixty-minute post-load glucose values were higher in women who developed adverse outcomes (147 [107.5-160] mg/dl vs. 111 [85-133] mg/dl, p=0.047). The 120-minute post-load glucose levels appeared higher in those who developed adverse outcomes, but this did not reach statistical significance (112 [92-138.5] mg/dl vs 95 [81-104] mg/dl, p=0.062). In contrast, fasting glucose values and hemoglobin A1c values did not differ between those who did and did not experience adverse outcomes (77 [73.5-81] mg/dl vs 80 [75-86] mg/dl, p=0.46 5.4 [5.2-5.5] %, p=0.38, respectively).Conclusion: Women with higher post-load glucose values in the first trimester may be more likely to develop hyperglycemia-associated adverse outcomes during their pregnancies.67 Mursal Atif, Bachelors in Neuroscience, Medicine Development and Integration of an Engagement Engine Algorithm to Create a Personalized and Reactive Activity Tracking Companion M. Atif1, O. Dyrmishi1, A. and K. Jethwani1,3,4 1Partners Connected Health, Partners Healthcare, Boston, MA, USA, 2Newton-Wellesley Hospital, Newton, MA, USA, 3Massachusetts General Hospital, Boston, MA, USA and 4Harvard Medical School, Boston, MA, USA Introduction: Wearable physical activity trackers are a promising way to track physical activity (PA) if barriers to long-term adoption and engagement are resolved. Using physical activity tracker and behavioral data collected from a pilot study, we developed a machine learning algorithm that predicted a participant's level of engagement with their PA tracker. Using this MLA and data from the initial pilot study, a technical system architecture called the \"engagement engine\" (EE) was developed using several software platforms to help tackle the barriers to use and adoption of activity trackers. This study aimed to leverage the EE to conduct a remote clinical trial where participants used study provided activity trackers to monitor their PA, while receiving \"just in time\" motivational text messages. This poster outlines some of the integration and regulatory lessons learned from this study. Methods: Technical components of the system architecture included 1) participant data captured via REDCap, 2) step data collected via Fitbit, 3) MLA output leveraging data captured in REDCap and Fitbit, 4) text messaging intervention, and 5) JavaScript software to automate the processing of data and messaging participants when needed. The virtual environment was set up to integrate each of the technical components. Using an iterative agile software development process the testing phase was conducted from an end user's (study participant) perspective, where within a 2-week period (sprints) subsets of \"user stories\" were worked on. At the end of each sprint, the system components were verified with the study team. Participant data was captured in REDCap by designing the platform to be able to remotely recruit and enroll study participants. After consent and confirmation of eligibility, enrolled participants were directed to the study enrollment surveys which collected demographics, current PA levels and attitude towards PA. To personalize the text messages, study staff conducted motiva - tional interviews (MI) with each participant to elucidate their underlying reason and motivation for increasing their PA levels. Participant's motivation was matched with a category of predefined group of text messages, which enabled the EE to send appropriate and personalized text message to the participant. Using REDCap API and Fitbit authorization, participant daily step count was pulled from Fitbit into the algorithm. The algorithm calculated the weekly step goal by taking the average of the steps taken the 1 st week and increasing it by 10%. JavaScript logic scripts interacted with the system components by receiving participant characteristics from REDCap; receiving daily step count from Fitbit and sending it to the MLA to calculate a weekly step goal and sending the weekly step goal back to Fitbit; sending text messages to participants via Twillo if weekly goal was not meet; and alerting study staff if participant's Fitbit was not tracking daily step count for more than two weeks, all the while keeping backups of the data sets. Compliance to the institution's data security requirements were met by providing detailed information on storage and flow of participant data to ensure that participant privacy was safeguarded before the start of the study. Results: One of the challenges was integrating the different platforms with the MLA with the overall goal of keeping the system scalable and adaptable with real world data set. The iterative process was completed within 12 months, with unanticipated errors adding two extra months when testing for empty data sets (non-engagement) scenarios. The MLA was programmed using R, while the data automation and processing of text messages were programmed in Javascript, which presented a unique integration challenge. The solution was to separate the original R script in to 3 subscripts and run the JavaScript code as needed in between each step. Given that each institution's regulatory requirements are unique, it is best to review the study proposal with privacy as well as IT departments to know guidelines as each software components is being designed.Conclusion: This study combined multiple software platforms to develop an intervention that will help activity tracker users to overcome barriers to long-term use and adoption to their PA trackers by sending them timely and personalized text messages. Agile software development with adequate testing time and early review with the institution's regulatory depart-ments are some of the lessons learned from phase II of this study.78 Ana Aulinas, MD PhD, Medicine - Endocrine-Neuroendocrine ENDOGENOUS OXYTOCIN RESPONSE TO FOOD INTAKE A. Aulinas1, Mancuso1, M. Slattery1, M. Misra3 and E. Lawson1 1Neuroendocrine Unit. Department of Medicine. Harvard Medical School, MGH, Boston, MA, USA, 2Eating Disorders Clinical and Research Program. Harvard Medical School, MGH, Boston, MA, USA and 3Neuroendocrine Unit. Pediatric Endocrine Unit. Harvard Medical School, MGH, Boston, MA, USA Introduction: Oxytocin (OT), a hypothalamic hormone, regulates a range of physiologic processes, including eating behavior and metabolism. Exogenous OT administration can reduce caloric intake and increase energy expenditure, leading to weight loss. While studies have not identified a link between supraphysiologic OT administration and subjective appetite, there are few data examining the response of endogenous OT to food intake or the relationship between OT and appetite. Our aims were to determine (1) levels of endogenous serum oxytocin in response to food intake and (2) the relationship between oxytocin levels and subjective hunger and satiety in healthy individuals.Methods: Fifty-five healthy normal-weight females, mean age 20.5 \u00b1 0.7 years were instructed to eat a mixed meal standard- ized for micro- and macro-nutrient content. Blood sampling for oxytocin occurred pre-meal and at 30, 60, and 120 minutes post-meal. Subjective appetite was assessed using Visual Analogue Scales (VAS) pre-meal and post-meal. Multivariate regression analysis was used to control for confounders.Results: Mean fasting OT levels were 1011 \u00b1 52 pg/mL. Mean OT levels decreased significantly from fasting OT levels to 30 and 60 minutes after the standardized meal (p=0.001 and p=0.003, respectively). Mean nadir OT levels and mean % change in OT from baseline to nadir were 802 \u00b1 44 pg/mL and -19.6 \u00b1 3.0 %, respectively. The greater was the decrease in OT after meal (baseline to nadir), the subjects had higher ratings of hunger (r s=-0.291, p=0.03) and lower ratings of fullness (rs=0.345, p=0.009) assessed by post-meal VAS. These relationships remained significant after controlling for food consumed at the meal, age and menstrual cycle status (p=0.023 and p=0.0001, respectively). Conclusion: Peripheral oxytocin levels decrease after food intake and are associated with subjective appetite. These data suggest that endogenous OXT may play a role in perceived hunger and satiety. 9 Magid Awadalla, MD, Medicine - Cardiology Influenza Vaccination and Myocarditis among patients receiving Immune 2Hematology/Oncology, Massachusetts General Hospital, Boston, MA, USA, 3Cardiac MR PET CT, Massachusetts General Hospital, Boston, MA, USA, 4Internal Medicine, Massachusetts General Hospital, Boston, MA, USA, 5Division of Cardiovascular Medicine, H. Lee Moffitt Cancer Center & Research Institute and University of South Florida, Tampa, FL, USA, 6Cardio-Oncology Program, Division of Cardiology, Brigham and Women's Hospital, Boston, MA, USA and 7Cardio-Oncology Program, Division of Cardiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Vaccination for influenza is recommended for patients with cancer. This vaccination leads to the development of antibodies to the influenza antigen. Immune checkpoint inhibitors (ICIs) allow the immune system to attack antigens on cancer cells. However, the immune system of patients on ICIs may target native non-cancer antigens leading to immune- mediated adverse effects (irAEs). Myocarditis is an uncommon but serious immune complication of ICIs and may also result from infection with influenza. Therefore, it is unclear if administration of the flu vaccine is safe among patients on ICIs. Objectives: To test the association between administration of the flu vaccine and the development of, and outcomes with, myocarditis among patients receiving ICIs. Methods: Using a multi-center registry, we compared rates of flu vaccination (FV) between 71 patients who developed myocarditis on ICIs (cases) to 162 patients who did not develop myocarditis on ICIs (controls). Influenza vaccination was defined as administration of the FV within 6 months prior to ICI or at any time on ICI therapy. Major adverse cardiac events (MACE), was defined as the composite of cardiogenic shock, cardiac arrest, and hemodynamically significant complete heart block. Results: FV was administered to 10/71 (14%) of cases as compared to 49/162 (30%) of controls (p=0.004). When cases who received the FV were compared to cases who did not, there was no difference with respect to age (68\u00b110 vs 67\u00b120 years, p=0.78), sex (male, 50% (melanoma 40% vs 41%, of 8or single ICI therapy (40% (59\u00b14% vs a median follow-up of 79 days (IQR 46 to 210), there were 28% MACE. There were 4 MACE among the 10 cases who got the FV and 16 MACE among the 61 cases who did not (p=0.44). Similar findings were noted when restricting date of FV to within 3 months prior to, or on ICI therapy. Conclusion: The administration of the flu vaccine was not associated with an increased rate of myocarditis or MACE among patients on ICI who developed myocarditis. In contrast, rates of flu vaccination were lower among patients who did develop myocarditis on an ICI. 10 Andrea L. Axtell, MD, Surgery Early Structural Valve Deterioration and Reoperation Associated with the Mitroflow Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Structural valve deterioration (SVD) is a known limitation of bioprosthetic valves. Recent reports have suggested a concerning rate of early SVD in patients receiving a Mitroflow aortic bioprosthesis. We therefore compared the incidence of SVD and SVD requiring reoperation among patients receiving a Mitroflow versus a common contemporary bioprosthesis.Methods: A retrospective cohort analysis was performed on 592 patients receiving a Mitroflow aortic bioprosthesis at our institution between 2010 and 2014. Patients were matched 1:1 with patients receiving a Carpentier-Edwards Magna Ease aortic bioprosthesis during the same period. The incidence of SVD (defined as a mean transprosthetic gradient 30mmHg or moderate to severe intraprosthetic regurgitation), reoperation for SVD, and cumulative survival were compared between prosthesis types.Results: The cumulative incidence of SVD at five years for all patients receiving a Mitroflow aortic bioprosthesis was 16% (13-21%) SVD. Implantation of a Mitroflow valve was associated with an increased risk of SVD compared to the comparator valve (HR 2.60 [1.51-4.49], p<0.01). Older age had a protective effect against SVD (HR 0.95 [0.93-0.98], p<0.01). There was no significant difference in overall survival between valve types (p=0.80), although this was limited by a short duration of followup. Conclusion: The Mitroflow aortic bioprosthesis is associated with increased rates of early SVD and reoperation for valvular dysfunction. Enhanced clinical and echocardiographic follow-up is advisable after Mitroflow implantation. Figure 1: Cumulative Incidence of Structural Valve Deterioration Cumulative Incidence of SVD compared between the Magna Ease and Mitroflow matched pairs. Log-rank p=0.02.911 Xiang Bai, MD, PhD, Anesthesia, Critical Care and Pain Medicine Brachial Plexus Block with Liposomal Bupivacaine (EXPAREL) Compared to Placebo Showed Improved Analgesia and Reduced Opioid Consumption after Shoulder Surgery X. Perioperative and Pain Medicine, Brigham and Women's Hospital, Chestnut Hill, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Department of Anesthesia, Critial Care and Pain Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: In treating postoperative pain after major shoulder surgery, commonly used postoperative approaches like single-injection of analgesics are limited by their short duration of analgesia and continuous peripheral nerve blocks are associated with catheter-related complications. Therefore, there is a clinical need for single-dose local anesthetics that can provide prolonged analgesia after this type of major surgery. EXPAREL, a formulation that consists of liposomal bupiva-caine, has been shown to reduce pain and opioid consumption in the first 72 hours post-surgery when used as local infiltration analgesia. This double-blind, multicenter Phase 3 study evaluated the efficacy, safety, and pharmacokinetics of single- injection of EXPAREL in brachial plexus block for shoulder surgery. Methods: Adult patients scheduled for primary unilateral total shoulder arthroplasty or rotator cuff repair were randomized 1:1 to receive a single-injection of EXPAREL 133 mg or the same volume (20 ml) of saline placebo by ultrasound-guided brachial plexus block. The primary efficacy endpoint was area under the curve of visual analog scale pain intensity scores through 48 hours post surgery. Secondary efficacy endpoints included total postsurgical opioid consumption, percentage of opioid-free patients, and time to first opioid rescue through 48 hours. Adverse events were documented. Blood samples were collected through 120 hours after EXPAREL/placebo injection for pharmacokinetic testing.Results: One hundred and forty shoulder surgery patients received the brachial plexus block with EXPAREL or placebo (EXPAREL, 133 mg, n=69; placebo, n=71). The area under the curve of pain scores (least squares mean [SE], 136.4 [12.09] vs 254.1 [11.77]; mg; P opioid rescue (4.2 vs 0.6 h; P <0.0001), and percentage of opioid-free patients through 48 hours (14% vs 1.8%, P =0.0194) were significantly improved in the EXPAREL group in comparison with the placebo group. There was no difference in incidence of adverse events between the groups.Conclusion: Brachial plexus block with single-injection of 133 mg EXPAREL was safe and well tolerated in shoulder surgery patients and demonstrated improved analgesia with reduced opioid use. 12 James Barnett, Neurology Clinical Characteristics of Sunflower Syndrome J. Barnett, B. Fleming, P. Bruno, N. Folan, J. Freedman and E. Thiele Pediatric Nuerology, Massachusetts General Hospital, Boston, MA, USA Introduction: Sunflower syndrome is a rare and complex photosensitive epilepsy characterized by highly stereotyped seizures. During these seizures, individuals simultaneously turn toward a bright light and wave one hand in front of their eyes. Sunflower syndrome is poorly characterized and poorly understood. Due to this, individuals with Sunflower syndrome are often misdiagnosed and mistreated. We conducted a retrospective review of patients with Sunflower syndrome seen in the MGHfC pediatric epilepsy program, and also designed/distributed an online survey to the broader Sunflower syndrome patient population. Methods: We identified 12 patients seen in the MGHfC pediatric epilepsy program with symptoms characteristic of Sunflower syndrome, and reviewed their medical history. A detailed online survey was then created and administered to those patients; patients for the survey were also recruited from the Sunflower Syndrome - Self Induced Photosensitive Epilepsy Facebook group. The survey included questions about diagnosis, clinical features, family history, previous and current treatments, as well as overall quality of life. A total of 29 individuals completed the survey. Only survey results of individuals who reported hand waving as a component of their seizures were analyzed.Results: Of the individuals that participated in the survey and reported hand waving episodes as part of their seizures (n=28), 70% were female and 30% were male. The mean age of patients was 13.3 years (Max= 54.7, Min = 3.6 years, Median=11.2 years). The mean age at onset of hand waving was 6.1 years, with 72% reporting abnormal behaviors prior to the onset of hand waving. These abnormal behaviors included episodes of eye fluttering, eye blinking, head shaking, and attraction to or \"being drawn to\" bright light. The frequency of hand waving episodes were highly varied among patients, and appeared to be dependent on the degree of light exposure. A variety of antiepileptic medications and non-pharmaceutical interventions 10were reported as being used with varying efficacy. All participants reported that stimulus avoidance via baseball caps or tinted lenses helped reduce episode frequency. None of the treatments or interventions were completely effective at eliminating the hand waving episodes. All of the patients reported that hand waving episodes were in some way a detriment to their academics and/or their social wellbeing. Conclusion: Sunflower syndrome is a photosensitive, epileptic disorder that usually develops within the first decade of life. The syndrome is characterized by highly stereotyped seizures triggered by a stimulating bright light. These seizures vary in frequency and are refractory to most pharmacologic interventions. Misunderstanding of this disorder often leads to poor self-concept and self-esteem for the patients. More research is required to further elucidate the mechanisms of this photosensitive epilepsy. 13 Esteban A. Barreto, M.A., Medicine - Mongan Institute for Health Policy A culture of respect: patient and workforce perceptions at MGH. C. Michael1 and J.R. Betancourt1,4 1Mongan Institute Health Policy Center, Massachusetts General Hospital, Boston, MA, USA, 2University of Puerto Rico, San Juan, Puerto Rico, 3Harvard Medical School, Boston, MA, USA and 4The Disparities Solutions Center, Boston, MA, USA Introduction: The Massachusetts General Hospital (MGH) portrays an ongoing commitment to diversity and inclusion through several policy statements, such as: the MGH nondiscrimination policy statement, the MGH mission statement, the MGH Credo, and the most recent 2016 MGH Diversity and Inclusion statement. The MGH Diversity and Inclusion statement includes 3 principles: a) because of diversity we will excel; b) through inclusion we will respect; c) focused on equity we will serve, heal, educate and innovate. The MGH has implemented several initiatives to create a welcoming -and respectful- environment for patients and workforce. Over the past decade, we have assessed patients' perceptions of care and respectful treatment at MGH, with special sampling for underrepresented minorities who might not otherwise complete routine patient satisfaction surveys. Most recently, we have also assessed workforce perceptions of the MGH work culture in regards to diversity and inclusion. In the period 2016-2018 we surveyed 6,793 employees and 799 patients about the culture of diversity and respectful treatment at MGH.Methods: Study design: Cross-sectional mixed methods survey research study. Eligible population: Eligible patients included ambulatory, ED, and inpatient populations from April 2017 to July 2017. We stratified eligible patient data by race/ethnicity and language; and sampled within strata. Eligible employees included all current employees listed as of September 15 th, 2016 and actively working during study period. Measures: a) experience of respectful treatment, b) experience of unfair or disrespectful treatment, c) willingness to speak up if treated unfairly or with disrespect, d) willingness to share information about personal identity, culture religion or background, e) willingness to recommend MGH as a place to get medical care. Data collection: Patient and workforce data collection was completed during October-November, 2016; and during August, 2017-January, 2018, respectively. Patients had the option to complete the survey on paper or by telephone. Patient surveys were completed in English, as well as in Spanish, Portuguese, Arabic, Chinese, Khmer, Haitian Creole and Russian. Employees had the option to complete the survey online, on paper or by telephone. Results: N = 799 patients completed the survey. N = 6,793 employees across multiple sites, departments, and roles at MGH completed the survey. More than 1,000 employees skipped at least one demographic question and many skipped all. Percentages include missing responses. Respondent characteristics are shown in Table 1. Experience of respectful treatment Overall, most patients and employees reported experiences of respectful treatment at MGH. 97.6% of patients reported \"yes definitely/somewhat\" to being treated with courtesy respect by hospital staff. 87.5% of employees strongly/somewhat agree to being treated with respect by co-workers and colleagues. Experience of unfair or disrespectful treatment Employees were more likely to report experiences of unfair and disrespectful treatment. We found statistical differences by gender, sexual orientation, race, ethnicity and disability (p< .05). Women (15.2%) were more likely to report unfair and disrespectful treatment than men (7.8%). Approximately 1 in 5 employees who identified as LBGTQ, or as having a disability, reported unfair or disrespectful treatment frequently or sometimes. 38.3% of Black or African American, 26.9% of Asian or Asian American, and 20.3% of Hispanic or Latino employees reported unfair or disrespectful treatment frequently or sometimes. In contrast, 5% or fewer patients reported unfair or disrespectful treatment frequently or sometimes. Willingness to speak up if treated unfairly or with disrespect Overall, patients were more likely to speak up if they experience unfair or disrespectful treatment (74.4% of patients and 66% of employees strongly/somewhat agree). We found significant differences by employees' race (p < .05). Asian or Asian American (62.1%) employees were less likely to speak up than Black or African American (76.8%), White non-Hispanic (74.6%) and Hispanic/Latino (74.7%) employees. We also found significant differ-ences by patients' race (p < .05). Asian or Asian American (58%) patients were less likely to speak up than Black or African American (84.3%), White non-Hispanic (76.1%) and Hispanic/Latino (82.4%) patients. Willingness to share information 11about personal identity, culture religion or background Overall, patients (80.9%) were more likely to share information about personal identity, culture, religion or background than employees (67.8%). We found significant differences by employees' race (p < .05). Employees who identified as Black or African American were less likely to share personal information with co-workers. We also found significant differences by patients' race (p < .05). Patients who identified as Asian or Asian American were less likely to share personal information with doctors and nurses. Willingness to recommend MGH as a place to get medical care Overall, most patients and employees would definitely or probably recommend MGH as a place to get medical care (94% and 84.7%, respectively). We found significant differences by employees' race/ethnicity (p < .05). Employees who identified as Black or African American (91.9%) were less likely to recommend MGH as a place to get medical care than White non-Hispanic (97.4%), Asian or Asian American (94.1%), and Hispanic/Latino (95.5%) employees. We did not find significant differences by patients' race/ethnicity. Conclusion: Understanding patient and workforce perception in regards to respectful treatment is essential to assess the MGH principles of diversity and inclusion. Although most survey respondents reported being treated with respect at MGH, employees were more likely to experience unfair and disrespectful treatment because of individual characteristics, such as: gender, sexual orientation, race, ethnicity and disability. 14 Sara V. Bates, MD, Pediatrics MRI Analysis and Machine Learning for Brain Lesion Detection and Neurocognitive Outcome Prediction Boston, MA, USA, 2Pediatrics, MGH, Boston, MA, USA, 3Neurology - Pediatric Neurology, MGH, MA, USA, 4Radiology, BCH, Boston, MA, USA, 5Pediatrics, BCH, Boston, MA, USA, 6Isomics, Boston, MA, USA, 7Radiology, MGH, Boston, MA, USA, 8MIT, Boston, MA, USA, 9Dartmouth College, Hanover, NH, USA and 10Psychiatry, MGH, Boston, MA, USA Introduction: Hypoxic-ischemic encephalopathy (HIE) is a leading cause of neurologic morbidity and mortality in childhood. Brain imaging using MR is standard of care for assessing the presence and severity of brain injury in neonates with HIE. Currently, clinical experts reading MRI scans often disagree upon the subtle findings that indicate which areas of the brain appear abnormal and/or lesioned. The first goal of our work is to develop computer assisted image analysis tools to identify areas of abnormal signal from Diffusion Weighted MRI scans in neonates with HIE. Further, neonatal brain injuries caused by HIE often result in long-term neurocognitive impairments in survivors. Specific impairments vary widely across patients. Thus, valid biomarkers that are predictive of longer-term neurocognitive outcomes have the potential to provide critical information that could be used to guide difficult decisions on optimal management during care in the first days of life in the Neonatal Intensive Care Unit (NICU). Though magnetic resonance imaging (MRI) biomarkers have been extensively studied and are beginning to be validated for outcome prediction in adult brains, current MRI biomarkers for neonatal HIE are mostly based on expert interpretation, suffering from inconsistency and having limited abilities to explain heterogeneous outcomes across patients. Our second goal is to create a more objective way using machine learning of brain and lesion patterns from MRI for more accurate and consistent outcome predictions at 2 years of age. Methods: A retrospective, IRB approved review identified clinical cases of HIE treated in the MGH NICU who were born between 2009 and 2017. Data were collected and entered in the Partner's Research Electronic Data Capture (REDCap) system by expert physician chart review. Cases were included in the outcome prediction part of this study if neurodevelopmental outcome information- e.g. presence or documented absence of cerebral palsy, hearing impairment and visual impairment- as assessed in survivors beginning at 2 years of age in the MGHfC Newborn Developmental Follow-up Clinic and other clinical services was available. MRI scans were accessed from the hospital PACS (Radiology archives), and the quantitative Apparent Diffusion Coefficient (ADC) maps were identified for each case. Automated image processing tools, optimized for neonatal images, were used to extract and register the brain into the same standardized space as our normative neonatal ADC atlas [Ou et. al, 2017]. Features characterizing each voxel in the ADC image (326 features for each voxel, from absolute and/or relative to atlas ADC maps) were feed into a multi-class classifier, which labels each voxel as normative (white matter, gray matter or CSF) or affected by HIE lesions. Voxel-wise lesion detection was tested using eight fully annotated 3D MRI scans, and on average only 2.2% of voxels were lesioned. To examine the performance of our models, we used cross validation, leaving out one patient as a validation set for a total of 8 runs. We used Naive Bayes and Random Forest for our classical models, and a U-Net convolutional neural network for deep learning. To test the accuracy of computer-aided feature detection for 2-year outcome prediction, 71 patients treated for HIE at MGH with clinically acquired neonatal MRI scans and 2-year neurocognitive outcomes (cerebral palsy, hearing impairment and visual impairment) were identified. After automated brain extraction and regional segmentation of their MRI, volume and histogram analysis of ADC values at each of the 62 12auto-segmented brain regions were used as input features, which went through our iterative forward inclusion and backward elimination feature selection algorithm in combination with a linear regression model, to predict outcomes.Results: In our analysis of voxel wise lesion detection, results from the 8 runs were averaged, and we used the F1-score as our benchmark for performance. The best performing machine-learning model was a Random Forest classifier where the negative samples were clustered into 3 classes, giving an F1-score of 28.3. The best performing deep learning model was down sampling the negative class, with an F1-score of 31.4. The application of machine learning techniques in predicting development delay and cerebral palsy achieved 59% and 95% accuracy (sensitivity and specificity being 0.8 and 0.42 for the former and 0.5 and 1 for the latter). Conclusion: Though preliminary, our results demonstrate that 1) voxel-wise lesion detection is possible and promising using both classic and deep learning methods, and 2) MRI analysis and machine learning without the need for the error-prone explicit lesion detection may have promise to predict neurocognitive outcomes at 2 years of age. Our ongoing work is using more advanced classifier (e.g., support vector machine) and especially automated feature selection to find a subset of features that together maximize the potential contribution of MRI to outcome prediction. We will continue to collect prospective data to evaluate associations between clinical factors, imaging biomarkers and long-term neurodevelopmental outcomes. We hope to use clinical and quantitative neuroimaging data to aid physicians in clinical management and long-term prognostication in patients with HIE. Using relative ADC values (Zmaps) to detect lesioned regions. 15 Benjamin I. Bearnot, MD, Medicine - General Internal Medicine Access to treatment for alcohol use disorder at U.S. health centers: a national study B.I. Bearnot, N. Rigotti and T. Baggett Internal Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Alcohol use disorder (AUD) is a substantial and growing public health problem in the U.S., particularly among medically-underserved and low-income populations. Treatment rates for AUD remain low despite the existence of evidence- based behavioral and pharmacologic treatment approaches. Little is known about the barriers to accessing AUD treatment at federally-funded U.S. health centers, which provide care to 26 million vulnerable individuals annually. The aim of this study was to examine the continuum of alcohol use disorder care, location of treatment delivery, and barriers to receiving AUD treatment among adult patients with symptoms of AUD attending HRSA-funded health centers. Methods: We analyzed data from adult ( 18 years old) respondents to the 2014 Health Center Patient Survey, a cross- sectional, nationally representative, in-person survey of Health Resources and Services Administration (HRSA) health center patients conducted between 09/2014 and 04/2015 with a 3-stage sampling design described elsewhere. (4) The main measure was self-reported symptoms of AUD, assessed using the World Health Organization (WHO) Alcohol, Smoking and Substance Involvement Screening Test (ASSIST) questionnaire, which includes items that correspond to 5 of 11 Diagnostic and Statistical Manual of Mental Disorders (DSM-5) alcohol use disorder criteria. Individuals with 2 current symptoms were defined as having a diagnosis of AUD. Additional alcohol use behaviors included days of heavy alcohol use and average number of drinks on days of alcohol use in the past year. We examined responses to a series of questions regarding receipt of AUD-related counseling and care, the location of this care, and the primary reason individuals were unable to access treatment. We examined whether these measures differed between 2 HRSA health center subtypes where AUD was most common: Community Health Centers (CHC) and Health Care for the Homeless (HCH) programs. We conducted analyses in SAS (SAS Institute), version 9.4, using strata, cluster, and weight variables to account for the complex sampling design. Percentages are weighted to reflect health center patients nationally. Rao-Scott 2 tests were used to compare proportions, and t-tests were used to compare means, with a 2-sided P value <0.05 for significance. The Partners Human Research Committee exempted this study.Results: Of 5,547 eligible respondents, 555 (7.5%) had AUD. The Table summarizes alcohol use behavior, demographic, health status, and past year health care access differences between those with and without AUD. Compared to those without AUD, those with AUD had significantly more days of heavy alcohol use and mean drinks per day in the past year. Among respondents with AUD, 37% reported discussing alcohol use with a health professional, 21% reported wanting or needing treatment, and less than 15% received treatment for AUD (Figure). Individuals receiving care at HCH programs were signifi-cantly more likely to have discussed alcohol use (55%), want or need AUD treatment (36%), or receive AUD treatment (31%) relative to those attending a CHC (35%, 20%, and 13% respectively). Among 98 individuals who received AUD treatment, 1326% received treatment that was delivered at or paid for by a HRSA health center, while 74% sought treatment elsewhere. Among 62 respondents who wanted but did not receive AUD treatment, the most commonly reported reasons for not obtaining treatment were skepticism about treatment (46%), stigma (38%), logistical barriers (11%) and financial reasons (2%). Conclusion: Symptoms of AUD were common in this nationally-representative study of health center patients. A minority of individuals with AUD discussed their alcohol use with a health professional, even fewer reported wanting or receiving treatment, and three-quarters of those who received treatment did so outside of the health center. Losses at each stage of the AUD care continuum were less pronounced at HCH relative to CHC settings. Barriers to receiving AUD-focused care appear to include skepticism about treatment, stigma, and logistical and financial barriers. Marked decreases across the continuum of AUD care highlight the considerable barriers to routinely identifying, referring, and linking individuals to effective addiction care in U.S. health centers. 16 Julie Berrett-Abebe, PhD, Medicine - Mongan Institute for Health Policy Social Work in Health and Aging: Past, Present, & Future J. Berrett-Abebe1, P. Maramaldi2, B. Berkman1 and K. Donelan1 1Massachusetts General Hospital, Boston, MA, USA and 2Simmons College, Boston, MA, USA Introduction: The U.S. population is rapidly aging; within 40 years the number of older adults is projected to be double its current number. Meeting the complex medical and social needs of older adults will require development of new partnerships hospital and community-based organizations. As a community-based profession concerned with addressing social determi - nants of health, social workers have bridged hospital, community health, mental health and other social service delivery systems for over 100 years. Social workers are trained in ecological systems theoretical perspective, with competencies in counseling, communication skills, supervision, and advocacy. Methods: As part of a larger, interprofessional project, Aging Patients and Health Professionals: New Roles in a Changing Health System, out of the Mongan Institute Health Policy Center at Massachusetts General, this poster synthesizes secondary data on the history and current state of social work education, competencies, licensure, and practice. Results: Graphics display the growing numbers of social work degree graduates (although the field remains majority female) as well as increasing rigor in social work competencies and requirements for licensure. Current challenges and opportunities for the profession as it seeks to partner with other professions to care for older adults and their families are also identified.14Conclusion: As the health system seeks new ways of integrating health care across the continuum from acute care to community care, social workers are positioned as vital professional team members in addressing the complex care needs of older adult populations. 17 Emily D. Bethea, MD, Medicine - Gastroenterology Preemptive Pan-genotypic Direct Acting Antiviral Therapy in Donor HCV-positive to Recipient HCV-negative Cardiac Transplantation: A Novel Strategy to Enhance Donor Organ Supply E.D. Bethea1,2, K. Gaj2,3, J. Gustafson1, A. and R.T. Chung1,2,10 1Gastroenterology Division, Massachusetts General Hospital, Boston, MA, USA, 2Transplant Center, Massachusetts General Hospital, Boston, MA, USA, 3Cardiology Division, Massachusetts General Hospital, Boston, MA, USA, 4Partners Healthcare Specialty Pharmacy, Boston, MA, USA, 5Transplant Surgery Division, Massachusetts General Hospital, Boston, MA, USA, 6Cardiothoracic Surgery Division, Massachusetts General Hospital, Boston, MA, USA, 7Division of Pharmacy, Massachusetts General Hospital, Boston, MA, USA, 8Nephrology Division, Massachusetts General Hospital, Boston, MA, USA, 9Infectious Diseases Division, Massachusetts General Hospital, Boston, MA, USA and 10*Co-senior author, Boston, MA, USA Introduction: Donor heart availability continues to be a limiting factor in the number of cardiac transplants performed in the United States. While heart failure prevalence continues to rapidly rise, the number of annual transplants has remained unchanged over the last decade. As a result, cardiac transplantation is currently available to <1:1,500 patients with heart failure. In Region 1, which encompasses the New England states, there were 284 patients awaiting a heart transplant and a total of 143 cardiac transplants performed in 2016. During the same one-year-period, 23% of hearts meeting standard criteria for cardiac donation were discarded based solely on HCV positivity. As the shortage of transplant viable organs in the United States persists, it is of paramount importance that all potentially transplantable organs are identified. With this in mind, members of the MGH Transplant Center, including the Cardiology, Hepatology, Renal, Infectious Diseases, and Transplant Surgery Divisions, together with the Inpatient and Partners Specialty Pharmacy Departments and Core Laboratory Support Services, combined to develop a novel non-industry funded protocol supporting HCV donor-positive to recipient-nega - tive transplantation. We sought to determine if preemptive administration of pan-genotypic direct-acting antiviral (DAA) therapy could successfully prevent the development of chronic HCV infection in HCV-negative cardiac transplant recipients receiving HCV-positive donor hearts.Methods: In this single center proof-of-concept trial, 25 patients were enrolled and their status on the cardiac transplantation waitlist updated to reflect a willingness to accept a HCV-positive heart donor. If an enrolled patient received an organ offer for an HCV-negative donor, they proceeded with transplant per usual cardiac transplant center protocol. If an enrolled patient received an offer for an HCV-antibody positive donor without detectable circulating virus by nucleic acid testing (NAT) they were followed with a reactive approach and started on oral glecaprevir-pibrentasvir (GP) therapy if they developed viremia. Patients receiving a NAT-positive heart were started on preemptive GP therapy and administered their first treatment dose prior to transport to the operating room. Each patient then completed an 8-week course of GP therapy post-transplant. HCV RNA monitoring was performed to ensure adequate viral suppression and achievement of sustained virologic response. Results: Twelve patients have undergone transplantation with an HCV-positive donor heart between November 2017 and June 2018. One underwent a simultaneous heart-kidney transplant. Seven of these heart transplants were performed on long-term inpatients with immediate life-threatening conditions including intractable ventricular tachycardia, left ventric - ular assist device thrombosis, and dependence on biventricular mechanical circulatory support in the context of absent intrinsic cardiac function. All patients achieved viral suppression with undetectable or nonquantifiable HCV RNA by day 7 following transplant. Median pre-transplant waiting time for patients following enrollment in the HCV protocol (11.5 days, IQR 5-35 days) has been significantly lower than that for the 97 patients who have undergone heart transplantation outside of the HCV protocol since 1/2015 (113, IQR 40-366 days, p=0.0001) (Figure 1). The median time patients spent in the highest risk category pre-transplant (status 1A) was 8 (IQR 3-14) days in the HCV protocol and 30 (IQR 9-43) days outside of the HCV protocol (p=0.006) (Figure 2). There have been no treatment failures to date, and all HCV RNA assessments following discharge have remained negative. No adverse drug reactions or interactions have necessitated a lapse or cessation of therapy. Additionally, a review of patients hospitalized pre-transplant demonstrated that by conservative measures, based on a projected additional 3 weeks in the ICU and/or inpatient floor, the cost savings have been $2.6 million ($1.4 million in direct cost). To date, cardiac allograft and patient survival is 100% over 1,340 cumulative days of follow up.15Conclusion: Preemptive administration of a well-tolerated oral pan-genotypic direct-acting antiviral therapy results in rapid viral suppression and the prevention of chronic HCV infection development in HCV-negative cardiac transplant recipients receiving HCV-infected donor hearts. With the rising number of HCV-positive donors, there is a time-sensitive and critical need to document both efficacy and detailed implementation strategy surrounding successful use of HCV-positive organs. This novel strategy has great potential to increase the static donor pool, decrease heart transplant wait times, and improve post-transplant outcomes. 18 Nitasha Gajanthodi Mudalaje Bhat, M.D., Dermatology Use a swallowable image the duodeum:The MGH experience and Tearney1,2 1Dermatology, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Pediatrics, Massachusetts General Hospital, Boston, MA, USA Introduction: The prevalence of Celiac Disease(CD) has been rapidly increasing over the last several decades & is presently believed to affect 1-2% of the US population. Health care costs for untreated celiac disease exceed 30B USD each year. The adverse health & financial impact of untreated CD has already prompted nationwide screening for CD in certain European countries like Italy. Given that patients can suffer with symptoms for years without being diagnosed & potential long-term complications of longstanding celiac disease, accurate diagnosis is imperative to reduce patient morbidity & mortality. Even though a variety of serologic tests exist for screening & diagnosis of CD there is still a need to confirm the diagnosis with an endoscopy & random biopsy. However, endoscopic features are not sensitive or specific for CD since CD is often patchy & involves different portions of the small intestine. Furthermore, lack of standards regarding where to obtain biopsies, low adherence to guidelines regarding biopsy acquisition & variability in histopathologic interpretation have resulted in the frequent occurrence of biopsy sampling errors. These issues contribute to long delays between first onset of symptoms & diagnosis. The Tearney Lab has developed a new paradigm for diagnosing gastrointestinal(GI) diseases on a microscopic scale that eliminates the sampling errors associated with endoscopic biopsy - all in a simple, rapid & inexpensive procedure. The technique, termed tethered capsule endomicroscopy (TCE), involves swallowing an optomechanically-engineered pill that obtains microscopic images at a resolution of 10(axial)\u00d730(lateral)-\u00b5m of the entire GI tract wall as it traverses the organ via peristalsis. Here we present our experience with using TCE to image the small intestines of healthy controls as well as subjects with a diagnosis of celiac disease.Methods: The inclusion criteria for this study was an age of 18 years & older for healthy volunteers &16 years and older for subjects with suspected/diagnosed celiac disease. We excluded subjects with an inability to swallow capsules, esophageal/ intestinal fistulas, strictures, or a history of Crohn's disease. Subjects were asked to fast for 4 hours before the procedure. Subjects were either imaged with an 11 or 8 mm-diameter tethered capsule. Administration of the TCE device was similar to that of video capsule endoscopy devices; the non-sedated subject swallowed the capsule voluntarily while sipping water. Subjects were given a maximum of five swallowing attempts and offered the option of using Pill Glide, a flavored lubricant spray. Once swallowed, the capsule was allowed to descend on its own via peristalsis & gravity to reach the first part of duodenum. Once in the duodenum, the capsule operator used the tether to navigate the capsule up & down the duodenum while imaging. Following imaging, the capsule was pulled back up using the tether & removed through the mouth.Results: To date, we have enrolled 23 & imaged 20 subjects. 3 subjects were unable to swallow the capsule. Of the 23 enrolled subjects, 17 were healthy volunteers, 5 were subjects with diagnosed celiac disease and 1 was a subject with suspected celiac disease. Typical study set up for a clinical study is shown in Figure1. We were able to obtain images of the duodenum in 18 of the 20 subjects who swallowed the capsule. In 2 subjects the capsule did not progress into the duodenum within the 3-hour timeframe approved by the IRB. Of the 20 subjects who swallowed, 17(85%) were able to swallow the capsule on the 1st attempt, 3 subjects needed 2 attempts to swallow. Average duration of the procedure, from swallowing to imaging the duodenum & then removing the capsule, was 94 minutes, shortest being 45 minutes & longest being 3 hours. Average transit time into the duodenum was 43 minutes, with shortest being 10 & longest being 103 minutes. 70% of our subjects said that 16they would recommend this procedure to others & would prefer it over an endoscopy. A typical image of intestinal villi, as seen by the OCT TCE device, is shown in Figure 2.Conclusion: The TCE technology has been well received by study subjects here at MGH. A relatively short duration of examination, no sedation, no blood draws & minimal preparation are some of the advantages offered by TCE. In order to further expand & exploit the capabilities of TCE to image the small intestine, we are also conducting imaging in pediatric volunteers in Pakistan through a separately funded study. With an eye towards improving subject experience & obtaining usable images for subjects who are unable/unwilling to swallow capsules, we are also developing and clinically testing trans-nasally administered imaging devices. 19 Allyson M. Blackburn, B.A., Psychiatry Dropout from Psychotherapy for PTSD in Veterans and Active-Duty Service Members Exposed to Sexual Trauma: A Systematic Review A.M. Blackburn, E.R. Toner, E.M. Goetter and E. Bui Psychiatry, MGH, Somerville, MA, USA Introduction: Rates of sexual trauma amongst U.S. Veterans and Service Members range from 20-43% (Suris & Lind, 2008). Exposure to sexual trauma predicts a number of mental health problems, most notably Posttraumatic Stress Disorder (PTSD). Mental health concerns for military or veteran survivors of sexual trauma is undertreated given the unique barriers faced by this population. While systematic reviews have investigated drop out related to combat trauma (Goetter et al., 2017), none have focused on sexual trauma for veterans and active duty service members. Methods: The present meta-analytic review aims to examine dropout rates from outpatient, psychosocial PTSD interventions provided to veterans with PTSD following sexual trauma. Eligible articles were treatment studies, with drop out data, for a population of veterans or active duty service members who had survived military sexual trauma. Out of the 386 articles identified by our search and reviewed, only 3 studies (pooled n=246) were eligible for the analysis. Results: Dropout rates ranged from 21% to 38% with the overall pooled dropout rate being 33.2%, 95%CI [22.7% - 43.7%]. Conclusion: Dropout is common among treatment-seeking veterans with sexual trauma. Clinical and research implications will be discussed. Given the paucity of literature on treatment outcomes for military and veteran survivors of sexual trauma, further research is warranted. 20 Howard Blanchard, MEd, MS, Medicine - Cardiology Giving consent for percutaneous coronary intervention: the patient perspective of a complex process H. Blanchard1, D. Carroll2, J. Albert2 and F. Astin3 1Cardiology, MGH, Boston, MA, 2MGH, Boston, MA, 3University of Huddersfield, Huddersfield, United Kingdom Introduction: Percutaneous coronary intervention (PCI) is the most common revascularization procedure performed for coronary artery disease. All medical procedures carry a risk and therefore patients are required to give informed consent. 17This process is a part of good clinical practice, is a universal right, and reflects ethical principles. There are several stages to the informed consent process making it complex, yet little is known of the patient experience of informed consent for PCI. Purpose: To evaluate patient understanding of the informed consent process and PCI outcomes. Methods: A prospective cross-sectional survey was administered to a convenience sample treated with elective or urgent PCI at a single center in USA. Subjects completed the 36-item questionnaire to assess their understanding of the informed consent process, the risks benefits and potential outcomes of PCI. Items were rated on a scale of 1 completely agree to 5 completely disagree. Results: Eighty-two subjects, mean age of 65 years, 64 (78%) males, 59 (72%) college-educated, completed the questionnaire after PCI. Forty-nine subjects (60%) were urgent cases. Forty seven percent received written information pre-PCI. Most (82%) recognized the key components of informed consent (choice, risk/benefit, alternatives), but 55% did not remember and 42% did not understand all the information provided to them as a part of informed consent. Eighty-three percent wanted information about all possible risks associated with PCI. Over 80% of elective PCI participants had misconceptions about the outcomes of PCI, which did not match clinical trial evidence. Conclusion: Subjects understood the general components of informed consent but did not precisely understand, or remember details, about information and the outcomes of PCI. Participants wanted to know both common and uncommon risks adding to the complexity of informed consent. Participants requested more family involvement in informed consent and information given in advance to help them to understand and remember information given during informed consent. 21 Jasmine B. Blodgett, MS, Medicine - Cardiology Patterns of Oxygen Recovery Kinetics Following Maximal Exercise in the Framingham R. Vasan2 and G. Lewis3 1Cardiology Clinical Research, Massachusetts General Hospital, Framingham, MA, USA, 2Boston University, Framingham, MA, USA, 3Massachusetts General Hospital, Boston, MA, USA and 4Veterans Affairs Boston Healthcare System, Boston, MA, USA Introduction: Patterns of oxygen uptake (VO2) recovery post-exercise reflect disease severity and predict prognosis in patients with heart failure but have not been investigated in non-referral populations. We assessed the correlates of VO2 recovery patterns in middle-aged community-dwelling adults. Methods: We performed maximum incremental cycle ergometry cardiopulmonary exercise testing (CPET, age 28\u00b15, peak ml/kg/min) on 1973 Framingham Heart Study participants. Breath-by-breath gas exchange measures were obtained throughout exercise and during 4-min of recovery. Mean response time (MRT) and VO2/ work slope (aerobic efficiency) were derived to characterize VO2 kinetics during exercise. VO2 recovery delay (VO2RD, time until post-exercise VO2 falls permanently below peak VO2) and T1/2 (time for VO2 to decline 50%) were measured. Results: Mean (\u00b1 SD) values were 80\u00b1 10.2\u00b1 for VO2RD. detectable VO2RD >5s and 69% with T1/2 >62s, based on comparison to mean values in previously investi - gated normal controls), and was associated with higher BMI, prevalent diabetes, smoking, hypertension and lack of habitual cycling (Figure ). Longer MRT, more shallow VO2/W slope and 3min-exercise systolic blood pressure also correlated with VO2 recovery. Each variable, except MRT, was associated with VO2RD and T1/2 in multivariable models adjusted for age, sex, BMI and resting and peak VO2 (all p<0.002). Conclusion: In middle-aged adults in the community, delay in VO2 recovery post-exercise is common and is associated with modifiable CV risk factors and lower aerobic efficiency during exercise. VO2 kinetics are easily measured during CPET and may detect maladaptive VO2 responses to exercise prior to onset of overt CV diseases. 1822 Kimberly G. Blumenthal, MD, MSc, Medicine - Allergy/Immunology/Rheumatology Risk of methicillin-resistant Staphylococcus aureus and Clostridium difficile in patients with a documented penicillin allergy: A population-based matched cohort study Allergy, and Immunology, Department of Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Medical Practice Evaluation Center, Massachusetts General Hospital, Boston, MA, USA, 3Harvard Medical School, Boston, MA, USA and 4Division of Infectious Disease, Department of Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Documented penicillin allergies impact future antibiotic use, potentially resulting in increased antibiotic resistance and healthcare-associated infections; >90% of patients with penicillin allergy tolerate penicillins. We evaluated the relation between penicillin allergy and development of methicillin-resistant Staphylococcus aureus (MRSA) and Clostridium difficile (C.difficile) . Methods: Using a UK general practice database (1995-2015), we studied a matched cohort of adults without prior MRSA or C.difficile . Patients with incident penicillin allergy were matched with up to five penicillin users without allergy by age, sex, and entry time. We calculated relative risks (RR) for the association of penicillin allergy with incident MRSA and C.difficile , adjusting for potential confounders. We also examined beta-lactam alternative antibiotic use and whether it was a mediator for MRSA/C.difficile incidence. Results: Among 64,141 penicillin allergy patients and comparators, 1,345 developed MRSA and 1,688 developed C.difficile over 6.0 years of mean follow-up. The adjusted hazard ratios (HRs) among penicillin allergy patients were 1.69 (95%CI 1.51-1.90) for MRSA and 1.26 (95%CI . The adjusted incidence rate ratio (IRR) for antibiotic use among penicillin allergy patients were 4.15 (95%CI 4.12-4.17) beta-lactam alternative antibiotic use accounted for 55% of the increased MRSA risk and 35% of the increased C.difficile risk. Conclusion: In this population-based cohort study, documented penicillin allergy was associated with an increased risk of MRSA and C.difficile that was mediated by the increased use of beta-lactam alternative antibiotics. Systematically addressing penicillin allergies may be an important public health strategy to reduce MRSA and C.difficile incidence among patients with a penicillin allergy label. Impact of listed penicillin allergy on risk of methicillin- resistant Staphylococcus aureus and Clostridium difficile *Age-, sex-, entry time-matched and adjusted for age, sex, body mass index, socioeconomic status, smoking alcohol, Charlson comorbidity index, hemodialysis, antibiotic prescriptions, proton pump inhibitor use, corticosteroid use, other antibiotic allergies, nursing home living, general practitioner visits, and hospitalizations. Abbreviations: MRSA, methicillin-resistant Staphylococcus C.difficile, Clostridium difficile 1923 Jason Bowman, M.D., Emergency Pulmonary Hypertension in the MGH ED J. Bowman1, D. Dutta1, D. Zheng2 and D. Wilcox1,3 1Department of Emergency Medicine, Massachusetts General Hospital, Brookline, MA, USA, 2Biostatistics, Massachusetts General Hospital, Boston, MA, USA and 3Heart Center ICU, Massachusetts General Hospital, Boston, MA, USA Introduction: Pulmonary hypertension patients represent a complex subset of patients presenting to the emergency depart - ment (ED), yet little is known about their experiences and outcomes within the ED. The heterogeneity of etiologies, and diverse patient populations affected by each, have likely contributed to the difficulty in recognizing and diagnosing PH clinically. No study in the literature to-date has described the frequency, dispositions, and rates of ED return visits. Understanding the burden of disease is critical to developing future research and clinical improvements around the evaluation and management of PH within the ED.Methods: We performed a retrospective cohort analysis of patients with existing ICD 9 and 10 codes for PH presenting to our urban, tertiary, academic medical center ED from April 1 st, 2016 thru December 30th, 2017. The primary outcome was high frequency ED visits, defined as 4 or more visits in a 12-month period. Secondary outcomes included return ED visits within 72 hours and 30 days, as well as hospital readmissions within 30 days. Results: 684 unique patients with known pulmonary hypertension visited our ED a total of 1447 times during the study period, accounting for 0.8% of all ED visits. Their ED revisit rate was high (52.73% total, 18.3% within 30-days). Average admission rate from the ED (to inpatient or ED Observation) for PH patients in this study was 60.68%, with 479 unique patients being admitted a total of 878 times. In contrast, the average admission rate across all patients in our ED for this time was approximately 22.4%. Of the 878 PH patient admissions, 5.60% were to an ICU and 0.28% were to our ED Observation Unit, as compared to 1.5% to ICU and 9.6% to ED Observation for all patients, respectively. In total, 178 unique PH patients had a total of 378 re-admissions during the study period. Of these, 131 re-admissions (34.66%) occurred within 30 days of a previous discharge date. Over half (52.25%) of the patients with a readmission had two or more. Out of the 878 admissions for PH patients during the study period, 54 resulted in the patient's death, for an in-hospital mortality rate of 6.15%. Addition- ally, three other PH patients expired in the emergency department prior to admission. High-utilization was identified in 84 patients (12.2% of PH patients). The high-utilizers had a mean of 6.42 visits (95% CI 3.22 - 9.62) for the study period. The statistically significant factors associated with high-utilization visits in this population were having Group 1 PH (OR 4.67, 95% CI 1.90 - 11.47), having liver disease (OR 2.01, 95% CI 1.06 - 3.85), or having rheumatic disease (OR 2.50, 95% CI 1.27 - 4.91). Statistically significant factors associated with lower utilization including female gender (OR 0.53, 95% CI 0.33 - 0.85) and living farther from the study site hospital (OR 0.98, 95% CI 0.96 - 0.99).Conclusion: Patients with an existing diagnosis of PH comprised nearly 1% of ED visits, had high rates of return ED visits, as well as high admission and re-admission rates. They also had increased ICU admission rates and mortality rates. A subset of PH patients are high-utilizers, and high-utilization is associated with having Group 1 PH, liver disease, and rheumatic disease. 2024 Laura Brattain, PhD, Radiology Prediction of Fatty Liver Disease Using an Automatically Extracted Sonographic Obesity Biomarker MGH, Boston, MA, Lincoln Boston, MA, USA Introduction: Nonalcoholic fatty liver disease (NAFLD) is exceptionally common, with an estimated one hundred million afflicted people in the United States alone. NAFLD prevalence is underestimated and progression is under recognized by primary care physicians. It is estimated 95% of people with NAFLD remain undiagnosed and untreated. This diagnostic gap has major consequences: by 2030, NAFLD is projected to be the leading cause of end-stage liver disease (ESLD) and the dominant reason for liver transplantation in the United States. At early stages of the disease, NAFLD is reversible, therefore screening for NAFLD is cruitial in disease management. Currently, screening is performed with conventional B mode ultrasound, and this imaging technique is not sensitive and specific enough for disease detection. Accurate, fast and cost effective screening tools are needed to detect the disease at early stages and determine the necessary treatment strategy. In this study, we aimed to demonstrate the feasibility and accuracy of automatically extracted sonographic skin-to-liver-capsule distance (SCD) measurement as a biomarker for NAFLD, to determine a cut-off SCD value for NAFLD prediction.Methods: The institutional review board approved this single-institution retrospective study, which was performed in 159 patients who underwent sonography-guided liver biopsy to assess chronic liver disease (mean age 49.3\u00b113.9 years, 79 men, 80 women) between January 2014 and March 2015. B-mode sonographic scans were obtained via an intercostal approach. Biopsy samples were evaluated by a blinded pathologist using the METAVIR staging system. An image processing algorithm was developed to automatically extract the SCD measurement from manually selected B-mode sonographic images (one image per patient). Image processing steps included: 1) image smoothing/denoising, 2) skin and capsule line/border detection, and 3) skin to capsule distance calculation. Receiver operating characteristic (ROC) curve analysis was performed and an optimal cut-off value for NAFLD detection was derived. Results: The root-mean-squared error (RMSE) between manually and automatically extracted SCD measurements is 2.68 mm. The area under the ROC curve (AUROC) S0vsS1-4, 0.73(95%CI, 0.57-0.94, 2.66 cutoff), respectively. Automatically extracted SCD measurements showed a Spearman correlation value of 0.40 with steatosis stage noninvasive, sonographic biomarker of NAFLD that has the potential to yield clinically useful information at minimal incremental cost. 25 Hannah Broos, B.A., Psychiatry Emergent Cognitive Deficits During Nicotine Withdrawal Do Not Predict Short-Term Smoking Abstinence H. Broos, R. Schuster, G. Pachas, K. Lowman, M. Fava and A.E. Evins Psychiatry, Massachusetts General Hospital, Somerville, MA, USA Introduction: Cognitive deficits are a core phenotype of nicotine withdrawal and are suggested to be a barrier to achieving abstinence. Although there has been interest in evaluating the efficacy of cognitive-enhancing agents for smoking cessation, little research to date has evaluated whether the extent of cognitive deficits experienced during nicotine withdrawal translates to increased risk for poor smoking cessation outcomes.21Methods: Participants were 155 adult smokers (10 cigarettes/day), recruited as part of a larger Phase IIb parent project, designed to evaluate the effects of encenicline (EVP-6124) and NRT patch on cognitive and abstinence outcomes. Partic - ipants completed a satiated and abstinent baseline (18-48 hours of biochemically verified abstinence), during which they completed the Wisconsin Smoking Withdrawal Scale (WSWS) and computerized measures of psychomotor processing speed, attention, memory, and executive functioning (Conners Continuous Performance Task, N-Back, and Cambridge Neuropsychological Test Automated Battery). Participants were randomized after abstinent baseline in a 2x2 design to NRT or placebo patch and encenicline or placebo capsule. For the current analyses, participants were evaluated separately by NRT treatment group, collapsed across encenicline. Seven-day point-prevalence abstinence status was evaluated 1-week post-randomization and Twenty-five percent (n=39) of participants had 7-day biochemically-confirmed with rates greater in those randomized to NRT (32.4% vs 18.5%). Participants reported a 7-22% increase in symptoms of withdrawal (p-values < 0.04), except depression (p-value = 0.18), as well as a 2-9% slowing in reaction time (p-values < 0.03), 16-19% increase in reaction time variability (p-values 0.003), in inhibition errors (p-value = 0.001), and 4-7% decrease in verbal memory recall and recognition (p-values < 0.002). Among those on NRT, greater worsening in self-reported depres-sion (p-value = 0.04) and anxiety (p-value = 0.03) with overnight abstinence was associated with a lower likelihood of being abstinent at one week, controlling for baseline smoking dependence. Change in other symptoms of nicotine withdrawal were not predictive of 1-week abstinence status among those on NRT (p-values > 0.10), and there were no associations between changes in any self-reported overnight withdrawal symptoms and 1-week abstinence status for those not on NRT (p-values > 0.25). The magnitude of cognitive deficits experienced during overnight abstinence was not predictive of 1-week abstinence status in either NRT treatment group (p-values > 0.11).Conclusion: This study replicates a robust literature on the emergence of self-reported withdrawal symptoms and deficits across multiple domains of cognition with smoking abstinence. Changes in emotional aspects of withdrawal were the only predictors of 1-week abstinence status in those on NRT. Contrary to hypotheses, the magnitude of cognitive deficits experi-enced during withdrawal was not predictive of short-term abstinence status in those on or off NRT. This challenges the notion that cognitive worsening secondary to nicotine withdrawal is a viable target for smoking cessation therapy. Future studies with larger abstinent samples are needed for replication. Future studies should also evaluate the role of mood, and potentially mood-enhancing agents, in smoking cessation treatment outcomes. 26 Hannah Brown, MD, Psychiatry Randomized, controlled, double-blind trial of sodium nitroprusside for outpatients with schizophrenia H. Brown1,5, O. Freudenreich1,5, X. and 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Psychiatry, UMass Memorial Health Care, Worcester, MA, USA, 3Psychiatry, NYU School of Medicine, New York, NY, USA, 4Psychiatry, Zucker Hillside Hospital, New York, NY, USA and 5Harvard Medical School, Boston, MA, USA Introduction: Schizophrenia is a debilitating illness that causes significant dysfunction. Antipsychotic medications have proven efficacy in the treatment of positive symptoms such as delusions and hallucinations; however, these medications do not improve the functionally impairing negative and cognitive symptoms. Further, antipsychotic medications carry a substan- tial adverse effect burden, and a subset of patients do not fully respond even to multiple trials of antipsychotic medications. As such, there is a pressing need for new pharmacologic treatments. A prior pilot investigation suggested the rapid efficacy of sodium nitroprusside (SNP) administered intravenously (IV) in the treatment of patients with schizophrenia. We conducted an adequately-powered randomized, double-blind, placebo-controlled, multi-center study to characterize the efficacy and safety of a single dose of IV SNP in treating the positive, negative, and cognitive symptoms of patients with schizophrenia. Methods: This study was conducted at four academic medical centers (MGH, UMASS Medical School, NYU, and the Zucker Hillside Hospital) between 6/2015 and 3/2017. We recruited patients with schizophrenia ages 18-65. Participants had at least moderate psychotic symptoms as measured by the Positive and Negative Syndrome Scale (PANSS) and Clinical Global Impressions-Severity (CGI-S) despite ongoing antipsychotic medication treatment for at least 8 weeks and stable dosing for at least 4 weeks, with some degree of previous antipsychotic treatment resistance. Participants underwent cognitive testing as measured by the Measurement and Treatment Research to Improve Cognition (MATRICS) in Schizophrenia Consensus Cognitive Battery (MCCB). Safety and tolerability were also monitored. Participants were excluded if they had any major medical illness, treatment with medications that interfere with the metabolism or excretion of SNP, were pregnant or breast-feeding, current substance use disorder (except nicotine), or at imminent risk for suicide or harm to others. All participants underwent a physical examination, routine laboratory tests, urine toxicology, and 12-lead ECG. We used a Sequential Parallel Comparison Design (SPCD), in which individuals are initially randomized to active drug or placebo infusion, with placebo 22nonresponders randomized to placebo or active drug for the second infusion. Participants received either SNP diluted with 5% dextrose infused at 0.5 mg/kg/min over 4 hours or a placebo solution of 5% dextrose at 0.5 mg/kg/min infused over 4 hours, with continuous vital sign and ECG monitoring (See Figure). The primary outcome regarding improvement in symptoms as measured by the PANSS total, positive, and negative scales was tested using the Tamura approach to SPCD for continuous data, where effect estimates from the two infusion stages were weighted (using a weighted z-test) to compare differences between SNP and placebo groups. The primary outcome analysis was performed for all participants in the SNP and placebo groups, and then in distinct treatment subgroups (clozapine vs no clozapine treatment). Results: There were no significant differences between SNP and placebo groups for p=0.85) scores. When stratified by treatment status, there were no significant differences between SNP and placebo groups for PANSS-total, PANSS-positive, PANSS-negative, PANSS-general within either the clozapine-treated or the non-clozapine treated groups. There were no significant differences in MATRICS total scores between SNP and placebo groups (weighted b=1.11, z=-0.98, p=0.34). No significant differences in safety or tolerability between groups were identified. Conclusion: In this multicenter randomized, controlled trial of adjunctive IV SNP, we identified no evidence of efficacy for outpatients with schizophrenia who had previously failed to achieve resolution of psychotic symptoms after at least one trial of an antipsychotic medication. Neither primary nor secondary measures of efficacy demonstrated symptomatic improvement. We note several important limitations in interpreting our results. Our participants were on average older and experienced multiple episodes of psychosis. A non-refractory or early episode population may respond better to SNP. We used IV SNP only at one dose and one duration of treatment, making it difficult for us to observe any dose-dependent effect; it is possible that those individuals with a longer illness course require a higher dose and longer treatment duration for resolution of psychotic symptoms. Future studies examining the role of NO donors in treatment of psychotic symptoms may focus on a population earlier in the course of illness, with alternate dosing and duration of drug delivery. Nonetheless, our results suggest that SNP itself is unlikely to represent an efficacious treatment for antipsychotic-resistant psychosis. 2327 Lydia Brown, PhD, Psychiatry Are Positive Psychology Interventions Efficacious in Reducing Anxiety in Patients with a Medical Disease? A Meta-Analysis L. Brown1,2 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA and 2Psychology, University of Melbourne, Parkville, VIC, Australia Introduction: There is increasing interest in positive psychology interventions for use in patients with a medical condition. Positive psychology interventions have been found to be effective at reducing symptoms of depression and in improving well-being in medical settings, but less is known about their efficacy in reducing patient anxiety. This study responds to this issue, by reviewing and quantifying the effects of positive psychology interventions in reducing patient anxiety.Methods: This meta-analysis was conducted according to the PRISMA guidelines and registered on the PROSPERO database for systematic reviews (registration # CRD42018102179). Systematic searches were conducted using the electronic databases Medline PsycINFO and SciELO, as well as the clinical trial registries Cochrane (Central) and clinicaltrials. gov. The search strategy identified articles with two co-occurring components: i) A positive psychology intervention, and ii) An anxiety based outcome measure (symptom severity or diagnostic status). Results were limited to studies of patients with medical illness during the screening phase.Results: Our search yielded a total of 1,024 unique studies. After screening against inclusion criteria, 28 studies have been identified as meeting criteria for inclusion review, including 17 controlled trials and 11 single arm pre-post studies. Results from preliminary random effects meta-analysis of controlled trials indicates that positive psychology interventions are superior to control conditions in reducing symptoms of anxiety in medical patients (group difference in standardized mean change = -.45, 95% CI [-.64, -.27]). Analysis of single-arm trials revealed results (Standardized mean change = 0.44, 95%CI [0.54, 0.34]). Analysis is ongoing, and results are expected to be complete by August, 2018. Conclusion: Our findings show that positive psychology interventions may be useful in reducing patient anxiety in medical settings. Further research is required to investigate practical strategies to translate this finding to inform standard medical practice. 28 Mackenzie Brown, Psychiatry Associations Between Stress, Worry, and Pain in Generalized Anxiety Disorder R. Lubin1, K. Szuhany1, S. Hoeppner2, M. Brown2, S. Hofmann3 and N.M. Simon2 1NYU School of Medicine, New York, NY, USA, 2Psychiatry, Massachusetts General Hospital, Boston, MA, USA and 3Boston University, Boston, MA, USA Introduction: Subjective pain is an important topic given its prevalence and association with the ongoing opioid crisis. Research suggests that subjective pain is comorbid with psychiatric conditions, particularly Generalized Anxiety Disorder (GAD). GAD has a lifetime prevalence of 5.7% and is associated with functional impairment and increased pain. Little is known about the drivers of co-occurring pain in GAD. Depression is highly comorbid with GAD and is associated with negative pain-related outcomes. Emerging data suggest mindfulness-based interventions may be effective for many chronic pain conditions and may impact neural perception of pain. Less is known about the association between mindfulness and subjective pain in adults with GAD. The current study examined the independent associations of worry, perceived stress, mindfulness, and depression with self-rated pain in adults with GAD. We hypothesized that worry, perceived stress, and mindfulness are associated with greater reported pain after controlling for current depression. Methods: At baseline in a clinical trial, adults with a primary DSM-5 diagnosis of GAD (n=179; Mean age=32\u00b112 years; 71% female) completed the Five Facet Mindfulness Questionnaire (FFMQ), Perceived Stress Scale (PSS), Penn State Worry Questionnaire (PSWQ), WHO Quality of Life-BREF scale (WHOQOL-BREF), and Symptom Checklist-90 (SCL-90). The WHOQOL-BREF assesses quality of life across 4 domains: physical health, psychological health, social relationships, and environment. The SCL-90 includes 4 pain-specific items: headaches; pains in the heart/chest; pains in the lower back; and soreness of muscles. We measured pain using: 1) a summed score of the 4 pain items indicating current overall pain; and 2) a binary pain variable indicating at least moderate pain on 1 or more of the 4 items.Results: 64% of the GAD sample indicated current pain of at least moderate severity, which was associated with decreased quality of life in physical health (r=-.19, p<.05). In t-tests examining the three (t=-2.59, p<.05) and PSWQ (t=-2.34, p<.05) were associated with current at least moderate pain. A linear regression indicated 24that PSS also predicted summed scores of subjective pain (B=.18, p<.01) after controlling for depression, gender, and age; FFMQ and PSWQ were not significant in this model.Conclusion: These results support past research that subjective pain is an impactful correlate of GAD and perceived stress may be an important target in treatment for pain in GAD. Future research should examine other predictors of perceived pain in GAD and cross-diagnostically. 29 Sydney Brumfield, Athinoula A. Martinos Center for Biomedical Imaging Using PBR-28 in a dual MR-PET scanner and magnetic resonance spectroscopy to measure neuroinflammation in myalgic encephalomyelitis/chronic fatigue syndrome patients' brains. S. Brumfield, P. Lara Mejia and M. VanElzakker Psychiatry, Martinos Center for Biomedical Imaging, Allston, MA, USA Introduction: Myalgic encephalomyelitis/chronic fatigue syndrome (ME/CFS) is an often-debilitating and highly stigma - tized disease, afflicting up to 30 million people worldwide. The name \"myalgic encephalomyelitis\" suggests inflammation as a causal factor. However, direct evidence for neuroinflammation, classically defined as peripheral immune cell penetration into brain parenchyma, is lacking. Microglial activation is a necessary component of neuroinflammation, which involves translocator protein (TSPO) expression. Therefore, TSPO is an attractive target for in vivo imaging of neuroinflammatory processes. Using PK-11195, a first generation TSPO-binding PET radioligand, Nakatomi et al. (2014) found increased binding in ME/CFS patients' brains, relative to healthy controls. However, this study used volume distribution, a non- quantitative analysis method, and second-generation radioligands with reduced nonspecific binding are now available.Methods: This poster describes an ongoing study that builds upon Nakatomi et al. (2014) in several ways. We use arterial lines to quantitate brain uptake PBR-28, a second generation PET radioligand. Furthermore, given our access to a dual MR-PET scanner, we are able to collect concurrent magnetic resonance spectroscopy (MRS) data, a noninvasive complemen - tary method for showing neuroinflammation. These methodological improvements will replicate and verify the pioneering work of Nakatomi et al. (2014). Results: Given our hypothesis of an exaggerated sickness response in ME/CFS (VanElzakker, 2013), we expect increased PBR-28 signal in related neurocircuitry. Furthermore, we hypothesize an increase in inflammation-associated spectra in the cerebrospinal fluid, caudate nucleus, and the nucleus of the solitary tract.Conclusion: Provided our MRS findings coincide with our PET data, future studies can utilize spectroscopy as a less invasive and less expensive option compared to PET, to confer neuroinflammation. 30 Thomas F. Burke, MD, Emergency A Ketamine Package for Sedation Supports a Human Rights Imperative in Acutely Painful Procedures T.F. Burke1,2,3, S. Suarez1, M. Omotayo1 1Division of Global Health and Human Rights, Department of Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Harvard T.H. Chan School of Public Health, Boston, MA, USA, 4Department of Outcomes Research, Anesthesiology Institute, Cleveland Clinic, Cleveland, OH, USA, 5African Institute for Health Transformation, Sagam Community Hospital, Luanda, Kenya, 6College of Surgery for East, Central, and Southern Africa, Arusha, Tanzania, United Republic of and 7Kenya Obstetrics and Gynaecologic Society, Nairobi, Kenya Introduction: Relief of pain in painful procedures is the standard of care across well-resourced settings worldwide. For example, over the past 25 years, the practice of using \"brutane\" for reductions of fractures and dislocations has been replaced by safe methods of procedural sedation outside the operating room. Adequate pain management for painful procedures improves the quality and safety of patient care and has become an accepted and expected basic human right. In low-resource settings, pain relief in painful procedures is scarce due to \"cultural, attitudinal, educational, legal and system-related reasons\". A practice of 'hold still', where patients are forcibly held down by four or more individuals remains common in Kenya and elsewhere. In December 2013, we deployed the ESM-Ketamine package in support of emergency operations when no anesthetist is available. Over the past 4 \u00bd years, ESM-Ketamine providers independently broadened use of their skills to provide procedural sedation for patients in need of painful procedures when an anesthetist would not have been previously called. We present data from this emerging practice.Methods: A 5-day ESM-Ketamine competency based training program for non-anesthetist providers was administered to medical officers, nurses, and clinical officers. Each facility active with ESM-Ketamine was provided with ESM-Ketamine wall 25charts, checklists, and kits. Patient demographics, pre-operative diagnoses, procedure(s) performed, medications adminis - tered, and ketamine-related adverse events were recorded and analyzed. Central tendency and dispersion were estimated. Results: Between December 2013 and July 2018, 62 ESM-Ketamine providers across 11 facilities supported 512 painful procedures in non-training settings, where an anesthetist would previously not have been called. 273 (53.3%) were male and median age was 23 years (IQR 11-36 years). The five most common indications were incision and drainage and/or debridement (n=159, 31.1%), fracture body removal The median ketamine dose was 2.0 mg/kg (IQR: 2.0-3.0) per procedure. Hallucinations or agitation treated with diazepam and brief oxygen desaturation occurred in 45 (8.8%) and 22 (4.3%) cases, respectively. Oxygen desaturations below 92% for more than 30 seconds occurred in 2 (0.4%) cases. The lowest recorded desaturation was 85% and both cases were associated with ESM-Ketamine pathway deviations. All patients recovered uneventfully. There were no deaths or injuries associated with ketamine use.Conclusion: The ESM-Ketamine package appears safe for use by trained providers in support of procedural sedation when previously an anesthetist would not have been called. Scale of the ESM-Ketamine package may support the human rights imperative that every person deserves pain relief when undergoing a painful procedure. 31 Lei Cai, MD, Ph.D, Surgery - Surgical Oncology Clinical Impacts of the Immune Environment in Pancreatic Neuroendocrine Tumors (PanNET) L. Ferrone1 and C. Ferrone1 1Surgery, Massachusetts General Hospital, Boston, MA, USA and 2Pathology, Massachusetts General Hospital, Boston, MA, USA Introduction: Characterization of the role of immunological events in the pathogenesis and clinical course of pancreatic neuroendocrine tumors (PNETs) may lead to the identification of prognostic biomarkers and to the design of effective immunotherapeutic PanNETs CD8+ T cell, CD4+ cell and macrophage infiltration, as well as immunologically relevant molecule expression. Results were correlated with histopathological characteristics and with disease-free (DFS) and disease- specific (DSS) survival.Results: Fifty-seven and 47 patients were WHO grade 1 and 2, respectively. PD-L1 and B7-H3 were expressed in 53 and 78% PanNETs, respectively. High intratumoral CD8 + T cell density correlated with prolonged DFS (P=0.05), especially when the number of tumor associated macrophage (TAMs) was low. In contrast, high peritumoral CD4+ cell and TAM density correlated with worse DFS and DSS, especially when intratumoral CD8+ T cell density was low. TAM density (P=0.02), WHO grade (P=0.04), T stage (P=0.01), and lymph node positivity (P=0.04) were independent predictors of DFS. Even in WHO grade 1 PanNET patients, high TAM density (P=0.026) and T stage (P=0.012) were independent predictors or DFS. TAM density was the sole independent predictor of DSS (P=0.02). HLA class I expression was defective in about 70% PanNETs. HLA class I downregulation correlated with poor DSS in PD-L1-negative tumors (P=0.02).Conclusion: Immunological events appear to play a critical role in the clinical course of PanNET. The association of HLA class I expression with DSS only in patients without detectable PD-L1 expression by PanNET cells suggests that HLA class I expression may be a useful biomarker to identify PanNET patients who may benefit from checkpoint inhibitor-based strategies. Furthermore, the negative impact of TAMs on DFS and DSS suggests that PanNETs may benefit from TAM-targeting therapies. 32 Kevin M. Callans, BSN, Nursing The Parental Experience with Decision Making to Have a Hypoglossal Nerve Stimulator Implantation in Their Child with Down Syndrome and Sleep Apnea-Preliminary Findings K.M. USA, 2MGH, Boston, MA, USA, 3ENT, MEEI, Boston, MA, USA, 4Hospital for Children, MGH, Boston, MA, USA and 5Graduate School of Nursing, University of Lowell, Lowell, MA, USA Introduction: Sleep apnea is present in 30-80% of children with Dwn Syndrome resulting in increased morbidity and a decrease in quality of life. Children with Down Syndrome often undergo Adenotonsillectomy to reduce sleep apnea. about 65% will have persistent sleep apnea. Treatment often requires positive pressure airway support, other surgical options or home oxygen. These treatments are poorly tolerated. A pilot study has been undertaken for the insertion of a hyperglossal 26nerve stimulator in adolescence with Down Syndrome and sleep apnea. The stimulator is approved for use in adults that meet selected criteria. This study is interested in describing the process that parents go through to reach the decision to consent for their child to have a hypoglossal nerve stimulator implanted. With an understanding of the Purpose of Hypoglossal Nerve Stimulator, parents agreed and adolescences assented to the implantation. Methods: Parents agreed to interviews for this for this study. These parents consented to the participation of their child in A Pilot Study to Evaluate the Safety and Efficacy of the Hypoglossal Nerve Stimulato in Adolescents With Down Syndrome and Obstuctive Sleep Apnea. (NCT02344) at the MEEI After obtaining informed consent, interviews were conducted by telephone and audiotaped. An open ended qualitative descriptive design was used with a semi structured guide for data collection. Descriptive content anaylysis was used to analyze data.Results: Nine mothers participated in the indepth interviews. Four themes identified-Parents running out of options resulting in distress Having trust in surgical team allows parents to be confident in their decision to ursue this life changing interven - tion. The team's ability to recognize the uniqueness of each child personalized the experience. Being supported by nurses throughout the experience allows parents to feel empowered. Conclusion: Parents describe having been at a breaking point on options for treatment for their child's sleep apnea, trust in the health care team, the Hypoglossal Nerve Stimulator was life changing for the child and the family The Hypoglossal Nerve Stimulator is novel option for children with Down Syndrome and sleep apnea. 33 Karen J. Campoverde Reyes, MD, Medicine - Endocrine-Neuroendocrine Weight loss surgery utilization in patients aged 14-25 with severe obesity among several healthcare institutions in the United States K.J. Campoverde Reyes1,2, M. Misra1,3, H. Lee4 and F.C. Unit, Massachusetts General Hospital, Boston, MA, USA, 2Liver Research Center, Beth Israel Deaconess Medical Center, Boston, MA, USA, 3Pediatric Endocrinology, Massachusetts General Hospital, Boston, MA, USA, 4Biostatistics Center, Massachusetts General Hospital, Boston, MA, USA, 5Medicine- Gastroenterology, Massachusetts General Hospital, Boston, MA, USA and 6Weight Center, Massachusetts General Hospital, Boston, MA, USA Introduction: Obesity rates have reached pandemic levels in the United States with prevalence rates of 39.8% in adults and 18.5% in youth. Although previous reports on obesity in children and adolescents had shown a stable obesity prevalence since 2013-2014, current reports show no evidence of a decline in obesity prevalence at any age. Conversely, a significant increase in severe obesity in children and an upward trend in many subgroups, including adolescents, has been found. A high body mass index (BMI) for age, especially 99th BMI percentile, is associated with early co-morbidities, such as type 2 diabetes, non-alcoholic fatty liver disease (NAFLD), dyslipidemia, and heart disease, and early mortality. It is important to ascertain the most effective strategies to manage weight loss in these populations, especially for those with severe obesity. Weight loss surgery (WLS) in adolescents with severe obesity reliably achieves safe and lasting improvement in BMI and resolution of comorbid diseases superior to other treatment modalities. Even though WLS has shown to be the most effective to date to create sustainable changes in metabolic derangements for moderate to severe obesity and its comorbidities, factors surrounding low utilization among young patients in the clinical practice are unclear. This study wants to show and compare the prevalence of severe obesity and WLS utilization in adolescents and young adults among several healthcare institutions in the United States Methods: The prevalence of obesity, severe obesity, and WLS utilization between 2000 and 2017 in patients between 14-25-years-old were obtained using the Scalable Collaborative Infrastructure for a Learning Health System (SCILHS) and the Research Patient Data Registry (RPDR) query web-based tools. The SCILHS enables a common data model across 10 health systems, covering more than 8 million patients, enabling a national research network formed on an advanced information technology (IT) infrastructure (17). The RPDR is a centralized clinical data registry that gathers clinical information from various hospitals in the Partners Healthcare system, such as Massachusetts General Hospital (MGH) and Brigham and Women's Hospital (BWH), covering around 4 million patients. Data on gender, age, race, and WLS utilization were available from 8 healthcare systems: Washington University in St. Louis, Morehouse Medical School, University of Texas-Houston, Wake Forest Baptist Medical Center, Beth Israel Deaconess Medical Center (BIDMC), Boston Children's Hospital (BCH), Boston Medical Center (BMC), and Partners Healthcare.Individuals with severe obesity were identified using the ICD-9 code for severe obesity and BMI 40. Data for WLS were also obtained using ICD-9 codes. Statistical analyses involved a series of univariate analyses consisting of Fisher's exact test to evaluate the difference in WLS utilization rates between each healthcare institution and Partners Healthcare system. This study was funded by the Physician/Scientist Development Award (PSDA) granted by the Executive Committee on Research (ECOR) at MGH.27Results: Among 2500635 individuals 14-25 years old, 18008 (0.7%) had severe obesity, 61% were female, and the racial breakdown was as follows: 0.1% Native American, 1.2% Asian, 25.6% African American, 40.4% White, 17.3% Other and 15.5% Unknown. At Partners, 404 (21.5%) of 1879 patients with severe obesity underwent WLS. In contrast, 44 (2.5%) of 1788 at Washington University, 13 (2.3%) of the 575 at BIDMC, 43 (1.4%) of the 2969 at BMC, and 37 (0.4%) of 8908 individuals with severe obesity at BCH underwent WLS.Conclusion: WLS is a safe and effective treatment for adolescents and young adults with severe obesity, however it has been widely underutilized. Inadequate education and awareness, suboptimal support, and inadequate tools and access to navigate the decision-making process regarding bariatric surgery might influence this outcome. These factors remain unclear and further studies need to be conducted to ensure WLS is adequately utilized for those patients who would achieve the most benefit. 34 Julia E. Carp, Psychiatry Quality of Life and Psychological Distress in Patients with Acute Myeloid Leukemia (AML) J.E. Carp1, J. Temel1 and A. El-Jawahri1 1Massachusetts General Hospital, Boston, MA, USA, 2Dana-Farber Cancer Institute, Boston, MA, USA and 3Duke University Medical Center, Durham, NC, USA Introduction: Older patients with AML face difficult treatment decisions as they can be treated either with multi-drug 'intensive' chemotherapy requiring a prolonged hospitalization, or 'non-intensive' chemotherapy. Although intensive chemotherapy is often perceived by clinicians as more burdensome, studies comparing patients' quality of life (QOL) and psychological distress while receiving these treatments are lacking. Methods: We conducted a longitudinal study of older patients ( 60 years) newly diagnosed with AML receiving intensive (i.e. 7+3: cytarabine/anthracycline combination) or non-intensive (i.e. hypomethylating agents) chemotherapy at two tertiary care hospitals. We assessed patient's QOL [Functional Assessment of Cancer Therapy-Leukemia], and psychological distress [Hospital Anxiety and Depression Scale [HADS]] at baseline and 2, 4, 8, 12, and 24 weeks after diagnosis. We compared the proportion of patients in each group reporting clinically significant depression or anxiety (HADS subscale cut off 7) and used mixed linear effects models to compare QOL and psychological distress longitudinally between groups.Results: We enrolled consecutive patients within 72 hours of initiating intensive (n=50) or non-intensive (n=50) chemotherapy. There were no differences in baseline QOL, depression, or anxiety symptoms between the groups. At baseline, 33.33% (33/100) and 30% (30/100) of the overall cohort reported clinically significant depression and anxiety, respectively, with no differences between groups. At 4 weeks, 41.98% (34/81) of patients in the overall cohort reported clinically significant depression, with no differences between groups. In mixed linear effects models, there were no differences in QOL (=-0.71, between groups over time. Conclusion: Older patients with AML receiving intensive and non-intensive chemotherapy experience similar QOL impair - ments and high rates of psychological distress. These findings underscore the need to develop supportive care interventions for older patients with AML, regardless of their initial treatment strategy.28 35 Catherine Cebulla, B.S., Neurology [11C]-PBR28 PET Imaging in Symptomatic C9orf72 Repeat Expansion Carriers and Sporadic Amyotrophic Lateral Atassi1 1Neurology, Neurological Clinical Research Institute (NCRI), Boston, MA, USA and 2Radiology, A. A. Martinos Center for Biomedical Imaging, Charlestown, MA, USA Introduction: Glial activation is implicated in amyotrophic lateral sclerosis (ALS). Recent in vivo molecular neuroim- aging studies using [11C]-PBR28 positron emission tomography (PET) have demonstrated increased [11C]-PBR28 uptake in the motor cortices in people with ALS. This current study focuses on the C9orf72 genetic form of the disease. Structural neuroimaging studies have shown that symptomatic C9orf72 carriers (C9+) have greater brain atrophy and more diffuse cortical thinning compared to sporadic (C9-) ALS patients and healthy controls (HC). Further investigations demonstrate that patients with C9+ genotype have a significantly higher incidence of comorbid frontotemporal dementia (FTD) and shorter survival than patients with C9- ALS. Here, we employed an integrated technology of PET and magnetic resonance imaging (PET-MR) to compare the spatial distribution of [ 11C]-PBR28 PET-MR measurements between people with C9- ALS and C9+ ALS, compared to HC.Methods: [ 11C]-PBR28 PET and MRI were performed in ten C9+ ALS, 72 C9- ALS and 23 HC individuals. Individ- uals with ALS were clinically evaluated using the revised ALS functional rating scale (ALSFR-R), upper motor neuron burden (UMNB), and ALS cognitive behavioral screen (ALS-CBS). The brain uptake of [ 11C]-PBR28 was quantified as standardized uptake value from the 60-90 min post-radiotracer injection and normalized by whole brain mean (SUVR). Whole brain surface-based analyses were conducted in FreeSurfer (v6.0) to compare cortical thickness as well as SUVR between the groups, and to correlate PET signal with the clinical outcome measures. All analyses were set to keep clusters that have cluster-wise p values (p< 0.01), and the voxel-wise/cluster forming threshold was Z=2 (p<0.01). Age, gender and [ 11C]-PBR28 binding affinity (which is predicted by the Ala147Thr polymorphism in the TSPO gene) were modeled as a regressor of no interest in these analyses.29Results: Surface-based analyses revealed increased [11C]-PBR28 uptake in all ALS patients compared with HC. This elevated [11C]-PBR28 PET signal was mainly observed in the precentral and paracentral gyri in (C9-) ALS patients compared with HC, and largely co-localized with cortical atrophy in the same anatomical regions (Figure 1A, 1B; pcw<0.01). (C9+) ALS patients showed a unique pattern of [11C]-PBR28 PET signal distribution in both frontal and temporal cortices as well as in the precentral and paracentral gyri compared with HC (Figure 1C, These findings were associated with widespread cortical atrophy in largely overlapping anatomical regions. In addition, higher [11C]-PBR28 uptake and more cortical thinning were observed in the temporal, frontal and occipital regions in (C9+) compared with (C9-) ALS (Figure 1E, 1F; pcw<0.01). Finally, [11C]-PBR28 PET signal in the precentral and paracentral pcw<0.01), and negatively correlated with ALS-CBS score in left temporal cortex in (C9+) ALS (Figure 2B; pcw<0.06, trend). No correlation was detected between [11C]-PBR28 uptake and ALS-CBS score in any brain region in (C9-) ALS (Figure 2D). Conclusion: [11C]-PBR28 PET signal is elevated in the motor cortices in sporadic ALS and in the frontotemporal and motor cortices in C9+ ALS. The same spatial pattern largely co-localized with areas of cortical atrophy. Worse upper motor neuron dysfunction measured by UMNB correlated with the [ 11C]-PBR28 uptake in the motor cortices, and cognitive dysfunction measured by ALS-CBS correlated with [11C]-PBR28 uptake in the temporal lobe. A. Ceranoglu, MD, Psychiatry Efficacy of Transcranial Near-Infrared Light Treatment in Autism Spectrum Disorder: Interim Analysis of an Open-Label Proof of Concept Study of a Novel Approach T.A. Ceranoglu, B. Hoskova, P. Cassano, J. Biederman and G. Joshi Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: Autism spectrum disorder (ASD) is characterized by variable presentation of difficulties with socializa - tion, communication, and restrictive/repetitive behaviors (RRB), for which no established pharmacological treatment exists. Transcranial Laser Emitting Diode (LED) Therapy (TLT), is a non-invasive intervention with near-infrared light to forebrain. It is absorbed by Cytochrome-C Oxidase in mitochondria and increases energy metabolism, leading to cellular plasticity and cytoprotection. Recently, TLT was found to be safe and effective in treatment of depression. Effects of TLT on ASD symptoms remains to be explored.Methods: Patients with diagnosis of ASD, between 18-55 years of age, were enrolled to receive twice a week TLT for 8 weeks in an open-label single group design. Demographic and clinical information were recorded, ASD symptoms were measured at baseline, midpoint and endpoint, by clinician and patient rated scales including MGH-Social Emotional Competence Scale-Revised (MGH-SECS-C) Social Responsiveness Scale-Second Edition (SRS-2), Behavior Rating Inventory of Executive Function - Adult Self-Report Version (BRIEF-A), Adult ADHD Self-Report Scale (ASRS), Clinical Global Impression (CGI) and Global Assessment of Functioning (GAF). A 30% reduction in SRS-2 total score and/or CGI-ASD-Improvement score 2 was considered as response. Adverse events were recorded during visits. Paired-samples t-test analyses were performed. Results: Six participants (5 male; 30.2 yrs, SD 13.3) completed the study. Four patients (67%) met responder criteria. A statistically significant change was noted in SRS-2 RRB subscales (p=0.003), MGH-SECS-C, all CGIs and GAF at midpoint and endpoint. A statistically significant change in SRS-2 Social Motivation subscale (Mot; p=0.009) was noted at endpoint. Changes in other scales did not reach statistical significance. One patient developed headaches after initial treatment, and spontaneously recovered. No other side effects or serious adverse events occurred. Adherence rate was 98%. Conclusion: Transcranial LED Therapy may be a promising treatment for core social deficits associated with ASD, and is a safe, feasible treatment approach. Further research into its efficacy in treatment of core features of ASD is necessary and warranted. Demographic Characteristics 37 Akhil Chawla, MD, Surgery - Surgical Oncology Evaluation of Clinical and Radiographic Response in Patients with Metastatic Melanoma Treated with Immunotherapy Frederick and G.M. Boland Oncology, Massachusetts General Hospital, Boston, MA, USA Introduction: Melanoma patients undergoing immunotherapy (IT) demonstrate a wide range of treatment responses. While immune-related response criteria (irRC) have been introduced, it has yet to become as broadly utilized as Response Evalua - tion Criteria in Solid Tumors (RECIST). Oncologists also utilize clinical designations to describe tumors with atypical responses. We compared the relationship between radiographic response, clinical response, and overall survival (OS). Methods: Patients treated with IT targeting programed cell death 1 receptor (aPD-1) with or without combination cytotoxic T-lymphocyte-associated protein 4 (aCTLA-4) therapy were identified. The best responses while on IT were collected. 31For radiologic response, if RECIST staging was not available, radiology reports were reviewed to characterize response. Clinical response was derived from review of oncology reports. OS was calculated from start of treatment and analyzed using the Kaplan-Meier method. Prognostic performance was assessed using the Akaike information criterion (AIC) from univariate Cox proportional hazard analysis. Results: 146 patients with metastatic melanoma were included. 69.9% were male and 77.3% with median age of 65.5 (IQR: 54-73). 23.3% of patients who were characterized clinically as having mixed response were catego-rized as having progressive disease radiologically. There was a significant difference in OS between groups when stratified by radiographic response (complete, partial, progressive and stable disease; Figure 1A), as well as clinical response (responders, mixed responders, and non-responders) (p<0.0001). Furthermore, separating out mixed responders from those deemed to have progressive disease radiographically, demonstrated a survival benefit in mixed responders (log-rank p<0.0001; Figure 1B). The AIC was similar for both clinical and radiographic staging.Conclusion: These results suggest that the survival of mixed responders may be underestimated by using traditional radiographic response and may be better classified using clinical criteria as a cost-effective and timely surrogate for irRC response. Anti-PD1 Cohort Characteristics When separated out from those deemed to have progressive disease radiographically, mixed responders demonstrated a survival benefit.3238 Youbai Chen, MD, Oral and Maxillofacial Surgery Risk factors of long-term skeletal relapse after mandibular advancement using bilateral sagittal split M. August1 Maxillofacial Surgery, MGH, Boston, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Bilateral sagittal split osteotomy (BSSO) is the most commonly performed orthognathic surgery to advance mandible for the correction of mandibular retrognathia in patients with Class II malocclusion. However, one major concern of BSSO mandibular advancement (MA) is the postoperative stability. The long-term skeletal relapse (LTSR), which may continue to occur several years postoperatively, is difficult to predict and solve due to its complex multifactorial etiologies. Although previous studies showed some potential risk factors of LTSR, it remains unclear and controversial in terms of their associations and independent contributions to LTSR due to differences in patient populations, small sample size, different definitions and measurements of cephalometrics, and short period of follow-up. The purpose of this study was to answer the following clinical question: Among patients who underwent BSSO MA, what were the independent risk factors of LTSR and how do they affect the LTSR? Methods: After the approval by the Partners Institutional Review Board (protocol number 2017P002223), we retrospectively reviewed the medical records of all 315 patients who underwent BSSOMA in the Department of Oral and Maxillofacial Surgery at Massachusetts General Hospital from June 1, 2005 through June 1, 2016. Patients who had complete medical records including preoperative (T0), immediate postoperative (T1), and a minimum of 2 years postoperative (T2) lateral cephalograms were included. The outcome variable was the LTSR, which was defined as the positional changes of the B-point between T1 and T2 lateral cephalograms. Univariate and multivariate linear regressions were applied using LTSR as the continuous dependent variable. Multivariate logistic regression was performed using dichotomized LTSR as the dependent variable with the 2 mm cutoff. The receiver operating characteristic (ROC) analysis was performed. P-value less than 0.05 was considered statistically significant. Statistical analysis was done using STATA v15.0 (StataCorp LLC, College Station, Texas, USA).Results: A total of 315 patients underwent BSSO MA in our department from June 1, 2005 through June 1, 2016. Only 81 cases were included according to the inclusion and exclusion criteria. 57 (70%) were females and 24 (30%) were males with an average age of 29 years (ranged from 18 to 56 years). The average follow-up was 3.5 year (ranged from 2 to 10 years). 70(86%) of them received preoperative orthodontic treatment. 54(67%) cases had preoperative temporo-mandibular joint disorder. They showed a preoperative mandibular plane angle of an average 36.7\u00b0 (18\u00b0 to 40\u00b0). 72(89%) underwent double jaw surgery. In 50(62%) participants, the mandible was advanced under a counter-clockwise rotation. 57(70%) underwent concurrent genioplasty. The average magnitude of mandibular advancement was 9.2\u00b1 5.8 mm at B-point. 66(82%) osteotomies of BSSO were using bicortical by miniplate and monocortical (11%) and hybrid (7%). We observed an average 2.1\u00b1 1.7 mm horizontal relapse at B-point, corresponding to 23% of the surgical advancement. The vertical relapse at B-point 1.3\u00b1 1.1 mm. In the subgroup analysis, we found the risk of relapse is 2.3 times higher in patients with 7 mm compared to < 7 mm mandibular advancement (95%CI: 1.4, 3.5, P=0.021). The horizontal LTSR in the low-angle group and high-angle group was 0.4\u00b1 0.7 mm and 2.7 \u00b1 1.8 mm, respectively (p < 0.01). Horizontal LTSR in single jaw surgery was 1.2\u00b10.8 mm vs. 2.8\u00b11.8 mm in double jaw group (P<0.05). Pearson correlation analysis showed a positive correlation between the LTSR and preoperative TMD, mandibular plane angle, magnitude of advancement, direction of rotation. 61(75%) of the patients displayed a < 2 mm horizontal relapse. Multivariate regression showed mandibular plan angle, magnitude of advancement, counter-clockwise rotation, obstructive sleep apnea, preoperative TMD was signifi- cantly associated with LTSR, whereas age, sex, single or double jaw surgery, genioplasty, and duration of postoperative maxilla-mandibular fixation had no association with LTSR.Conclusion: The magnitude of advancement, preoperative MP angle, and counterclockwise rotation were the most important risk factors of LTSR. Advancement of greater than 7 mm was associated with an increased tendency to relapse. Patients with higher mandibular plane angle had higher risk of relapse than those had medium angle and low angle.3339 Wenbo Cheng, Massachusetts Eye and Ear Infirmary (MEEI) - Ophthalmology The EyeTurn App for School Vision Screening W. Cheng1, M. Lynn1,2, C. Shi1, M. Tomasi1, S. Pundlik1, L. Gang1 and K. Houston1 1Schpens Eye Institute, Boston, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: To evaluate the feasibility of a photographic Hirschberg smartphone app (EyeTurn) during routine school screening with the primary outcome of measurement success rate. Secondary outcomes include a preliminary report of sensitivity and specificity for the app done by comparing EyeTurn's ability to detect cases of strabismus as compared to conventional school screening (acuity with Lea Symbols and stereopsis). Methods: A school nurse performed the conventional school screening and then attempted to take 3 measurements with the EyeTurn mobile app for 133 children grades 1 to 5 (ages 6 to 11). A successful measurement was defined as by the app's ability to fit the border of the iris and the location of the corneal reflection. Positive strabismus measurements from EyeTurn were defined by a threshold calculated by clustering data into two classes.. Students who met criteria for a positive screen with this threshold were then re-evaluated with prism cover-test. Specificity and sensitivity were calculated by comparing the number the students with strabismus as defined by either >8exophoria or >6esophoria (recommended by AAPOS) by prism cover test to the 44 students with positive screens for strabismus with the EyeTurn app measurement.Results: 123 (92.5%) children had at least 1 successful measurement; 66.0% had 2; 33.0% had 3 for an image capture success rate of 85.0% (322/379). The median strabismus degree was 2.78, the 25th and 75th percentile were 1.17 and 3.27 separately. The cut-off point was calculated as 3.30 . Specificity and sensitivity when using this threshold were 84.0% (105/125) and 83.3% (5/6). There were 2 cases of intermittent strabismus and 3 cases of large phoria (presumably dissociated by the camera flash) identified by the app which were missed by the conventional screening. The most common issue reported by the nurse was sensitivity to the flash resulting in eye closing and app measurement failure. Overall the nurse reported her experience using the app as positive and intuitive. Conclusion: It is feasible for a school nurse to use Eyeturn during routine screening as demonstrated by the ability to obtain at least 1 suitable image in 92.5% of children. Eyeturn may offer improved screening capabilities as demonstrated by the app's ability to detect cases of strabismus that were missed during conventional screening. Further improvements to address sensitivity to flash and improve user interface are currently underway. 40 Dhinakaran M. Chinappen, Master of Engineering, Neurology The Probability of Seizures during EEG monitoring in High-Risk Neonates L. Worden1, Westover1,3 and C.J. Chu1,3 1Neurology, Massachusetts General Hospital, Boston, MA, USA, 2Mathematics and Statistics, Boston University, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: Continuous EEG monitoring (cEEG) of high-risk neonates has been recommended due to the high incidence of seizure in this population; however, optimal duration of monitoring and frequency of review remain uncertain for this limited resource. We hypothesized that seizure risk would decrease with duration of monitoring and that both monitoring indication and early epileptiform activity would impact the risk of seizure over time. We characterize seizure risk over time in a large population of neonates undergoing cEEG following ACNS criteria and evaluated the impact of early epileptiform activity and monitoring indication. We further explored the impact of several early EEG background features and post-menstrual age.Methods: All neonates (<= 48 weeks post menstrual age, PMA) who underwent continuous EEG (cEEG) monitoring at MGH from January 2011 to October 2017 were reviewed (n=338). Medical records were reviewed for monitoring indication and etiology. Early EEG reports were reviewed for presence of epileptiform activity as well as the presence of abnormal background features including: continuity, amplitude, voltage asymmetry, asynchrony, excessive sharp transients, and burst suppression. PMA was categorized as preterm (<37 weeks), term if in between 37 and 44 weeks, and post-term if > 44 weeks. Seizure risk was analyzed using Kaplan-Meier survival analysis and significance was computed using a log-rank test (Bewick et al., 2000). Figures are displayed as Kaplan-Meier survival curves with 95 % confidence intervals with the cumulative probability of seizure recurrence plotted as a function of time per hour of cEEG monitoring. We report the risk of subsequent seizure from the time at EEG and compare the impact of monitoring indication, early IEDs, early EEG background features, and age on subsequent seizure risk. Patients were censored at the time of seizure of when cEEG ended. Significant predictors were then included in a multivariate Cox proportional hazard model to evaluate adjusted hazard ratios with p-value set at <0.05 and log likelihood ratio test at >0.10. To characterize the relationship between seizure risk and time, we built logistic regression model of seizure risk for each significant predictor. To account for censoring, we included duration of follow-up as a predictor in each model.34Results: Seizures occurred in 28% (94/338) of neonates if ANC, 21% if OHR condition). Average time to seizure was 7.4 \u00b1 14.8 hours. Among those with seizures, the first seizure occurred in <1 hour in 33%, 1-2 hr in 7% and hr in 2%. Among all neonates who underwent cEEG, the odds of seizure decreased over time such that there was a 10.5% (95% CI [7.9%, 13.1%], p<e-13) reduction in the odds of a seizure with each 1 hour increase in monitoring. Indication for monitoring predicted seizure risk, where neonates with a CESS had the highest risk and those with neonatal encephalopathy the lowest risk (p=0.028). Among neonates monitored was a 23.7% (95% CI [16.3%, 30.5%], p<e10-8) decrease in the odds of first seizure per hour of cEEG. The rate was similar among neonates observed for Among neonates undergoing cEEG for ANC, the rate of decline was slower, where there was only a 4.6% (95% CI [2.3%, 6.9%], p<e-04) per hour of cEEG. The presence of epileptiform activity on the early EEG report predicted seizure risk (p<0.0001). Among neonates with epileptiform activity on early a 26.6% (95% CI [18.6%, 33.9%], p<e10-8) decrease in the odds of first seizure per hour of cEEG. On exploratory analysis, PMA appeared to predict seizure risk, where term babies had the highest risk of seizure (p=0.038). Among all early background features explored, only the presence of asynchrony appeared to increase seizure risk (p<0.0001). In a multivariate cox regression analysis including indication, early epileptiform activity, PMA, and asynchrony, only the presence of early epileptiform activity remained a significant predictor of seizure risk (p<0.0001). Conclusion: Neonates who undergo cEEG following ACNS criteria are at high risk of seizure. Patients with early epilep - tiform activity are at highest risk of subsequent seizure and resources should be triaged to pay careful attention to these neonates, especially in the first hours of monitoring. Although half of all neonates that will have seizures are identified in the first six hours, this rate varies based on indication and neonates undergoing cEEG for neonatal encephalopathy have a slower risk reduction with monitoring duration supporting the need for a minimum of 72 hours of cEEG and frequent review. Except for asynchrony, early abnormalities in background features did not predict seizure risk. Figure 1. KM Curves for Seizure Proportion based on PreLTM Epileptiform Activity. Comparison of Kaplan-Meier survival curves for length of time until occurrence of primary endpoint (first seizure event) based on the presence (red) or absence (blue) of Pre Long Term Epileptiform activity in EEG recordings, using log-rank testing. 41 Carolina Chiou, M.D., Massachusetts Eye and Ear Infirmary (MEEI) - Ophthalmology Characterization of Wedge-Shaped Defects in Prelaminar Tissue in Primary Open Angle Glaucom C. Chiou1, M. Wang1,2, E. Taniguchi1,3, L. Pasquale1,6 and L. Shen1 1Massachusetts Eye and Ear, Boston, MA, USA, 2Schepens Eye Research Institute, Boston, MA, USA, 3Universidade Federal de S\u00e3o Paulo, S\u00e3o Paulo, Brazil, 4Universidade de S\u00e3o Paulo, S\u00e3o Paulo, Brazil, 5Ophthalmology and Visual Services, Atrius Health, Boston, MA, USA and 6Channing Division of Network Medicine, Brigham and Women's Hospital, Boston, MA, USA Introduction: Glaucoma is a heterogeneous group of diseases that result in the progressive loss of retinal ganglion cells and their axons within the eye, leading to peripheral vision deficits and potentially significant visual disability. These structural features are secondary to changes in both the prelaminar and lamina cribrosa tissue of the optic nerve head (ONH). The prelaminar component, located within Bruch's membrane opening (BMO) of the ONH composed of both neuronal and connective tissue, is therefore a relevant and important structure in the pathophysiology of glaucoma. Various studies have 35described a global decrease in the thickness of the prelaminar tissue in diseased eyes. However, clinically, the glaucoma - tous ONH has focal changes associated with functional deficits. Therefore, localized damage may be a better surrogate for evaluating disease. There is a scarcity of literature describing focal prelaminar patterns, likely due to limitations of available imaging modalities. However, the advent of novel ophthalmologic imaging devices, such as swept-source OCT (SS-OCT), has allowed for non-invasive high-resolution and high-speed imaging. Compared to conventional imaging devices, SS-OCT has longer central wavelengths and higher scanning speeds. Imaging for each eye can be achieved in a few seconds, minimizing motion artifacts. These characteristics allow for deeper tissue penetration, less shadowing from overlying blood vessels in the ONH, and denser scanning patterns. In this study, we utilize SS-OCT to identify prelaminar wedge defects (PLWDs) and aim to elucidate the associated demographic, ophthalmic and functional characteristics.Methods: This is a prospective, cross-sectional study of patients with POAG and PLWDs, patients with POAG and no PLWDs, and age-matched healthy control subjects. Imaging was performed with SS-OCT (Topcon DRI Swept Source OCT) and spectral-domain OCT. POAG patients with severe glaucoma indicated by Humphrey Visual Field Mean Deviation (HVF MD) <-12 dB, history of penetrating glaucoma surgery, and eyes with significant non-glaucomatous optic neuropathy were excluded. PLWDs, defined as lesions where the curved contour of the optic nerve head cup is lost and a triangular (wedge-shaped) defect is present in the prelaminar layer, were identified by two observers masked to diagnosis. Two-sided t-test, analysis of variance with Bonferroni correction, and multivariate logistic regression analysis adjusting for systemic and ocular characteristics were performed. Results: Sixty-three subjects (40 POAG patients and 23 controls) were included in this study. PLWDs were found in 16 POAG patients (40.0%) versus 2 control subjects (8.7%, p=0.018). POAG patients with and without POAG patients with PLWDs, 9 (56.3%) had subsequent imaging, all demonstrating persistent PLWDs and no additional new PLWDs. Laminar defects were present in 7 patients with POAG and PLWDs (43.8%), 0 POAG patients without with a PLWD (p= 0.001). loss was present 33.3% (p=0.20). The POAG patients with had more frequent history of disc hemorrhage (DH) (37.5%) compared to POAG patients without PLWDs (8.3%, p=0.04, Table 1). The locations of DHs (superior vs. inferior) and PLWDs were found to be concordant in 83.3% (5 out of 6) of cases for patients with POAG and PLWD. Multivariate analysis showed that POAG patients with PLWD were younger in age (p=0.03) and had increased odds of having a history of DH (odds ratio=14.1, p=0.02) compared to POAG patients without PLWD after adjusting for gender (p=0.37), VF MD (p=0.20) and maximum intraocular pressure (IOP) (p=0.14).Conclusion: PLWDs are more commonly found in POAG patients, and are associated with a history of DHs, laminar defects, and younger age. DHs are hemorrhages perpendicular to the optic disc that suggest poorly controlled disease and indicate a need for more aggressive therapy. Given that DHs are a marker of glaucoma progression but are often transient and can be missed on clinical examination, PLWDs may also be useful in identifying high-risk patients and particularly helpful because of their persistent nature and reliable identification. Eyes with laminar defects have been reported to experience more rapid glaucomatous VF progression and additional medical treatment is thought to improve functional outcomes; thus, the associa - tion with PLWD may prove to be clinically relevant. Our findings suggest that PLWD may depict a specific pattern of damage in glaucoma and, along with younger age, may represent an optic nerve phenotype associated with a distinct POAG subset that may benefit from both increased surveillance and medical intervention.36 42 Michelle Chiu, Pediatrics Improving Patient Experience: Increased Inpatient Satisfaction Scores by Optimizing Physician Check-ins M. Chiu, J. Paolino, J. Lillemoe, V. Madhavan, J. Kulla, C. Benjamin and Z. Kerstin Pediatrics, MGH, Boston, MA, USA Introduction: Two main principles of family-centered care, as described by the American Academy of Pediatrics Committee on Hospital Medicine, are \"information sharing\" and \"partnership and collaboration.\"[1] Morning interdisciplinary team rounds are often identified as the key time to integrate these principles in the inpatient setting. There are barriers to optimized communication with families during morning rounds, however, including time constraints and availability of family members. Additionally, new information becomes available throughout the day, and there is no formal rounding process during which these updates can be shared with the family. To address these issues, a system of frequent check-ins was implemented to provide further opportunities for communication with patients and families. Using the preexisting post-admission Pediatric Inpatient Experience survey, we identified an area of improvement in satisfaction with physician check-ins. Herein we report on an ongoing quality improvement initiative that was begun in the second half of 2017 to improve the experience of patients and families cared for by the pediatric medical teams on the MGH inpatient floors. [1] American Academy of Pediatrics Committee on Hospital Care. (2003). Family-centered care and the pediatrician's role. Pediatrics, 112, 691-697. Methods: A multidisciplinary team of nurses, physician attendings and residents, and representatives from the Family Advisory Council was assembled to approach this problem through a series of PDSA cycles. We identified several barriers to satisfaction with physician check-ins, one of which was families and patients not recognizing the roles of their care team members. We hypothesized that improved recognition of key members of the team would allow patients and family members to direct their questions or concerns to the appropriate people and recognize that check-ins occur in many forms throughout the day. A three-pronged intervention was implemented to address this problem. The first piece involved teaching residents and medical students to identify themselves by their title and their role on the primary team, as well as stating that they were there to \"check in\" as they visited throughout the day, creating the opportunity for dialogue on patient care. The second prong was the distribution of badge-backers that clearly identify residents as \"Resident Doctors.\" The third part was improving an existing distribution process for \"facesheets,\" which include names and headshots of the physician team, and which all patients should receive at the time of admission. An MGHfC Quality Improvement grant was obtained to fund these efforts.37Results: Through the end of 2017, on Ellison 17, patient satisfaction with physician check-ins increased from 54.4% to 57.7%. An analysis was conducted to assess the adequacy of \"facesheet\" distribution revealing that almost no families received this information at the time of admission; the sheets were typically handed out the following day on rounds, revealing an area for ongoing improvement. Conclusion: This is an example of a quality improvement initiative in which obstacles to patient satisfaction with physician check-ins were identified and are being addressed in a targeted fashion through the collaboration of an interdisciplinary team. Interim data from the Inpatient Experience Survey thus far demonstrates a meaningful improvement in patient satisfaction with physician check-ins, earning the unit an MGH 2017 Patient Experience Award. Next steps involve continued efforts to improve timely distribution of the \"facesheets,\" as well as creation of \"sorry we missed you\" cards with contact information to provide ways for families with tight schedules to contact physicians 43 Hak Soo Choi, Radiology Anti-TLR4 Antibody-Conjugated Targeted Agents for Tissue-Specific Choi1 1Department of Radiology, Gordon Center for Medical Imaging, Boston, MA, USA and 2The Second Affiliated Hospital, Scientific Research Centre, Xi'an, China Introduction: Circulatory cells have received a considerable interest as new drug delivery vehicles. Targeting macrophages and monocytes in particular has the potential to treat cancers by stimulating the innate immune system. Anti-TLR4 antibody raised against toll-like receptor 4 (TLR4) is a pathogen recognition receptor that confer functional specificity to macrophages. In the Choi laboratory, numerous NIR fluorophores have been developed that are feasible for bioconjugation with various kinds of specific small molecules, peptides, proteins, and many antibodies. In this study, the detailed mechanism of action of tumor-associated macrophages was investigated by conjugating the anti-TLR4 antibody with a nonsticky zwitterionic NIR fluorophore and observing its tumor targetability in xenograft liver tumor mice. Methods: The NHS ester form of ZW800-1C was conjugated on the antibody in phosphate-buffered saline (PBS), pH 7.8 for 3 h. The optical properties of ZW800-1C conjugated with the anti-TLR4 antibody (ZW-TLR4) was then analyzed using a spectrophotometer, and the labeling ratio of NIR fluorophores on an antibody was calculated based on their extinction coefficients. The specificity of NIR fluorescent ZW-TLR4 to macrophages was evaluated using mouse RAW264.7 cell lines. The biodistribution and targeting was monitored in the xenograft tumor model 24 h post-injection of ZW-TLR4 using the Fluorescence-Assisted Resection and Exploration (FLARE) real-time intraoperative imaging system.Results: The labeling ratio of ZW800-1C on TLR4 antibody was found to be about 0.5. ZW-TLR4 depicted strong NIR fluorescent signals in the cytosol of RAW264.7 cells, indicating receptor-mediated endocytosis. In the xenograft tumor model, ZW-TLR4 targeted the tumor site successfully, with major uptake in the kidneys. Real-time live-body imaging effectively reported the dynamic process of the biodistribution and clearance of ZW-TLR4 in vivo. Conclusion: The results suggest that anti-TLR4 antibody conjugated with NIR fluorophores could be useful for target- specific cancer diagnosis as well as hepatocellular carcinoma immunotherapy. NIR fluorescence imaging also has the potential to improve the sensitivity and specificity of in vitro and in vivo diagnostics; thus, our study is likely to have its greatest impact on the targeted imaging, diagnosis, and treatment of various human cancers and immune-related diseases. 44 Karmel Choi, PhD, Psychiatry Using Mendelian Randomization to Test Causal Bidirectional Influences Between Physical Activity and Depression K. Choi1,2, C. Chen1, M. Stein3, Y. Klimentidis4, M. Wang2, K. Koenen2,1 and J. Smoller1 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Harvard T.H. Chan School of Public Health, Boston, MA, USA, 3University of California San Diego, La Jolla, CA, USA and 4University of Arizona, Tucson, AZ, USA Introduction: Increasing evidence shows that physical activity is associated with reduced risk for depression, pointing to a potential modifiable target for prevention. However, the direction of this association is not clear: physical activity may protect against depression, and/or depression may result in decreased physical activity. Here, we used bidirectional two-sample Mendelian randomization (MR) that leverages the random assortment of genetic variants (i.e., SNPs) prior to birth to establish instruments free of usual sources of environmental or genetic confounding, to test causal influences between physical activity and depression.38Methods: For genetic instruments, we selected independent top SNPs associated with major depressive disorder (MDD, N = 143,265) and two physical activity phenotypes\u2014self-reported (N = 377,234) and objective accelerometer-based (N = 91,084)\u2014from the largest available, non-overlapping genome-wide association results. We used two sets of genetic instruments: (1) only SNPs previously reported as genome-wide significant, and (2) top SNPs meeting a more relaxed threshold (p < 1x10-7). For each direction of influence, we combined MR effect estimates from each instrument SNP using inverse variance weighted (IVW) meta-analysis, along with other standard MR methods such as weighted median, MR-Egger, and MR-PRESSO. Results: We found evidence for protective influences of accelerometer-based activity on MDD (IVW odds ratio (OR) = 0.74 for MDD per 1 SD unit increase in average acceleration, p = .006). In contrast, we found no evidence for negative influences of MDD on accelerometer-based activity (IVW b = 0.08 change in average acceleration for MDD versus control status, p = .70). Furthermore, we did not see evidence for causal influences between self-reported activity and MDD, in either direction and regardless of instrument SNP criteria. Conclusion: For the first time using Mendelian randomization and genome-wide association results from international consortia, we provide triangulating evidence that physical activity is associated with reduced risk for MDD. We further demonstrate that protective effects are specific to objective (i.e., accelerometer-based)\u2014and not self-reported\u2014physical activity, pointing to the importance of precise measurement of physical activity in epidemiological studies of mental health. Together, our findings provide further support for the hypothesis that physical activity is an effective prevention strategy for depression. 45 Joseph H. Chou, MD PhD, Pediatrics Machine learning analysis of maternal pregnancy clinical notes to predict newborns at risk for neonatal abstinence syndrome J.H. Chou MGH, Boston, MA, USA Introduction: Widespread adoption of electronic health records (EHRs) is generating vast amounts of data, but leveraging this big data to improve clinical care and drive research requires finding efficient ways to utilize data which is often unstruc-tured, inconsistent, and incomplete. Manual identification of data features is time consuming, and subject matter experts may overlook predictors not previously appreciated. In this study, machine learning methods were applied to unstructured free text notes during the first two trimesters of pregnancy to generate a predictive model for identifying newborns at risk for neonatal abstinence syndrome.Methods: The Partners eCare EHR was queried to identify mothers who delivered at the Massachusetts General Hospital between April 2016 and June 2018, with at least one clinical free text note during the first two trimesters of pregnancy. The prediction target of whether their newborns were suspected to be at risk for neonatal abstinence syndrome (NAS) was defined by newborns with >5 Finnegan NAS scores documented, excluding those with iatrogenic NAS. The full dataset was split into an 80% training set and a held-out 20% test set. For each mother, unigram word features were extracted and term frequency - inverse document frequency (TF-IDF) values calculated relative to the entire training set corpus. The TF-IDF values for each mother were used as predictors for binary outcome training via LASSO (L1 regularized) penalized logistic regression, which performs implicit feature selection, with cross validation to determine penalty weight. The resulting predic- tive model performance was assessed on the training set and the held-out test set.Results: There were 6,761 pregnancy episodes yielding 6,941 newborns for whom at least one maternal note was available during the first two trimesters. There were a total of 128,909 notes, with a median of 17 (IQR 11 to 24) per episode. Of the 6,941 newborns, 84 were considered at risk for developing NAS (1.21%). The training set of 5,554 newborns included 68 at risk for NAS (1.22%). The maternal notes from the training set contained 57,105 unique unigram word features, yielding a training set predictor matrix of 317,161,170 TF-IDF scores. Twenty-fold cross validation was used to determine a LASSO L1 regularization penalty weight. The resulting predictive model included 9 implicitly selected features (nas, heroin, clonidine, opioid, subutex, methadone, suboxone, abuse, and detox) and had an area under the receiver operator characteristic curve (AU-ROC) of 0.916. A threshold to limit the false positive rate to 1% achieved a sensitivity of 72.1%, specificity 99.7%, PPV 73.1%, NPV 99.7%, and prediction accuracy of 99.3% (accuracy > no information rate p < 0.0001). Applying the predictive model to the hold out test set of 1,387 newborn (16 at risk for NAS, 1.15%) yielded an AU-ROC of 0.930 and applying the same threshold yielded a sensitivity of 68.8%, specificity 99.8%, PPV 78.6%, NPV 99.6%, and prediction accuracy of 99.4% (accuracy > NIR p = 0.021). To demonstrate that this approach was generalizable, the same dataset was used to identify features associated with other outcomes. Prediction of preterm delivery before 35 weeks yielded features generally related to multiple gestation, a known risk factor, including triplet, mfm, monochorionic, mono, twins, pprom, ttts, and twin. Prediction of newborns with birthweight greater than the 90th percentile yielded features suggestive of diabetes, including macrosomia, diabetes, insulin, retinopathy, and joslin.39Conclusion: We demonstrate that a machine learning model of natural language processing of unstructured clinical notes can yield well-performing predictive models, despite using no prior knowledge and no subject matter expertise. Specifically, newborns at risk for NAS were predicted based on maternal pregnancy clinical notes. The model selected sparse and interpre- table word features. All data were extracted via database query from the EHR without manual chart review, and minimal computational resources were required. The approach was generalizable to identify features associated with other outcomes. Future work will focus on extending the approach by incorporating other types of predictors (e.g., laboratory data, problem lists, flowsheet documentation, and features identified by subject matter experts). Applications of this approach might include improved patient care (e.g., identification of at risk newborns at the point of care), prospective identification of patient cohorts (e.g., mothers who might benefit from enrollment in substance use disorder clinics), and retrospective identification of patient populations (e.g., patient cohorts for epidemiological studies). With the ever increasing availability of EHR data comes a tremendous opportunity to improve clinical care. Predictive model word features Unigram word features selected from clinical notes during the first two trimesters of pregnancy predictive of newborns at risk for neonatal abstinence syndrome. Beta: logistic regression coefficients to be used with TF-IDF predictors. IDF: inverse document frequency from the training corpus. 46 Jaeyoon Chung, PhD, Center for Genomic Medicine (CGM) Cross-Phenotype Meta-Analysis Intracerebral 1CGM, MGH, Boston, MA, USA, 2Program in Medical and Population Genetics, Broad Institute, Cambridge, MA, USA and 3Department of Neurology, MGH, Boston, MA, USA Introduction: Intracerebral hemorrhage (ICH) and small vessel ischemic stroke (SVS) are the most severe manifesta- tion of cerebral small vessel disease (CSVD), with high associated morbidity and mortality and no established preventive approaches beyond hypertension management. We hypothesized that simultaneous consideration of these two diseases through cross-phenotype analyses will improve statistical power to detect novel genetic associations across ICH and SVS, elucidating underlying disease mechanisms that may form the basis for future treatments.Methods: We performed three cross-phenotype meta-analyses using association summary statistics from genome-wide association studies (GWAS) for lobar, non-lobar, and all ICH (Woo et al.) and SVS (Malik et al.), which are the largest available genetic studies of these diseases to date. For each cross-phenotype analysis, we applied the validated software tool \"Multi-Trait Analysis of GWAS\" (MTAG), which integrates GWAS summary data across related traits and generates combined trait-specific effect estimates. Analyses were performed for individual single nucleotide polymorphisms (SNPs) and in gene-based testing. Genome-wide significant SNPs arising from MTAG analyses were further examined by cross- referencing with expression quantitative trait locus (eQTL) data. Results: Significant (P < 5x10 -8) associations were observed for the non-lobar ICH model enhanced by SVS with 1) rs2758605 (MTAG P = 2.6x10-11; P in non-lobar ICH = 1.7x10-8 and P in SVS = 9.2x10-5) at 1q22; 2) rs7088080 (MTAG P = 2.3x10-8; P in non-lobar ICH = 3.0x10-3 and P in SVS = 6.7x10-7) at 10p15; and 3) rs9515201 (MTAG P = 5.2x10-10; P in non-lobar ICH = 7.8.0x10-4 and P in SVS = 6.7x10-7) at 13q34 (Figures 1 and 2). The MTAG association strengths of these loci were at least one order of magnitude more significant than those from their individual GWAS. Among these three loci, 10p15 and 13q34 are novel associations for non-lobar ICH risk, while 1q22 had been detected in the previous Woo et al. study. Gene-based testing revealed study-wide significant (P < 2.6x10-6) associations in non-lobar ICH enhanced by SVS with adjacent genes FAM43A (MTAG in thyroid), respectively. Conclusion: Our cross-phenotype study of ICH and SVS reveals CSVD loci, including KLF6, COL4A2, FAM43A and LSG1. We also identified the same PMF1 association as previously seen in ICH, supporting the validity of this approach. 40We have begun to replicate these associations in independent whole genome sequencing sample of ICH as well as across other CSVD traits such as white matter hyperintensity burden. Regional plots of genome-wide significant associations from cross-phenotype meta-analysis of non-lobar ICH and SVS by MTAG Association summary statistics of genome-wide significant associations from cross-phenotype meta-analysis of non-lobar ICH and SVS by MTAG 47 Taylor R. Church, Psychiatry EnBrace HR for Depression Treatment and Prevention in Women Trying to Conceive and Early Pregnancy M.P. Freeman, G.M. Savella, T.R. Church, Noe, A. Kaimal and L.S. Cohen Massachusetts General Hospital, Boston, MA, USA Introduction: Women with major depressive disorder (MDD) often seek to avoid or minimize treatment with antidepressant medications during pregnancy. Therefore, many pregnant women and those planning pregnancy discontinue antidepressants, despite a high risk of relapse. Growing evidence regarding various folate-related agents for the treatment of MDD suggest that folate may be a treatment option that would avoid the potential antenatal risks of antidepressants and confer benefits such as prevention of congenital birth defects. The goal of this study was to obtain preliminary data on whether EnBrace HR, a prescription prenatal dietary product containing methylfolate, appears to have efficacy in 1) depressive relapse prevention for women who discontinue antidepressant medications and 2) treatment for women with acute depressive episodes who opt to avoid starting an antidepressant or increasing the dose of an antidepressant they had been taking.Methods: This was a twelve-week open-label study of EnBrace HR in women with MDD who were pregnant (<28 weeks gestational age) or planning to conceive. Twenty-five women ages 18 years were enrolled in the study and were eligible if they met criteria for one of two groups. Group 1 participants were well at enrollment on antidepressant maintenance medication (n=13) and planned to discontinue antidepressant treatment. Group 2 participants (N=6) were experiencing major depressive episodes at enrollment, and opted to avoid starting an antidepressant medication or increasing the dose of one if they presented on an antidepressant. The presence or absence of a major depressive episode was verified with the Mini International Neuropsychiatric Interview (MINI), and depressive symptoms were measured with the Montgomery-Asberg Depression Rating Scale (MADRS) as the primary depression scale. Participants were evaluated for mood and anxiety symptoms every two weeks for 12 weeks while taking the EnBrace supplement.Results: Group 1 participants (the relapse prevention group) experienced significantly lower rates of depressive relapse (23.1%; p = 0.001) during the 3 months of supplementation and monitoring than would be expected compared to histor- ical rates of relapse in similar groups. As hypothesized, Group 1 participants did not experience significant differences in MADRS score from baseline over the study period. One patient in Group 1 experienced a relapse of MDD that necessitated a psychiatric hospitalization, the only serious adverse event that occurred during the study. Group 2 participants (depressed at baseline) experienced significant decreases in depressive symptoms on the primary depression measure, the MADRS (F(6, 29) = 5.16, p = 0.001). All 6 participants in Group 2 achieved an improvement in MADRS score from baseline to the final visit. Five participants (83.3%) experienced improvements >50% and the sixth participant experienced a 33.3% improvement in MADRS score. Significant improvements in mood symptoms were also detected in Group 2 on the Quick Inventory of Depressive Symptomatology-Self Report (QIDS-SR; F(6,29) = 6.49, p = 0.0002) and Edinburgh Postnatal Depression Scale questionnaires (EPDS; F(6,29) = 4.31, p = 0.003), although the Quality of Life Enjoyment and Satisfaction Questionnaire - Short Form (QLESQ-SF) did not show significant changes in this group. There were no significant differ- ences found in Group 1 on these secondary measures, as expected.Conclusion: In this preliminary study, we assessed EnBrace HR in two samples of women planning pregnancy or during pregnancy, to obtain data regarding: 1) the prevention of depressive relapse in women with histories of MDD, and 2) the acute treatment of depression in women who were depressed and either wanted to avoid the use of an antidepressant or did not want to increase the dose of one that they were already taking. While the numbers of participants were small in each group, these data support further study for these indications. The results of this study suggest that EnBrace is a novel and well-tolerated intervention with potential efficacy for the prevention and treatment of depression among women planning pregnancy and who are pregnant. Therefore, larger controlled trials are necessary to definitively determine efficacy and the role of EnBrace in the armamentarium of treatments for antenatal depression.4148 Kristina M. Cieslak, Psychiatry Integrated Diabetes Management for Individuals with Serious Mental Illness - Results from Year 1 K.M. Cieslak1, C. Cather1, S. Maclaurin2, O. Freudenreich1,2 and A. Evins1 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Erich Lindemann Mental Health Center, Boston, MA, USA and 3Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Premature mortality due to cardiovascular disease in those with schizophrenia is the largest lifespan disparity in the US and is growing; adults in the US with schizophrenia die on average 28 years earlier than those in the general population. An estimated one in five people with severe mental illness (SMI) has diabetes; lifetime rates of diabetes among those with schizophrenia are two to three times higher than for those in the general population. Contributing factors to this astonishingly high rate of diabetes include effects of antipsychotic medication, unhealthy lifestyle, and likely factors related to schizophrenia itself. High rates of tobacco dependence and poor understanding of diabetes management combine to cause to the extraordinarily high morbidity and mortality associated with diabetes in those with SMI. Importantly, though those with SMI may have lower diabetes-specific knowledge, they are receptive to medication, disease self-management strate - gies, and weight loss programs when provided. One large study found greater adherence to oral hypoglycemic medications among diabetes patients with than without comorbid schizophrenia. A 2016 Cochrane review identified only one, negative, randomized control trial published to date evaluating the effect of diabetes education interventions for those with schizo-phrenia, highlighting a significant gap in the literature for theory and evidence-based interventions to improve the ability of those with SMI to manage their diabetes. Methods: We have developed a 16-week tailored behavioral and educational group intervention for individuals with schizo-phrenia and diabetes, utilizing the concept of 'reverse integrated care,' bringing medical intervention into the community mental health setting. Core features of this intervention include motivational interviewing, basic education, and problem-solving. The primary outcome of this study is glycemic control, as measured by hemoglobin A1C (HbA1C). Secondary outcomes include lipid panel, measures of diabetes knowledge and self-management (Brief Adherence Rating Scale Score, Short Diabetes Knowledge Instrument Score, Summary of Diabetes Self-Care Activities Score, Problem Areas in Diabetes Score), blood pressure, weight, BMI, and step count. Topics of the intervention were divided into the below modules: Module A: What does diabetes mean for me? / Blood sugar (2wks) Module B: Diet (3wks) Module C: Exercise/Weight loss (3wks) Module D: Medications, practical aids to medication adherence (2wks) Module E: Stress management with referral to smoking cessation resources for smokers (3wks) Module F: Positive psychology/empowerment (2wks) Review: Review of key concepts and practical applications (1wk)Results: Thirty individuals were consented and randomized to a two-period crossover design consisting of a 16-week group intervention and a 16-week observation period. Average HbA1c at baseline = 7.5, range = 5.9-13.4. Common identi - fied barriers included healthy food being too expensive, not cooking, lack of healthy options eating out, lack of exercise, sugar-containing beverages, and low diabetes knowledge. Seventeen individuals successfully completed the intervention. Pooling data from patients during their 16-week active intervention period, an average 0.59 point reduction in glycosolated hemoglobin was observed from baseline to the end of the 16 week intervention (t=1.99, DF=17, p=0.063). A marginally significant weight reduction was observed from baseline to week 16 in the active condition of 5.3 pounds (t=2.07, DF=17, p=0.054). Ten participants lost greater than five pounds. Significant changes were observed in increased average step count of 3189 steps/day (t=2.25, DF=17, p=0.038), diabetes self-care measures. Promising decreases were seen in systolic blood pressure - those with baseline 130 systolic blood pressure reducing from an average of 138 to 125; diastolic blood pressure - those with baseline 90 reduced from an average of 93 to 80; a 10-point average reduction in total cholesterol (t=-1.13, DF=17, and 50-point average reduction in triglycerides (t=-1.29, DF=17, p=0.21).Conclusion: There is a pressing need to address the morbidity and premature mortality related to modifiable health behaviors in this underserved population, yet individuals with SMI and diabetes are much less likely to be identified or to receive recommended diabetes care and monitoring. Failure to identify and treat diabetes in this population equates to a preventable cost of approximately eight billion dollars to the healthcare system. We hope to further establish and refine a standard of care diabetes education curriculum, tailored for individuals with SMI, a population with high prevalence of diabetes but low rates of diabetes diagnosis, education, and treatment, resulting in disproportionate diabetes-related medical morbidity and premature mortality. Results from year one demonstrate this program to be easily implementable, well-accepted, socially relevant and effective.4249 Jacqueline Clauss, Psychiatry Changes in emotion recognition following the Emotional Leadership Development Group: A pilot study of enhancing resilience and preventing psychopathology in children in Chelsea, Massachusetts. J. Clauss1,2,3, K. Han1,3, Y. Pimental Diaz4,5, A. Lambert5 and D. Holt1,3 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Psychiatry, McLean Hospital, Belmont, MA, USA, 3Harvard Medical School, Boston, MA, USA, 4Wellness Therapist, LLC, Chelsea, MA, USA and 5Massachusetts General Hospital - Chelsea Health Center, Chelsea, MA, USA Introduction: Most psychiatric illnesses have their roots in childhood. To have the most impact on psychiatric disorders, we must find ways to prevent disorders from beginning in the first place. Many risk factors are shared across different types of psychopathology, including exposure to trauma, parent psychopathology, sub-syndromal symptoms, and living in an urban environment. Children from Chelsea, Massachusetts are exposed to many of these risk factors, as well as other factors that increase daily stress levels, such as exposure to violence, uncertain immigration status and poverty. Thus, children living in Chelsea are at elevated risk for developing psychiatric illness. Studying such children can help us to understand how to prevent psychiatric illness and how these illnesses develop. Methods: Children 11-14 years old were recruited from the Massachusetts General Hospital (MGH) Chelsea Health Center. Children were screened for sub-syndromal psychiatric symptoms using the Strengths and Difficulties Questionnaire (SDQ). Sub-syndromal symptoms were measured as symptoms below the threshold for a diagnosis, but above the normal range. Children participated in an intervention that included elements of cognitive behavior therapy (CBT), dialectical behavior therapy (DBT), and mindfulness. The group was comprised of 10 sessions. Topics discussed in the group included identifying your own and others' emotions, kindness and compassion, communication skills, and problem solving. The group interven - tion was led by an experienced community-based therapist and a psychology post-doctoral fellow. Pre-intervention and post- intervention measures were collected. These measures included measures of psychiatric symptoms: the parent-report Children's Behavior Checklist (CBCL), and child self-report Screen for Childhood Anxiety and Related Disorders (SCARED). Child self-report of emotion regulation strategies was also collected, including the Emotion Regulation Questionnaire (ERQ) and the Emotion Regulation Index for Children and Adolescents (ERICA). Finally, a computerized task measuring emotion recognition was collected. The task consisted of the presentation of images of emotional faces (fear, happy, angry, neutral). Faces were morphed to display different degrees of emotion intensity ranging from 50% emotion (e.g., 50% angry and 50% neutral) to 100% emotion (e.g., 100% angry). Children were asked to identify the emotion of each face using a button press. Number of emotions correctly identified, number of faces for which no button was pressed, and reaction times were recorded.Results: Sixty children were screened. Thirty-six children were eligible for the intervention and 11 children were enrolled in the intervention (mean age 12 years). One child dropped out of the intervention, resulting in 10 children completing the intervention. Changes in symptom scores will be reported at a later time point. After the intervention, children were more likely to select the correct emotion for 100% happy faces (t = 2.5; p = .04). There was a trend towards children being more likely to select the correct emotion for 50% angry faces (t = 1.9; p = .10), 75% fearful faces (t = 2.1; p = .07), and 100% fearful faces (t = 1.8, p = .10).Conclusion: Using an emotion recognition task as an objective measure of outcomes, we found that, following the interven - tion, children were better able to identify emotions, particularly fearful faces, which are often ambiguous and difficult to interpret. Thus, the intervention may have led to improvements in facial affect recognition, which may result in positive behavioral change over time. We plan to follow these children longitudinally to determine if these effects are sustained and whether symptom and behavioral improvements occur as any newly learned emotion recognition skills are implemented in the real world. 50 James Clemmons, Emergency Pre-Hospital Stroke Care: A Descriptive Analysis J. Clemmons1, L. Schwamm2 and K.S. Zachrison1 1Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA and 2Neurology, Massachusetts General Hospital, Boston, MA, USA Introduction: With the advent of evidence for mechanical thrombectomy in strokes due to large vessel occlusion, stroke systems of care must be designed to efficiently triage patients in the pre-hospital setting. Stroke severity scales are helpful for pre-hospital providers to assess patients with potential stroke and to guide destination decisions. As of 2018, performance of pre-hospital stroke severity testing is now mandated in Massachusetts.43Methods: We performed a retrospective analysis of stroke patients discharged from the Massachusetts General Hospital between December 2016 to May 2018. Charts were identified using the institutional Get with the Guidelines-Stroke database. Charts were reviewed in EPIC to collect data on location of onset, ambulance arrival, and performance of pre-hospital stroke scale. Chart review was performed by a single investigator (JC), and 32 charts with uncertainty were validated by a second investigator (KSZ).Results: Among 770 encounters reviewed thus far, most strokes occurred at home (69%). There were 395 encounters where patients were transported directly to MGH (51%) while 375 encounters indicated arrival to MGH via transfer from another hospital (49%). Of the 395 non-transferred encounters, 53% (n=208) arrived via EMS. Of these 208 EMS-transported direct arrivals to MGH, 14% of the ambulance records acknowledged performance of a stroke severity scale. None of these encoun-ters included a recorded stroke severity score. It is important to note that the great majority (97%) of these encounters had arrivals in 2017, which was prior to the mandate.Conclusion: In this single institution retrospective analysis, we found that most patients experienced onset of stroke symptoms at home, and that among those transported by EMS, the majority did not have documented stroke severity scale performance. If these findings are confirmed in larger datasets, this highlights an area for improvement in optimization of stroke systems of care. 51 Craig Cochran, Performance Analysis and Improvement/Practice Improvement Direct Scheduling: Does It Contribute to Lower No Shows Rates? C. Cochran, S. Santos, S.D. Aranha and L. Susser MGH, Boston, MA, USA Introduction: In 2017, MGH Primary Care Back Bay began a pilot with MGH/MGPO Ambulatory Management to utilize the direct scheduling functionality in Epic with established patients. This Epic functionality allows patients to schedule themselves for specific types of appointments. Direct Scheduling is now being rolled out extensively. Direct Scheduling provides an established patient the ability to view and book an appointment using Patient Gateway (MyChart). Patients select an appointment based on real-time availability in the provider's schedule. The practice receives an InBasket message when the appointment is booked to ensure that the appointment is appropriate. Patients are able to book only with providers they have seen previously & all appointments are visible to practice staff when booked for modification if needed Direct Scheduling can be customized by provider, visit type, and appointment time.Methods: A practice must be up and operational on a patient portal (Partners Patient Gateway (PPG) here). A practice should have good In Basket management and good pool management techniques in place prior to direct scheduling implementation. A practice should have all providers participate in direct scheduling for best results Schedule templates should be optimized to take full advantage of direct scheduling functionality in Epic. Marketing details include modifying recorded phone messages include mention of direct scheduling option, PPG message to patients in the specific practice with direct scheduling function-ality announcements, updating any practice print materials, as needed, to include direct scheduling information, and updating practice website and Find-a Doc updates to include direct scheduling option. Results: Over the past year, this practice has seen a reduction in no show rates for those patients who schedule themselves fall to 1% from 5% in a specific DEP. Aggregate DEPs show a 3% No Show rate among patients who schedule themselves (N=4431) when compared to the 6% (N=682,051) of practice-scheduled appointments. Over 50% of online bookings completed when practice is closed 2-point increase in CG-CAHP scores for \"Got Urgent Care Appointment\" and \"Got Routine Care Appointment Conclusion: Direct Scheduling is patient -centered, efficient for patients and for practice staff, and contributes to schedule access in a practice through increased arrival rates and decreased no show rates, as shown in the trailing 12-month data. In 2018, many patients expect to be able to schedule appointment themselves at a time that is convenient to them. By taking advantage of underutilized Epic scheduling functionality, direct scheduling can contribute to increased arrival rates and deceased no show rates. We plan to continue to investigate direct scheduling, and other underutilized Epic scheduling functionalities, across the outpatient practice division. IPORT_ All BB DEPS: Trailing 12 months: May 2017 - June 20184452 Yhan E. Colon Iban, BA, Orthopedics The Value of Patient-Reported and Physician Diagnosed Mental Health Status in Predicting Outcomes of Total Hip Arthroplasty Y.E. Malchau1,2 1Harris Orthopaedics Laboratory, Massachusetts General Hospital, Cambridge, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Recent studies have shown that mental health impacts a patient's objective and subjective outcomes after total hip arthroplasty (THA). Mental health disorders may predispose patients to negative objective outcomes and affect a patient's appraisal of pain relief and functional improvement after THA. When assessing the effects of mental health, most studies focus on patient-reported mental health as quantified by aspects of questionnaires such as the anxiety/depression dimension (A/D) of the Euro-QoL 5 Dimension (EQ-5D), as well as the Mental Health Score (MH) and the Mental Component Summary (MCS) of the Short Form 36 (SF-36) rather than on physician diagnosed mental health disorders. The relationship between patient-reported and diagnosed mental health status remains underinvestigated. It is also unclear whether patient-reported or diagnosed mental health status is predictive of THA outcomes. The primary aim of this study was to investigate the association between patient-reported and physician diagnosed mental health disorders in patients undergoing primary THA. Our secondary aim was to assess whether patient-reported or diagnosed mental health is most predictive of patient-reported outcome measures (PROMs) and 90-day readmission after THA. Methods: We queried an institutional database for patients who underwent primary THA at a tertiary academic medical center from 2000 to 2016. Data collected included age at surgery, sex, comorbidities, mental disorder diagnoses, all-cause readmission, and four PROMs: the EQ-5D and SF-36 pre-operatively to collect patient-reported mental health status, as well as the Harris Hip Score (HHS) and a numerical rating scale (NRS) Satisfaction post-operatively to quantify subjective outcomes. Mental health diagnoses were collected and categorized based on the International Classification of Diseases 10 th (ICD-10) codes. Disorders identified in at least 500 patients were included in the analyses, creating four categories: any mental health disorder (N=1439), mental and behavioral disorders due to psychoactive substance use (N=581), mood affective disorders (including depression) (N=799), and neurotic stress-related and somatoform disorders (including anxiety) (N=808). Patients who had completed PROMs were included in the final cohorts. We used receiver operator characteristic (ROC) analysis with mental health diagnoses as the state variable and the mental health dimensions of collected PROMs as the test variables. The area under the curve (AUC) was used to assess the strength of the association between patient-reported and diagnosed mental health status. HHS was dichotomized per literature defining patient acceptable symptom state (PASS) at 75 points. NRS Satisfaction was dichotomized such that a score of 2 was classified as satisfied and > 2 as dissatisfied. We assessed three different outcomes by binary logistic regression analysis: 90-day readmission, HHS PASS, and satisfac-tion. For each outcome, we built seven models: four with each mental health disorder diagnosis introduced individually as a potential predictor, and three with each patient-reported mental health status component. A total of 21 models were created. Age, sex, and the Charlson Comorbidity Index (CCI) were controlled for in each model. Results: Within the study period, 4407 primary unilateral THA operations were conducted at our institution. Preoperatively, 1732 (39.30%) and 424 (9.62%) patients completed the EQ-5D and the SF-36 respectively. Of the 1732 patients with a preoperative EQ-5D, 861 (49.71%) also had a postoperative HHS, while 882 (50.92%) also had NRS Satisfaction. Of the 424 patients with preoperative SF-36, 200 (47.17%) also had a postoperative HHS, while (46.93%) also had NRS Satisfac - tion. Each patient-reported mental health measure was strongly associated with each of the four physician-reported mental health diagnoses (P<0.002) (Table 1). All four mental health disorder diagnoses were associated with higher probability of 90-day readmission and of not achieving the HHS PASS compared to patients without diagnoses (P<0.043). All mental health disorder diagnoses except mental and behavioral disorders due to psychoactive substance use (P=0.088) were predictive of not achieving satisfaction (P<0.049). The EQ-5D Anxiety/Depression score was the only PROM predictive of an outcome, specifically patients not attaining HHS PASS (P<0.001; Odds Ratio (OR) = 1.9, P=0.007) (Table 2). Conclusion: Despite the association of patient-reported and physician diagnosed mental health, diagnosed mental disorders are better at predicting both objective and subjective outcomes. This is consequential as most studies use patient-reported mental health when adjusting for mental health issues. Future studies should consider mental health diagnoses when performing risk adjustment for comparative effectiveness research and reimbursement models.45Table 1. Associations between physician diagnosed mental health disorders and patient-reported mental health tested via receiver operator characteristic analysis. Table 2. Results of binary logistic regressions evaluating the ability of physician diagnosed mental health disorders and patient-reported mental health in predicting THA outcomes. 53 Drew Coman, Ph.D., Psychiatry Childhood Trauma as a Predictor of Psychotic-like Experiences In Healthy Young Adults D. Coman1, O. Terechina2, A. Farabaugh1, M. Fava1 and D. Holt1 MGH, Boston, MA, USA and 2Berkshire Health Systems, Pittsfield, MA, USA Introduction: Childhood trauma is associated with far-reaching and detrimental effects on the development and health of exposed individuals. Early traumatic experiences, particularly those that are interpersonal, intentional and chronic, confer significant risk for the onset of complex neurodevelopmental abnormalities along with severe mental illness. This includes, but is not limited to, impaired cognitive functions, poor language and academic achievement, anxiety, depression, posttrau- matic stress, posttraumatic stress disorder (PTSD), psychosis, and schizophrenia (De Bellis & Zisk, 2014, Grubaugh et al., 2011). Indeed, the extant literature supports that over 49% of individuals with a psychotic disorder have experienced trauma, over 75% report multiple exposures to traumatic experiences, and 12.5% display symptoms that meet full diagnostic criteria for PTSD (Achim et al., 2011; Grubaugh et al., 2011; Keen, Hunter, Peters, 2017). Recent work has also proposed specific mechanisms for these associations, such as linkages between the re-experiencing symptoms of PTSD and hallucinations as well as connections between interpersonal traumas and paranoia (Bentall, Wickham, Shevlin, Varese, 2012; Gracie et al., 2007; Hardy al., 2016; Keen, Hunter, Peters, 2017, Lovatt, Mason, Brett, Peters, 2010). While the relationship between trauma exposure and risk for psychotic illness has become clearer, historically this association has been investigated in individuals who have been managing a psychotic condition for an extended period. Moreover, from a clinical standpoint, it is common for psychotic experiences themselves to be either traumatic in nature and/or cause stress, or lead to stressful life events, that in turn can exacerbate symptoms of psychosis. Individual differences, such as resilience levels, can mediate and moderate these relationships as well. Thus, these previously reported associations between trauma and psychosis are ultimately confounded by these factors. Fortunately, novel approaches, such as examining the spectrum of subclinical expres- sions of psychosis in non-clinical populations, and advanced conceptualizations, such as understanding this disease on a dimensional continuum, are now being employed to investigate the mechanisms of psychotic disorders (Calkins et al., 2014). Such approaches allows for a clearer understanding of how certain risk factors, such as childhood trauma, may result in the onset of psychotic experiences and, in some cases, progress into a full syndrome of psychotic illness. Thus, in an effort to further understand the associations previously observed between clinical psychosis and a history of early trauma, we examined the relationships between childhood trauma and psychotic symptoms in individuals without a formal psychotic illness. We hypothesized that, while controlling for age, recent stressful life events, resilience, and current perceived stress, higher levels of childhood trauma would be associated with an increased number of delusional experiences, namely paranoia, as well as increased hallucinations. Methods: Participants were 370 young adults (M age = 19.56; SD = 1.30; 28.4% male) that were recruited at four Boston- area universities. Subjects were asked to complete a demographic form, the Childhood Trauma Questionnaire (CTQ), the Perceived Stress Scale (PSS), the Stressful Events Checklist (SEC), the Connor-Davidson Resilience Scale, the Peters et al. Delusions Inventory (PDI), and the Launay-Slade Hallucination Scale (LSHS). Statistical analyses included descriptive statistics and hierarchical regression modeling. Results: In this non-clinical sample of young adults, participants reported a range (minimum = 0; maximum = 21) of PDI total scores with M = 5.29, SD = 3.52. LSHS scores ranged from 0 to 63 with M = 11.33, SD = 10.49. Controlling for age, current perceived stress, recent stressful life events, and self-reported resilience, levels of childhood trauma experiences were positively associated with delusions, R 2 = .01, F(5, 399) = 18.31, p < .001, = .11, p = .035. Additionally, childhood trauma 46was significantly associated with hallucinations, R2 = .02, F(5, 395) = 32.31, p < .001, = .16, p = .001 while controlling for these same confounds. Conclusion: Beyond the impact of recent life stressors, an individual's resilience level, and age, these results demonstrate that early life trauma increases risk for psychotic experiences in a non-clinical population. Notably, these data suggest that early experiences of trauma can have significant and adverse impacts on individuals' mental health, even in individuals who are currently enrolled in college, with relatively normal levels of day-to-day functioning. These findings support the continuation of efforts aimed at prevention of early life trauma, as well as the importance of targeting the effects of traumatic experiences in psychosocial interventions designed for individuals who are at risk for or are experiencing psychotic illness. 54 Caroline Copacino, BS, Anesthesia, Critical Care and Pain Medicine Prospective Study to Evaluate the Clinical Utility of Perioperative Pharmacogenomic Testing C. Copacino1,2, X. Bao1, and S. Nedeljkovic2 1Department of Anesthesia, Critical Care, and Pain Medicine, Massachusetts General Hospital, Boston, MA, USA and 2Department of Anesthesia, Perioperative and Pain Management, Brigham and Women's Hospital, Boston, MA, USA Introduction: Advances in genotyping have improved the understanding of the relationship between genotype and the efficacy and toxicity of specific drugs in surgical and non-surgical settings. Pharmacogenomic testing (PGx) provides the ability to predict the response of genes involved in the metabolism of common perioperative and postoperative drugs such as oxycodone, codeine, morphine, fentanyl, and tramadol. PGx helps to identify individuals at a higher risk for an adverse event associated with a specific medication preceding the drug's administration to reduce harm and increase overall benefit; however, the clinical implications of personalized testing through PGx have not yet been adequately assessed. It is necessary to investigate the role of PGx in clinical decision-making, and the effects of this information on perioperative analgesic dosing and surgery outcomes. This research is being conducted to determine: 1) the utility of genotypic profiles using periopera - tive PGx in the care of patients; and 2) the effect of PGx on post-surgical outcomes. PGx testing will provide information regarding drug-drug interactions and analgesic metabolism. Measured outcomes will include the percentage of anesthesiolo - gists who find PGx testing to be clinically useful and overall clinical outcomes, evaluated through opioid consumption, pain assessments, length of hospital stay, patient satisfaction, and additional, unscheduled patient visits and calls. We hypothesize that using PGx testing perioperatively to understand the genotypic drug profile of a patient will be useful to anesthesiologists in medication dosing for surgery, to reduce adverse reactions to drugs, and in improving clinical outcomes.Methods: For this study, 1000 adults planning to undergo elective surgery will be enrolled. Study staff will prescreen surgeons' schedules at both Brigham and Women's Hospital and Massachusetts General Hospital and approach potentially eligible patients undergoing inpatient surgery a minimum of four days before the date of their surgery. After the study is explained, the subject's eligibility is confirmed, and their consent is obtained, research staff will record medical and surgical history, demographic information, and past medication use. Buccal swabs will be collect and shipped to an outside laboratory for PGx analysis. PGx will provide results of the patient's genotypic profile compiled from analyzing a minimum of 42 genes involved in drug metabolism. On the day of surgery, the research team will present the PGx results to the anesthesiologist attending to the patient prior to the surgery. After the procedure, the anesthesiologist will be asked to complete a survey of the clinical utility of the PGx information. Twenty-four hours after surgery, we will report intraoperative and postoperative use of opioids and other medications, surgical and anesthesia duration times, length of stay on recovery floor, pain level, nausea and vomiting, sedation and delirium, subject satisfaction, and quality of recovery. The last assessment will take place 30 days after surgery in which electronic medical records will be reviewed for length of stay in hospital, number of prescribed opioids, hospital readmissions, emergency room visits, and unscheduled phone calls. Results: Data will be analyzed using propensity score matching to compare outcomes of subjects whose PGx information was reported to be clinically useful to subjects whose PGx information was not deemed useful. The propensity scores will be calculated based on subject characteristics and treatment interventions to create a baseline for comparing the groups. This will enable analysis of the potential benefits of PGx in a perioperative setting. Conclusion: It is expected that the data collected from this research will confirm that there is substantial genetic variability between patients in drug metabolism. We intend to use PGx testing to confirm the association of certain gene mutations with drug metabolism and to inform both patients and clinicians how the genetic profile may affect response to certain medications commonly used in the perioperative period. We hypothesize that increased knowledge of this information will improve clinical outcomes when used perioperatively and reduce the incidence of adverse drug-drug interactions and other negative outcomes.4755 Elizabeth V. Cory, B.S., Psychiatry Mood states as predictors of risk-taking behavior on the Balloon Analogue Massachusetts General Hospital, Cambridge, MA, USA, 2T.H. Chan School of Public Health, Harvard University, Cambridge, MA, USA and 3Harvard Medical School, Cambridge, MA, USA Introduction: Appropriately perceiving and responding to risks is crucial for adaptive decision-making. Past research suggests that individuals with mood disorders, such as bipolar disorder, are often predisposed to making maladaptive, risky choices, at times resulting in harmful behavior. In line with previous research seeking to characterize specific mood-re- lated predictors of risky behavior, this study examines the relationship between various self-reported mood states and risky behaviors on a cognitive risk-taking task among a broad online sample of people with mood disorders.Methods: We recruited participants from MoodNetwork, an online platform run through the National Patient-Centered Clinical Research Network (PCORnet) that is designed for patients and families of those with mood disorders. PCORnet is a part of the Patient-Centered Outcomes Research Institute (PCORI), which allows patients to actively participate in every aspect of the research process. Through MoodNetwork, consenting participants can complete a variety of mood assessments and cognitive behavioral tasks at any time. For the current analyses, we examined participants' mood states in relation to their performance on the Balloon Analogue Risk Task (BART), a validated laboratory measure of real-world risk-taking behavior. Specifically, we examined whether each participant's self-reported mood via the Quick Inventory of Depressive Symptoms - Self Report (QIDS-SR) and Depression and Bipolar Support Alliance Wellness Tracker (DBSA) predicted three indices of risk-taking: frequency of balloon explosions, number of risky balloon pumps, and reaction time (RT). Our sample consisted of 65 participants who completed the QIDS-SR (female = 60, mean age = 45.7, SD = 9.6) and 189 participants who filled out the DBSA (female = 172, other = 1, mean age = 43.4, SD = 10.3).Results: Linear regression analyses revealed no significant relationship between total QIDS-SR score and our three indices of risky behavior (explosion frequency: adjusted R2 = -0.0058, F(64) = 0.63, p = 0.43; pumps: adjusted 0.00026, F(64) = 0.32; RT: adjusted R2 = -0.0069, F(64) = 0.56, p = 0.46). We also found that wellness ratings on the DBSA did not significantly predict risk-taking behavior on the BART (explosion frequency: adjusted R2 = 0.010, F(188) = 3.0, p = 0.085; pumps: adjusted R2 = -0.0053, F(188) = 0.0015, p = 0.97; RT: adjusted R2 = 0.00073, F(188) = 1.14, p = 0.29). The results did reveal that participants responded significantly faster on trials when the balloon exploded than on trials when they cashed-out before explosion (t(2752) = 9.0, p < 0.001).Conclusion: Our null findings are congruous with past research yielding non-significant BART performance differences among participants with bipolar disorder (Holmes et. al, 2009). However, these preliminary findings warrant follow-up analyses with a larger, more diverse sample of participants and deeper investigation into the influence of impulsivity and self-control on risk-taking. Given the broad range of mood states bipolar individuals experience, continuing this understudied line of research is particularly important for better understanding how risky behaviors shift with mood. 56 Avilash Cramer, MS, Radiology Multi-source Static Gupta2 1HST, MIT, Cambridge, MA, USA, 2Radiology, MGH, Boston, MA, and 3CIMIT, Boston, MA, USA Introduction: Computed tomography (CT) is used as a first line three-dimensional imaging modality to diagnose a variety of emergent conditions. Modern CT scanners employ an x-ray source mounted on a rotating gantry. As the gantry revolves around the patient, the system acquires x-ray projections from multiple angles that are processed to generate a volumetric image. Such an arrangement cannot be easily operated in a vehicle or spacecraft because it has significant net angular momentum. In addition, the rotating gantry mechanism also adds considerable inertial mass, size, and power requirements to a CT system, reducing its ability to be used in other resource-constrained environments. This lack of portability not only makes advanced healthcare delivery in rural communities challenging, it affects pre-hospital care in essentially all demographics. A light-weight, motion-free, modular tomographic imaging system could address some of these issues. Non-rotating CT systems using carbon nanotube field emission sources have been proposed, but not yet widely adopted. We describe a nonrotating CT system composed of multiple photocathode-driven x-ray modules that can be assembled into a ring or other geometry for tomographic imaging48Methods: We created a novel type of x-ray tube that can be turned on or off rapidly in a programmable fashion for distributed generation of X-rays from multiple points without any mechanical motion. The x-ray elements in our system are miniature x-ray sources that are triggered by ultraviolet light. In each x-ray source, a small number of electrons are generated by photo-emission when illuminated. We use a UV LED as the trigger for photo-electron generation from a magnesium photoca- thode. The light input is provided by a 255 nm UV LEDs via quartz windows from outside the vacuum housing of the X-ray source module. Metallic photocathodes produce too weak a photocurrent to be directly used for human-scale CT, which requires hundreds of milli-amperes of x-ray tube current. To amplify the tube current, in each x-ray element we deposit thin film of magnesium on the active input surface of a Channeltron electron multiplier (Magnum 5900) with a bias voltage of 3000-4000V. The Channeltron amplifies the current produced by the photocathode on its input surface by a factor of 10^8 or more to provide approximately 1mA tube current emanating from its output. We constructed a photocathode by depositing a thin layer of magnesium on the active input surface of the Channeltron.Results: We constructed a tomosynthesis module consisting of an arc of seven of these photocathode-based x-ray elements inside of a vacuum manifold (Fig 1). The module spans 17.5 degrees of arc radius 227.5 mm. The x-ray sources are digitally controlled by the LED illumination using a microcontroller mounted on top of the x-ray source module. A beryllium window seals in the vacuum manifold. Beryllium was chosen for its low atomic number (z = 4), and relative stability in atmosphere. A vacuum flange on the rear of the module allows for the housing to be connected to a turbopump. Individual source elements produce up to 1 mA of tube current. This tube current can be increased by increasing the gain of the Channeltron, through its bias voltage or temperature. Our custom-built controller board allows us to control the frequency, duty cycle, and luminance of the pulsed UV LED illumination of each x-ray element in order to achieve a uniform x-ray output from each source. Using a clinical flat-panel detector, we acquired projection images of a catheter being inserted into the bronchioles of a pig lung (Fig 2A). We simulated a full 360 degree ring of x-ray sources using a rotation stage for the specimen. For each position of the rotation stage, seven images, one from each x-ray element, were acquired. We implemented a Filtered Back Projec- tion (FBP) reconstruction algorithm to create a 3D volumetric images from a series of projection images (Fig 2B and 2C). The imaging shown here was performed at a tube voltage of 35 kVp..Conclusion: Our prototype demonstrates several specific advantages that could enable CT imaging in spacecraft and other austere environments. Timing of the UV LEDs rather than current modulation, controls the x-ray flux. The system has no moving parts and each module weighs approximately 1 kilogram. Individual CT modules could be stored and transported in a compact storage box and then assembled in space or in the field for operation. When not in use, the CT system can be disassembled and stored away. Modules can be brought up to atmospheric pressure for repairs and replacement of individual sources. In acquiring tomographic images with a series of miniaturized, photocathode-based sources, we have demonstrated a novel method of volumetric x-ray imaging, and a fundamental re-imagining of medical x-ray production for tomographic acquisition. We have also demonstrated the utility of a metallic photocathode as the electron generation method for x-ray imaging 57 Thomas F. Cunningham, Bachelor of Science, Medicine - Cardiology Iron Deficiency and Elevated Hepcidin Predict Abnormal Pulmonary Vascular Responses to Exercise in Patients with Preserved Ejection Fraction Undergoing Evaluation of Dyspnea T.F. Cunningham, Wooster, J. Rouvina, C. White, H.L. Barnes, J. Ho, G. Lewis and R. Malhotra Cardiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Hepcidin is a master regulator of iron homeostasis that reduces iron absorption when circulating levels are elevated. In preclinical models, iron deficiency and inappropriately elevated hepcidin levels have been implicated in pulmonary arterial hypertension. However, the role of hepcidin dysregulation on pulmonary vascular function in patients with dyspnea on exertion (DOE) and preserved ejection fraction remains unclear. We hypothesized that a low ratio of transferrin saturation to hepcidin is associated with abnormal pulmonary vascular responses during exercise.49Methods: Cardiopulmonary exercise testing (CPET) using cycle ergometry with invasive hemodynamic monitoring was performed in 127 patients with preserved ejection fraction (LVEF 50%) undergoing evaluation for dyspnea. Serum hepcidin and transferrin saturation (Tsat) were measured at the time of CPET and the ratio (Tsat/Hepcidin) was used as a measure of iron homeostasis and hepcidin dysregulation, with low values reflecting inappropriate elevation in hepcidin level relative to iron bioavailability.Results: Patients had the following characteristics (mean inversely correlated with pulmonary vascular resistance (PVR) at rest (r = -0.25, p=0.005) and peak exercise (r = -0.27, p=0.002, A-B). Lower Tsat/Hepcidin ratio was also associated with lower peak VO 2, lower cardiac index, higher PA pressures, and higher PAP / CO slope (C), but did not associate with exercise pulmonary arterial wedge pressure (PAWP; r = -0.13, p = 0.13). Conclusion: Hepcidin dysregulation, reflected by low Tsat/Hepcidin ratio, is more closely related to pulmonary vascular dysfunction than left ventricular dysfunction during exercise in patients being evaluated for dyspnea on exertion. 58 Adam G. Dalia, MD, MBA, Anesthesia, Critical Care and Pain Medicine Impella Placement Guided by Echocardiography Can be Used as a Strategy to Unload the Left Ventricle During Peripheral Venoarterial ECMO A.G. Dalia, A. Fiedler, M. Villavicencio, D. D'Alessandro Cudemus stabilizes patients in cardiogenic While VA ECMO therapy improves hemodynamics, an increased afterload and lack of ventricular ejection causes left ventricular distension (LV). VA-ECMO alone cannot decompress the distended LV. We present our experience utilizing the Impella-2.5 to vent the LV during VA-ECMO. Methods: Patients cannulated for VA ECMO with placement of an Impella as an LV vent were identified through our institutional ECMO database from 2015 to May 2017. We collected demographic, survival, clinical, echocardiographic, and operative details. Results: Fifty-nine patients were placed on VA-ECMO from 2015 to January 2017. Twelve patients had concurrent placement of an Impella-2.5 Impella-2.5 was most common (n= 8), followed by Impella-CP (n = 4). In-hospital survival was 58% (n = 7), compared to an institutional survival average for VA-ECMO patients of 42.6%. Survivors were less likely to have presented with STEMI and to have undergone PCI. Survivors were less likely to have experienced hemolysis as a complication as measured by LDH. Survivors were bridged to recovery (n=5), heart transplantation (n=1), and durable LVAD (n=1). A trend towards improvement in LV function after ECMO and Impella therapy was noted on echocardiography in patients who survived. Pre-cannulation LV function on echocardiography did not appear to play a role in survival.50Conclusion: Impella with VA-ECMO permits LV unloading and a greater than 50% hospital survival. We observed greater survival advantage in patients with myocarditis or mechanical complications of STEMI. Impella patients who died tended to have echocardiographic evidence of LV dilation during VA ECMO/Impella therapy; while survivors had a reduction in cavity size and improved systolic function after device removal. 59 Hassan S. Dashti, PhD RD, Center for Genomic Medicine (CGM) Decreased glucose tolerance in night versus day oral glucose tolerance test: preliminary results from the SHIFT Study 1Center for Genomic Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Department of Integrative Physiology, University of Colorado, Boulder, Boulder, CO, USA, 3Department of Physiology, University of Murcia, Murcia, Spain and 4Division of Sleep Medicine, Harvard Medical School, Boston, MA, USA Introduction: Melatonin secretion during the biological night impairs glucose tolerance and insulin secretion. Small studies in healthy volunteers show that exogenous melatonin administered during the day acutely decreases glucose tolerance in healthy participants and other small crossover studies (n =10) show that oral glucose tolerance is reduced during the night (8pm) versus the day (8am). We test the hypothesis that an oral glucose tolerance test (OGTT) performed during the biological night, when circulating melatonin levels are elevated, will reveal impaired glucose tolerance and delayed insulin secretion compared to an OGTT performed during the biological day when clinical glucose tolerance tests are typically performed in the Shift work, Heredity, Insulin, and Food Time (SHIFT) Study (Study #NCT02997319), a multicenter, large, observational study. Methods: Here, we describe preliminary results from the first 87 non-night workers with two 2-hour 75-g oral glucose tolerance tests: day (3 hours after self-reported habitual free day wake time) and night (1 hour before self-reported habitual free day bedtime). Blood glucose, insulin, and melatonin were determined. Incremental area under the curve (AUC) for increases in glucose during the OGTT was calculated by the trapezoidal method.Results: First, we confirmed higher endogenous melatonin in the night than in the day OGTT (P <0.001). Compared to the day OGTT (start time ranging from 6.30am to 4pm), during the night OGTT (start time ranging from 6.30pm to 1am), we observed 81% higher postprandial glucose AUC first 30 minutes) was 23% lower in the night OGTT (36.7 \u00b1 22.3 uIU/mL vs. night, 29.4 \u00b1 22.0 uIU/mL; P =0.003), but insulin at 120 minutes was significantly higher in the night OGTT (38.4 \u00b1 35.5 P Conclusion: These observations support the hypothesis that an OGTT performed during the biological night reveals impaired glucose tolerance and delayed insulin secretion compared to an OGTT performed during the biological day, suggesting that the timing of an OGTT may likely influence results and possibly diabetes diagnosis.5160 Brett J. Davis, B.A., Psychiatry Bioclimatic predictors of depressive symptom severity and suicidality B.J. Nierenberg1,3 1Dauten Family Center for Bipolar Treatment Innovation, Massachusetts General Hospital, Boston, MA, USA, 2T.H. Chan School of Public Health, Harvard University, Cambridge, MA, USA and 3Department of Psychiatry, Harvard Medical School, Cambridge, MA, USA Introduction: Evidence suggests that mood symptoms are sensitive to light and ambient temperature, but it remains unclear whether these effects create observable geographical patterns in symptom severity. Methods: Based on mood and demographic assessments from members of MoodNetwork, a large, online community of people with mood disorders, we conducted simple and multiple regressions to identify whether affective symptoms correlated with bioclimatic data for each U.S. state. Depressive symptoms were measured through total scores on the Quick Inventory of Depressive Symptoms - Self Report (QIDS-SR), suicidality was measured through the suicidality item on QIDS-SR, and bioclimatic data (average number of sunny days, average maximum temperature, average minimum temperature, and average maximum heat index of each U.S. state) were collected from a federal climate database. Results: Analyses included mood and demographic assessments from 579 members of MoodNetwork (82.72% female; 87.21% white; mean age 41.47, SD 13.77). Amount of sunlight correlated inversely with severity of suicidality (adjusted R-squared=0.007515, F(1, 577)=5.376, p=0.021). A multiple regression analysis revealed that a model with sunlight and temperature variability significantly predicted depressive symptoms (adjusted R-squared=0.007515, F(2, 566)=5.376, p=0.044), where temperature variability correlated significantly with symptoms (t=2.063, p=0.040), but sunlight did not (t=-0.613, p=0.5401). No other associations between depressive scores or suicidal ideation and other bioclimatic variables were significant (p>0.05).Conclusion: These findings support that living in a state with more sunny days is associated with less severe suicidality among people with mood disorders. Additionally, after accounting for sunny days, greater temperature variability predicts worse depressive symptoms among people with mood disorders. It is possible that sunlight and temperature have effects on mood by impacting biological rhythms. 61 Kathryn A. Davis, M.A., Center for Genomic Medicine (CGM) Sensitive periods across development: prospective assessment of childhood adversity exposures and depressive symptoms in late adolescence K.A. Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Applied Statistics Group, University of the West of England, Bristol, United Kingdom, 3Erasmus Medical Center, Rotterdam, Netherlands, 4Department of Epidemiology, Mailman School of Public Health, Columbia University, New York, NY, USA, 5New York State Psychiatric Institute, New York, NY, USA, 6Department of Psychiatry, Harvard Medical School, Boston, MA, USA and 7Stanley Center for Psychiatric Research, Broad Institute of MIT and Harvard, Cambridge, MA, USA Introduction: Although exposure to environmental stressors are known determinants of poor mental health, a major challenge in psychiatric studies relates to modeling the effect of environmental exposures on subsequent disease risk, particularly when these exposures are assessed repeatedly across time. Innovative life course modeling approaches now allow researchers to test the effects of a given repeated exposure--as well as the effect of the timing and duration of that exposure--on mental health outcomes. However, few practical applications of these approaches exist, and among these even fewer have studied repeated exposures across a long developmental time frame, such as birth to late adolescence. The current study aimed to address these gaps by focusing on the role of two commonly-assessed environmental exposures on subsequent risk for depression in late adolescence. We were particularly interested in understanding whether there were \"sensitive periods\" in childhood and adolescence when exposure to these adversities conferred a particularly high risk for depression.Methods: Prospective data came from a two-decade-long birth cohort called the Avon Longitudinal Study of Parents and Children (ALSPAC); these analyses were based on an analytic sample comprising n=3,263 children. Two common risk factors for depression--physical or emotional abuse by a caregiver (characterized by physical or verbal cruelty) and financial stress (characterized by the inability to afford basic goods or services)--were each measured via parental report on at least 7 occasions between birth and age 18. Using a machine-learning approach grounded in least angle regression, we first evaluated if there was a time-period of exposure that explained the most variability (r 2) in depressive symptoms measured at age 18.5 52using the Short Moods and Feelings Questionnaire, and determined the corresponding magnitude of those associations. We then explored whether a competing life course theoretical model, specifically an \"accumulation of risk\" model, explained more of the observed variation in depressive symptoms than any single sensitive period. Results: We identified three sensitive periods: the first was in very early childhood (spanning 8 months to 2.75 years of age) for the effect of financial stress in girls (r2=0.45%), the second was in middle childhood (spanning 7 to 9 years of age) for the effect of caregiver physical or emotional abuse in boys (r2=0.81%), and the third was in early adolescence (11 years of age) for the effect of caregiver physical or emotional abuse in girls (r2=0.20%). Substantial effect sizes were observed (standardized mean differences: 1.62 for financial stress in girls; 2.55 abuse in boys; 2.14 for caregiver physical/emotional abuse in girls). For boys, the effects of this sensitive period persisted even after accounting for the accumulation of exposure (r2=0.71%). Conclusion: These findings suggest that to better understand the link between environmental exposures and psychiatric disease risk we may need to move beyond comparisons of those \"exposed\" vs. \"unexposed\" and begin to account for the timing of adversity exposure. Novel life course modeling approaches can help identify sensitive periods in development, which could guide intervention efforts in the prevention and treatment of psychiatric illness. 62 Reinier J. de Vries, Center for Engineering in Medicine (CEM) Supercooling of Human Livers to Extend the Preservation Time for Transplantation R.J. de Uygun1 1Surgery, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA and 2Surgery, University of Amsterdam, Amsterdam, Netherlands Introduction: Optimizing preservation of donor organs has the potential to dramatically improve the outcome of organ transplantation by diminishing the donor organ shortage, enabling near perfect global HLA matching, permitting recipient immune tolerance induction, and allow transplantation in an elective surgical setting. For livers, the current preservation standard, hypothermic preservation at +5\u00b0C (hP) allows for a maximum preservation time of 12 hours. Recently, it has been shown that rat livers can be preserved in a supercooled state at -6\u00b0C for 3 days with 100% survival after transplantation. The main goal of this study is to translate supercooling preservation to human livers.Methods: Five human livers, rejected for transplantation, were procured in standard fashion and transported to our hospital under conventional HP conditions. The livers were recovered from HP and the baseline viability was measured during 3 hours of sub-normothermic machine perfusion (SNMP). Next, the livers were preconditioned with cryoprotectant (CPAs) 3-O-methylglucose (3OMG), polyethylene glycol (PEG), glycerol and trehalose to prevent freezing and hypothemic injury during subsequent supercooled storage at -4\u00b0C for 20 hours. After supercooling the CPAs were washed out and the livers were recovered during 3 hours of SNMP. Viability parameters during SNMP were compared to their baseline values. SNMP conditions before and after supercooling were identical to allow direct comparison of pre- and post-supercooling viability parameters. Three livers were additionally reperfused at 37\u00b0C with non-leukoreduced red blood cells and fresh frozen plasma to simulate transplantation. Viability parameters included vascular resistance, oxygen uptake, lactate clearance, electrolytes, liver enzymes and bile production which were measured every 30 minutes during SNMP and blood reperfusion. Bilateral wedge biopsies for the measurement of adenylate energy charge and assessment of histology were also sampled throughout the protocol.Results: Adenylate energy charge, considered the most important marker for transplant success, was the same before and after supercooling (p=0.398). Also, we observed no difference arterial resistance (p=0.809), portal resistance (0.537), oxygen consumption (p=0.272) and bile production (p=0.433) comparing pre- and post-supercooling SNMP values. The livers cleared lactate before and after supercooling and lactate levels were significantly lower during the first hour of SNMP after supercooling as compared to the pre-supercooling values (p=0.040). Markers of (hepato)cellular injury such as AST, ALT and potassium release in the perfusate were identical during pre- and post-supercooling SNMP (p=0.919, p=0.243, p=0.455, respectively). Similarly, histology shows mild reversible signs of injury after supercooling with no difference in the percentage of apoptotic cells (p=0.115). During normothermic blood reperfusion the adenylate energy charge, vascular resistances and markers of (hepato)cellular injury were stable and bile production, lactate clearance and oxygen consumption increased. Histology shows focal signs of irreversible hepatocellular injury an increase in apoptotic cells (p=0.045) as is normally the case during transplantation and similar to values reported in literature.53Conclusion: In this study, we successfully supercooled human livers and for the first time demonstrated the feasibility of the subzero human organ preservation. In the case of preclinical human tissue studies, ex vivo viability assessment during machine perfusion has strong theoretical background and is supported by experimental and clinical transplantation studies: these all indicate that supercooled human grafts retained their viability despite significantly extended preservation as compared to the clinical standard. Moreover, we measured viable parameters during simulated transplantation of marginal livers up to 44 hours after procurement. 63 Taquesha Dean, Bachelor of Arts, Psychiatry Illness Beliefs in Depressed Chinese Americans in Primary Care T. Dean1, R. Norton1, T. Phan4, A. Yeung1,2,3 and C. Borba5,2 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Behavioral Health, South Cove Community Health Center, Boston, MA, USA, 4Clinical Psychology, William James College, Newton, MA, USA and 5Psychiatry, Boston Medical Center, Boston, MA, USA Introduction: The Surgeon General's report (2001) on culture, race, and ethnicity reported that Asian Americans often underuse mental health services. For depression, specifically, Yeung, Chang, Greshman, Nierenberg, and Fava (2004) found that when Chinese American patients were seeking help from mental health specialists, they oftentimes underreported their mood symptoms and were generally unfamiliar with treatment options for their depression. In this poster, we aim to expand upon previous findings by examining the chief complaints of Asian American patients seen at a Behavioral Health Clinic in Boston, MA over the span of 3 years. This was done in order to understand how Asian Americans define their own mental illness in the context of Primary Care settings.Methods: The data used for this sample was collected from 2013-2015 at South Cove Community Health Center through English and Chinese-translated intake forms. These forms asked each patient \"Why are you here today? What problems are you experiencing?\" and provided an open-response area for patients to write in their chief complaints. The complaints were then qualitatively coded by the research staff to find the most common chief complaints. We characterized the sample by separating responses into four categories that were based on the patient's clinical diagnoses: All Disorders, Mood Disorders, Anxiety, and Mood Disorders + Anxiety. We then reported on the top five complaints listed within each diagnostic group. We will also examine the relationship between the psychologization of the complaints with various demographic characteristics, including gender, years since immigration, and highest education level through linear and logistic regression.Results: Of the total sample (n=304), 19% complained of having insomnia, 15% had anxiety or fear, 13% experienced depression, sadness, or low affect (DSL), 12% had decreased cognition, and 8% reported having somatic symptoms. Of those who were diagnosed with mood disorders (n=182), 24% reported insomnia, 18% DSL, 13% anxiety and fear, and 10% for both somatic symptoms and decreased cognition. For those diagnosed with anxiety (n=25), 44% complained of having anxiety or fear, 24% had insomnia, 20% had DSL, 12% had no complaints but mentioned that they were referred, and 8% had psychotic symptoms. Finally, within the combined group of mood disorders and anxiety (n=207), 24% of patients had insomnia, 18% had DSL, 16% had anxiety and fear, 10% somatic symptoms, and 8% had decreased cognition. Conclusion: Upon reflection, the most common complaints were shown to be insomnia, depression/sadness/low affect, anxiety or fear, and somatic symptoms. Additionally, many patients reported symptoms that were not directly aligned with their diagnosis, causing a wide spread of complaints, which highlights a potential underreporting of mood symptoms. This may relate to previous findings, which show that Asian Americans are more likely to underreport their mood and over-report somatic symptoms. At the same time, however, only a small portion of patients reported somatic symptoms, which therefore contrasts with the previous research. As a result, we hope that this information will drive further analysis; looking specifi - cally into the levels of psychologization of the chief complaints and determining a more comprehensive profile of depression symptoms in Asian American populations. Top Chief Complaints (All Diagnoses) * Total percentages do not equal 100% because participants were coded multiple times if they disclosed multiple complaints. ** Other category includes comments that did not directly answer the presented questions of \"Why are you here today? What problems are you experiencing?\"5464 Daniela Del Campo, OB/GYN Barriers to Influenza Vaccination in Pregnancy During the 2017-2018 Flu Season D. Del Campo, L.E. Riley and I.T. Goldfarb OB/GYN, Massachusetts General Hospital, Boston, MA, USA Introduction: The influenza virus places pregnant women at high risk for morbidity and mortality. In order to minimize these risks, the Centers for Disease Control and Prevention (CDC) as well as the American College of Obstetricians and Gynecologists recommends that all pregnant women receive the influenza vaccine regardless of trimester and that Tamiflu be administered early for treatment of influenza-like illness or as prophylaxis after exposure. Despite these recommendations, influenza vaccine uptake among pregnant women has remained low across the country at a rate of 36% (CDC November 2017). In the early stage of the current flu season, a CDC advisory was released regarding a possible link between influenza vaccination and miscarriage, however this warning was subsequently re-evaluated. The effect of this report and its subsequent media coverage on pregnant women's attitudes toward vaccination is unknown. We sought to understand barriers and predic - tors to flu vaccine acceptance and Tamiflu use among pregnant women during the 2017-2018 flu season.Methods: An anonymous questionnaire was distributed to all post-partum patients at MGH from June 2018-July 2018 in both English and Spanish. Data obtained included demographics, pregnancy and vaccine history as well as attitudes toward the flu vaccine. REDCap was used for database creation and STATA was used for analysis. Continuous variables were compared using a student's T-test and categorical variables were compared with chi-squared with a p-value of < 0.05 representing statistical significance. Results: The study recruited 301 participants with a response rate of 83%. The study population had an average age of 32 with 70% self-reported as Caucasian, and 16% Hispanic. Eighty-two percent of women reported to accept the flu vaccine during pregnancy. The influenza vaccine was significantly more likely to be accepted among women who reported that their health care provider offered the vaccination. (84% vs 31%, p-value < 0.001). Additionally, those who reported any prior flu vaccina - tion were much more likely to accept vaccination during pregnancy, (84% vs. 56%, p-value = 0.001). Statistically significant disparities in flu vaccine uptake were noted with lower acceptance rates found among women of lower educational status and those attending certain community health centers for prenatal care. In this study population, race and ethnicity were not identified as significant predictors of vaccine uptake. Despite a high overall acceptance rate among study participants, those who declined the flu vaccine most commonly selected, \"I am concerned the vaccine is not safe for my baby\" and \"I never get the flu shot\" as factors affecting decision making while concerns regarding miscarriage remained very low. Notably, only 7% of women reported delaying vaccination from the first trimester. This population of pregnant women reported concerns about the flu; 34% experienced influenza-like illness during pregnancy, and 24% were exposed to the flu, but out of those, only 13% and 15%, respectively, took Tamiflu.Conclusion: Vaccines during pregnancy offer lifesaving primary prevention against influenza to a vulnerable popula- tion. Clinician recommendations has previously been demonstrated as a significant predictor and was again noted to drive influenza vaccine uptake in our patient population while media reports of miscarriage risk was noted to be an inconsequential barrier to vaccination. While a high vaccination rate was identified in our patient population overall, education level and prenatal care site were identified as sources of disparity. These findings need further investigation so that future influenza vaccine campaigns can reach all of our patients. Medical literacy must be considered in our messaging around important public health interventions so that we may provide equitable care across all patients. Additionally, though our population was largely accepting of the vaccination program, patients with symptoms of or exposure to influenza did not often receive Tamiflu treatment according to CDC guidelines. Clinician and patient education will need to be geared toward the benefits of early treatment of pregnant women with influenza-like illness. 65 Gabriel A. del Carmen, Surgery Does the Day of the Week Predict a Cesarean Section?: A Statewide Analysis G.A. del Carmen1, S. Stapleton1, and M.G. del Carmen2 1Codman Center for Clinical Effectiveness in Surgery, Massachusetts General Hospital, Waltham, MA, USA and 2Division of Surgical Oncology, MGH Cancer Center, Massachusetts General Hospital, Boston, MA, USA Introduction: While guidelines for clinical indications of Cesarean sections (CS) exist, non-clinical factors may affect CS practices. We hypothesize that CS rates vary by day of the week.55Methods: An analysis of the Office of Statewide Health Planning and Development database for California from 2006-2010 was performed. All female patients admitted to a hospital for attempted vaginal deliveries were included. Patients who died within 24 hours of admission were also excluded. Weekend days were defined as Saturday and Sunday; weekdays were defined as Monday through Friday. The primary outcome was rates of CS relative to vaginal delivery. Multivariate regression was performed, adjusting for patient demographic, clinical, and system factors.Results: 1,855,675 women were analyzed. Overall CS rates were 9.02%. On unadjusted analysis, CS rates were significantly lower on weekends than on weekdays (6.65% vs. 9.58%, p<0.001). On adjusted analysis, women were 27% less likely to have a CS on weekends than on weekdays (OR: 0.73, 95% CI 0.71-0.75, p<0.001). In addition, Hispanic ethnicity and delivery in a teaching hospital were also associated with a decreased likelihood of CS (OR 0.91, 95% CI 0.86-0.96, p=0.01; OR 0.80, 95% CI 0.69-0.93, p <0.001, respectively).Conclusion: CS rates are significantly decreased on weekends relative to weekdays, even when controlling patient, hospital, and system factors. Further exploration of this novel finding is warranted and will lead to improved quality of care for patients. 66 Wisteria Deng, BA, Psychiatry Sustained Effects of Resilience Training in At-Risk College Students W. Deng1,2, A.S. Burke1,3, Hospital, Boston, MA, USA, 2Athinoula A. Martinos Center for Biomedical Imaging, Boston, MA, USA, 3Psychiatry, Harvard Medical School, Boston, MA, USA and 4Behavior Medicine, Boston University, Boston, MA, USA Introduction: Symptoms of psychosis and other mental illnesses typically begin during late adolescence and early adulthood, including the college years. Many types of stress-coping and self-care curricula have been previously developed for the college population, such as mindfulness-based training. However, there is little empirical evidence from a longitudinal perspective that supports the efficacy of such treatments in the long run. Given this, we have designed and tested a 4-session resilience-boosting curriculum for at-risk college students with subthreshold symptoms of psychopathology, specifically subclinical psychosis-like symptoms and depression. Following the intervention, we observed a significant reduction in subthreshold symptoms and a significant increase in resilience-promoting factors. Longitudinal data are now being collected every six months over the four-year follow-up period, in order to examine longer term outcomes for the group participants, as well as the sustainability of the immediate post-intervention effects.Methods: On-campus mental health screenings were held at a local university. Young adults (n = 416) attending the univer-sity completed self-report symptom measures. Students endorsing elevated depressive and/or psychosis-like symptoms, but not receiving current mental health treatment, were eligible to participate in the resilience training intervention. The group intervention contains elements of three evidence-based approaches: mindfulness, mentalization-based treatment, and self-compassion. Self-report symptom questionnaires and other measures about mental well-being and resilience factors were collected before and after the intervention, as well as after a six-month interval. Participants who have both completed the six-month follow-up assessments and were involved in more than two group sessions were considered \"group-completers\" (n = 36). Results: Group-completers showed significant increases in measures of self-compassion and general self-efficacy, as well as significant decreases in symptoms of subthreshold psychosis, paranoia, and social anhedonia (all ps < 0.05). The increase in resilience-related factors and the decrease in symptoms were maintained from the post-intervention time point to six 56months after group completion. There was no significant change in any measures from post-group assessment to six-month follow-up (all ps > 0.05). Conclusion: Our findings suggest that a brief resilience training intervention delivered on the college campus has sustained positive effects, particular with respect to increasing resilience-related factors and reducing symptoms related to psychosis proneness. The results indicate that resilience training in at-risk young adults can produce a rapid, measurable change in both protective and risk-related factors, potentially contributing to mitigating long-term risk for the onset of serious psycho-pathology in this population. 67 Danielle Detelich, MD, Surgery - Transplant Normothermic Machine Perfusion of Human Kidneys Without Use of An Oxygen Carrier D. Detelich1,2, C. Eymard1,2, C. Carroll1,2, M. Aburawi1,2, K. Uygun1,2, H. Yeh1,2 and J. Markmann1,2 1Transplant Surgery, Massachusetts General Hospital, Boston, MA, USA and 2Center for Engineering in Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: The increasing demand for kidney transplantation has led to a greater reliance on extended criteria donors. These grafts have a greater risk of delayed graft function or non-function and are frequently discarded. Normothermic machine perfusion (NMP) supports full metabolism of the organ ex-vivo, offering a promising solution for pre-transplant reconditioning and viability assessment. Due to the high metabolic activity, all clinical kidney NMP protocols employ an oxygen carrier in the perfusate, typically packed red blood cells (RBC). This introduces additional logistical hurdles and potential immunologic risk for the recipient. The aim of this feasibility study is to evaluate whether an oxygenated, cell culture medium-based perfusate with no added oxygen carrier is sufficient to sustain the ex-vivo function and viability of human kidney grafts, compared to a RBC-based perfusate.Methods: 12 human kidneys (3 pairs, 6 singlets), declined for transplantation by all regional transplant centers and consented for research use by the New England Organ Bank (NEOB) were included. The kidneys were perfused for 6 hours at 37\u00b0C using the Liver Assist pump (Organ Assist, Groningen, The Netherlands). The Liver Assist is a pressure-controlled, commer- cially-available ex-vivo organ perfusion device that we modified for kidney perfusion. The kidneys were transported to the lab on a portable hypothermic machine perfusion pump or in a sterile ice box. The kidneys were kept on ice during back bench cannulation of the renal artery and flushed to remove the preservation solution. Kidneys were perfused utilizing a RBC-based perfusate gas mixture of 95% O 2/5% CO2 was administered to achieve a perfusate oxygen tension of 450-700 mmHg throughout perfusion. Real-time evaluation parameters included renal artery flow, vascular resistance, perfusate blood gas and chemistry analysis and oxygen delivery, extraction and consumption. Kidney function was assessed by total urine production and decrease in perfusate creatinine. Hourly tissue biopsies were collected for histologic assessment and mass spectrometric quantification of ATP as a marker of energy restoration.Results: Hemodynamic parameters were superior in the WE-group. The WE-group vascular resistance declined steadily throughout perfusion whereas the RBC-group demonstrated an initial spike in resistance after commencing perfusion, peaked at one hour, and subsequently declined for the remainder of perfusion, resulting in a similar mean resistance to the WE-group (p=0.0591). Mean weight-adjusted renal artery flow was higher in the WE-group throughout perfusion (p=0.0002). As expected, O 2 delivery was higher in the RBC-group (p=<0.0001) and the O2 extraction ratio (ER) was higher in WE-group (p=0.0014), with a maximum mean ER in the WE-group of 36.5%. O2 consumption was similar between groups (p=0.1661), indicating that sufficient dissolved O2 is present in the perfusate and there is adequate functional capacity of the aerobic metabolic machinery in the kidney during perfusion (Figure 1). ATP content was similar between groups (p=0.1304). Energy charge (EC), which is a more complete reflection of the energy status in the tissue, was also similar between groups (p=0.8785), indicating that the kidneys demonstrate a similar overall energy state during perfusion (Figure 2). A trend towards greater decrease in creatinine, expressed as percentage of the baseline value, was observed in WE-kidneys without reaching statistical significance (p=0.4506). Cumulative urine output was also greater in the WE-group without reaching significance (p=0.2773). Histologically, kidneys in both groups maintained tubular architecture without evidence of edema. Conclusion: Ex-vivo NMP human kidneys is an innovative technology for pre-transplant graft conditioning and viability assessment and is particularly suited for evaluation of marginal kidney grafts. However, the requirement for an oxygen carrier adds complexity to the procedure and exposes the recipient to potential infectious and immunologic risk. Our study demonstrated that oxygenated perfusion without the use of additional oxygen carrier is feasible for up to 6 hours, resulting in stable hemodynamics adequate oxygen delivery to support aerobic metabolism and cellular function, and similar ATP restoration compared to the current standard perfusion protocol. Based on these results, we posit that kidney NMP does not require use of a RBC-based perfusate and confirmation of these results in a human pilot clinical trial is warranted.57 Figure 1: Oxygen parameters. Top left: oxygen delivery. Top right: oxygen extraction. Bottom: oxygen consumption. Data expressed as mean \u00b1 SEM. Figure 2: Energy analysis. Top left: ATP concentration. Top right: ATP:AMP ratio. Bottom left: ATP:ADP ratio. Bottom right: energy charge. Data expressed as mean \u00b1 SEM. 68 Connor Devoe, Dermatology A Pilot Study Evaluating the Usability of a Mobile Application for Overactive Bladder Disease Management C. Devoe1, S. Bane1, J. Hirschey1, Palacholla1,2,3, A. K. Jethwani1,2,3 and J. Kvedar1,2,3 1Connected Health Innovation, Partners Healthcare, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Massachusetts General Hospital, Boston, MA, USA Introduction: Overactive bladder(OAB), defined by urinary urgency with or without urge urinary incontinence(UI), usually with frequency and nocturia, can significantly impact participant's quality of life. Tracking symptoms is an important part of OAB management and has been shown to assist in enhancing participant interaction with health care providers(HCP) when discussing solutions for symptom management. The primary goal of this study was to assess the usability and acceptability of an Android smartphone mobile application (app) designed to help participants learn about OAB symptom management through tracking and self-management. Secondarily, we also assessed engagement with the app over the three-month study period.Methods: Eligible participants were experiencing OAB symptoms without an existing enlarged prostate or urinary tract infection(BPH/UTI), and enrolled through referrals from within the Partners Healthcare network. The mobile app was installed at the enrollment visit, and participants were instructed to complete monthly, 3-day symptom journals, as well as surveys and optional free-text notes for 12 weeks. Additionally, medication reminders, Kegel and bladder training exercises were available for use in the app. A visit with their HCP was scheduled between weeks 6 and 12 of the study for the HCP and participant to review collected symptom data via an app-linked portal. Qualitative input from the HCP, closeout partic - ipant interviews and app usage data (% viewed and number of hits) were used to assess participant engagement. Closeout interviews(n=10) also assessed usability of the various app features. Demographic and usability satisfaction data were collected via questionnaires developed by investigators. Descriptive analyses were conducted to present the demographic and usability data. NVivo for Mac (version 11) was used to conduct a thematic analysis on qualitative data. Results: Of the total enrolled(n=33), 26 participants completed the study. Participant engagement with the app was 100% for months one and two of the study then dropped to 72% by month three. Most participants (80%) reported using the app as needed vs. regularly. As a group, female participants >50 years demonstrated the highest engagement(75%) at closeout. The most used app feature was the free-text diary feature (100%; 5516 hits), followed by the \"event log\" (100%; 2105 hits). The majority of other app features were also rated as useful by participants(52-100%). Participant interviews found the app was a valuable OAB information source, simplifying symptom tracking and follow-through on clinician recommendations. Perceived usefulness of the portal varied between primary care providers and specialists. Participants indicated the app was \"Easy to Learn\"(96%), \"Simple to Use\"(92%), useful for understanding changes in symptoms(91%), enabled better symptom tracking (96%), and facilitated communication with their HCP(75%).Conclusion: A mobile app to increase awareness of OAB symptoms improved confidence in self-management for partici - pants and increased access to data for decision making and participant communication for specialists. Participant-reported outcomes indicate that the tracking void frequency and urgency features were very useful, while other features such as medication reminders, pad usage, bladder and Kegel trainings were used less frequently among participants.5869 Jacob R. Dewitz, Neuroscience, B.A., Neurology Retrospective Chart Review of Patients with ALS referred for Shoulder Injections K. Burke2, A.S. Elrodt2, F. and S. Paganoni2,3,4 1Sports Medicine Service, Massachusetts General Hospital, Boston, MA, USA, 2Neurological Clinical Research Institute (NCRI), Massachusetts General Hospital, Boston, MA, USA, 3Spaulding Rehabilitation Hospital, Boston, MA, USA and 4VA Boston Healthcare System, Boston, MA, USA Introduction: Shoulder pain is a common secondary complication of amyotrophic lateral sclerosis (ALS) that can contribute to functional decline and decreased participation in daily activities. Adhesive capsulitis has been reported in 18% of individ - uals with ALS and shoulder pain. When stretching and range of motion exercises are not sufficient to relieve the symptoms, individuals may be referred for intra-articular glenohumeral injections. Methods: This was a retrospective chart review of six individuals who were referred from the ALS multidisciplinary clinic for intra-articular glenohumeral joint injections. All available data related to their shoulder pain, range of motion, and functional limitations related to the shoulder were collected pre and post injections. The solution injected and complications were recorded. Results: In this review of six women with ALS who were referred to physical medicine and rehabilitation for shoulder injection from January 2017 through April 2018, all of them are status post ultrasound guided glenohumeral joint injection with a solution of 4mLs of 1% Lidocaine and 1mL of 40 mg/mL of Kenalog for treatment of adhesive capsulitis. For each, the joint capsule was noted to distend with the fluid injected. All patients reported pain and demonstrated impaired range of motion at the glenohumeral joint prior to injection. They all reported improvements in their pain post injection. One individual was referred for a second injection due to ongoing shoulder pain. There were no functional outcome measures documented for any of the patients pre or post injection. There were no range of motion measurements documented in the follow-up appointments. All six patients reported improvement in their pain after the shoulder injection.Conclusion: For individuals with ALS and shoulder pain with range of motion limitations and pain consistent with adhesive capsulitis, ultrasound guided glenohumeral joint injections may be offered for pain relief. Future studies should focus on functional limitations, in addition to pain relief, pre and post injections. 70 Bram R. Diamond, B.Sc., Athinoula A. Martinos Center for Biomedical Imaging Optimizing the Accuracy of Cortical Volumetric Analysis in Patients with Traumatic Brain A. Martinos Center for Biomedical Imaging, Department of Radiology, Massachusetts General Hospital, Boston, MA, USA, 2Center for Neurotechnology and Neurorecovery, Department of Neurology, Massachusetts General Hospital, Boston, MA, USA, 3Department of Radiology, Icahn School of Medicine at Mount Sinai, New York, NY, USA, 4Department of Neurological Surgery, University of Washington School of Medicine, Seattle, WA, USA, 5Department of Rehabilitation Medicine, Icahn School of Medicine at Mount Sinai, New York, NY, USA and 6Department of Neurology, Icahn School of Medicine at Mount Sinai, New York, NY, USA Introduction: Cortical volumetric analysis using FreeSurfer is widely used to study the neuroanatomic basis of a broad range of neuropsychiatric diseases. However, the application of this technique to patients with focal cortical lesions has been limited because magnetic resonance imaging (MRI) abnormalities tend to compromise the accuracy of the cortical surfaces that are reconstructed and used by FreeSurfer to generate volumetric measurements. Development of a tool that accounts for cortical lesions in volumetric analysis would create new opportunities to elucidate the neuroanatomic basis of diseases associated with focal brain lesions, such as traumatic brain injury (TBI). Here, we propose a novel FreeSurfer-based lesion correction method and test its impact on cortical volumetric measures in patients with TBI. Methods: We prospectively enrolled 20 patients with a history of TBI at an academic medical center. Patients were included if they had sustained a moderate-to-severe TBI (Glasgow Coma Scale [GCS] score 3-12) or a complicated mild TBI (GCS score 13-15 with abnormal brain imaging) at least one year prior to enrollment. Patients underwent standardized MRI using a T1-weighted multi-echo MPRAGE (MEMPRAGE) sequence at 1 mm spatial resolution. We focused on measurements of cortical gray-matter (GM) volume within an atlas-based functional limbic network, because this fronto-temporal network is often affected by contusive lesions in patients with TBI. We first processed all MEMPRAGE data using the standard FreeSurfer pipeline (version 6.0) for cortical surface reconstruction and GM volume estimation. Using FreeSurfer-based 59tools, we registered a 7-Network resting-state functional connectivity atlas (Yeo et al., 2011) onto each subject's reconstructed brain surface (Figure, row 4) and then isolated the limbic network. For the pre-correction analysis, we measured the total GM volume of the limbic network registered onto the surfaces produced by FreeSurfer's standard pipeline. Next, to correct for lesion-induced surface inaccuracies and ensure precise anatomic localization, we parcellated each hemisphere surface into 642-faced (each ~100 mm 2) polyhedra generated by 3rd-order super-sampled icosahedra (Figure, row 3). Then, all lesions were visually identified (Figure, row 1) and sites of lesion overlap with the cortical surface were manually labeled on the icosahedron to produce lesion labels. If the limbic network appeared anatomically inaccurate at a site of lesion overlap (Figure, row 2), we removed the intersecting sub-region from the network. For the post-correction analysis, we reran FreeSurfer's tools to measure the total GM volume based on the updated limbic network. Finally, we calculated the difference in limbic network cortical volume in the pre- versus post-correction analyses to assess the statistical impact of our method.Results: The 20-subject cohort was comprised of 15 men, with an average +/- SD age of 65.2 +/- 5.9 years. The mean +/- SD duration from TBI to MRI was 8.5 years +/- 4.4. Fourteen of 20 TBI patients (70%) had lesions that involved the fronto- temporal regions of the limbic network. Of the patients with limbic lesions, all but one had cortical surfaces that appeared anatomically inaccurate in the pre-correction analysis. Of these thirteen patients, the mean +/- SD limbic network cortical volume measures in the pre- versus post-correction analyses were +/- 4,340.3 mm 6,937.1 mm3, respectively. Our proposed lesion a 7,030.9 +/- 4,4317.2 change In TBI patients with cortical lesions, our proposed lesion correction method led to substantial changes in the FreeSurfer-derived cortical volumetric measurements. This lesion correction method has the potential to increase the accuracy of cortical volumetric measurements in patients with TBI and other neuropsychiatric diseases associated with focal lesions. Validation of the technique by comparison of pre- and post-correction cortical volumetric measures with cognitive-behavioral symptoms is now underway. Figure: Lesion-Correction Method for a Representative Subject Row 1: Axial, coronal, and sagittal views of the subject's anatomical T1 images were used to visually determine the location of a TBI-induced cortical lesion in the left temporal pole (red arrows). Row 2: Surfaces near the lesion are inaccurate and pass through the damaged tissue. Row 3: We registered a 642-faced polyhedron onto each hemisphere to allow for precise lesion localization. Row 4: Lesion labels were used to update the limbic network (cream color) of the 7-Network Yeo resting-state functional connectivity atlas and ensure accurate surface measurements.6071 Laura E. Dichtel, MD,MHS, Medicine - Endocrine-Neuroendocrine The Neuroactive Steroid Allopregnanolone Across the Menstrual Cycle and in Menopause Dichtel1, Nyer2, Boston, MA, USA, 2Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 3The Psychiatric Institute, Department of Psychiatry, University of Illinois at Chicago, Chicago, IL, USA, 4National Center for PTSD, Department of Veterans Affairs, VA Boston Healthcare System, and Department of Psychiatry, Boston University School of Medicine, Boston, MA, USA and 5Butler Hospital, Brown Department of Psychiatry and Human Behavior, Providence, RI, USA Introduction: The neuroactive steroid 3 -5-tetrahydroprogesterone (allopregnanolone is a positive allosteric modulator of GABA-A receptors, and low levels have been implicated in mood disorders, including depression, anxiety, and post-traumatic stress disorder. However, it is not known whether such GABAergic neuroactive steroids vary across the menstrual cycle or are low in postmenopausal women. Given the high prevalence of adverse luteal phase mood disorders, we hypothesized that serum allopreg/PROG decreases in the luteal phase. We also hypothesized that healthy postmenopausal women have lower levels of allopregnanolone than healthy premeno - pausal women in the follicular phase, given lower levels of its substrate, PROG. Methods: 13 healthy premenopausal women were studied fasting at 0800 hours in the follicular (d 1-7) and luteal (d 21-23) phases of the menstrual cycle. No subject was receiving estrogen. Ovulatory function was confirmed by luteal serum PROG >5,000 pg/mL. An additional 26 healthy in the follicular phase and 28 non-depressed postmenopausal women were studied. Serum neuroactive steroids were measured by gas chromatography/mass spectrometry. Data were reported as mean \u00b1SD. Results: In the menstrual cycle cohort, mean age and BMI were 36\u00b17 y and 26\u00b14 kg/m2. Allopreg and PROG allopreg/PROG ratio decreased significantly through the menstrual cycle, with a nearly 8-fold decrease from the follicular to luteal phase vs 0.04\u00b10.02 BMI higher the postmenopausal (POST) women vs follic - ular phase premenopausal (PRE) (age 62\u00b1 7 vs 9 vs 24\u00b1 4 kg/m2, p<0.02). Allopreg was significantly lower in 14.6 vs 113.2\u00b1 145.9 pg/mL, p=NS). The allopreg/PROG POST vs PRE (0.46\u00b10.47 vs 2.05\u00b12.53, p<0.008).Conclusion: Although allopreg rose significantly in concert with its substrate, PROG, through the menstrual cycle in premenopausal women, the allopreg/PROG ratio decreased 8-fold between the follicular and luteal phase. This may be due to reduced metabolism of PROG to allopreg and could have implications for luteal phase and pre-menstrual-associated mood changes or even luteal phase disorders such as catamenial epilspey. Additionally, non-depressed postmenopausal women have lower allopreg levels but similar allopreg/PROG ratios vs healthy premenopausal women in the follicular phase of the menstrual cycle. Whether these data have implications for mood disorders merits further study. Figure 1: (A) Allopregnanolone, (B) Progesterone and (C) Allo/Prog ratio by menstrual cycle phase. *p<0.005 and **p<0.0001. Figure 2: (A) Allopregnanolone, (B) Progesterone and (C) Allo/Prog ratio by menopausal status. Note: Premenopausal levels were all measured in the follicular phase. *p<0.0001 and **p<0.008.6172 Irene Dimitriadis, MD, OB/GYN Automated smartphone-based system for measuring sperm viability, DNA fragmentation, and hyaluronic binding assay score. Shafiee2 1Ob/Gyn, Division of Reproductive Endocrinology and Infertility, Massachusetts General Hospital, Boston, MA, USA and 2Department of Medicine, Brigham and Women's Hospital, Boston, MA, USA Introduction: A low-cost, automated, smartphone-based semen analyzer can shift the paradigm in male infertility manage - ment at the point of care. We have developed a smartphone-based system for at-home male infertility screening through automatic and rapid measurement of sperm concentration and motility. Here, we evaluated the feasibility of using a similar smartphone-based system for measuring: a) Hyaluronan Binding Assay (HBA) score, which determines the functionality and maturity of sperm in a fresh semen sample by providing a quantitative score for evaluation, b) sperm viability, which assesses sperm membrane integrity, and c) sperm DNA fragmentation using the Halosperm G2 assay\u00ae. (Figure)Methods: Design: Prospective proof of concept study Interventions : We sought to perform the following semen analysis tests using the smartphone-based optical system: a) Automated HBA analysis: unprocessed semen samples from 40 patients were loaded into the microfluidic chips after sample liquefaction and were incubated for 10 minutes. The hardware includes a smartphone attachment and a disposable microfluidic device for semen sample handling. The software is a smartphone application that enables rapid on-phone image processing using recorded videos taken by the optical attachment and cellphone camera. The microfluidic device was designed such that half of the sample reservoir was coated with hyaluronic acid (HA). b) Sperm viability testing: a routine dye-exclusion technique using eosin and nigrosin cell staining was utilized to assess sperm viability distinguishing viable from non-viable sperm from 102 semen samples. c) DNA fragmentation analysis: the Halosperm G2 assay for sperm DNA fragmentation was performed on a total of 47 fresh semen samples obtained from men undergoing routine fertility testing. Blinded manual quantification of sperm DNA fragmentation was performed using a conventional microscope and the smartphone-based semen analyzer. All three sperm tests were measured with the smartphone system and their results were compared to those obtained through traditional manual analysis. Setting: Academic fertility center Outcome Measures : a) HBA score, b) Viability percentage, and c) DNA fragmentation results Statistics: Agreement between the results obtained from the cell-phone device and the manual analysis was evaluated using root of squares linear correlation method for both the HBA and the sperm viability analysis. The DNA fragmentation results were evaluated by comparing the smartphone results with the manual microscopic-based results using Bland-Altman analysis. Results: Excellent agreements were observed between conventional HBA measurements and smartphone-based measurements. There was a very high linear agreement between the smartphone-based system and manual analysis for the HBA score with an R2 value of 0.82 with a regression slope and intercept of 0.92 and 1.6425, respectively. The accuracy of the device in classifying semen samples based on HBA score threshold of 80 was 95%. Similarly, the sperm viability and DNA fragmentation tests were also shown to be compatible with the smartphone-based system when tested with 102 and 47 semen samples, respectively. Conclusion: We report the development and evaluation of an automated, simple, and portable smartphone-based system that can rapidly analyze an unwashed semen sample and identify abnormal samples with accuracies similar to traditional manual methods by assessing not only sperm motility and concentration but also sperm viability, HBA score, and DNA fragmentation. 6273 Kathleen L. Donahue, Neurology Aphasia at Presentation Predicts Poor Functional Outcome After Acute Ischemic Stroke K.L. Donahue1, A. Giese1,2, M. Etherton1, O. Wu1,3 and N. Rost1 1Neurology, Massachusetts General Hospital, Boston, MA, USA, 2Program in Medical and Population Genetics, Broad Institute of MIT and Harvard, Cambridge, MA, USA and 3Athinoula A. Martinos Center for Biomedical Imaging, Department of Radiology, Massachusetts General Hospital, Charlestown, MA, USA Introduction: Symptom severity at stroke onset is known to predict long-term outcomes; however, it's unclear if specific functional domains affected by ischemia may determine the extent of recovery after AIS. Aphasia is an important cause of post-stroke disability, yet the effect of acute aphasia on long-term recovery after AIS is poorly understood. Methods: We retrospectively analyzed a cohort of 103 consecutive patients from the Genes Affecting Stroke Risk and Outcomes Study (GASROS), 44 with aphasia as a presenting symptom of AIS and 59 AIS patients as non-aphasia controls. Basic demographic data, vascular risk factors, stroke severity (National Institutes of Health Stroke Scale [NIHSS]) at admission and 90-day outcome (modified Rankin Scale [mRS]) were obtained. Strokes were subtyped using the Causative Classification of Strokes (CCS). We obtained clinical MRIs performed within 48 hours of symptom onset for all subjects. White matter hyperintensity (WMH) lesions were manually outlined and quantified for all subjects using FLAIR imaging. WMH lesion incidence maps were produced using MRICron's nonparametric mapping software.Results: The AIS patients presenting with aphasia (n=44, 42.7%) were well matched to nonaphasia controls by age (mean: 66.6 \u00b116.5 vs. 66.8 \u00b115.7 years), sex (45.5% vs. 45.8% female), and race (95.5% vs. 98.3% Caucasian). In univariate analysis, aphasia cases had higher NIHSS scores on presentation (median NIHSS [interquartile range (IQR) 4-18] vs. 5 fibrillation 13.6%, p=0.05), compared to non-aphasia controls. CCS subtypes differed significantly (p=0.03), predominantly driven by distribution of CE (40.9% vs. 22%) and SAO (2.3% vs. 20.3%) strokes between cases and controls. In ordinal regression analysis, aphasia at stroke onset predicted higher mRS score (OR 2.63, 95% CI, 1.32-5.26) at 90 days. Incidence maps of WMH lesion burden showed no significantly different patterns between cases and controls. Lesion patterns were also not significantly different in aphasic patients who completely recovered (mRS=0) compared to those who did not completely recover (mRS1).Conclusion: In AIS, aphasia symptoms at presentation increase the odds of poor long-term functional outcomes. Aphasia at stroke presentation varies significantly by stroke subtype and is significantly associated with higher NIHSS and 90-day mRS scores. While mechanisms of post-stroke recovery are not well understood, future studies investigating the role of specific functional domains may inform outcome prediction models and facilitate targeted rehabilitation strategies. 74 Karen Donelan, ScD, EdM, Medicine - Mongan Institute for Health Policy Are Primary Care and Geriatric Clinicians Providing Care at Home to Frail Older Adults? K. Donelan1,2, C. Michael1, Y. Chang3, D. Auerbach7 and R. Dittus8 1Mongan Institute Health Policy Center, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Division of General Internal Medicine, Boston, MA, USA, 4Harvard Medical School, Beth Israel Deaconess Medical Center, Boston, MA, USA, 5Open Notes, Boston, MA, USA, 6Massachusetts General Hospital, Boston, MA, USA, 7Center for Interdisciplinary Health Workforce Studies, Bozeman, MT, USA and 8Vanderbilt University Medical School, Nashville, TN, USA Introduction: As the population ages, there is increasing interest in home based health care for people with serious illness. Recent studies emphasize future technologies and services that will bring more intensive care to the home. The past decade has seen increasing reports of home hospitals, urgent care, telehealth, and mobile health, yet current national estimates of clinician provision of these services are needed. This paper uses a 2018 survey of primary care and geriatric clinicians about the care of frail elderly patients in the community to explore factors that predict the provision of in person and virtual care in the home to frail older adults.Methods: These analyses use data from a national survey of Primary Care and Geriatric Clinicians being conducted by our team during March-July 2018; 415 primary care and geriatric physicians and nurse practitioners in the US completed the survey, with an oversample of 217 in California. Clinicians were selected from practices stratifying by practice organization (MD without NP or PA, MD with NP, NP without MD or PA) to assure inclusion of differently staffed practices. The survey was developed through 22 site visits and 20 focus groups in 5 US regions in 2017. Key analyses will include several predictors of three outcomes 1) \"Our practice is designed to provide primary care to frail older adults in their homes\" 2) a composite variable of use of virtual care technologies and 3) ratio of clinician time spent providing home-based vs. office based services. Predictors include professional type, patient panel, payer mix, practice staff size and mix, specialty and demographics.63Results: Preliminary data show that 57% of clinicians disagree (28% strongly) that their practice is designed to provide primary care to frail older adults in their homes. 30% agree (11% strongly). The proportion of practices provide the following technology-based services: mobile health (10%), remote monitoring (11%), telehealth/telemedicine visits (16%), portal to electronic health record (68%). 31% say they provide a clinic or visits at assisted living facilities. 85% of clinicians spend 20 or more hours in a typical week doing office-based evaluation and management of patients; 80% of clinicians spend 0 hours on home visits (no difference NP v MD); 5% spend 20 hours or more on home visits. Multivariate analyses explore whether providers of different training (primary care, geriatric), or practices with different staffing, % of frail patients, or payer mix, are more likely to provide care at home. Conclusion: While there is considerable interest in increasing the provision of primary and geriatric care in the home, home visits still represent a small share of work hours for most clinicians who provide them and the majority disagree that their practice is designed for this purpose. Virtual technologies are also still rarely used. We hope pending analyses of these data will illuminate factors in staffing and payer mix that increase the likelihood of provision of these services. 75 Michelle Dossett, MD, PhD, MPH, Psychiatry - Benson Henry Institute for Mind Body Medicine Stress Management and Resiliency Training for Healthcare Professionals M. Dossett1 and D. Mehta1,2 1Massachusetts General Hospital, Boston, MA, USA and 2Brigham and Women's Hospital, Boston, MA, USA Introduction: Burnout is epidemic among healthcare professionals. Prior studies have suggested that resiliency training programs incorporating mind-body skills may reduce provider burnout. We examined the effects of a stress management and resiliency training (SMART) program developed for clinical populations and adapted it to healthcare professionals. Methods: We conducted a pre-post pilot study of physicians and nurse practitioners in the Department of Medicine at Massachusetts General Hospital who volunteered to participate in an 8-session SMART Program. Fifty-nine clinicians enrolled in the program and completed pre-program measures which included the Perceived Stress Scale-10 (PSS-10), global physical and mental health and job satisfaction questionnaires. Thirty-six clinicians completed post-program measures which included the pre-program questions, plus additional likert scales and free-text response questions regarding the relevance of the program to their life and work, the helpfulness of the skills taught, and how the program affected them personally and professionally.Results: Participants attended an average of 5 of 8 sessions. There was a significant reduction in perceived stress (p < 0.001, Cohen's d = 0.78) and significant improvements in global mental health (p = 0.001, Cohen's d = 0.61), physical health (p = 0.045, Cohen's d = 0.35), and job satisfaction (p = 0.047, Cohen's d = 0.34). All participants except for two agreed that the program was relevant to their life and that the skills taught were helpful. Qualitative analysis of free text responses revealed that participants developed greater presence with patients, ability to live in the moment, and empowerment to make positive life changes.Conclusion: Delivering the SMART Program to healthcare professionals is feasible and may serve as a useful tool for increasing resilience to stress. Many participants found the program transformative and the majority felt they benefited from the skills learned. This data forms the background for an upcoming randomized controlled trial. 76 Kevin F. Dowling, BA, Psychiatry Prenatal Gene Expression and Folic Acid Exposure Interact to Influence Brain Development in Late Adolescence K.F. Dowling1,2, T.W. Soare1,3,4, H. Eryilamz1,2 and J.L. Roffman1,2,3 1Psychiatry, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA, 2Athinoula A. Martinos Center for Biomedical Imaging, Charlestown, MA, USA, 3Psychiatric and Neurodevelopmental Genetics Unit, Center for Human Genomic Research, Massachusetts General Hospital, Boston, MA, USA and 4Stanley Center for Psychiatric Research, The Broad Institute of Harvard and MIT, Cambridge, MA, USA Introduction: Recent evidence associates prenatal folic acid (FA) with brain health benefits beyond prevention of spina bifida. Specifically, increased prenatal FA exposure confers reduced risk of autism spectrum disorder and severe language delay, while starvation and low FA during pregnancy have associated with increased risk for schizophrenia in replicated cohorts. Work from our group (Eryilmaz et al, JAMA Psychiatry 2018) provides the first biological evidence to support an association between prenatal FA exposure and subsequent reduction in psychiatric risk. We found that prenatal exposure to population-wide FA fortification of grain products increased cerebral cortex thickness in youths, with protective effects 64against psychotic symptoms. Further, we and others have found that fetal genetic programming can have enduring effects on brain development through adolescence. As a methyl donor, FA potentially contributes to epigenetic priming in fetal life with consequences for postnatal brain development, but it has remained unclear which genes mediate this effect: those expressed primarily in utero, or in childhood, or those expressed constitutively. Here, we examined whether prenatal FA exposure interacts with development-specific gene expression profiles to influence cortical thickness in late adolescence, the period of greatest risk for emergence of serious mental illness.Methods: Demographically matched groups of 54 healthy late adolescents (M = 18.5, SD = 0.4, 16 males) who gestated just before (Pre, N = 26) versus just after (Post, N = 28) the implementation of a mandatory U.S. government FA fortification program were recruited to participate in a genetic and neuroimaging research study. As fortification doubled blood folate levels in women of reproductive age, fortification exposure (Pre/Post) was used as a dichotomous measure of gestational FA. Participants underwent standardized structural imaging with MRI, using a single Siemens Skyra 3T scanner. Scans were processed and analyzed using FreeSurfer v5.3. DNA samples were genotyped using the Illumina PsychArray, imputed with the 1000 genomes reference panel, and underwent standard quality control procedures in plink v1.9. Following on our previous work, we used expression data from second trimester fetal (n=35) and young adult (ages 18-35, n=96) brain tissue samples (BrainCloud) to group brain-expressed genes into deciles, based on the ratio of their expression in fetal and young adult brain tissue. For example, decile 1 and decile 10 genes are preferentially expressed in the fetal and young adult brains, respectively, while decile 5 genes are expressed both during fetal development and young adulthood. Based on a training set of 1,318 previously scanned and GWASed individuals, we defined cumulative genetic variation across each decile for each subject using polygene scores. All analyses were covaried for age and sex, and for genetic ancestry (top 5 principal components).Results: Prenatal FA exposure significantly interacted with decile 1 polygene score to predict cortical thickness in two regions: right inferior temporal gyrus (ITG, p max < right posterior cingulate cortex (PCC, pmax < 0.000005). These results remained significant at the level of p<.005 after correction for multiple comparisons across 10 deciles, and across the entire cortical surface using 10,000 Monte Carlo simulations (Figure). In contrast, no significant interaction was observed between prenatal FA exposure and polygene score for any other decile (2-10). Conclusion: These results suggest that effects of gestational FA exposure on adolescent cortical development are mediated specifically by genes that reach their peak expression during fetal life. Intriguingly, this pattern implicates longitudinal effects of variable expression in prenatal genes, rather than enduring epigenetic modification of postnatal genes, in structural variation of the adolescent cerebral cortex. Given the relatively small size of this sample, replication with larger cohorts is necessary, as are prospective and mechanistic studies relating prenatal folic acid levels to subsequent brain development. Regardless, these findings shed new light on how the fetal methylome influences postnatal cortical development, and by extension, risk for psychiatric disease. 77 Alysa E. Doyle, PhD, Psychiatry Polygenic risk for bipolar disorder associates with clinical phenotypes in a child psychiatry outpatient Massachusetts General Hospital, Boston, MA, USA, 2Department of Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 3Harvard Medical School, Cambridge, MA, USA, 4Stanley Center for Psychiatric Research, Broad Institute, Cambridge, MA, USA and 5Center for Quantitative Health, Massachusetts General Hospital, Boston, MA, USA Introduction: The diagnosis of bipolar disorder (BPD) in youth is fraught with controversy, in part because its symptoms overlap with more common forms of child psychopathology such as ADHD. Objective biological markers of illness could facilitate more efficient identification of individuals with emerging psychopathology as well as those at risk for the condition 65who are not yet symptomatic. To date, however, biomarkers suitable for use in child psychiatric settings have not been available. Recently, unbiased genomewide scans (GWAS) of record size have revealed a polygenic component of risk for bipolar disorder (BPD), representing the aggregate effect of potentially thousands of small-effect alleles. In the current analyses, we examined the extent to which this polygenic component of BPD risk associated with phenotypes in children and adolescents consecutively referred for neuropsychiatric evaluation to an MGH clinic. By determining how BPD polygenic risk manifests phenotypically at different stages of development in a clinical cohort, we aimed to generate testable hypotheses regarding ways to promote improved diagnostics and risk stratification of child psychiatric outpatients. Methods: Participants were children and adolescents, ages 7 to 18, who were consecutively referred for to the MGH Learning and Emotional Assessment Program and who agreed to participate in research. Our project, the Longitudinal Study of Genetic Influences on Cognition (LOGIC) has enrolled over 1500 youth to date. Patients in the current project were probands who had been genotyped at the time of these analyses (with the Illumina Infinium PsychArray Beadchip; N=437). Their diagnoses were determined subsequent to enrollment and reflected a range of different psychopathological conditions and comorbidity. We calculated the polygenic burden of BPD-related common variants in each patient based on different significance thresh- olds from the most recent PGC meta-analysis. First, using regression, we associated BPD risk scores to two core symptom domains of the condition (mania and irritability). Second, using mixed modeling, we examined the multivariate clinical profiles associated with a high burden of risk for the condition. These profiles were based on nine symptom domains relevant to but also extending beyond BPD. Results: In youth referred for neuropsychiatric evaluation, we found an association between BPD polygenic risk and cross-diagnostic symptoms of mania (but not irritability). For the mania symptoms, after correction for multiple testing, significant interactions for age group emerged at several thresholds from the BPD GWAS discovery sample, indicating that the associations with BPD polygenic risk scores (PRSs) were stronger in the adolescents. Permuted p-values fell between .0138 and .042. Post hoc comparisons showed significant associations between BPD PRSs and mania symptoms in adoles- cents, with BPD risk explaining up to 4% (F(1,125)=5.42, p=.022) of the variance. No significant associations were found between BPD risk and mania in the younger group. Additionally, the clinical profile associated with a high (upper 20%) BPD polygenic burden was distinct and, like our univariate analyses, show developmental effects. Adolescents with a high BPD polygenic risk showed a more severe symptom profile, characterized by greater levels of mania symptoms as well as symptoms related to inattention and impulsivity. Although these latter symptoms are also characteristic of ADHD, their typical onset in that condition is earlier in childhood. There were no differences in the clinical profiles of younger children with high and low BPD risk.Conclusion: In a multi-diagnostic clinical sample, variation in BPD risk associated with cross-diagnostic mania symptoms, which are pathognomonic to the condition and which associate with conversion to illness in the literature. Moreover, a high polygenic burden was associated with a distinct and more severe clinical profile that included but was not limited to BPD- related phenotypes. If confirmed in longitudinal samples, these results suggest that BPD polygenic scores could serve as an objective indicator of risk in youth with emerging BPD symptomatology and could also provide a means of identifying youth at risk before they become symptomatic. Additionally, while BPD polygenic risk associates with ADHD-like symptoms, those associations showed distinct age-related patterns that could potentially enhance clinical prediction models. 78 Yu Duan, MD, Radiology Predictive value of duplex ultrasound for significant in-stent restenosis after percutaneous transluminal renal artery and of Medical Ultrasonics, The First Affiliated Hospital, Sun Yat-sen University, Guangzhou, China Introduction: In-stent restenosis (ISR) is one of the major percutaneous transluminal renal angioplasty with stent placement (PTRAS) and is associated with poor prognosis. Renal artery duplex ultrasonography (DUS) parame - ters has been used for the predicting of ISR. However, most of the risk factors for restenosis have not been well controlled in previous studies, raising questions about the reliability of the results. The purpose of this study is to investigate the value of DUS parameters in predicting significant ISR after PTRAS using a propensity score matching (PSM) analysis method.Methods: Retrospective chart review of patients with atherosclerotic renal artery stenosis (ARAS) who underwent PTRAS, pre-stenting and early post-stenting (<1 months) DUS exams between January 2011 and December 2014 at our institution was performed. Post-PTRAS follow-up intervals were no less than 18 months. Significant ISR was defined as more than 60% luminal stenosis on angiography. Using the propensity score model matching analysis, cases with significant ISR and controls without ISR were matched 1:1 on multiple risk factors of renal artery restenosis in pre-stenting and early post-stenting groups. Renal length difference, peak systolic velocity (PSV) of renal arteries and renal aortic ratio (RAR) on DUS were compared between cases and controls.66Results: 96 ARAS patients were identified, including 29 with significant ISR and 67 without significant ISR. As ultrasound parameters were missed in some patients, 28 pre-stenting ISR cases and 16 post-stenting ISR cases were matched with same number of cases without ISR. The pre-stenting case-control comparison showed the renal length difference of non-lesion side and lesion side was significantly higher in cases than that in controls (18.73\u00b116.87 mm vs. 11.12\u00b116.87 mm, P = 1.79\u00b10.59, P = 0.004) were higher in the case group. The AUROC of pre-stenting renal length difference for ISR was value of 1.75 (sensitivity 81.5%, specificity 50.0%).Conclusion: With risk factors adjusted using the propensity score matching method, pre-stenting renal length difference and early post-stenting PSV and RAR were found to be associated with significant ISR, which may serve as non-invasive markers to predict post-stenting restenosis. 79 Yu Duan, MD, Radiology Risk stratification and management optimization of cytological indeterminate thyroid nodules Using Thyroid Imaging Reporting and Data Systems Y. Duan1,2, Q. Li1 and A.E. Samir1 and of Medical Ultrasonics, The First Affiliated Hospital, Sun Yat-sen University, Guangzhou, China Introduction: The risk stratification of cytological indeterminate thyroid nodules (TNs) is challenging and the current management strategy remains controversial. The aim of the current study was to evaluate the value of American College of Radiology Thyroid Imaging Reporting and Data System (ACR TI-RADS) for malignancy stratification of the cytological indeterminate TNs, by comparing with American Thyroid Association (ATA) guidelines.Methods: This retrospective study was approved by the institution review board and the requirement of informed consent was waived. The FNAB indeterminate thyroid nodules were recruited between January 2012 and December 2016 in our institu - tion, including atypia of undetermined significance or follicular lesion of undetermined significance, follicular neoplasm or suspicious for a follicular neoplasm, suspicious for malignancy. All enrolled nodules had resection proved pathology and pre-procedure thyroid ultrasonography. Ultrasound (US) features, such as internal composition, echogenicity, margin, shape, and calcifications were evaluated and scaled according to 2017 ACR-TIRADS system and 2015 ATA guidelines. The risks of malignancy were compared and Multivariate logistic regression analysis for predicting malignancy was performed within ACR TI-RADS categories and ATA sonographic patterns. The diagnostic criteria based on ACR categories or ATA patterns were compared, using pathologic diagnosis as the reference standard.Results: Of the 184 indeterminate thyroid nodules on FNAB, 84 (45.7%) were malignant and 100 (54.3%) were benign. The median TIRADS scores of malignant nodules (6, range 4.5-7) was higher than that of benign nodules (3, range 3-4) (p<0.001). The malignancy risks 84.1% for ATA very low suspicion pattern to high suspicion pattern, respectively. After adjusting the nodule size, the ORs of nodules with TR4 and 5 were 23.3 (95% CI 2.6, 206.9) and 555.3 (95% CI 28.9, 10666.2) (95% CI 8.7, 309.4). When TR3-5 TNs were assigned as suspicious malignant, the sensitivity was 99% and negative predictive value was 93%. When only TR5 TNs were assigned as suspicious malignant, the specificity was 99% and positive predictive value was 96%. None of the ATA diagnostic criteria showed comparable results. The ORs of nodules with US features of solid composition, hypo-echogenicity, taller than wide morphology, irregular margins and presence of micro-calcification were 1.7 (95%CI 0.6, 4.8), 5.5 (95%CI 2.4, 12.6), 2.7 (95% CI 0.9, 8.3), 38.1 (95%CI 8.0, 182.1), and 13.2 2.0, 86.9), respectively.Conclusion: Our results showed the ACR TI-RADS systems provide important information to further risk stratify the indeter - minate TNs after the initial FNAs. The TR2 category can help to rule out cancer, and TR5 category is strongly associated with malignancy. These results have important implications to optimize the management strategy for patients with indeterminate cytological result.67 80 Caitlin M. Dugdale, Medicine - Infectious Diseases Risks and benefits of dolutegravir-based ART for women with HIV of childbearing age in South Africa: A model- Practice Evaluation Center, Boston, MA, USA, 2Division of Infectious Diseases, Massachusetts General Hospital, Boston, MA, USA, 3Harvard Medical School, Boston, MA, USA, 4The Desmond Tutu HIV Centre, University of Cape Town, Cape Town, South Africa, 5Division of General Internal Medicine, Massachusetts General Hospital, Boston, MA, USA, 6Institute of Infectious Disease and Molecular Medicine, Faculty of Medicine, and Division of Epidemiology and Biostatistics, School of Public Health & Family Medicine, University of Cape Town, Cape Town, South Africa, 7ICAP at Columbia University, Columbia University, New York City, NY, USA and 8Mailman School of Public Health, and College of Physicians & Surgeons, Columbia University, New York City, NY, USA Introduction: Although widespread use of dolutegravir is promoted globally, early data from the Tsepamo study suggest there may be an increased risk of neural tube defects (NTDs) associated with dolutegravir use at conception. Some policies are therefore continuing to recommend efavirenz for women living with HIV (WLWH) of childbearing age. Few data inform the systematic consideration of the clinical outcomes of dolutegravir- versus efavirenz-based ART for WLWH and their infants.Methods: Using the CEPAC-International and Pediatrics models, we simulated a cohort of WLWH (15-49y) in South Africa currently taking first-line ART (2,128,900) or newly initiating ART (363,200/year) and their infants over five years. We compared two strategies: continuation or initiation of first-line efavirenz-based ART (EFV) versus switch to or initiation of first-line dolutegravir-based ART (DTG). We derived ART efficacy suppression: EFV 86%/83%, DTG 94%/91%). prevalence and fertility rates informed number of births among women on ART. NTD incidence was from preliminary data from the Tsepamo study in Botswana (EFV: 0.05%, DTG: 0.94%), assuming 100% NTD mortality. Modeled adult outcomes included cumulative deaths, severe opportunistic diseases (ODs), and sexual HIV transmissions. Modeled pediatric outcomes included number of births, NTDs, perinatal HIV infections, and deaths. In sensitivity analyses, we varied dolutegravir-associated NTD incidence and efavirenz efficacy.Results: Over five years, compared to EFV , DTG averted 28,400 deaths and 92,100 severe ODs among WLWH, and 52,800 sexual HIV transmissions. Using Tsepamo point estimates, DTG resulted in 10,000 additional NTDs (EFV: 600, DTG: 10,600; Table) among 1,126,300 infants born to WLWH. DTG reduced projected pediatric HIV infections and non-NTD deaths, but increased total pediatric deaths by 8,400. Over a wide range of inputs for dolutegravir-associated NTD risk and efavirenz efficacy, deaths averted among women with HIV on DTG versus EFV exceeded pediatric deaths added unless dolutegravir-associated NTD incidence was >2.4% (Figure). Conclusion: Even if NTD risks are higher with dolutegravir than efavirenz at conception, we project that dolutegravir would ultimately lead to many fewer deaths among WLWH, HIV transmissions to partners, and HIV-infected infants. These risks and benefits should be carefully weighed as new data on the safety of dolutegravir in WLWH of childbearing age become available.68 81 Erin C. Dunn, Doctor of Science, Psychiatry Genes, exposure to adversity, and sensitive periods in risk for depression Smoller1,2,3 1Center for Genomic Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Psychiatry, Harvard Medical School, Boston, MA, USA, 3Stanley Center for Psychiatric Research, The Broad Institute of Harvard and MIT, Cambridge, MA, USA, 4Epidemiology, Harvard T.H. Chan School of Public Health, Cambridge, MA, USA, 5Molecular and Cellular Biology, Harvard University Center for Brain Science, Cambridge, MA, USA and 6Department of Neurology, Boston Children's Hospital and Harvard Medical School's F.M. Kirby Neurobiology Center, Boston, MA, USA Introduction: Across multiple disciplines, there is a long-standing belief that there are developmental windows when experi- ence can have lasting effects in shaping behavior, health, and ultimately risk for disease. During these \"sensitive periods,\" experience may become biologically embedded due to occurring at a stage in development when the foundation of organs, tissues, and physiological systems is established. For example, an exposure coinciding with peak periods of brain plasticity may shape brain circuitry and function more than the same exposure occurring earlier or later in development. Although sensitive periods shaping risk for depression are not yet identified, they likely arise through a complex orchestration of genes and experience. With respect to genes, preclinical studies in animals have identified specific genes involved in regulating the timing and duration of sensitive periods. For instance, studies using mouse knock-in genetic models in the visual system have shown that increased Bdnf expression can initiate a sensitive period, whereas disruptions to Gad2 delay sensitive period onset. With respect to experience, epidemiological studies are beginning to implicate specific age stages when the effect of adversity, including exposure to child maltreatment, is more potent in conferring risk for depression. Building from these two lines of evidence, the goal of the current study was to examine the role of sensitive period-regulating genes, alone and in interaction with exposure to adversity, on risk for depression.Methods: To accomplish this goal, we: (1) performed gene-level and gene-set-level analyses using data from the Psychiatric Genomics Consortium (PGC) to evaluate the effect on risk for major depressive disorder (MDD) of 53 genes shown in animal studies to regulate sensitive periods; (2) evaluated the developmental expression patterns of these sensitive period-regulating genes by analyzing data from BrainSpan, a transcriptional atlas of 57 healthy, post-mortem donors (ages 5.7 weeks post-conception to 82 years); and (3) tested for gene-by-development interplay by analyzing the combined effect of common variants in these sensitive period genes and timing of exposure to adversity within a population-based study of children (n=6255).69Results: Results indicated that as a set, genes regulating the opening of sensitive periods were most associated with MDD risk; this included in MDD risk were also developmentally regulated. That is, age was significantly associated with expression levels in about half of the 15 MDD-implicated genes, explaining up to 56% of the variation in gene-level expression. Among genes associated with MDD, 75% (6 of 8) had a nadir of expression between ages 1 and 5. Gene-by- development interplay analyses revealed a significant main environmental effect of adversity exposure between ages 1 and 5 years ( = 0.85, 95% C.I. = 0.57 - 1.12, p-value = .007), but no genetic main effect or gene-by-development interaction effect. Conclusion: These results indicate that genes involved in regulating sensitive periods may be implicated in depres-sion vulnerability and be differentially expressed across the lifecourse, but that larger studies will be needed to identify developmental GxE effects. 82 Laura Duque, Research Fellow, Psychiatry Association between positive psychological characteristics and adherence in patients with acute coronary syndrome L. Duque1,2, General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Medicine, Brigham and Women's Hospital, Boston, MA, USA Introduction: Patient adherence to recommended guidelines related to physical activity, diet, and medications following an acute coronary syndrome (ACS) is a critical aspect of treatment that can contribute to better outcomes and prognosis. Optimism, a positive expectancy of one's future, may play an important role in adherence. Dispositional optimism has been associated with both increased adherence to health behaviors and better cardiovascular health outcomes. However, disposi- tional optimism has largely been considered a static construct that may be difficult to promote using psychological interven - tions. Less is known about the relationships between state optimism and adherence to health behaviors. In this secondary analysis of baseline data from an 8-arm, randomized, factorial trial (N=128) of a health behavior intervention, we examined the associations between dispositional and state optimism and health behavior adherence in patients following an ACS. Methods: Participants were recruited during hospitalization for ACS (myocardial infarction or unstable angina). During or shortly after hospitalization, participants completed self-report measures of overall adherence (Medical Outcomes Study Specific Adherence Scale), physical activity adherence (7-day Physical Activity Recall Scale), dietary adherence (Meats, Eggs, Dairy, Fried foods, fat In baked goods, Convenience foods, fats added at the Table, and Snacks instrument), medication adherence (Self-reported medication Adherence questionnaire), positive affect (Positive and Negative Affect Schedule), optimism (Life Orientation Test-Revised (LOT-R)), and state optimism (a modified version of the LOT-R). Sociodemo-graphic and medical variables were obtained via chart review. Univariable and multivariable linear regression analyses were used to examine the relationships between psychological constructs and health behavior adherence, while controlling for relevant covariates, including other psychological constructs. Findings were considered significant if p < .05. Results: A total of 128 ACS patients were included in the analyses. In univariable analysis, both dispositional and state optimism were associated with overall self-reported adherence (dispositional optimism: = 0.11, Standard error (SE) = 0.04, p= 0.003; state optimism: = 0.12, SE = 0.04, p= 0.010). State optimism also was associated with lower pre-ACS physical activity (= -2.94, SE = 0.92, p= 0.002), while dispositional optimism was not. Multivariable analyses, adjusting for age, gender, medical comorbidity, and negative psychological constructs revealed that state optimism continued to be associ- ated with lower pre-ACS physical activity (= -2.8, SE = 0.99, p= 0.005), while dispositional optimism was not (=-0.96, SE = 1.05, p= 0.366.)Conclusion: This study provides evidence that dispositional and state optimism may behave divergently regarding the amount of physical activity performed by patients prior to ACS. There are several possible explanations for this relation - ship, including the possibility that individuals with more severe cardiac symptoms (and subsequently less physical activity) prior to ACS may feel more optimistic in the hospital following revascularization. Continued work is needed to clarify the different relationships between dispositional and state optimism and both health behaviors and cardiovascular health outcomes cross-sectionally and prospectively.7083 Sayon Dutta, MD, MPH, Emergency An electronic clinical decision support tool reduces potentially unnecessary tetanus booster vaccinations in the emergency department S. Dutta1,2, R. McMurry3, D. Mcevoy2, B. White1, USA, 2Partners eCare, Boston, MA, USA, 3Boston University, Boston, MA, USA, 4Brigham and Women's Hospital, Boston, MA, USA and 5Medicine, Brigham and Women's Hospital, Boston, MA, USA Introduction: Updating tetanus vaccinations for patients presenting to the emergency department with wounds is common, although often patients do not have good recollection of their prior immunization history. The CDC recommends tetanus updates if the timing of the prior vaccination was unknown or more than ten years ago, or for contaminated wounds more than five years ago. Patient's immunization history may not always be accurate in the Electronic Health Record (EHR), but when it is, emergency providers may not be reviewing this data prior to ordering a tetanus booster. We studied the effect of a real-time electronic Clinical Decision Support (CDS) tool that warns providers upon ordering a tetanus booster vaccination when the patient has a documented prior vaccination within the last ten years. The alert displays guidance text on when tetanus boosters are indicated, and provides a link to the patient's immunization history. Methods: This was a prospective quasi-experimental trial in three hospital emergency departments (two academic and one community) in the Boston area. We studied adults (age >18 years) where the ED provider ordered a tetanus booster vaccina - tion. The study was conducted in two equal length phases between December 2016 and April 2017. The CDS tool was silent to ED providers in the initial baseline phase, and visible in the intervention phase. We compared the rate of administration of a tetanus booster vaccination during the ED encounter.Results: For the eligible population of 60,983 ED encounters, the rate of documented prior tetanus vaccination in the EHR within the last five and ten years was 22% and 35% respectively. Of the eligible population, 339 were included in the study population as they had a tetanus vaccination ordered with a prior documented history of vaccination within the last ten years. The median age was 51 years and 54% were female. During the baseline monitoring phase, a tetanus vaccination was administered 91% of the time the order was placed, compared to 55% in the intervention phase (OR = 0.12; 95% CI 0.07 to 0.22). Tetanus vaccines administered in the intervention phase were frequently for contaminated wounds or proximity to the time of the next suggested tetanus booster. Conclusion: A simple clinical decision support tool that warns users that a patient may have an up-to-date tetanus status reduces potentially unnecessary vaccinations. CONSORT Diagram 7184 Odeta Dyrmishi, Masters, Dermatology Remote Clinical Research Trials: A comparison or recruitment/enrollment rates, retention and demographics of traditional in-person research study and a fully remotes study O. Dyrmishi1, A. Centi1, M. Atif1, J. Phillips1,2, K. Jethwani1,3,4 and J. Kvedar1,3,4 1Connected Health Innovation, Partners HealthCare, Worcester, MA, USA, 2Newton Wellesley Hospital, Newton, MA, USA, 3Medical School, Harvard University, Cambridge, MA, USA and 4Massachusetts General Hospital, Boston, MA, USA Introduction: Remote recruitment and enrollment of study participants offers great potential to advance crucial research. The most obvious benefit of this recruitment and enrollment method is that it places the patient (participant) at the center of the study. Further-more, this method saves the participants and research staff time and resources thereby increasing efficiency of the research team. This method also has the potential to yield a more diversity pool of participants. However remote studies might also offer challenges to recruitments, engagements and completion. The purpose of this analysis is to compare the recruitment and enrollment rates between a traditional research study and a remote research study in order investigate the effectiveness of remote research studies.Methods: Data for two research studies were compared, Feat Forward and iTrack Fitness. Feat Forward (n=275) was a 24-week RCT research study that recruited study participants through a combination of physician recommendation, flyers and tabling at Partners affiliated health clinics. This study offered patients a hyper-personalized physical activity smartphone application and conducted three in-person visits. iTrack Fitness (n=184) was a 24-week remote research study that recruited study participants through a combination of online advertisement (craigslist) and flyers at Partners affiliated clinics and online research portals (Partners Clinical Trials portal). This study offered patients a physical activity tracker and a machine learning algorithm that provided \"just in time\" motivational text messages and conducted three remote visits/events (online surveys). Recruitment, enrollment and retention rates were calculated for these studies were calculated and compared. Demographics (age, sex, race, education & employment status) of study participants for both research studies were analyzed. Recruitment rates were calculated by dividing enrolled participants by screened participants. Enrollment rates were calculated by dividing the recruitment period (enrollment start date - enrollment end date) by the number of participants enrolled. Retention rate was defined as the proportion of participants who completed all required study visits (two visits at mid-point and two at close-out). Retention rates at study midpoint and closeout were calculated for both studies and compared. Differences in demographics of between the RCT and remote study were summarized and compared.Results: The 24-week RCT study (n=275) had a recruitment rate of 22% and it an enrollment rate of 3.12 participants per week. The 24-week remote study (n=184) had a recruitment rate of 42% and an enrollment rate of 7.67 participants per week. Participant retention for the 24 week RCT (n=275) research study was 33.5% at midpoint and 66.9% at closeout. Participant retention for the 24-week remote study (n=184) was 15.76% at midpoint and 48.91% at closeout. The mean age for the 24-week RCT study and 24-week remote study was 51.31 and 37.02 respectively. The remote study reported higher ratios of females (71.20%) and employed participants (83.15%). In comparison, the RCT study had 57% female participants and an employment rate of 52%. The racial composition of the RCT study was more varied (50.2% white; 32.3% black; 14.9% white; 32.3% black; 21.19% other).Conclusion: Remote research studies can be a quicker and more efficient way to increase study recruitment. They offer higher rates of recruitment and faster recruitment times. Our preliminary analysis also suggests that they may be more suitable to younger and employed populations. However, this may limit the pool of potential research participants. This analysis also suggests that traditional RCT research studies might offer higher retention rates that might occur as a result of the personal touch that they offer to study participants. Further research is needed into remote research studies to evaluate the advantages of a remote recruitment over conventional recruitment methods. 85 Beverly Ejiofor, Pediatrics Positive End-Expiratory Pressure Generated by High-Flow Nasal Cannula in a Pediatric Model B. Ejiofor1, R. Carroll1,2, W. Bortcosh1 and R. Kacmarek1,2 1Massachusetts General Hospital, Boston, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: High-flow nasal cannulas (HFNC) have been increasingly used in the pediatric critical care patient population 1-3. Different mechanisms have been theorized as to how HFNC reduces work of breathing, including diminishing upper airway dead space by the washout of carbon dioxide. However, one of the likely primary mechanisms by which HFNC reduces work of breathing is by generating positive end-expiratory pressure (PEEP). There is limited data to date assessing the PEEP delivered by moderate flows (8 to 50 liters per minutes, LPM) of HFNC, which are used most commonly in pediatric patients.72Methods: Pediatric upper airway models were created with a 3D printer and connected to an ASL 5000 lung simulator (version 3.5, IngMar Medical, Pittsburgh, Pennsylvania). Age-specific flows were delivered via five commensurate Teleflex Comfort Flo HFNC devices. Pressure throughout the simulated airway were measured at HFNC flows (6 LPM to 60 LPM) with 25%, 50%, and 75% air leak to simulate open-mouth breathing. Results: PEEPs of 1.2 to 36 cm H2O were generated by HFNC flows of 6 to 60 LPM. In general, for each specific cannula, increasing the flow and decreasing the air leak resulted in higher levels of PEEP delivered (p < 0.001 and >10% difference). Changes in lung mechanics as generated by the ASL 5000 to simulate different patient ages did not have a linear relationship with PEEP delivered. Conclusion: High flow nasal cannulas deliver a varying amount of PEEP at the alveolar level with flows of 6 to 60 LPM. Increasing flow and decreasing leak resulted in the generation of greater PEEP. PEEP levels differed across cannulas and model weights at the same leak level, likely related to differences in the nasal interface between the HFNC device and the model nares. Figure 1. Schematic of HFNC testing system. Pressure measurements indicated are shown in Table S1 and S2. Figure 2. Lung model alveolar end-expiratory pressure or PEEP as a function of HFNC set flow rate with various leaks. Each curve represents a different leak. Individual data points are shown for each pressure measurement. Mouth leak is equal to indicated percentages of the set flow rate. 86 Kristen K. Ellard, Ph.D., Psychiatry Functional connectivity between anterior insula and key nodes of frontoparietal executive control and salience networks distinguish bipolar depression from unipolar depression and healthy controls K.K. General Hospital/Harvard Medical School, Boston, MA, USA, 23Department of Biomedical Graduate Studies, University of Pennsylvania, Philadelphia, PA, USA, 3Department of Psychology, Tufts University, Medford, MA, USA and 4Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Charlestown, MA, USA Introduction: Bipolar patients are characterized by dysregulation across the full spectrum of mood, differentiating them from unipolar depression. The ability to switch neural resources between default mode network (DMN), salience network (SN), and executive control network (ECN) has been proposed as a key mechanism for adaptive mood regulation. The anterior insula is implicated in the modulation of functional network switching. Differential connectivity between anterior insula and functional networks may provide insights into pathophysiological differences between bipolar and unipolar mood disorders, with implications for diagnosis and treatment.Methods: Resting state fMRI data were collected from 98 subjects (35 unipolar, 24 bipolar and 39 healthy controls). Pearson correlations were computed between bilateral insula seed regions and a priori defined target regions from the DMN, SN, and 73ECN. After r-to-z transformation, a one-way MANCOVA was conducted to identify significant differences in connectivity between groups. Post-hoc pairwise comparisons were conducted and Bonferroni corrections were applied. Receiver-operator characteristics (ROC) were computed to assess diagnostic sensitivity. Results: Bipolar patients evidenced significantly altered right anterior insula functional connectivity with the inferior parietal lobule (IPL) of the ECN relative to unipolar patients and controls. Right anterior insula-IPL connectivity significantly discriminated bipolar patients.Conclusion: Impaired functional connectivity between the anterior insula and the IPL of the ECN distinguishes patients with bipolar depression from unipolar depression and healthy controls. This finding highlights a pathophysiological mechanism with potential as a therapeutic target and a clinical biomarker for bipolar disorder, exhibiting reasonable sensitivity and specificity. 87 Samantha C. Ernst, Bachelor of Science, Psychiatry Agreement between caregivers and their children on parent-to-child maltreatment and predictors of reporter discordance S.C. Ernst Psychiatry, Massachusetts General Hospital, New Britain, PA, USA Introduction: Although child maltreatment is known to be associated with many poor health outcomes, there is currently no gold standard to measure child maltreatment exposure. Thus, obtaining accurate accounts of these adverse experiences remains a major problem for research.Methods: The current study first examined mother-partner concordance, and then caregiver-child concordance, in reporting of physical and emotional abuse in a sample from the Avon Longitudinal Study of Parents and Children (ALSPAC). Question- naires about child maltreatment in the home were completed by 8,782 mother-partner pairs and 3,009 caregiver-child pairs. There were three parts to our analysis where we examined: (a) agreement between mother and partner reports, (b) agreement between caregivers and children reports, and (c) predictors of discordance between reporters. Logistic regression analyses compared known demographic factors associated with child maltreatment exposure, including child race, child gender, maternal age, maternal marital status, homeownership, highest level of maternal education, parent social class, and maternal depression, as predictors of discordance between: (a) mother and partner reports and (b) prospective caregiver reports and retrospective child reports.Results: Results indicated high mother-partner concordance between parents' report of their own and each other's abusive behaviors. However, there was low to moderate concordance between caregiver and child reports. Caregiver-child concor- dance was improved moderately when prevalence of abuse was considered. LogisticConclusion: In summary, these results provide evidence to suggest that mother and partners are equally valuable as reporters of their own and each other's abusive behaviors, but that caregiver accounts of their child's childhood maltreatment should be interpreted cautiously and may underestimate the amount of abuse their child experienced. 88 Leda Espinoza, BA, Physical Medicine and Rehabilitation The Long-Term Outcomes of Electrical Burn Injuries: A Burn Model Systems National Database Study L. Espinoza1, L.C. Simko1, K. Kowalske2, C. Ryan1 and J. Schneider1 1Boston-Harvard Burn Injury Model System, Spaulding Rehabilitation Hospital, Charlestown, MA, USA, 2University of Texas Southwestern Medical Center, Dallas, TX, USA, 3University of Washington, Seattle, WA, USA and 4University of Texas Medical Branch, Shriners Hospitals for Children-Galveston, Galveston, TX, USA Introduction: Electrical burns are severe injuries that often result in a different set of complications than other types of burns. The objective of this study is to examine long-term physical, mental health, and employment outcomes of burn survivors with electrical injuries and compare them to those of survivors with fire/flame injuries. Methods: Data from the Burn Model System National Database (1993 - 2015) were analyzed. Individuals over 18 years of age that were alive at time of discharge were included. Demographic and clinical characteristics of those with fire/flame injuries and those with electrical injuries were compared. The following outcome measures were assessed at 24 months post-injury: the Mental Health Composite Scale (MCS) and the Physical Health Composite Scale (PCS) of the 36/12-Item Short Form Health Survey, as well as employment status. Regression analyses were used to compare outcomes of burn 74survivors with fire/flame and electrical injuries at 24 months post-injury, controlling for age, gender, race/ethnicity, burn size, inhalation injury, number of days on a ventilator, and pre-injury employment status.Results: The study included 2,108 individuals with fire/flame burns and 216 with electrical burns. Those with electrical injuries were younger, had smaller burns and shorter lengths of stay, and were more likely to be male, be burned at work, undergo an amputation, and have neuropathy (Table). In regression analyses, those with electrical burns had significantly lower PCS scores ( =-0.534, p<0.001) and were about half as likely to be employed (OR=0.45, p=0.002) at 24 months post-injury compared to those with fire/flame injuries. MCS scores did not differ between the two groups. Conclusion: Burn survivors with electrical burns experience worse physical function and employment outcomes at 24 months post-injury compared to those with fire/flame injuries. Demographic and medical characteristics of the study population at discharge 89 Mark Etherton, Neurology Blood Brain Barrier Leakage Rates and Ischemic Tissue Outcomes in Patients with Advanced White Matter Disease M. Etherton, O. Wu, A. Giese, A. Lauer, G. Boulouis, B. Mills, L. Cloonan, K. Donahue, W. Copen, P. Schaefer and N. Rost Massachusetts General Hospital, Boston, MA, USA Introduction: The structural integrity of normal appearing white matter (NAWM) predicts outcomes after acute ischemic stroke (AIS); however, the underlying mechanisms are unclear. We evaluated blood-brain barrier (BBB) permeability and microstructural NAWM integrity in association with ischemic tissue outcomes.Methods: In a prospective observational study of patients with moderate to severe white matter hyperintensity (WMH) burden, 43 AIS patients had brain MRIs with dynamic susceptibilty contrast (DSC) perfusion performed within 12 hours and at 3-5 days after stroke onset. Acute infarct volume on diffusion-weighted imaging (DWIv) and final infarct volume (FIV) on follow-up FLAIR sequences were determined using a validated semi-automated method. Infarct growth (IG) was calculated from the acute DWIv subtracted from FIV. Median diffusivity metrics and BBB leakage rates (K2 coefficient) were measured in contralesional NAWM. Pearson correlation and linear regression were used for statistical analysis.Results: The K2 coefficient correlated NAWM and K2 coefficient were not predictive of infarct growth. In the multivariable model, only acute DWIv independently predicted IG.Conclusion: In a prospective study of AIS patients with moderate to severe WMH burden, BBB leakage rates correlate with acute infarct volume. These findings suggest that NAWM microvascular integrity contribute to the severity of ischemic tissue outcomes. 90 Luther T. Evans, Pathology Examining Quality Control Percentage of 9 Different USDA Grains in a Mixture with Magnetic Resonance Spectroscopy L.T. Evans1, Cancer Institue, Boston, MA, USA, 2Molecular Pathology, MGH, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: Our lab uses High-Resolution Magic Angle Spinning (HRMAS) Magnetic Resonance Spectroscopy (MRS) to examine metabolites in tissue to develop different cancer diagnostic methods based on machine learning to separate the contribution from different pathological components and correlate the resulting metabolic profile with disease status. However, HRMAS MRS can also allow us to determine grain composition in a simple fashion and has great potential to facilitate grain identification, which is a challenge for manufacturers purchasing and selling grain mixtures to each other. This 75technique, unlike mass spectrometry or infrared spectroscopy, can measure metabolites without chemical extraction. Our objective is twofold: (1) develop a simpler method for identification of grain mixture composition and (2) test the analytical model from this project for use in cancer research. The United States Department of Agriculture (USDA) provided 9 grains (flax, corn, sorghum, wheat, millet, rye, oat, rice, and barley) and 5 mixtures. Methods: For the purpose of validating the measurement results between two researchers, we began scanning a 4-grain mixture of millet, spelt, rye, and corn for three trials. 2.5mg of each pure grain was placed into a rotor which was then placed into the Bruker AVANCE spectrometer operating at 600 MHz to scan at a spin rate of 3600 rpm at 25oC. From this process, we can conclude that the reproducibility of the test is high based on the similar spectrums. After the initial 4-grain mixture scan we began scanning pure samples of each of the 9 grains. Thus far, we have conducted two measurement trials each for flax, corn, and sorghum.Results: In the process of predicting percentages using previously scanned pure mixtures: measured in different experimental conditions and eight peaks wasn't enough to allow accurate predictions. Reproducible between sample prep and scans. Conclusion: The project will continue by measuring the rest of the pure grains and mixtures from the USDA followed by analysis using an overdetermined linear regression model. This model allows us to predict the composition of the grain mixture. We want to use this method to aid us in our cancer research. This is plausible because the pathological features in a tissue sample are similar to a mixture of grains. While we scan a single piece of tissue it is actually a heterogeneous mixture of pathological features that contribute differently to measured metabolite levels. We will apply the analytical model to calculate and then mathematically remove these contributions so we can better compare tissues to identify cancer. 91 Melanie F. Pradier, Psychiatry Predicting discontinuation of psychotherapeutic treatment after change in antidepressant prescriptions M. F. Pradier1, McCoy2,3, M. Hughes1,4, R.H. Perlis2,3 Harvard University, Cambridge, MA, USA, 2Center for Quantitative Health, Massachusetts General Hospital, Cambridge, MA, USA, 3Harvard Medical School, Cambridge, MA, USA and 4Tufts University, Medford, MA, USA Introduction: While efforts at personalization of antidepressant (AD) treatment have focused on therapeutic response, on average modern ADs show greater differences in tolerability. For example, a recent large meta-analysis found significant differences for probability of acute discontinuation, but not efficacy, among 21 treatments investigated in randomized, controlled trials 1. Treatment discontinuation is likely to be multifactorial, with contributions from lack of perceived benefit to burden of adverse effects to cost or other logistical challenges in continuing treatment. Notably, efficacy and safety for AD therapies are hard to estimate prior to treatment initiation, with rates of initial treatment failure ranging from 40% to 60% or more, compounded by substantial rates of nonadherence 2. Our previous work has demonstrated that electronic health records (EHR) can be leveraged to generate sufficient sample sizes to facilitate predictive modeling studies of treatment outcomes3. Here, we apply machine learning to develop predictions of early discontinuation using large-scale EHR data from the network of a large medical center and characterize the performance of these models in a second center. Methods: The initial study cohort comprised 260,212 patients from 2 academic medical center treatment networks in the Northeast United States (subsequently referred to as Site A and Site B) who received at least one antidepressant prescription between 1997 and 2014. We restricted the cohort to patients age 18-80 who received at least one antidepressant prescription during the study period among 11 common primary treatments for MDD. Individuals were excluded if they had no further follow-up of any type in the EHR system after 90 days from the last prescription registered, as this might indicate transfer outside the current hospital system or mortality. We call \"dropout\" the event of early treatment discontinuation. This occurs if (a) the temporal interval for the last prescription is shorter than expected, and (b) the patient has not received a non- 76pharmacologic treatment for depression such as psychotherapy. In particular, we apply the dropout label when a patient stays for a period of time shorter than 90 days in the last prescription and no other psychiatric code is observed in the following 13 months. We aimed to predict whether any change in antidepressant medication was associated with patient dropout, given demographic information and a feature vector that represents a patient's known medical history prior to that treatment visit. Specifically, patient history consists of all available coded billing data from the hospital, including ICD9 diagnoses, CPT lab tests and procedures, and medication prescriptions. From an initial set of 23,949 possible codes, we applied frequency thresholding to select 7,859 codes (or \"codewords\") which occur in at least 100 patients in Site A. For each patient and each encounter date, we built a compact representation of past event history via a count vector indicating how often each of these codewords appeared in the patient's past history. Sociodemographic variables include gender, race/ethnicity and age at treatment event. We also include the calendar date of the treatment to capture any secular trends in drug popularity over time. Because antidepressant medications are often given in combination, this predictive task is a multiple binary label prediction problem. We trained a separate classifier on the Site A training set for each of the 11 target drugs and evaluate at every change in treatment prescription. We considered two standard classifiers, logistic regression (LR) and random forests (RF). Hyperparameters were tuned on the Site A validation set, using grid search to find the parameter combination that performs best on the area-under-the-ROC-curve (AUC) metric. Finally, model performance was compared using AUC for each of the 11 drug prediction tasks in the held-out testing set from Site A, then in the independent Site B. Results: Among 75,471 individuals with 11 treatments in site A, 32,782 (43.44%) patients failed to return after adjustment of antidepressant medication. In a total of 130,642 med change prescriptions, 32,782 (25.09%) of them were associated with failure to return, ranging from 3,293 (31.03%) for paroxetine to 902 (17.35%) for duloxetine. Models using demographic information and date of prescription yielded AUCs ranging from 0.48 (duloxetine) to 0.63 (sertraline) with a mean of 0.60. Incorporation of coded ICD9/10 data improved AUCs to a mean of 0.7 (from 0.68 for paroxetine to 0.78 for venlafaxine). AUC values hold similar in a external evaluation with 50,193 patients on site B (drop of 0.02 on average) for models trained on site A.Conclusion: In this investigation of 125,664 individuals across 2 academic medical center-based health systems, we improve significantly on chance and on simple sociodemographic models in discriminating dropout risk by incorporating coded clinical features in logistic regression and random forest classifiers. Standard machine learning applied to coded EHR may facilitate identification of individuals at high risk for failure to return following change in antidepressant medication. Next steps will include application of more advanced machine learning approaches, further validation of outcomes, and examina - tion of model performance in additional health systems. The models described here could either help in targeting interventions aimed at retention in treatment and adherence, including follow-up phone calls, deployment of adherence applications, or earlier return visit, as well as guide the clinician to select the medication with least risk for treatment discontinuation, all other things being equal. Since the models require no additional data collection, they are straightforward to apply in real time at point of care. 1. Cipriani, et.al. (2018). 2. Masand, P. S. (2003). 3. Perlis, R. H., et.al. (2012). 92 WuQiang Fan, M.D., Ph.D., Medicine Estimating HbA1c From Timed Self-Monitored Blood Glucose Values W. Fan1, H. Zheng2, N. Wei3 and D. Nathan3 1Department of Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Department of biostatistics, Massachusetts General Hospital, Boston, MA, USA and 3Diabetes Center, Massachusetts General Hospital, Boston, MA, USA Introduction: Self-monitored blood glucose (SMBG) concentrations provide key information for the day-to-day manage - ment of diabetes. We analyzed mathematical relationships between timed SMBG values and HbA1c to identify the values that correlate most strongly with HbA1c. Methods: The A1c-Derived Average Glucose (ADAG) study defined a linear relationship between HbA1c levels and estimated average glucose (eAG). We utilized the average premeal (Pre) and 90-min postmeal (Post) SMBG results from 547 ADAG study participants (285 type 1, 178 type 2 (T2DM) and 84 non-diabetic) to analyze the mathematical relationships with HbA1c levels obtained at the end of the 3-month observational study. Specific times of daily SMBG that best correlate with HbA1c were identified. Results: Linear regression analyses showed the following correlations for Pre and Post, Pre only and Post tively: 0.6572, of the 6 individual timepoints, pre-dinner SMBG had the strongest correlation with HbA1c (R2 = 0.577). This was followed by 77pre-breakfast (R2 = 0.562) and post-dinner (R2 = 0.487). Examining combinations of timepoints revealed that pre-breakfast + pre-dinner (R2=0.666) or 3 times with pre-breakfast + pre-dinner + 6-timepoints (pre-meals+post-meals, R2= 0.712). Conclusion: We have established mathematical relationships between HbA1c and timed SMBG values and identified pre-dinner and pre-breakfast as the two SMBG timepoints that best correlate with HbA1c in patients with T2DM. 93 Abigail Farrell, BS, Psychiatry Further Evidence of High Level Persistence of Pediatric Bipolar-I Disorder from Childhood Onto Late Adolescent and Early Adult Years: A 6-Year Longitudinal Follow-Up Study Wozniak1,3 1Clinical and Research Program in Pediatric Psychopharmacology and Adult ADHD, Massachusetts General Hospital, Boston, MA, USA, 2Departments of Psychiatry and Neuroscience & Physiology, SUNY Upstate Medical University, Syracuse, NY, USA and 3Department of Psychiatry, Harvard Medical School, Boston, MA, USA Introduction: Pediatric bipolar (BP)-I disorder affects a considerable portion of children and adolescents and is considered a prevalent public health concern. However, the literature on the persistence of pediatric BP-I disorder from childhood onto later years remains limited. The aim of this study was to extend the findings from our 4-year follow-up study examining the persistence of pediatric BP-I disorder and subsyndromal states in youth transitioning into adolescence and early adulthood 6 years following our original baseline study.Methods: We conducted a 6-year follow-up study of 56 youth aged 6-17 years with BP-I disorder at ascertainment followed up into adolescent and young adult years (16.0 \u00b1 4.2 years old). All subjects were comprehensively assessed with structured diagnostic interviews and psychosocial, educational, and treatment history assessments. BP-I disorder was considered persistent if subjects met full criteria for DSM-IV BP-I disorder at follow-up. We also report on subsyndromal persistence of mania and depression.Results: Of 105 youth with BP-I disorder evaluated at baseline, 78 returned for the first follow-up after 4 years, 68 returned for follow-up at year 5, and 56 returned for follow-up at year 6. Due to an incomplete interview, 1 youth was excluded from subsequent analyses. Of the 55 youth assessed, 45% (N=25) continued to meet full diagnostic criteria for BP-I disorder at the 6-year follow-up point and only were euthymic. Of the 31% (N=17) remaining, 13% to meet diagnostic criteria for subthreshold BP-I disorder, 9% for full major depressive disorder, and 9% for subthreshold major depressive disorder.Conclusion: This 6-year follow-up study shows further evidence for a high level of persistence of pediatric BP-I disorder and its subsyndromal states associated with high levels of morbidity and dysfunction into adolescent and young adult years. The results of this follow-up study contribute further to the literature on the persistence of pediatric BP disorder and the clinical need for early detection and treatment. 7894 Emily H. Feig, PhD, Psychiatry Do positive psychological characteristics relate to health behavior adherence in bariatric surgery patients? A survey study E.H. Feig1,2, S. Kim1, D. Smith1 and J.C. Huffman1,2 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA and 2Psychiatry, Harvard Medical School, Boston, MA, USA Introduction: Obesity is a major public health concern, and weight-loss surgery (WLS) is the most effective treatment for severe obesity. However, even with surgery, adherence to health behaviors including physical activity, a high protein low calorie diet, and vitamin use are critical to long-term weight loss maintenance and avoiding nutritional deficiencies. Unfortunately a significant subset of individuals who undergo WLS do not meet expectations for sustained weight loss, and this is often attributable to a lack of adherence to these important health behaviors. Positive and negative emotions may play an important role in health behavior adherence. The purpose of this study is to examine relationships between positive and negative psychological factors, health behavior adherence, and weight loss in people who have undergone WLS.Methods: Data are being collected via an online REDCap survey posted on several WLS online support forums. Psycho- logical measures include the Life Orientation Test - Revised (measuring optimism), the Positive and Negative Affect Scale, the Hospital Anxiety and Depression Scale, the Self-Efficacy for Exercise Scale, the Barriers to Being Active Quiz, and the Weight Bias Internalization Scale - Modified. Behavioral measures include the Bariatric Surgery Self-Management Questionnaire (measuring adherence to surgery-specific behaviors) and the International Physical Activity Questionnaire - Short Form (measuring physical activity in the past week). We expect a total sample size of 150 by the Clinical Research Day. Multiple regression analyses tested (1) the relationships between positive and negative psychological measures and behavioral adherence, (2) the relationships between positive and negative psychological measures and weight loss, and (3) the relationships between behavioral adherence and weight loss. Time since surgery was included as a covariate for all analyses, and body mass index (BMI) at the start of the surgery process was included as a covariate for analyses including weight loss as an outcome. All analyses were completed with Stata version 15. It was hypothesized that positive emotional factors would be associated with higher health behavior adherence and weight loss, and negative emotional factors would be associated with lower health behavior adherence and weight loss.Results: The sample so far consists of 88 adults (M age = 45, SD = 11), who are 93% female and 88% white. Eighty-three percent of respondents have had surgery while the rest are considering WLS. In those who have had surgery, it occurred on average 2.4 years ago (range from 3 days ago to 17 years ago). Post-surgical participants started the surgery process with a BMI of 47 on average (SD = 8), had surgery at a BMI of 44 (SD = 8), and reported a current BMI of 36 (SD = 9). They have lost 83 lbs since starting the surgery process (SD = 49) on average, and had regained on average 6 lbs (SD = 17) from their lowest post-surgical weight. There was a trend toward a positive relationship between optimism and adherence (b = 0.034, p = 0.070), and positive affect was positively associated with adherence (b = 0.57, p < 0.001), controlling for time since surgery. These relationships became nonsignificant after controlling for depressive symptoms. Negative affect was negatively associated with adherence, (b = -0.39, p = 0.027), as we depression (b = -1.20, p < 0.001) and anxiety (b = -0.63, p = 0.007), controlling for time since surgery. Weight bias internalization, self-efficacy for exercise, and barriers to being active were unrelated to adherence. A trend toward a positive association between positive affect and physical activity was seen (b = 318.17, p = 0.058), but this no longer held after controlling for depression. Negative affect, optimism, depression, anxiety, self-efficacy for exercise, barriers to being active, and weight bias internalization were unrelated to physical activity, controlling for time since surgery. As for weight loss, of all psychological and behavioral measures tested, only weight bias internalization was associated with less weight loss (b = -8.88, p = 0.002), controlling for time since surgery and starting BMI. Conclusion: These preliminary results suggest that both positive and negative psychological measures are related to health behavior adherence, although the negative factors (e.g., depression) may be stronger drivers of the relationship. Contrary to hypotheses, only weight bias internalization was associated with weight loss. Emotional factors clearly play a role in behavioral and health outcomes after WLS. Future research should further disentangle these different emotional constructs. Interventions targeting emotional factors may be helpful to improve health behavior adherence and weight loss after WLS.7995 Amy G. Fiedler, MD, Surgery - Cardiac Low Partial Pressures of Oxygen in Circulatory Death Donors is Associated with Decreased Survival in Lung Transplant Recipients A.G. Fiedler1, A.L. Axtell1, M.A. Villavicencio1 1Surgery, Massachusetts General Hospital, Boston, MA, USA and 2Pulmonary and Critical Care Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Brain dead donor (DBD) lungs have been increasingly utilized from donors meeting \"expanded criteria.\" The utilization of expanded criteria as applied to circulatory death donors (DCD) has not been extensively studied. We sought to determine if donor age, smoking, or partial pressure of oxygen in DCD negatively impacts survival. Methods: A retrospective analysis utilizing the UNOS database was performed on 230 patients who received a lung transplant from a controlled DCD between 2001 and 2015. Patients were analyzed considering the following risk groups: donor age>45 years, donor smoking>10 pack years, and pre-death donor PaO 2<300mmHg. Baseline characteristics were compared and analyzed using a Cox regression model.Results: There were no significant differences in recipient or donor characteristics between groups. Post-transplant survival for patients receiving a DCD with a PaO 2<300mmHg was reduced by 5.1 years (9.5 vs 4.4 years, p=0.02) when compared to those receiving a DCD with a PaO2>300mmHg. On multivariable analysis, donor PaO2<300mmHg was an independent predictor of mortality (HR 1.94, p=0.03). On stratified subanalysis by PaO2 level, a cutoff greater than 400mmHg was associ - ated with the greatest long term survival. When independently considering donor age and smoking, neither had a significant effect on overall survival. Conclusion: DCD PaO2 less than 300mmHg is associated with decreased survival. These results suggest that DCD lungs with a PaO2<300mmHg should not be routinely accepted for transplantation and that a PaO2>400mmHg should be targeted. Further investigation into the utility of recruitment maneuvers to improve oxygenation is warranted to potentially expand the DCD pool. Figure 1: Overall Post-Transplant Survival Kaplan Meier survival curves after lung transplantation. Log-rank test between groups, p=0.02. 96 Phoebe Finneran, BS, Medicine - Cardiology Assessment and Utilization of Patient-Reported Outcome Measures to Improve Cardiovascular Disease Prevention P. Finneran, T. Sanborn, X. Guo, K. Traynor, S. Kathrisean and P. Natarajan Cardiovascular Research Center, Massachusetts General Hospital, Boston, MA, USA Introduction: The American Heart Association's Life's Simple 7 (LS7) are measures of clinical factors and health behaviors associated with improved cardiovascular health. The prevalence of these factors among patients cared for by cardiovascular and cerebrovascular specialists in an academic tertiary-care hospital is unknown. We sought to quantify this health gap and implement interventions based on patient-reported outcome measures.80Methods: Patients cared for by cardiovascular and cerebrovascular specialists at Massachusetts General Hospital were surveyed for the LS7 between May 2014 and March 2016 in clinic waiting rooms. We assessed the prevalence of optimal health factors. \"High-risk\" patients (i.e., recent smokers, or ASCVD and not taking an antiplatelet or statin) and their physicians were contacted by a risk factor nurse to address high risk health-related behaviors. Lastly, we identified individuals with metabolic syndrome who reported inadequate diet and physical activity. We recruited 12 individuals to participate in an 6-mo lifestyle modification pilot with wearable sensor monitoring and nursing-led coaching. Changes in cardiometabolic risk factors were assessed. Results: Surveys for 5950 patients were collected. Only 730 (14%) achieved ideal status for at least 5 factors. Out of 404 recent smokers, 94 (23%) were successfully contacted, and 71 (75%) of those contacted accepted support. Of those with ASCVD, 868 (25%) reported not taking a statin or antiplatelet, or both. The discrepancies for 559 (64%) of these patients were successfully reconciled or contraindication was confirmed. Of the 12 patients that participated in the pilot program, 75% experienced weight loss (mean -8.9 lbs P=0.03). In secondary analyses, participants experienced a reduction in BMI (P=0.02), % body fat (P=0.02), waist circumference (P=0.02), and systolic blood pressure (P=0.02). Conclusion: Despite largely receiving guideline-appropriate care, the prevalence of ideal clinical and behavioral cardio-vascular risk factors among patients cared for by cardiovascular and cerebrovascular specialists remains extremely low. Assessment of patient-reported outcomes facilitates scalable interventions to improve cardiovascular disease prevention. 97 Adam S. Fisch, MD, PhD, Pathology Biallelic EGFR Massachusetts General Hospital, Boston, MA, USA and 2Hematology-Oncology, Massachusetts General Hospital, Boston, MA, USA Introduction: Epidermal growth factor receptor (EGFR) amplification is an acknowledged and well-characterized oncogenic event in several cancers. While assessment of gene amplification at the cellular level is routinely available and gene-to-copy number (GCN) cutoffs have been hybridization (FISH), biallelic EGFR amplification patterns have not been widely examined.Methods: We re-examined a case series of all 355 FISH assays testing for EGFR amplification from September 2016 to February 2018. We included the primary tumor site, the number of EGFR and CEP7 signals, the GCN-ratio, and at least 2 representative images of tumor nuclei. We assessed nuclei with at least diploid (2) CEP7 signals. Based on the distribu- tion pattern of EGFR signals to CEP7 signals we distinguished three patterns: presence of \"biallelic\" amplification as two distinct groups of EGFR clustered around respective CEP7 signals (Figure 1A); \"monoalleleic\" amplification with EGFR signals clustered in proximity to a single distinct CEP7 (Figure 1B); \"indistinct\" encompasses diffuse EGFR amplification, amplification too to count (GCN > 25), amplification with underlying polysomy, and cases where CEP7 signals were too close to distinguish nuclear territories (Figure 1C). Results: A total of 69 EGFR-amplified cases (n = 38 CNS, 23 lung, 5 gastrointestinal, 3 other tissues including larynx, bladder, and unknown origin) were reviewed, and an overview of the amplification pattern is shown in Table 1. Notably, two cases of biallelic EGFR amplification occurred in EGFR-mutant lung cancer patients at of progression on tyrosine kinase inhibitor (TKI) therapy (re-biopsy specimen). Review of the original samples showed no amplification and next-generation sequencing data showed, when corrected for cellularity, a ~1:1 estimated ratio of mutant to wild-type alleles. These findings indicate that wild-type and mutation-specific EGFR amplification observed in 36% of our series, it is more common than anticipated. The pattern is generally not captured when reporting GCN ratios. Our data provide a starting point to explore the clinical relevance of biallelic EGFR amplification as a biomarker - and in particularly the relevance in the setting of TKI resistance. Table 1: Positive cases of EGFR amplification stratified by tissue type and nuclear EGFR amplification pattern. 8198 Elizabeth Fischer, MPH, Psychiatry Care Online Engagement and Possible Predictors of Success P. Pedrelli2,3, S. Sprich2,3, E. Fischer1, S. Wilhelm2,3 and S. Rauch5 1Center for Population Health, Partners HealthCare, Somerville, MA, USA, 2Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 3Harvard Medical School, Boston, MA, USA, 4McLean Research Programs, McLean Hospital, Belmont, MA, USA and 5President's Office, McLean Hospital, Belmont, MA, USA Introduction: Care Online is a program offered by Partners' Center for Population Health behavioral health initiative, adapted from an internet-based Cognitive Behavioral Therapy program validated by McLean Hospital for patients with mild to moderate depression. The program, originally developed at the University of New South Wales, consists of six lessons, each of which teach skills, such as behavioral activation, assertive communication, sleep hygiene, and reframing thoughts. It was launched in October 2016 at three hospitals in the Partners network. Among Care Online patients who completed at least five lessons, the program has been shown to be associated with an average reduction of depressive symptoms, measured by the Nine-Item Patient Health Questionnaire (PHQ-9), of 50%. However, the program is also associated with a high dropout rate; there is 67% attrition from enrollment to completion of lesson five and 70% attrition from enrollment to program completion. Through this analysis, we sought to identify characteristics of patients who are successful in this program to further adapt program rollout and improve adoption.Methods: Patients included in the study population initiated the program between October 1, 2016 and June 30, 2018. We compared patients who completed at least five of the six lessons to patients who initiated at least lesson 1 but then did not interact with the Care Online software in over one month. Patients who either never logged in to access the program or chose not to participate were excluded from the analysis. We examined patient age, gender, marital status, education level, employment status, reasons why patients were referred for Care Online, severity of depressive symptoms at baseline as measured by PHQ-9, severity of anxiety symptoms at baseline as measured by the Generalized Anxiety Disorder Seven-Item Scale (GAD-7), and whether patients initiated treatment with an antidepressant medication within four weeks before enroll- ment in the program, to identify differences between patients who complete at least 5 of the 6 lessons and those who did not. Results: There were no differences between completers and non-completers across all characteristics. Overall both completers and non-completers were largely female (79% of non-completers and 70% of completers), between the ages of 45 and 64 (46% of non-completers and 40% of completers), married (48% of and of completers), and employed full time (44% of non-completers and 50% of completers). We found no statistically significant differences among patients referred to the program for depression treatment and those referred for other reasons. Further, there were no significant differences in mean scores on the pre-treatment PHQ-9 (non-completers' and no relationship between PHQ-9 scores and completion.Conclusion: Care Online is a low-cost, highly effective treatment program for patients with low to moderate depression. Patients who participate and find success free up our limited primary care and psychiatry resources for other patients for whom this type of self-directed, online treatment is not appropriate. It is likely that the differences between these two popula-tions are not easily identifiable quantitatively, or difficult to measure, such as treatment expectancies or levels of motivation. Therefore, we will continue to investigate differences between these two populations qualitatively, which will enable us to better identify appropriate patients, market our program, and update our workflow to expand the reach and enhance the effectiveness of Care Online. 99 Maura Fitzgerald, MPH, Psychiatry Evidence of Poor Patient Engagement in Stimulant Treatment for ADHD: Electronic Medical Records Data Mining Study from a Large Health Care Organization M. Fitzgerald2, R. Fried1, R. Perlis1, Y. Woodworth2, I. Biederman2 Biederman1 1Psychiatry, MGH/Harvard, MA, USA and 2MGH, Boston, MA, USA Introduction: ADHD is a prevalent and morbid neurobiological disorder that has been associated with a wide range of adverse outcomes. Data from large datasets document that stimulants decrease the risks for many adverse outcomes, yet compliance with stimulants remains very poor. A growing body of evidence shows that patients who engage with their health care providers have much better clinical outcomes; therefore, the rate of renewal of the first stimulant prescription in new patients initiating treatment is an important objective metric of adherence. The main aim of the present study is to evaluate objective rates and correlates of patient engagement in ADHD stimulant treatment82Methods: Prescription and demographic data were extracted from the Partners HealthCare Research Patient Data Registry (RPDR) for patients 4 to 17 years of age who were prescribed a CNS stimulant between January 1, 2015 and December 31, 2016. ADHD patient engagement was systematically evaluated by examining the rate of renewal for the first prescription for stimulant medication within this period. We defined patient engagement in stimulant treatment as having at least one initial prescription refilled within 90 days.Results: Of the 2,392 individuals who were prescribed stimulants, only 1,070 (45%) refilled their initial prescription. There were signiciant differences in age, stimulant formulation and prescription source bewteen those who were and were not engaged in treatment. Prescription source (psychiatry clinic vs. non-psychiatry clinic) was the only characteristic that remained a significant predictor of patient engagement when controlling for all other demographic and treatment character - istics (OR=1.67, 95% CI: 1.34, 2.08; p<0.001).Conclusion: Results of this study support the hypothesis that patient engagement in stimulant treatment for ADHD is extremely poor. A review of electronic medical record (EMR) data from a large healthcare organization showed that only 45% of 2,392 patients engaged in treatment within 90 days of their initial stimulant prescription. These findings provide compelling evidence for poor rates of patient engagement in stimulant treatments for ADHD and are consistent with findings reported in recent literature review. 100 Michael R. Flaherty, D.O., Pediatrics Evaluation of a Nurse-Driven Albuterol Weaning Protocol for the Management of Status Asthmaticus in the Pediatric Intensive Care Unit M.R. Flaherty, O. Alshareef, K. Whalen, J. Lee, P. Yager and B. Cummings Pediatric Critical Care Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Asthma is the most common chronic disease of childhood in the United States, affecting 6.2 million children with over 135,000 hospital admissions per year. Clinical protocols for the treatment of asthma help to reduce practice variation and to improve the quality of care in inpatient units, but little is known about standardized management in the intensive care unit. The aim of our study is to evaluate the impact of a quality improvement intervention, implementation of a nurse-driven albuterol weaning protocol, on patient outcomes in the pediatric intensive care unit (PICU). Methods: A retrospective analysis of children admitted to the PICU before and after the implementation of a continuous albuterol weaning protocol. Patients 12 months of age or older with documented diagnosis of asthma or respiratory distress who were treated with continuous albuterol nebulization were included. Patients were excluded if they required intubation, bilevel positive airway pressure, escalation to terbutaline or theophylline, had underlying cardiac disease, chronic lung disease or were in impending respiratory failure.Results: A total of 56 patients (32 in the pre-implementation group and 24 in the post-implementation group) were evaluated. There was a 51.6% adherence rate to the protocol. After protocol implementation, there was no difference in the median time on continuous albuterol (36 [19, 60] vs. 28.6 hours [15.3, 56.3], P=0.68) and median time to de-escalation to every (38.7 [20.2, 57.3] vs. 32.1 hours [17.4, 49.5], P=0.49). There was no difference in the total dose of albuterol received (300 [157.2, 592] vs. 270 mg [133, 589.8], P=0.68) in the pre-implementation and post-implementation groups respectively. The ICU length of stay was significantly reduced by 1 day in the post-implementation group (3 [2.75, 4] vs. 2 days [1, 2], In an early analysis of a nurse-driven albuterol weaning protocol with relatively low adherence, a reduced time on continuous albuterol was not yet observed, but reduced ICU length of stay was seen. Adaption and maximizing adherence may be helpful in reducing asthma ICU length of stay. 101 Morgan Fogarty, Athinoula A. Martinos Center for Biomedical Imaging Visualization of human brain cytoarchitecture: a comparison between Optical Coherence Tomography/Microscopy and Histology M. Center, Massachusetts General Hospital, Charlestown, MA, USA, 2Department of Anatomy and Neurobiology, Boston University School of Medicine, Boston, MA, USA and 3Computer Science and AI Lab, MIT, Cambridge, MA, USA Introduction: In defining diagnosis and understanding neurological diseases, it is essential to study the brain's cytoarchi - tecture postmortem. Traditionally, histology is used to examine neurons and axons in the brain; however, this introduces distortion artifacts. Examples of these distortions include tears, shrinking and mispositioning. Optical coherence tomography (OCT) and optical coherence microscopy (OCM) greatly reduce distortions due to imaging the tissue prior to cutting, as well as using the tissue's intrinsic optical properties to provide the contrast, eliminating the need for staining. In this study, we compared OCT and OCM to Nissl staining to illustrate the similarities of the two techniques as well as the differences in distortions in various cortical layers and area. Methods: OCT/OCM measures the light backscattered by the tissue, in a similar way to that of ultrasound. The resolu- tion, 3.5\u00b5m (OCT) and 1.25\u00b5m (OCM), makes it ideal for imaging cortical layers in large blocks of tissue and individual neurons respectively. Each sample was imaged with OCT and/or OCM, and sectioned into 50\u00b5m slices for Nissl staining. Afterwards, both linear and nonlinear registration between OCT/OCM to the digitized Nissl stained slices, layer labeling, and neuron segmentation were used for the qualitative and quantitative comparison of OCT/OCM to Nissl images. The distortion reduction was assessed by overlapping the blockface photo (true geometry of the tissue) to the OCT and Nissl images of the samples. The labeling of the OCT images highlighted that layers observed in OCT correspond to the layers observed in the Nissl stain. As for OCM, the segmented neurons were used for visualizing the correspondence between neurons in the OCM and Nissl images. For these neurons, colocalization matrices and percentages were used to assess the similarities in OCM and Nissl images. Results: We show that imaging prior to sectioning reduces distortion therefore increasing the colocalization of the blockface image reference from 94.6% for the Nissl to 99.1% for the OCT. Figure 1 validates the change by visually comparing the blockface (Fig. 1A), Nissl (Fig. 1B), and OCT (Fig. 1C) images. The red and green arrows on the Nissl image (B) point to an overlap and tear, respectively. We also confirm that OCT detects the laminar structure of the cortex as seen with Nissl (Fig. 1D). For OCM, the average neuron colocalization percentage in BA 21 was 60% for OCM (colocalized neurons / neurons in OCM image) and 52% for Nissl (colocalized neurons / neurons in Nissl image) and in BA 32 the percentages for OCM and Nissl were 67% and 47% respectively. The average colocalization percentages were calculated across three cases and four layers (layer II, III, V, and VI) for both BA 21 and BA 32. Figure 2 shows the OCM, Nissl, and overlap of the segmented neurons for layers III and VI in BA 21 and layers V and III in BA 32 by displaying the neurons in the OCT data as red, Nissl as green, and the overlap in yellow. We show that the performance of the OCM is dependent of the tissue integrity and composition, ie. its cell density and size as well as the level of myelination.Conclusion: Both results demonstrate that OCT and OCM are promising tools to the studies of cytoarchitecture, from the laminar structures to the individual neurons. Moreover, due to the limited distortions, OCT can be used to create undistorted 3D volumes of tissue without requiring between-slice registration, and the time and labor intensive segmentation process can be improved with automatic segmentation algorithms. Future studies using OCT and OCM can further our understanding of the development of neurological diseases and their impact on the brain.84 Figure 1 Figure 2 102 Carla A. Fortes-Monteiro, Bachelor of Arts in Biology., Neurosurgery How Do Children Compare to Adults as Research Subjects? C.A. Fortes-Monteiro1, J. Shen1, E. Evans1,2 and A. Duhaime1,3 1Neurosurgery, Massachusetts General Hospital, Quincy, MA, USA, 2MGH-Institute of Health Professions, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: Transforming Research and Clinical Knowledge in Traumatic Brain Injury (TRACK-TBI) is a multicenter observational longitudinal study in which detailed clinical data from subjects of all ages with injuries of all severities are collected and analyzed. Additionally, blood biospecimens, CT and MR imaging, and detailed neurocognitive outcome assess- ments are collected over one year with the goal of better understanding the multidimensional factors influencing physical, cognitive and emotional outcomes. Because the study enrolls patients of all ages, we wished to learn how pediatric subjects compared to adults as research subjects with respect to study participation and completion of study milestones.Methods: TRACK-TBI at our site has enrolled 272 subjects from various social and economic backgrounds with ages ranging from 0 to 87 years old. Subjects receive stipends totaling approximately $700 paid in installments after each completed milestone. Data were extracted from the TRACK TBI database and followup rates and milestone completion were analyzed for pediatric (0-17 years) and adult subjects. We also estimated coordinator effort required by analyzing the number of telephone contacts needed to schedule a successfully completed followup visit. Statistical analysis was performed using IBM SPSS Statistics for Windows, Version 25.0.0.1 (Armonk, NY: IBM Corp). Medians, interquartile ranges and percentages were used to describe the sample. Comparisons between pediatric and adult subject were made using the chi-square test of homogeneity for categorical variables, and Mann-Whitney U tests for continuous variables.Results: Patients were excluded if they withdrew from the study, expired, or had not yet met the 12 month followup timepoint. There were 100 pediatric (median age 8 years, 62% male) and 102 adult (median age 43 years, 56% male) subjects for analysis. 48% of pediatric subjects were admitted to the Intensive Care Unit (ICU), 42% were admitted to a non-ICU unit, and 10% were discharged from the Emergency Department (ED). Of the adult subjects, 24% were admitted to the ICU, 25% were admitted to non-ICU units, and 52% were discharged from the ED. At the 2 week, 3 month, and 6 month followup timepoints, neurocognitive assessment completion rates were comparable between the two subject groups (pediatric 86%, 79%, 71%; adults 87%, 72%, 69% respectively). Pediatric subjects a significantly higher 12-month neurocognitive assessment completion rate (70%) than adults (55%) (p=0.03). Pediatric subjects were more likely to complete all 4 followups (55%) compared to adult subjects (43%). The number of phone calls required to schedule a completed followup visit was lower for children than adults for the overall study (p=0.03) and per each followup timepoint (p=0.01). Conclusion: Few single studies enroll patients across the entire age spectrum. Since more adult subjects were discharged from the emergency department, we anticipated that followup completion rates might be higher for adults, as they might have fewer conflicts related to injury recovery or followup care requirements. Additionally, research in children is sometimes assumed to be more difficult than research involving adults for the same disease process. However, the current findings demonstrate that pediatric patients had higher followup rates and required less effort from research staff to successfully complete study milestones. Besides the obvious gain in scientific knowledge about children, these results suggest that there is no detriment to including pediatric patients in combined longitudinal research studies, and in fact that their inclusion may be beneficial for study goals.85Table 1. Completed Followups and MRI Pediatric vs. Adult Participants Chi square test results, comparing pediatric (<18 years) or adult participants who completed outcome assessments at 2 weeks, 3 months, 6 months, and 12 months. Participants from each category who completed all four follow-ups. Results comparing pediatric/adult participants who completed MRIs at 2-week and 6-month follow-ups, Including only participants from the initial CA-MRI cohort (Subject who agreed to obtain MRI at 2-week follow-up)Table 2 - Contact Rates for Follow-ups Pediatric vs. Adult Participants *Includes only participants with >0 follow-ups Results of Mann- Whitney U tests comparing children and adults on: number of contacts, number of follow ups, and median number of contacts per follow up (only includes subjects with at least 1 follow up) 103 Melanie Freedman, BS, Psychiatry Factors affecting health behavior adherence in heart failure: the REACH for Health study qualitative research phase M. Freedman, J. Huffman, C. Mastromauro, E. Beale, E. Park and C. Celano Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: Adherence to health behaviors, including physical activity, medications, and a low sodium diet, are critical for patients with heart failure (HF); however, these patients frequently struggle to adhere to these health behaviors. Psychological states have been associated with adherence to health behaviors but have had limited study in patients with HF. We aimed to identify barriers to health behavior adherence and associations between psychological states and adherence in patients with HF.Methods: We performed semi-structured, qualitative interviews with 30 patients with moderate HF during a medical hospitalization, then again 3 months later. Interviews focused on current health behaviors, barriers to these behaviors, and the associations between psychological states and health behavior adherence. All interviews were professionally transcribed, and content analysis was performed in NVivo 8.Results: Participants noted the greatest difficulty with increasing physical activity and attributed this to medical (e.g., HF symptoms), environmental (e.g., weather), social (e.g., lack of social support), and psychological (e.g., fear of worsening symptoms) factors. Barriers to a low sodium diet included inconvenience, the complexity of dietary recommen - dations, and lack of knowledge about how to reduce sodium intake. Medication nonadherence was attributed to forgetfulness and side effects. Participants also noted links between psychological states and adherence. Negative psychological states could serve as motivators (e.g., through fear) or inhibitors of adherence. In contrast, positive psychological constructs (e.g., connectedness, determination) consistently led to improved adherence.Conclusion: Both physical and psychological factors may influence health behavior adherence in patients with HF. Interventions that combine psychological and behavioral approaches may be well-suited to increasing adherence in this high-risk population. 104 Esther E. Freeman, MD PhD, Dermatology Novel point of care diagnostic strategies for R. Anderson1 and D. Kang5 1Dermatology, Massachusetts General Hospital, Boston, MA, USA, 2Medical Practice Evaluation Center (MPEC), Massachusetts General Hospital, Boston, MA, USA, 3Infectious Disease Institute, Kampala, Uganda, 4Memorial Sloan Kettering, New York, NY, USA and 5University Tucson, AZ, USA Introduction: AIDS-related Kaposi's sarcoma (KS) continues to have a high burden and poor survival in sub-Saharan Africa. One reason for poor survival is late diagnosis. In resource-poor settings, challenges in obtaining histopathologic diagnosis contribute to diagnostic delay. Novel point of care cancer diagnostics during HIV care visits could reduce delay. We developed a low-cost smartphone portable confocal microscope with the aim of visualizing cellular details of the skin at point of care, and assessed its feasibility for use among patients with suspected KS at the infectious Diseases Institute (IDI) Uganda.Methods: The novel smartphone confocal microscope images the skin through a series of lenses displaying them on a smartphone in real-time (Figure 1a). The device can be used in vivo on intact skin and ex vivo on skin that has been biopsied. At an HIV care center, among HIV-infected patients with suspected KS referred for a biopsy, we performed both in vivo and ex vivo imaging to evaluate cellular features in the epidermis and deeper in the dermis.86Results: Both in vivo and ex vivo images for 130 patients were taken using the portable confocal microscope. Imaging took <15 minutes to complete. The device cost 10-20 times less than a full-size confocal microscope. In vivo confocal images visualized characteristic cellular features of the skin and capillaries (1b), but the best quality images were limited to the epidermis and the dermoepidermal junction due to light scattering in darker skin tones. Ex vivo imaging, in contrast, was able to visualize cellular nuclei in epidermis and dermis (including collagen and adipocytes), as well as irregular capillary patterns, which may be indicative of KS (1c).Conclusion: We developed a low cost portable confocal microscope for use in resource-limited settings, which was able to detect cellular features of skin similar to those from traditional dermatopathology. Ex vivo confocal microscopy performed on skin biopsy samples shows promise as a possible tool for rapid point-of-care KS diagnosis. This study highlights the feasibility of developing low-cost point of care diagnostic tools for HIV-associated cancer as a method to integrate HIV and cancer care in resource poor settings. Figure 1: Photo of the portable confocal microscope (A) and confocal images of human skin in vivo (B) and ex vivo (C). B - image visualizes - Ex vivo confocal image reveals capillaries of irregular sizes and shapes, indicative of skin tumor. 105 Monika C. Gabriele, Undergraduate, Orthopedics Are Mental Disorder Diagnoses Associated with Readmissions and Patient-Reported Outcome Measures after Orthopaedics Laboratory, Boston, MA, USA and 2Department of Orthopaedic Surgery, Harvard Medical School, Boston, MA, USA Introduction: In recent years, payment systems for total hip arthroplasty (THA) have shifted from volume- to value-based. Such systems often use bundled payments, whereby the provider is incentivized to minimize costs by reducing complications and improving patient-reported outcome measures (PROMs). There is concern, however, that providers will be disincentiv - ized to treat patients predisposed to complications and low PROMs due to preexisting conditions. These conditions must be identified to enable adequate risk-adjustment in bundled payments. One factor that has been hypothesized to affect outcomes after THA is mental health. While some studies have examined patient-reported mental disorder symptoms and THA, the effect of physician diagnosed mental disorders on outcomes remains largely unexplored. The primary aim of this study was to investigate the relationship between preoperative mental disorder diagnoses on 30- and 90-day readmissions in patients undergoing primary THA. Our secondary aim was to investigate the relationship between mental disorder diagnoses and PROMs after THA.Methods: We queried an institutional database from a tertiary academic medical center for all primary, unilateral THA patients from 2000-2016. Data obtained included age at surgery, sex, comorbidities, mental disorder diagnoses, all-cause readmissions, and three PROMs: the Harris Hip Score (HHS), a numerical rating scale (NRS) for pain, and a NRS for satisfac - tion. The HHS and NRS pain were administered pre- and post-operatively; NRS satisfaction was assessed post-operatively. Preoperative medical comorbidities were compiled to obtain a composite score, the Charlson Comorbidity Index (CCI). Each diagnosis was categorized by the International Classification of Diseases 10 th Edition (ICD-10). Mental disorder catego- ries encompassing less than 300 patients were excluded from our analyses, leaving the following disorder categories: mental and behavioral disorders due to psychoactive substance use; mood affective disorders (including depression); and neurotic, stress-related, and somatoform disorders (including anxiety) (Table 1). For the three mental disorder categories analyzed, 87a propensity score was generated for each patient based on age at surgery, sex, and CCI and used to create a 1:1 matched population within a 0.1 matching threshold. These matched populations by category formed the final populations on which we performed our analyses. NRS postoperative pain and pain change were each dichotomized using a threshold of 2 points based on the literature-defined patient acceptable symptom state (PASS) and minimal clinically important improvement (MCII) respectively. NRS satisfaction was dichotomized using a threshold of 2 points. These outcomes, in addition to 30- and 90- day readmissions, were analyzed using a binary logistic regression. Postoperative HHS and HHS change were analyzed using a linear regression model. Each model was adjusted for age, sex, and CCI. Results: Of the initial 4408 primary, unilateral THA patients identified in the database, 204 (4.62%) were excluded due to incomplete demographic data, yielding a final cohort of 4204 patients (Table 1). None of the three mental disorders were associated with 30-day readmissions (p>0.069), but each disorder was associated with readmissions at 90-days (p<0.026), compared to patients without a diagnosis. Each disorder was associated with lower pre- and postoperative HHS (p<0.010) compared to patients without a diagnosis, except for the substance use and neurotic, stress-related, or somatoform disorder, which were not associated with a lower preoperative HHS (p=0.472). None of the three mental disorders were associated with the magnitude of HHS improvement (p>0.303). Each disorder was associated with not achieving the NRS pain PASS compared to patients with no diagnosis (p<0.006). None of the three mental disorders was associated with the likelihood of achieving the pain MCII (p>0.353). Patients with a neurotic, stress-related, or somatoform disorder were the only group more likely to be dissatisfied with the result of their THA(p=0.025, odds ratio =1.395) (Table 2). Conclusion: This study highlights a crucial intersection within the holistic evaluation and care of patients. In a physician's conversations with a patient suffering from hip osteoarthritis as well as a mental health disorder, there should be a strong consideration of concurrent mental health treatment alongside THA. Further, to enable proper risk-adjustments within bundled payment systems and to protect patients predisposed to complications and low PROMs, mental disorders should be recognized as significant conditions. Table 1: Patient population numbers and demographic data by mental health category. Table 2: Analysis of outcome results for mental health categories analyzed 106 Smitha Ganeshan, B.S., Medicine Clinical Characteristics and Utilization Patterns of Post-Incarcerated Patients at CCC-Chelsea S. Ganeshan1, M. Cohen3 and B. Blanchfield2 1Department of Medicine, Harvard Medical School, Cambridge, MA, USA, 2Brigham and Womens Hospital, Boston, MA, USA and 3Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: The objective of this study is to compare the baseline clinical characteristics and healthcare utilization of individuals released from prison (IRP) receiving care at Crimson Care Collaborative (CCC) - Chelsea, a student-faculty collaborative clinic (SFCC), to all other patients receiving care at CCC-Chelsea. Understanding IRP's utilization patterns as they relate to patients with similar socioeconomic status and social factors may serve to identify opportunities for improved care delivery and coordination of care. IRP patients are more likely to suffer from chronic diseases, including HIV, substance use disorders, and diabetes than the general population. After release, these individuals often face barriers accessing basic primary care services, including insurance status, probation or parole requirements, stigma, and other social determinants. Patients are at a 12-fold increased risk of death in the first two weeks following release from prison. Substance use, uninten- tional drug overdose, and suicide attempts amplify this risk. Despite this knowledge, patterns of healthcare utilization among 88the IRP population remain poorly characterized. One study on IRP's healthcare utilization found that patients had an ED visit within 1 month of release, and were more likely to visit the emergency department for ambulatory-care sensitive conditions, mental health disorders, and substance use disorders, which are all better assessed and managed in long-term outpatient settings. Student-faculty collaborative clinics (SFCC), such as Crimson Care Collaborative, offer a unique opportunity to provide a convenient source of primary care. Understanding care utilization among this population is required to better orient care delivery models to meet patient needs.Methods: A seven-year retrospective analysis of CCC-Chelsea patients identified 43 who had been released from state prison or jail prior to seeking care at CCC. Patients with a history of incarceration and at least one CCC-Chelsea visit were included in the study. Patients who enrolled in CCC after calendar year 2017 were excluded from the study. Two sided t-tests were used to compare healthcare utilization between IRP and all other CCC-Chelsea patients. Data was obtained from the Partners Healthcare' Research Patient Data Registry (RPDR).Results: IRP had an average of 9.65 inpatient, 18.85, outpatient, and 4.011 emergency department (ED) visits annually. Average length of stay for inpatient hospitalizations was 3.53. In addition, the most common reasons for inpatient admission in order of incidence include: alcohol withdrawal, COPD, and infection. IRP had on average, 2.83 chronic conditions with 65% having 4+ chronic diseases. Almost half had 4+ mental health conditions, including 56% with opioid use disorder. Average annual IRP ED visits of 4.01, exceeded the average, 3.10 of all other CCC-Chelsea (p<0.05); IRP outpatient visits,18.85, IRP of 5.42, (p<0.05).Conclusion: CCC-Chelsea IRPs utilize healthcare services at significantly higher rates than other CCC-Chelsea patients. Data suggests this may be multi-factorial due to higher burden of chronic disease and mental health morbidity. However, social factors and health-seeking behaviors may also play a causal role. Understanding these patterns of utilization offer CCC opportunities to redesign care to better serve these patients. 107 Anna H. Gao, BSN, Dermatology Tolerability of a tethered capsule endomicroscopy procedure for Esophagus Hospital, Boston, MA, USA, 2Gastroenterology, Massachusetts General Hospital, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: Currently, very little is known about the microscopic natural history of Barrett's Esophagus (BE), a precursor to esophageal adenocarcinoma. To fill this important knowledge gap, we have initiated a study that uses a swallowable, tethered capsule endomicroscope (TCE) to evaluate the progression of Barrett's Esophagus (BE). This 5-center study, which includes MGH, will use TCE to longitudinally image the esophagi of 500 patients with untreated BE over a period of 4 years. Here, we report initial results from the MGH center about study participant tolerability of the device.Methods: Tethered capsule endomicroscopy (TCE) is a novel approach developed by the Tearney laboratory at MGH. Unsedated study participants swallow a TCE capsule that circumferentially scans an Optical Coherence Tomography (OCT) beam inside of the body to obtain microscopic images of the esophagus. Participants were asked to sip water to facili- tate swallowing the capsule. They were also given the option to use \"Pill Glide Swallowing Spray\", an over-the-counter water-based lubricating spray and \"Chloraseptic\", a mild throat numbing spray that reduces the potential for irritation of the back of the throat. Once the imaging procedure was finished the capsule was removed from the esophagus through the mouth. Feedback questionnaires were conducted after the procedure. Questions regarding the level of discomfort during the procedure (10-point scale), recommendation to other patients (4-point Likert-type scale), preference over endoscopy (4-point Likert-type scale), and recommendations to improve the study procedure were asked. Results: 33 participants have been enrolled so far at the MGH site. 26 subjects were able to swallow the capsule. A total of 33 questionnaires were collected from March of 2017 to now. The average overall level of discomfort during the procedure on a scale of 0-10 (0 is no discomfort, 10 is extreme discomfort) was 2.72. 85% of participants would definitely recommend it to other patients if the procedure was approved for clinical use. 58% of participants were extremely likely to prefer it over endoscopy if the procedure was available as an approved alternative to endoscopy. The top three recommendations to improve the study were to make the size of the capsule smaller, the tether softer, and to provide stronger pharyngeal numbing.Conclusion: In this initial cohort of study participants with BE, the TCE device was found to be well accepted and preferable to endoscopy. Based on the feedback obtained from study participants, we are working on ways to improve the participant experience by making the capsule smaller and minimizing the profile of the tether.89108 Lorraine F. Garg, Emergency Perceptions, Facilitators and Barriers of Uterine Balloon Tamponade in Honduras D. Suarez-Rebling1, P. Bernal2 and T.F. Burke1,3,4 1Division of Global Health and Human Rights, Department of Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Inter-American Development Bank, Washington D.C., DC, USA, 3Harvard Medical School, Boston, MA, USA and 4Harvard T.H. Chan School of Pubic Health, Boston, MA, USA Introduction: Postpartum hemorrhage (PPH) is a leading cause of maternal mortality in Honduras. While uterine balloon tamponade (UBT) had been in the national policy since 2010, it has been significantly underutilized. In response, the Inter-American Development Bank and the Honduran Ministry of Health partnered with the Massachusetts General Hospital Division of Global Health and Human Rights to introduce the Every Second Matters - UBT package in Honduras. This study was conducted to understand baseline provider practices, perceptions, barriers and facilitators on UBT.Methods: Prior to training providers on the ESM-UBT package, written and phone surveys, focus groups and semi-structured interviews were performed to collect baseline information. 21 OBGyn Master Trainers (MTs) were interviewed in focus groups and completed written surveys, 4 senior country-level OBGyn MTs were individually interviewed in-person and 7 skilled birth attendants from 4 Maternal and Child Health Clinics (CMIs) completed phone surveys. Transcripts were independently coded by two staff members and discrepancies resolved. Text was analyzed to identify prominent themes.Results: More than 80% of survey respondents considered UBT to be mostly or extremely effective for PPH treatment. However, only 55% of written survey respondents and 14% of CMI survey participants had ever used UBT. Barriers to UBT use included lack of training, unavailable and inaccessible supplies, fear, perceived ease of hysterectomy, cost, concern about wasting time and a culture of performing hysterectomy. Facilitators included provider motivation, experience with UBT, refresher training, and hands on practice. Experience with UBT improved ease of use and assembly time. There was strong support for training CMI staff.Conclusion: The majority of Honduran OBGyn and CMI-staff participants were aware of UBT and considered it effective, however many had never used the technique. Providers identified both barriers and facilitators to UBT use. Consistent training of all provider cadres, use of a pre-assembled UBT kit, and creating opportunities for mentored experience may increase UBT uptake. 109 Jeffrey A. Gelfand, MD, Medicine - Infectious Diseases A Pilot Clinical Trial of a Near-Infrared Laser Vaccine Adjuvant: Safety, Tolerability, and Poznansky2,1 of Massachusetts General Hospital, Boston, MA, USA, 2Vaccine and Immunotherapy Center, Massachusetts General Hospital, Charlestown, MA, USA, 3Department of Dermatology, University of California Irvine, Irvine, CA, USA, 4Beckman Laser Institute, University of California, Irvine School of Medicine, Irvine, CA, USA and 5Emergency Department, Massachusetts General Hospital, Boston, MA, USA Introduction: Vaccination remains the single most important and cost effective medical technology yet developed. Vaccines often require adjuvants to boost their efficacy. Furthermore, intradermal (ID) vaccination has significant advantages over the intramuscular (IM) route of administration, including reduced pain and a more direct route to the immune system via cutaneous dendritic cells (DC's) and draining lymphatics. Recently, a novel adjuvant has been developed to be paired with intradermal vaccination, namely nondestructive laser-light stimulation of the skin. In numerous murine studies, we and others have demonstrated significant changes in cutaneous immune cell trafficking, increased trafficking of antigen to draining lymph nodes, and increased protective effects of ID immunization with the laser adjuvant. Near- infrared wavelength was chosen because light of this wavelength is minimally absorbed by melanin, thus enabling it's use with all skin phototypes. Methods: This study was conceived and designed at MGH. The Beckman Laser Institute/UCI owned the FDA- approved 1064nm derm laser and thus the clinical arm of the study was performed there. Ten healthy individuals (ages 18-50) with no skin abnormalities or immunosuppressive factors were exposed to CW 1064nm laser light over a 5 mm diameter spot on their back for 60 seconds at 0.2 W increments to determine maximal tolerable dose (MTD). MTD was determined by the lowest minimally discomforting dose of the 10 subjects. The MTD was administered and 4 hours later the laser spot & contralateral control spot biopsied. Biopsies were divided in half, kept blinded as to treatment, one half formalin fixed, paraffin embedded (ffp); one half flash-frozen, sent to MGH Dermatopathology. Sections were analyzed for tissue injury, 90immunohistochemical (IHC) analysis for CD1a + Langerhans' cells and CD11c+ dendritic cells in 5 representative high-pow- ered fields. The second half of the biopsy was evaluated at MGH, Vaccine and Immunotherapy Center for RNA expression by qPCR encoding specific cytokines and chemokines. For qPCR results, we ran paired t tests with Bonferroni, stepdown Sidak and false discovery rate corrections on fold-increase values of laser-treated samples to the paired non-treated controls for each gene. Data analysis was conducted using SAS/STAT\u00ae software (SAS Institute) and Prism 7 (GraphPad). All scoring was performed in a manner that was blinded to the observer.Results: Clinical: The Mean Tolerated Dose (MTD) of irradiance for all subjects was 1 W/cm2; at this dose, no subject had discomfort. At a slightly higher dose (0.2 Watts higher), one subject had discomfort. Histopathology and Immunohistopa - thology: Paired control and laser- irradiated biopsies were evaluated by a Dermatopathologist.. After biopsies were scored, data for the 10 subjects were unblinded and analyzed by paired two tailed Student's t-test. CD1a+ Langerhans cells were less numerous within dermis as compared to epidermis in both irradiated and normal control biopsies. Compared to controls, laser treatment was associated with reduction in the number of CD1a+ Langerhans cells in both epidermis (mean \u00b1 SE of 1.6 vs. 36.1\u00b1 1.6, dermis (21.5\u00b1 2.0 vs. 11.8\u00b1 1.6, p-value 0.0001) (Figure 1). Laser treatment led to highly significant, 45% reduction, in Langerhans cells within the dermis, while the epidermal reduction was not signif- icant. Laser treatment was also associated with diminished Langerhans dendritic processes; this morphologic alteration of individual Langerhans cells was selectively observed within the dermal compartment. A significant reduction in CD11c+ dermal dendritic cells (30%) was identified following laser treatment (14.8\u00b11.1 vs 10.4\u00b11.4, p-value=0.0013), although no definite morphological change was seen. No histopathological signs of thermal injury were detected. Expression of Cytokines and Chemokines: Due to the very small size of the biopsies and priority for immunohistochemical analysis, only five pairs of biopsy specimens were evaluable for qPCR analysis. Changes in gene expression in the laser-treated biopsies showed substantial fold-increases in CCL17 (10.4 +/-4.7s.e.m.), CCL17 (10.4+/-4.7s.e.m.) and CCL20(18.3+/-15.5s.e.m.), as were also seen in prior murine experiments. However, owing to the small sample size evaluable, none of these changes were statistically significant after Bonferroni, Sidak, and False Discovery corrections.Conclusion: Many vaccines require adjuvants to enhance their immunogenicity. Murine studies have validated the potency of laser illumination of the skin as a novel adjuvant for ID vaccination with advantages over traditional chemical adjuvants. We report a pilot clinical trial of low power, continuous wave, near infrared laser adjuvant treatment, representing the first human trial of the safety, tolerability, and cutaneous immune cell trafficking changes produced by the laser adjuvant. In this trial we demonstrated a mean tolerated energy dose of 300 joules/cm2 to a spot on the lower back. Similar to prior murine studies, highly significant reductions in CD1a+ Langerhans cells in the dermis and CD11c+ dermal dendritic cells were observed, corresponding to the increased migratory activity of these cells; changes in the epidermis were not significant. There was no evidence of skin damage. The laser adjuvant was otherwise found to be safe, and has the potential to be a well-tolerated and effective adjuvant for ID vaccination in humans. Left panel laser. Right panel control. CD1a stain. 110 Laura F. Goodfield, Surgery - Trauma Developing a Blood Biomarker Model for Predicting Multiple Infection Episodes Following Blunt Trauma L.F. Goodfield and A. Tsurumi Molecular Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Severe blunt trauma injury renders patients susceptible to multiple infection episodes. The ability to predict specific patients at particularly high risk of infections is highly advantageous, yet remains challenging. Methods: We have previously developed a machine learning pipeline in severely burned patients that identified a blood biomarker panel for identifying patients who are hypersusceptible to multiple infection episodes soon after burn injury. Using a similar approach, we performed secondary analysis of 140 blunt trauma adult ( 16 years) patients from the Host Response to Injury Study (\"Glue Grant\"), prospective, longitudinal multi-center study. Eligible patients were those who had early leukocyte samples obtained within 48 hours since trauma injury, who developed the first infection two days or more after blood sample collection, and who remained in the study (did not die or were discharged) for at least 10 days. Among these 91patients, there were 91 controls who did not develop infection, or developed just one infection episode; and 39 hypersus- ceptible patients who developed multiple (2) infection episodes (MIE) over the course of recovery. We identified a panel of 6 transcriptome probe sets mapping to the following genes: kelch repeat and BTB domain containing 7 (KBTBD7), zinc protein 354A (ZNF354A), butyrophilin subfamily 3 member A1 (BTN3A1), T (TRD), cAMP responsive element modulator (CREM), SKI proto-oncogene (SKI).Results: Our logistic regression model developed with the biomarker panel was highly predictive of MIE, with Area Under Receiver Operating Characteristic Curve (AUROC) [95% CI] of 0.89 [0.93-0.95]. This model significantly outperformed various models based on clinical severity scores, including Acute Physiologic Assessment and Chronic Health Evaluation (APACHE) II, with AUROC of 0.62 [0.52-0.72], Injury Severity Score (ISS), with 0.61 [0.51-0.71], and New Injury Severity Score (NISS), with 0.59 [0.50-0.70]. Gene Ontology analyses of up- and down-regulated genes comparing control and hypersusceptible patients showed early alterations in various immune-related pathways, as expected.Conclusion: We conclude that early blood biomarkers may be an effective tool for early triage of blunt trauma patients. Given that clinical injury severity scores lacked the ability to sufficiently predict infections outcomes in this cohort, developing tools based on genomics may lead to the development of novel preventative and therapeutic approaches against infections. 111 Gabrielle Grant, Surgery - Burn Assessing Family Outcomes: A Systematic Review of Generic and Burn-Specific Family Outcomes Instruments and Subscales Children- Boston, Boston, MA, USA, 2Department of Health Law, Policy, and Management, Boston University School of Public Health, Boston, MA, USA, 3Harvard Medical School, Boston, MA, USA, 4Department of Physical Medicine and Rehabilitation, Spaulding Rehabilitation Hospital, Charlestown, MA, USA and 5Department of Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: As part of a study to expand upon the Burn Outcomes Questionnaire 0-4 Family subdomain, we conducted a systematic review of generic and burn-specific family outcomes instruments and subscales. Utilizing the Institute of Medicine's (IOM) framework outlining family influences as a conceptual guide, we conducted a review of family outcomes literature to evaluate the current state of family outcomes research and identify gaps in their assessments. Family influences, including parenting, family learning environment, parental mental health and parental substance abuse, are major factors affecting normal child health and development. This review identifies and summarizes family outcomes instruments and sub-scales, and describes their content. Further research is needed to assess the impacts of family outcomes as they relate to children with burn injuries. Methods: We conducted a literature review of generic and burn-specific family outcomes instruments and subscales via PubMed, Web of Science, and a manual reference check; and consulted with clinical experts. Our data collection efforts focused on identifying generic and burn-specific instruments and subscales that assessed different aspects of family outcomes, including parenting, family learning environment, parental mental health, and parental substance abuse. Using the IOM's Family Influences framework as a conceptual model to guide our review, we evaluated to what extent the literature assessed these four family outcomes subdomains.Results: In total, 10 instruments and 1 family subscale were identified in our literature review, 7 instruments (70%) and the 1 family subscale (100%) were included in our review. Identified instruments and subscales were conceptually linked to the IOM's framework detailing family influences. One instrument fell within the IOM's Parenting subdomain, assessing parenting style and approaches. Two instruments, the Home Observation for Measurement of the Environment Short Form (HOMES-SF) and the Family Environment Scale fell under the IOM's Family Learning Environment subdomain. One subscale of the Survey of Well-being of Children, Family Questions, fell under the IOM's Parental Mental Health and Parental Substance Abuse subdomains. Four instruments did not exclusively fall under any IOM Family Influences subdomain. The Family Adaptability and Cohesion Scale IV (FACES-IV), Family Assessment Device, Family Relationship Index and the PROMIS Parent Proxy Item Bank- v1.0- Family Relationships had items that reflected some aspects of the IOM's Family Learning Environment subdomain but also had many items assessing family relationships and interactions.Conclusion: We systematically reviewed family outcomes instruments and subscales in an effort to expand upon the Burn Outcomes 0-4 Family subdomain. This review highlights the importance of assessing family outcomes in child health and development in children. Our findings will inform the development of family functioning items for a parent-reported, computer adaptive test (CAT) of burn and health outcomes among children 1 to 5 years of age with burn injuries. The develop-ment of burn-specific family outcomes instruments may be an important focus of future research.92112 Sophie L. Greenebaum, BA, Psychiatry Race as a predictor of suicidal ideation and behavior Hospital, Avon, CT, USA, 2T.H. Chan School of Public Health, Harvard University, Cambridge, MA, USA and 3Harvard Medical School, Cambridge, MA, USA Introduction: Previous research indicates an inconsistent relationship between race and suicidal ideation and behavior. Certain research indicates that Caucasian individuals are more likely to report suicidal ideation than people of color (POC), whereas other research supports the opposite. The purpose of this study was to determine this relationship among an online research community of people with mood disorders. Methods: MoodNetwork is an online platform through the National Patient-Centered Clinical Research Network (PCORnet) designed for patients and families of those with mood disorders. PCORnet is a part of the Patient-Centered Outcomes Research Institute (PCORI) that allows patients to actively participate in every aspect of the research process. The current MoodNetwork race breakdown is 84.10% Caucasian and 15.90% POC. Due to the large imbalance, all POC were grouped together for these analyses. Suicidal ideation was measured through a variation of item A3g on the Mini-International Neuropsychiatric Interview (M.I.N.I.) Major Depressive Episode section and the demographics were collected via self-report during the sign-up process for MoodNetwork. T-tests were used to analyze POC and Caucasian group proportion means for suicidal thoughts. The results do not assume equal variances; therefore, we used the Satterthaite-Welch adjustment to calculate degrees of freedom. Results: Based on the M.I.N.I. (n=656, 18.45% male, Caucasian), Caucasian participants reported feeling suicidal (M=0.81, SD=0.39) at a significantly higher rate than POC participants (M=0.70, SD=0.46) where t(110.73)=2.09, p=0.038. There were 109 Caucasian individuals who were not suicidal and 457 who were suicidal, whereas 27 POC were not suicidal and 63 were suicidal. Conclusion: These data suggest that POC MoodNetwork members are less likely to experience suicidal ideation and behavior than Caucasian participants. It is possible that POC are less willing to report their suicidal thoughts, specifically on surveys administered through an online community, than Caucasian participants. Alternatively, the spectrum of severity of mood disorders of POC participants could be less severe than Caucasians. This supports past research that has found an association between POC status and fewer experiences of suicidal behavior. Further research is warranted to investigate the mixed results of POC, especially regarding online platforms. 113 Kelsy Greenwald, MD, Emergency A social-ecological framework to understand barriers to HIV clinic attendance in Nakivale Refugee Settlement in Uganda: a qualitative study K. O'Laughlin1, K. Greenwald1, Z. Faustin2, Rahman2, A. Tsai2, N. Ware2 and I. Bassett2 1Emergency Medicine, Mass General, Boston, MA, USA and 2Massachusetts General Hospital, Boston, MA, USA Introduction: The social-ecological model proposes that efforts to modify health behaviors are influenced by multiple levels of constraints. We sought to use this framework to identify barriers to HIV care and also identify potential modifiers of those barriers in a unique humanitarian setting. Methods: Willing adult participants newly diagnosed with HIV were invited to interview 90 days after HIV testing. Trained multi-lingual research assistants conducted semi-structured interviews with 24 clients who linked to HIV clinical care and with 8 clinic staff. Three analysts employed a directed content analysis approach to explore the potential influence of the following levels of constraints to engagement in HIV clinical care: individual, social environment, physical environment, and policies. Results: Refugee and Ugandan participants were motivated to attend the HIV clinic because of perceived quality of clinic services and the belief that antiretroviral therapy improves their health. Barriers to clinic attendance included distance to clinic, cost of transport, and heavy rain. Stigma was another barrier, as participants rarely disclosed their seropositivity beyond a few individuals or only disclosed to clinic staff. Clients often chose to bypass clinics nearer to their homes, to avoid accidental disclosure. Clinic staff spoke of temporary migration of clinic attendees away from Nakivale as a barrier to care. Participants also spoke of times away from Nakivale, primarily to visit family and to look for work, as a temporary obstacle to HIV care.93Conclusion: Clients living with HIV in Nakivale Refugee Settlement often choose not to disclose their seropositivity to friends and family. Nondisclosure makes it difficult for many to rely on community support to overcome obstacles to effectively engage in HIV clinical care. Interventions to facilitate safe HIV status disclosure, mobilize social support, and provide more flexible HIV services may help overcome barriers to HIV care in this setting. 114 Moytrayee Guha, MPH, Emergency WHO Technical Consultation on Postpartum Hemorrhage Care Bundles M. Guha1, S. Lightbourne1 and T.F. Burke1,8,9 1Division of Global Health and Human Rights, Department of Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Department of Obstetrics, University of California, San Francisco, San Francisco, CA, USA, 3Department of Social Medicine, University of Sao Paulo, Sao Paulo, Brazil, 4Department of Maternal and Child Health Research, Institute for Clinical Effectiveness and Health Policy, Buenos Aires, Argentina, 5The Bill and Melinda Gates Foundation, Seattle, WA, USA, 6World Health Organization, Geneva, Switzerland, 7Safe Motherhood Program, University of California, San Francisco, San Francisco, CA, USA, 8Harvard Medical School, Boston, MA, USA and 9Harvard T.H. Chan School of Public Health, Boston, MA, USA Introduction: Despite global efforts, postpartum hemorrhage (PPH) remains the leading cause of maternal mortality and morbidity. Suboptimal care contributes to poor maternal health outcomes, however the majority of PPH deaths are preventable through timely interventions. Consistent application of bundles, small sets of evidence-based interventions implemented together, has been associated with improved adherence to clinical recommendations and better patient outcomes. The objective of this technical consultation was to systematically develop evidence-based bundles of care for PPH.Methods: A WHO PPH Bundle Steering Group (SG) convened weekly for 6 months and developed draft PPH bundles informed by a systematic literature search on bundle definitions, designs and implementation experiences, as well as technical consultations using a modified Delphi method. PPH bundle interventions were selected from the WHO Recommendations for PPH Prevention and Treatment using the validated \"GRADE Evidence to Decision\" framework. A PPH Bundle Technical Advisory Group (TAG), comprised of 17 global maternal health experts, reviewed and guided on recommendations for the final number and composition of the PPH Bundles. Results: Two PPH management bundles for implementation at health facilities were defined. The \"First Response Bundle\" consists of uterotonics, isotonic crystalloids, tranexamic Bundle\" includes compressive measures (aortic or bimanual uterine compression), Intrauterine Balloon Tamponade, and the Non-pneumatic Anti-shock Garment. Advocacy, training, teamwork, quality assurance and compliance were identified as vital PPH Bundle supporting elements. Conclusion: Two evidence-based PPH Bundles were designed for facility level implementation worldwide. Future research should assess PPH Bundle feasibility, acceptability and effectiveness, as well as identify optimal bundle design and implemen - tation strategies. 115 Daniel L. Hall, PhD, Psychiatry Mind-Body Interventions for Fear of Cancer Recurrence: A Systematic Review General Hospital, Boston, MA, USA, 2Treadwell Library, Massachusetts General Hospital, Boston, MA, USA, 3Chungnam National University College of Nursing, Dae Jeon, Korea (the Democratic People's Republic of), 4Massachusetts General Hospital Cancer Center, Boston, MA, USA, 5Mongan Institute Health Policy Center, Massachusetts General Hospital, Boston, MA, USA and 6Beth Israel Deaconess Medical Center, Boston, MA, USA Introduction: Fear of cancer recurrence (FCR) is a common source of distress among adults with a cancer history. Multiple randomized controlled trials (RCTs) have examined mind-body approaches to mitigating FCR. We summarized characteristics of these trials and calculated their pooled effects on decreasing FCR. Methods: Six electronic databases were systematically searched from inception to May 2017, using a strategy that included multiple terms for RCTs, cancer, mind-body medicine, and FCR. Data extraction and reporting followed Cochrane and PRISMA guidelines. Pooled effects on self-report measures of FCR were computed using random-effects models.94Results: Nineteen RCTs (pooled N =2806) were included. Most studies (53%) were published since 2015 and targeted a single cancer type (84%; mostly breast). Intervention sessions (median=6, mode=4) tended to last 120 minutes and occur across 1.5 months. Delivery was predominantly in-person (63%) to either groups (42%) or individuals (42%). Most interventions incorporated multiple mind-body components commonly cognitive-behavioral skills (58%) or .001). Potential modifiers (control group design, group/individual delivery, use of cognitive-behavioral or mindfulness skills, number of mind-body components, cancer treatment status, and number of sessions) did not reach statistical significance. Conclusion: Mind-body interventions are efficacious for reducing FCR, with small-to-medium effect sizes that persist after intervention delivery ends. Recommendations include testing effects among survivors of various cancers and exploring the optimal integration of mind-body practices for managing fundamental uncertainties and fears during cancer survivorship. 116 Marek A. Hansdorfer, M.D., Surgery - Plastic & Reconstructive A Clinically Relevant Animal Model to Quantitatively Evaluate Functional Outcomes of Large-Gap Peripheral Nerve Repair Medical School, Boston, MA, USA, 2Neurology, Massachusetts General Hospital/Harvard Medical School, Boston, MA, USA and 3Wellman Center for Photomedicine, Massachusetts General Hospital/Harvard Medical School, Boston, MA, USA Introduction: Segmental peripheral nerve deficits are challenging injuries associated with poor outcomes. Gold-standard large-gap (>3cm) reconstruction requires nerve autograft. Alternatives are sought in clinical scenarios where donor limb is not available (e.g. multiple extremity trauma) or donor site morbidity (e.g. painful neuroma formation, loss of function) makes autograft use suboptimal. A variety of alternatives have been studied: allografts, conduits, and tissue-engineered grafts. Most of these studies have been performed in a rodent sciatic nerve model, which allows for histological evaluation and some functional assessment but may not be clinically relevant to the study of large nerve gaps. We report a non-human primate (NHP) model which recapitulates human anatomy, allows for objective quantitative functional outcomes testing, electrophysiology and histomorphometry, with minimal morbidity to subjects. The objective nature of this array of end-points provides a detailed evaluation of nerve regeneration.Methods: Twelve rhesus macaques underwent 4cm proximal radial nerve defect creation in the right upper extremity, the radial nerve transected proximally at the humeral spiral groove, distally prior to the branch to brachioradialis. The radial nerve was selected as it is responsible for a unique function with no input from other nerves which may confound recovery data in other models. Three repair techniques were evaluated for proof of principle: autograft/suture, acellular nerve allograft (ANA)/suture, ANA photosealed in place with light-activated human amnion wraps. An objective functional outcome test was conceived using an apparatus that accurately measures the degree of wrist extension as a function of time after defect repair [Figure 1A,B]. Electromyography (EMG) is performed at 0, 120, 240, and 365 days (euthanasia). Histomorphometry, muscle mass retention, and optical coherence tomography (evaluating remyelination) are performed at euthanasia. 8-month data will be available by October, 2018.Results: No morbidity or mortality was seen. Average loss of wrist extension was 87.0\u00b0 after radial nerve defect creation [Figure 1B,C]. Autograft group animals recovered 67\u00b0 of extension at 5 months demonstrating that recovery is nearly complete in the control group at 5 months in this model [Figure 1B,D]. Wrist extension recovery was slower, as expected, in the ANA groups.Conclusion: This radial nerve defect model improves upon existing animal models by allowing for large nerve gap testing in a primate model more analogous to the clinical problem in humans. It provides multiple end-points for analysis including objective functional outcome assessment, histomorphometry, and EMG, and can be used as a versatile tool for analysis of the full spectrum of nerve reconstruction techniques and translation to human clinical trials.95 117 Marek A. Hansdorfer, M.D., Surgery - Plastic Occipital and Intra-Operative Evidence Gfrerer, R. Ortiz, K.P. Nealon and W.G. Austen Plastic Surgery, Massachusetts General Hospital/Harvard Medical School, Boston, MA, USA Introduction: Recent clinical and basic scientific data supports a theory that aberrant anatomy and inflammation of structures surrounding peripheral/ extra-cranial sensory nerves can provoke migrainesthrough compression/ irritation. Anecdotally, at the occipital trigger site, intra- operative anatomy of migraine surgery patients is distorted with thickened fascia/ muscle, dilated vessels that are tightly adhered to nerves, and atypical nerve course. This study scientifically evaluated this observation.Methods: 92 subjects scheduled to undergo migraine surgery at the occipital trigger site (Greater occipital nerve [GON] release) were enrolled in a prospective fashion. At the time of surgery, the senior author evaluated intraoperative anatomy and notes were made on anatomic variables using an intraoperative anatomy form and detailed operative report. The resulting data was examined.Results: Preoperatively, 67% of subjects reported bilateral pain. Pain on both sides was associated with abnormal tissue anatomy bilaterally (0.016). Unilateral pain was not predictive of one-sided tissue aberration. In fact, 19/30 (63%) subjects with pain on one side had abnormal findings on both sides. In 94% of subjects, abnormally thick trapezius fascia was seen, and in 30% of cases the nerve was encased in or compressed by fibrotic tissue at the muscle/ fascia interface. The occipital artery interacted with the GON in 88% of cases and 20% had dilated veins. The GON had an anomalous course in 42% of patients, and appeared crushed/discolored in 32%. Conclusion: In an ongoing effort to understand the extra-cranial pathophysiology of occipital neuralgia/ migraine, it is critical to describe the anatomic/ tissue changes encountered during migraine surgery. Although nerve compression/ irritation seems the common endpoint, it is currently unclear which tissues are involved in triggering migraine. Interestingly, patients with unilateral pain had bilateral pathology, further highlighting the importance of release on both sides. Pathology varied between subjects with both anatomic abnormalities (aberrant GON course, interaction of the occipital artery and nerve), as well as tissue pathology (thickened fascia and muscle, dilated vessels). Interestingly, the majority of subjects operated on had thickened/ fibrotic appearing trapezius fascia(94%), indicating a much more important role of soft tissues surrounding the nerve than previously implicated. Further, interaction of the occipital artery was seen in 88% of cases. This nerve/ artery interaction has a much higher incidence in migraine surgery patients than previously reported in dissection of cadavers (0-54%). This work is the basis for further research to elucidate pathophysiology of migraine.96118 Stephanie Hansel, Emergency Survival After Use of a Condom UBT Package in Women with Uncontrolled Postpartum Hemorrhage in India S. Hansel1, P. Shivkumar2, S. Suarez1 and T. Burke1,3,4 1Division of Global Health and Human Rights, Department of Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Mahatma Gandhi Institute of Medical Sciences, Sevagram, India, 3Harvard Medical School, Boston, MA, USA and 4Harvard T.H. Chan School of Public Health, Boston, MA, USA Introduction: Uncontrolled postpartum hemorrhage (PPH) is the leading cause of maternal mortality and morbidity. Timely intervention is critical. Over the past 8 years, we developed and refined the award-winning package, Every Second Matters-Uterine Balloon Tamponade (ESM-UBT), designed to arrest uncontrolled PPH. The authors previously reported on high survival rates among women with uncontrolled PPH associated with use of an ESM-UBT device, in Kenya, Tanzania, Sierra Leone and Nepal. This study examined the outcomes of women who had ESM-UBT devices placed for uncontrolled PPH across 10 medical colleges in Maharashtra, India.Methods: Data were collected prospectively on all women with uncontrolled PPH who received ESM-UBT devices for any reason. Shock class was assigned based on recorded blood pressures and mental status at the time of UBT placement. Class III shock was defined as uncontrolled PPH and a systolic blood pressure (SBP) 90 mmHg and > 70 mmHg and/or altered mental status. Class IV shock was defined as uncontrolled PPH and SBP 70 mmHg and/or unconsciousness. Results: 176 women had an ESM-UBT device placed between January 2017 and May 2018, 145 (82.3%) of whom had uncontrolled PPH from uterine atony. 141 of the 145 (97.2%) women survived overall. 100 (68.9%) women had normal vital signs or were in class I/II shock, 35 (24.1%) were in class III shock, and 10 (6.9%) were in class IV shock. Survival rates were 100% (normal vitals or class I/II shock), 97.1% (class III shock), and 70% (class IV shock) respectively.Conclusion: Use of an ESM-UBT device for uncontrolled postpartum hemorrhage from atonic uterus is associated with high survival rates, especially when placed prior to class IV shock. Additional studies are needed to evaluate the feasibility and impact of ESM-UBT in lower level health centers in India to inform national scale. 119 Christina A. Hansen, BA, Center for Genomic Medicine (CGM) Plasma Galectin-3 predicts functional outcome after acute ischemic stroke C.A. Hansen, R. Woodburn and W.T. Kimberly Division of Neurocritical Care and Center for Genomic Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Galectin-3 (Gal-3) is a member of the beta-galactoside-binding protein family that has been implicated in cardiovascular disease and stroke. We sought to determine if baseline Gal-3 predicts functional outcome after acute ischemic stroke, and to compare its performance relative to other validated biomarkers.Methods: We measured plasma Gal-3, soluble ST2 (sST2), and fatty acid binding protein 4 (FABP4) in 383 patients who presented with acute ischemic stroke and were enrolled in a two-center biomarker study. Functional outcome was assessed at 3 months with the modified Rankin Scale (mRS), with good outcome defined as mRS 0-2, and poor outcome defined as mRS 3-6. The relationships between each marker and outcome were evaluated using univariate and multivariate logistic regression.Results: Median admission Gal-3 383 patients (mean age 69+/- 15 years; 41.5% women). In univariate analysis, each biomarker was associated with poor outcome (Gal-3: OR 1.8, 95% CI 1.4 - 2.4, p<0.001; sST2: OR 1.6, 95% CI 1.2 - 2.0, p<0.001; FABP4: OR 1.9, 95% CI 1.4 - 2.6, p<0.001). After adjustment for age, sex, and NIHSS, Gal-3 remained an independent predictor of poor outcome (OR 1.5, 95% CI 1.1 - 2.0, p=0.019), but other biomarkers did not. In predicting mortality, both Gal-3 and sST2 were independent predictors of death (Gal-3: OR 1.9, 95% CI 1.2 - 3.3, p=0.009; sST2: OR 1.7, 95% CI 1.1 - 2.6, p=0.027). Conclusion: Among three previously validated biomarkers, plasma Gal-3 level predicted functional outcome, and Gal-3 and sST2 predicted mortality.97120 Kathryn M. Hardin, BS, Medicine - Cardiology Determinants of Changes in Systemic Oxygen Content During Exercise in Heart Failure with Preserved Ejection Fraction K.M. Hardin, Lewis Cardiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Elevation in left heart filling pressure leads to pulmonary edema and low systemic arterial oxygen (PaO2) levels in patients presenting with acute decompensated heart failure (HF). However, less is known about whether elevation in left heart filling pressure leads to a reduction in PaO2 and oxygen saturation during incremental exercise in ambulatory patients with HF. Because multiple organ system dysfunction is common with heart failure and preserved ejection fraction (HFpEF), we sought to determine correlates of PaO 2 responses to exercise in patients undergoing evaluation of dyspnea on exertion. We hypothesized that changes in PaO2 in response to exercise are governed by extra-cardiac organ systems rather than the degree of elevation in pulmonary capillary wedge pressure (PCWP) in HFpEF.Methods: We performed maximum incremental cardiopulmonary exercise testing with invasive hemodynamic monitoring in patients meeting AHA/ACC criteria for the diagnosis of HFpEF (i.e. left ventricular ejection fraction > 50%, characteristic signs and symptoms of HF). We excluded patients with O 2-dependent lung disease. Arterial blood gasses were measured every minute during exercise to determine PaO2 levels and simultaneous O2 saturations were continuously monitored. Results: In 411 BMI 29.7\u00b10.7, that increased from 94\u00b10.7 to 97\u00b10.9 and overt O2 desaturation was uncommon patients PaO2 <55mmHg and O2 sat 88%). exercise PaO2 with peak VO2 (r=0.22, p<0.001). Change in PaO2 with exercise was not resting PCWP (r=0.05, p=0.27) or exercise PCWP (r<0.01, P=0.7) (Figure 1). However, change in PaO2 with exercise was correlated with % predicted diffusion capacity of the lungs (DLCO) (r=0.28, p<0.001) (Figure 2) despite relatively normal 93\u00b1 1, FEV1 89\u00b1 5, FVC/FEV 96\u00b1 5, % predicted mean\u00b1 SEM). Pulmonary artery pressure (PAP) and transpulmonary pressure gradient (TPG) were also associated with change in PaO 2 (r=-0.18 (PAP), r=-0.25 (TPG), p<0.001) in multivariable models adjusting for age, sex, and BMI.Conclusion: Pulmonary and pulmonary vascular function, but not left sided filling pressures, relate to changes in PaO 2 during incremental exercise in HFpEF. These findings highlight the important role of extra-cardiac organ systems, specifically the lungs, in mediating systemic O 2 bioavailability during exercise in HFpEF as well as the low likelihood of observing O2 desaturation in the setting of isolated elevation in left heart filling pressures in ambulatory patients with heart failure. 121 Maya Hareli, Bachelor of Arts, Psychiatry The Moderating Role of Sex and Pubertal Onset on Cannabis Use and Psychiatric Symptoms M. Hareli1, Dechert1, and R.M. Schuster1,2 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Cannabis use has been linked with adverse psychiatric correlates among adolescents, yet this association may differ by sex. Several factors may contribute to sexual dimorphisms in adolescent cannabis use and psychiatric functioning, including sex-based differences in brain development as well as use patterns, motives for use, and likelihood to present for psychiatric treatment. Further, among female users, early pubertal onset may be a risk factor for negative psychiatric outcomes secondary to cannabis use due to a combination of biological and social sequelae associated with early puberty. The current study examined whether frequency of past 90-day cannabis use was associated with current anxiety and depression 98symptoms, and whether these associations varied by sex. This study also examined whether, among female users, age of menarche moderated the association between frequency of cannabis use and current anxiety and depression symptoms.Methods: Participants included 76 (55% male) non-treatment seeking adolescent cannabis users, ages 18-25 years. Data for the current project came from the baseline assessment of a larger parent study which examined cognitive changes associated with 30 days of cannabis abstinence. Frequency of past 90-day cannabis use was quantified during a timeline follow-back interview. The Mood and Anxiety Symptom Questionnaire (MASQ) was used to assess past week depression and anxiety symptoms, and yielded four subscales measuring severity of anxious arousal, anxious distress, general distress, and anhedonic depression. Results: There was an interaction effect between sex and cannabis use on anxious arousal, such that more frequent cannabis use in the past 90 days was associated with higher levels of anxious arousal among females (p = 0.002) but not males (p = 0.53). All other interactions between sex and cannabis use were not significant (p-values > 0.13). Among female users, the positive associations between more frequent cannabis use and anxious and general distress were stronger with earlier age of menarche (p-values < 0.01). Age of menarche did not moderate the association between cannabis use frequency and anxious arousal and anhedonic depression (p-values > 0.33).Conclusion: Given that widespread cannabis legalization will likely result in decreased barriers to access for adolescents, it is critical to identify those users who are most vulnerable to experiencing negative psychiatric outcomes with cannabis use. Findings from this study suggest that more frequent cannabis use is more strongly associated with higher levels of depression and anxiety among females, particularly those with an earlier onset of puberty. Future longitudinal studies are needed to clarify whether cannabis has a causal role in this relationship, as well as the underlying biological and social mechanisms. 122 Kamber L. Hart, AB, Psychiatry Gender trends in authorship across medical specialties from 2008-2018 K.L. Hart1,2 and R.H. Perlis1,2 1Psychiatry, Center for Quantitative Health, Boston, MA, USA and 2Division of Clinical Research, Boston, MA, USA Introduction: While gender parity has been reached for medical school matriculants, women are still underrepresented in academic medicine, especially among senior faculty positions. Since publication activity is a key criterion for promotion and awarding of research funds, we examined temporal trends in female authorship across nine specialties (anesthesiology, dermatology, internal medicine, neurology, obstetrics/gynecology, oncology, pediatrics, psychiatry, and radiology) as well as in four high-impact cross specialty journals (New England Journal of Medicine, Journal of the American Medical Associ-ation, British Medical Journal, and the Lancet) by conducting a large-scale bibliometric study.Methods: We examined three measurements of female representation (percent of first authors who were women, percent of last authors who were women, and the overall proportion of authors who were women) in the top fifteen journals selected by impact factor for each specialty from January 2008 - May 2018. We assessed changes in these measurements over time, as well as the relationship of these measures to journal impact factor. Additionally, we compared the rate of transition to senior author status between men and women.Results: Across the 271,123 articles analyzed, women represented 39.1% of all authors, 44.0% of first authors, and 29.8% of last authors. When comparing the change in overall percent female authors between departments over time, obstetrics/gynecology showed the greatest rate of increase. For both the percent female first authors and percent female last authors, high-impact cross specialty journals and obstetrics/gynecology showed the greatest increase over time as compared to other specialties (Figure 1). There was no significant correlation between the proportion of female authors and journal impact factor (Figure 2). In secondary analysis, articles with a woman as the last author were 13.0% more likely than those with a male last author to have a woman as the first author (X 2=2416.6, p<2.2e-16). Women exhibited slower rates of transition to the last author position (log rank p=2e-16); time to 10% transition was 5 years for men and 9 years for women (Figure 3). Conclusion: Overall, these results indicate continued improvement in the representation of women as authors in academic medicine, especially in the last author position. Of note, the four high-impact cross specialty journals showed the second largest increase in percent female last authors over the study period. However, the rate of improvement varies substantially between specialties, emphasizing the need for continued efforts to support the advancement of women in academic medicine across disciplines.99 Figure 3 123 Lile He, Surgery - Surgical Oncology Adoptive immunotherapy of thyroid cancer L. He MGH, Boston, MA, USA Introduction: The goal of this research program is to develop a novel effective adoptive immunotherapy for the treatment of poorly differentiated thyroid cancers such as anaplastic thyroid cancer (ATC) or poorly differentiated thyroid cancer (PDTC); both of them have very poor prognoses.. This goal stems from the following pieces of information: i) no effective therapy is available for anaplastic thyroid cancer ii) we have found for the first time that ATC and PDTC thyroid cancer cells express the tumor antigen (TA) chondroitin sulphate proteoglycan 4 (CSPG4), which is an attractive target of antibody- based immunotherapy; and iii) we have available in our laboratory the reagents required to develop adaptive immuno- therapy of undifferentiated thyroid cancer with T cells genetically engineered to express a TA-specific chimeric antigen receptor (CAR).Methods: We have selected CAR T cells as effector cells, since this strategy allows rapid generation of polyclonal T cells with TA-specificity and potent cytotoxic activity. It is noteworthy that CAR T cells have already been used for treatment of thyroid cancer in an experimental setting. The target antigen used is ICAM1. Therefore the positive results obtained cannot be translated to a clinical setting given the broad expression of ICAM 1 in a number of normal tissues. We have selected CSPG4 as a target, since this antigen is highly expressed on malignant cells including anaplastic thyroid cancer cells, but has a restricted distribution in normal tissues. According to the information in the literature and according to our own extensive data, CSPG4 is only detectable on activated pericytes in the tumor microenvironment. As a result, immune targeting of CSPG4 is expected to selectively not only inhibit tumor cells but will also inhibit neo-angiogenesis in the tumor microenvi - ronment, contributing to the elimination of thyroid cancer cells, even those which do not express CSPG4, without the side effects associated with the systemic administration of anti-angiogenic drugs.Results: We have selected CAR T cells as effector cells, since this strategy allows rapid generation of polyclonal T cells with TA-specificity and potent cytotoxic activity. It is noteworthy that CAR T cells have already been used for treatment of thyroid cancer in an experimental setting. The target antigen used is ICAM1. Therefore the positive results obtained cannot be translated to a clinical setting given the broad expression of ICAM 1 in a number of normal tissues. We have selected CSPG4 as a target, since this antigen is highly expressed on malignant cells including anaplastic thyroid cancer cells, but has a restricted distribution in normal tissues. According to the information in the literature and according to our own extensive data, CSPG4 is only detectable on activated pericytes in the tumor microenvironment. As a result, immune targeting of CSPG4 is expected to selectively not only inhibit tumor cells but will also inhibit neo-angiogenesis in the tumor microenvi - ronment, contributing to the elimination of thyroid cancer cells, even those which do not express CSPG4, without the side effects associated with the systemic administration of anti-angiogenic drugs.Conclusion: The antitumor activity of CSPG4 CAR T cells can be enhanced by strategies we used that counteract the escape mechanisms utilized by thyroid cancer cells.100124 Elbert H. Heng, ScB, Surgery - Cardiac Preoperative Epidural Placement Provides Improved Analgesia without Added Morbidity in and Villavicencio1 1Surgery, Massachusetts General Hospital, Boston, MA, USA, 2Anesthesia, Critical Care, and Pain Medicine, Massachusetts General Hospital, Boston, MA, USA and 3Pulmonary and Critical Care Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Epidural analgesia provides effective pain control after lung transplantation. Postoperative placement may be delayed due to poor mental status, coagulopathy, and difficult positioning. Conversely, preoperative safety can be questioned due to the potential need for anticoagulation for extracorporeal membrane oxygenation or bypass. We sought to compare pain control and pulmonary and epidural morbidity between patients receiving preoperative versus postoperative epidurals.Methods: Our institutional lung transplant database was analyzed to compare postoperative pain control, primary graft dysfunction, adverse pulmonary events, and epidural morbidity in patients receiving pre or postoperative epidurals following bilateral lung transplantation. Pain control was measured as an average of all visual analog scale (VAS) pain scores (0-10) recorded daily in the first 72 hours post-transplant. Pulmonary complications included a composite of pneumonia, prolonged intubation greater than 5 days, and reintubation and or tracheostomy.Results: A total of 103 patients underwent bilateral lung transplantation between 2014-2017 and received a thoracic epidural. Of these, 72 (70%) had an epidural placed preoperatively and 31 (30%) had an epidural placed within 72 hours post- transplant. Of the postoperative group, 38% had delayed placement by 48 hours or greater. There were no significant differences in the rates of cardiopulmonary bypass (3% vs 0%, p=0.59) however patients with a preoperative epidural were less likely to be put on ECMO intraoperatively (25% vs 52%, p=0.01). The mean duration of epidural therapy was similar (6.6 vs 6.6 days, p=0.11) between groups. Pain control was non-statistically different at 24 hours (1.2 vs 1.7, p=0.05), however patients with a preoperative epidural reported lower average pain scores at 48 (1.2 vs 2.1, p=0.02) and 72 hours post-transplant (0.8 vs 1.7, p=0.02). Within the first 72 hours post-transplant a total of 30 (42%) patients in the preoperative epidural group and 17 (56%) patients in the postoperative group had any degree of PGD (p=0.28), however this dropped to 9 (13%) and 6 (19%) by 72 hours (p=0.58). There were no significant differences in the length of mechanical ventilation (19.5 vs 24hrs, p=0.18), or rates of adverse pulmonary events (33% vs 52%, p=0.12) and no adverse events resulted from epidural placement in either group. One year survival was equivalent (95% vs 92%, p>0.05). Conclusion: Preoperative epidural placement provides effective analgesia without increased morbidity following lung transplantation. Given the immediate postoperative analgesic benefit, preoperative epidural placement should be considered in appropriate patients. 125 Markus D. Herrmann, Pathology Implementing the DICOM Standard for Digital Pathology for Clinical Data Science, Boston, MA, USA, 2Surgical Planning Laboratory, Brigham and Women's Hospital, Boston, MA, USA, 3Harvard Medical School, Boston, MA, USA, 4PixelMed Publishing LCC, Bangor, PA, USA, 5Isomics Inc, Cambridge, MA, USA, 6Department of Pathology, Massachusetts General Hospital, Boston, MA, USA, 7Department of Pathology, Brigham and Women's Hospital, Boston, MA, USA, 8Enterprise Medical Imaging, Massachusetts General Hospital, Boston, MA, USA, 9Department of Radiology, Brigham and Women's Hospital, Boston, MA, USA and 10Department of Radiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Digital Imaging and Communication in Medicine (DICOM\u00ae) is the ubiquitous standard for the represen- tation and communication of medical images and related information in a clinical environment. A DICOM file format and communication protocol for pathology has been defined; however, adoption by vendors and in the field is pending. Here, we implemented essential aspects of the standard and assessed its capabilities and limitations in a multi-site healthcare network. Methods: We selected relevant DICOM attributes from the Visible Light Whole Slide Microscopy Image information object definition. We developed a program that extracts pixel data and pixel-related metadata from proprietary whole slide image files, integrates patient and specimen-related metadata (obtained from the laboratory information system), populates and encodes DICOM attributes and stores DICOM files. As a proof of concept, we generated files using image data from four 101vendor-specific image file formats and clinical metadata from two departments with different laboratory information systems. We validated the generated DICOM files by manual expert review and use of recognized DICOM validation tools. We measured encoding, storage and access efficiency for JPEG, JPEG-LS and JPEG 2000 image compression methods. Lastly, we investigated options for reading the generated files using existing DICOM software libraries and evaluated storing, querying, and retrieving data over the web using existing DICOM archive software.Results: Whole slide image data can be encoded together with relevant patient- and specimen-related metadata as DICOM objects. DICOM files stored locally as well as in remote archives can be accessed efficiently using existing software implementations. Performance measurements show that the choice of image compression method has a major impact on data access efficiency. For lossy compression, JPEG achieves the fastest compression/decompression rates. For lossless compression, JPEG-LS significantly outperforms JPEG 2000 with respect to data encoding and decoding speed. Technical challenges remain with respect to size and complexity of whole slide image data sets and interoperability with laboratory information systems. Conclusion: Implementation of DICOM allows efficient access to image data as well as associated metadata. By leveraging a wealth of existing infrastructure solutions, the use of DICOM facilitates enterprise integration and data exchange for digital pathology. 126 Wilson Ho, Psychiatry Misconceptions of Medical Marijuana: No Improvement in Clinical Outcomes After Three Months of Treatment W. Ho1, A. Dechert1, R. Plummer1, C. Massachusetts General Hospital, Wakefield, RI, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Legalization of medical marijuana (MM) has given rise to an increase in individuals seeking MM to treat a number of conditions, including pain, insomnia, and affective disorders (anxiety/depression). Many patients believe that use of MM will allow them to reduce use of prescription medications, reduce consumption of other substances such as alcohol, and relieve pain. To date, there are few studies investigating whether MM is associated with these outcomes. Methods: Data for the current study came from an ongoing clinical trial examining participants seeking a MM card for either chronic pain, insomnia, or affective disorders (target N = 200). Participants all used marijuana twice a week or less. Participants were randomly assigned to either an active MM arm, in which they could obtain MM cards without delay, or to a waitlist control arm (WLC), in which they were asked to wait three months before obtaining a card. Participants were assessed at baseline, one month after baseline, and three months after baseline for MM use behaviors, perception of disease symptomatology (e.g. pain), other medication use, and recreational alcohol use. They were also asked to rate their expectan - cies for MM in reducing pain, medication use, and alcohol consumption. Two separate mixed-effects models for repeated measures were used to evaluate time (baseline, one month, three months) by group (MM, WLC) effects for (1) number or dose of prescription medication and (2) frequency of alcohol use per week on average over the past month. To assess change in pain severity, we fit a mixed effects model examining time by group effects in each subgroup (participants randomized for pain, insomnia, affective disorders).Results: Participants included 49 (66% female) adults ages 18-65 (M = 39.84, SD = 15.09). Before starting the study, 55.1% of participants reported using marijuana in the past three months (1-2 times per month on average), 36.7% (n = 18) reported no marijuana use in the past year, and 8.2% (n = 4) reported no lifetime use. At baseline, 28.5% (n = 14) of partic - ipants agreed or strongly agreed with the statement \"marijuana is a substitute for prescription medication\". However, there were no differences in the number or dose of medications over three months in either group (p > .05). At baseline, 30.7% (n = 15) of participants agreed or strongly agreed with the statement \"marijuana is a substitute for alcohol,\" yet the frequency of alcohol use did not change over three months in either group (p > .05). There was also no association between the frequency of alcohol and marijuana use (p = 0.46). Finally, 77.3% (n = 38) of participants agreed or strongly agreed with the statement \"marijuana relieves pain.\" We found no change in self-reported pain over three months by group assignment (MM, WLC) or randomization subgroup (pain, insomnia, affective disorder) (p > .05).Conclusion: Despite expectations that MM would reduce the need for prescription medications, reduce alcohol consump- tion, and improve pain symptoms, data did not indicate any significant effects of MM on these measures in the first three months of MM use. The results of this study indicate that patients' expectations of MM may be incorrect. With the increasing accessibility to MM, decreased barriers to access, and increased social acceptance of MM, there is an urgent need for both patients and clinicians to better understand potential benefits and harms of MM.102127 Jennifer Y. Hsu, OB/GYN Mullerian-inhibiting substance/anti-Mullerian hormone (MIS/AMH) as predictor Pepin M.E. Sabatini Massachusetts General Hospital, Boston, MA, USA Introduction: Abnormalities of parturition are common and have serious medical and economic consequences. Preterm birth, defined as delivery prior to 37 weeks, affects approximately 10% of pregnancies in the United States and is the leading cause of infant death and disability in developed countries. Markers that predict adverse obstetric outcomes for patient risk stratification are needed. In addition to enabling us to identify individuals at risk, this knowledge may allow insight into the underlying etiologies of preterm birth and labor arrest and direct specific interventions for these adverse outcomes. Expression of the MIS type II receptor (MISRII) in the uterine mesenchyme is well established in animal models, and there is evolving human data with the identification of MISRII to solidify a role for MIS/AMH in the adult uterus. These insights led us to suspect a probable uterine effect of MIS/AMH and to investigate its effects in pregnancy, with the hypothesis that extreme levels of maternal serum MIS/AMH can result in adverse obstetric outcomes. We sought to determine if (1) high MIS/AMH levels would be associated with increased risk of spontaneous preterm delivery and (2) low MIS/AMH levels would be associated with increased risk of Cesarean delivery for arrest of labor.Methods: A retrospective chart review was conducted of all consecutive cases of patients undergoing IVF at the Massachu-setts General Hospital Fertility Center between January 2012 and October 2016. Demographic and clinical information pertaining to infertility evaluation, treatment, and pregnancy outcome was extracted from the electronic medical record. Cycles were included if they resulted in pregnancy, documented an AMH level within 1 year prior to IVF, and had complete information about pregnancy outcome (e.g. gestational age at birth, mode of delivery, and obstetric complications). Cycles were excluded if they utilized oocyte donation or gestational carrier, resulted in multiple gestation, suffered pregnancy loss prior to 20 weeks gestation, or were missing information. Medically indicated PTDs were also excluded from the analysis. Preterm was defined as delivery prior to 37 weeks. The study was approved by the Partners Healthcare Institutional Review Board. There were two primary outcomes, (1) preterm birth and (2) Cesarean delivery for arrest of labor. Because MIS/AMH level is highly skewed by certain infertility diagnoses, the preterm birth analysis was stratified by polycystic ovary syndrome (PCOS) diagnosis, and the Cesarean delivery for arrest of labor analysis was stratified by diminished ovarian reserve (DOR) diagnosis. Chi-squared, Mann-Whitney, and t-test were used as appropriate. A p-value of < 0.05 was considered statistically significant.Results: Among women with PCOS, those who delivered prematurely had substantially higher MIS/AMH levels (18 vs. 6.4 ng/mL, p = 0.003) than those who delivered at term. At the highest MIS/AMH values, preterm deliveries predominated; above the 90th percentile in PCOS women, all deliveries were premature. No effect of MIS/AMH level was observed in women without PCOS. We found no association between MIS/AMH values and Cesarean delivery for labor arrest. Conclusion: In women with PCOS, substantially elevated MIS/AMH levels are significantly associated with preterm birth, suggesting closer follow-up and further studies to elucidate the underlying mechanisms. 128 Michael C. Hughes, Ph.D., Psychiatry Precision medicine using supervised topic models for M.C. Tufts University, Cambridge, MA, USA, 2School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA, 3Harvard Medical School, Cambridge, MA, USA and 4Center for Quantitative Health, Massachusetts General Hospital, Boston, MA, USA Introduction: Without readily-assessed and clinically-validated predictors of treatment response, pharmacologic manage - ment of major depressive disorder (MDD) often involves trial and error. While currently clinicians rely on patient-specific features to personalize treatment decisions, evidence for these decisions is often limited because existing clinical trials often do not use measurements that are routinely collected during care, and also do not capture less selected populations with broader medical and psychiatric comorbidities and less systematic follow-up.Methods: We have developed predictive models that estimate the probability of achieving stable treatment with each of 11 standard antidepressants, based on readily available diagnosis, procedure, and prescription drug codes extracted from a patient's electronic health record history. We define a stable treatment regimen as an uninterrupted interval of the same antidepressant prescription occurring at least twice for at least 90 days. While baseline models could simply use each patient's 103ICD9/10 and CPT code count vectors as input features, it may be difficult to inspect and validate how the many thousands of possible codes yield predictions. Given many noisy code features, it may also be difficult to learn useful non-linear interactions that various comorbid diseases have in the prediction process without overfitting. Instead, we applied a recent - ly-developed supervised topic modeling approach which explains each patient's history as a sparse mixture of a small set of \"topics\" (subsets of codes which frequently occur together in a patient's history). Each topic might be interpreted post-hoc as high-level clinical concepts such as \"bipolar disorder\", \"cancer treatment\", or \"obesity\", but crucially these concepts are learned from data and not defined in advance, reducing burden on human annotators and potentially revealing useful patterns in the data unknown to experts. We trained our supervised topic models using a recent prediction constrained optimization algorithm [1], so that the resulting models should balance interpretability and predictive quality better than previous training methods. From 260,212 adults with mood disorders across two hospital systems -- Mass. General Hospital (MGH) and Brigham and Women's (BWH) -- we identified 59,606 subjects with an ICD9/10 diagnosis of MDD who reached a stable treatment regimen. We trained random forest predictors on a training set of 29,774 subjects exclusively from the MGH site. We compared several possible feature representations of patient history, including our proposed learned topic representations, baseline code count features, and baseline demographics (age and race). Predictor hyperparameters were selected via grid search based on performance on a small MGH-only validation set. We report prediction results on held-out sets from both MGH (3,722 subjects, internal validation) and BWH (22,389 subjects, external validation). Results: We found that our learned topic representations add predictive value over baseline count features. On the MGH heldout set, when averaging over all 11 antidepressants we achieved a mean area-under-the-ROC-curve (AUROC) of 0.692 (95% CI 0.682 - 0.701) using the combination of topics, code counts, and demographic features. In contrast, baselines achieved an AUROC of 0.669 (95% CI 0.658 - using alone and 0.630 (95% CI 0.619 - 0.640) for demographics alone. Second, we suggest that the learned representations, as well as the learned predictive models based on these representations, generalize well to external datasets. When the full combination model was evaluated on 22,389 subjects from the separate BWH heldout set, we achieved a similar AUROC of 0.691 (95% CI 0.686 - 0.695). Finally, by visualizing our learned supervised topics and comparing these side-by-side with unsupervised topics, we can identify how supervision causes topics to evolve to produce better predictive performance. Conclusion: Supervised topic models produce generalizable predictions about stable antidepressant treatment while yielding clinician-interpretable features. While integration with emerging quantitative measures or biological measures will likely allow further improvement, our results provide a simple and transparent baseline for large-scale assessment of antidepressant treatment stability. Next steps include refining treatment outcomes and updating predictive models to capture two subpop-ulations: patients treated in psychiatry and those treated by non-psychiatrists, so that model predictions might be tailored to these distinct use cases. REFERENCES [1] Michael C. Hughes, Gabriel Hope, Leah Weiner, Thomas H. McCoy, Jr., Roy H. Perlis, Erik B. Sudderth, and Finale Doshi-Velez. \"Semi-Supervised Prediction-Constrained [2] Perlis. R., Iosifescu, D., Castro, V., ... Smoller, J. (2012). Using electronic medical records to enable large-scale studies in psychiatry: Treatment resistant depres- sion as a model. Psychological Medicine, 42(1), 41-50. doi:10.1017/S0033291711000997 129 Nasrien E. Ibrahim, MD, Medicine - Cardiology Blood Kidney Injury Molecule-1 Predicts Short and Longer-term Kidney Outcomes in Patients Undergoing Diagnostic Coronary and/or Peripheral Angiography- Results from the Catheter Sampled Blood Archive in Cardiovascular Diseases (CASABLANCA) Study N.E. Ibrahim 1, C.P. Massachusetts General Hospital, Boston, MA, USA, 2Baim Institute for Clinical Research, Boston, MA, USA, 3Internal Medicine, Massachusetts General Hospital, Boston, MA, USA and 4Cardiology Division, Radboud UMC and Cardiology Division, Maastricht UMC, Nijmegen, Netherlands Introduction: Kidney injury is common in patients with cardiovascular disease and has substantial impact on patient manage- ment and prognosis, including progression to chronic kidney disease (CKD). Tools to accurately predict kidney injury prior to their incidence are lacking. We determined whether blood measurement of kidney injury molecule-1 (KIM-1), would predict kidney outcomes in patients undergoing angiography.Methods: 1251 patients undergoing coronary and/or peripheral angiography with or without intervention between 2008 and 2011 were prospectively enrolled at the Massachusetts General Hospital in Boston, Massachusetts, USA; 1208 had peri-procedural blood samples available for KIM-1 measurement. Peri-procedural acute kidney injury (AKI) was defined as AKI within 48 hours of contrast exposure. Non-procedural AKI was defined as AKI beyond 48 hours to study conclusion. Development of CKD was defined as progression to an estimated glomerular filtration rate (eGFR) <60 mL/min/1.73m 2 by 104the end of follow up. Univariate and multivariate Cox proportional hazards analysis with clinical and laboratory variables were used to identify predictors of non-procedural AKI, while univariate and multivariate logistic regression analysis was used to evaluate peri-procedural AKI and predictors of progression to CKD; for the latter analysis, logistic regression was used because the reason for eGFR measurement at various timepoints is unknown to us. Results: Peri-procedural AKI occurred in 5.0%. During mean follow up of 4 years, non-procedural AKI in 27.4%, and 12.4% developed new reduction in <60 mL/min/1.73m 2. KIM-1 concentrations above median (140.5 pg/mL) were associated with cardiovascular comorbidities and worse left ventricular function at baseline. In adjusted logistic regression analyses (the variables included: age, history of CKD, diabetes, peripheral artery disease, and heart failure, and creatinine), elevated post-procedural KIM-1 concentrations strongly predicted not only peri-procedural AKI (odds ratio [OR] 1.54, 95% confidence interval [CI] 1.10-2.15, p=0.01) but also progression to CKD (OR 2.05, CI 1.31-3.22, p=0.002). Additionally, in the adjusted Cox regression analysis, elevated post-procedural KIM-1 concentrations strongly predicted first non-procedural AKI (hazard ratio 1.46, 95% CI 1.23-1.74, p<0.001, also see Figure ). Conclusion: In a typical at-risk population undergoing coronary and/or peripheral angiography for various acute and non-acute indications, blood concentrations of KIM-1 predicted incident peri-procedural and non-procedural AKI, as well as progression to CKD. Subsequent studies should consider how patients with elevated concentrations of biomarkers such as KIM-1 might benefit from interventions to mitigate future risk. 130 Nasrien E. Ibrahim, MD, Medicine - Cardiology A Clinical, Proteomics and Artificial Intelligence-Driven Model to Predict Acute Kidney Injury in Patients Undergoing Coronary Angiography-Results from the Catheter Sampled Blood Archive in Cardiovascular Diseases Study N.E. Ibrahim 1, General Hospital, Boston, MA, USA, 2Internal Medicine, Massachusetts General Hospital, Boston, MA, USA and 3Prevencio, Kirkland, WA, USA Introduction: Acute kidney injury (AKI) following cardiac procedures has substantial impact on patient management and prognosis. Standard measures of kidney function are only modestly useful for accurate prediction of risk for AKI. Proteom-ics-based biomarker measurements together with clinical risk factors would predict procedural AKI risk in an all-comer population undergoing coronary angiography. Methods: Using Luminex xMAP technology, we measured 109 biomarkers in blood from 889 patients undergoing coronary angiographic procedures for various indications. Procedural AKI was defined as an abrupt reduction in kidney function with an absolute increase in serum creatinine of more than or equal to 0.3 mg/dL, a percentage increase in serum creatinine of 50%, or a reduction in urine output (documented oliguria of <0.5 mL/kg per hour for >6 hours), within 7 days after contrast exposure. Clinical and biomarker predictors of AKI were identified using least-angle regression (machine learning) and a final prognostic model was developed with LASSO.Results: 43 (4.8%) of patients developed procedural AKI. Six predictors were present in the final model: four (history of diabetes, blood urea nitrogen to creatinine ratio, C-reactive protein and osteopontin) had a positive association with AKI risk, while two (CD5 antigen-like and Factor VII) had a negative association with AKI risk. The final model had a cross-validated area under the receiver operating characteristic curve (AUC) of 0.80 for predicting procedural AKI, and an in-sample of (P<0.001). The optimal score cut-off had 77% sensitivity, 75% specificity and a negative predictive value of 98% for procedural AKI. An elevated score was predictive of procedural AKI in all subjects (odds ratio=9.87; P<0.001).Conclusion: We describe a clinical and proteomics-supported biomarker model with high accuracy for predicting procedural AKI in patients undergoing coronary angiography.105 131 Nasrien E. Ibrahim, MD, Medicine - Cardiology Endothelin-1 Predicts Incident Heart Failure, Incident Myocardial Infarction, Cardiovascular Mortality, and All- Cause Mortality in Patients Undergoing Diagnostic Coronary Angiography- Results from the Catheter Sampled Blood Archive in Cardiovascular Diseases (CASABLANCA) Study N.E. Ibrahim 1, S. General Hospital, Boston, MA, USA, 2Internal Medicine, Massachusetts General Hospital, Boston, MA, USA, 3Baim Institute for Clinical Research, Boston, MA, USA and 4Cardiology Division, Radboud by vascular endothelial cells and may play a role in cardio - vascular disease. We investigated prognostic meaning of ET-1 in 1084 patients referred for coronary angiography. Methods: ET-1 was measured. Patients were followed for a median of 3.8 years for outcomes including incident heart failure (HF), myocardial infarction (MI), cardiovascular (CV) mortality, and all-cause mortality.Results: of ET-1 was 2.57 pg/mL. Patients with supramedian ET-1 concentrations were more likely to have prevalent risk factors for CV disease, worse renal function and higher concentrations of high sensitivity troponin, N-terminal pro P<0.001). ET-1 concentrations were not associated with presence or severity of coronary artery disease. In Cox regression models adjusted for clinical variables and prognostic biomarkers, supramedian concentrations of ET-1 predicted 2.68, confidence (HR 1.73, CI 1.16-2.58, P=0.007) as well as composite of incident HF/MI/CV mortality (HR 2.05, CI 1.61-2.62, P<0.001) (Figure 1). Conclusion: In an at-risk population referred for coronary angiography, increased ET-1 concentrations powerfully predict incident CV events and death. 106132 Nasrien E. Ibrahim, MD, Medicine - Cardiology Differential Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Internal Medicine, Massachusetts General Hospital, Boston, MA, USA, 4Hospital for Special Surgery, New York, NY, USA, 5Laboratory Medicine & Pathology, Hennepin County Medical Center & University of Minnesota, Minneapolis, MN, USA and 6Mayo Clinic, Rochester, MN, inhibition, BNP concentrations increase however it remains unclear if changes in BNP concentrations are similar across all assays. Effects of NEP inhibition on ANP or CNP concentrations in humans are unknown. Lastly, the impact of neprilysin inhibition on (NT-proBNP), and proBNP1-108 are not understood. Measurement of a broad range of natriuretic peptide (NP) molecular forms derived from well-collected and handled blood samples from sequential stable heart failure (HF) patients would be expected to provide invaluable insights to the effects of NEP inhibition with sacubitril/valsartan treatment on their concentration, inform how benefit of sacubitril/valsartan might be monitored, and ultimately whether NPs may be used as a guide to understanding potential mechanisms of benefit from sacubitril/valsartan therapy. Methods: Twenty-three consecutive stable patients with systolic HF initiated and titrated on sacubitril/valsartan. Changes NT-proBNP assays), proBNP1-108, and CNP were measured over three visits.Results: Average time to three follow up visits was 22, 46, and 84 days. Average achieved total dose of sacubitril/valsartan was 132.6 mg daily, with 30% of patients receiving 24/26 mg twice daily, 22% receiving 49/51 mg twice daily, and 48% achieving the target dose of 97/103 mg twice daily. ANP rapidly and substantially increased with initiation and titration of sacubitril/valsartan, more than doubling by the first follow up visit (+105.8%). Increases in ANP concentrations were sustained across all visits. Concentrations of MR-proANP generally declined after treatment (Table 1). The magnitude of absolute ANP increase was greatest (+188%) in those with concentrations above the median at baseline compared to those with lower ANP concentrations at baseline whose increase was more modest (+44%). Reduction in MR-proANP was relatively similar regardless of baseline concentration (Table 2). Treatment with sacubitril/valsartan led to inconsistent changes in BNP, which varied across methods assessed. In contrast to BNP, concentrations of NT-proBNP and proBNP1-108 tended to decrease, with relatively more consistent change across study visits. CNP concentrations were very low and did not demonstrate consistent changes in the 9 patients with blood samples available for testing (Table 3). The largest magnitude of BNP increase appeared by second follow up visit, and occurred in those with higher baseline BNP concentrations. In analogy to greatest increase seen in those with highest BNP, largest reductions in NT-proBNP and proBNP1-108 values occurred in those with higher baseline biomarker concentrations (Table 4). Conclusion: Neprilysin inhibition increased ANP and BNP concentrations, and such increases were more likely in those with higher baseline NP values; magnitude of relative ANP increase was substantially greater than those seen with BNP. Notably, changes in BNP after NEP inhibition was variable across assays studied. MR-proANP, NT-proBNP, and proBNP1-108 concentrations decreased after initiation and titration; such reductions were also variable from assay to assay but appeared somewhat more predictable. Consistent with its hypothesized paracrine role, circulating concentrations of CNP were low and not consistently affected by NEP inhibition. Our results suggest effects of sacubitril/valsartan might be mediated in part through increases in circulating ANP. Though measured in only a small number of consecutive, stable HF subjects, our results are extremely novel in the broad range of natriuretic peptides assessed and provide useful insights regarding behavior of these biomarkers in individual patients and with various assays.107 133 Dana Im, MD, MPP, MPhil, Emergency Emergency providers' attitudes towards opioid use disorder emergency department-initiated buprenorphine a S. Weiner2 and M. Samuels-Kalow1 1Emergency Department, Massachusetts General Hospital, Newton, MA, USA and 2Emergency Department, Brigham and Women's Hospital, Boston, MA, USA Introduction: Emergency department (ED) visits related to opioid use disorder (OUD) have increased by nearly 100% over the last decade. Treatment with buprenorphine decreases opioid use and related overdose deaths. Although patients who have buprenorphine treatment initiated in the ED are more likely to remain engaged in treatment compared to those who are referred out, little is known about ED providers' attitudes toward prescribing buprenorphine in the ED. The objective of this study is to better understand ED providers' attitudes, current clinical practice, anticipated barriers, and self-perceived preparedness around the initiation of buprenorphine treatment in the ED.Methods: We conducted a survey of 174 ED providers (attending physicians, residents, and physician assistants) and individual semi-structured interviews with 17 attending physicians working in an ED at a tertiary care academic hospital. The survey was conducted online using an 11-point Likert scale to assess providers' attitudes, current practice, and preparedness to care for patients with OUD. The Kruskal-Wallis and Mann-Whitney tests were performed to determine the differences in the responses based on years of practice and training type. For the qualitative study, individual interviews were recorded, transcribed and coded by four independent coders. Themes were identified using a modified grounded theory approach. Results: Ninety-three ED providers completed the survey, a response rate of 53%. Fifty-seven percent agreed that buprenor-phine should be administered in the ED for patients requesting treatment for OUD. Twenty-two percent reported that they did not feel that prescribing buprenorphine in the ED was within their scope of practice. Compared to providers with fewer than five years of practice, those with more years of practice were: (1) less likely to approve of ED-initiated buprenorphine and (2) more likely to believe that buprenorphine is replacing one addiction with another (p<0.01 for each). They also felt more prepared to discuss overdose prevention and naloxone with patients (p<0.03). Attending physicians and residents viewed ED-initiated buprenorphine more favorably than physician assistants (p<0.01). Analysis of interview data demonstrated several themes. Physicians described a current standard of referring patients with OUD to detoxification programs, which some characterized as a limited or ineffective approach. Providers had mixed feelings about the role that the ED should play in treating OUD. Most physicians felt that a buprenorphine-based intervention in the ED would be feasible with institutional support, including training opportunities, protocol support within the electronic medical record, counseling and support staff, and a robust referral system for outpatient follow-up.Conclusion: Near 80% of ED providers felt that ED provision of buprenorphine was within their scope of practice. However, their perception of buprenorphine varied by years of practice and training type. Most ED providers did not feel prepared to initiate buprenorphine in the ED. Qualitative interviews identified several addressable barriers to ED-initiated buprenorphine.108 134 Vladimir Ivkovic, PhD, Psychiatry Assessment and Prediction of (Post)Traumatic Stress in Operational Firefighting: Pilot Studies Using Multimodality Brain and Psychophysiological Massachusetts General Hospital, Charlestown, MA, USA, 2Psychiatry, Massachusetts General Hospital, Charlestown, MA, USA, 3Center for Space Medicine, Baylor College of Medicine, Houston, TX, USA, 4Internal Medicine, Steward Medical Group, Boston, MA, USA, 5Internal Medicine, Tufts University School of Medicine, Boston, MA, USA, 6Psychology, Tufts University, Medford, MA, USA and 7Translational Research Institute for Space Health, Houston, TX, USA Introduction: First responders are at elevated risk of post-traumatic stress disorder (PTSD) and psychiatric comorbidities, including suicide. The prevalence of PTSD in first responders [6%-32%] is almost five times higher than in the general public [0.3%-6.1%] Predicting PTSD susceptibility remains elusive because realistic firefighting stressors cannot be reproduced in controlled laboratory settings, while gold standard brain and psychophysiological monitoring tools\u2014such as fMRI\u2014cannot be deployed in realistic firefighting environments. The goals of this project are to (1) validate a novel approach for assessing functional brain and psychophysiological indicators of PTSD, and (2) investigate firefighters' responses to operational stressors. The approach is based on integrated use of our custom-developed NINscan technology (Figure 1) in combination with gold-standard biopotential monitoring (ECG, indicators of PTSD. The NINscan system enables mobile brain imaging and psychophysiological monitoring through integrated near infrared spectroscopy [NIRS], (ECG, EDA, EMG, EOG), respirometry, and accelerometry. Here we report preliminary results of two laboratory, and one operational firefighting pilot study conducted at the Boston Fire Department Training Academy [BFDTA]. Methods: Validating NINscan electrodermal activity [EDA] capability. While N=4 (3 males, 1 female; aged 24\u00b13 years) healthy volunteers viewed vertiginous videos to elicit sympathetic response, we collected EDA and ECG using NINscan and standard biopotential monitoring system (BioRadio, Great Lakes Neurotechnologies, Valley View, OH). Standard ECG and EDA montages were used. Cross-validation of NINscan NIRS brain monitoring and EDA. N=2 volunteers (1 male, 1 female; aged 29 years) underwent a standard acoustic-startle protocol. Prefrontal cortical hemodynamics were monitored using NINscan optical sensors. EDA and ECG were simultaneously monitored via NINscan, and standard biopotential recording system (Biopac, Goleta, CA). NINscan deployment in operational firefighting. N=4 members of the BFD participated in pilot data collection at the BFDTA. The data were collected during standard BFDTA training evolutions, which lasted a total of 3 hours. Participants were first fitted with NINscan sensors inside their full personal protective equipment (PPE, Figure 2A), and then performed high-fidelity operational drills including (1) aerial ladder operations (Figure 2B), (2) confined space/ sensory deprivation orientation and self-extrication (Figure 2C), (3) structure fire with \"firefighter down\" scenario (Figure 2D). Brain and scalp/skull oxygenation and perfusion were recorded by four NINscan optical sensors spanning approximate locations F3, Fpz, and F4 (International10-20 system). EDA sensors were placed on the left wrist, and ECG per standard torso montage. A tri-axial accelerometer and gyroscope were attached on the frontal brain pole, and monitored head orientation by tracking changes in linear and angular acceleration. Brain and physiologic data were recorded using NINscan at baseline, during, and after each drill. Psychometric assessments (PCL-PTSD Checklist [PCL-5], State Trait Anxiety Index [STAI], and Positive and Negative Affect Schedule [PANAS]), NINscan usability evaluations were performed at baseline and after each drill. The participants wore full PPE with self-contained breathing apparatus [SCBA].109Results: Validating NINscan EDA: EDA and ECG responses from the two devices were nearly identical (6\u00b12mS) even in single trials (Figure 3). Cross-validating NINscan NIRS: Both NIRS and biopotential data from NINscan indicated sensitivity to acoustic-startle in line with previously published work; the latter correspond to gold standard Biopac data (Figure 4). Firefighting Deployment: The obtained psychometric scores indicated no PTSD symptoms in the four partici - panting firefighters (PCL-5=4.4\u00b17.2), and low levels of perceived stress (STAI=31.9\u00b15.7, PANAS POS=33.8\u00b15.9, PANAS NEG=10.8\u00b11.13). NINscan and data were recorded (Figure 5) for all four firefihter participants. In Figure 5, note the steady temperature rise while inside (building internal air temperature was ~470 deg C at head level), the high and rising HR (derived from ECG), the increase in EDA particularly after reaching the victim, and the differential blood flow response in deep (vl/dl/dmPFC) vs. shallow (scalp) tissue while extricating the \"victim\".Conclusion: These results demonstrate (a) congruence of NINscan and gold standard biopotential recordings, (b) sensitivity of NIRS to standard acoustic-startle paradigm, (c) feasibility of using NINscan for reliably monitoring vl/dl/dmPFC, EDA and other systemic physiological activity-in laboratory and operational firefighting settings. Based on these findings, a full study will be performed on (1) twenty healthy individuals and 20 PTSD patients who will undergo standard fear conditioning and auditory-startle protocols, and (2) twenty firefighter recruits exposed to operational firefighting tasks. We gratefully acknowledge the Boston Fire Department and Boston Firefighters Local 718 for their help and support in design and conduct of this project. 135 Smita Jagtap, Ph.D, Center for Genomic Medicine (CGM) Investigation Cellular Bioenergetics In CDKL5 And RETT Neural Progenitor Cell General Hospital, Boston, MA, USA, 2Center for Genomic Medicine, Center for Quantitative Health, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: A subset of individuals with Rett Syndrome (RTT) but no detectable MeCP2 mutation have been found to have mutations in the X-linked cyclin-dependent kinase-like 5 (CDKL5/STK9). Such mutations are responsible for an early-onset seizure variant of RTT associated with intellectual disability and infantile spasms occurring predominantly in females. Mitochondrial function may play a role in the pathophysiology of these disorders, as they play a critical role in regulating cellular functions in neurons. Specifically, they may contribute to neuroplasticity through neural differentiation, neurite outgrowth, neurotransmitter release and dendritic remodeling. Targeting both mitochondrial bioenergetics and glycolysis pathway is an effective way to investigate mitochondrial function.Methods: To examine metabolic phenotypes in vitro, we have generated induced pluripotent stem cells from RTT (V247X) and and generated clonal isogenic neural progenitor cell lines expressing either the wild type or mutant MeCP2 and CDKL5. Employing isogenic neural models from patients with RTT and CDKL5 deficiency syndrome, we have developed cell-based assays to we identified novel evidence of mitochondrial profile in RTT and CDKL5 patient-derived neural progenitor cells, as measured by differences in oxygen consumption and extracellular acidification as indicators of atypical mitochondrial metabolism which were not apparent in patient fibroblasts. A detectable increase in MitoSOX staining intensity, consistent with bioenergetic profiling of CDKL5 NPC, leads to increase reactive oxygen species (ROS) generation. This study suggests that RTT and CDKL5 deficiency 110syndrome can lead to atypical metabolic phenotypes, reactive oxygen species and induction of aberrant mitophagy-related signals contributing to the complex pathology of the disease. Conclusion: Detailed profiling of cellular bioenergetics of RTT and CDKL5 deficiency syndrome isogenic clones can provide new insight into understanding of the metabolic profile of neural cells and the design of therapeutic strategies. 136 Anthony J. James, BA, BS, Medicine - General Internal Medicine HIV testing in a large community health center serving a multi-cultural patient population: A qualitative study of providers A.J. James1, Marable2, Cubbison1, Levison1 of General Internal Medicine, MGH, Boston, MA, USA, 2Center for Community Health Improvement, MGH, Boston, MA, USA, 3Infectious Diseases Division, MGH, Boston, MA, USA and 4Medical Practice Evaluation Center, MGH, Boston, MA, USA Introduction: In the United States, 15% of people with HIV do not know their HIV serostatus, leading to both individual morbidity and HIV transmission. While CDC guidelines recommend HIV screening for all individuals aged 13-64 years, racial and ethnic minorities in the US continue to present to care with advanced disease. Methods: Our objective was to assess providers' perspectives on factors affecting the provision of HIV testing at an urban community health center serving a predominantly racial/ethnic minority population of low socio-economic status. We conducted 5 focus groups from January 2017 to November 2017 with 74 health center staff: 20 adult medicine/primary care providers, 34 community health workers (CHWs) and community health administrators, 6 urgent care physicians, and 14 behavioral health providers. Interviews were digitally recorded. Three study staff analyzed the transcripts using a grounded theory approach and open coding to develop themes.Results: We identified five primary determinants affecting HIV testing in this setting: 1) provider perception of patient attitudes and beliefs; 2) time and the prioritization of medical and social issues; 3) interprofessional communication; 4) clinical indicators for testing; and 5) knowledge about the importance of routine HIV testing. Primary care physicians desired easier mechanisms to identify patients due for HIV testing and assistance with offering testing to non-English language speaking patients.Conclusion: Training to improve provider comfort with HIV testing, integrating CHWs into medical practice, and a focus on patients' cultural beliefs may all increase HIV testing in a diverse community health center population. 137 Paul S. Jansson, BA, Emergency Implementation of anonymity into a departmental morbidity and mortality conference Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Department of Emergency Medicine, Brigham and Women's Hospital, Boston, MA, USA and 3Department of Emergency Medicine, Harvard Medical School, Boston, MA, USA Introduction: Morbidity and mortality (M&M) conferences are ubiquitous at academic medical centers and are a long-standing feature of post graduate medical education and hospital quality assurance requirements. During these confer-ences, adverse events are presented and analyzed with a goal of improving care. As the culture of safety moves towards balanced accountability for both individual behaviors and system errors, creating a non-punitive culture is critical. We sought to analyze the effect of an anonymous M&M conference on participants' attitudes towards the educational and punitive nature of the conference. We theorized that an anonymous conference might be more educational, less punitive, and would shift analysis of cases towards systems based analysis and away from individual cognitive errors.Methods: We implemented an anonymous M&M conference at an academic emergency medicine M&M conference over a seven-month period. Using a pre-post design, we assessed attitudes towards the educational and punitive nature of the conference as well as the perceived focus on systems versus individual errors analyzed during the conference presentation. Means and standard deviations were compared using a paired T-test.Results: Fifteen conferences were held during the study period and 53 cases were presented. The M&M presenter did not identify any participants but at least one physician voluntarily self-identified in 47% of the cases. Sixty percent of eligible participants (n = 38) completed both the pre- and post-test assessments. There was no difference in the perceived educational value of the conference (4.42 vs. 4.37, p = 0.661) but the conference was perceived to be less punitive (2.08 vs. 1.76, 111p = 0.017). There was no difference between the perceived focus of the conference on systems (2.76 vs. 2.76, p = 1.00) versus individual (4.21 vs. 4.16, p = 0.644) errors. The majority of participants (59.5%) preferred that the conference remain anonymous. Conclusion: We assessed the effect of anonymity in our departmental M&M conference over a seven-month period and found no difference in the perceived effect of M&M on the educational nature of the conference but found a small improve - ment in the punitive nature of the conference. There was no difference in the focus of systems versus individual errors analyzed. Attitudes towards the educational and punitive nature of the conference were favorable at baseline so it may not have been possible to see a large change in the attitudes with our intervention. Furthermore, at least one clinician involved chose to self-identify during almost half of the cases presented which limited the effect of the intervention but also indicates a deeper culture of self-identification in the conference. Overall, there was a small improvement in the punitive nature of the conference, the majority of conference participants preferred the conference to remain anonymous, and there were no negative effects of the implementation measured. 138 Gagan Joshi, MD, Psychiatry MR Spectroscopic Glutamate Activity in High-Functioning Autism Spectrum Disorder Adolescents with and without Emotional Dysregulation G. Joshi1,2, A. G\u00f6nenc3,2, A. Belser1, B. Leon1, C. McDougle1,2 and J. Biederman1,2 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Cambridge, MA, USA and 3McLean Hospital, Belmont, MA, USA Introduction: Previous research has noted significantly high glutamate (Glu) activity in the dorsal anterior cingulate cortex (dACC) of adolescents with high-functioning autism spectrum disorder (HF-ASD) (Joshi et al. 2012), as well as significant correlations between Glu levels in the dACC and severity of Emotional Dysregulation (ED) in youth (Wozniak et al. 2012). This study examines the spectroscopic glutamate activity in the dACC of HF-ASD adolescents with and without ED. Methods: We measured Glu concentrations in the dACC of 36 HF-ASD adolescents (aged 8-18 years) and age- and sex-matched Healthy (HCs), using high field (4.0 Tesla) proton MRS. HF-ASD subjects were categorized with and without ED, as defined by the aggregate score of the CBCL subscale profile for ED (N=29). ASD subjects with ED (>180) were further separated into those with severe emotional dysregulation (SED) (>210) Glu levels in the dACC of adolescents with HF-ASD were significantly higher than age and sex matched HCs (p=0.005). ASD+ED subjects had significantly higher Glu levels than adolescents with only ASD and HCs (p=0.006). Severity of ASD, as measured by the Social-Responsiveness-Scale (SRS), was positively correlated (p=0.057) with Glu levels in the dACC. Adolescents with ASD+SED had the strongest positive correlation (p=0.001) between severity of ED and Glu levels in the dACC.Conclusion: These results highlight the potential for glutamatergic dysregulation in the dACC to serve as a biomarker of ASD and ED in adolescents.112Demographic Characteristics Values expressed as N (%) or Rutwij Joshi, M.D., Medicine - General Internal Medicine Expanding Access to Intranasal Naloxone for High Risk Patients Upon Hospital Discharge R. Joshi2 and E. Tavares1 1Pharmacy, MGH, Boston, MA, USA and 2Internal Medicine, MGH, Boston, MA, USA Introduction: Drug overdose and opioid-involved deaths continue to increase in the United States. In Massachusetts, 2094 cases of fatal opioid-related overdoses were reported in 2016, a 24% increase from 2015 and a 54% increase from 2014. Intranasal naloxone can reverse the effects of an opioid overdose and save lives. Currently there is no clearly defined process for providing naloxone for eligible patients being discharged from Massachusetts General Hosptial. A preliminary review of patients discharged from Ellison 16, an internal medicine and oncology floor at MGH, sought to investigate if patients at high risk of opioid overdose were prescribed intranasal nalxone upon discharge. Patiens with opioid use disorder, discharged with opioid withdrawal medications, or taking 50 mg or greater of morphine equivalents per day were determined to be at high risk for opioid overdose. This restrospective chart review investigated all opioid outpatient prescriptions written for Ellison 16 patients upon discharge from April, 2017 until July, 2017. Of 170 patients reviewed, 94 patients were identified as being at high risk for opioid overdose. Of those 94 patients identified, thirteen (13.8%) patients were prescribed intranasal naloxone upon discharge. A prospective internal review of patients at risk for opioid overdose discharged from MGH from June 23, 2016 to August 25, 2016 investigated how many prescriptions for intranasal naloxone written upon discharge were filled at outpatient pharmacies. Out of 49 patients who were identified as high risk and prescribed intranasal naloxone, 19 (38.8%) filled their prescriptions. Using these two reviews, areas for improvement to expand access to intranasal naloxone included identification of patients at high risk and bedside delivery of intranasal naloxone upon discharge to address any barriers to obtaining the medication. A pilot program was performed on Ellison 16, where a pharmacist driven protocol was implemented to increase intranasal naloxone access for high risk patients upon inpatient discharge. Methods: The pharmacist driven protocol on Ellison 16 involved the daily screening of patients meeting criteria for high risk of opioid overdose by the covering inpatient pharmacist. Criteria included patients taking 50 mg or greater per day of morphine equivalents, patients with active or a history of opioid use disorder, or those on opioid withdrawal agents. Responding clinicians would then be alerted by the pharamcist of their high risk patients and naloxone would then be 113recomended upon discharge. Upon agreement with the responding clinician, the patient would then be seen by the pharma - cist, who would counsel them on the signs and symptoms, and risks of opioid overdose. They would then be asked if they would like a prescription for intranasal naloxone upon discharge. If the patient agreed, the pharmacist would write the order and deliver the medication to the patient while still inpatient, in order to ensure that they left with the medication in hand at discharge. All patients who agreed to bedside delivery were counseled on the proper use of intranasal naloxone, along with their friends and family if available. This initiative took place from December 15, 2017 until February 15, 2018. Conducted as an internal quality improvement project, this investigation did not require IRB approval. The goal of this quality improve - ment project was to increase the average percentage of patients receiving intranasal nalxone upon discharge from Ellison 16 to twenty percent.Results: Upon reviewing the data post intervention, the 28% of patients eligible for intranasal naloxone discharged from Deccember 15, 2017 to January 15, 2018 received intranasal naloxone.Conclusion: This pilot was successful in increasing access to intranasal naloxone to high risk patients upon discharge. The modest percentage increase may be due to limited outpatient pharmacy hours, evening or weekend discharges, and the availability of a pharmacist to counsel patients on naloxone before patients are discharged. Areas for improvement and expansion included a collaborative practice agreement for trained pharmacists in opioid use disorder to prescribe intranasal naloxone upon discharge. 140 Juratli, characterize outcome T. Juratli1, Tummala1, and Brastianos3 1Neurosurgery, Massachusetts General Hospital, Boston, MA, USA, 2Pathology, Massachusetts General Hospital, Boston, MA, USA and 3Neurology, Massachusetts General Hospital, Boston, MA, USA Introduction: Although most meningiomas are WHO (World Health Organization) grade I benign tumors, up to 20% are atypical (grade II) or malignant (III). WHO grades II and III meningiomas relapse in 50-90% of cases after surgical resection, indicative of an accelerated natural history. There is currently no effective systemic therapy to offer patients after surgery or radiation failure, due to the limited understanding of the genetic drivers underlying meningioma recurrence and progression. Methods: We performed a comprehensive molecular characterization of 169 meningiomas from 53 patients with progressive/high-grade tumors, including matched primary and recurrent samples.Results: Exome sequencing in a discovery cohort (n=24) detected frequent alterations in genes residing on the X-chromosome, with somatic intragenic deletions of the dystrophin-encoding and muscular dystrophy-associated DMD gene as the most common alteration (n=5, 20.8%), along with alterations of (by genomic deletion or loss of protein expression) was ultimately detected in 17/53 progressive meningioma patients (32%). Importantly, patients with tumors harboring DMD inactivation had a shorter overall survival (OS) than their wild-type counterparts [5.1 years (95% CI 1.3-9.0) vs. median not reached (95% CI 2.9-not reached), p=0.006)]. Given the known poor prognostic association of TERT alterations in these tumors, we also assessed for these events and found seven patients with TERT promoter mutations and three with TERT rearrangements in this cohort (n=10, 18.8%), including a recurrent novel RETREG1-TERT rearrange - ment that was present in two patients. In a multivariate model, DMD inactivation CI 1.5- 9.9) were mutually independent in predicting unfavorable outcomes. Conclusion: Alterations of the mesodermal gene DMD identify a subset outcomes.114 Autosomal/allosomal alterations and genome-wide SCNA significance levels Mutations and copy number alterations in autosomes and allosomes in samples with whole-exome sequencing (n=32 samples from 24 patients). Outcome of meningioma patients with DMD inactivation (a) Representative MRI of a patient (MGH011) with a WHO grade II cavernous sinus meningioma that acquired a DMD deletion during progression to WHO grade III. (b) Patients with a DMD inactivation had a shorter progression-free survival (1.6 years, 95% CI 1.1-2.2 versus 2.6 years, 95% CI 2.3-2.8, p=0.038). Likewise, patients with DMD inactivation had a median OS of 5.1 years (95% CI 1.3-9.0), which is significantly shorter than patients in the dystrophin-retained cohort (median not reached, 95% CI 2.9-not reached, p=0.006). (c) The independent clinical association of DMD inactivation was substantiated by multivariate analyses, predicting unfavorable outcome in a model inclusive of NF2 mutations, TERT alterations, Age > 60 years, gender and initial grade. 141 Homan Kang, PhD, Radiology Tumor Targetability of PEGylated Fluorophores via Small Size EPR Effect H. Kang, S. Hu, Y. Baek, G. El Fakhri and H. Choi Radiology, Mass General Hospital, Charlstown, MA, USA Introduction: The enhanced permeability and retention (EPR) effect is a crucial concept for solid tumor targeting in cancer nanomedicine. There is, however, a trade-off between the long-term blood circulation of nanoparticles (NPs) and their nonspecific tissue uptake. To define this size-dependent tumor targetability, we report here the EPR effects of small size polymeric NPs in terms of their molecular weight/hydrodynamic diameter (HD), passive tumor targeting, biodistribution, pharmacokinetics, and renal clearance. Methods: To conduct this pharmacokinetic study, mice were injected with 10 nmol of different sizes of ZW800-PEGs (with HDs of c.a. 1, 3, 7, 11, 13 and 19 nm) and blood was collected at the following time points (1, 3, 5, 10, 30, 60, 120, 180, and 240 min). After 4 h post-injection, mice were sacrificed to image organs, and urine was collected from the bladder. The efficiency of tumor targeting was investigated in terms of tumor-to-background signal ratio (TBR) and potential toxicity within HeLa tumor-bearing The elimination half-life (24.37 g -1 min) of PEGylated fluoro- phores increased gradually with the molecular weights of PEGs (1-60 kDa). PEGs smaller than 11 nm showed minimal uptake in major organs except for the kidneys, while PEGs larger than 13 nm started to accumulate in major organs such as the lungs, liver, and pancreas, and stayed longer in blood vasculature. In addition, size-dependent renal clearance was observed that renal clearance of PEGs exponentially decreases (~85 %ID to ~5 %ID) with the HD increase of PEGs. PEG 20 kDa with an HD of 11 nm showed the best performance in tumor targeting with maximized TBR and minimized potential toxicity.115Conclusion: We found that the small size of polymeric NPs (<10 nm) can target the tumor site by the EPR effect, and the total body elimination of administered NPs is vital to enhance TBR and reduce toxicity. Our results lay the foundation of design considerations of NPs for tumor diagnostics and therapeutics. Passive tumor targeting efficiency of ZW800-PEGs. A) ZW800-PEG was injected intravenously into Hela xenograft mice, and the TBRs (tumor/muscle) were measured for 24 h post-injection. White arrowheads indicate tumor sites, and white circles indicate urinary excretion to bladders. B) TBR was measured in xenograft mice shown in (A) at 0.5, 1, 4, 12 and 24 h post-injection. C) The relationship between TBR and molecular weight at 24 h post-injection. The axis for HD size is not in scale. A) Correlation of renal excretion and AUC values with a molecular weight of ZW800-PEG. B) Illustration of size effect on renal clearance and passive tumor targeting efficiency via EPR effect. 142 Lee M. Kaplan, MD, PhD, Medicine - Gastroenterology Identification of novel genetic loci associated with progressive NASH in a bariatric surgery cohort J. Brancale1, I.H. 1GI Division- Obesity Metabolism and Nutrition Institute, Massachusetts General Hospital, Charlestown, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Pathology, Massachusetts General Hospital, Boston, MA, USA and 4Center for Genomic Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Despite the high prevalence and clinical impact of NAFLD, little is known about the factors that determine whether hepatic steatosis is or becomes associated with inflammation, injury and fibrogenesis. Several association studies have demonstrated a significant genetic contribution to the development of hepatic steatosis and to the association of steatosis with elevated serum transaminases. To assess potential genetic contributors to the pathological manifestations of NAFLD, we performed a genome-wide association study (GWAS) on a cohort of patients with severe obesity and histologically-char - acterized NAFLD identified at the time of bariatric surgery.Methods: Patients in this bariatric cohort (N=957) underwent liver biopsy at the time of surgery. Each biopsy was reviewed by a single pathologist and characterized for NAFLD Activity Score (NAS) and stage of fibrosis. Genetic analysis was performed on 717 Caucasian patients in this cohort. For this analysis, we excluded all patients with a history of alcohol abuse, a diagnosis of hemochromatosis or the presence of granulomas, PAS/d-positive globules, or excess iron deposition in the liver biopsy. We employed a two-stage GWAS strategy, with the overall cohort separated into a discovery cohort (cases= 160, controls=283) and a validation cohort (cases=52, controls=222) where cases were patients with NASH (NAS Score 4 or fibrosis stage 2) and controls were patients with steatosis alone. 650K SNPs were assessed directly by microarray, with imputation of an additional 6.9M SNPs. The most significantly associated SNPs in the discovery cohort were tested in the validation cohort, and variants considered significant at a Bonferroni-adjusted P < 0.05. A combined meta-analysis was then conducted on the entire cohort.116Results: Patients in the cohort were 75.3% female with an average age of 46 years and BMI of 47.8 kg/m2. Thirteen SNPs were significantly and PValidation adjusted <0.05) with advanced fibrosis (stage 2) and 7 SNPs were significantly associated with a high NAS ( 4). Of the 20 nominally significant SNPs, the variant rs3745545 within the INSR gene had the highest odds ratio (OR) for an advanced fibrosis stage (OR=16.8 and P= 2.4E-06 in and P= 1.1E-03 in the validation cohort). The variant with the highest OR for an increased NAS score was rs17218712 located within the FREM1 gene (OR=3.421 and P=3.8E-05 in discovery cohort; OR=3.649 and P= 1.3E-03 in the validation cohort). The combined analysis had several significant variants within the PNPLA3 gene correlating signifi-cantly with steatosis, but not NAS score or fibrosis. Conclusion: Patients in the cohort were 75.3% female with an average age of 46 years and BMI of 47.8 kg/m 2. Thirteen SNPs were significantly 5E-05 and PValidation adjusted <0.05) with advanced fibrosis (stage 2) and 7 SNPs were significantly associated with a high NAS (4). Of the 20 nominally significant SNPs, the variant rs3745545 within the INSR gene had the highest odds ratio (OR) for an advanced fibrosis stage (OR=16.8 and P = 2.4E-06 in the discovery cohort; OR=10.9 and P= 1.1E-03 in the validation cohort). The variant with the highest OR for an increased NAS score was rs17218712 located within the FREM1 gene (OR=3.421 and P=3.8E-05 in discovery cohort; OR=3.649 and P= 1.3E-03 in the validation cohort). The combined analysis had several significant variants within the PNPLA3 gene correlating significantly with steatosis, but not NAS score or fibrosis. 143 Amel Karaa, MD, Pediatrics Development of a Mitochondrial Disease Care Network (MCN) in the US A. Karaa Genetics, MGH, Boston, MA, USA Introduction: The Mitochondrial Medicine Society and US-based Patient Advocacy Groups have collaborated to form a clinical care network to formally unify US-based clinicians who provide medical care to individuals with mitochondrial disease; to define, design and implement best practices in mitochondrial medicine building on current consensus guidelines for mitochondrial diagnosis and care; and to optimize management and care to improve patient outcomes.Methods: While clinical research is a crucial part of the endeavor, the primary goal is to optimize mitochondrial disease clinical care including proper evaluation, diagnosis and comprehensive multi-disciplinary care. An oversight committee for the MCN was formed in 2017 involving representatives of the MMS and three US-based PAGs (Foundation for Mitochon-drial Medicine, Mitoaction, and the United Mitochondrial Disease Foundation). Patients-focus groups have been conducted at mitochondrial disease annual meetings. Patient's and families priorities have been catalogued and cross referenced with physicians and health care providers priorities obtained through national surveys. Final criteria for centers selection was based on patients, families, patients advocacy groups and health care providers feedback. Prior experiences from existing models of rare disease centers was also evaluated and considered.Results: We previously highlighted the current clinical landscape and physician practice patterns of mitochondrial medicine in the US, the MMS's attempt at developing consensus criteria for diagnosis and care and the patient's need for improved coordinated care. Here we review the steps taken after the initial review of MCN goals and expectations by stake-holders and showcase advances made in defining the MCN leading to the development of a Request-For-Applications, review of initial applications and launch of the mitochondrial clinical network in June 2018. Twenty-six applications were received after the initial RFA, 2 applications were discarded as they were incomplete, and 2 applications were merged as they were from the same medical center. Twenty-three total applications were scored and reviewed. Academic Centers were primarily distributed along the Eastern, Midwest and Western regions of the US. The South and rural areas were underrepresented. Conclusion: After a successful launch, a pilot phase was initiated with a list of priority projects that will be undertaken by the MCN centers. Projects will be initiated in the fall of 2018 and will include demographic data collection, database creation and clinical outcome measures.117144 Mihriban Karaayvaz, PhD, Cancer Center Unravelling subclonal in Karaayvaz1, MGH, Boston, MA, Institute, Cambridge, MA, USA, 3Cancer Center/Pathology, MGH, Cambridge, MA, USA, 4Pathology, MGH, Cambridge, and 5Surgery, MGH, Cambridge, MA, USA Introduction: Triple-negative breast cancer (TNBC), defined clinically as lacking estrogen receptor (ER) and progesterone receptor (PgR) expression as well as human epidermal growth factor receptor 2 (HER2) gene amplification, represents up to 20% of all breast cancers and is associated with a more aggressive clinical course compared to other breast cancer subtypes. TNBC is a disease entity characterized by extensive inter- as well as intra-tumor heterogeneity, and likely represents multiple clinically and biologically distinct subgroups that have not yet been clearly defined. Multiple lines of evidence suggest that the intratumoral diversity of TNBC is not only a driver of pathogenesis, but also of treatment resistance, metastasis and poor clinical outcomes. To address this issue, we have conducted single-cell RNA-sequencing on >1500 cells from six freshly-collected, untreated primary TNBC tumors. Through detailed computational analyses of individual tumor cells and the subpopulations they encompass, we sought to reveal the phenotypes and biology underlying the genetic evolution and clinical behavior of TNBC.Methods: Human tumor specimens. Fresh tumors from TNBC specimens were collected at Massachusetts General Hospital with approval by the Dana Farber/Harvard Cancer Center Institutional Review Board, and signed informed consent was obtained from all patients. Tumor tissues were mechanically and enzymatically dissociated. Single cell suspensions were collected after removing large pieces of debris. Flow cytometry and sorting Single cells were sorted into 96 well plates containing lysis buffer. Plates were spun briefly, snap-frozen on dry ice immediately, and then stored at -80\u00b0C until further processing. Smart-seq2 was performed on single sorted cells. Libraries from single cells were sequenced as 38 bp paired end on NextSeq 500. Bioinformatics analysis FASTQ files were quantified to transcript per million (TPM) expression values with RSEM 5478 with default parameters, using the reference genome version GRCh38. We employed a two-step combination approach to identify the different cell types that tumors consist of: (1) literature-based list of specific expression markers previously established to define cell types; (2) clustering. In order to identify copy number alterations, each single cell was normalized by subtracting, from the expression of each cell, the average expression of 240 normal epithelial cells profiled in a different study. Expression was quantified as log 2(TPM + 1)/ 10, and all genes with average expression across all cells < 0.1 were removed. The epithelial cells were clustered using the algorithm developed in Monocle and regressing out the patient effect. Expression heatmaps were plotted using the ComplexHeatmap R package 6071.Survival analyses were performed using Cox proportional hazards regression models, and p-values were obtained from log-rank tests.Results: We identified distinct cell populations within the tumors using a multi-step approach involving marker genes together with clustering. As our clustering analysis demonstrated sub-groups of epithelial cells sharing common transcrip- tional profiles but derived from multiple tumors, we next sought to reveal the common biology of these groups via clustering of all epithelial cells while excluding patient-specific effects through linear regression. Using this approach, we identified five clusters of cells. We revealed that distinct cell subsets within each tumor demonstrated the presence of cells with discrete epithelial differentiation status and diverse malignant transcriptional phenotypes. Since single cell analysis may provide enhanced power to reveal tumor cell subpopulations driving poor clinical outcomes, we analyzed the malignant cluster subpopulations for enrichment of distinct gene expression signatures related to aggressive clinical behavior. Remarkably, all three aggressive disease signatures tested were most highly enriched in a single cluster, which we named as cluster 2. We next derived a unique signature consisting of the top most significantly differentially expressed genes in cluster 2. We applied this signature to METABRIC dataset. We observed a statistically significant association between tumors with high expression of the cluster 2 signature and shortened overall survival. Finally, we found that the most enriched pathways were associated with glycosphingolipid biosynthesis and lysosomal turnover, which impinge on cytokine pathways of the innate immune system that were also enriched. Conclusion: By unveiling clinically relevant states and their relationship to genomic evolution at the level of individual cells, this study substantiates the far-reaching promise of single cell analysis in TNBC and other cancer types characterized by extensive intra-tumor heterogeneity. We conclude that intra-tumor heterogeneity is central to the clinical behavior of this disease. Subclonal diversification can give rise to tumor-specific cell populations, but TNBCs are also characterized by subpopulations with shared biological properties across patients. We found that a single malignant subpopulation (cluster 2) was most highly enriched for the highly aggressive disease phenotypes. We then revealed the biological pathways associated with these cluster 2 cells, demonstrating enrichment of genes involved in glycosphingolipid/lysosome function and innate immune sensing and inflammation. Given their relevance to clinical outcomes, our findings may form the basis for the future development of patient and tumor-specific markers that could more accurately identify refractory subpopulations at the time of diagnosis, and thereby predict treatment resistance and prognosis in TNBC.118 145 Megan Kasetty, BS, Massachusetts Eye and Ear Infirmary (MEEI) - Ophthalmology A Novel Contrast Sensitivity Test as a Measure of Functional Vision in Cataract Disease M. Kasetty1,2, R. Silverman1, Z. Luo2, R. Vasan2, and J.B. Miller2 1Tufts University School of Medicine, Boston, MA, USA, 2Ophthalmology, Massachusetts Eye and Ear, Boston, MA, USA, 3Adaptive Sensory Technology, Inc., San Diego, CA, USA and 4Optometry, Nova Southeastern University, Fort Lauderdale, FL, USA Introduction: Traditional letter visual acuity does not always adequately describe the visual limitations or pathologic changes in patients with cataracts. Often times, patients with cataracts will have 20/20 visual acuity but present with functional visual complaints. A complementary test of visual function is contrast sensitivity, which yields information about an individual's ability to see low contrast targets over a range of sizes. Herein, we evaluate the utility of a computerized contrast sensitivity testing device in cataract disease.Methods: Prospective, observational, IRB-approved study. All participants had a cataract in one or both eyes, and underwent quick contrast sensitivity function (qCSF) testing in the eye(s) with cataract using the novel device (Adaptive Sensory Technologies, San Diego, CA) at their regularly scheduled visit. The qCSF method uses computerized, Bayesian, adaptive testing to track changes in a patient's contrast sensitivity with varying spatial frequencies to calculate an area under the curve (AUC). Contrast sensitivity was compared to previously collected data for 71 age-matched healthy controls, represented by calculating a Z-score (AUC-AgeMean)/AgeStdDev.Results: We tested 23 eyes with cataract (n = 17 patients), with a mean BCVA of logMAR 0.10 \u00b1 0.12, and 71 age-matched control eyes. The mean area under the CSF (AUC) (0.90 \u00b1 0.3) was significantly lower in eyes with cataract than controls (1.2 \u00b1 0.3) (p = 0.00002). The same was observed for CS at all individual spatial frequencies tested (p < 0.05). Even the 14 eyes with \"good\" visual acuity (20/25 or better), presented a significantly reduced AUC (p = 0.00003). The reduction in contrast function can be further illustrated by Z-scores, demonstrating a nearly 1.5 standard deviation difference; Z = -1.46 \u00b1 1.16 generally and Z = -1.43 \u00b1 0.87 in patients with good visual acuity. Conclusion: We present a novel contrast test that demonstrated a statistically significant difference in contrast sensitivity function in patients with cataract compared to healthy controls. Further analysis of high letter visual acuity patients still revealed statistically significant reduction in contrast function. CSF assessment may provide another useful clinical endpoint in disease monitoring of patients with cataracts, particularly in patients with high letter visual acuity and functional visual complaints. 146 Wataru Katagiri, MS, Radiology Real-Time Tracking of Vaccination Using NIR Imaging W. Katagiri1,2, J. Lee1,2, M. Tetrault1,2, Hu1,2, S. Kashiwagi1,2 and Choi1,2 1Radiology, Gordon Center for Medical Imaging, MGH, Charlestown, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Efficient and timely delivery of vaccine antigen to the secondary lymphoid tissue is critical to enhance the protective immune response to vaccine. However, there are insufficient studies that determine longitudinal biodistribution of injected vaccine antigen in vivo , as most studies are focused on monitoring immune cell behaviors. Therefore, a new method- ology to track the fate of injected vaccine is desired to establish an optimal dose, formulation, and route of clinical vaccines. Here we report a real-time fluorescence imaging technology that enables visualization of vaccine in vivo and quantitative measurement of their efficacy.Methods: To determine the biodistribution of vaccine antigen in mice, an NHS ester form zwitterionic The labeling ratio of ZW800-1C on a nanoparticle was calculated based on Beer's Law and varied between 0.5 and 4.5. A multispectral real-time fluores-cence imaging system was used to monitor the behavior of NIR-vaccine in animal models under the 800 nm channel. Dose dependancy and pharmacokenical evaluations followed, quantifying the amount of uptake of the NIR fluorophores in lymph 119nodes and major organs. ZW800-1C was also utilized in flow cytometry analysis ex vivo to determine the activation and antigen uptake of antigen presenting cells isolated from the lymph nodes of vaccinated mice.Results: Our results show that (i) vaccine antigen reaches the draining lymph nodes within 30 min after intradermal injection, (ii) accumulation of vaccine antigen at the popliteal lymph node reaches its peak in 6 h, and (iii) > 10% of lymphoid tissue- resident dendritic cells were positive for antigen at 6 h. These results demonstrate that the FLARE system is a powerful tool to determine the biodistribution and quantification of vaccine antigen in vivo in a real-time manner. Conclusion: We successfully demonstrated vaccine visualization with the model antigen conjugated with ZW800-1C. Since vaccine delivery and accumulation in an unexpected site is connected to significant side effects, this method is not only important for optimal vaccine design, but also for evaluation of the safety of vaccine. Fig. Mouse leg and lymph nodes at 24 hours after the injection. 147 Raviv Katz, Massachusetts Eye and Ear Infirmary (MEEI) - Vascular and Ear Infirmary, Boston, MA, USA, 2Faculty of Medicine, University of Portugal, Universit\u00e1rio de Coimbra, Coimbra, Portugal and 4Association for Innovation and Biomedical Research on Light, Coimbra, Portugal Introduction: The role of the choroidal vasculature in the pathogenesis of Age-related Macular Degeneration (AMD) remains only partially understood. Recent studies have assessed choroidal thickness (CT) in this disease using spectral- domain optical coherence tomography (OCT). Swept-source OCT (SS-OCT) has important advantages for choroidal imaging, including enhanced penetration and resolution along with automated segmentation. Furthermore, the acquired en face images can provide additional vessel measurements within the choroid. Herein, we compare choroidal vascular features of eyes with AMD with a control group using SS-OCT.Methods: Multicenter, cross-sectional study. We recruited subjects with AMD and controls without any vitreoretinal disease (> 50 years). All participants were imaged with color fundus photographs, used for AMD staging according to the AREDS classification. In the same visit, they were also imaged with SS-OCT (3D volume). En face images of the choroidal vascula - ture were obtained and converted to binary images on ImageJ. Choroidal vascular density (CVD) was calculated as a percent area occupied by choroidal vessels in the central macular region (a 6-mm diameter circle centered on the fovea) averaged throughout all image slices. CT was obtained using SS-OCT automated software. The central macular choroidal vascular volume (CVV) was calculated by multiplying the CT by the average CVD within this area. Multilevel mixed linear models were performed for analyses.120Results: We included 610 eyes of 327 patients, 85% (n= 520) with AMD (105 early, 320 intermediate and 94 late), and 15% (n= 91) controls. Controlling for confounding factors, we observed that late AMD (vs controls) was associated with a significantly decreased CVD (b= -0.028, p= 0.039), but not CT or CVV. The remaining stages of AMD did not demonstrate any significant association. Among eyes with late AMD, the presence of geographic atrophy (vs choroidal neovasculariza - tion) Our results demonstrate that choroidal vascular density is significantly reduced in advanced stages of AMD, particularly in eyes with geographic atrophy. New imaging modalities should allow further exploration of the contributions of choroidal vessel disease to AMD pathogenesis. 148 Kimberly W. Keefe, MD, Medicine - Endocrine-Reproductive Endocrine Missense Mutations in SOX2 contribute to non-syndromic forms of Isolated GnRH Deficiency (IGD) Revealing a Differential Sensitivity for SOX2 in GnRH vs. Olfactory Neurogenesis K.W. Keefe, M. Stamou, D.L. Keefe, L. Plummer, W.F. Crowley and R. Balasubramanian Harvard Reproductive Endocrine Sciences Center of Excellence in Translational Research in Human Reproduction and Infertility and the Reproductive Endocrine Unit of the Deparment of Medicine, Massachusetts General Hospital, Boston, MA, USAIntroduction: Mutations in SOX2, a gene that encodes a transcription factor critical for the early embryonic development of neural stem cells, cause a severe syndromic form of IGD with anophthalmia / microphthalmia, neurocognitive delay, and epilepsy in addition to their hallmark failure of sexual development and hypogonadotropism. However, the true prevalence, mutational and phenotypic spectra, and biologic windows of importance of SOX2 mutations in IGD patients lacking ocular and neurologic phenotypes (i.e. the of IGD) are not known.Methods: Whole exome sequencing (WES) was examined in 800 IGD patients for rare sequence variants (RSVs) in SOX2 that: a) had a Minor Allele Frequency (MAF) of <0.1% in gnomAD; b) were predicted deleterious by 1 bioinformatic predic - tion program (Polyphen, SIFT a/o Combined Annotation Dependent Depletion [CADD]), c) could be confirmed by Sanger sequencing; and d) examined for segregation analysis in available pedigrees to determine mode of inheritance and penetrance.Results: In IGD patients lacking mutations in known IGD-associated genes, 7 harbored the following new SOX2 RSVs: a) 5 missense, 3 novel, and 1 de novo RSVs; b) 1 novel de novo frameshift; c) 1 novel de novo nonsense mutation; and d) of these 7 SOX2 RSV-bearing IGD patients, 5 were normosmic, 2 were none had complete anosmia. While SOX2 frameshift/nonsense RSVs had previously been shown to cause the syndromic phenotypes of IGD, all these new phenotypes ocular or phenotypes. Interestingly, one IGD patient with a missense SOX2 mutation underwent reversal of his IGD later in adulthood suggesting that the adult requirements for SOX2 may differ from those during development. Conclusion: 1. This largest and most comprehensive cohort of SOX2+ RSVs in IGD reveals that novel LOF contribute 0.6% missense mutations suggests that SOX2 signaling may be more critical for GnRH than for olfactory neurogenesis. 3. A spontaneous reversal of IGD in a SOX2+ IGD patients suggests that SOX2 signaling, while clearly critical for early GnRH neurogenesis, may be more dispensable in postnatal GnRH neurons. 4. Such detailed phenotyping and genotyping studies in carefully assembled human cohorts provide valuable biologic insights that would be difficult if not impossible to obtain by other model systems. 149 Bridget Kelly, Surgery - Surgical Oncology Use of a cathepsin-activatable fluorescent agent and imaging system for real-time, intraoperative detection of residual breast cancer in lumpectomy Kelly1, Oncology, Massachusetts General Hospital, Boston, MA, USA, 2Department of Pathology, Massachusetts General Hospital, Boston, MA, USA and 3Lumicell Inc., Wellesley, MA, USA Introduction: Acquiring tumor-free margins in breast-conserving surgeries is essential in the treatment of breast cancer. Unfortunately, 20-40% of lumpectomy patients require a second surgery for positive margins. Better margin assessment tools 121are needed. Recent research on improving lumpectomy margin assessment has focused on enhanced evaluation of the surface of excised specimens, which can create problems for accurately localizing and excising residual tumor in the lumpectomy cavity. We tested the LUM Imaging System (Lumicell Inc., Wellesley MA), which includes an intravenous cathepsin- activatable dye, a handheld sterile probe, and image analysis software, for instantaneous tumor detection within the lumpec - tomy cavity to achieve tumor-free margins.Methods: Women undergoing MGH received intravenous LUM015 at 1.0 mg/kg 4\u00b1 2 hours prior to surgery. Standard lumpectomy surgery was performed, then the lumpectomy cavity was imaged with the LUM Imaging System. Any areas with LUM015 fluorescence indicating residual tumor were excised. Image acquisition took approximately 1 second per 2.6 cm diameter image. Tumor:normal (T:N) fluorescent signal ratios were determined by transecting excised lumpectomy specimens ex vivo and using the probe on the cut surface where tumor and normal tissue were present. Results were used to refine the tumor detection software algorithm and compared to standard histopathology. Results: Of the 45 patients (median age 60 years, range 44-79) assessed using the LUM Imaging System, 31 (69%) patients were postmenopausal, and 14 (31%) were peri- or premenopausal. 2 (5%) patients had extremely dense tissue, 24 (53%) had heterogeneously dense tissue, 18 (40%) had scattered areas of fibroglandular density, and 1 (2%) had almost entirely fatty tissue. 34 (75%) patients had invasive carcinoma with or without DCIS, 8 (18%) had DCIS only, and 3 (7%) had no residual carcinoma in the main lumpectomy specimen at lumpectomy. Mean tumor size was 1.2cm (0.1-3.5cm) of which, 6 patients had grade 1 tumors, 16 patients had grade 2 tumors, and 17 patients had grade 3 tumors. In total, 569 cavity margin surfaces were imaged intraoperatively and colocalized with histopathology of excised tissue. The LUM Imaging System showed 84% sensitivity and 72% specificity for intraoperative detection of residual tumor within the lumpectomy cavity. Of the 8 patients with positive margins on standard pathology, the LUM Imaging System detected positive margins in 2 (25%) patients, sparing them from a second surgery. In 2 (25%) patients, a negative re-excision was correctly predicted by the LUM Imaging System. In the remaining 4 (50%) patients, the LUM Imaging System corrected predicted positive margins, but the surgeon declined excision. Breast density, menopause status, and histology had no effect on the ability of the system to detect areas of residual tumor. Conclusion: The LUM Imaging System allowed for rapid intraoperative identification of residual tumor in the cavity during breast-conserving surgeries and is a promising tool in reducing positive margins and second surgeries in breast cancer patients. 150 Ani Keshishian, BA, Psychiatry An examination of co-occurring psychopathology among three Avoidant/Restrictive Food Intake Disorder profiles Becker1, MA, USA Introduction: Avoidant/Restrictive Food Intake Disorder (ARFID) was introduced as a new diagnostic category under Eating and Feeding Disorders in the Diagnostic and Statistical Manual of Mental Disorders, 5th edition (DSM-5). Notably, DSM-5 suggests that ARFID comprises three clinical profiles, which may occur alone or in combination: 1) an apparent lack of interest in eating or food; 2) avoidance based on the sensory characteristics of food; 3) fear of aversive consequences resulting from eating. However, there is limited research examining the nature of these profiles, such as associated comorbidity and related distress. Therefore, this study investigated the association between co-occurring psychopathology, specifically anxiety and depression, and the severity of ARFID profiles.Methods: As part of an ongoing study on the neurobiology of ARFID, participants with ARFID and related presentations (n = 45, ages 10- 22, 57% male) completed the Beck Anxiety Inventory, Beck Depression Inventory, and Strengths and Difficulties Questionnaire (SDQ). We assessed the severity of ARFID symptom profiles using the Pica, ARFID, Rumination Disorder Interview.Results: Sensory sensitivity was positively correlated with anxiety (r = .320, p = .034), depression (r = .328, p = .028), and SDQ emotional problems (r = .394, p = .010). Lack of interest and fear of aversive consequences were not significantly associated with depression, anxiety, or SDQ. Conclusion: These results suggest that among individuals with ARFID and related presentations, those with greater sensory sensitivity have greater levels of co-occurring psychopathology and emotional distress. More research is needed on the nature of comorbidities associated with lack of interest in eating and fear of aversive consequences. ARFID is a heterogeneous disorder, and greater understanding of the co-occurring psychopathology of the three profiles may help individualize treatment.122151 Amjad P. Khan, Ph.D., Dermatology Image Guided Therapy a Low Cost Liu2, Center for Phtomedicine, MGH, Boston, MA, USA, 2Department of Physics, University of Massachusetts, Boston, MA, USA, 3Oral and Maxillofacial Surgery, Massachusetts General Hospital, Boston, MA, USA and 4Department of Computer Engineering, University of Massachusetts, Boston, MA, USA Introduction: We develop an approach for the treatment of early oral cancer. Surgery, radiation and chemo therapies are the mainstay of management and can be effective. For early disease however, associated morbidities result in fibrosis, dry mouth and scarring with impaired function. This project combines engineering, optics and biochemistry to produce a mobile LED-based light source with 3D printed light applicators for smart phone-based, image-guided photodynamic therapy (PDT). After validating the devices in preclinical studies, the ergonomics of the printed light applicators was tested in a human study of volunteers at the MGH using video analysis. Once these initial steps were established at the MGH, the device was tested in clinical studies of early oral cancer in India as required by our NCI grant. A pre-cursor of a photosensitizing agent, 5-aminolevulinic acid (ALA) is also part of the heme biosynthesis pathway, sensitized the oral mucosa, smart phone based imaging localized the lesion and the applicators were used to deliver light. The findings suggest that 1.) the LED-based device gave the same results as a commercial medical laser, 2.) the applicators were in general well tolerated and 3.) Of the 16 treatments, all but 3 responded with no residual/recurrent disease so far. None of the cases showed scarring or fibrosis. The significance of this work is that it offers an alternative treatment modality for early disease without associated morbidities. There improved functional outcome which is likely to increase compliance.Methods: A low cost battery-operated PDT device was developed for this study. The main device enclosure houses a high-output 635nm LED with fiber optic coupling, lithium battery, digital microcontroller and touch screen user interface. The PDT light delivery device is also integrated with smartphone-based fluorescence imaging of photosensitizer fluores- cence which entails a small array of blue/violet LEDs that mount over a phone camera and enable fluorescence imaging to help delineate lesion margins to guide light delivery. Intraoral light delivery applicators which attach to the optical fiber were 3D printed (Stratsys 3D printer) with modular design, so that applicators which control beam spot (with pre-calibrated dosimetry) for a particular lesion size, can be coupled with bite blocks that position the applicator angle for a particular lesion position (anterior/posterior buccal or retromolar). Preclinical evaluation used disease model and murine squamous carcinoma (A431) xenografts. In these studies, identical PDT doses were delivered using the experimental battery-powered LED platform side by side with an established turn-key commercial laser system (Intense). Endpoints were tumor volume measured by ultrasound and depth of necrosis. To evaluate the comfort and stability of applicators we performed a study approved by Partners IRB on 10 subjects. The applicators for anterior buccal cheek, posterior buccal cheek and retromolar positions were covered in a hygienic disposable sleeve and positioned for 10 minutes each and subjects were asked to rate comfort and fatigue on a numerical scale. The applicators were fitted with an endoscope that recorded positions of ink marks at target spots (anterior/posterior buccal and retromolar) during the testing period to evaluate stability of the applicators. To assess positioning stability, the recorded videos were processed with custom designed algorithms in MATLAB to track movement of ink marks After most of the research and development was done at the MGH, a clinical study was initiated on eligible patients with confirmed T1N0M0 oral lesions at JN Medical College, Aligarh India. Briefly, patients are given the photosensitizer precursor 5-ALA (DUSA Pharmaceuticals) dissolved in orange juice and treated with a total light dose of 100J/cm2.Results: In preclinical hamster model studies the ALA-PDT treated malignant layer was shed 3 days post-PDT. In A431 murine xenografts with tumor diameters 4-6mm in-vivo ultrasound and photoacoustic imaging techniques consistently show 1-3 mm necrosis and identical results were achieved for both laser and experimental LED sources. The ergonomic study showed that the 3D printed applicators can enable stable and ergonomic positioning for intraoral light delivery during PDT procedures. The device has been deployed for oral Ca treatment in India where of 15 patients treated to date, all but 3 have shown complete tumor response at followup. A year later the first patients shows no signs residual or recurrent cancer. Importantly there was no fibrosis either at almost 16 months after treatment. This was noted consistently with all patients with early cancer and shallow lesion size.Conclusion: With costs of healthcare increasing, we report an affordable technology for the treatment of early oral cancer. Results show that a low-cost LED (cost ~ $500) device provided the same effect in PDT as using a medical laser (~$100K) in preclinical validation and in clinical studies. Furthermore, the 3D printable light applicators provided customizability. The integration with smart-phone imaging to the electronics provides delineation of the cancer to guide light delivery. The results in two human studies, an ergonomic study of normal volunteers and a cancer therapeutic investigation of oral cancer suggest that this overall approach is effective with potential for individualized treatment. Future studies will focus on the precision medicine aspect of the project, where light applicators with different materials and quantitative smart-phone imaging will provide personalized treatments.123152 Hazar Khidir, Emergency Sexual relationship power and periconception HIV-risk behavior among HIV-infected men in serodifferent relationships Mosery2, Matthews6 1Department of Emergency Medicine, Massachusetts General Hospital, Brookline, MA, USA, 2Maternal Adolescent and Child Health Research Unit, Durban, South Africa, 3University of the Witwatersrand, Johannesburg, South Africa, 4Bennett Statistical Consulting Inc, Ballston Lake, NY, USA, 5Simon Fraser University, Burnaby, BC, Canada, 6Massachusetts General Hospital, Boston, MA, USA, 7University of Miami, Miami, FL, USA and 8Oregon Health & Science University, Portland, OR, USA Introduction: Many people living with HIV in South Africa are in stable serodifferent relationships. Gender norms affect HIV risk and likely impact reproductive decision-making within these partnerships. We assessed the level of sexual relation - ship power described by men living with HIV (MLWH) to evaluate how sexual relationship power affects periconception HIV-transmission risk behavior.Methods: We conducted in-person surveys with 82 MLWH reporting a recent partner-pregnancy with an HIV-negative or unknown-serostatus partner in eThekwini District, KwaZulu-Natal, South Africa. Surveys assessed decision-making dominance (DMD), a subscale of the Pulerwitz et al. sexual relationship power scale (SRPS); partnership characteristics; and periconception HIV-risk behaviors including disclosure, condom use, ART use, and sexual and reproductive health communi- cation. Multivariable logistic regression models evaluated associations between DMD score and HIV-risk behaviors.Results: Only 27% (n=22) of men reported disclosing their HIV-serostatus to their partner prior to the referent pregnancy. Nearly 30% of participants reported having condomless sex since pregnancy regularly. Higher DMD scores (corresponding to higher male decision-making dominance within the partnership) were independently associated with non-disclosure of HIV-serostatus to pregnancy partner (aRR 2.00, 95% CI 1.52,2.64), not knowing partner's HIV status (aRR 1.64, 95% CI 1.27,2.13), increased risk of engaging in condomless sex since pregnancy (aRR 1.92, 95% CI 1.08,3.43), and increased risk of having concurrent relationships (aRR 1.50, 95% CI 1.20,1.88).Conclusion: MLWH dominate sexual and reproductive health decisions within partnerships. Efforts to minimize pericon- ception HIV-risk behavior must address gender norms and power inequities to promote the health of men, their partners and families. Decision-making Dominance and HIV-risk Behavior 153 Franklin King, MD, Psychiatry Total Health: Pragmatic Collaborative Care for Cardiac Inpatients with Depression or Anxiety F. King, C. Celano, J.C. Huffman, L. M. Bell Psychiatry, MGH, Boston, MA, USA Introduction: Major depression (MDD) and anxiety disorders are common in patients with acute coronary syndromes (ACS) and heart failure (HF) and are linked to low health-related quality of life (HRQoL), hospital readmissions, and mortality, independent of other risk factors. Despite their associations with poor outcomes, these disorders are underdiagnosed and undertreated in this population. Collaborative care (CC) models, which utilize non-physician care managers to identify psychiatric conditions, coordinate care between psychiatric specialists and outpatient providers, and deliver multimodal evidence-based interventions may help to better identify and treat psychiatric illnesses in cardiac patients. Traditional CC, which focuses simply on the treatment of psychiatric illness, is effective at reducing depression and anxiety but has had limited impact on cardiovascular outcomes. Blended CC is a new model of CC that targets both psychiatric and medical conditions simultaneously and has the potential to impact both psychiatric and medical outcomes. However, blended CC has never been tested for the management of patients with acute cardiac illnesses during the critical post-hospitalization time 124period. Accordingly, we developed a 26-week, phone-delivered blended CC program for patients hospitalized for ACS or HF and found to have a current psychiatric illness (MDD, generalized anxiety disorder (GAD), or panic disorder (PD)) that preceded the admission. We currently are examining its efficacy at improving physical function, HRQoL, mental health, and other key patient-reported outcomes in a randomized, controlled trial (goal N=260) compared to an enhanced usual care (eUC) condition.Methods: Participants enrolled in this study will consist of adults over the age of 18 who are admitted to MGH or Salem Hospital with ACS (unstable angina or acute myocardial infarction) or acute HF, who are found to meet objective criteria for clinical depression, GAD, or PD. Participants with complex psychiatric disease, language barriers and cognitive deficits will be excluded. Following the completion of baseline outcome measures in the hospital, participants will be randomized to the CC or eUC interventions. CC participants will be followed by a trained nurse care manager (CM), who will contact the participant via phone weekly (weeks 1-6) and then monthly (weeks 6-26). Interventions will include medication recommen - dations from the study psychiatrist and/or cardiologist during weekly CC team meetings, which will be communicated to the PCP by the CM. Additional evidence-based behavioral interventions such as relaxation response, cognitive behavioral therapy, or motivational interviewing may also be provided by the CM. For participants randomized to eUC, the study team will notify the participant, inpatient team, and primary care physician of the participant's psychiatric diagnosis and provide the participant with a BP cuff and scale. Follow-up assessments will be completed by phone at 26 and 52 weeks by a blinded assessor. Functional status (Duke Activity Status Index) will be the primary outcome, although we also will measure psycho- logical, health behavior, healthcare utilization, and cardiovascular outcomes will be measured as well.Results: The study is ongoing; baseline characteristics for the initial 35 enrolled participants have been examined. The mean age at time of enrollment was 64.3 years, with self-reported race of black/African-American (17.14%), white (74.29%) and other/nonwhite (8.57%). Pre-existing diagnosis rates were found, respectively, of coronary artery disease (65.71%), ACS (31.43%), prior coronary artery bypass graft (20.00%), type 2 diabetes (51.43%), hyperlipidemia (71.43%), and hypertension (82.86%). 8.57% were smokers. Baseline mean scores for depression using the Symptom Checklist for Depression (SCL-20, range 0-80) were 27.37, and for anxiety using the anxiety subscale of the Hospital Anxiety and Depression Scale (HADS-A, range 0-21). Conclusion: Blended CC models have shown promise in improving outcomes in complex medical patients with psychiatric and medical comorbidities. This study utilizes a pragmatic (effectiveness-focused) design with broad inclusion criteria to mirror \"real-world\" clinical scenarios, and with an inclusive focus on acutely ill patients with multiple psychiatric comorbid - ities. If the intervention employed in this study demonstrates significant improvement in patient-centered outcomes in a cost-effective manner, this intervention may be employed in the clinical care of cardiac patients, with a substantial benefit in functional outcomes, quality of life, as well as healthcare economic burden. 154 Benjamin P. Kleinstiver, Ph.D., Pathology Enhanced CRISPR-Cas12a Nucleases Charlestown, MA, USA, 2Pathology, Harvard Medical School, Boston, MA, USA and 3Center for Genomic Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Genome editing technologies have demonstrated their utility for permanently modifying genetic sequences (gene and base editing), transiently regulating gene expression (epigenome editing), and identifying and reporting nucleic acid species (DNA detection). CRISPR-Cas12a (formerly Cpf1) nucleases possess beneficial properties for all of these applications compared to the more widely used Cas9 nucleases, owing to their recognition of an extended T-rich PAM, catalysis that generates 5'-overhangs in DNA, the requirement for only a single short ~40 nt crRNA, and the ability of Cas12a orthologs to detect DNA sequences and exhibit collateral cleavage of a reporter molecule. However, broader use of Cas12a has been limited by the requirement for an extended TTTV protospacer adjacent motif (PAM) compared to an NGG PAM for Cas9. Methods: To address this restriction, we examined high-resolution 3D-crystal structures of Acidaminococcus sp. Cas12a (AsCas12a) to identify amino acid residues that contribute to PAM recognition. We generated variant proteins with rationally directed mutations at residues that lie within 10 angstroms of the PAM nucleotides, and identified a subset of amino acids whose substitution alter PAM recognition. We generated variants harboring combinations of mutations at these positions and characterized their on-target activities and genome-wide specificities in human cells, and also examined their abilities to enhanced and expand DNA-detection with Cas12a nucleases.Results: We engineered an enhanced AsCas12a variant (enAsCas12a) with dramatically expanded targeting range that encompasses many previously inaccessible PAMs (TTTT, TTCN, VTTV, TRTV, and others). Surprisingly, we also found that enAsCas12a generally showed two-fold higher genome editing activity on canonical TTTV PAM target sites as well 125as significantly improved nuclease activity at lower temperatures. We show that introducing an amino acid substitution associated with this increased activity phenotype into other previously described AsCas12a variants can also improve their editing efficiencies. We demonstrate that enAsCas12a improves the efficacy of important applications of Cas12a, including multiplex nuclease targeting, endogenous gene activation, and C-to-T base editing, all to levels previously unachievable with wild-type AsCas12a. We also find that enAsCas12a improved DNA-detection, by allowing recognition of sequences previously inaccessible with AsCas12a. Furthermore, enAsCas12a possesses high genome-wide specificity and can be paired with novel high-fidelity substitutions to further reduce off-target effects. Conclusion: Taken together, our results demonstrate that enAsCas12a provides an alternative and optimized version of Cas12a with improved targeting range and activity that can be broadly used for a variety of applications. The enAsCas12 nuclease has superior properties compares to wild-type Cas12a for genome editing, epigenome editing, base editing, and DNA detection. 155 Renata M. Knoll, Massachusetts Eye and Ear Infirmary (MEEI) - Otolaryngology Inner Ear Concussion: A Multi-Institution and Ear Infirmary, Boston, MA, USA, 2Physical Medicine and Rehabilitation, Spaulding Rehabilitation Hospital, Boston, MA, USA, 3Otopathology Laboratory, Massachusetts Eye and Ear Infirmary, Boston, MA, USA, 4Sports Medicine, Massachusetts General Hospital, Boston, MA, USA, 5Occupational Therapy, Spaulding Rehabilitation Hospital, Boston, MA, USA, 6Radiology, Massachusetts Eye and Ear Infirmary, Boston, MA, USA and 7Otolaryngology, Massachusetts General Hospital, Boston, MA, USA Introduction: Inner ear concussions, also known as labyrinthine concussions, have long been recognized as one of the possible consequences of head injury. While hearing loss and vestibular dysfunction in the setting of head trauma with concurrent temporal bone (TB) fracture are generally well described, less is known about the pathophysiology of head injury without TB fracture, such as following mild traumatic brain injury (TBI). Dating back to the descriptions of boxers afflicted with Punch Drunk Syndrome in the 1920s, later known as dementia pugilistica, athletes involved in contact sports that sustain mild head injury, such as concussions, have routinely indicated symptoms of audiovestibular dysfunction. Moreover, military personnel and civilians also report tinnitus and dizziness following head injury. Taken together, while hearing loss and vestib-ular dysfunction secondary to head injury is a recognized clinical phenomenon, the precise mechanism remains unknown and under-investigated. Using a multidisciplinary approach utilizing resources across several Massachusetts General Hospital (MGH) institutions, we hypothesize that 1) auditory and vestibular symptoms following head injury are associated with decreased quality of life (Aim 1); and, 2) pathological changes occur in the cochlea and the peripheral vestibular organs following head injury (Aim 2). Methods: For Aim 1, individuals with a history of mild TBI and audiovestibular symptoms were prospectively evaluated at a tertiary care rehabilitation hospital (Spaulding Rehabilitation Hospital) using validated patient reported outcome measures: Hearing Handicap Inventory for Adults (HHIA), Tinnitus Handicap Inventory (THI), Dizziness Handicap Inventory (DHI), and 36-Item Short Form (SF-36) survey. SF-36 scores were converted into health utility values (HUV) using a validated algorithm, where 0.3 is defined as poor health and 1.0 is defined as perfect health. For Aim 2, temporal bones (TBs) of separate cohort of patients evaluated at the Massachusetts Eye and Ear Infirmary with a history of head injury without TB fracture were evaluated by light microscopy. In auditory system, the evaluation included counts of spiral ganglion cells (SGC) with age-matched controls, as well as quantification of hair cells (HC) and pillar cells. In vestibular system, otopathologic evaluation included counts of Scarpa ganglion cells (ScGC) in the vestibular nerves with age-matched controls, vestibular HC and/or dendrites degeneration in vestibular The presence of cochlear and/or vestibular endolymphatic hydrops and obstruction of the endolymphatic duct ware also assessed.Results: In the prospective clinical study (Aim 1), thirty-five patients met inclusion and exclusion criteria. The average age was 52.5 \u00b114.3 years old and 63% were female. The most common symptoms reported were dizziness (74%), hearing loss (68%), tinnitus p=0.024) and DHI scores associated with decreased HUV (r=-0.222, p=0.346). For pathologic study (Aim 2), six TBs from five patients met inclusion and exclusion criteria. The average age of death was 81 years, ranged from 66 to 96 years old, and the average time elapsed between the injury and death was 19, ranged from 5 to 37 years. Individuals with a history of head injury had an average of 65% of SGC loss (range: 43%-84%) compared to historical controls (Figure 1). Two of the six (33%) cases had severe loss of HC and pillar cells along the length of the cochlea, and 67% cases demonstrated moderate-severe loss at the basal turn of the cochleae. In the vestibular system there was an average of 46% of ScGC loss (range 44 to 60%) compared to historical age-matched controls, 126and 50% of TBs had degeneration of HC and/or dendrites of vestibular-end organs. Additional findings include cochlear and/ or vestibular hydrops in 50% the endolymphatic duct (17%, n=1 TB).Conclusion: Patients with a history of head injury demonstrate significant audiovestibular symptoms that is associated with decrease in healthy utility and quality of life. Further, otopathologic analysis in patients with history of head injury demonstrate peripheral cochlear and vestibular changes concerning for long-term degeneration of the sensory neuroepithe - lium. These multidisciplinary findings from across several MGH institutions have implications for mechanism of peripheral cochlear and vestibular pathology in patients following head injury and will assist in creation of head injury prevention and treatment guidelines. Figure 2. High-power view of the modiolus of the left temporal bone in Case 1L This patient had a history of multiple concussions during childhood, and experienced bilateral and progressive hearing loss, with profound deafness on the left side at age of 17. There was no past history of noise exposure, otologic diseases or family history of hearing loss.. Photomicrograph shows spiral canal of the cochlea consists of very few spiral ganglion cells (SGC, arrows). 156 Ann S. Kogosov, Psychiatry Long-term outcomes for survivors of childhood burns; A comparison of Honduran and US samples A.S. Kogosov1, Research, Hospitals for Children -- Boston, Boston, MA, USA, 2Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 3Health Law, Policy and Management, Boston University School of Public Health, Boston, MA, USA, 4Surgery, Massachusetts General Hospital, Boston, MA, USA and 5Psychiatry, Harvard Medical School, Boston, MA, USA Introduction: The Burn Outcome Questionnaire (BOQ) is a validated instrument that assesses the recovery of children and adolescents recovering from burn injuries. The BOQ for 5-18 year olds is a parent completed questionnaire that covers twelve burn-specific physical and psychosocial domains (e.g. pain, appearance concerns). Although the Multi-Center Benchmarking Study (MCBS) collected BOQs on over 800 patients from four Shriners burn hospitals over nearly two decades, there have been no studies using the BOQ in developing countries. On a recent Shriners Hospital outreach trip to Honduras, the parents of patients seen in a follow up clinic completed the BOQ 5-18. The goal was to evaluate the feasibility of using the BOQ5-18 in this setting and its usefulness for conducting a needs assessment.Methods: This project compared the BOQ scores of Honduran patients with those from the MCBS by matching patients on Total Body Surface Area burned (TBSA) and time since burn, which commonly affect burn recovery. Scores on the 12 BOQ domains were compared to examine differences in patient outcomes in the US vs Honduras. BOQ subscale scores are standardized to a mean of 50 with a standard deviation of 10. A patient has an \"at risk\" score if the BOQ domain score is one standard deviation or more below the mean. Demographic information including TBSA, age, and time since burn were obtained for subjects in both samples. A chi-square test was performed to examine differences in rates of risk on the BOQ subscales in the MCBS and Honduran samples.Results: Twenty-nine patients were seen in a Honduran clinic and 28 of them (96%) completed a BOQ 5-18. Risk scores in the Honduran sample were compared to risk scores from all MCBS study participants with BOQs completed at Time 8 (T8), 4 years after baseline. Average time since burn in MCBS T8 (3.88 years) was most comparable to the time since burn of the Honduran sample (4.08 years). Table 1 compares the two samples on time since burn at survey, age, gender and TBSA. The Honduran sample had a mean TBSA of 25.30% and a mean age of 12.0 years, compared to the MCBS 127T8 sample mean of 29.23% TBSA and 14.77 years of age. Findings revealed significant differences in mean age and percentage of male patients (39.3% Honduras vs 72.0% US). Table 2 lists the percentage of each sample scoring at risk on the 12 domains of the BOQ5-18. Results show significant differences in Itch risk in the Honduran sample compared to the US sample (17.9% vs. 0.0%, 2 (2, 77) =13.48, p <.001). Although not statistically significant, a greater proportion of Honduran youth were at risk on Pain, Physical Sports, Transfers and Mobility, Appearance, Family, Emotions, and School subscales compared to the US sample. Most notably, the largest non-significant difference between the two samples was on the Pain subscale (17.9% Honduran vs 6.1% US). Conclusion: BOQs were obtained for 96% of the patients seen in a Honduran outreach clinic for children with burn injuries. This provides compelling evidence that the use of this questionnaire in Honduras, and likely other developing countries, is feasible. There were significant differences in age and gender, while TBSA and time since burn were comparable in both samples. Overall, results show that risk scores on the BOQ did not significantly differ on eleven of the twelve subscales, suggesting consistency of the BOQ's performance among the two sites. Given the current study's trends in higher risk percentages on seven of the twelve BOQ measures for the Honduran sample, our findings point to the possible usefulness of the BOQ to assess needs in the Honduran clinic. Additionally, Pain risk scores were three times higher in the Honduran sample compared to the US sample over four years post burn (on average), which may have wider implications for clinical care. The accessibility to professional medical care and medications for itch and pain is much lower in developing countries. Findings suggest that these disparities in care may have measurable consequences in long-term outcomes for children with burns in Honduras and other low-income countries. The BOQ 5-18 could be a useful addition to routine follow-up care for youth with burn injuries in developing countries. Further studies using the BOQ, with special attention to prevalence of pain and itch, are clearly warranted. 1. Demographics * p<.05 **p<.01; 1 t76 = 2.82, p =.006; 2 2 (1, 78) = 8.02, p =.0052. BOQ Subscale Risk Scores * p<.05 **p<.01 ***p<.001; cutoff score 40. 157 Karen A. Kuhlthau, Ph.D., Pediatrics A Virtual Resiliency Treatment for Parents of Children with Autism: A Randomized Pilot Trial Park1,3 1Psychiatry, Massachusetts General Hospital, Benson-Henry Institute for Mind Body Medicine, Boston, MA, USA, 2Pediatrics, Massachusetts General Hospital, Boston, MA, USA, 3Harvard Medical School, Boston, MA, USA and 4Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: Parents of children with Autism Spectrum Disorder (ASD) experience higher levels of caregiving stress compared to parents of typically developing children, and those of children with other developmental disabilities.1,2 Emotional and financial stressors, paired with a lack of supports and access to health services for their child and family, may impact the health and wellbeing of both parent and child. Resiliency is the ability to cope and adapt when faced with stressful events, and characteristics that promote resiliency may help to buffer parents from the stress related to caring for a child with ASD. 3-6 A comprehensive treatment program focused on the needs of parents of children with ASD in relation to their stress and health has not been developed, particularly one using a video conferencing platform. Methods: This randomized, waitlist controlled pilot trial examines the feasibility and acceptability an adapted virtual mind-body group intervention, the Stress Management And Resiliency Training-Relaxation Response Resiliency Program (SMART-3RP) for parents of children with ASD. The program seeks to decrease distress and increase resiliency, stress coping, social support, and mindfulness with groups (9 x 1.5-hour sessions) delivered through videoconferencing platform. A virtual platform offers the opportunity to unite parents across the United States and enables participation because of scheduling flexibility. From 11/2016-12/2017, 25 parents were randomized to an immediate intervention group and 26 to a delayed control group. Surveys were administered at baseline (T1 intervention group; T1 and T2 3 months apart for control group) and post-treatment (T2 for intervention group; T3 for control group). Outcomes included feasibility, acceptability, 128and efficacy, including distress (analog scale; primary), resiliency (CES), stress coping (MOCS-A), social support (MOS) and mindfulness (CAMS-R) were examined.Results: In terms of feasibility, investigators pre-specified program attendence compliance as 6 sessions; 65% of intervention participants completed 6 sessions or more. Among 72% of intervention group participants who completed the post-treatment survey, 83% reported practicing relaxation response exercises at least a few times a week. In response to the question, \"How successfully do you think this treatment will reduce your stress-related symptoms\" (1=not at all to 9=very), intervention participants responded on average 6.7 (SD=1.8). From T1 to T2, the immediate treatment group showed greater improvement in resiliency relative to the delayed treatment group, (CES; M difference 5.78; p=.038). The immediate treatment group showed a small improvement in distress (VAS) relative to the delayed treatment group, although these differences did not reach statistical significance (p=.23). Immediate treatment participants showed improvements in stress coping (MOCS-A; M difference 7.78; p=.001), social support (MOS; M p=.04) and mindfulness (CAMS-R; M difference 2.68; p=.018). Maintenance effects were observed in the immediate treatment group from T2 to T3 in resiliency (CES), stress coping (MOCS-A), social support (MOS), and mindfulness (CAMS-R). Conclusion: Findings from our randomized pilot trial showed promising feasibility, acceptability, and efficacy. Video conferencing-based interventions may help to better reach, and connect, parents of children with ASD who may otherwise be difficult to engage in programs due to the demands of caregiving The immediate group showed improvements (resiliency, stress coping, social support, mindfulness) or no difference (distress) compared to the waitlist group, as expected. Among the two primary outcomes, improvements in resiliency reached statistical significance, however, distress did not. Post-treatment improvements in psychosocial outcomes were sustained at T3 (6 months post-enrollment). Next steps include modifications to improve the intervention in a larger trial. Funding: This project was supported by the Health Resources and Services Administration (HRSA) of the U.S. Department of Health and Human Services (HHS) under cooperative agreement UA3 MC11054 - Autism Intervention Research Network on Physical Health. This information or content and conclusions are those of the author and should not be construed as the official position or policy of, nor should any endorsements be inferred by HRSA, HHS or the U.S. Government. This work was conducted through the Autism Speaks Autism Treatment Network serving as the Autism Intervention Research Network on Physical Health. 158 Tomohiro Kurokawa, M.D., Ph.D., Surgery - Surgical Oncology C. Ferrone1 and S. Ferrone1 1Department of Surgery, Massachusetts General Hospital, Arlington, MA, USA and 2Department of Microbiology and Immunology, University of North Carolina, Chapel Hill, NC, USA Introduction: The need for an effective therapy for ICC has prompted us to develop a novel immunotherapeutic strategy which targets differentiated ICC cells and ICC cancer initiating cells (CICs). The effector mechanism we have selected is represented by B7-H3-specific CAR T cells since this strategy allows rapid generation of polyclonal T cells with tumor- specificity and potent cytotoxic activity with B7-H3 as the target. Methods: B7-H3 expression was tested by FACS analysis of cells stained with mAb 376.96. The latter was used to generate a B7-H3 CAR. The antitumor activity of B7-H3 CAR T cells was tested in vitro with coculture experiments and in vivo with NSG mice orthotopically grafted with human ICC cells. Results: B7-H3 is expressed on ICC2 and ICC3 cell lines and on the 10 resected human ICC tumors. B7-H3 CAR T cells eliminated almost 100% of human ICC cells in vitro. Intravenous administration of 8, 4 and 2 x 10 6 B7-H3 CAR T cells significantly inhibited the growth of human ICC cells grafted in NSG mice. Survival was extended by 11 days.Conclusion: These results suggest that B7-H3 CAR T cells may be useful for immunotherapy of ICC.129159 Anna A. Kutateladze, Medicine - Endocrine-Reproductive Endocrine Mutations in SOX10 are in Kallmann Syndrome (KS) compared to Waardenburg syndrome (WS) and can occur in KS individuals lacking any WS-like features A.A. Kutateladze, M. Stamou, L. Plummer, D.L. Keefe Jr, W.F. Crowley Jr and R. Balasubramanian Harvard Reproductive Endocrine Sciences Center and Reproductive Endocrine Unit of the Dept of Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Isolated Gonadotropin Releasing Hormone (GnRH) Deficiency (IGD) is a rare Mendelian disorder that presents either as Kallmann syndrome (KS) when Hypogonadotropic (nIHH). (deafness, skin/hair/iris hypopigmentation, Hirschsprung's disease, and neurologic defects), have recently also been implicated in the KS form of IGD. The purpose of this study was to explore the allelic spectrum of SOX10 mutations in IGD and how they differ from those occurring in WS.Methods: Whole exome sequencing (WES) data from 800 IGD patients (382 KS; 418 nIHH) at the Harvard Reproductive Endocrine Sciences Center was queried for SOX10 rare sequence variants (RSVs) fulfilling the following criteria: minor allele frequency (MAF) in gnoMAD control database; b) deleterious predictions by either SIFT, Polyphen or CADD scores; c) confirmed by Sanger sequencing; and d) segregate with IGD within pedigrees. Clinical charts and patient question-naires were reviewed for phenotypic evaluation. Anosmic and hyposmic individuals considered to have KS. KS and WS-associated SOX10 RSVs were also obtained from the Leiden Open Variation Database (LOVD) (Pingault V et al, Am J Hum Genet. 2013;92:707-24).Results: Fifteen new SOX10 RSVs were identified in the IGD population - 1 novel de novo frameshift; 1 novel inherited frameshift; 2 novel frameshifts with indeterminate inheritance; and individ from the IGD cohort. Of the 15 IGD individuals, 14 subjects exhibited KS (8M:6F) while the remaining 1 male subject had nIHH. WS-like phenotypic features were identified in 8/15 IGD individuals: loss (n=3); Hirschsprung's disease and hearing loss Hirschsprung's disease and hearing pigmenta- tion (n=1). Interestingly, a single KS individual harboring a novel SOX10 variant (R397H) underwent spontaneous reversal of his IGD later in adulthood. An additional seven SOX10 RSV [6 missense; 1 frameshift] were also identified in KS individuals reported in the LOVD database. Compared to WS-associated SOX10 variants in the IGD vs. 16/59=27% in WS; of SOX10 missense mutations in IGD (11/17) were localized to the HMG domain of the SOX10 protein, this localization did not differ between IGD and WS.Conclusion: 1. SOX10 mutations that are found in IGD patients primarily associate with the KS phenotype, the neurodevelop- mental form of IGD, compared to nIHH; 2. These SOX10 mutations contribute ~3% of the genetic etiology of KS; 3. SOX10 missense mutations are significantly enriched in IGD vs. WS; 4. The distribution of the missense variants across the SOX10 protein did not differ between IGD and WS; 5. In addition, 7/15 SOX10 RSVs were found in KS individuals who lacked any WS-like phenotypic features. 6. These findings provide yet another example in which mutations in a single mendelian gene previous associated with one clinical phenotype can give rise to pleiotropic phenotypes and other OMIM disorders, probably secondary to distinct genetic/epigenetic modifiers that may determine their varied patterns of phenotypic outcomes. 160 Ilana Ladis, Bachelor of Arts, Psychiatry Avoidance as a Mediator of the Relationship between Disgust and Obsessive-Compulsive Disorder Symptoms: Testing Disgust within the CBT model of OCD I. Ladis1, H. Weingarden1, K. Renshaw2 and S. Wilhelm1 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA and 2George Mason University, Fairfax, VA, USA Introduction: The cognitive-behavioral model of obsessive compulsive disorder (OCD) posits that avoidance and rituals performed in response to anxiety maintain OCD symptoms (Salkovskis, 1999). Recent research has examined the role of emotions other than anxiety in OCD, as well. For example, prior studies have demonstrated a significant, positive relation - ship between disgust and OCD severity when controlling for anxiety, in both clinical and non-clinical samples (Olatunji, Ebesutani, David, Fan, & McGrath, 2011; Tolin, Woods, & Abramowitz, 2006). Moreover, prior studies have used behavioral avoidance tasks to demonstrate a relationship between disgust sensitivity and avoidance responses in college student samples (Deacon & Olatunji, 2007; Olatunji, Lohr, Sawchuk, & Tolin, 2007). To evaluate whether avoidance and rituals performed 130in response to disgust may also maintain OCD symptoms, we examined whether the extent of avoidance or rituals might mediate the relationship between disgust-proneness and OCD symptom severity, when controlling for anxiety.Methods: Participants (N = 174) were recruited through OCD organization websites and met self-reported diagnostic criteria for OCD (mean Yale-Brown Obsessive Compulsive Scale [Y-BOCS] = 24.53, SD = 5.64). Participants completed online self-reports of disgust-proneness (Disgust Scale-Revised), severity of avoidance and rituals (Y-BOCS), OCD symptom severity (Obsessive Compulsive Inventory-Revised), and anxiety (Depression Anxiety Stress Scales-21). We tested two mediational models using Preacher and Hayes' (2008) approach, with 5,000 bootstrapped resamples: one with severity of avoidance as a mediator, and the second with severity of rituals as a mediator. In each model, we estimated direct and indirect effects of disgust on OCD symptom severity, controlling for anxiety severity.Results: When examining avoidance as a mediator, the direct effect (b = .14, p = .01.) was significant, and the indirect effect (b = .02, 95% CI: 0.001 to 0.056) was significant, with a small effect. On the other hand, when examining rituals as a mediator, only the direct effect (b = .15, p < .01) was significant, whereas the indirect effect (b = .006, 95% CI: -.0131 to 0.0419) was non-significant. Conclusion: Results suggest that avoidance in response to disgust may play a role in maintaining OCD symptom severity. This indirect effect may be stronger in an OCD sample that specifically reports disgust-relevant symptoms, like contamination obsessions. More research is needed that examines the role of avoidance and rituals performed in response to disgust within a clinician-diagnosed sample, using temporally-sensitive data. 161 Paula Lara Mejia, B.A., Psychiatry Using fMRI to Investigate Brain and Autonomic Correlates of Post-Exertional Malaise in Chronic Fatigue Syndrome (ME/CFS) P. Lara Mejia1,2 and M. VanElzakker1,2 1Psychiatry, Massachusetts General Hospital, Cambridge, MA, USA and 2Martinos Center for Biomedical Imaging, Boston, MA, USA Introduction: Chronic fatigue syndrome (CFS) is a complex and often disabling disorder, the cause of which has not yet been elucidated. It affects as many as 2.5 million people in the United States, and can be so severe that it confines patients to their home or bed. Post-exertional malaise (PEM) is the cardinal symptom of CFS, which causes a large temporary spike in symptom severity one to two days after exercise. Despite the importance of PEM in CFS, its mechanisms are poorly understood. This study uses high-resolution functional magnetic resonance imaging (fMRI), before and 1-2 days after exercise challenge, to look for resting-state differences in CFS patients versus matched healthy controls.Methods: Previous CFS fMRI studies have focused on structures associated with cognitive function. Because CFS symptoms include neurological and autonomic abnormalities, this study will focus on neuroimmune activation-based fatigue following an exercise challenge. This approach allows us to test a novel hypothesis of CFS (VanElzakker 2013), which states that CFS is caused by an inflammatory process in the afferent vagus nerve. The nucleus of the solitary tract (NST) contains 80-90% of afferent vagus nerve cell bodies, and will thus be used as a seed region to look for differences in functional connectivity in CFS patients versus controls. Mild autonomic challenge will be conducted during the fMRI scan, consisting of a mild breathing challenge and a cold vest challenge. We will also examine afferent vagus nerve function with acute transcutaneous vagus nerve stimulation (tVNS) during the fMRI scan. Blood and urine samples will be taken before and after exercise to measure markers of stress and inflammation.Results: In line with the novel hypothesis of CFS, we expect increased afferent nerve signaling if the afferent vagus nerve is infected. Thus, we expect to see increased resting-state functional activity in the NST of CFS patients. We also predict that mild autonomic challenge and tVNS will reveal functional connectivity abnormalities in the NTS, dorsal vagal complex, and autonomic structures of CFS patients. Lastly, we expect to see pre- and post-exercise challenge differences in markers of stress and inflammation in CFS patient relative to controls. Conclusion: Altogether, this study uses novel approaches to further understand the mechanisms of PEM in CFS.131162 Anna Larson, MD, Neurology Malignancy in tuberous sclerosis complex A. Larson1, A. Geffrey1, Paolini1, V. and E. Thiele1 1Herscot Center for TSC, Massachusetts General Hospital, Boston, MA, USA, 2Neuropathology, Massachusetts General Hospital, Boston, MA, USA, 3Pathology, Massachusetts General Hospital, Boston, MA, USA, 4Radiation Oncology, Massachusetts General Hospital, Boston, MA, USA, 5Pediatric Nephrology, Massachusetts General Hospital, Boston, MA, USA and 6Cancer Genetics Program, Brigham and Women's Hospital, Dana Farber Cancer Institute, Boston, MA, USA Introduction: Tuberous sclerosis complex (TSC) is an autosomal dominant genetic disorder that exhibits numerous benign hamartomas in multiple organ systems. Malignant pathologies in the setting of TSC have also been described in case reports and case series over the past half-century. We aim to further characterize the genotype-phenotype correlations of malignancy in TSC. Methods: The study cohort included patients with TSC evaluated at the Herscot Center for TSC (2005-2017, n = 520) and pediatric patients with chordoma evaluated at the Francis H. Burr Proton Therapy Center (1981-2014, n=122). For each patient identified with a history of malignancy and TSC, the clinical, genetic, radiological, and pathological data were retrospectively reviewed. Results: Seven percent (37/520) of TSC patients had a history of malignancy. Average age at diagnosis of malignancy was 31 years. Seventeen types of cancer were identified. Five individuals had multiple cancer types. Renal cell carcinoma (RCC) and pancreatic neuroendocrine tumor (PanNET) were the two most common types of malignancy identified. Sixty-five percent (26/40) of the TSC-malignancy cohort were female. Thirty-eight percent (10/26) of individuals with had a TSC1 mutation (25% of the MGH TSC population), and population). Thirty-three percent (13/39) of malignancies were diagnosed under the age of 20 years. Thirty-one percent (12/39) of cases developed multifocal or bilateral disease, and 10% (4/39) of cases developed second cancers. There were five parent/child pairs affected by TSC and malignancy. Twelve percent (5/39) of individuals died of their cancer.Conclusion: Malignancy in TSC is more common than previously described. Though many of these cases likely represent co-incident pathologies, RCC, PanNET, and chordoma histological spectrum of the malignancies identified was broad, but epithelial and neuroendocrine were the most common. The data from this study suggest an increased rate of pediatric malignancy in the TSC population. There are some parallels to other malignancy syndromes with a subset of the individuals developing multifocal disease or second cancers, as well as a series of families of TSC and malignancy. Future studies are indicated to further explore disease pathology, diagnostics, screening protocols, and prognostics. 163 Johanna Ledley, MS, Anesthesia, Critical Care and Pain Medicine The efficacy of VVZ-149 in the treatment of the affective component of pain after laparoscopic colorectal surgeries and S. Nedeljkovic2 1Department of Anesthesia, Critical Care and Pain Medicine, Massachusetts General Hospital, Chestnut Hill, MA, USA and 2Department of Anesthesiology, Perioperative and Pain Medicine, Brigham And Women's Hospital, Chestnut Hill, MA, USA Introduction: The pain response is composed of three characteristic states: sensory-discriminative (pain score rating), affective-motivational, (unpleasantness of the pain), and cognitive-evaluative (attitude about pain). Studies have shown that individuals who have a broad affective perspective are less pain tolerant and are more likely to have negative affective state postoperatively. Previous studies have shown a relationship between negative affect and poor pain outcomes which suggests that one way to manage pain is to improve on its negative aspects. VVZ-149 (opiraserin) is a dual antagonist for the glycine transporter, GlyT2, and the serotonin receptor, 5HT2A, and has pain relief effects similar to morphine. The current double blind, randomized, placebo controlled study investigated the efficacy of VVZ-149 in the treatment of pain after laparoscopic colorectal surgeries. The primary outcome was the Numeric Rating Scale (NRS) pain score at rest of subjects receiving VVZ-149 vs placebo, hypothesizing that the pain intensity would be less in the subjects receiving VVZ-149. Secondary outcomes included opioid consumption in the 24-hour period after dose initiation, total Patient Controlled Analgesia (PCA) demands, and the total number of rescue doses of opioid given.132Methods: Adults undergoing laparoscopic colorectal surgery who met the inclusion/exclusion criteria were randomized in a 2:1 ratio to receive an infusion of VVZ-149 or placebo for 8 hours immediately post-operatively. If a pain score of 4 or higher was recorded postoperatively on arrival to the PACU, both the study drug as well as a hydromorphone PCA were started. Assessments at 0.25, 0.5, 1, 2, 4,6, 8, 9, 12, 16 and 24 hour after surgery were documented to assess pain intensity at rest, pain relief, Hospital Anxiety and Depression Scale (HADS), Richmond Agitation-Sedation Scale (RASS), Post Operative Nausea and Vomiting (PONV), total opioid consumption, PCA demands, respiratory depression, Adverse Events, concomitant medications, and vital signs. Results: Eighty subjects were screened, twenty were screen fails; 60 subjects were enrolled, 40 received VVZ-149 and 20 received placebo. The data from 7 subjects was excluded due to failure to meet study inclusion/exclusion criteria, and one outlier was removed, for a total of 52 analyzed subjects (36 who received study drug, 16 who received placebo). Results were compared between VVZ group and placebo group as well as between the subjects who received rescue doses (Rescue group) and those who did not receive rescue doses (Non-Rescue group). The results showed non-significant differences in pain intensity between the VVZ and placebo groups. There was a decrease of 34% in opioid use over the 24 hours for those in the VVZ-149 group over placebo, although this decrease not significant. Those in the Rescue group were noted to have higher HADS anxiety-depression scores and Pain Catastrophizing (PCS) scores. Rescue group patients were found to have a positive correlation between pre-dose pain intensity (the pain score taken upon arrival in the PACU) and HADS anxiety score. Individuals in the Rescue group who received VVZ-149 had lower total opioid consumption in the first 9 hours. In patients who required Rescue dosing, For those patients in the Rescue group who were in the placebo arm, the pain intensity remained greater or equal to 4 through the first 24 hours post operatively. Patients In the Rescue group who received placebo also showed higher levels of total opioid consumption compared to those who received VVZ-149.Conclusion: The results show that subjects who received VVZ-149 instead of placebo had a non-significant reduction in pain intensity while using almost 50% less opioids over 24 hours without an increase in adverse events. As all subjects were able to use IV PCA and self-titrate opioids in the postoperative period, this allowed for pain intensity to be reduced similarly between the two groups. Analysis of subjects' HADS scores indicates that anxiety was a major contributor for patients to receive Rescue doses of opioid in addition to PCA self-dosing. This suggests an association between anxiety, pain intolerance, and higher levels of opioid consumption. Individual differences when reporting pain intensity are likely modulated by the affective component of pain. In the group of patient who required Rescue dosing of opioids, which correlated to higher levels of anxiety, the subjects who received VVZ-149 used less opioids and had lower pain intensities compared to subjects who received placebo. This indicates that one of the mechanisms of action of VVZ-149 may be to decrease the negative affective state of pain, leading to a decrease in the use of opioids in the postoperative period. 164 Grace C. Lee, MD, Surgery - General and Gastrointestinal What is the Risk of Anal Carcinoma in Patients with Anal Introduction: The risk of anal squamous cell carcinoma (SCC) following a prior diagnosis of anal intraepithelial neoplasia (AIN) III is unclear. In this study, we estimate the risk of anal SCC in patients with AIN III, and identify predictors for subsequent malignancy.Methods: We performed a retrospective review using the Surveillance, Epidemiology, and End Results (SEER) registry (1973-2014) and identified a cohort of patients with AIN III. Patient demographics, tumor factors, treatment administered, and date of last follow-up were recorded, as was prior and subsequent neoplasm development. Patients who had a diagnosis of anal cancer prior to AIN III diagnosis were excluded. The primary outcome was rate of subsequent anal SCC. Predictors of anal SCC development after AIN III diagnosis were identified using logistic regression and Cox proportional hazard models.Results: A total of 2,074 patients with anal intraepithelial neoplasia III were identified and followed for a median (interquar - tile range) time of 4.0 (1.8-6.7) years. Of the cohort, 171 patients (8.2%) subsequently developed anal cancer. Median (interquartile range) time from anal intraepithelial neoplasia III diagnosis to anal cancer diagnosis was 2.7 (1.1-4.5) years. Fifty-two patients (30.4%) who developed anal carcinoma were staged T2 or higher. Ablative therapies for initial anal intraepithelial neoplasia III were associated with a reduction in risk of anal cancer (odds ratio 0.3, 95% confidence interval 0.1-0.7, p=0.004). Time-to-event analysis revealed anal intraepithelial neoplasia III was 9.5%, or approximately 1.9% per year. Conclusion: In the largest published cohort of patients with anal intraepithelial neoplasia III, nearly 10% of patients were projected to develop anal cancer within 5 years. Nearly one-third of anal cancers were diagnosed at stage T2 or higher despite a prior diagnosis of anal intraepithelial neoplasia III. Ablative procedures were associated with decreased risk of cancer. 133This study highlights the considerable rate of malignancy in patients with anal intraepithelial neoplasia III and the need for effective therapies and surveillance. Figure. Kaplan-Meier curves depicting anal squamous cell carcinoma-free survival in patients with previous AIN III, separated by type of surgical intervention for prior AIN III. 165 Jeungchan Lee, PhD, Athinoula A. Martinos Center for Biomedical Imaging Machine learning-based prediction of clinical pain using multimodal neuroimaging and Biomedical Engineering, Charlestown, MA, USA, 2Korea Institute of Oriental Medicine, Daejeon, Korea (the Republic of), 3Brigham and Women's Hospital, Boston, MA, USA, 4Center for Pain Research, University of Pittsburgh, Pittsburgh, PA, USA, 5Lausanne University Hospital (CHUV), Lausanne, Swaziland and 6Beth Israel Deaconess Medical Center, Harvard Medical School, Boston, MA, USA Introduction: While self-report pain ratings are the gold standard in clinical pain assessment, they are inherently subjective in nature and significantly influenced by multidimensional contextual variables. While objective biomarkers for pain would substantially aid pain diagnosis and development of novel therapies, reliable markers for clinical pain have been elusive. In this study, individualized physical maneuvers were used to exacerbate clinical pain in chronic low back pain patients, thereby experimentally producing lower and higher pain states.Methods: Multivariate machine-learning models were then built from brain imaging (resting-state blood-oxygenation- level-dependent and arterial spin labeling functional imaging) and autonomic activity (heart rate variability) features to predict within-patient clinical pain intensity states (i.e. lower versus higher pain) and were then applied to predict between- patient clinical pain ratings with independent training and testing datasets. Results: Within-patient classification between lower and higher-clinical pain intensity states showed best performance (Accuracy=92.45%, AUC=0.97) when all three multimodal parameters were combined. Between-patient prediction of clinical pain intensity using independent training and testing datasets also demonstrated significant prediction across pain ratings using the combined model (Pearson's r=0.59, P<0.001). Classification of increased pain was weighted by elevated cerebral blood flow in the thalamus, prefrontal, and posterior cingulate frontoinsular Our machine-learning approach introduces a model with putative biomarkers for clinical pain, which has multiple clinical applications, from pain assessment in non-communicative patients to the identification of objective pain endophenotypes that can be used in future longitudinal research aimed at discovery of new approaches to combat chronic pain.134 Increased clinical pain intensity after physical maneuvers A. Significant brain features contributing to within-patient classification (lower vs. high clinical pain states), B. Significant difference in high-frequency heart rate variability power between low and high clinical pain states. C. Synergistic effect of multimodal putative biomarkers for within-patient classification, D. Prediction of clinical low back pain ratignsusing all multimodal parameters 166 Yvonne Lei, Psychiatry Goals of Care and Priorities for Discussion Among Patients with Advanced Breast Cancer Y. Lei, K. Quain, G.K. Perez, R.B. Jimenez, J.A. Shin, K. Sepucha and J. Peppercorn Massachusetts General Hospital, Boston, MA, USA Introduction: Patients with metastatic breast cancer (MBC) face complex treatment decisions with implications for quality of life and survival. We sought to promote individualized care for patients with metastatic breast cancer by empowering patients to identify their preferences and goals of care and to facilitate shared decision in the breast oncology clinic through a simple and scalable intervention.Methods: Based on structured patient interviews, we developed a brochure and 1 page Individualized Goals of Care Discus- sion Guide (IGCDG) survey that was administered to patients with MBC at the time of treatment decisions. Eligible patients were any adult with metastatic breast cancer within 3 years of diagnosis who was faced with new diagnosis, disease progres-sion, or the need to change therapy. Patients also completed a baseline sociodemographic and the National Comprehensive Cancer Network Distress Thermometer. For this analysis, we examined the baseline reported goals of care and preferences for discussion as reported on the IGCDG. Results are descriptive and include preliminary evaluation of sociodemographic and distress correlates of response. Results: Among 36 patients approached for the study, 24 participated and completed the baseline and intervention surveys. This 67% participation rate exceeds the 50% participation rate used to define feasibility of the intervention. Accrual is ongoing with a target of 40 participants. Participants were between the ages of 31 and 76 (average age of 58) and the majority were white (87.5%) with household incomes above $50,000 (80%). The mean baseline distress level was 3.3, with a range between 0 and 8 on a scale of 10 on the NCCN Distress Thermometer. In terms of goals of cancer care, virtually all patients expressed concern for cancer response (96%), survival (92%), and quality of life (92%). A sizeable minority of patients were also concerned with convenience of therapy (46%) and impact on appearance (43%). Top priorities for discussion at the next visit were: 1) Quality of life/symptom control (74%) and 2) Disease directed therapy (68%). Many patients wished to discuss physical symptoms including pain (50%) and fatigue (39%). Younger patients (below the mean of 58), compared to older patients, were less interested in discussing clinical trial options (46% vs. 67%) and nausea (9% vs. 33%) and more concerned with discussing insomnia (33% vs. 8%). Due to the small sample size, these differences are not statistically significant. Patients with high distress levels (defined as 4 or above on the NCCN Distress thermometer) were more concerned with treatment directed at the cancer (89% vs. 54%), coping with cancer (89% vs. 64%), and pain (70% vs. 36%) compared to patients with lower distress. Of note, patients who had not previously participated in clinical trials were equally interested in discussing trial options as those who had participated in clinical trials.Conclusion: Administration of a brief brochure and 1 page Individualized Goals of Care Discussion guide is feasible and provides patients with metastatic breast cancer a chance to define their goals of care and priorities for discussion in clinic. While all patients in this study were concerned with both survival and quality of life, priorities of discussion at the time of 135treatment decision varied. Individual preferences may guide care and should be assessed in all patients at the time of treatment decisions. Further investigation will evaluate the impact of the IGCDG on patient satisfaction and shared decision making. 167 Chotinij Lertphanichkul, MD, Dermatology Characterization of Transient Adaptation from Physicians' and Patients' Perspectives C. Lertphanichkul, M. Cueva, M. Alora-Palli and M. Wanner Dermatology, Massachusetts General Hospital, Boston, MA, USA Introduction: \"Transient Adaptation\" is the term used by the cosmetic industry to refer to skin reactions that follow initiation of new skin care products and that resolve after a short period of time. This terminology is not recognized in the medical literature. Is \"Transient Adaptation\" a real phenomenon? What is it? How prevalent is it? Ultimately, understanding these transient skin reactions can better serve patients by identifying those who are susceptible and by potentially preventing these reactions. The aim of this study is to characterize the nature of the \"Transient Adaptation\". Methods: We developed two questionnaires: one for health care personnel and another one for patients. The questionnaires were validated by 2 dermatologists (MW and MBA). The health care personnel survey (Figure 1), which was conducted between 11/1/2017-4/30/2018 and was distributed at the 2017 Harvard Laser and Aesthetic Skin Therapy meeting, the MGH Dermatology faculty meeting, the 2018 American Society for Laser Medicine and Surgery meeting, and through the RedCap Survey to other dermatologists. The patient survey (Figure 2), which was conducted between 3/23/2018-5/21/2018 and was completed by patients from the MGH Medical Dermatology Clinic, MGH Cosmetic Center, and subjects who responded to our advertisement on Craigslist. Craigslist participants completed the questionnaire at the MGH.Results: Part I - the health care personnel survey. A total of 93 health care personnel (HCP) completed the questionnaire. Seventy-nine percent of HCP surveyed were dermatologists who have practiced for a mean of 12.5 years. Eighty-seven percent of those surveyed reported identifying reactions to topical products in their patients. Irritant contact dermatitis, including patient reported symptoms of stinging, burning, and itching was the most common skin reaction (72% of reactions). Allergic contact dermatitis and rosacea flare were the next most common skin reactions, occurring in 34% and 31%, respec-tively. Nearly 80% of the reactions were seen 2-8 days after starting the product. The reactions typically lasted 2-8 days (72%). HCP identified rosacea (66%) and eczema (72%) as predisposing factors. Many also felt that a prior history of skin reactions (74%) and concomitant use of multiple products (65%) increased the risk of these reactions. Most providers recommended discontinuing the product or decreasing the frequency of use. Part II - the patient survey. Among 105 subjects who completed the questionnaire, 94 subjects were eligible for the data analysis if they had used skin care products on the face. The mean age of the participants was 45.21 \u00b1 13.61 years, 89% were female, and 72% were Caucasian. Eighty percent of the participants reported skin reactions after starting skin care products. Among these participants, 50% had reactions at least sometimes and 12% had skin reactions every time or most of the time when they started products. The most frequent baseline skin conditions were dryness (50%), followed by acne (28%), rosacea (18%), and eczema (11%). Of subjects who reported eczema as a skin condition, 80% reported that they experienced reactions at least sometimes. Conversely, only 4% of participants who never or rarely had skin reactions after starting products reported a history of eczema. The most common skin reactions were dryness (45%) and burning/stinging (42%). Other reactions that were not as common were acne (36%) and flaking (29%). Skin reactions usually occurred within the first 8 days after starting the new skin care products and lasted for 8 days. Most participants (73%) stopped using the product when the reaction occurred. Others decreased the frequency of use (24%), applied moisturizer (22%), or applied cortisone (12%). A great majority of participants reported that they could use the product again, despite developing a reaction. Conclusion: Transient Adaptation appears to be a real phenomenon. Most providers reported seeing reactions after patients initiate new skin care products. Also, 50% of patients reported skin reactions at least sometimes to new skin care products, which occurred within the first 8 days and lasted for 8 days or less.136 168 Nina Levar, Ph.D., Psychiatry Altered amydala function during emotion processing is associated with PTSD and marijuana use N. Levar1,2,3, Massachusetts General Hospital, Somerville, MA, USA, 2Center for Addiction Medicine, Massachusetts General Hospital, Boston, MA, USA and 3Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Boston, MA, USA Introduction: Deficits in emotion processing and emotion regulation have been implicated in both the development and maintenance of post-traumatic stress disorder (PTSD) in people who have experienced a traumatic event. Comorbid marijuana (MJ) use has sharply increased in veterans with PTSD in the past decade, with many individuals using MJ to 'self-medicate' the disorder or the accompanying symptoms. Previous research demonstrates dysfunction of the limbic and paralimbic system circuitry in emotion processing tasks in individuals with PTSD, particularly within the amygdala. Similar deficits in emotion-processing and altered brain activity have been reported in people who use marijuana, however the relationship between MJ use and PTSD symptoms has not been fully explored through neuroimaging. The aim of this neuroimaging study was to investigate the neural correlates of PTSD and the role of MJ on neural function during the processing of emotional pictures.Methods: Forty-five adults (target N=54) between 18-55 years (Mean = 29.67, SD = 8.13) who experienced a serious traumatic event completed a two-day neuroimaging study. Fifteen marijuana users with current PTSD symptoms (PTSD+MJ; PCL score 30), 12 non-users with current PTSD symptoms (PTSD; PCL score 30), and 18 non-using trauma-exposed controls with no current or past PTSD (TEC; PCL < 20) were enrolled. During the screening visit, participants completed a timeline follow-back interview to assess MJ use patterns over the past three months as well as lifetime use. PTSD symptoms were assessed using the Posttraumatic Checklist for and the Clinician-Administered PTSD Scale for DSM-5 (CAPS-5). During the second visit, MRI scans were acquired using a Siemens Skyra 3.0 Tesla with a 32-channel head coil at the Martinos Center for Biomedical Imaging. Participants completed an emotion processing task using negative, positive, neutral (International Affective Picture System; IAPS) and MJ-related pictures to examine emotion processing during functional MRI (fMRI). Functional MRI data were analyzed using FSL (FMRIB's Software Library, www.fmrib.ox.ac.uk/fsl).137Results: Functional MRI analyses showed that brain activity during emotion processing was associated with both PTSD and MJ use. We used a region-of-interest (ROI) analysis to examine group differences in amygdala function during positive and negative emotion processing. During the processing of negative compared to neutral pictures, PTSD showed increased activa- tion relative to TEC in the left amygdala (Figure 1). Similarly, PTSD+MJ showed increased activation in the left amygdala compared to TEC, as well as greater activation than PTSD. During positive versus neutral picture processing, both PTSD and PTSD+MJ showed reduced activation in the right and left amygdala. There were no significant differences in amygdala activation between PTSD and PTSD+MJ. PTSD and PTSD+MJ did not differ in PCL scores and CAPS-5 symptom severity scores (PCL: p = 0.40; CAPS-5: p = 0.97). Conclusion: PTSD and MJ use were associated with altered amygdala function during emotion processing. PTSD and PTSD+MJ both showed increased amygdala activation compared to TEC in response to negative pictures, and decreased activation compared to TEC in response to positive emotional pictures. Together, these findings suggest that MJ use might further increase hyperactivity of the amygdala during negative emotion processing but might not have a significant impact on the PTSD-related alterations in amygdala function during the processing of positive pictures. Future analyses will examine whether alterations in amygdala function are associated with differences in symptom severity and MJ use patterns. 169 Qian Li, MD, Radiology The Value of Ultrasound Shear Wave Elastography in Quantification of the Liver Status in Machine-perfused Pig Livers Q. Li1, N. USA and 2Division of Transplantation, Department of Surgery, MGH, Boston, MA, USA Introduction: Reliable quantitative imaging marker has not been established to evaluate the viability of donor livers during subnormothermic machine perfusion (SNMP) preservation. We aim to explore the feasibility and validity of shear wave elastography (SWE) to quantitatively evaluate the status of donor liver in machine-perfused pig livers. Methods: Five pig ex-situ liver were perfused through the portal vein (PV) and hepatic artery (HA) using the subnormo- thermic machine perfusion (SNMP). In three livers, SNMP began immediately after 2 hours of static cold storage (SCS), and in the other two livers, SNMP began after 2 hours of SCS and 1 hour of warm ischemia. The SWE measurement was performed using a linear array ultrasound probe in a homogeneous parenchymal region without big intra-hepatic vessels, which was kept consistent throughout each experiment. The variability of SWE measurements was calculated by comparing difference of individual measurements from the median value at each time point. Results: As the SWE variation (ranging 0.01-0.07 m/s) was minimal compared to the SWE median (ranging 1.28-1.98 m/s), the mean value of the10 SWEs at each time point was used for liver stiffness analyses. During the machine perfusion, the SWE pattern in all cases is similar in the perfusion duration independent of warm ischemia: SWE decreases with SNMP resuscitation of the graft liver. The SWE changes became minimal after the 2hours of perfusion (case 3: 1.78 vs. 1.75 m/s, case 4: 1.54-1.3 m/s, and case 5: 1.29-1.3 m/s), indicating the liver stiffness may reach a plateau after 2 hours perfusion. Compared to the first time-point measurements, the SWEs decreased significantly after 1-hour perfusion in all cases, which demonstrated the liver status changes after perfusion were detectable on SWE. The liver stiffness increased during warm ischemia, and after the perfusion began, SWE experienced an immediate reverse increase in case 5, from 1.44 to 1.60 m/s, indicating the ischemia-reperfusion liver damage. SWE changes with perfusion were validated by the improvement of ATP contents, vascular resistance, and lactate levels. (Figure 1)Conclusion: In the ex-situ machine-perfused pig livers, our preliminary results demonstrated the SWEincreased during the warm ischemia, and the changes of liver stiffness was detectable on SWE. The SWE offered potential for the quantitative evaluating the quality improvement of the machine-perfused pre-transplant liver, more importantly, it possibly can provide the information regarding graft ischemic injury distribution.138 Figure 1. Comparison of changes in vascular resistances and the changes in SWE during warm ischemia time and MP. (A) PV resistance increased during warm ischemia and decreased during oxygenated MP which was in line with the changes in the SWE values. (B) HA resistance showed a similar decreasing trend during MP after warm ischemia as the SWE values. (The values are presented as median \u00b1IQR). Figure 2. Comparison of changes in lactate levels in perfusate, the liver ATP content (pmol/mg) and the changes in SWE(m/s) during warm ischemia time and SNMP. The SNMP started right after 2-hour cold ischemia in case 3, while started after the extra 1-hour warm ischemia in case 4 and 5.(A)Lactate levels in the perfusate increased during warm ischemia time and then decreased significantly during the following oxygenated MP (p=0.008) in case 3.(The values are presented as median\u00b1IQR) As liver ATP content increased during oxygenated MP, SWE values decreased in case 3 and 5 (B, D), indicating improvement in liver quality, but both metrics dropped with the perfusion in case 4 (C), which may be ascribed to the excessive liver damage of the prolonged warm ischemia. 170 Qian Li, MD, Radiology The Optimal Measurement Number of Shear Wave Elastography (SWE) for F 2 Liver Fibrosis Diagnosis: Percentage of Color Filling Index (PCFI) Stratification Analysis using Automated Image Quality Analysis Q. Li1, N.D. Mercaldo2,1, M. MGH, Boston, MA, USA, 2Radiology, Institute for Technology Assessment (ITA), Boston, MA, USA and 3MIT, Cambridge, MA, USA Introduction: Automated image quality analysis may have the potential to reduce the SWE measurements number for liver fibrosis diagnosis, thus improving the scanning process and diagnostic performance of SWE. This study aims to explore the optimal and minimumnumberofSWEmeasurementsfor F2 fibrosis considering measurement variability and diagnostic accuracy.Methods: Prospectively collected adult patientswhounderwentSWEbeforenon-focal liver biopsy in three years. 10 SWE measurements were performed for each patient at the identical biopsy locations (Segment 8) by one experienced sonogra-pher right before the biopsy. Fibrosis stages were evaluate using METAVIRcriteria (F0-F4). The optimal and minimum measurement number for F2 were analyzed: (1) for each measurement number (1-9), correlated median SWE estimates were obtained using a nested bootstrap simulation; (2) ROC curves, and the associated AUROC values, were estimated using each set of SWE measurements (e.g., 2-10 measurements); (3) steps (1) and (2) were repeated 1000 times to obtain empirical distributions of median SWE estimates, and AUC estimates. Two variation parameters were evaluated, including (1) intra-subject SWE variation (m/s): the absolute median difference between individual SWEs and median SWE, and (2) interval width difference (IWD): the width differences of AUC 95%CI between a measurement number (1-9) and 10. A subgroup of patients with 50% PCFI images (percentage of valid SWE pixels over the entire SWE image box, a metric for SWE image quality assessment) was also selected.Results: 245 cases were SWE estimate decreased from 3.12 to 0.46m/s in 1-10 measurements in overall patients (n=245), and decreased from 3.74 to 0.37 in images with 50% PCFI (n=106). we didn't detect AUC differences between measurement numbers 1-9 and 10, but IWD, which represents the variation of diagnostic accuracy, dramatically increased when measurements number 7 in overall patients, and 6 in PCFI50% group, depending on the clinicians desired precision of IWD (within 0.040 units). Conclusion: Current results demonstrated that, for F2 fibrosis diagnosis, the minimum SWEn umbers may be reduced to7 in overall images, and 6 when 50% PCFI was applied for image quality control. This study also provided the methodology to comprehensively evaluate the optimal measurement numbers for liver fibrosis, considering both the measurement variations and diagnostic performances.139Table 1. The Comparison of AUC and IWD between Different SWE Measurement Number and 10 * Optimal measurement numbers in overall patients, and in patients with images of PCFI 50% IWD: interval width difference, AUC: area under the curve. Figure 1.The variations changes with SWE measurement numbers. As the number of measurements increases, the intra-subject variation (m/s) decreases (a), and the IWD which represents the AUC variation decreases (b). Blue lines: overall cases, orange lines: cases with images of 50% PCFI, IWD: interval width difference. 171 Selena Li, Surgery - Surgical Oncology Outcomes of extended lymphadenectomy (LAD) for gastroesophageal carcinoma (GEC): A large Western D.W. Rattner2 and J.T. Mullen2 1Harvard Medical School, Cambridge, MA, USA and 2Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: To examine perioperative risk and survival outcomes in a large Western series of patients undergoing D1 versus extended (D1+/D2) LAD for GEC.Methods: Single institution, retrospective chart review. Clinicopathologic and treatment factors were analyzed for their impact on survival by univariate and multivariate regression analyses. Setting: Academic medical center. Patients: 520 patients with GEC who underwent potentially curative gastrectomy from 1995-2017, including 362 (70%) patients undergoing D1 LAD and 158 (30%) undergoing D1+/D2 LAD. Interventions: None Main Outcome Measures: Perioperative morbidity and mortality, lymph node yield, and overall survival.Results: Median follow up was 3.1 years. Patients undergoing D1+/D2 LAD were more likely to have distal tumors, to undergo distal/subtotal or total gastrectomy, and to undergo surgery at a more contemporary time (median year 2011 versus 2006) than patients undergoing D1 LAD. The median number and percentage of patients with 16 examined lymph nodes were 16 and 53% versus 27 and 89% in the D1 and D1+/D2 groups. There were no differences in the rates of major complications (16.6% versus 14.6%; p>0.05) or operative mortality (2.8% vs 0.6%; p=0.22) between the D1 and D1+/D2 groups, respectively. Patients undergoing D1+/D2 LAD had a significantly improved overall survival (HR 0.67, p=0.013) on multivariate analysis compared to those undergoing D1 LAD (Figure), although this survival benefit disappears when stratifying by surgery year. There is no significant survival benefit when isolating surgeries from 1995-2005 (p=0.61) and 2006-2017 (p=0.66).Conclusion: Gastrectomy with extended (D1+/D2) LAD can be safely performed at a high-volume Western institution, ensures optimal staging in the vast majority of patients. Extended LAD correlates with overall survival benefit, which may be attributed to differences in surgery year. D2 LAD are increasingly performed in more contemporary surgeries, which may confer survival benefits related to other advances in care.140 Effect of extended LAD on overall survival during total study period (1995-2017). Overall survival benefit of extended LAD removed when stratifying by year of surgery. 172 Selena Li, Surgery - Surgical Oncology Morbidity and mortality of total gastrectomy: A comprehensive analysis of 90-day outcomes D.W. Rattner2 and 1Harvard Medical School, Cambridge, MA, USA and 2Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Total gastrectomy (TG) is the preferred treatment for upper and middle gastric tumors. Patients may experi- ence post-operative sequelae past standard 30-day outcome metrics. Larger studies from high-volume centers are needed to provide benchmarks for high-quality care for this complex procedure which carries significant risk for morbidity and mortality. Methods: Single institution, retrospective review of a comprehensive gastric cancer database of 148 patients undergoing curative intent TG from 2000-2017. Clinicopathologic and treatment factors were analyzed for effects on morbidity and mortality.Results: The median age of the cohort was 66 years, and 61% were male. Open gastrectomy was performed in 93% (n=137), and 7% (n=11) were performed laparoscopically. while a concomi- pancreatectomy or splenectomy were performed in 4.7% and 19%, respectively. The 30- and 90-day mortality rates were 2.0% and 3.4%, respectively. At least one 90-day complication was experienced by 43% (n=63) of patients, and 16% (n=24) experienced a Clavien-Dindo grade 3 or higher complication. Anastomotic leak occurred in 5.4% (n=8), of which 4 patients required invasive intervention. Median length of stay (LOS) was 8 days, but 22 days in patients experiencing an anastomotic leak.Conclusion: This study defines 30- and 90-day postoperative outcomes after total gastrectomy in a high-volume center. Major complications occur a minority of the time. Anastomotic leak rate is low with the majority managed conservatively, but significantly impacting LOS. Dehydration and nutritional deficiencies contribute to the significant 30-day readmission rate. Mortality rates at 30 and 90 days approximate each other. 173 Xinhua Li, Ph.D., Radiology A quality metric for monitoring CT dose from multiple series examination X. Li, K. Yang and B. Liu Radiology, MGH, Boston, MA, USA Introduction: With the increasing use of x-ray CT in medical imaging and therapeutic procedures, medical institutions are implementing patient dose monitoring programs, based on volumetric CT dose index (CTDIvol) and size-specific dose estimate (SSDE). But both metrics are not applicable to routine CT examinations with tube current modulation or multiple series. This work was to illustrate a quality metric for tracking patient dose from the above CT examinations.141Methods: With IRB approval, this retrospective study included three explanatory examples of abdominal/pelvic examina - tions (patient weight 29, 80, 102 kg) with 5, 4, and 2 acquisition series, respectively, on GE Lightspeed 16. Tube voltage was mostly 120 kV, except 100 kV in one series. Gantry rotation time was 0.5 seconds for all series. Helical pitch was 1.375 for two larger patients and 0.938 for the smallest patient. Tube current modulation (TCM) was adopted in 7 series with scan lengths from 18.1 to 52.9 cm, while tube current was fixed in two short scan lengths (3.9, 5.1 cm). CTDIvol was 15.13 (4.72\u201417.81) mGy in median(range). For each series, a previously reported method was used to calculate cross-sectional average dose along the z-axis of a water phantom, with the inputs of CTDI vol, scan length, tube current, and water equivalent diameter at the patient's scan range center. In a multiple series examination, the dose at each z-location was accumulated over all series, and the maximum value along the z-axis indicated the peak dose of the examination. Results: Cross-sectional average dose largely changed with CTDI vol, scan range, tube current profile, and the z-location. Peak dose was 96.25, 50.05 and 43.46 mGy for patient weight 29, 80 and 102 kg, respectively. The peak dose of each examination was 1.9 to 8.9 times as high as the maximum dose in individual series. Conclusion: For single series of fixed tube current and scan length of 15\u201430 cm, peak dose may be equivalent to SSDE. But peak dose is more accurate than SSDE in considering dose increase with scan length, and is uniquely suited under TCM or multiple series of non-overlapping scan ranges. Diagram illustrates peak dose evaluation that takes into account patient size, radiation output (CTDIvol), scan length, mA change across patient image slices, and dose accumulation from multiple series. Graphs show tube current and dose profiles from three abdominal/pelvic CT examinations on GE Lightspeed 16 CT. 174 Lloyd L. Liang, Medicine - Pulmonary Gadofosveset-enhanced Lung Magnetic Resonance Imaging and Radiographic and Physiologic Measures of Fibrosis in Patients and J.L. Cho1,2 1Pulmonary and Critical Care Medicine, Massachusetts General Hospital, Boston, MA, USA and 2Center for Immunology and Inflammatory Diseases, Massachusetts General Hospital, Charlestown, MA, USA Introduction: Idiopathic pulmonary fibrosis (IPF) is an irreversible disease of unknown etiology that involves progressive scarring of the lung tissue, leading to respiratory failure and death. IPF is thought to develop from repetitive lung injury and aberrant wound healing that leads to the generation of fibrous tissue rather than restoration of normal tissue. It has been suggested in mice that vascular leak after lung injury contributes to the development of lung fibrosis. Gadofosveset is an intravascular enhancing, gadolinium-based contrast agent used with magnetic resonance imaging (MRI) that reversibly binds to albumin. Our research group has developed a new method to assess disease activity in IPF patients using gadofosveset- enhanced lung MRI. In published work, we have demonstrated that this technique can be used to detect and measure albumin extravasation. We have found that albumin extravasation, as defined by an albumin extravasation index, significantly and diffusely increased in the lung of patients with idiopathic pulmonary fibrosis compared to healthy controls.Methods: In this study, we measured regions of interest (ROI) in areas of known fibrosis and ground glass (as determined by comparisons to high-resolution computed tomography (HRCT) in 6 patients with IPF. We calculated an albumin extrav - asation index, change in signal intensity post-contrast minus pre-contrast in each ROI in the lung parenchyma divided by post- minus pre-contrast signal intensity in the ROI in the aorta. We then evaluated the correlation between AEIs and high- resolution computed tomography (HRCT) abnormalities (fibrosis and ground glass) and pulmonary function testing (PFT). Results: While not statistically significant, AEI was more strongly correlated with fibrosis (interstitial abnormalities) than ground-glass (alveolar abnormalities). There was a suggestion of trend towards correlation between AEI and change in 142percent predicted forced expiratory volume in the first second (FEV1), forced vital capacity (FVC), and diffusion capacity of carbon monoxide adjusted for hemoglobin (DLCO) [Hb].Conclusion: Gadofosveset-enhanced lung MRI is a novel means to detect albumin extravasation in vivo . Further research is needed to determine associations between vascular leak and HRCT abnormalities and pulmonary function. 175 Alicia Lightbourne, Emergency A ketamine package to accelerate the human rights imperative of universal access to emergency cesarean section A. Lightbourne1, M. Omotayo1,6, S. Suarez1, and T. Burke1,6,7 1Division of Global Health and Human Rights, Department of Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Department of Outcomes Research, Anesthesiology Institute, Cleveland Clinic, Cleveland, OH, USA, 3African Institute for Health Transformation, Sagam Community Hospital, Luanda, Kenya, 4College of Surgery for East, Central, and Southern Africa, Arusha, Tanzania, United Republic of, 5Kenya Obstetrics and Gynaecologic Society, Nairobi, Kenya, 6Harvard Medical School, Boston, MA, USA and 7Harvard T.H. Chan School of Public Health, Boston, MA, USA Introduction: Cesarean section is the most common emergency surgery on earth. Universal access to emergency cesarean section is considered a basic human right and is a priority of the global movement toward 'universal health coverage'. Although, target cesarean section rates for optimal maternal and neonatal outcomes vary between countries, WHO has found that maternal and newborn death rates are lowest in countries where at least 10% of women undergo cesarean section. Unfortunately, in some regions of Africa and elsewhere cesarean section rates range between 0 and 2%. Lack of access to anesthesia services is a primary barrier to universal access to cesarean section. In December 2013, we deployed the ESM-Ketamine package in support of emergency and essential surgeries when no anesthetist is available, and in this study, we examine how this innovation may support acceleration toward achieving the human rights imperative of universal access to emergency cesarean section. Methods: Select non-anesthetist medical officers, nurses, and clinical officers underwent 5-day competency based ESM- Ketamine training and received checklists, wall charts, and ESM-Ketamine kits. Patient demographics, pre-operative diagnoses, procedure(s) performed, medications administered, and ketamine-related adverse events were recorded. Descrip- tive analyses, and central tendency and dispersion of all non-training cesarean section cases were conducted. Results: Between December 2013 and July 2018, 50 ESM-Ketamine providers across seven facilities supported 202 cesarean sections. The median ketamine dose was 6 mg/kg (IQR: 4-7.1) per emergency operation. Hallucinations or agitation treated with diazepam and brief oxygen desaturation occurred in 24 (11.9) and 6 (3.0%) cases respectively. 192 (95%) of the 202 cesarean sections resulted in healthy discharge of operatively delivered newborns. Ten (5.0%) cesarean sections resulted in delivery of stillborn babies. There were no maternal or infant deaths or injuries associated with ketamine use. Conclusion: The ESM-Ketamine package appears safe for use by trained ESM-Ketamine providers in support of emergency cesarean sections. Scaling the ESM-Ketamine package may support acceleration toward the human rights imperative of universal access to cesarean section. 176 Ann Lin, BS, Psychiatry Gender differences in associations between resiliency and depression among informal caregivers of patients admitted to the Neuroscience Intensive Care Unit A. Lin1, M. Jacobo1, J. Jacobs1, and Zale3 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Neuroscience Intensive Care Unit, Massachusetts General Hospital, Boston, MA, USA and 3Psychology, Binghamton University, Binghamton, NY, USA Introduction: Informal caregivers (e.g., family and friends) of patients hospitalized in the intensive care unit are at risk for developing depression, which can be detrimental to both caregiver and patient functioning. Initial evidence suggests that resiliency may reduce the risk of depression. However, the role of gender differences in associations between multiple psychosocial resiliency factors and depression has never been examined among neuroscience ICU (Neuro-ICU) caregivers. We explored interactions between gender and resiliency factors on depression symptom severity at baseline through 3- and 6-months post discharge. Specifically, we sought to 1) identify gender differences in depressive symptoms, 2) identify gender differences in psychosocial resiliency factors (coping, mindfulness, intimate care, caregiver preparedness, self-efficacy), 3) identify the role of resiliency factors on the trajectory of depression symptoms, and 4) identify the interactions between gender and resiliency factors on depression symptoms across all time points.143Methods: Caregivers (N = 96) were enrolled as part of a prospective, longitudinal study in the Neuro-ICU at Massachuestts General Hospital. Patient-caregiver dyads were approached and screened in the hospital room for the following inclusion criteria: 1) age 18 years, 2) English fluency/literacy, and 3) caregivers would be the primary source of informal care following hospital discharge. Dyads were excluded if patients were not cleared by medical staff to participate. Caregiver sociodemographics and resiliency factors (coping, mindfulness, intimate care, caregiver preparedness, self-efficacy) were assessed during the patient's hospitalization (i.e., baseline). Depression symptoms were measured using the Hospital Anxiety and Depression Scale at baseline, 3 months, and 6 months post-discharge. Results: On average, caregivers were 53.3 years old, and most self-identified as female (61.5%), White (86.5%), currently married (75.0%), and employed full-time (55.2%). Caregivers were most often romantic partners (69.8%) to patients admitted for stroke/aneurysm (41.7%). Among Neuro-ICU caregivers, baseline depression predicted depressive symptoms at both 3- and 6-month follow-up. Greater baseline levels of coping, mindfulness, and preparedness for caregiving were individually associated with lower levels of concurrent depression (ps < 0.006). The main effect of baseline coping remained significant at 3-month follow-up (p = 0.045). We observed a trend-level interaction between gender and baseline intimate care such that among male caregivers only, high baseline intimate care was associated with lower depression at 3-month follow up (p = 0.054). At 6-month follow up, we observed a significant interaction between caregiver gender and baseline intimate care such that male caregivers reporting high intimate care reported lower symptoms of depression than females reporting high intimate care (p = 0.043). Conclusion: Taken together, our findings offer early evidence of gender differences in depression trajectories among Neuro-ICU caregivers. Results support early implementation of psychosocial interventions that provide concrete skills in mindfulness, adaptive coping, and preparedness to caregivers of Neuro-ICU patients. We observed prospectively that intimate care (e.g., physical and emotional affection with their loved one) may be particularly protective for depression among male caregivers. Gender differences in associations between depression and intimate care suggest that tailored programs should promote the quality of the patient-caregiver dyadic relationship. 177 David J. Lin, MD, Neurology Clinical and Neuroimaging Biomarkers Acquired During the Acute Stroke Admission to Predict Upper Extremity Motor Recovery After Stroke D.J. Lin1, A. Cloutier1, S. Finklestein1 and L. Hochberg1 1Neurology, Massachusetts General Hospital, Boston, MA, USA, 2Physical Therapy, Massachusetts General Hospital, Boston, MA, USA, 3Occupational Therapy, Massachusetts General Hospital, Boston, MA, USA, 4Occupational Therapy, MGH Institute of Health Professions, Boston, MA, USA, 5Physical Therapy, MGH Institute of Health Professions, Boston, MA, USA and 6Neurology, University of California Irvine, Irvine, CA, USA Introduction: Recovery of upper extremity motor impairment is critical for functional independence after stroke. Clinical, neuroimaging (i.e. MRI), and neurophysiological (i.e. transcranial magnetic stimulation, TMS) assessments are being developed to provide predictions of upper extremity motor recovery after stroke. However, not all clinical centers have access to the technology required, and unpredictable length of stay and follow-up in the current post-acute care continuum make validation of these predictors challenging. More detailed, longitudinal predictors of upper extremity motor impairment, as well as an understanding of how recovery generalizes across International Classification of Functioning (ICF) domains, are needed to accurately capture stroke outcomes and better personalize rehabilitation. Here our aim is to define robust and clinically translatable predictors of motor recovery before discharge from the acute stroke admission and to understand how upper extremity motor recovery generalizes across ICF Domains in the first 3 months after ischemic stroke.Methods: We enrolled fifty participants (n=50) with unilateral arm weakness after an ischemic stroke in an ongoing single-center, prospective, observational cohort study at an academic tertiary care center in Boston, Massachusetts. Assessments 144were performed during the acute stroke hospitalization, as well as in follow-up at 6 weeks and 3 months. These spanned WHO ICF domains for loss of body structure-function [arm impairment\u2014 grip strength and Fugl-Meyer arm motor assess-ment (FMA-UE); globally, NIH stroke scale (NIHSS)], activity limitations (arm impairment, Box and Blocks, 9 Hole Peg; globally, Barthel Index and modified Rankin Scale), and participation restrictions (Stroke Impact Scale and PROMIS-10). Neuroimaging (MRI and CT images), acquired as part of the clinical standard of care, were analyzed for lesion mapping.Results: Preliminary results demonstrate that participants with mild-moderate initial impairment (FMA-UE > 22) recover to approximately 70% of their available recovery at 3 months, confirming the proportional recovery rule of stroke motor impairment. In participants with severe initial impairment (i.e. baseline FMA-UE < 22), pre-discharge NIHSS (t-test, p<0.05) and injury to the corticospinal tract (t-test, p<0.01) independently distinguished those who recover proportionally from those who do not. Proportional recovery generalized to measures of Grip Strength as well as activity measures (Box and Blocks and 9-Hole Peg) but not to measures of participation. In n=32 participants with follow-up data at both 6 weeks and 3 months, 98% of the variance in motor impairment and activity measures at 3 months could be explained at 6 weeks. Conclusion: Our findings suggest that clinical assessments and neuroimaging performed before discharge from the acute stroke hospitalization can predict upper extremity motor recovery at 3 months. Proportional recovery of motor impairment generalized to activity domains of the ICF. The majority of spontaneous motor recovery occurred within the first 6 weeks. These insights may inform discharge decisions as well as personalized rehabilitation strategies across the post-acute care continuum. 178 Katherine G. Lindeman, Medicine - Endocrine Fracture risk after bariatric surgery among Medicare recipients: Roux-en-Y bypass MA, USA, 2Pharm Epid & Phar Econ, Brigham and Women's Hospital, Boston, MA, USA and 3Center for Surgery and Public Health, Brigham and Women's Hospital, Boston, MA, USA Introduction: Roux-en-Y gastric bypass (RYGB) leads to greater improvements in metabolic health than adjustable gastric banding (AGB), but RYGB also causes accelerated bone loss and may increase fracture risk. The scope and magnitude of RYGB-associated fracture risk is unclear, especially in an elderly population. Thus, we evaluated the comparative fracture risks after RYGB and AGB in a large cohort of severely obese adults enriched with patients over the age of 65 Methods: We conducted a cohort study to examine fracture rates among Medicare Parts A, B and D enrollees with severe obesity who underwent either RYGB or AGB surgery between 2006-2014. We estimated the incidence rates (IRs) and risk of non-vertebral fracture (e.g. hip, humerus, forearm, and pelvis) among the two surgical groups. Severe obesity, surgery type, and fracture outcomes were based on a combination of diagnosis and procedure codes, defined using previously validated algorithms. Incidence rates were calculated, and survival curves were generated using the Kaplan-Meier method. A multivar- iate Cox proportional hazards model evaluated the risk of non-vertebral fractures in the RYGB versus AGB groups, and included adjustments for age, sex, race, index date, geography, markers of health care utilization intensity, comorbidities, and medications. We used interaction tests to assess whether fracture risk differed by age, sex, diabetes status, or race. We also performed a subset analysis on subjects aged 65 or older to assess fracture risk in an older population. Results: The cohort was comprised of 42,345 adults (79% women) who underwent RYGB (n=29,624) or AGB RYGB patients were younger than AGB patients (51 \u00b1 12 yrs vs. 54 \u00b1 12 yrs) with a smaller proportion being over the age of 65 (17% vs. 30%) (p<0.001 for all). Average follow-up time was shorter in the RYGB group than the AGB group (3.3 \u00b1 2.2 yrs vs. 3.9 \u00b1 2.1 yrs) (p<0.001). A total of 658 non-vertebral fractures occurred throughout the follow-up time. The estimated IR of non-vertebral fractures per 1000 person-years was 6.6 (95% confidence interval (CI) 6.0-7.2) among the RYGB patients, and 4.6 (95% CI 3.9-5.3) among AGB patients. Multivariable Cox regression demonstrated that RYGB patients had an increased risk of any non-vertebral fracture (hazard ratio (HR) 1.73, 95% CI 1.45-2.08) compared with AGB patients. Specifically, RYGB conferred a striking increased risk of hip fracture (HR 2.81, 95% CI 1.82-4.49) compared to AGB. RYGB patients also had increased risk of wrist (HR 1.70, 95% CI 1.33-2.14) and pelvis fractures (HR 1.49, 95% CI 1.08-2.07) relative to AGB patients. RYGB-associated fracture risk was not modulated by age, sex, diabetes status or race. The risk of non-vertebral fracture in RYGB patients age 65 or older (HR 1.75, 95% CI 1.22-2.52) was similar to the overall cohort, and included an increased risk of fractures at the hip (HR 2.51, 95% CI 1.25-5.39) and wrist (HR 1.65, 95% CI 1.00-2.77). Conclusion: In a large U.S. population-based cohort, RYGB was associated with a 73% increased risk of non-vertebral fractures relative to AGB, including a 181% increased risk of hip fracture and increased risk of wrist and pelvis fractures. Patients aged 65 or older showed a 75% increased risk of any fracture, with a 151% increased risk of hip fracture after RYGB compared to AGB. Long-term skeletal health should be discussed with severely obese adults undergoing RYGB, especially with older patients.145 179 Bob Liu, Ph.D., Radiology Radiation dose monitoring for fluoroscopically-guided interventional procedures: impact on patient radiation exposure B. Liu, J.A. Hirsch, X-ray fluoroscopy is a valuable tool for minimally invasive diagnostic procedures and therapeutic interven - tions for a wide range of medical conditions. However, even the state-of-the-art systems containing advanced features for optimizing radiation dose can result in skin doses that produce deterministic effects in patients. Multiple incidents of radiation injury from these procedures have been reported. We herein analyzed the continuous monitoring, control and follow-up of radiation dosage from fluoroscopically-guided vascular interventional procedures in a radiology program over an 8-year period.Methods: In this retrospective study, an in-house semi-automated system was developed for fluoroscopic dose monitoring. Technologists recorded four dose surrogates in the custom fields of institutional dictation software (PowerScribe 360) through a Web interface. Radiation dose data were transferred automatically to the radiology report and a centralized dose database when the radiologist initiated procedure dictation. A physicist performed data analysis weekly and reported K a,r (air kerma at the reference point, accumulated for each procedure) 5 Gy procedures to a division designated radiologist and hospital radiation safety committee, who requested or enforced the attending radiologist to set up patient follow-up appointment. This program was integrated into the clinical workflow. The quarterly number of procedures from January, 2010 to December, 2017 were analyzed with linear regression for estimating quarterly change rate, Y = + 1X1+2X2 + . (1) Y was the quarterly number of procedures with 2 Gy Ka,r < 5 Gy or Ka,r 5 Gy, respectively, was the intercept term, X1 was quarter (n = 1-32), and was the error term. We included a control variable (X2, the quarterly number of vascular interventional procedures) in the analysis to correct for and estimate effects on high Ka,r procedures from changes in overall fluoroscop- ically-guided vascular interventional activity. We sought to estimate two slopes (1 and 2) to test two-tailed hypothesis (slopes were significantly different from zero). A P value of less than 0.05 was considered significant.Results: The total number of procedures was 41585. and was 1555 and 240, respectively. High Ka,r procedures generally decreased over time, whose percentage was lower in 2017 than in 2010 by 3-fold for Ka,r in 2-5 Gy, and by 8.2-fold for Ka,r 5 Gy. The overall goodness-of-fit statistics were R2 = 0.65-0.66 and significance F (or p-value) < 0.001, which indicated that at least one of 1 and 2 was different from zero at a significance level of 0.05. The slope coefficient was 1 = -1.48 (P<0.001) for Ka,r (P=0.0013) for Ka,r 5 Gy, indicating that the linear change in high Ka,r procedure volume per quarter was statistically significant at the above significance level. However, the effect of the overall fluoroscopically-guided vascular interventional clinic activity was statistically insignificant at the above significance level, indicated by that 2 = 8.77E-3 (P = 0.643) for Ka,r in 2-5 Gy and for Ka,r 5 Gy. Conclusion: In a radiology safety pilot program, long period temporal trends demonstrate a reduction in substantial radiation doses from fluoroscopically-guided vascular interventional procedures. The program can provide feedback to interventional radiologists, facilitate proper patient care and promote radiation safety.146 Figure 1. Flow chart illustrating dose data collection, radiation exposure control, and patient follow-up procedures for fluoroscopically-guided vascular interventional procedures. RSC= radiation safety committee. IR liaison is a division designated interventional radiologist. Figure 2. Illustration of fluoroscopically-guided vascular interventional procedures over January, 2010 - December, 2017. Graphs in (a), (b), and (c) show the total quarterly number of all procedures, that of procedures with K a,r in 2-5 Gy, and that of procedures with Ka,r 5 Gy, respectively. Graphs in (d) and (e) show the percentage of procedures with Ka,r in 2-5 Gy and Ka,r 5 Gy, respectively. The red lines in (b) and (c) show the linear regression of Equation (1). Each regression line was not a straight line, because the regression model included a control variable for the overall procedure volume. 180 Chang A. Liu, MD, MSc, FAAP, Anesthesia, Critical Care and Pain Medicine Spinal Anesthesia in Infants Presenting for Infraumbilical Surgeries Can Improve Operating Room Efficiency - a Masschusetts General Hospital for Children Case Series C.A. Liu, N. Davis and E.S. Shank Anesthesiology, Critical Care and Pain Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Spinal anesthesia has been a safe anesthetic performed in children for more than 100 years. However, spinal anesthesia in children fell out of favor due to the introduction of improved inhalational and neuromuscular blocking agents in the early to mid 1900s. During 1980s to1990s, after investigation into several cases of post-operative apnea in high-risk and preterm infants, spinal anesthesia was reintroduced in the 1990-2000s as an alternative to general anesthesia in high-risk and preterm infants. This reintroduction is related to evidence that spinal anesthesia in high-risk preterm infants can reduce the risk of postoperative apnea including decreasing the severity of postoperative desaturation and reducing the incidence of bradycardia in this patient population. In recent years, there has been addtional interest in spinal anesthesia in infants due to overwhelming evidence that sevoflurane can have neurotoxic effects in vitro. In fact, the FDA issued a blackbox warning against sevoflurane for its potential to produce neurotoxicity in infants and fetuses of pregnant women in 2016 based on existing in vitro evidence. Over the last decade, spinal anesthesia have become a useful technique in infraumbilical, urologic and lower limb surgeries in infants at a number of specialized pediatric institutions.Methods: We described a number of high-risk preterm infants undergoing infraumbilical surgical procedures under spinal anesthesia in lieu of general anesthesia at Massachusetts General Hospital for Children (MGHfC). We performed retrospec - tive chart review of 16 infants with postmenstrual age younger than 60 weeks undergoing infraumbilical surgical procedures such as inguinal hernia repair, heel cord tenotomy and circumcision during the year 2017 to 2018. Successful spinal anesthesia was defined as no additional need for sedation and/or inhalational agents other than spinal anesthesia for the entire duration of the surgery. Furthermore, we measured several operating room (OR) time intervals including \"OR entry to incision time\", \"Bandage to exit OR time\" and \"PACU time\". We compared these times between infants receiving spinal anesthesia and those receiving general anesthesia. We report here the anesthetic agent, dosage and success rate of our infant spinal anesthesia. We also measure rates of overnight admission for apnea monitoring for infants who underwent spinal versus general anesthesia.Results: Infant demographics are reported in Table 1. Eight infants underwent hernia repair, circumcision or heel cord tenotomy under spinal anesthesia and 8 underwent these procedures under general anesthesia. Success rate of infant spinal anesthesia is 87.5%. The mean postmenstrual age is 44.2 weeks for infants undergoing spinal anesthesia and 43.7 weeks for infants undergoing general anesthesia. The local anesthetic agent used for the spinal, dosage and success rate of infant spinal anesthesia is also reported in Table 1. Table 2 outlines OR time intervals comparing spinal anesthesia and general anesthesia times. OR entry to incision time for spinal compared to general anesthesia is 13 minutes versus 28 minutes. Bandage to exit OR time for spinal compared to general anesthesia is 5 minutes versus 23 minutes. PACU time for spinal compared to general anesthesia is 53 minutes versus 69 minutes (Table 2). There were no episodes of post-operative apnea in these 16 infants in our cohort in the immediate 12 hours period post-operatively. Two term infants were discharged home directly from the PACU. Conclusion: Spinal anesthesia is a useful technique in infraumbilical and lower limb surgeries in high-risk and premature infants due to evidence that it reduces incidence of post-operative apnea in this patient population compared to general anesthesia. At the MGHfC, we have an 88% success rate for spinal anesthesia in infants younger than postmenstrual age 60 147weeks undergoing infraumbilical surgical procedures. Notably, our OR time interval data show that spinal anesthesia can improve OR efficiency and PACU discharge times compared to general anesthesia. Table 1 Patient Demographics Table 2 Operating Room Time Metrics 181 Jianing Liu, B.S., Neurology GE-179 PET Imaging of NMDA Receptor Availability in Amyotrophic Lateral Sclerosis J. Liu1, N. Guehl2, D. Wooten2, Fakhri2, M. Normandin2 and N. Atassi1 1Neurological Clinical Research Institute, Massachusetts General Hopsital, Boston, MA, USA, 2Radiology, Gordon Center for Medical Imaging, Massachusetts General Hopsital, Boston, MA, USA, 3GE Global Research, Niskayuna, NY, USA and 4A.A. Martinos Center for Biomedical Imaging, Massachusetts General Hopsital, Charlestown, MA, USA Introduction: Glutamate excitotoxicity is considered one of the main mechanisms leading to motor neuron degeneration in ALS. Glutamate released from the presynaptic Among glutamate receptors, NMDA receptors have the highest affinity for glutamate and hence forms an important role in pathogenesis of glutamate excitotoxicity in neurons. In this study, we aim to measure NMDA receptor activity in people with ALS using 18F-GE-179 PET, a ligand that selectively binds to the open NMDA receptor ion channel. In this report, we present the progress which had been made in this ongoing study.Methods: A total of ten ALS and ten healthy controls (HC) will be enrolled by the end of this study. As of June 24, 2018, Four ALS patients and two healthy controls have screened and underwent magnetic resonance (MR) and positron emission tomography (PET) brain imaging at Massachusetts General hospital (MGH). The clinical evaluation of ALS participants included the upper motor neuron burden scale (UMNB) and the revised amyotrophic lateral sclerosis functional rating scale (ALSFRS-R). Dynamic PET and metabolite corrected arterial plasma input functions were acquired for 90 min after [ 18F] GE-179 administration. Total volume of distribution (VT), the equilibrium ratio of radiotracer in tissue relative to arterial plasma, was estimated by compartmental modeling (two-tissue model with blood volume correction) as an outcome parameter reflective of total uptake of radiotracer and directly proportional to density of the available receptor, the gold standard for PET quantification. PET images were also created 60-90 minutes post radiotracer injection and quantified as standardized uptake value (SUV). To account for whole brain PET signal variability, SUV were normalized by whole brain mean and expressed as this SUV ratio (SUVR) to explore this outcome as a simplified alternative to kinetic analysis with blood sampling. V T and SUVR values were estimated within multiple brain regions of interest (ROIs) that are positioned in various locations including the precentral gyrus. SUVR and V T values were compared between ALS and HC within these ROIs. Results: In the current cohort, the mean [SD] of Vd values in the precentral gyrus were not different between values in the precentral gyrus were not different between 2 HC: 0.973 [0.015]; p=0.99). V T and SUVR values were not significantly different between ALS and HC in any brain region (Figure 1A-B-C). Details about demographics and clinical assessment are listed in Table1 Conclusion: In this small cohort of ALS participants, there was no difference in the [18F]GE-179 uptake between people with ALS and healthy controls. The larger planned cohort is needed to shed more light on the role of NMDA receptors in ALS pathogenesis.148Demographic and Clinical Information for ALS and HC 182 Olivia M. Losiewicz, AB, Psychiatry Impact of the Use of Safety Behaviors on Anxiety and Psychophysiology as Assessed by Smartphone-Based Experience Sampling Methods 1Center for Anxiety and Traumatic Stress Disorders, Massachusetts General Hospital, Boston, MA, USA and 2Langone Medical Center, New York University, New York, NY, USA Introduction: Safety behaviors (SB) represent ineffective attempts to reduce or eliminate anxiety (e.g., carrying a water bottle to reduce physiological sensations that arise during anxiety, compulsively hand-washing to reduce contamination fear; Helbig- Lang & Petermann, 2010). SBs may relieve distress in the moment; however, long-term anxiety returns. SBs may reinforce conditioned fear of physiological sensations, propagating a feedback loop wherein anxiety leads to SB use, which in turn exacer-bates anxiety and panic symptoms. Safety behaviors (SB) are thought to play a central role in the etiology and maintenance of 149anxiety disorders, such as panic disorder (PD), however, experimental evidence has been mixed. Existing studies are limited in their temporal conclusions and ecological validity. New technologies such as smartphones and other, wearable devices permit time-intensive investigation of these phenomena in the natural environment in which they occur, thus improving external validity. Methods: The present study utilized novel smartphone-based experience sampling methods to examine the effect of SB use on anxiety and psychophysiological response in PD. Analyses were conducted using N=910 data points from participants (N=13) who completed a brief questionnaire of panic symptom severity and SB use 5 times a day for 14 days.Results: SB use was highly correlated with anxiety and predictive of later anxiety level. Increased SB use at time 1 predicted increased anxiety at times 2, 3, 4, and 5 (t(1, 100)'s > 4.26; p's < .001). SB use at time 1 was a significant predictor of anxiety at time 2, even when controlling for anxiety at time 1 (t(2, 103) = 2.83; p = .006). Corresponding psychophysiological response will also be presented.Conclusion: In line with theoretical conceptualizations of PD, our findings support that individuals engage in SBs when anxious and that SB use then robustly maintains and even heightens anxiety. Notably, these effects vary across patients. Future directions for novel technological, statistical, and personalized approaches to expand our understanding of SBs in anxiety disorders and implications for treatment will be discussed. 183 Kelsey Lowman, BA, Psychiatry Pilot Study of a Transdiagnostic, Emotion-Focused Group Intervention for Young Adults with Substance Use Disorders: Sample Characteristics and Preliminary Results K. Lowman, K.H. Bentley, J. McKowen, A. Yule, L. Cohen, L. Rines-Toth, J. Nargiso, O. Federico, L. Watt, M. Fava and A.E. Evins Psychiatry, Massachusetts General Hospital, Somerville, MA, USA Introduction: Compared to other age groups, young adults are at disproportionately high risk for developing a substance use disorder (SUD) and show poorer retention in and response to SUD treatment. SUDs are also highly comorbid with emotional disorders (e.g. anxiety, depressive, and related disorders), as well as suicidal and nonsuicidal self-injurious thoughts and behaviors. This places young adults with SUD at particularly high risk for potentially lethal outcomes such as unintentional overdose or suicide. Given the clear need for more effective and efficient approaches to engaging and treating this high-risk group, the current study implements the Unified Protocol for Transdiagnostic Treatment of Emotional Disorders (UP), an evidence-based cognitive-behavioral intervention designed to target key emotion dysregulation processes that underlie emotional disorders and potentially also SUDs. The aim of this ongoing pilot study is to test the feasibility, acceptability, and preliminary efficacy of the UP when delivered in group format to young adults with SUD and co-occurring emotional distress. Methods: Eligible participants (n=20) are young adults ages 18 to 26 with a DSM-5 SUD and elevated emotional distress (defined as current moderate or severe depressive or anxiety symptoms or suicidal ideation [SI] or nonsuicidal self-injury [NSSI]) who are engaged in treatment at the MGH Addiction Recovery Management Service (ARMS). Participants are randomized in blocks of five to either treatment as usual at ARMS (TAU alone condition) or 16 twice-weekly UP sessions plus TAU at ARMS (UP + TAU condition). All participants complete three assessments (baseline, mid-treatment, and post-treatment), which include the Patient Health Questionnaire (PHQ-9), Generalized Anxiety Scale (GAD-7), Overall Depression/Anxiety Severity and Impairment Scales (ODSIS/OASIS), Brief Experiential Avoidance Questionnaire (BEAQ), Emotion Reactivity Scale (ERS), and Drug Use Motives Questionnaire (DUMQ; administered only at baseline). Participants randomized to UP + TAU also complete the Client Satisfaction Questionnaire (CSQ-8) at post-treatment. Here, we present baseline sample characteristics (n=20) and preliminary results for the UP + TAU condition (n=10), as only five participants have completed the TAU alone condition thus far.Results: Sample characteristics. Eligible participants at baseline (n=20) were mostly female (55%) and white (75%), with a mean age of 21.3 (SD=1.83). Half the sample met diagnostic criteria for two or more SUDs. The most common primary SUD was alcohol use disorder (70%), followed by cannabis (15%), opioid (10%), and benzodiazepine (5%) use disorders. Almost all participants (95%) presented with at least one co-occurring psychiatric disorder, most commonly a unipolar depressive disorder (65%), followed by anxiety disorder (60%), ADHD (35%) and PTSD (30%). The majority (60%) of participants reported a history of SI, 40% a history of NSSI, and 30% a past suicide attempt. The most frequently endorsed primary reason for problematic substance use was coping with negative affect (e.g. \"because it helps you when you feel depressed or nervous\") on the DUMQ (endorsed by 45% of participants), followed by pleasure-enhancement, coping with illness, and social motives. Preliminary results (UP + TAU condition only). For participants who completed the UP + TAU condition, we observed declines from baseline (n=10) to post-treatment (n=7) in mean scores for two therapeutic targets of the UP: emotion reactivity (ERS; SD=6.1; post M =7.4 [mild], SD=4.7). The mean attendance rate for the UP group was 48.1% (M =7.7 UP sessions attended, SD=4.9). On average, participants 150reported an satisfaction rating (\"mostly satisfied\") on the CSQ-8 (M=26.6, SD=6.0) for the UP intervention. Prior to presentation, analyses will be updated to include new participants. Conclusion: An initial examination of our baseline sample echoes recent epidemiological studies that show high comorbidity of SUDs, emotional disorders, and self-injurious thoughts and behaviors in young adults. The prominence of using substances to cope with negative affect emphasizes the potential value of teaching adaptive coping skills for managing intense negative affect in this high-risk group. Preliminary results indicate that the UP may effectively engage its putative therapeutic mechanisms of emotion reactivity and experiential avoidance, and may impact anxiety and depression symptoms; however, we have not yet examined whether changes in these constructs and symptoms differ for participants who receive the adjunc - tive UP versus TAU alone. High satisfaction with the UP and satisfactory attendance rates suggest that the UP may be both acceptable and feasible for delivery within an existing outpatient substance use program. Future work will include between-group comparisons, analysis of clinician-rated substance use, and moderators of treatment effects. 184 Debra M. Lundquist, Cancer Center The Experience of Young Women Living with Advanced Breast Cancer: A Hermeneutic Phenomenological Study Boston, USA, 2Dana Farber Cancer Institute, Boston, MA, USA, 3Penn State College of Nursing, University Park, PA, USA and 4Boston College William F. Connell School of Nursing, Chestnut Hill, MA, USA Introduction: Little is known about the daily life experiences of young women living with advanced breast cancer. Limited research suggests they face unique challenges that differ from those of women at other life stages as well as with earlier stages of breast cancer. The purpose of this study is to describe and interpret the lived experiences of young women with advanced breast cancer. This study is an important initial step to advance our understanding of the needs of this population and inform the development of person-centered interventions.Methods: Van Manen's hermeneutic phenomenological method was employed in this longitudinal qualitative study. Women aged 25-39 with advanced breast cancer were purposively recruited via private Facebook groups specifically for women with breast cancer. Data were collected through one or more semi-structured interviews over six months depending upon participant willingness, desire, or ability. Journals were provided at the completion of the first interview for participants to write any additional thoughts. Analysis was conducted using NVivo for Mac software and followed van Manen's method.Results: Twelve women aged 25-39 women were included. All had at least one child and were married. Most (n=7, 72.7%) worked full-time. Twelve participated in the first interview, 9 in a second interview, and 6 in a third interview. Three wrote in and returned journals. The meaning of their multidimensional experiences is captured by the overarching theme: Wearing the mask of wellness in the presence of life-threatening illness. Five major themes were identified: Wanting to be known as the person I am, I'm still Mom, Living is more than surviving, Getting through it, and Being connected to others. Conclusion: This study contributes insights about the experiences of young women living with advanced breast cancer. The overarching theme and major themes that emerged from the data begin to characterize the phenomenon of young women living with advanced breast cancer. Findings highlight that they are managing multiple roles and responsibilities despite the ongoing challenges of treatment and symptom management. They feel that their needs and struggles are not well understood because to outsiders they do not look ill. This study provides a base for further research and eventually interventions. Knowledge directly acquired from patient experiences can be used to design care that will improve the experience of living with advanced breast cancer in ways that are meaningful to the patient. Ultimately, this can lead to innovative interdisciplinary interventions that will improve the experience for young women living with advanced breast cancer. 185 Elizabeth N. Madva, MD, Psychiatry Development of an adaptive text message program to promote well-being and health behaviors in primary care patients E.N. Madva1, Legler1, Huffman1 1Psychiatry, MGH, Boston, MA, USA and 2East Boston Neighborhood Health Center, East Boston, MA, USA Introduction: The majority of the 117 million Americans with chronic medical conditions (CMCs) are unable to adhere to recommended health behaviors, including physical activity, healthy eating, and adherence to medications. Such non- adherence is linked to disease progression and poor health outcomes. CMCs are associated with elevated levels of psycho- logical distress and low levels of psychological well-being, and these psychological factors are prospectively associated 151with non-adherence, lower health-related quality of life (HRQoL), and poorer overall function, which are in turn linked to mortality. Conversely, positive psychological constructs, such as optimism and positive affect, have been prospectively and independently linked to increased participation in health behaviors and superior health outcomes, and a growing body of research has examined the impact of focused \"positive psychology\" interventions to promote these constructs. Programs to manage health behaviors and psychological health in patients with CMCs have the potential to both improve current medical problems and prevent the development of complications, but are often limited by the time commitment required and the lack of personalization. Mobile health-based programs\u2014which utilize wireless devices, such as cell phones, to deliver health- related information or interventions\u2014may address many of the limitations of existing, in-person health programs, and further create an opportunity to utilize positive psychology based messages, which have not previously been tried outside of pilot work from our group.Methods: We developed and implemented a machine-learning-based adaptive once-daily text message intervention to address this public health problem. The intervention aims to promote psychological well-being and to provide education and support around health behaviors, such as diet, medication adherence, and physical activity, for patients in an urban community primary care setting, with messages in both English and Spanish. This intervention allows patients to provide real-time feedback about each message, such that the machine-learning algorithm subsequently delivers increasingly tailored messages to each patient. Results: We implemented both fixed-message and adaptive text messaging interventions in a clinical demonstration project at a community health center. Seventy-four patients were enrolled in the fixed-message program, and all messages were delivered successfully. The machine-learning text message intervention has been created and clinically deployed, with enrollment ongoing.Conclusion: We have successfully developed, programmed, and implemented an adaptive text message intervention in a community health center. Additional work is required to understand the impact of this intervention on well-being and health-related outcomes. 186 Melissa C. Maravic, PhD, MPH, Psychiatry Baseline Findings from the PCORI Pragmatic Trial \"Integrated Smoking Cessation Treatment for Smokers with Serious Mental Illness\" Maravic1, Massachusetts General Hospital, Boston, MA, USA, 2Bay Cove Human Services, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: Nicotine addiction is highly prevalent among those with serious mental illness (SMI). Recent estimates indicate that 64%-79% of those with schizophrenia spectrum disorders smoke tobacco regularly, as do 44-71% of those with bipolar disorder, and 43% of those with major depressive disorder. Clinical practice guidelines recommend all smokers be advised to quit at every visit and be offered combined pharmacologic and behavioral smoking cessation aids. Despite extensive evidence that most with SMI want to quit smoking and that first-line pharmacotherapies for smoking cessation are effective and well-tolerated in this population, few are offered evidence-based smoking cessation medication by their primary care providers (PCP).Methods: This 3-year study enrolled smokers with SMI who receive psychiatric rehabilitation services from one of two community-based human services agencies. Participants completed a brief survey about their smoking behavior and how their PCPs addressed their smoking in the last year, and a carbon monoxide (CO) breath test.Results: Participants (n=1,166) most frequently reported smoking 11-20 cigarettes/mini-cigars per day (33.2%), and on average had an expired CO of 21.3\u00b1 16.0. Almost all (92.1%) reported that their PCP is aware they smoke tobacco products. While 69.7% reported that their PCP recommended cessation, only 36.4% report that their PCP prescribed treatment for cessation. Most common treatment recommended or prescribed was nicotine patch (71.2%), bupropion (3.8%).Conclusion: Baseline data indicate that participants are moderate smokers. While PCPs are largely aware that their patients smoke, only a little more than one third of smokers with SMI report their PCP recommended or prescribed a first-line pharma- cological treatment for smoking cessation in the past year. Very low rates of recommendation/prescription were found for varenicline, which has the highest efficacy in smoking cessation in this population.152187 Sandro Marini, MD, Center for Christopher1,2 Mass General Hospital, Boston, MA, USA, 2Center for genomic medicine, Mass General Hospital, Boston, MA, USA, 3Stroke Unit, IRCCS Mondino Foundation, Pavia, Italy, 4Department of Neurology, Yale University School of Medicine, New Haven, CT, USA and 5Department of Neurology and Rehabilitation Medicine, University of Cincinnati College of Medicine, Cincinnati, OH, USA Introduction: Risk of lobar and non-lobar intracerebral hemorrhage (ICH) varies among blacks, whites and Hispanics. We sought to determine whether these differences could be due to variability in the effects of Apolipoprotein E (APOE) epsilon () alleles, the most potent genetic risk factor for ICH. Methods: Primary ICH cases and controls were collected from US and European sites contributing to the International Stroke Genetic Consortium (ISGC). We meta-analyzed the effects of APOE allele status on ICH risk applying a two-stage clustering approach based on race/ethnicity and the contributing study. A propensity score analysis was used to model the influence of APOE against the burden of hypertension across races/ethnicities. Results: 13,124 subjects (54.5% male, median age 66 years) were included. In whites, APOE 2 (odds ratio (OR)=1.49, 95% with lobar ICH risk, however within self-identified Hispanics and blacks, no associations were found. After propensity score-matching for hypertension burden, APOE 4 was associated with lobar ICH risk among Hispanics APOE 2 and 4 did not show an effect on non-lobar ICH risk in any race/ethnicity. Conclusion: APOE 4 and 2 alleles affect lobar ICH risk. Associations are confirmed in whites but may be masked by an elevated burden of hypertension in non-white populations. Further studies are needed to explore interactions between APOE alleles and environmental exposures that vary by race and ethnicity in representative populations at risk for ICH. 153188 Anna Marmalidou, MD, Surgery A Novel Contrast Sensitivity Test as a New Measure of Visual Function in Central Serous Chorioretinopathy A. Marmalidou1, E. Lee and Ear Infirmary, Boston, MA, USA, 2Ophthalmology and Visual Sciences, Kellogg Eye Center, University of Michigan, Ann Arbor, MI, USA, 3College of Optometry, Nova Southeastern University, Fort Lauderdale, FL, USA and 4Adaptive Sensory Technology, San Diego, CA, USA Introduction: To evaluate the efficacy of computerized testing of contrast function in determining a patient's contrast sensitivity function in central serous chorioretinopathy (CSCR).Methods: This prospective, observational, multi-center IRB-approved study included 40 eyes affected by CSCR and 32 unaffected eyes. Patients with CSCR were enrolled from September 2016 through November 2017 and met the following criteria: age 18 years or older, history of current or prior CSCR, best-corrected visual acuity (BCVA) 20/200, trace to 1+ cataract or pseudophakia, and no other ocular pathology. All patients underwent spectral domain optical coherence tomography (SD-OCT) and testing of the contrast sensitivity function (CSF) using the quick CSF (qCSF) algorithm implemented on the novel AST platform (Adaptive Sensory Technology, San Diego, CA). The contrast sensitivity function was broadly summarized by the area under the log contrast sensitivity function (AULCSF), which measures the world that we can see via stimuli under the contour of visibility. Low-frequency contrast sensitivity (CS) is the lowest contrast seen with the largest optotype, while CSF acuity (CA) is the smallest optotype seen with the highest contrast. A two sample t-test was used to compare BCVA, AULCSF, CA, and CS at low, mid, and high spatial frequencies (1, 6, and 18 cpd) between affected and unaffected eyes. Results: Mean BCVA was logMAR 0.02 (~20/20) in unaffected eyes versus 0.12 (~20/25) in affected eyes. Affected eyes had a statistically significant reduction in the AULCSF and CA compared to unaffected eyes (1.12\u00b1 0.45 vs 1.40\u00b1 0.40, p=0.0006; 1.18\u00b10.28 vs 1.34 \u00b10.23, p=0.0090, respectively). Measured contrast sensitivities at 1, 6, and 18 cpd were 1.42 \u00b10.24, 1.07\u00b10.53, and 0.22\u00b10.28, respectively, in affected eyes, compared with 1.55\u00b10.24, 1.38\u00b10.41, and 0.42\u00b10.40, respectively, in unaffected eyes; the differences between the two groups were statistically significant (all P<0.05). Conclusion: The qCSF test confirms reduced contrast sensitivity in eyes affected by CSCR when compared to unaffected eyes. In patients with good VA, the qCSF test was able to identify limited visual function providing a promising visual endpoint that may allow clinicians to better evaluate and monitor the evolution of CSCR. 189 Felisha A. Marques, MPH, Medicine Implementation of Patient Driven Decision Aid Ordering to Promote Shared Decision Making F.A. Marques, K. Sepucha, L. Leavitt, C. Meyer, M. Mangla and L. Simmons Division of Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Shared Decision Making (SDM) is an approach to involving patients in medical decisions that is appropriate when clinical evidence supports more than one approach to care. The goal of SDM is to ensure that patients are well informed and receive treatments that match their goals and preferences. Patient Decision Aids (DA) have been shown to support SDM by increasing patients' knowledge and involvement in treatment decisions and reducing decisional conflict in randomized control trials. Partners Healthcare System (PHS) has supported access to high quality DAs for common medical tests and treatments. However, less is known about how to incorporate these tools in routine care. This study evaluates the patient driven ordering models for DAs by comparing patterns of patient driven and physician driven DA ordering to establish best practice workflows within PHS.Methods: Starting in 2012, five primary care clinics at Massachusetts General Hospital (MGH) participated in clinical process improvement activities aimed to increase the use of decision aids in the clinic by introducing patient driven ordering. Traditionally, DAs had been ordered for patients by their physician if they thought a particular DA would better inform their patient. In the patient-driven ordering model, patients received an order sheet with 12-15 DAs to choose from either prior to their visit or at the time of their visit. Patients were instructed to review the sheet, select up to two programs of interest, and return the order sheet to a medical assistant or front desk staff. The staff then placed an order for the appropriate DA in the patient's electronic medical record (EMR). Patients were mailed a DVD/booklet DA, or received an online DA via web-link link the patient portal. Each DA order was tracked in a database and documented in the patient's chart. Data on DA ordering rates was collected prior to, during, and after the introduction of the patient driven ordering sheet. Additionally, DA topics 154most frequently ordered by patients were compared to the topics most frequently ordered by physicians. Finally, qualitative feedback from physicians, nurses, and medical assistants involved in the program was gathered.Results: At all five practices that implemented patient driven DA ordering, DA order rates increased after the order sheet was introduced. Across the five practices, the DA order rate increased from an average of 19 DAs per month prior to implementing the order sheet to an average of 86 DAs per month with 58 DAs per month being ordered by patients after implementing the order sheet. Notably, physicians and patients ordered very different programs. The top programs ordered by physicians prior to implementing the patient order sheet were Advance Directives, Colon Cancer Screening, Hip Osteoarthritis, Insomnia, and Prostate Cancer Screening; the top programs ordered by patients were Depression, Insomnia, Anxiety and Acute Low Back Pain, and Menopause. Patient driven ordering facilitated by nurses and medical assistants was both feasible and well utilized in community and hospital based primary care practices. These projects demonstrate that there is a large unmet interest among patients to receive decision aids as a part of their medical care. Successful features of this pilot were the engagement of the entire care team in patient education and an improved workflow for DA ordering. Physicians were supportive of a DA order workflow that no longer relied solely on them to order programs. The front desk staff, medical assistants, and providers overall were more satisfied with DA implementation when it engaged the entire care team and the patient. We are currently working with practices at North Shore Physicians Group and Newton Wellesley Hospital to implement the patient-driven workflow. Conclusion: Shared decision making and DAs are tools to improve patient engagement and healthcare quality. Implementing patient driven ordering of DAs both increases targeted DA distribution and supports the work of a more collaborative care team approach to promote high value health care. Increasing distribution of these tools through innovative workflows will support better patient care, improve decision quality and help lead to better patient satisfaction. 190 Mohammad Matinnejad, BS, Emergency Appropriateness of MRI utilization in the Emergency Department for Patients with Low Back Pain M. Matinnejad, E. Aaronson, B. Yun, B.A. Parry, J.B. Weilburg, D.F. Brown, A.S. Raja and J. Lee Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Magnetic Resonance Imaging (MRI) is commonly used to image patients presenting to the emergency depart- ment (ED) with low back pain. In this quality improvement project, we sought to examine the use of MRI for the evaluation of patients with low back pain by emergency physicians. This abstract reports the first stage of a project which our emergency physicians received reports of their use of MRIs in patients they evaluated with low back pain.Methods: This study was performed at a tertiary care academic hospital ED with an annual volume of 115,000 patient visits. To generate the reports, we identified all ED visits with a chief complaint or discharge diagnosis in the low back pain category between October 2014 and June 2015. MRIs that were requested by non-ED services and trauma patients requiring a trauma consult were excluded. We then determined which of these patients received an MRI during their ED visit and determined appropriateness using the American College of Physicians (ACP) criteria. A trained abstractor extracted each component of the ACP criteria. The first 10 charts were monitored with a physician reviewer. Two reports were generated: one report showed the entire department's result and was given to the departmental leadership (Figure 1), and individual physician reports (Figure 2) were given to each physician during their annual clinical review.Results: Overall, there were 2,307 low back pain ED visits during the study period; 335 (14.5%) received at least one MRI for back pain. A total of 39 MRIs was excluded. Of the patients with MRIs, 198 met at least one ACP appropriateness criterion (66.9%), \"cancer\" were the next two most common reasons for the MRI. Comparing patients who received an MRI and those that did not receive an MRI, the rate of return to ED within 7 days for back pain was similar at 2.99% and 2.97% respectively (p=1.0). The majority of the patients (n=232; 70.5%) who received an MRI were placed in the observation unit. For patients who received an MRI, 6.99% were admitted to the hospital, as compared to 3.37% among patients that did not receive an MRI (p=0.005). Conclusion: The majority of MRIs for low back pain in our ED met ACP criteria for appropriate use. Receiving an MRI was not associated with a reduction in 7 day return to the ED for the same complaint. Future research will examine whether providing Emergency physicians their appropriateness rates with specific patient information might affect MRI use in the ED for low back pain.155 191 Thomas H. McCoy, Psychiatry Characteristics of reportable health data breaches, 2010-2017 T.H. McCoy, A.M. Pellegrini and R.H. Perlis Center for Quantitative Health, Massachusetts General Hospital, Boston, MA, USA Introduction: Protections for private patient data and mandatory public reporting of breaches of data confidentiality were established by the 1999 Health Insurance Portability and Accountability Act (HIPAA) and 2009 Health Information Technology for Economic and Clinical Health Act. The ongoing transition to electronic health records may increase such breaches. We utilized public data to examine the nature and extent of breaches from 2010-2017.Methods: Reporting to the U.S. Health and Human Services Office for Civil Rights health records breach database is required for any breach of 500 or more records. We downloaded all reported breaches and analyzed these in terms of number of breaches and number of individuals impacted by year, breached entity (provider, plan, business associate), the media breached, and type of breach.Results: We analyzed 2,149 breaches covering 176M breached records. Breaches ranged in size from 500 to 78.8M records. Over time the number of breaches grew, from 199 in 2010 to 344 in 2017. Healthcare providers accounted for the majority of breaches (70%) and healthcare plans accounted for the majority of records breached (63%). Although physical records (paper / film) were the most common media type breached (24%), the majority of records breached (139.9M of 176.4M) were from networked electronic systems.Conclusion: Despite the ethical and legal obligation to protect patient privacy and efforts to establish best practices for healthcare information security, breach rates have increased and healthcare providers accounted for a large share of those breaches. Annual breach volume by HIPAA entity type.156192 Alexander Melamed, MD MPH, OB/GYN Minimally Invasive Radical Hysterectomy J. Rauh-Hain5 1Obstetrics and Gynecology, Massachusetts General Hospital, Boston, MA, USA, 2Columbia College of Physicians and Surgeons, New York, NY, USA, 3Health Policy, Harvard Medical School, Boston, MA, USA, 4Obstetrics and Gynecology, University of Wisconsin School of Medicine and Public Health, Boston, MA, USA and 5Gynecologic Oncology, MD Anderson Cancer Center, Houston, TX, USA Introduction: Minimally invasive surgery was adopted as an alternative to laparotomy for radical hysterectomy for early-stage cervical cancer before high-quality evidence regarding its impact on survival was available. We sought to determine the impact of minimally invasive surgery on all-cause mortality among women undergoing radical hysterectomy for cervical cancer.Methods: We performed a cohort study in women who underwent radical hysterectomy for stage IA2-IB1 cervical cancer during 2010-2013 in Commission on Cancer-accredited hospitals in the United States, using inverse probability of treatment propensity score weighting. Additionally, we conducted an interrupted time-series analysis of women who underwent radical hysterectomy for cervical cancer during 2000-2010 using the Surveillance, Epidemiology, and End Results database. Results: In the primary analysis, 1,225 of 2,461 women (49.8%) underwent minimally invasive surgery. Women treated with minimally invasive surgery were more often white, privately insured, and from zip codes with higher socioeconomic attain - ment, and had smaller, lower-grade tumors. Over 45 months median follow-up, hazard of death was higher among women who received minimally invasive surgery than among those who underwent laparotomy (hazard ratio=1.65; 95% confidence interval[CI]=1.22-2.22, p log rank =0.002); the corresponding 4-year mortality risks were 9.1% and 5.3%, respectively. Before adoption of minimally invasive radical hysterectomy (2000-2006), 4-year relative survival remained stable (annual percent change=0.3%; 95%CI=-0.1% to 0.6%). Adoption of minimally invasive surgery coincided with a decline in 4-year relative survival of 0.8% per year after 2006 (95%CI=0.3% to 1.4%; p change-of-trend =0.01). Conclusion: For stage IA2-IB1 cervical carcinoma, minimally invasive radical hysterectomy may lead to worse overall survival than laparotomy. Kaplan-Meier survival curves for the propensity-matched IPTW groups. Women who underwent MIS RH had inferior overall survival compared with those who underwent laparotomy (plog rank = 0.002). The probability of death within 4-years of diagnosis was 9.1% among MIS compared with 5.3% among those who had laparotomy. Interrupted time series evaluating the effect of adoption of minimally invasive radical hysterectomy on 4-year relative survival among women with CeCa. The 4-year survival among women receiving radical hysterectomy for cervical cancer (diamonds) and 95% CI (whiskers) are plotted annually 2000-2010. The proportion of surgeries undertaken using a minimally invasive approach (circles) is plotted on the right axis. Before adoption of minimally invasive radical hysterectomy (2000-2006), 4-year relative survival remained stable (annual percent change=0.3%; 95%CI=-0.1% to 0.6%). Adoption of minimally invasive surgery coincided with a decline in 4-year relative survival of 0.8% per year after 2006 (95%CI=0.3% to 1.4%; p change-of-trend =0.01).157193 Taylor Mezoian, BS, Neurology The Dominant Inherited ALS (DIALS) Network: Methods For a Work in Progress T. Mezoian1, B. Dedi1, T. Xu1, K. Burke1, Nicholson1 1Massachusetts General Hospital, Boston, MA, USA, 2Washington University, St. Louis, MO, USA and 3Mayo Clinic Jacksonville, Jacksonville, FL, USA Introduction: The pre-symptomatic period has not been defined for ALS and markers of clinical conversion have yet to be discovered. Identifying biomarkers in asymptomatic ALS gene carriers would shed light on inciting disease pathology and allow us to move up the diagnostic horizon. The validation of sensitive biological markers and outcomes to follow the earliest signs of disease will set the stage to conduct successful trials aimed at disease prevention in populations of asymptomatic gene carriers, such as the DIALS Network cohort. Objective: To identify the earliest biological and clinical markers of disease in people at high risk for developing ALS, such as asymptomatic carriers of ALS causative genes.Methods: The DIALS network consists of a multicenter infrastructure (Massachusetts General Hospital and Washington University) with a streamlined protocol for CLIA genetic testing of asymptomatic potential ALS gene carriers through the New York Genome Center (NYGC). CLIA-genetic testing includes assessment for the C9ORF72 repeat expansion and whole genome sequencing. Over 30 known ALS-causative mutations are checked for return of results. Participants have the option of genetic disclosure at 3 months. They are otherwise evaluated every 6 months with a neurological exam, biofluid collec - tion (plasma, serum, whole blood, CSF, PBMCs, urine, skin biopsy) and outcomes (e.g. functional, respiratory, strength, cognitive, speech, and mood). Clinical data is collected using NeuroBANK. Biomarker analysis thus far has included assess-ment of neurofilament heavy (pNFH) and dipeptide repeat proteins in collaboration with the Petrucelli Lab at the Mayo Clinic Jacksonville, where samples are being evaluated alongside those from other collections involving ALS gene carriers.Results: Thus far, 36 participants have enrolled in the DIALS Network, with an interval analysis performed on 30 of these individuals. Approximately half of these individuals harbor an ALS causative mutation (10 C9ORF72, 3 SOD1, one SOD1 and SQSTM1). Three C9ORF72 carriers have developed symptoms (2 ALS, 1 FTD) and one SOD1 carrier was determined pauci-symptomatic at baseline; twenty-six remain asymptomatic. Enrollment and follow-up and biomarker and outcome data analyses are underway. We hope to expand to additional sites to increase sample size. Conclusion: The DIALS Network is building a rich dataset in asymptomatic gene carriers. We have already characterized several symptom converters. Increased sample size and continued longitudinal follow-up will help define the fundamental components of the conversion period in familial ALS, and support the design of prevention trials in familial ALS. Acknowledgments: ALS Finding a Cure, Target ALS, Muscular Dystrophy Association, and ALS Association have provided funding support for this project. 194 Carie Michael, SM, Medicine - Mongan Institute for Health Policy Professional Staffing and Roles in Care Teams Serving Frail Elders Living in the Community C. Michael1, K. Donelan1, B. Roberge1 and P. Maramaldi2 1Mongan Institute, MGH, Boston, MA, USA and 2Social Work, Simmons College, Boston, MA, USA Introduction: In 2017, as part of a study to understand the evolving roles of nurses, physicians and social workers in leading and working in teams, our interprofessional team explored 22 sites of care for frail elderly adults in five US regions. The purpose of these site visits was to understand the current range of models of care for frail elders living in community, the roles of health professionals within those care models, and to inform national measure development. Methods: We visited 22 sites and conducted 102 individual and team interviews in 7 states: Massachusetts, New Hampshire, Vermont, Florida, Illinois, California, and Colorado. Our site visit team for all visits included a health services, policy and survey researcher, a geriatric nurse practitioner, a professor of social work, and an administrator with experience in health care practice management. Inclusion criteria for sites: 1) Provided care to frail elders who live in the community, 2) represented a range of care models, staffing, services and sites of service delivery, and 3) were variously led by medicine (MD or DO), nursing (RN, NP, other APRN) or social work (LCSW) professionals. Data: Mixed methods: practice and staffing charac - teristics;professional roles; team structure; and inclusion of caregivers in team.Results: We organized sites using typology that combines professional service provided and target patient population in the following groupings shown in the table, Characteristics of Site Visit Organizations. Primary Care for Frail and Seriously Ill sites all provided care in the home for seriously ill and homebound patients and while all employed MDs and NPs/PAs, RNs were on staff at 4 of 5 sites and SWs at 3 of 5. With an average of 9.3 staff members (6.1 clinician staff members) for 158every 300 patients, these organizations had the most intensive staffing outside of residential care, but there was significant variation across sites with staffing ratios ranging from 3.1 to 20.8 staff for every 300 patients. Primary Care for Seniors Only sites delivered patient care in teams and all included physicians, RNs, and SWs, while only 2 of 5 employed NPs/PAs. This group included three new Medicare Advantage for-profit practices as well as traditional senior health. The average staff for every 300 patients in this group was 3.3 (1.3 clinician staff members), and ranged from 1.6 to 6.2. At Care Manage- ment programs we visited, we observed nurse-led models with little social work, and social worker-led models with little nursing. We visited both system and community settings. The six Care Management sites we visited have the widest range in staff ratios of any of our groups, ranging from 0.8 to 20.0 staff for every 300 patients (with an average of 7.3). Similarly, the range of clinician staff for every 300 patients ranges from 0.8 to 15.6. In Residential Care organizations, we observed social workers performing similar roles as in the care management models and also taking on administrative and leadership tasks. Staffing ratios here include 24-hr staffing and average 102.1 staff members for every 300 residents (12.5 clinical staff), and range from 40.0 to 135.5. In Primary Care and Urgent Care for All Ages care models, we visited two sites focused on vulnerable populations, and one for-profit mobile urgent care provider linked to area PCP and senior health practices. (We did not compare staffing ratios in this group.) Staffing Configurations will be summarized in graphic form (see Staffing Diagram Example).Conclusion: We saw tremendous variation in the kinds of organizations serving frail elders, professional staffing config- uations and roles. This is an area with considerable innovation and private investment in practices. We saw collaborative teams as well as physician, nurse, and social work led teams. We observed few interactions with informal caregivers, except in homes. Comprehensive care models have expensive staffing. Fragmentation of system engenders care management solutions. Characteristics of Site Visit Organizations Staffing Diagram Example 195 Rachel Millstein, PhD, Psychiatry A proof of concept trial of a positive psychology and motivational interviewing group intervention to promote physical activity among patients with metabolic syndrome R. Millstein1, M. 2General Internal Medicine, MGH, Boston, MA, USA Introduction: Metabolic syndrome is comprised of five cardiometabolic risk factors that affect up to 34% of US adults. Patients with metabolic syndrome are at high risk for developing chronic diseases such as type 2 diabetes and cardiovascular diseases. Intervening at the pre-disease state, on lifestyle behaviors such as physical activity, can greatly improve outcomes for patient with metabolic syndrome. The ecological model of health behaviors provides a multilevel framework on which to base successful interventions.159Methods: Primary care patients with metabolic syndrome were recruited from the Charlestown MGH Healthcare Center. Recruitment methods included soliciting referrals from PCPs, in-person recruiting at the clinic, and RPDR searches. Eligible patients needed to be insufficiently physically active (self-report an average of <150 minutes/week of moderate-vigorous physical activity) and have PCP approval to participate. This intervention study consisted of the following components (see figure 1): 8-weekly 90-minute group sessions, physical activity behavioral goal-setting and self-monitoring using Fitbits and pedometers, weekly physical activity topics using a motivational interviewing (MI) framework, weekly positive psychology (PP) exercises related to physical activity, a weekly group walk, and a focus on neighborhood built environment and walkability. Participants wore an Actigraph GT3X+ accelerometer for one week prior to beginning the study and after the last session. They also completed questionnaires pre- and post-intervention assessing physical activity barriers, optimism, positive affect, anxiety, depression, and health related quality of life. Weight and blood pressure were measured at the beginning and end of the intervention. Participants completed pre- and post-activity ratings to assess feasibility and accept - ability of the intervention. This is a proof of concept phase study that is currently ongoing. All procedures were approved by the Partners IRB, and all participants signed informed consent forms. Results: Four participants enrolled in this first round of the study, and three completed follow-up assessments. The median number of group sessions attended was 6 out of 8, indicating initial feasibility. Participants rated the ease and usefulness of the group activities as: 6.5/10 and 7.6/10, respectively. Average weekly physical activity, as measured by steps, increased by 2200 steps/week. Pre-post scores on self-report questionnaires are currently being completed. Conclusion: This first phase proof-of-concept trial of a physical activity motivational interviewing-positive psychology neighborhood-based intervention demonstrates high feasibility and acceptability. Participants attended sessions consistently, rated their liking of sessions highly, and all increased their physical activity over 8 weeks. Pre-post self-report, accelerom - eter, and biometric data will be calculated as soon as they are available. Groups will continue to run both at the Charlestown Healthcare center and additional MGH community clinics. 196 Jouha Min, PhD, Center for Systems Biology (CSB) Computational optics enables breast cancer profiling in point-of-care settings J. Min1, H. Im1, H. Lee1 and R. Weissleder1,2 1Center for Systems Biology, MGH, Boston, MA, USA and 2Radiology, MGH, Boston, MA, USA Introduction: The global burden of cancer, severe diagnostic bottlenecks in underserved regions, and underfunded health care systems are fueling the need for inexpensive, rapid and treatment-informative diagnostics. Breast cancer is a prime example. While histopathological subtyping and molecular genetic analysis are routine in developed countries, developing nations frequently experience pathologist shortages, underfunded health care systems, and more limited therapeutic options. Here we report the development of a new low-cost digital diffraction platform, termed AIDA (artificial intelligence diffraction analysis), for cellular analyses, integrating cutting-edge developments in computational optics. Particularly, we implemented a lens-less digital inline holography strategy using inexpensive CMOS systems to acquire \"shadows of cells\" which can then be analyzed by deep-learning algorithms for automated and rapid breast cancer diagnosis of fine needle aspirates (FNA). Methods: Assay. A panel of breast cancer cell lines expressing different levels of target biomarkers (ER, PR, HER2) was used. We first captured cancer cells based on the cancer signature quad-marker combination (HER2, EpCAM, EGFR, and MUC1) to identify and differentiate cancer cells from normal host cells. Captured cancer cells were then stained with two immuno-chromogens (red for ER/PR and blue for HER2), and imaged at two wavelengths ( = 470, 617 nm). The target protein levels were obtained from color intensities of individual cells. Deep learning algorithm. Convolutional Neural 160Network (CNN) was trained for the following tasks: (i) Cell recognition. The detection routine consisted of 3 sub-rou- tines: a maximally stable extremal regions (MSER) blob detection algorithm, a faster non-maximum suppression algorithm, and a CNN. We used 19,592 annotated images of cells and non-cells (i.e., debris, air bubble) for training and validation. (ii) Color classification. The classification model had 8 convolutional layers with 3 fully connected layers which produce the final output of 4 classes: (ER/PR) cell diffraction images. The ground truth for each input image was obtained by performing numerical reconstruction of diffraction patterns. Clinical samples. Two FNA samples were obtained as part of a standard-of-care, clinically indicated image guided biopsy. AIDA analyses were conducted blinded to conventional pathology. Results: Here we developed a new generation of a hologram diagnostic platform, termed AIDA, which addresses the existing challenges in cancer diagnosis. This system incorporates two new innovations: i) multiplexed molecular detection through the use of chemically orthogonal immuno-chromogens and multiple light sources, and ii) deep-learning algorithms to automate and speed up image analyses. In this work, we optimized and validated AIDA for breast cancer detection and subtype classification. We first developed a compact (11.8 \u00d7 12.3 \u00d7 15.6 cm 3) and self-contained system with no intermediate optical components necessary. It offered a wide field-of-view (25 mm2, 100X larger than conventional microscope) benefit - ting from lensless, unit-magnification imaging. We then trained deep neural networks to identify and classify cancer cells directly from holograms (i.e., no need for image reconstruction). This allowed for high-throughput quantitative molecular profiling with high accuracy (>90%) on individual cells and revealed cellular heterogeneity. This deep learning algorithm enabled cellular analyses at high throughput (>300 cells per sec, 400X faster than the conventional reconstruction/counting method). A pilot clinical study with patient samples demonstrated AIDA's potential for on-spot, automated and rapid cancer diagnostics\u2014ideal traits for inexpensive, rapid and treatment-informative care in point-of-care settings. Conclusion: The main advantage of AIDA over conventional microscopy is its low cost and the fact that it does not require specialists for diagnosis. The developed system can interrogate a much larger number of cells with a single image acquisi-tion because of the lens-less optical configuration (~1,000 cells within 3 sec). Such a capacity surpasses even flow cytome-ters or skilled cytopathologists scanning across slides, and importantly could elucidate intratumor heterogeneity, which will undoubtedly improve cancer diagnosis and the design of therapies. In essence, combined with a trained neural network, AIDA can automatically extract clinically relevant information with minimal human curation in short periods of time. The current work was designed to prove the overall AIDA concept (i.e., multicolor imaging, deep learning algorithms). As such, extensive clinical testing was beyond the scope of this developmental study. The data obtained with the first prototype indicated that future systems could be further improved. This includes implementing a miniaturized lens-less fluorescent system with higher degree of multiplexing capabilities. Finally, we expect that systems and assays will become cheaper as they will be scaled up. We currently estimate the component costs to be <$350 and reagent costs to be <$10 per assay. We anticipate that a major application of the AIDA technology will be in low and middle income countries. Since the technology is low-cost, the systems operable by less-skilled health workers it is well-suited for point-of-care settings, health clinics and rural areas. 197 Melanie Molina, MD, Emergency Health-Related Social Needs in the Emergency Department: A Pilot Hospital, Boston, MA, USA and 2Harvard T.H. Chan School of Public Health, Boston, MA, USA Introduction: The National Academy of Medicine (NAM) recently published recommendations for standardized screening for health-related social needs (HRSN), focusing on five domains: housing instability, food insecurity, transportation needs, utility needs, and interpersonal safety. Little is known about the extent of these needs in an emergency department (ED) population. Our objective in this study was to examine the prevalence of HRSN among patients in a large urban ED. Methods: We used publicly available questions in the five domains of the NAM recommendations to create a brief screener for HRSN in the ED. We then conducted 48 hours of time-shift sampling (24 hours of weekday and 24 hours of weekend) 161in the Fast Track and Pediatric areas of a large urban ED. Bilingual (English-Spanish) research assistants approached every arriving patient for enrollment during their shift; consenting patients or parents completed a brief demographic questionnaire and the HRSN assessment. We used standard descriptive statistics to describe the prevalence of HRSN, and a multivariable logistic model to assess the association between demographic factors and HRSN. Results: We enrolled 131 participants, of whom 80 (61%) were adult patients in Fast Track and 51 (39%) were parents of pediatric patients; 77 (58%) were female and 14 (11%) completed the assessment in Spanish. Overall, 37 (29%) reported an HRSN: 17 (15%) reported housing instability, 22 (17%) reported food insecurity, 14 (11%) reported transportation needs, 8 (6%) reported utility needs, and 19 (15%) reported interpersonal safety concerns. In unadjusted analyses, HRSN were not associated with location of enrollment (Fast Track or Pediatrics) or sex, but were associated with language. In a multivariable model adjusting for location and sex, language remained significantly associated with presence of any HRSN (OR 4.15 [95%CI: 1.28-13.4]).Conclusion: Almost one-third of ED patients reported an HRSN in this pilot study, with Spanish-speaking patients at significantly higher risk. These data demonstrate the importance of HRSN screening in the ED, while also highlighting the significance of ensuring that screening programs and referral resources are accessible to non-English speaking patients. 198 Connor P. Mulligan, B.A., Medicine - Cardiology Sudden Cardiac Death Among Persons Living with HIV and Heart failure without an ICD C.P. Mulligan1, Department of Radiology and Division of Cardiology, Harvard Medical School, Massachusetts General Hospital, Boston, MA, USA, 2Bronx-Lebanon Hospital Center of Icahn School of Medicine at Mount Sinai, Bronx, NY, USA, 3Division of Infectious Diseases, Department of Medicine and Department of Pediatrics, Harvard Medical School, Massachusetts General Hospital, Boston, MA, USA, 4Yale New-Haven Hospital of Yale University School of Medicine, New Haven, CT, USA, 5Divisions of Infectious Diseases and General Internal Medicine, Department of Medicine, Harvard Medical School, Massachusetts General Hospital, Boston, MA, USA and 6Program in Nutritional Metabolism, Harvard Medical School, Boston, MA, USA Introduction: Heart failure (HF) is associated with an increased risk for sudden cardiac death (SCD), and persons living with HIV (PHIV) are at increased risk of HF. Among PHIV without HF, the risk of SCD is increased; whether the risk for SCD among PHIV with HF is increased is unknown. The objective of this study is to analyze the incidence of SCD among PHIV with HF and no ICD and test associated factors.Methods: This was a prospective, observational single academic center registry of patients with HF. Patients with an ICD were excluded. The outcome of interest was SCD. Among PHIV with HF, we used multivariable logistic regression to test the association between traditional and HIV-specific factors with SCD. Sub-group analyses were performed by strata of CD4 count (<200, 200 cells/mm 3), and viral (86%) did not have an ICD; of these, there were 344 PHIV and 1,805 uninfected controls. Among PHIV with HF, 313 (91%) were prescribed ART and 221(64%) were virally suppressed. Overall, there were 191 SCD's over a median follow-up period of 19 months. When compared to uninfected controls with HF, PHIV had a 4-fold increase in SCD rate (21 vs. 5%, a rate of 13%/year among PHIV). Among PHIV, cocaine use, lower LVEF, absence of a beta-blocker prescription, QRS width, QTc duration, and HIV-specific parameters VL) independent predictors of SCD. The SCD rate among PHIV with a CD4 count 200 cells/mm 3 or an undectectable VL was not signifi- cantly increased as compared to the rate among HIV-uninfected individuals. Similar findings of an increased baseline risk with HIV and modification of SCD risk by HIV measures were observed by LVEF strata. Among PHIV with HF and without a conventional indication for an ICD, the rate of SCD was 10% per year. Conclusion: PHIV with HF are at a markedly increased risk for SCD. SCD risk was increased in patients with a lower LVEF, lower CD4 count, and a high VL. A clear majority of the patients who do not fall in the standard criteria for ICD placement may need close supervision for arrhythmia risk.162 Figure 2: Kaplan Meier survival curves comparing sudden cardiac death among (A) PHIV and uninfected controls and PHIV with a CD4 count 200 cells/mm3, (B) PHIV and uninfected controls and PHIV with an undetectable VL, (C) PHIV with a <200 cells/mm3, (D) PHIV with an undetectable VL with those with a detectable VL. Figure 3: Kaplan Meier survival curves comparing sudden cardiac death among all individuals with LVEF 35% (A) PHIV and uninfected controls and PHIV with a CD4 count 200 cells /mm3, (B) PHIV and uninfected controls and PHIV with an undetectable VL, (C) PHIV with a <200 cells/mm3, (D) PHIV with an undetectable VL with those with a detectable VL. 199 Janet Murphy, MD MPH, Medicine - Hematology/Oncology Potentially curative combination of TGF- 1 inhibitor Losartan R0 resection rates and prelim. survival data from a prospective phase II study J. Murphy Oncology, MGH, Boston, MA, USA Introduction: Pancreatic cancer is a lethal malignancy, and surgical resection represents the only path to cure. The improved efficacy of the FOLFIRINOX regimen raised the question of its potential utility for downstaging LAPC with an eye toward surgical resection (1), and indeed several retrospective series have now evaluated neoadjuvant FOLFIRINOX in Borderline- resectable and LAPC in the preoperative setting and have demonstrated an ability to convert from unresectable to resectable disease. (2-6) Preclinical data suggests that manipulating the renin-angiotensin (R-A) system may have anti-tumor effect in pancreatic cancer. In addition to governing renal and cardiovascular homeostasis, R-A mediates cell proliferation, metabo - lism, signaling in fibroblasts impacts tumor fibrosis and desmoplasia, a key feature of LAPC, via TGF-1. The primary effector of R-A signaling is Angiotensin type II. Inhibition of R-A activity is achieved by AngII type 1 receptor blockers (ARBs), including Losartan. These agents have the potential to both reduce the malignant potential of cancer cells and alter the tumor microenvironment, activating immunity and normalizing the extracellular matrix (ECM) to allow for enhanced delivery of cytotoxic chemotherapy. (8-12) Retrospective observational cohort studies suggest that patients with pancreatic cancer already on ACE inhibitors or ARBs because of pre-existing cardiovascular disease have improved survival. (13) The purpose of this study was to prospectively evaluate the efficacy of combining losartan and neoadjuvant FOLFRINOX followed by individualized chemoradiotherapy in patients with locally advanced pancreatic cancer. Methods: LAPC pts (per NCCN vascular criteria), ECOG PS 0-1 were enrolled in a single institution, phase II study. Patients received 8 cycles FOLFIRINOX/Losartan. If the tumor was radiographically resectable after chemotherapy, patients received short-course chemoradiation fractions (protons 25 GyE, mg/m2 bid). If the tumor still abutted vasculature, patients received CRT to 50.4 Gy to 58.8 Gy. Primary endpoint: resection rate, by final pathology Secondary endpoints: PFS, OS, Toxicity Results: 50 pts enrolled from 8/2013 to 7/2017. One pt withdrew consent, and 49 pts were evaluable for this analysis. The median patient age was 63, with 53% women. The median tumor size was 41 mm [18-68 mm]. Tumors were in the head in 31 (63%) of patients, body in 14 (29%) of patients, and tail in 4 (8%) of patients. Overall, FOLFIRINOX-Losartan was well-tolerated, with no single grade 3 toxicity exceeding 14% (diarrhea). Worst toxicity was grade 3 in 20 (41%) and grade 4 in 5 (10%) of patients. CI: (61%) rate in LAPC patients, translating into promising mPFS and mOS. A multi-center randomized Phase II trial is planned.163 Radiographic Response among patients receiving 8 cycles FOLFIRINOX-Losartan Kaplan-Meier Survival Analysis 200 Aashna Narang, BS, Anesthesia, and Pain Medicine A Multicenter, Randomized, Double-blind, Active-controlled Study to Evaluate the Safety and Efficacy of EXPAREL When Administered via Infiltration into the Transversus Abdominis Plane (TAP) Versus Bupivacaine Alone in Subjects Undergoing Elective Cesarean Section A. Narang 1,2, C. Copacino1,2, J. Ledley1,2, USA and 2Anesthesia, BWH, Boston, MA, USA Introduction: In consideration of the current climate in the United States of America, where opioid overdoses take the lives of over 115 people per day, this research intends to evaluate an alternative pain management strategy that does not primarily rely upon the use of opioids in managing pain for women who undergo Cesarean section. Transversus abdominis plane (TAP) blocks are commonly used for post-operative analgesia in patients having lower-abdominal surgeries. We intend to evaluate opioid consumption in women who receive TAP blocks after undergoing C-section. We will explore if EXPAREL, or liposomal bupivacaine, provides improved analgesia and leads to less opioid consumption as compared to plain bupiva-caine. We hypothesize that subjects who receive TAP blocks with EXPAREL will consume less opioids than the subjects who receive plain bupivacaine TAP blocks. Secondary outcomes that will be evaluated include the time to first postsurgical opioid rescue medication, Visual Analog Scale (VAS) pain intensity scores, and the percentage of opioid-free subjects. Methods: In this Phase 4 multicenter, randomized, double-blind, active-controlled study, participants undergoing elective cesarean sections are randomized in a 1:1 ratio to one of two groups receiving a TAP block following spinal anesthesia within 90 minutes of the end of the procedure. Subjects in the first group receive EXPAREL and plain bupivacaine TAP infiltration and those in the second group receive only plain bupivacaine TAP infiltration. Subjects in both groups receive 15 mg of intravenous ketorolac and 1000 mg of intravenous acetaminophen at skin closure. All subjects subsequently are provided with 650 mg acetaminophen orally (PO) and 600 mg PO ibuprofen every 6 hours, beginning 6 hours after procedure end for up to 72 hours. Subjects can request 5-10 mg oral oxycodone as needed for breakthrough pain and are asked to document their pain level immediately before taking their medication. Pain levels, vital signs, and any adverse events are assessed every 6 hours. Outcomes that are being assessed include opioid use, time to first unassisted ambulation, discharge readiness, subject satisfaction with postsurgical pain control, overall benefit of anesthesia, and quality of recovery. Subjects are asked to record their daily pain level and pain medications in a patient diary for 14 days after undergoing cesarean section. About 14 days after surgery, a phone call is made to the subject to inquire if any unscheduled phone calls, appointments, or hospital admissions relating to pain took place in the interim and any adverse events that have occurred are recorded.Results: After 80 subjects are enrolled in the study, an interim analysis will be conducted to evaluate the data. Subject data will be assessed to determine efficacy of the TAP block with liposomal bupivacaine compared to plain bupivacaine in regards to reducing total opioid use and its effects on the above-mentioned secondary outcomes. Depending on the results of the interim analysis, it will be determined whether the study should continue enrolling subjects and if so, whether or not to increase the target sample size.Conclusion: We hypothesize that the data will show a significant decrease in the total opioid use in subjects who received the EXPAREL in their TAP block over subjects who received the standard-of-care plain bupivacaine. To highlight this and the analgesic benefits of the TAP block supplemented with liposomal bupivacaine (EXPAREL) for post-cesarean-section 164patients, it may be useful to compare outcomes between this group of subjects and those who do not receive any TAP block at all. We would expect the opioid use for the former group to be significantly less than the latter group. Eventually, it may be meaningful to conduct a non-opioid study where subjects are given TAP blocks without intrathecal opioids given as part of their spinal anesthetic to further optimize post-operative analgesia in this group of patients while limiting opioid consumption. 201 Zahra Nasiriavanaki, M.D, Psychiatry The role of the neural system that monitors personal space in social functioning: relationship to social motivation Z. Nasiriavanaki1, A. Harikumar1, J. L. Mow1, L. Tuominen1,2, O. Terechina1,2, Barbour1,2, C. Coman1,2, E. Hines1, R. Tootell3,2,4 and D. Holt1,2,4 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Radiology, Massachusetts General Hospital, Boston, MA, USA and 4Athinoula A. Martinos Center, Biomedical Imaging, Boston, MA, USA Introduction: One type of non-verbal social communication that is altered in several neuropsychiatric conditions, including autism and schizophrenia, is social spacing or \"personal space\", the preferred physical distance maintained from other individuals. Although many of the psychological features of personal space have been well-studied, the neurobiological mechanisms governing its regulation are less well-understood. We have found that a parietofrontal cortical network, that has a well-established role in monitoring peripersonal space in primates, selectively responds to stimuli that appear to enter personal space in humans (Holt et al, 2014). We have also found that the strength of the connectivity of this network predicts the size of personal space, as well as interest in spending time with others. In other words, weaker connectivity within this network was observed in individuals who, relative to other healthy subjects, are less comfortable with the physical proximity of others (i.e., have larger personal space) and have less interest in spending time with other people. We observed a similar pattern of findings in a cohort of patients with schizophrenia. To better understand the functioning of this neural system, we have developed a modified version of our original paradigm in an effort to reliably produce robust data in individual subjects. Obtaining reliable single subject data is a necessary prerequisite for understanding individual differences and for clinical translation. Using this modified paradigm, we predicted that we would find associations between levels of social interest/motivation and 1) behavioral characteristics of personal space and 2) functioning of the parietofrontal network that monitors personal space.Methods: In several cohorts of healthy subjects, we measured personal space using the classic Stop-Distance Paradigm (Hayduk LA, 1983). In this paradigm, the participant and an experimenter initially stand 3 meters apart; then the experi- menter slowly approaches the participant. The participants says \"stop\" when the experimenter has reached the boundary of his/her personal space. Also, participants performed the line bisection task, which measures a line bisection bias induced by spatial neglect; these data were used to calculate the transition between \"near\" and \"far\" space in each individual (Longo, & Lourenco, 2007). Questionnaires, including the Chapman Revised Social Anhedonia Scale (SAS-R; Chapman, Chapman, & Raulin, 1976), the Chapman Revised Physical Anhedonia Scale (PAS; Chapman et al, 1976) and the Temporal Experience of Pleasure Scale (TEPS; Gard, Gard-Germans, Kring, & Oliver, 2006), the Time Alone Questionnaire (TAQ), were also administered to measure levels of motivation (social and non-social). Lastly, functional MRI data were collected in a subset of subjects; during scanning, the subjects viewed 16 computer generated realistic faces (8 males, 8 females, presented for 16 seconds each) with neutral emotional expressions, with their eyes open or closed, presented in a random order. The stimuli were presented one face at a time and appeared to either approach or withdraw from the subject at approximately the speed of walking. Following the scan, preferred \"distances\" to each of the 16 faces viewed were measured while the subject remained in the scanner. The fMRI data were analyzed using FreeSurfer (http://surfer.nmr.mgh.harvard.edu).Results: In two independent cohorts of healthy subjects, significant negative correlations between the size of personal space, as measured by the Stop Distance Paradigm, and levels of social interest were found, i.e., a smaller personal space size was associated with greater levels of social interest. No correlation between personal space size and the spatial neglect-based measurement of the boundary between \"near\" and \"far\" space was observed, however. Within-scanner measurements of preferred distances from the computerized faces were correlated with personal space sizes. In examining the fMRI data, signif-icant activation of an area of the parietal cortex, the dorsal intraparietal sulcus (DIPS), was found in response to approaching compared to withdrawing faces. Moreover, both right and left DIPS activation to approaching versus withdrawing faces was positively correlated with levels of anhedonia.Conclusion: The size of personal space, which reflects the degree of comfort with the physical proximity of other people, reflects (or represents a manifestation of) an individual's level of interest in spending time with others. Moreover, this associ- ation is linked to variation in the function of a parietal area that is known to play a role in monitoring the space near the body. Taken together, these data support the proposal that certain basic perceptual processes, such as the visual perception of space, represent fundamental building blocks of the neural mechanisms that generate social behavior.165202 Mark Nazal, MPH, Orthopedics Villonodular Synovitis of the Hip Managed with Arthroscopic Synovectomy: An Analysis of 19 Cases with up to 10-year Follow-up M. Nazal1,2, J. Stelzer1, A. Parsa1 and S. Martin1 1Department of Orthopaedic Surgery, Sports Medicine, Massachusetts General Hospital, Boston, MA, USA and 2Wake Forest School of Medicine, Winston Salem, NC, USA Introduction: A tenosynoival giant cell tumor known as pigmented villonodular synovitis (PVNS) is a rare, benign, yet locally aggressive neoplasm arising for the synovium. Although most frequently encountered in the knee joint, PVNS is second most commonly seen in the hip joint (or acetabulofemoral joint). Occurrence of PVNS in the hip may be detrimental by destroying intra-articular soft tissue and bone, and at times invading extra-articular tissues. There are two categoriza- tions of PVNS: localized and diffuse. The localized manifestation, often referred to as nodular, involves a focal or limited area of the synovium, while the diffuse manifestation involves more widespread areas throughout the synovium. Surgical management by resection of the PVNS within the acetabulofemoral joint has been described with open procedures and arthroscopic procedures including interportal and T-capsulotomy techniques. Although surgical management has been shown to be promising, PVNS has a high recurrence rate, of 15-50%. To date, we present the largest cohort of acetabulofemoral PVNS managed by arthroscopic synovectomy with a puncture capsulotomy technique.Methods: Following institutional review board approval, 19 patients were diagnosed with acetabulofemoral PVNS and underwent arthroscopic synovectomy between 2008 and 2016 by a single surgeon at a single center. Patients were identified preoperatively based on suspected MRI findings of PVNS or intraoperatively after unexpected PVNS was identified during arthroscopy for other pathology. We assessed patients' functional ability at a minimum of 2 years follow-up using five patient reported outcome measures (PROMs): mHHS (modified Harris Hip Score), HOS (Hip Outcome Score), iHOT-33 (international Hip Outcome Tool), NAHS (Non-Arthritic Hip Score), and LEFS (Lower Extremity Functional Score). Statistical analysis was performed using STATA 14.2 (STATA SE 2015) and Microsoft Excel 2015 (Microsoft Corp). Categorial statistics were represented as number and percentage, while continuous statistics were represented as mean and standard deviation. This study has been approved by the institutional review board.Results: Nineteen patients with acetabulofemoral PVNS were treated with an arthroscopic hip synovectomy utilizing a puncture capsulotomy technique. Seventeen of the patients completed patient reported outcome measures. This cohort consisted of 7 (44%) females and 9 (56%) males. The mean patient age was 37 years (range, 25-54). Mean follow-up was 83 months (6.9 years), ranging from 24 months (2 years) to 123 months (10.3 years). Eight (50%) patients had diffuse PVNS and 8 (50%) of patients had nodular PVNS. Laterality of the arthroscopic procedure was 8 (50%) right hip and 8 (50%) left hip. Five (31%) of the patients had a smoking history, while 11 (69%) were non-smokers. Four (25%) had a cam lesion, 4 (25%) had a pincer lesion, 8 (50%) had neither cam nor pincer lesion, and 0 (0%) had combined cam and pincer Fourteen (88%) patients had histologically confirmed PVNS. Eleven (69%) patients had MRI suspected PVNS prior to arthroscopy, while 5 (31%) patients had unexpected PVNS discovered intraoperatively. Concomitant pathology addressed at the time of arthroscopy, includes 11 (79%) labral tear repair. For the seventeen patients that completed patient reported outcome measures, the mean mHHS was 78.2 (SD, 10.6). Mean HOS-ADL was 10 (SD, 10). Mean HOS-SSS was 21 (SD, 20). Mean iHOT-33 was 69.4 (SD, 16.3). Mean NAHS was 86.5 (SD, 10.2). Mean LEFS was 70.8 (SD, 7.7). A mean percentage of maximal function was 88.6% (SD, 9.8%). To date, 0 (0%) patients have shown evidence of recurrence upon physical exam, follow-up MRI, or repeat arthroscopy.Conclusion: Based on the largest arthroscopically managed cohort of PVNS in the hip to date, we conclude that arthroscopic synovectomy with a puncture capsulotomy technique is a reliable and effective treatment of acetabulofemoral PVNS. Patients reported good functional outcomes without evidence of recurrence in a 19 patient cohort with an average followup of almost 7 years. 203 Mark Nazal, MPH, Orthopedics Follow-up of Endoscopic Repair Tendon Tears: Nazal2,1, J. Stelzer2 and S. Martin2 1Wake Forest School of Medicine - Winston Salem, NC, Winston Salem, NC, USA and 2Department of Orthopaedic Surgery, Sports Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Surgical advances have made endoscopic repair of full-thickness gluteus medius and gluteus minimus tears favorable. After failed conservative management, endoscopically repaired full thickness hip abductor tendon tears have 166shown excellent outcomes thus far; however, there has been a lack of long-term follow-up in the literature. The purpose of this study was to prospectively assess the surgical outcomes of full-thickness abductor tendon repairs.Methods: Clinical outcome data were prospectively collected from patients who underwent endoscopic gluteus medius and minimus repair by a single surgeon between 2009 and 2017. Patients with a minimum 2-years follow-up were evaluated using the following patient reported outcome measures (PROMs): modified Harris Hip Score (mHHS), Hip Outcome Score-Activity of Daily Living (HOS-ADL), Hip Outcome Score-Sports Sub-scale (HOS-SSS), and International Hip Outcome Tool (iHOT-33). All analyses were run using STATA 14.2 (STATA SE 2015) and Microsoft Excel 2015 (Microsoft Corp). Categorical statistics were represented as number and percentage, while continuous statistics were represented as mean and standard deviation. This study has been approved by the institutional review board.Results: Twenty-one primary abductor tendon repairs were performed in 21 patients, of which 16 (76%) were women and 5 (24%) were men. All 21 patients completed follow-up of at least 2 years, with a mean follow-up period of 68 months or 5 years 8 months (range, 23 to 117 months). The mean patient age was 73 years (range, 50 to 86 years). For mHHS, the mean (SD) preoperative enrollment, 1-year follow-up, and final follow-up scores were 51 (SD, 15), 72 (SD, 16), and 80 (SD, 12), respectively. For the 9 (43%) patients with at least 5-years follow-up, the mean (SD) mHHS was 86 (SD, 31). For mHHS, the mean improvement was 29. For HOS-ADL, the mean (SD) preoperative enrollment, 1-year follow-up, and final followup scores were 47 (SD, 16), 73 (SD, 19), and 75 (SD, 25), respectively. For HOS-ADL, the mean improvement was 28. For HOS-SSS, the mean (SD) preoperative enrollment scores were 34 (SD, 31), 56 (SD, 33), and 61 (SD, 35), respec- tively. For HOS-SSS, the mean improvement was 27. For iHOT-33, the mean (SD) preoperative enrollment scores were 35 (SD, 14), 64 (SD, 22), and 73 (SD, 24), respectively. For iHOT-33, the mean improvement was 38. All 21 patients (100%) with at least 2-years follow-up reported satisfaction with the surgical treatment received.Conclusion: In our prospective study with at least 2-years follow-up, endoscopic repair of fullthickness gluteal tendon tears was an effective surgical intervention. The cohort of 21 patients had positive improvements according to four patient reported outcome measures. Although variability in length of outcome measure follow-up existed, the mHHS of the cohort as a group improved from \"poor\" to \"good.\" In conclusion, endoscopically management of fullthickness ab Flavia C. Nery, Ph.D., Center for 3Pediatric Nephrology, MGH, Boston, MA, USA and 4Cellular and Molecular Medicine, University of Ottawa, Ottawa, ON, Canada Introduction: While spinal muscular atrophy (SMA) is primarily a disease of motor neurons, studies in both mouse models and humans increasingly support more widespread systemic involvement. Organ dysfunction outside the nervous system could be related to primary tissue deficiency of SMN protein or to secondary complications related to immobilization and profound weakness. Patients with infantile and early childhood onset SMA universally demonstrate osteoporosis, increased fracture risk, and skeletal abnormalities including thin ribs, coxa valga, and scoliosis. Lack of weight-bearing and defects in bone remodeling may result in hypercalcemia, hypercalciuria, and suppressed parathyroid hormone (PTH), increasing the risk for kidney stones or nephrocalcinosisMethods: Participants. Kidney samples obtained at the time of autopsy from 12 patients with SMA Type I and 13 age- and sex-matched control cases (NIH NeuroBioBank) were analyzed. All SMA Type I patients had homozygous deletions of the SMN1 gene and 2 copies of the SMN2 gene. Mouse Model. The Smn2B/- mice were housed at the University of Ottawa animal facility and cared for according to the Canadian Council on Animal Care (1). Kidney tissues were collected at P19 from symptomatic Smn2B/- and wild-type control mice. Laboratory Analysis. All available data from diagnostic studies performed in the course of routine care of SMA patients prior to death were extracted from the clinical records and analyzed. 167Renal Gene Expression. Total RNA was extracted from frozen kidney samples using TRIzol Reagent or Qiagen RNeasy mini kit according to the manufacturer's protocol (ThermoFisher Scientific, Waltham, MA/Qiagen, Venlo, The Netherlands). All the samples had a 280/260 ratio 1.9. cDNA was generated from 1 \u00b5g of purified RNA using the RT2 First Strand Kit (Qiagen) protocol or 300 IV VILO (11766050, ThermoFisher Scientific). The Ct values were normalized to an average of 5 housekeeping genes in the Qiagen array. Fold change was calculated based on the average of the normalized Ct values. For the TaqMan Gene Expression Assay, the Ct values were normalized to the average of 2 housekeeping genes in the array (GAPDH and SDHA). Gene expression in for mouse studies were normalized to PolJ and Sdha. Histological Analysis. Formalin-fixed paraffin-embedded tissue sections of the autopsy kidney samples were prepared using hematoxylin and eosin (H&E), Periodic acid-Schiff (PAS), Masson's trichrome, Alizarin Kossa stains and analyzed by light microscopy. Immunoblot. The antibodies signaling Technology, DM1A-1:2000). Secondary antibodies used included IRDye 680 and 800 (Li-Cor-1:10000). Signals were detected with Odyssey CLx (Li-Cor). The results were normalized to -tubulin. Statistical Analysis. The results are expressed as mean \u00b1 SD. Differences between controls and SMA were analyzed by the Student's t-test or the chi-square test.Results: Autopsy studies were performed in a cohort of 11 SMA type 1 infants and children who died at ages ranging from 4 months to 10 years of age. Kidney specimens were available for gross and microscopic histologic studies in 9 subjects. Nephrocalcinosis and fibrosis were present in 5/9 (56%) and 6/9 of the cases microscopically examined, respectively. Nephrocalcinsois and fibrosis were absent in all 9 age- and sex-matched controls. CD68 and CD3 staining in 3/3 with the most severe nephrocalcinosis confirmed macrophage and T-lymphocyte infiltration, respectively, around medullary calcium deposits, indicating an inflammatory response. Given that the insulin growth factor-1 (IGF-1) and insulin-like growth factor binding proteins (IGFBPs) play a critical role in the maintenance of normal renal function, the pathogenesis, and progression of chronic kidney disease, we investigated the expression of these genes in kidney samples by qPCR. Our results on SMA patient kidney specimens (N=5) and age- and sex-matched control specimens (N=5) confirmed an increased up-regulation of IGF-1 and IGFBP-1 in SMA samples when compared to controls. Interestingly, IGFBP-1 was also elevated in kidney from Smn2B/- mouse model as compared to wild-type control specimens. Conclusion: IGF-1 signaling dysregulation in SMA kidney may be related to alterations in calcium metabolism and contribute to secondary nephrocalcinosis and tubular dysfunction. More studies are needed to confirm this hypothesis, and to determine the degree to which renal dysfunction and defects in bone density and remodeling may contribute to adverse outcomes in SMA patients. Careful monitoring of renal function may be indicated given the significant abnormalities noted in this study. 205 Emily D. Nguyen, BS, Dermatology Risk Factors Predisposing Patients to Recurrence 1Dermatology, Massachusetts General Hospital, Boston, MA, USA and 2Nephrology, Massachusetts General Hospital, Boston, MA, USA Introduction: Calciphylaxis, or calcific uremic arteriolopathy, is a rare and potentially fatal disorder characterized by widespread cutaneous necrosis secondary to calcium deposition within small- and medium-sized vessels and subsequent thrombosis. Patients with comorbid end-stage renal disease, hyperparathyroidism, diabetes, and warfarin use are at increased risk of developing this disease. Although previously associated with a one-year mortality of up to 60%, increasing clinical awareness has resulted in earlier treatment initiation and subsequently complete resolution, lower incidence of complications such as sepsis, and decreased psychological trauma. While complete resolution of lesions has increasingly been reported, 168some patients go on to develop recurrent disease. There remains a paucity of literature exploring risk factors which predispose patients to recurrence of calciphylaxis. The purpose of this study was to compare risk factors between patients with a single episode of calciphylaxis and those who develop disease recurrence. Methods: A retrospective analysis of 131 patients admitted to Massachusetts General Hospital for calciphylaxis from 1/2000 to 12/2018 was conducted. Chi square analysis was performed to compare the two groups.Results: Demographic data, comorbidities, medications, and disease course were assessed in 99 patients with a single episode of calciphylaxis and 32 patients with recurrent disease. Of those with recurrent calciphylaxis, patients had a mean of 1.6 recurrences overall with a mean of 18.9 months from the initial lesion to first recurrent lesion. Mortality from calciphy - laxis-related causes was higher in the recurrent group (28.1%) compared to the single episode group (5.1%) (p=0.0003). Mean BMI was 35.9 in patients with recurrence and 28.7 in patients with resolved calciphylaxis (p<0.0001), indicating that recurrence may be associated with obesity. Regarding treatment, 43.8% of the patients with multiple episodes had been treated with a parathyroidectomy compared to 9.1% of the single episode group (p<0.0001). Of note, of the patients who received IV sodium thiosulfate, 93% had a single episode while 7% experienced recurrence (p<0.0001). 15.6% of patients who had recurrent episodes of calciphylaxis were treated with vitamin K supplementation, while only 2% of patients with single episode calciphylaxis were supplemented with vitamin K (p=0.003). Risk factors found frequently in patients with disease recurrence include end-stage renal disease, which affects 93.8% of patients with recurrence and 71% of those with only one episode (p=0.009).28% of patients with recurrence also had chronic liver disease, compared to 5.1% of patients with single episode calciphylaxis (p=0.0003). Current smoking status was seen in 25% of patients who recurred and in 4% of those who had no recurrence (p=0.0004). Disease recurrence risk is 4.64 times more likely in the setting of chronic systemic corticosteroid use (p<0.0001). Vitamin D use was also found in 53.1% of patients with recurrence and only in 21.1% of patients with a single episode (p=0.0005). Conclusion: Patients with higher BMI, hyperparathyroidism with parathyroidectomy, chronic corticosteroid use, vitamin K supplementation, current smoking status, and active vitamin D use are found to be likely to develop recurrent disease compared to those without these factors. Repeated episodes of calciphylaxis is also associated with higher mortality secondary to calciphylaxis-related causes. Of note, patients who received intravenous sodium thiosulfate during the treatment of initial lesions were found to have decreased incidence of disease recurrence. This is suggestive that a shift towards aggres- sive initial intravenous therapy may provide long-term reduction of recurrence. Additionally, monitoring for such risk factors at initial presentation may reduce likelihood of recurrence and improve patient outcomes. 206 Kristen Nishimi, MPH, Center for Genomic Medicine (CGM) Do different measures of psychological resilience correlate with each other and predict health outcomes to the same degree? Results from a large, epidemiological sample K. Nishimi2,1, A. Powers3, B. Bradley4,3 and E.C. Dunn2,5,6 1Social and Behavioral Science, Harvard TH Chan School of Public Health, Cambridge, MA, USA, 2Psychiatric and Neurodevelopmental Genetics Unit, Massachusetts General Hospital, Boston, MA, USA, 3Department of Psychiatry and Behavioral Sciences, Emory University School of Medicine, Atlanta, GA, USA, 4Atlanta VA Medical Center, Atlanta, GA, USA, 5Department of Psychiatry, Harvard Medical School, Boston, MA, USA and 6Stanley Center for Psychiatric Research, The Broad Institute of Harvard and MIT, Cambridge, MA, USA Introduction: Psychological resilience, the phenomenon of positive psychological adaptation in the context of adversity, has received increased attention across scientific disciplines. Through this work, there are now multiple definitions of the term \"resilience\" and a wide variety of measures used to capture psychological resilience. For example, psychological resilience has typically been defined as: (1) the absence of mental illness and/or presence of positive psychological functioning in trauma-exposed individuals; (2) psychological functioning relative to level of trauma exposure; and (3) perceived resiliency (i.e., self-reported ability to cope with adversity). However, little is known about whether these measures capture the same underlying construct of resilience. Moreover, it is also unknown whether these different measures are equally good predictors of health outcomes, converging to yield similar findings. To address these gaps, we examined the extent to which definitions of psychological resilience, as described above, correlate with each other and/or share similar associations with socio- demographic factors and body mass index (BMI), as a proxy measure of physical health using data from a large popula - tion-based sample of trauma-exposed adults. Methods: Data came from a cross-sectional sample of 1,429 African American adults recruited from an urban public hospital to participate in an ongoing community-based study of the consequences of trauma exposure. In keeping with existing conceptualizations of psychological resilience as positive adaptation in the context of adversity, we derived four measures of psychological resilience only among individuals exposed to child maltreatment as reported through the Childhood Trauma Questionnaire (n=1429 of 3364 total; 42.5%); all other participants were excluded from the analysis. The four measures 169were: 1) a binary variable indicating the presence (vs. absence) of resilience, defined as low symptoms of both depression and posttraumatic stress among individuals with child maltreatment histories (absence of distress), 2) a binary variable that required both the absence of distress and high positive affect (absence of distress plus positive functioning ), 3) a continuous variable indicating an individual's departure (or residual) from a regression model in which psychological distress symptoms were regressed on reported levels of child maltreatment; in this definition, positive residuals indicate relatively higher distress symptoms, whereas negative residuals indicate relatively lower distress symptoms thus lower scores indicate higher resilience (estimated resilience), and 4) a self-reported scale assessing resiliency (perceived trait resilience). Current Symptoms of depression and posttraumatic stress, as well as levels of positive affect and self-reported resiliency, were assessed using standard self-report scales. Socio-demographic factors included were sex, age, education, income, and employment status. Self-reported height and weight were used to calculate BMI. Two-way Pearson correlations and multiple linear regression models were used to determine associations between each of the resilience measures and with BMI, adjusting for socio- demographic variables. Results: The psychological resilience operationalizations were weakly to moderately correlated with each other (range 0.27-0.69). The strongest correlation identified (r=0.69) was between the absence of distress and absence of distress plus positive functioning operationalizations. Overall, the resilience operationalizations showed generally similar associations with socio-demographic factors, with higher socio-economic status related to higher resilience. After adjusting for socio- demographic variables, there was a strong negative association between psychological resilience and BMI with the absence of distress plus positive functioning operationalization, which suggested that people categorized as resilient had BMI scores that were 2 units lower than people categorized as non-resilient (=-2.10, 95%CI -3.96, -0.24), and with the perceived trait resiliency operationalization, which suggested that one standard deviation increase in resilience score was associated with 0.45 units lower BMI (=-0.45, 95%CI -0.84, -0.08). Neither absence of distress nor estimated resilience operationalizations were significantly associated with BMI.Conclusion: The main finding of this study is that different measures of psychological resilience are moderately correlated with each other and show similar associations with socio-demographic factors, but different relationships to a physical health outcome. Future research should continue to examine measurement validity across multiple definitions of psychological resilience in diverse samples. 207 Sharon Odametey, MPH, Dermatology Evaluating the Usability and Acceptability of a Caregiver Support Platform in Primary Caregivers of Breast Cancer Patients S. Odametey1,2,3, R. Palacholla1,2,3, S. Bane1,2, J. Hirschey1,2, S. Agboola1,2,3, K. Jethwani1,2,3 and J. Kvedar1,2,3 1Connected Health Innovation, Partners Healthcare, Boston, MA, USA, 2Massachusetts General Hospital, Boston, MA, USA and 3Harvard Medical School, Boston, MA, USA Introduction: Caregivers of older patients and patients with advanced illnesses suffer from a high burden of care, physical and emotional struggles, in addition to balancing jobs and caregiving tasks [1]. The diagnosis of cancer presents a major crisis not only to the patients but also to the patients' primary caregivers. Patients with cancer often require 24-hour assistance with their basic personal needs and with medication management. However, caregiver involvement is vital for fostering targeted and patient-centered care for people with such advanced illnesses. With minimal preparation and uneven guidance from the healthcare system, caregivers often suffer from a multidimensional biopsychosocial reaction resulting from an imbalance of care demands [2]. To address some of these psychosocial needs, the Cancer Caregiver Guidebook (CCG) was developed to provide adequate information to caregivers on what to expect when caring for a loved one with cancer, helpful strategies to cope with challenges, and tools and tips to successfully manage the day-to-day tasks of caregiving. The goal of this project is to evaluate the usability and acceptability of the CCG among primary caregivers of patients with breast cancer.Methods: Enrolled participants (N=15) were 18 years or older and served as primary caregivers of breast cancer patients (stage I, II, III or IV). All participants were instructed to use the CCG for 4 weeks. To assess user satisfaction and acceptability of the CCG, questionnaires were administered to all participants. To better understand caregiver and clinician experiences with the CCG, participants volunteered to participate in open ended interviews. Descriptive analysis was conducted to present the usability data. Qualitative thematic data analysis was conducted using NVivo to extract themes from interviews.Results: 12 caregivers were included in the analysis. Majority of the participants were white (87%), over 50 years old (77%) and belonged to a higher socio-economic status with an annual income greater than $100,000 (47%). 80% of the participants used the CCG multiple times in a week for the study duration. 100% of the participants found the information provided in the CCG helpful in their caregiver journey. 85% of participants felt confident caring for their loved one using the CCG. At least 60% of participants used all the features and 100% found the CCG \"Tips\" section as the most useful. There was an increase in participant's appraisal of caregiving (burden of caregiving); however, this was not statistically significant (p=0.13). 170Two caregivers and 2 clinicians were included in the qualitative analysis. The themes from qualitative analysis were that organization of the content of the CCG was intuitive, and the resources and tools provided in CCG were valuable. However, the caregivers felt that locating specific information was sometimes challenging and additional information about psychological help would be valuable.Conclusion: Acceptability and usability of the CCG was high among caregivers showing a clear need for a caregiver support tool. The CCG usage varied among caregivers based on characteristics such as level of education and stage of patient disease. A larger study would be required to further analyze the CCG impact on appraisal of caregiving (caregiver burden). 208 Katelyn Oliver, Psychiatry Nightmare and bad dream recall rates correlate with hyperarousal in individuals exposed to psychological trauma M. Ulsa1, J. Pace-Schott1,2 1Psychiatry, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA and 2MGH/HST Athinoula A. Martinos Center for Biomedical Imaging, Charlestown, MA, USA Introduction: Symptoms of physiological and psychological hyperarousal are common following a traumatic stressor and, when severe, are a key diagnostic criterion for posttraumatic stress disorder (PTSD). Another key diagnostic criterion of PTSD are experiential intrusions related to the trauma which include nightmares. We have suggested that Hyperarousal includes sleep disturbance and that this may disrupt consolidation of emotion regulatory processes such as extinction memory and habituation thereby contributing to the other symptoms of PTSD. Here we examine whether an index of hyperarousal predicts the rate of nightmares and bad dreams in traumatized individuals.Methods: Seventy participants aged 18-40 (45 females) who were exposed to a traumatic event meeting DSM-5 severity criterion for PTSD (Cluster A) within the past two years completed the Clinician-Administered PTSD Scale (CAPS-5). Of these, CAPS-5 scores ranged from 0-50 (mean 21.4, SD 12.8, median 19) and 32 met criteria for PTSD. All participants completed a sleep diary immediately upon waking over 11-27 nights (mean 14.7, SD 2.3, median 14). The diary contained a nightmare questionnaire that asked whether a dream was recalled, what type of dream it was (choices were: nightmare, bad dream, sleep terror, recurrent, lucid, or other, based upon written definitions provided), and the degree to which it resembled the traumatic event (exactly, similar, possibly similar, or unrelated). Per-diary percentages were calculated for total dreams, nightmares, bad dreams, and their sum as well as for nightmares or bad dreams that exactly resembled or were similar to the trauma. A Composite Hyperarousal Index (CHI) was computed from 3 subscales: hyperarousal items (Cluster E) on the CAPS-5 and PTSD Checklist for DSM-5 administered during a psychiatric interview and a published hyperarousal scale completed at home. For the CHI, subscales were converted to 0-100 scales and averaged. A similar Combined Psychopa- thology Index (CPI) was computed from the Depression, Anxiety and Stress Scale, the World Health Organization Disability Assessment Schedule and the Symptom Checklist-90. Relationships between CHI or CPI with each of the above dream percentages were examined using simple regression. Results: The CHI did not vary with total dream recall rate (R=0.18, p=0.14) but significantly varied with nightmare recall rate (R=0.32, p=0.008), bad dream recall rate (R=0.33, p=0.006), and their combined rates (R=0.411, P=0.0007) but not with nightmares and bad dreams exactly-like or similar to the trauma (R=0.18, p=0.15). However for percentages on a per-dream basis, although CHI didn't predict nightmares alone (R=0.17, p=0.17), it did predict number of bad dreams (R=0.29, p=0.02), combined nightmares and bad dreams (R=0.41, p=0.0008) and remained a trend for nightmares and bad dreams exactly-like or similar to the trauma (R=0.24, p=0.059). Relationships of CPI with these dream variables was weaker either on a per-night basis [nightmare recall rate (p=0.52), bad dream recall rate (p=0.013), their combined rates (p=0.049), and nightmares and bad dreams exactly-like or similar to the trauma (p=0.68)] or a per-dream basis (p=0.51, 0.037, 0.016 and 0.51 respectively).Conclusion: These results suggest that whereas overall dream recall rates did not increase with hyperarousal (CHI), those that were nightmares, bad dreams, either of these, or those specifically resembling the trauma, were predicted by CHI. Interestingly, the relationship of general psychopathology (CPI) with such negative dreams was weaker. These results suggest that hyperarousal is a stronger predictor of negative dreams than overall level of psychopathology and, thus, hyperarousal symptoms (Cluster E) may play a role in generating the intrusion symptoms (Cluster B) of PTSD over and above levels of general psychopathology. It is possible, however that the sleep disturbance component of the hyperarousal symptom cluster reflects contribution of nightmares to overall sleep disturbance. Nonetheless, results suggest a close relationship between hyperarousal symptoms and one key aspect of PTSD intrusion symptoms.171209 Moshood Omotayo, MBBS, MPH, PhD, Emergency Analysis of the ethical dimensions of global scale-up of a Ketamine package in resource limited settings in low and middle-income countries (LMICs) M. Omotayo1,2, Burke1,2,7 1Division of Global Health and Human Rights, Department of Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Department of Outcomes Research, Anesthesiology Institute, Celeveland Clinic, Cleveland, OH, USA, 4African Institute for Health Transformation, Sagam Community Hospital, Luanda, Kenya, 5College of Surgery for East, Central, and Southern Africa, Arusha, Tanzania, United Republic of, 6Kenya Obstetrics and Gynaecologic Society, Nairobi, Kenya and 7Harvard T.H. Chan School of Public Health, Boston, MA, USA Introduction: Shortages of anesthetists, anesthesia equipment and anesthetic drugs remains a critical problem in low and middle-income countries. This shortage of anesthesia services often leads to limited or delayed access to critical operative procedures, or the endurance of iatrogenic and acute pain in emergency settings. In December 2013, we deployed the ESM-Ketamine package in support of emergency and essential surgery when no anesthetist is available. The ESM-Ketamine package includes a toolkit and a 5-day training program for non-anesthetist providers. The toolkit is comprised of wall charts, checklists and an ESM-Ketamine kit. Trained non-anesthetist ESM-Ketamine providers included medical officers, nurses, and clinical officers. In this paper, we analyzed the ethical dimensions of scaling up the ESM-Ketamine package across resource-limited settings in LMICs. Methods: Over the past 4 \u00bd years, select non-anesthetist providers were trained in the ESM-Ketamine package. Ethical implications of scaling up the Ketamine package were analyzed using the Beauchamp and Childress framework with a focus on the pharmacological properties and clinical effects of Ketamine, the health system context in resource-limited settings in low and middle-income countries, and the ethical principles of beneficence, non-maleficence, and respect for persons and justice. We analyzed the ethical implications of 4 common scenarios: a) emergent condition where life or limb is at risk and 'standard anesthesia' is unavailable b) conditions where delay will compromise care, and 'standard anesthesia' is not immediately available c) acute painful conditions in which anesthesia would not have been provided by the standard of care in this setting and d) when standard anesthesia is available but patient refused a necessary procedure because they could not afford standard anesthesia, but then consented to proceed with ESM-Ketamine. Results: Strengthening health systems to provide standard anesthetic care in resource-limited settings is the long-term and definitive solution, but an interim measure that can be established over a shorter period is ethically desirable. Extensive clinical and pharmacological data in the anesthesia literature, as well as implementation data of the ESM-Ketamine package, indicate that safety profile for ESM-Ketamine package is satisfactory in this context, despite ketamine having more side-effects relative to standard anesthesia. We argue that the ethical implications of providing the ESM-Ketamine package can be scenario-dependent. In specific situations, particularly when critical and emergency procedures would have been forgone or conducted in painful circumstances, due to health systems constraints prevalent in resource-limited settings, the principles of non-maleficence, justice and respect for persons demand that patients be given the option of anesthesia with Ketamine by a trained ESM-Ketamine provider. Conclusion: Scaling up a safe Ketamine package in resource-limited settings in low and middle-income countries is ethically warranted. Attention should be paid to understanding feasibility and other dimensions of integrating the ESM-Ketamine package into care delivery in resource-limited settings, while health systems are being strengthened to eventually provide universal access to standard anesthetic care. 210 Ricardo Ortiz, BSc, Surgery - Plastic & Reconstructive Migraine Surgery at the Frontal Trigger Site: An Analysis of Intraoperative Anatomy R. Ortiz, L. Gfrerer, M. Hansdorfer, K. Nealon and W.G. Austen Plastic Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Previous studies have suggested that the development of migraine headaches may involve the entrapment of craniofacial peripheral nerves at specific trigger sites. Cadaver studies in the general population have confirmed potential anatomic compression points of the supraorbital (SON) and supratrochlear (STN) nerve at the frontal trigger site, including bony supraorbital foramina and tight fascial bands. However, there are limited reports on anatomic compression points in patients undergoing surgery for headaches. In this study, we aim to describe the anatomy of patients undergoing migraine surgery at the frontal trigger site.172Methods: Subjects scheduled to undergo migraine surgery at the frontal site were enrolled in a prospective fashion. At the time of surgery, the senior author evaluated intraoperative anatomy and notes were made on anatomic variables using an intraoperative anatomy form and detailed operative report. The resulting data was analyzed. Results: A total of 126 sites (65 left and 61 right) in 67 patients were included in the study. The majority of subjects (80%) described pain from both left and right frontal sites. The SON course was through solely a notch in 50% of sites, a foramen in 48% (isolated foramen in 38%, notch plus foramen in 10%), and through neither a notch or foramen (solely a fascial band inferior to the supraorbital rim) in 1.7% of sites (Fig 1). The senior author noted that the SON and STN appeared compressed at 74% and 39% of sites, respectively. Reasons for suspected compression of the SON included a tight foramen in 38%, a notch with a tight band in 28%, a tight foramen plus notch in 11%, a STN and SON emerging via the same notch in 11%, and \"other\" in 12%.Conclusion: This study reveals that the anatomy of the frontal trigger site varies greatly between patients undergoing migraine surgery. We report that the prevalence of foramina at SON sites is 48%, which is greater than any previous cadaver studies of the general population. We also report the most common suspected causes of SON and STN compression in migraine surgery patients, which often include a tight band and tight foramen. Fig 1: SON emergence routes. 211 Asishana Osho, MD, MPH, Surgery - Cardiac Outcomes Are Not Affected by the Laterality of the First Implanted Lung in Bilateral Transplantation A. Osho1, A.L. Axtell1, M.A. Villavicencio1 1Surgery, Massachusetts General Hospital, Boston, MA, USA and 2Pulmonary and Critical Care Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Primary graft dysfunction (PGD) is a significant cause of morbidity and mortality in lung transplantation. During off pump bilateral lung transplantation, the first lung is subject to the full cardiac output resulting in increased edema. Consequently, a larger right lung that is implanted first may be better able to tolerate the full initial cardiac output with less resultant PGD.Methods: Our institutional database was analyzed to compare rates of PGD for patients who had the right versus the left lung implanted first. Additionally, a propensity-matched analysis utilizing the UNOS database was conducted on all adult patients who received a bilateral lung transplant between 2006-2017. Survival was analyzed using a Cox proportional hazards model. Results: In our institutional cohort, 39 patients had the right lung implanted first and 46 had the left lung implanted first. Within the first 72 hours post-transplant, 21 (53%) patients in the left lung first group and 23 (50%) in the right lung first group had any degree of PGD (p=0.72). Of these, only 4 (10%) in the left lung first group and 5 (11%) in the right lung first group had grade 3 PGD (p=0.22). Utilizing UNOS data, a total of 4,912 propensity-matched pairs were identified. There was no significant difference in survival (p=0.86). Conclusion: There are no significant differences in PGD or survival in patients who have the right lung implanted first during off pump bilateral lung transplantation. The side implanted first should be determined by the pre-operative perfusion scan, anatomy, and surgeon preference.173 Figure 1: Overall Post-Transplant Survival Kaplan Meier survival curves after double lung transplantation. Log-rank p=0.86. 212 Juan Pablo Ospina, MD, Neurology Individual Differences in Social Network Size Linked to Nucleus Accumbens and Hippocampal Volumes in Motor Functional Neurological Disorders J. Ospina1,2,3, R. Jalilianhasanpour1,2,3, B. Williams1,2,3, I. Diez 1Department of Neurology, Cognitive Behavioral Neurology Unit, Massachusetts General Hospital, Boston, MA, USA, 2Department of Psychiatry, Neuropsychiatry Unit, Massachusetts General Hospital, Boston, MA, USA and 3Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Boston, MA, USA Introduction: Background: In the biopsychosocial formulation of Functional Neurological Disorders (FND), little is known about relationships between social behavior and other neuropsychiatric factors. Objective: The study investigated how self-reported social network size related to predisposing vulnerabilities, symptom severity, and structural brain profiles in motor FND patients. We hypothesized that reduced social network size would correlate with trauma burden, depression/anxiety, maladaptive personality traits and structural alterations within salience/reward processing brain areas.Methods: In 38 motor FND patients (13 woman, 25 men), we analyzed how Social Network Index (SNI) network size related to demographics, risk factors, and symptom severity. Data from 23 patients with MRIs were used to perform FreeSurfer analyses adjusted for age, gender, and total intracranial volume. Cortical thickness analyses were corrected at the cluster-wise level and subcortical analyses were Bonferroni corrected.Results: In psychometric analyses, social network size positively correlated with physical and mental health, graduating college and working full-time; SNI-social network size inversely related to lifetime trauma burden, PTSD severity, neuroticism and pain catastrophizing. Across motor FND subtypes, patients with Psychogenic Nonepileptic Seizures showed a tendency towards increased social network size compared to other motor FND subtypes. In neuroimaging analyses, a larger social network correlated (p whole-brain-corrected(wbc) =0.045) and hippocampal (pwhole-brain-corrected(wbc) =0.009) volumes; these results remained significant when adjusting for trait anxiety but not for depression. Conclusion: This study suggests that risk factors for the development of FND negatively impact social behaviors, and supports a role for the nucleus accumbens in social behaviors across clinical and non-clinical populations. 213 Lauren Ostrowski, Neurology Dysmature focal white matter microstructure in developmental focal childhood epilepsy L. Ostrowski1,2, D. Song1, E. Thorn1, E. Ross1, S. Stufflebeam5,6 and C. Chu1,6 1Neurology, Massachusetts General Hospital, Boston, MA, USA, 2Neuroscience, Brown University, Providence, RI, USA, 3Mathematics and Statistics, Boston University, Boston, MA, USA, 4Psychological Assessment Center, Massachusetts General Hospital, Boston, MA, USA, 5Martinos Center for Biomedical Imaging, Charlestown, MA, USA and 6Harvard Medical School, Boston, MA, USA Introduction: Benign epilepsy with centrotemporal spikes (BECTS) is a common childhood epilepsy syndrome that predominantly affects boys and is characterized by self-limited focal seizures arising from the perirolandic cortex and fine motor abnormalities. Concurrent with the age-specific presentation of this syndrome, the brain undergoes a 174developmentally-choreographed sequence of white matter microstructural changes, including the maturation of association u-fibers abutting the cortex. These short fibers directly mediate local cortico-cortical communication and provide an age-sen-sitive structural substrate that could support a focal disease process. Methods: We evaluated white matter microstructure in regions corresponding to u-fibers underlying the perirolandic seizure onset zone in children with BECTS compared to healthy control children. To verify the spatial specificity of these features, we characterized global u-fiber and deep white matter properties in these children. We further explored the characteristics of the perirolandic white matter in relation to fine motor performance on the Grooved Pegboard task, gender, and cortical abnormal - ities observed on EEG. Children with BECTS (n=20) and healthy controls (n=14) underwent multimodal testing with high resolution MRI including DTI sequences with 64 diffusion encoding directions, sleep EEG recordings, and fine motor assessment. We compared white matter microstructural characteristics (axial, radial, and mean diffusivity, and fractional anisotropy) between groups in each region.Results: We found distinct abnormalities corresponding to the perirolandic u-fiber regions, with increased axial, radial, and mean diffusivity and fractional anisotropy values respec- tively). Increased fractional anisotropy in this region, consistent with decreased integrity of crossing sensorimotor u-fibers, correlated with inferior fine motor performance (p=0.029). There were gender-specific differences in white matter microstruc-ture in the perirolandic region such that BECTS males and females and healthy males had higher diffusion and fractional anisot- ropy values than healthy females (p 0.035 for all measures), suggesting that typical patterns of white matter development may disproportionately predispose boys to BECTS. There was no relationship between perirolandic white matter microstructure and seizure course or epileptiform burden. There were no differences in diffusivity or fractional anisotropy in u-fiber regions outside of the perirolandic region between groups. Deep white matter showed a diffuse increase in radial diffusivity and decrease in fractional anisotropy in BECTS, consistent with a global delay in white matter maturation (p=0.022 and p=0.027). Conclusion: These data provide evidence that atypical maturation of white matter microstructure is a basic feature in BECTS and may contribute to the clinical comorbidities observed in this disorder. 214 Arinc Ozturk, MD, Radiology Quantification of Pre-Load Force and Shear Wave Elastography in the Thyroid A. Ozturk1, R. Zubajlo2, MGH, Boston, MA, USA, 2Massachusetts Institute of Technology, Cambridge, MA, USA and 3Radiology, Lahey Hospital & Medical Center, Burlington, MA, USA Introduction: Thyroid elastography has shown to have an incremental advantage in the evaluation of thyroid nodules. However, there have been reports to show some variability in its assessment. We hypothesized that this was largely due to inter-observer and intra-observer variability in pre-load forces. The objective of the study was to evaluate whether stiffness elasticity measurements of the normal thyroid, as measured by thyroid shear-wave elastography, varied due to pre-load force.Methods: In this IRB approved, HIPAA compliant prospective study, an in-house developed force-measuring device was used to perform thyroid elastography measurements on healthy volunteers. Elastography measurements were obtained from the right lobe of the thyroid at various force ranging between 2 and 10 Newtons (N). A linear mixed effects model was constructed to quantify the association between pre-load force and stiffness elasticity while accounting for correlations between repeated measurements within each subject. Pre-load force was modeled using both a linear and quadratic term to account the possible non-linear association between these variables.Results: 19 healthy volunteers without any known thyroid disease participated in the study. Subjects were 368 years of age, 74% female, 74% normal BMI, and 95% white 2N pre-load force were 16.7kPA (95%CI: 14.1, 19.3) whereas the values at 10N were 29.9kPa (24.9-34.9).Conclusion: Quantitative standardization of pre-load forces in the evaluation of thyroid nodules using elastography is an integral factor for improving accuracy of thyroid nodules evaluation using this technology. Pre-load force was significantly, and non-linearly, associated with shear-wave elastography estimates of thyroid stiffness. 175215 Gladys N. Pachas, MD, Psychiatry Effect of Type of Tobacco Product on Carbon Monoxide Exposure among people with SMI G.N. Pachas1,2, M. Maravic1,2, and A. Evins1,2 1Psychiatry-Center for Addiction Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3BayCove Human Services, Boston, MA, USA Introduction: Exposure to CO is a well-known risk factor for cardiovascular disease. The detrimental effects of CO are more profound in the myocardium than the periphery and has been implicated in the pathophysiology of atherosclerosis. Tobacco smoke contains many chemicals, including nicotine, tar and carbon monoxide (CO) However, not all tobacco products deliver the same amount of chemicals or have the same cost. Laboratory experiments have shown that the mean deliveries per liter of smoke and tar, nicotine, and carbon monoxide are highest for mini cigars, followed by hand-rolled and manufactured cigarettes. In addition, the average retail price of mini cigars and loose tobacco is lower than that of cigarettes, as are tax rates. The lower cost for mini cigars makes them especially appealing to low income adults. People with serious mental illness (SMI) tend to have low income and are 3-4 times more likely to smoke tobacco, smoke more tobacco products per day and have a more intense smoking topography characteristics extracting more nicotine and CO from each cigarette than non-psychiatric smokers. CDC reported in 2013 that 7% of people with SMI smoked mini cigars and 1.3% smoked loose tobacco. It is not known, however, if those with SMI who use mini-cigars or loose tobacco have higher expired CO concentrations than those who use cigarettes Methods: A representative sample of smokers with SMI in Greater Boston receiving Department of Mental Health contracted psychiatric rehabilitation services provided self-report of their smoking behavior and expired air for CO measurement from November 2016 to September 2017.Results: 1058 of 1166 enrolled participants (90.7%) provided expired breath for CO and were included in the analysis; 108 participants were excluded from the analysis (66 didn't provide a CO measure, 21 had incomplete data, 2 had quit at the time of data collection and 19 used e-cigarettes) Participants reported smoking only cigarettes (62.3%), only mini cigars (11.6%), only loose tobacco (4%), dual use of cigarettes and mini cigars (18.7%) and dual use of cigarettes and loose tobacco (3.4%). Controlling for tobacco products used per day and time since last smoking occasion, participants who smoked only mini cigars had a significantly higher CO level than those who smoked only cigarettes (29.5\u00b119.4 vs 18.4\u00b112.8, =-0.38, p<0.0001) and those who smoked only loose tobacco (29.5\u00b119.4 vs 24.3\u00b118.5, =-0.30, p=0.035). There was no difference between those who smoke only mini cigars and dual users. The number of tobacco products used per day strongly predicted CO readings ( =0.24, SD=0.03, p < 0.0001), and longer time since last smoking was associated with lower expired CO (= -0.43, SD = 0.03, p < 0.0001).Conclusion: Thirty-eight percent of smokers with SMI reported that they smoked either cigars or loose tobacco, which is higher than previously reported. Smokers with SMI who smoked only mini cigars had higher CO levels than those who smoked only cigarettes. Higher CO levels associated with mini cigars potentially increase health risk over and above smoking cigarettes alone, contributing to the higher rates of cardiovascular disease that exits in this population 176216 Luis Paixao, MD, MSc, Neurology Brain Age Neurology, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Pulmonary, Critical Care & Sleep, Department of Medicine, Beth Israel Deaconess Medical Center, Boston, MA, USA Introduction: Healthy aging is one of the most pressing global health targets. Efforts to define mechanisms of aging and to extend healthspan may benefit greatly from development of aging biomarkers. The concept of \"brain age\" as a biomarker has emerged recently, which represents the age predicted by comparing brain measurements from an individual with age norms. Yet, there is a critical unmet need to develop a brain age biomarker that is economical, broadly accessible, and patient friendly. To meet this challenge, we propose to introduce and evaluate the clinical utility of an electroencephalogram (EEG)-based biomarker of brain age. We hypothesize that the brain age index (i.e. EEG-predicted brain age - chronological age) independently predicts mortality. Methods: In preliminary work, we developed a machine learning model to predict brain age based on 510 features per sleep EEG from a database of 3,100 subjects who underwent overnight sleep EEG at the Massachusetts General Hospital sleep lab. About 94% of these subjects were free of major neurological and/or psychiatric conditions. Out of the 3,100 subjects, 2,100 were used in the training set and the remaining 1,000 were used in the testing set. We used this model to determine the brain age of 39 year old adults belonging to a subset of subjects (N=1,736) from the Sleep Heart Health Study with high-quality, paired and complete sleep EEGs. The age predicted by the model is termed brain age. The brain age index (BAI) was obtained by subtracting chronological age from EEG-based brain age. Kaplan-Meier survival curves were used for graphical presentation of time to death, and log-rank statistics were used to assess difference by brain age index levels. Cox proportional hazards survival analysis regression models were used to calculate adjusted hazard ratios for mortality. Results: The average follow-up period was 11.9 years during which 8% (n=137) of subjects died. Mean BAI was signifi- cantly higher in the deceased group compared to the alive group [1.273 (s.d.= 7.024) years vs -0.109 (s.d.= 7.037) years, P<0.05]. A Kaplan-Meier curve showed significantly higher mortality among patients with an old brain (i.e. highest quartile BAI) compared to a young brain (i.e. lowest quartile BAI) (P < 0.01). Having a higher BAI score (i.e. brain age older than one's chronological age) was significantly associated with increased mortality risk, even after adjusting for important covari- ates (including sex, race, smoking status, body mass index, hypertension, diabetes, and education level) (adjusted hazard ratio [aHR], 1.033; 95% confidence interval [CI], 1.006-1.060; p-value < 0.05). Each extra year of BAI (i.e. having an EEG-based brain age - chronological age of +1) yielded a 3.3% relative increase in the risk of death. Other variables significantly associated with mortality included current smoking (aHR, 2.691; 95% CI, 1.626-4.451; p-value CI, p-value < 0.01).Conclusion: Brain age index (BAI), an EEG-based brain age biomarker, is an independent predictor of mortality, with higher BAI (i.e. an older brain) being associated with increased mortality. This work helps to validate BAI as a novel, clinically relevant, non-invasive, low-cost biomarker of a subject's brain health that may provide valuable information about the individual aging process and provide more accurate prediction of functional capability and life span than chronological age alone. Table. Multivariate Cox proportional hazards regression analysis for all-cause mortality Abbreviations: CI, confidence interval; y, years. * Brain age index = EEG-based brain age - Chronological age Education level: 0, < 10 years of education; 1, > 10 years of education 177217 Lauren L. Palazzo, BA, Radiology Outcomes and Cost-effectiveness of Inst for Technology Assessment, Massachusetts General Hospital, Boston, MA, USA, 2Brigham and Women's Hospital, Boston, MA, USA, 3Hospital of the University of Pennsylvania, Philadelphia, PA, USA and 4Harvard Medical School, Boston, MA, USA Introduction: Our purpose was to investigate the patient outcomes and cost-effectiveness of different follow-up interval lengths for Lung-RADS Category 2 subsolid pulmonary nodules. Methods: We developed a Markov model to simulate a cohort of patients with ground glass nodules receiving follow-up CT scans according to the Lung-RADS pulmonary nodule management regimen. At each follow-up scan, each patient may be assigned to wait for a number of months before the next scan or may be sent to treatment for lung cancer, depending on the size and growth of the nodule. Nodule growth parameters, including growth of solid components, were informed by the literature, and data from the National Lung Screening Trial were used in specifying subsolid nodule malignancy incidence and patient characteristics. We varied from one to three the number of years that a patient was assigned to wait after follow-up for Lung-RADS Category 2 nodules, which have a low risk of clinically significant malignancy.Results: We investigated outcomes given one-, two-, and three-year follow-up interval lengths (FIL). The sensitivity and false positive rate for cancer treatment were, respectively, 82.7% and 11.6%, 79.1% and 11.4%, and 70.9% and 10.5%. For the follow-up cohort as a whole, increasing the FIL from one year to two resulted in a decrease of 3.8 days in life expectancy, and increasing from two to three yielded a further decrease of 3.6 days. The amount of radiation exposure decreased from 13.4, to 7.1, to 5.1 mSv. The average cost of follow-up and treatment decreased from $4324.25 to $3339.31 with a two-year FIL, and to $2911.04 with a three-year FIL. Given a willingness-to-pay (WTP) of $50,000 per life-year, a two-year FIL is the most cost-effective option. Conclusion: Extension of the Lung-RADS Category 2 FIL results in decreasing costs and radiation exposure, but also in decreasing life expectancy, treatment sensitivity, and proportion of Stage 1 cancer at treatment. Two-year follow-up of Category 2 nodules was found to be cost-effective while a one-year FIL, as implemented in Lung-RADS, was found not be. 218 Elyse R. Park, Ph.D., MPH, Psychiatry - Benson Henry Institute for Mind Body Medicine A virtual resiliency treatment for parents of children with learning and attentional disabilities (LAD): 1Psychiatry, Massachusetts General Hospital, Benson-Henry Institute for Mind Body Medicine, Boston, MA, USA, 2Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 3Psychiatry, Harvard Medical School, Boston, MA, USA and 4Pediatrics, Massachusetts General Hospital, Boston, MA, USA Introduction: One in five children have a Learning or Attentional Disability (LAD). Parents of children with LAD are vulnerable to high levels of distress. Despite their vulnerability, evidence-based resiliency treatments for parents of children with LAD have not been developed, particularly one using a video conferencing platform. Methods: We adapted a virtual mind-body group intervention, the Stress Management And Resiliency Training- Relaxation Response Resiliency Program (SMART-3RP) for parents of children with LAD. Groups (9 x 1.5-hour sessions) were delivered through videoconferencing. A video conferencing platform offers the opportunity to unite parents across the United States and enables participation because of scheduling flexibility. From 9/16 - 4/17, 32 parents were randomized to an immediate intervention group (plus 2 pilot participants) and 22 to a delayed control group. Surveys were administered at baseline (T1 intervention group; T1 and T2 3 months apart for control group) and post-treatment (T2 for intervention group; T3 for control group). Outcomes included feasibility, acceptability, and efficacy, including distress (analog scale; primary), resiliency (CES), stress coping (MOCS-A), social support (MOS) and mindfulness (CAMS-R) were examined.Results: Fifty-four participated. of 6 sessions. Among 62% of interven - tion participants who completed the T2 survey, and 81% reported practicing relaxation response exercises at least weekly. Intervention participants responded on average 7.1 (1 not at all - 9 very; SD=1.9) on how succussfully the treatment will their stress-related symptoms. Among intervention participants, T1-T2 improvements were reported on distress, resiliency (CES), mindfulness (CAMS-R), and stress coping (MOCS-A) (all ps<.05). T1-T2 comparisons found that intervention vs control 178participants showed significant improvements in primary outcomes: distress (VAS M =-1.48; CI= M =6.52; CI=1.37-11.68; T1-T2 comparisons also found significant outcomes: mindfulness (CAMS-R M =3.24; CI=.77-5.71; (MOCS-A M =7.48; CI=3.45-11.50; p=.001), but not in social support. Maintenance effects were observed in the immediate treatment group from T2 to T3 in resiliency (CES), stress coping (MOCS-A), social support (MOS), and mindfulness (CAMS-R).Conclusion: Pilot trial findings showed promising feasibility, acceptability, and efficacy. The virtual delivery modality facilitates its implementation and dissemination on a national scale. We demonstrated that a virtually-delivered resiliency treatment improved parents' overall levels of distress, stress coping, and resiliency. Post-treatment improvements in psycho- social outcomes were sustained at T3 (6 months post-enrollment). Next steps include modifications to improve the interven - tion and a national implementation trial. Funding: Marino Foundation 219 Kate Park, PhD Candidate, Radiology Structure-Inherent NIR Imaging of Brown Adipose Tissue K. Park, Y. Baek, G.E. Fakhri and H. Choi Radiology, Massachusetts General Hospital, Concord, NH, USA Introduction: It was recently discovered that brown adipose tissue (BAT) persists into adulthood and is responsible for maintaining metabolism, playing a vital role in the propensity to gain weight and develop obesity. Clinical significance of BAT in human health and disease is being unraveled, but the mechanism of transitioning back and forth between brown and white fat remains unclear due to scarce detection methods for the longitudinal monitoring. Engineering contrast agents that specifically target BAT remains a challenging research endeavor due to the unknown targetable molecular receptor. We focused on the development of structure-inherent brown fat targeted near-infrared (NIR) fluorophores by strategically targeting mitochondria that are abundant in BAT cells. After a systematic screening of small NIR molecules, we were able to define a generic pharmacophore that showed appealing BAT localization. The NIR region is unique in that it features minimal human tissue absorbance and fluorescence characteristics\u2014this offers an appealing potential for imaging with low background, reduced tissue attenuation, and high signal given the contrast agent that exists for a particular tissue. Methods: We engineered novel structure-inherent brown fat targeted NIR fluorophore agents that specifically target BAT through systematically screening >300 small molecules that feature a hydrophobic core with a central cation. We were able modify functional groups to define a generic pharmacophore that showed appealing BAT localization.Results: We observed that both the unsubstituted parent and the 5-methyl substituted compound offered an appealing increase in BAT targeting and began modifying the heterocyclic nitrogen alkyl chains while maintaining the active hetero- cyclic units. Elongating the alkyl chain to butyl had the highest BAT specific uptake.Conclusion: We successfully demonstrated long-term and real-time analysis of brown adipose tissue in vivo by utilizing a novel NIR-fluorescent BAT-targeted contrast agent. A BAT-targeted contrast agent could be used for monitoring BAT mass change, BAT activation, and browning of WAT. Since these probes could be easily adapted for labeling with radioactive isotopes, clinical translation of our approach is feasible and will be pursued in the near future. Figure 1. General physiological and cellular diagrams of brown and white fat. (left) Presence and location of brown and white fat at different stages of development and health, (right) cellular differences between brown adipose tissues. Nucleus is represented by purple and fat is represented by white circles. Figure 2. The alkyl-optimized chemical structures, electrostatic potential maps and corresponding imaging characteristics including in vivo BAT targeting and ex vivo resected organs showing overall biodistribution.179220 Hiren Patel, MD, MPH, Emergency Patient characteristics from an emergency care center in rural western Kenya H. Patel1,2, S. Suarez1, J. Owuor1, L. and T.F. Burke1,4,5 1Division of Global Health and Human Rights, Department of Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 2African Institute for Health Transformation, Sagam Community Hospital, Luanda, Kenya, 3Emergency Medicine, Boston Medical Center, Boston, MA, USA, 4Harvard Medical School, Boston, MA, USA and 5Harvard T.H. Chan School of Public Health, Boston, MA, USA Introduction: Emergency care is a neglected area of focus in many low- and middle-income countries. There is a paucity of research on types and frequencies of acute illnesses and injuries in low-resource settings. The primary objective of this study was to describe the demographics and emergency conditions of patients that presented to a new emergency care center (ECC) at Sagam Community Hospital in Luanda, Kenya.Methods: Patient demographics and mode of arrival, chief complaint, triage priority, self-reported HIV status, tests performed, interventions, discharge diagnoses, and dispositions were collected on all patients that presented to the Sagam Community Hospital ECC between October 1, 2016 and September 30, 2017. Results: Over the 12 months, 14,518 patients presented to the ECC. The most common mode of arrival to Sagam Community Hospital was by foot (n=12605, 86.8%). 8,931 (61.5%) were female and 5,571 (38.4%) Of the total visits, 12,668 (87.3%) were triaged Priority III (lowest priority), 1,239 (8.5%) Priority II, and 293 (2.0%) Priority I (highest priority). The most common chief complaints were headache (n=3,923, 15.2%), hotness of body or chills (n=2,877, 8.8%), and cough (n=1,827, 5.5%). The three most common discharge diagnoses were malaria (n=3,692, 18.9%), acute upper respiratory infection (n=1,242, 6.3%), and gastritis/duodenitis (n=1,210, 6.2%).Conclusion: Although opening an ECC in rural Kenya attracted patients in need of care, access was limited primarily to those that could arrive on foot. ECCs in rural sub-Saharan Africa have the potential to provide quality care and support the drive toward attaining the Sustainable Development Goals. 221 Amelia M. Pellegrini, BA, Psychiatry MGH NeuroBank as a Resource for Cellular Modeling of Neurodevelopmental and Neurodegenerative Disorders K.L. Hart1, R.H. Perlis1,2 1Center for Quantitative Health, Massachusetts General Hospital, Boston, MA, USA and 2Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: Emerging protocols for human induced pluripotent stem cell (iPSc)-derived neural lines enable generation of patient-derived cellular models of disease. To facilitate these efforts, we sought to create a tissue repository of patient-derived fibroblasts and induced pluripotent stem cell lines. Methods: We enrolled adults age 18-80 from outpatient clinical settings, including a range of neuropsychiatric disorders as well as healthy control participants. All participants signed written informed consent to study procedures and to allow access to their electronic health records. In addition to a range of self-report questionnaires, participants underwent structured clinical interview as well as neuropsychiatric assessment using the Cambridge Neuropsychological Test Automated Battery (CANTAB). A 3mm dermal biopsy was performed to culture dermal fibroblasts, which were then reprogrammed via modified mRNA to induced pluripotent stem cells. For a subset, stable neural progenitor cell lines were also generated.Results: To date, the NeuroBank includes 410 fibroblasts, 262 induced pluripotent stem cell lines, and 30 neural progenitor cell lines. In an initial analysis, among 309 participants with complete neurocognitive data, 67 had major depressive disorder, 42 had bipolar disorder, 43 had schizophrenia, 25 had schizoaffective disorder, 19 had another neuropsychiatric disorder, and 113 were healthy controls.Conclusion: This repository of patient cell lines linked to electronic health records data, clinician assessment, and self-report measures provides a hospital-wide resource to facilitate investigation of Surgery Preoperative Endoscopic Retrograde Cholangio-Pancreatography (ERCP) is a Risk Factor for Site Infections after Laparoscopic Cholecystectomy T. Peponis1, N. Panda1, P. Fagenholz1 1Surgery, MGH, Boston, MA, USA, 2MGH, Boston, MA, USA, 3University of Miami, Miami, FL, USA and 4Medical College of Wisconsin, Milwaukee, with endoscopic retrograde cholangio-pancreatography increases the risk of surgical site infections after pancreaticoduodenectomy. We hypothesized that preoperative ERCP with sphincterotomy would increase the risk of surgical site infections following laparoscopic cholecystectomy. Methods: Patients admitted to an academic hospital from 2010 to 2016, who were older than 18 and had a laparoscopic or a laparoscopic converted to open cholecystectomy for complicated biliary tract disease (choledocholithiasis, cholangitis, acute cholecystitis, or gallstone pancreatitis) were included. We compared those who had a preoperative ERCP with sphincter - otomy to those who did not. Our primary endpoint was the rate of surgical site infections. Stepwise logistic regression was used to identify independent predictors of surgical site infections. Results: A total of 640 patients were included. Of them, 122 (19.1%) received preoperative ERCP and 518 (80.9%) did not. The ERCP patients were older (median age 59 versus 46, p<0.001) and more frequently males (54.1% versus 36.7%, p<0.001). Their preoperative diagnoses were also different (choledocholithiasis versus 0.4%], p<0.001). The rate of surgical site infections was higher in the preoperative ERCP patients (11.5% versus 4.0%, p=0.005). In a multivariable analysis controlling for diagnosis and various comorbidities, conversion to open (odds ratio=2.57, predictors of surgical site infections. Conclusion: Preoperative ERCP is associated with a threefold increase in the risk of surgical site infections in patients undergoing laparoscopic cholecystectomy. 223 Roy H. Perlis, MD, MSc, Psychiatry Collaboration Acceleration Tool (CAT): Scaling Translational Research across a Health System T.H. McCoy and R.H. Perlis Center for Quantitative Health, Massachusetts General Hospital, Boston, MA, USA Introduction: An increasing number of clinical and translational studies include permission for data sharing, but the process for identifying sharing opportunities remains highly inefficient. We therefore developed a portal that allows researchers to identify overlap between their studies and others with complementary data types.Methods: The portal acts as an 'honest broker' between potential collaborators, allowing investigators to run queries about potential overlap in research cohorts while protecting identified data. Investigators seeking to collaborate can determine, by querying the repository, whether there are other overlapping cohorts, enabling them to approach potential collaborators. Importantly, the repository only holds one unique hashed subject identifier per study participant, along with descriptive information about data available from each study. No additional subject-level data is retained. Since this repository, entirely within the Partners firewall, is only meant to be a resource to identify possible collaborations, IRB review and amendments will typically be required for tissue/data sharing to occur between the PI's. However, by including standard sharing language within their protocols, investigators can substantially increase the potential utility of their own samples, and access to others across the institution.Results: Screenshots illustrate the process of interacting with the portal, using the example of the MGH NeuroBank. There are two means of querying: by lists of medical record numbers, or by comparing studies. In both types of queries, filters may be applied to set the desired data (e.g., longitudinal clinical data; functional MRI) or material type (e.g., DNA; induced pluripotent stem cells). Similar safeguards to the Research Patient Data Registry (RPDR) protect against efforts to reidentify participants.Conclusion: In health systems with clinical and translational studies that allow data sharing, the Collaboration Acceleration Tool can facilitate the process of collaboration at scale.181224 Liz Perzanowski*, MD, Pediatrics Improving the Pediatric Daily Progress Note: A QI Project L. Perzanowski*, J. Kendall*, H. Raab, S. Wainwright and V. Madhavan Division of Pediatric Hospital Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Daily progress notes are an essential form of medical communication and medicolegal documentation. Currently, various resident-generated templates are used to file notes for pediatric patients admitted to the medical teams on Ellison 17 and 18. There has been no systematic assessment of resident-generated progress notes since implementation of the new electronic medical record, EPIC. Notes contain a compilation of information including overnight events, medications, physical examination, laboratory results, imaging results, a brief summary statement identifying the patient and their reason for hospitalization, and a problem-based assessment and plan. Some of these elements are EPIC SmartPhrases linked to data generated in other areas of the chart, and some of these elements are manually entered. Notes are typically written by interns while on rounds. Resident-signed notes are then attested by the attending of record. Considering our own experiences with note writing and in discussions with our fellow residents and attendings, there was a general sense of dissatisfaction with the process of writing daily progress notes and with their quality and utility. In addition, concerns were raised about both excessive data inclusion (aka \"note bloat\") given the ease with which data can be linked to the note and, more troublingly, concerns about the accuracy of notes, a medicolegal risk. Given these issues and lack of standardization we sought to system - atically understand the utility of the current notes as well as the areas where residents felt the notes needed most improvement. Our goal was to create a standardized progress note template that would begin to address these concerns. Methods: We conducted a focus group with members of the second- and third-year resident classes to gather themes to investigate and discuss potential interventions. Informal interviews were conducted with first-year residents on their inpatient pediatrics rotation to elicit their perspective, given their lack of input during the focus group. From these discussions an initial survey was created to assess the overall quality, accuracy, and readability of the current daily progress notes as well as the areas where they needed most improvement, and was distributed by email to residents of all four classes (including medicine-pediatrics residents). Simultaneously, we created a standardized form to evaluate the accuracy of the current daily progress notes, with data gathered via in-person audits of notes and pediatric medical rounds. After this round of baseline data collection, a new progress note template was piloted, with changes targeted to improve note readability, standardization, and to ensure all aspects of a SOAP (Subjective data, Objective data, Assessment, and Plan) note were consistently included. A follow-up survey to understand the impact of the changes on this pilot was sent by email to the residents who used it to complete PDSA (Plan-Do-Study-Act) cycle 1.Results: Thirty-four residents responded to the initial survey (53% response rate). Overall quality was rated as \"Neutral\" by 44% and as \"Poor\" or \"Fair\" by 44%; no respondents rated the notes as \"Excellent\". Accuracy was rated by 97% of respon-dents as being one of the most important factors in the quality rating they gave, followed by readability (59% of respondents). On a scale of 1-5, accuracy was rated at 2.68, with \"time to write notes\" and \"should be easier to fully update the Medications, Assessment, and Plan on rounds\" being the two most important factors, both cited by 74% of respondents. Interestingly 38% of respondents cited \"lack of clarity about the plan on rounds\" as one of the most important factors impacting accuracy. Overall readability was ranked at 2.97 out of 5, with 79% of respondents selecting \"the information in the note could be simplified\". Overall efficiency was rank 2.97 out of 5, with the most important factors being \"the location where informa - tion is documented could be simplified\" at 62% of respondents. Baseline audit data collected on 70 daily progress notes demonstrated that 97% of notes lacked an overall assessment statement of the patient's condition (stable versus improving versus worsening, distinct from the \"Assessment and Plan\" documentation section in EPIC) and 80% lacked a subjective statement from the patient and/or their family, though this was consistently asked during patient interviews on rounds. The second survey was sent to the group of residents who were on the inpatient pediatric medicine rotation during the pilot of the new note template. Five residents responded, indicating overall quality had improved to \"Neutral\" or \"Good\" (4 out of 5 respondents), with accuracy and efficiency for this cohort improving to 3.20 and readability improving to 3.80. Interest - ingly, \"time to write notes\" and \"should be easier to update Medications, Assessment, and Plan on rounds\" were still cited as the most important factors impacting the accuracy of notes, according to 4 out of 5 respondents. An audit of the notes after the template change, which included prompts to residents to document patient/family subjective statements and an overall assessment of the patient's status demonstrated 69% of notes with subjective information and 100% of notes included an overall assessment statement.Conclusion: Efficiently completing accurate and readable medical documentation during an inpatient pediatric medicine rotation is a difficult task, but implementation of and education about a carefully curated template can change practice in this area to improve readability and accuracy while still including required documentation elements. Future PDSA cycles will strive to simplify documentation in the \"Assessment and Plan\" section, and to better understand how attending pediatricians and other providers on the team (such as nurses, pharmacists, nutritionists, etc) use resident-generated notes to complete their own documentation and to provide their recommendations and care.182225 Nicole Polanco, Bacherlor's, Emergency A Comparison of Diversity Between Studies Conducted at Partners Connected Health Innovations N. Polanco1,2, S. Masoom1, O. Dyrmishi1,2, S. Odametey1,2,3, M. S. Abgoola1,2 and J. Kvedar1,2 1Partners Connected Health Innovations, Partners Healthcare, Boston, MA, USA, 2Dermatology, Massachusetts General Hospital, Boston, MA, USA and 3Harvard Medical School, Cambridge, MA, USA Introduction: The importance of diversity in technology based research is underscored by the fact that use feasibility in one population may not be transferrable to another. For this reason, it is imperative that research efforts consider demographic information of participants to truly generalize the results produced by a technological solution. The purpose of this project was to evaluate the diversity among participants in technology-based clinical research at Partners Connected Health (PCH) against studies conducted outside the organization using five categories of demographic data and to examine some possible ways of improving performance in these demographic population measures in research.Methods: We selected five studies from PCH as our sample on the following inclusion criteria: under 300 participants per study, evaluation of a mobile app or medical device as a heath care solution, focus on a condition or health topic unique from the other chosen studies, and completion within the past five years. The outside studies were selected based on the same inclusion criteria and were centered on a range of conditions and topics. This collection of studies yielded a sample of 619 participants for our analysis purposes. Demographics across studies were summarized and compared using a Pearson's chi square test. The analysis also consisted of comparing different recruitment methods used in research done at PCH and those outside of the organization as well. The most popular recruitment methods at PCH were mailing out letters to partici - pants that expressed interest during certain primary care visits, as well as advertising and recruiting from primary care clinics at Partners. The studies selected outside of Partners focused on recruiting participants from treatment clinics and centers, as well as using flyers and brochures to advertise.Results: The PCH study participant pool and outside study participant pool consisted of 616 and 619 participants, respectively. Furthermore, there is no statistically significant difference between the mean age of PCH study participants (51.6 +/- 9.6) and outside study participants (53.6 +/- 11.1). Upon analysis of Race and Ethnicity variables, there is a higher proportion of participants identifying as \"Black or African American\" in outside studies (31.8%) than PCH studies (23.9%), a higher proportion of participants identifying as \"White\" in outside studies (59.5%) than PCH studies (58.0%), and a lower proportion of participants identifying as \"Other\" in outside studies (8.7%) than PCH studies (23.7%). The comparison of Employment rates displays a lower percentage of working participants in outside studies (31.5%) than PCH studies (54.9%) while Education displays a higher proportion of participants with High School or Some College background in outside studies (64.1%) than PCH studies (40.1%). The Pearson's chi squared value was 1.78x10 -8, and thus, the result is not significant at p < 0.05 (p = 1).Conclusion: Although there are various methods to recruit participants for research, diversity in technology based research still lacks inclusion of the various groups of people that represent our population overall. While the available data showed that PCH performance in the demographic categories varies when using outside studies as a standard, the small data set was a limitation in our secondary analysis in this project. The benefit of a larger data set is further analysis to understand the current trends in diversity among research studies and insight into greater inclusivity. Further data collection in research studies may also enable a more holistic definition of diversity in that religion, sexual orientation, and other cultural identity factors are taken into consideration rather than those with strictly biological underpinnings. The benefit of this will be participant pool for research that are a more accurate representation of the general public. 226 Jonah Poster, BA, Surgery - Burn A 54-Year Retrospective Chart Review at a Pediatric Specialty Burn Unit J. Poster1,2 and R. Sheridan1,2 1Burns and Trauma Research, MGH, Newton, MA, USA and 2Shriners Hospital for Children - Boston, Boston, MA, USA Introduction: Mortality rate is a primary quality control metric for specialized burn units. Thanks to improved burn care treatments national pediatric mortality rates have steadily decreased in the last decade. This improvement can be attributed to increased specialized burn units, prompt and precise fluid resuscitation, early excision and improved antibiotic methods. The main objective of this study is to assess changes in frequency, timing, and cause of death after burn injury over 54 years at a specialized pediatric burn unit.183Methods: A retrospective chart review was performed for pediatric patients who died at our institution from 1964 to 2018. Data collection points included: Total Burn Surface Area (TBSA), Full-thickness TBSA, Cause of Burn, Inhalation Injury, Length of Stay (LOS), Days to Death, etc. We grouped patients by their decade of death: 1964-1973, 1974-1983, 1984-1993, 1994-2003, 2004-2013 and 2014-2018. Descriptive statistics (mean, standard deviation) were analyzed for the overall patient population as well as across each of the six decades.Results: A total of 141 patients died at our specialized burn unit since 1964. In each subsequent decade, the number of deaths decreased. The overall descriptive statistics across the entire 1964-2018 time period included an average age of 7.5 \u00b1 5.66, 57% male, an average TBSA of 68.82 29.13%, 54.60% patients an average of 8.37 \u00b1 26.46 days ventilated, an average of 6.09 \u00b1 19.67 days from injury to admission, an average of 17.78 \u00b1 23.45 days from admission to death, an average of 23.43 \u00b1 30.31 days from injury to death date, and an average of 2.98 \u00b1 4.40 number of operations. The most common cause of burn was by fire or flame (124 patients). The second most common cause of burn was by scald (10 patients). Conclusion: Our results show the impact of burn prevention programs and improved burn care methods to minimize burn- related deaths. This is evidenced by the steady decrease in the number of deaths in each decade. Burn prevention programs can further minimize burn-related deaths by focusing on prevention of fire/flame and scald related burns. As survival rates have improved dramatically over the past 54 years, burn care teams should extend their expertise beyond acute wound closure to include reconstruction and rehabilitation methods. 227 Kathleen Powis, Pediatrics Antiretroviral Prescribing Trends in Pregnancy Compared with United States Department of Health and Human Services Perinatal Treatment Guidelines: 2008 - 2017 K. Powis1,2, R. Van Dyke8 and E. Chadwick9 1Pediatric Division of Global Health, Massachusetts General Hospital, Boston, MA, USA, 2Department of Immunology and Infectious Diseases, Harvard T.H. Chan School of Public Health, Boston, MA, USA, 3Center for Biostatistics in AIDS Research, Harvard T.H. Chan School of Public Health, Boston, MA, USA, 4Maternal and Pediatric Infectious Disease Branch, National Institute Of Child Health and Human Development, Rockville, MD, USA, 5Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA, USA, 6Department of Epidemiology, Harvard T.H. Chan School of Public Health, Boston, MA, USA, 7Department of Infectious Diseases, Northwestern University, Feinberg School of Medicine, Chicago, IL, USA, 8Head of Sectioon of Infectious Diseases, Department of Pediatrics, Tulane University School of Medicine, New Orleans, LA, USA and 9Department of Pediatrics, Northwestern University, Feinberg School of Medicine, Chicago, IL, USA Introduction: The United States (US) Department of Health and Human Services (DHHS) issues guidelines for maternal use of antiretrovirals (ARVs) in pregnancy, providing recommendations that balance risk of HIV transmission with pregnancy safety data. Comparing ARV prescribing patterns with DHHS guidelines may inform future prescribing practices and guideline updates. Methods: ARVs prescribed preconception and during pregnancy were abstracted from medical records of women enrolled from January 2008 through June 2017 in the PHACS Surveillance Monitoring for ART Toxicities (SMARTT) study dynamic cohort which enrolls pregnant women living with HIV (WLHIV) and their infants to evaluate ARV safety in the US, including Puerto Rico. Maternal eligibility in this analysis required availability of ARV regimens and dates. We applied an algorithm to classify individual ARVs and regimens based on first regimen prescribed in pregnancy and compared this with DHHS perinatal guidelines issued from 2006-2015. Treatment was categorized according to DHHS guidelines as preferred, alternative, special circumstances, not mentioned, insufficient evidence or not recommended. The percent by category was calculated by timing of maternal ARV initiation.184Results: 1,884 pregnancies from 1,594 women with complete ARV prescribing data were included in this analysis. Of these, 795 (42%) pregnancies involved maternal ARVs from conception, 651 (35%) resumption of ARVs in pregnancy, 421 (22%) ARVs first initiated in pregnancy, and 17 (1%) had no ARV use. A higher percentage of pregnancies during which ARVs were initiated for the first time involved DHHS designated preferred or alternative treatment compared to pregnancies with ARV resumption or use at conception (70% vs 53% vs 35% respectively; p < 0.001) [Figure 1]. More pregnancies with ARV use from conception involved ARVs with insufficient safety evidence compared with pregnancies with ARV resumption or initiation (33% vs 25% vs 14% respectively; p < 0.001). Conclusion: Most women starting ARVs during pregnancy were prescribed preferred or alternative regimens. However, those conceiving on or resuming ARVs in pregnancy were more likely to be prescribed ARVs that deviated from DHHS guidelines, commonly receiving regimens with insufficient safety evidence in pregnancy. Overall, 35% of infants in the cohort had in utero exposure to ARVs with unestablished safety profiles, highlighting the importance of long-term safety monitoring. 228 Patricia L. Pringle, MD, Medicine - Gastroenterology Cirrhotic Patients Require More Fecal Microbiota Capsules to Cure Refractory and Recurrent Clostridium Difficile Infection MGH, Cambridge, MA, USA and 2Infectious Disease, MGH, Boston, MA, USA Introduction: Fecal microbiota transplant (FMT) is an established therapy for recurrent and refractory Clostridium difficile (rCDI), but has not been specifically studied in cirrhosis. Cirrhotic patients, even those without CDI, have intestinal microbial dysbiosis compared to controls. There is minimal evidence that FMT can correct this dysbiosis. Our study evaluated the efficacy of FMT for rCDI in cirrhotic patients. Methods: Patients receiving oral frozen FMT capsules for rCDI were enrolled in a prospective registry from July 2013 to December 2017. Patients with at least three mild-to-moderate episodes of CDI, two episodes requiring hospitalization, or a single episode of refractory CDI were offered FMT. Those with CDI recurrence after FMT were offered another FMT dose, up to three doses per patient. \"Cure\" was defined as no CDI recurrence off CDI antibiotics and survival at 8 weeks. Retrospective chart review identified patients with a clinical diagnosis of cirrhosis. Results: Among 272 patients aged 7 to 99y (median 65y), 259 (95.2%) were treated with FMT for recurrent CDI and 13 (4.8%) for refractory CDI. A total of 225 (82.7%) patients achieved cure with 1 FMT dose, 31 (11.4%) with 2-3 doses, and sixteen (5.9%) patients were not cured by FMT and did not pursue additional doses due to critical illness, death, or preference. A multivariate logistic regression model found that only the presence of cirrhosis predicted requiring 2-3 doses of FMT. When controlling for lactulose use, cirrhosis was associated with requiring 2-3 doses of FMT (OR 18.24; 95% CI 3.18-104.89, P=0.001). Two patients developed bacteremia and none developed bacterial peritonitis after FMT, with no difference between cirrhotic and non-cirrhotic patients (P>0.10).Conclusion: Our findings suggest that cirrhotic patients require more doses of frozen oral FMT to cure rCDI. We did not identify safety concerns in this small cohort of cirrhotic patients.185229 Christiana K. Prucnal, BSN, Emergency Analysis of Partial Thromboplastin Times in Patients with Pulmonary Embolism During the First 48 Hours of 1Department of Emergency Medicine, Center for Vascular Emergencies, Boston, MA, USA, 2Department of Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 3Department of Emergency Medicine, Brigham and Women's Hospital, Boston, MA, USA, 4Department of Emergency Medicine, Harvard Medical School, Boston, MA, USA, 5Department of Medicine, Division of Hematology, Massachusetts General Hospital, Boston, MA, USA, 6Biostatistics Center, Massachusetts General Hospital, Boston, MA, USA, 7Department of Medicine, Harvard Medical School, Boston, MA, USA and 8Emergency Department, Massachusetts General Hospital, Boston, MA, USA Introduction: For patients with pulmonary embolism (PE), unfractionated heparin (UFH) infusion is often used for antico - agulation (AC). However, response to UFH is variable, requires frequent laboratory monitoring and dose adjustment to maintain a therapeutic level of AC. We sought to determine the proportion of PE patients treated with UFH who achieve a therapeutic PTT within 48 hours of UFH initiation. Methods: We analyzed prospectively collected data from our PE Response Team database of patients treated with a standard dose of UFH titrated by aPTT levels. We analyzed all aPTT values available in the electronic medical record, grouping aPTT values into six-hour time windows after the initiation of AC. An aPTT of 60-80 was considered therapeutic. Percentage of patients in the therapeutic range was calculated. Results: 505 patients were analyzed. For patients receiving a bolus and infusion of UFH, the proportion of patients in the therapeutic range was 19.0% at 12h, 26.3% at at 48h. For titrated infusion only, the proportion of patients was 23.3% at 12h, 41.4% at 24h, 37.0% at 36h No patient had all therapeutic aPTT values.Conclusion: The majority of patients with acute PE spend most of their first 48 hours outside of the therapeutic range of AC. Over half of the patients fail to achieve any therapeutic PTT level within 24 hours of UFH initiation and no patient had all therapeutic aPTTs. Future research should focus on identifying factors associated with achieving therapeutic AC with UFH. 186230 Elaina Pullano, B.A., Oral and Maxillofacial Surgery Assessment of changes in medical comorbidities following maxillomandibular advancement surgery in the treatment of obstructive sleep apnea E. Pullano1,2, R. Ngo1, Z. Peacock1, E. Lahey1 and M. August1 1Oral and Maxillofacial Surgery, Massachusetts General Hospital, Brookline, MA, USA and 2Harvard School of Dental Medicine, Boston, MA, USA Introduction: The purpose of this study is to answer the following question: in patients with obstructive sleep apnea (OSA) documented by polysomnography who undergo successful maxillomandibular advancement with genial tubercle advance - ment (MMA/GTA), is there a resultant change in the medical comorbidity profile after a minimum of two years postopera-tively? We hypothesize that improvement in sleep quality will lead to a reduction in medical comorbidities, a decrease in the quantity of prescription medications, and a decrease in average weight and BMI postoperatively.Methods: To answer this clinical question, we completed a retrospective review of all patients with documented OSA treated with MMA/GTA at the Massachusetts General Hospital from January 2000 through December 2015. Patients were identified through the OMFS Patient Data Registry. Inclusion criteria consisted of availability of complete clinical records that included full, updated health history and medication history. Thirty-nine subjects met these criteria. Postoperative data was collected a minimum of 2 years after surgery. A data intake form was used to record specific medical conditions commonly linked to an OSA diagnosis as reported in our previous study. Statistical analysis was performed using 2-tailed paired t-tests for continuous variables and Chi-Square or Fisher's exact tests for categorical variables. Results: We found that the average weight (205\u00b140.5 lbs preoperatively, p=0.04) significantly increased in the subjects postoperatively, which was contrary to our hypothesis. There was no significant change in the number of diagnosed comorbidities (4.6\u00b12.7 preoperatively, atively; p=0.13) postoperatively. There were no significant changes when each sex was analyzed independently. When analyzing individual medical comorbidities, no significant reductions were present.Conclusion: The results of this study lead us to conclude that the pathophysiology of OSA may not be as closely linked with the medical comorbidities described. Postoperative follow-up notes indicated either total resolution of OSA symptoms or mild residual symptoms in all patients. Without concomitant reduction in the comorbidity burden (including systemic hypertension, dyslipidemia, depression, GERD and diabetes), the OSA diagnosis was likely not the main contributing factor to these diagnoses. This is in contradistinction to authors who have reported reduction in both systolic and diastolic values among OSA patients who were compliant with CPAP as well as improvements in lipid profile and diabetes control. A possible confounding factor is the high preoperative BMI values in this cohort. Nearly all of the patients had a BMI> 25, placing them in the overweight or obese category, as defined by the World Health Organization. This did not change postoperatively. Obesity is linked to many of the same comorbidities described for OSA. Our high preoperative and postoperative average BMIs (29.7\u00b15.2, 30.8\u00b15.3 respectively) could account for the lack of improvement in the comorbidity profile. Future studies will focus on stratifying subjects by BMI and noting postoperative changes in comorbidities to try and elucidate the role of obesity in surgically managed OSA and associated conditions. 231 Julia Purks, B.S., Psychiatry Cross-Sectional Baseline Data from the National RLS Opioid Registry: a Longitudinal Observational Study of Long-term Treatment of Restless Legs Syndrome Using Opioids J. Purks1 and J.W. Winkelman1,2 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Restless Legs Syndrome (RLS) is a sensory-motor neurological disorder characterized by an irresistible urge to move the legs that is often paired with leg discomfort, occurring primarily during the evening or nighttime. Clinically significant RLS is present in roughly three percent of the population, and the physical distress and lack of sleep associated with RLS contribute to high levels of morbidity and poor quality of life. Low-dose opioid medications are frequently used in patients who have become refractory to first-line RLS treatments, particularly dopamine agonists. We aim to collect longitudinal data over at least 5 years on the primary outcomes of RLS treatment efficacy, required dosage changes, and tolerability of opioid medications in this population to better inform clinical decisions during this time of increasingly strict opioid regulations.187Methods: Patients currently taking an opioid for diagnosed RLS who had a previous therapeutic response to dopaminergic agonists are included in the registry. Information on initial and current opioid dosages, side effects, concomitant RLS treatments, past RLS treatments, RLS severity, psychiatric history, and opioid abuse risk factors is collected at baseline via a 45-minute phone interview and baseline online survey. Follow-up online surveys are performed every 6 months thereafter. Online surveys are collected, and all data is stored in REDCap. Recruitment is through the RLS Foundation, social media, and individual clinicians treating patients with RLS.Results: Current enrollment is 245 subjects. Participants are primarily white, elderly, educated, and retired. Half of all subjects are on opioid monotherapy, and 25% of subjects are additionally taking an alpha-2-delta ligand (gabapentin or pregabalin). Nearly 50% of subjects are taking methadone, and one-third are taking oxycodone formulations. The most common side effect from opioid treatment is constipation. One-fifth of subjects have been RLS symptom-free in the past week, and 15% in the past month, with most subjects having mild to moderate symptoms. More than half of all subjects have a history of psychiatric illness, predominantly depression and anxiety disorders. Prior to opioid initiation, 37% of subjects had passive, and 20% active, suicidal ideation, both of which are reduced after opioid initiation. Significantly, 70% of this sample is at low risk for opioid abuse (based on the Opioid Risk Tool), and over 70% of subjects are taking 40 mg or less of morphine equivalents per day.Conclusion: Follow-up data on participants will examine RLS treatment efficacy, tolerability, and opioid dosage increases. Associations will be examined between participant characteristics and opioid tolerability, dose changes and discontinuation, and the longitudinal efficacy of opioid treatment in this population. 232 Adam B. Raff, MD, PhD, Dermatology Dual parameter predictive model utilizing skin temperature and diffuse reflectance spectroscopy facilitates the diagnosis of Massachusetts General Hospital, Boston, MA, USA and 2MGH Wellman Center for Photomedicine, Boston, MA, USA Introduction: Cellulitis is frequently misdiagnosed due to its many clinical mimics, collectively known as pseudocellulitis. We investigated the utility of combined diffuse reflectance spectroscopy and thermal imaging of the skin to differentiate cellulitis from pseudocellulitis in a prospective trial. Methods: Adult patients presenting to the emergency department of a large urban hospital with presumed cellulitis were enrolled (n=30). All patients received dermatology consultation. Thermal imaging and diffuse reflectance spectroscopy of both the affected skin area and the corresponding unaffected area of the contralateral side were imaged. Spectra of the affected skin area was assessed by ratiometric analysis of the spectral intensity at 556 nm (deoxyhemoglobin peak) to 542 nm (oxyhemoglobin peak).Results: Of the patients, 30% were diagnosed clinically by a dermatologist with pseudocellulitis. In patients with cellulitis, the mean of this spectral ratio was significantly higher compared to patients with pseudocellulitis (1.0736 [95% CI 1.0536-1.0936] vs 1.0159 [95% Using the spectral ratio alone for cellulitis diagnosis, a logistic regression predictive model demonstrated a classification accuracy of 77%. A dual parameter predictive model combining the spectral ratio with skin temperature difference using linear discriminant analysis demonstrated a 95.2% sensitivity, 77.8% specificity, 90.9% PPV, NPV, and 90.0% accuracy for the diagnosis of cellulitis. The area under the curve for this dual parameter model is 0.97.Conclusion: The results of the current study demonstrate that spectral ratios from diffuse reflectance spectroscopy combined with skin temperature accurately differentiates cellulitis from pseudocellulitis. Spectroscopy and thermal imaging are easy to use, low-cost, and non-invasive, and this dual parameter model may reduce cellulitis misdiagnosis, unnecessary hospitalization, antibiotic overuse, and healthcare costs.188233 Marina Rakhilin, Psychiatry State Population Density as a Predictor of Individual Disability Impairment for People with Mood Disorders M. Rakhilin1, N.E. 2T.H. Chan School of Public Health, Harvard University, Cambridge, MA, USA and 3Harvard Medical School, Cambridge, MA, USA Introduction: As opposed to urban-dwelling individuals, those living in rural or remote areas are more likely to face structural barriers to mental health treatment such as a lack of health care professionals and difficulties traveling between residential areas and treatment facilities. However, prior research on rural mental health outcomes presents mixed results; some researchers have found a higher prevalence of depression in rural areas as compared to urban areas, while others have found no differences. The purpose of this study was to investigate whether population density, aided by additional markers of isolation and limitations to accessing treatment, serves as a predictor of mood symptoms, suicidal ideation, or disability- related impairment in an online group of people with mood disorders.Methods: MoodNetwork is an online platform through the National Patient-Centered Clinical Research Network (PCORnet) designed for patients and families of those with mood disorders. PCORnet is a part of the Patient-Centered Outcomes Research Institute (PCORI) that allows patients to actively participate in every aspect of the research process. Participants complete a waiver of consent and a self-report demographics survey (including a question of how many individuals reside in one's household). Once registered, participants are granted access to a variety of questionnaires, such as the Sheehan Disability Scale (SDS), the Quick Inventory of Depressive Symptomatology (QIDS-SR16), the World Health Organiza - tion-Five Well-Being Index (WHO-5), and the Altman Self-Rated Mania Scale (ASRM). These outcome variables can be measured on an individual level (e.g. participant by participant) or on a state level (e.g. disability scores on the SDS averaged across all inhabitants of a particular U.S. state). For the purposes of this study, state-level data including US state populations, US state population densities, minimum wage values by state, median household income by state, percent of health professional need met by state, and state insurance coverage was collected from the U.S. Census Bureau, the National Conference of State Legislatures, and the Henry J Kaiser Family Foundation. First, the number of household members was examined together with state-level data as predictors of state-level as well as individual-level disability (SDS), depression symptoms (QIDS), reported suicidal ideation (item 12 on the QIDS-SR16), well-being (WHO-5), and of mania symptoms (ASRM). Subsequently, the final model was determined by following stepwise selection; only linear models were considered for selection. A multiple linear regression model including all the covariates of interest was used as the base model, and was pruned down to including population density as the only remaining predictor, since that univariate linear model resulted in the lowest AIC of all the models considered.Results: State-level population density was inversely related with individual-level disability, R 2 = 0.14, F (1, 82) = 0.15, p = 0.0003, but not state-level disability, R2 = -0.013, F (1, 24) = 0.68, p = 0.42. State-level population density was not a signif - icant predictor of depression, suicidal ideation, well-being, or mania (all p values > 0.13). When considering further predictor variables including the number of household members, state population, state minimum wage, state median household income, state percent of health professional need met, and state insurance coverage, none of these variables were signifi- cant predictors of disability, depression, suicidal ideation, well-being, or mania on a state-wide level (all p values > 0.06). Moreover, adjusting for these variables did not increase the amount of variability predicted in disability.Conclusion: Lesser population density predicts greater disability in terms of the extent to which one's work, school, and social life are impaired by their symptoms; however, population density does not significantly predict an individual's symptom severity. Additionally, based on the R 2 value, a minimal amount of variance in individual-level disability is explained by the univariate model including population density alone, therefore there are more factors that need to be accounted for in order to more accurately predict disability level rather than just density alone. Potentially, variables such as access to transportation, education, individual-level income, and mental health stigma may account for additional variability in individuals' experi - ence of disability; however, these variables were not surveyed for amidst this sample. Given the diversity of state population distribution, future research should investigate the impact of population density on a micro level (e.g., by city or county-level) for greater precision and to capture within-state variation. Future analyses should also investigate age, sex, and race/ethnicity as potential moderators in the relationship between population density and disability.189234 Chelsea Rapoport, BA, Psychiatry Communication between palliative care clinicians and patients with metastatic cancer about end-of-life care transitions: a qualitative study of real-time clinical visits C. Rapoport1, E. Wright1, A. El-Jawahri2, J. Temel2 and MGH, MA, and 2Hematology/Oncology, MGH, Boston, MA, USA Introduction: Shared decision-making between patients and clinicians has been posited as a means of facilitating optimal end-of-life care (EOL) decisions. However, few studies have explored how EOL care decisions are made in real-time. This study used audio-recorded visits between palliative care clinicians and patients/families with metastatic cancer, to explore the nature of longitudinal patient-clinician communication about EOL care decisions. Methods: Participants included 30 adults who initiated palliative care within 8 weeks of a new diagnosis of metastatic lung or colorectal cancer, as part of a larger randomized controlled trial of early palliative care integrated with oncology care. Partic - ipants consented to have their palliative care visits audio-recorded. We used a framework approach to analyze all available recorded visits which included reference to EOL care (n=9 participants; 16 recorded visits between study enrollment and end of current study or death). The audio recordings were transcribed and reviewed to develop a structure for coding discussions. Two research team members used this structure to independently code the transcripts. Results were used to identify themes in discussions of EOL care transitions with a focus on how communication may evolve from diagnosis to EOL.Results: Among 9 participants (55.6% male, 77.8% married/partnered, mean age=65.3 years [SD=10.2]) and 16 relevant recorded visits, 7 visits took place 5 months before death, 3 visits between 1 to 3 months before death, and 6 visits within 3 weeks before death. Early discussions set the context for later EOL care decision-making by 1) articulating patients' quality- of-life values and introducing information hospice; 3) exploring prognostic understanding; 4) normalizing and 'testing the waters' for discussing EOL concerns; and in some cases, 4) identifying preferences for future EOL care plans, including use of hospice. Later discussions, commonly triggered by an acute medical event or worsening health status, involved increasingly focused or frank communication, including clinician EOL care recommendations informed by quality- of-life values and goals.Conclusion: In the setting of early initiation of palliative care for patients with metastatic cancer, as patient health status worsens or acutely changes, palliative care clinicians and patients/families may draw upon longitudinal quality-of-life discussions for context as clinicians make increasingly frank EOL care recommendations. Results suggest decision-making pathways that may diverge from a shared-decision making framework. Future studies should continue to explore the extent to which early EOL care discussions influence the nature of EOL care decisions such as hospice enrollment, and the extent to which decision-making characteristics influence patient/family decisional conflict or satisfaction. 235 Revathi Ravi, MD, Emergency Outcomes of newborns on whom an innovative, ultra-low-cost, bubble continuous positive airway pressure (bCPAP) package with a novel blender was utilized in Maharashtra, India R. Ravi1,2,4, V. Dubey3, S. Bhagat3, L. Garg1 and A. Won1 1Emergency, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Pediatrics, Mahatma Gandhi Institute of Medical Sciences, Sevagram, India and 4Harvard T. Chan School of Public Health, Boston, MA, USA Introduction: Acute respiratory failure is a critical feature of prematurity, sepsis and pneumonia, the most common causes of death and disability of newborns worldwide. The majority of these deaths occur in low resource settings. Advanced respira- tory support has been shown to reduce neonatal mortality worldwide but is absent in many regions of low and middle-income countries. Bubble continuous positive airway pressure (bCPAP) is highly effective in addressing newborn respiratory failure, however high cost and the need for uninterrupted electricity and compressed air are barriers to access and scale. This study reports on a novel ultra-low-cost bCPAP package with an innovative blender, called Every Second Matters-Newborn and Infant Respiratory Bundle (ESM-NRB), designed to overcome these barriers. Methods: The authors deployed the ESM-NRB package across four hospitals in Maharashtra, India, in May of 2017. The ESM-NRB package includes a standardized bubble CPAP device, novel ultra-low-cost blender that does not use a motor nor require electricity, hospital grade pulse oximeter, clinical pathway, safety checklist, training manual, and a two-day intensive training program that includes both didactic and practical application for newborns. Newborns that met inclusion criteria of tachypnea for age, signs of respiratory distress, and were initiated on fluids or antibiotics were enrolled through convenience sampling, dependent whether the care provider felt comfortable invoking an ESM-NRB device and the 190newborn had no cardiac, gastrointestinal, anatomical, respiratory or neurological contraindications. Each time the ESM-NRB package was deployed detailed data was obtained including maternal and newborn demographics, health status, outcomes, co-interventions, adverse events, severe adverse events, vital signs, continuous pulse oximetry, and hourly respiratory severity scores (RSS). Survival and changes in vital signs and RSS were analyzed. Adverse events were defined as nasal septum erosion, pneumothorax, trauma, nasal obstruction, gastric distention, vomiting or aspiration associated with the bCPAP device. Severe adverse events were defined as death, hematemesis, epistaxis, pneumomediastinum, emphysema, or tympanic membrane rupture bCPAP device. Results: 8 Pediatricians and neonatologists, 40 nurses, and 12 post-graduate residents from four hospitals in Maharashtra, India completed the two-day intensive training on ESM-NRB. From May 26, 2017 to July 26, 2018, 92 newborns were placed on the ESM-NRB bCPAP device for impending respiratory failure. 49 (53.3%) of the 92 were premature and 77 (83.7%) survived to discharge. Diagnoses included nonspecific respiratory distress (40.2%), neonatal sepsis (19.6%), transient tachypnea of (2.6%) and hypoglycemia (1.3%). Among 50 (54%) newborns with initial RSS between 4 and 8 and who were on an ESM-NRB device at least 6 hours, the RSS decreased on average by 1.35 [95% CI 0.91, 1.80; p-value 0.000000218] at the 6-hour mark. Among 42 (45.7%) newborns with initial RSS between 4 and 8 and who were on an ESM-NIRB device at least 12 hours, the RSS decreased on average by 1.78 [95% CI 1.10, 2.46; p-value 0.0000054] at the 12-hour mark. Respiratory rates decreased on average by 8.02 breaths per minute [95% CI 2.43, 8.18; p-value 0.00000004] after 6 hours of treatment and by 10.18 breaths per minute [95% CI 6.99, 13.37; p-value 0.00000013]. One newborn with meconium aspiration syndrome developed a pneumothorax while on the ESM-NRB device and remained on the bCPAP device and was weaned to room air without further complications and resolution of the pneumothorax. No deaths or severe adverse events attributed to an ESM-NRB device have been reported. Conclusion: A promising ultra-low cost bCPAP package that does not require uninterrupted electricity or pressurized air was implemented successfully across four hospitals in India. Further research will examine opportunities to improve training, the device, and provider performance. A non-inferiority trial with a gold standard CPAP device will help identify ESM-NIRBs appropriate position within the health system. 236 Regina Roberg, Bachelors of Arts, Psychiatry The Psychophysiology of Cognitive Control in Emotional Regulation: The Preliminary Relationship Between Depression Severity and Heart Rate Change R. Roberg and B.G. Shapero Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: Depression is a disorder of impaired emotion regulation (Joormann & Gotlib, 2009), in which individuals have difficulty inhibiting cognitive processing associated with negative stimuli. However, emotions are a multifaceted processes involving changes in peripheral and central physiology (Thayer & Siegle, 2002), behavior, and cognitive processing often in response to one's environmental context. Emotion regulation largely depends on one's ability to modify physiological arousal in accordance with situational demands (Gross, 1998). One such measure of emotion regulation is with heart rate variability as an increased heart rate is characteristic of one's state of arousal (Appelhans & Luecken, 2006). According to Beck (1997), depressed individuals possess depressive schemas and dysfunctional beliefs that contribute to depressive thinking, which can then activate syndromal depression. Though a few studies have broadly examined the psychophysiological characteristics of emotion regulation, no research has been conducted that specifically examines the psychophysiology of biased cognitions. We hypothesized that individuals with higher levels of depressive symptoms and poorer cognitive control would have a greater difficulty regulating their emotional response to negative stimuli. Methods: Participants included 16 young adults ages 18-30 (M age = 19.9; 87.5% Female; 68.8% Caucasian) with at least mild depressive symptoms (Beck Depression Inventory [BDI>13]). The majority of the sample met diagnostic criteria for Major 191Depressive Disorder (87.5% met criteria for Major Depressive Disorder, and 62.5% met diagnostic criteria for an anxiety disorder [i.e., Generalized Anxiety disorder, Social Anxiety disorder, Panic disorder]). In addition, the sample had generally high severity of depressive symptoms (BDI; M =29.63, S.D.=9.09). Individuals with a history of psychosis or psychotic disorder, mania or hypomania, Autism, a substance use disorder in the past 6 months, or were at acute suicidal or homicidal risk were excluded. At baseline, participants were consented and completed a battery of self-report measures including the BDI and the Difficulties in Emotion Regulation Scale (DERS; Gratz & Roemer, 2004). In addition, all participants completed several behavioral computer tasks, including the Emotional Stroop Task (EST; Williams, Mathews, & MacLeod, 1996) which is used to examine response times to emotional words. For this computer task, participants were presented with words from 4 different categories (neutral, positive, negative, and colors) chosen from previous studies examining cognition and emotion. The latencies from stimulus presentation to the participants' color-naming responses are recorded for each category with longer latencies indicating a greater difficulty inhibiting response to that type of stimulus. Participants also completed a computerized emotion regulation (ER) task assessing cognitive reappraisal (Ochsner et al., 2002). For this task, participants were presented with aversive and neutral images (OASIS: Kurdi et al., 2017) and instructed to maintain (continue to feel the same emotion) versus regulate (reduce the emotions they are feeling). During this task, participant's responses to the trial were measured by their response (e.g., visual analogue scale ratings of emotions to the emotion regulation task). In addition, individuals' psychophysiological responses were measured using a Biopac during this task that measured Heart Rate Variability (HRT). We conducted a bivariate correlation analysis to assess the associations between BDI scores and performance on the cognitive control task with Heart Rate Variability (HRT) in response to emotional versus neutral images.Results: Bivariate correlation analysis was conducted to assess the associations between depressive symptoms and cognitive control to negative emotional stimuli with Heart Rate Variability (HRT) in response to emotional versus neutral images. The results showed that high depressive symptoms were significantly associated with a lower capacity to regulate heart rate (r = -0.56, p= 0.029). Similarly, biased cognitions towards negative emotional words were significantly associated with a lower capacity to regulate heart rate (r = -0.68, p = 0.005). However, higher levels of difficulty in emotion regulation was not significantly associated with a lower capacity to regulate heart rate (r= -0.29, p= 0.292). Conclusion: Previous research suggests that depressed individuals have difficulty regulating their emotions when exposed to negative information. This in turn has been associated with difficulties in regulating physiological arousal from moment to moment. Our results suggest that individuals with higher levels of depressive symptoms and poorer cognitive control have difficulty regulating their physiological responses to emotional stimuli. Although these results are promising, given this pilot study's small sample size, future studies with a larger study population should be conducted to assess the replicability of these preliminary findings. In addition, comparisons between a depressed and non-depressed sample will be important to make firm conclusions. However, based on these findings we can speculate that researchers and clinicians may want to take a nuanced approach to understanding the importance of cognition in psychopathology and consider integrating mind-body therapies that address difficulties in psychophysiological regulation. 237 Michelle R. Roberts, PhD, Dermatology Genetic contribution to population attributable risk in basal cell General Hospital, Boston, MA, USA, 2Population Medicine, Harvard Pilgrim Healthcare Institute, Boston, MA, USA and 3Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA, USA Introduction: Basal cell carcinoma (BCC) is the most common cancer in the United States, and though typically non-lethal, represents a significant public health burden in terms of healthcare costs and associated morbidities. A number of genome-wide association studies (GWAS) have investigated the relationship between inherited genetic variation and risk of BCC. To quantify the contribution of genetic variability on BCC risk, we used published GWAS findings to calculate the population attributable risk (PAR).Methods: We abstracted data from published, two-stage GWAS of BCC risk. Single nucleotide polymorphisms (SNPs) were included in the calculation if they were replicated in an independent population at p < 0.05 and reached genome-wide significance (p < 5 x 10-8) in meta-analysis. For variants appearing in multiple studies and those in linkage disequilibrium (r 2 > 0.3), we included data from the larger study population. We identified 29 SNPs meeting those criteria. We calculated the multi-locus PAR using the additive odds ratio (OR) and minor allele frequency (MAF) for each SNP. For variants with a protective effect (OR < 1), we took the inverse OR and used the major allele frequency, so that the association was in the same direction for all SNPs. We also estimated the population distribution of the relative risk due to known risk alleles for males and females, using a published sex-specific estimate of BCC incidence.192Results: Using 29 SNPs, we estimated a PAR of 0.82, indicating that if the effects of all risk variants were removed, the incidence of BCC would be reduced by 82%. We found that the relative risk of BCC increases with higher percentiles of the polygenic risk score, and that males have a higher relative risk than females. For males, the risk of BCC is twice the average population risk at the 76 th percentile, while for females, this occurs at the 83rd percentile. These results are similar to recently published distributions for cutaneous squamous cell carcinoma. Conclusion: Our estimates indicate that there is a significant impact of genetic variation on the risk of developing BCC, and that this impact may be greater for men than for women. Polygenic risk scores may be clinically useful tools for risk stratification, particularly in combination with other known risk factors for BCC development. 238 Alexander P. Rockhill, BS, Psychiatry Visualization of Biomarkers of Aversion Reward Conflict in EEG A.P. Rockhill, E.M. Hahn, D.D. Dougherty and T. Deckersbach Psychiatry, Massachusetts General Hospital, Charlestown, WA, USA Introduction: Mediation of the conflict between aversion and reward is critical to social function, and logically is involved in the pathology of many psychiatric illnesses. In our aversion-reward conflict (ARC) task, participants were given a choice of a larger reward for a variable chance of being shocked or a safe option for a smaller reward. We observed several potential EEG biomarkers of this cognitive process. The location of these biomarkers is comparable to our previously published findings using fMRI. These types of examinations of biomarkers can help clinicians diagnose psychiatric patients more comprehen - sively by allowing them to quantify the function of their patients' cognitive processes on multiple scales instead of having to categorize them, during which in some ways dissimilar patients are given the same psychiatric diagnosis.Methods: Ten participants were given the Mini International Neuropsychiatric Interview to confirm their eligibility as a healthy control. The participants were then fitted with a high-resolution, 96 channel EEG cap with active electrodes. These participants were then asked to turn up a shock delivery on their ankle until it was highly annoying but not painful. Then, they were given instructions and practice trials of the task; in each trial participants were presented with two choices, a safe option and a risky option. The safe option was always $5 and did not involve any chance of shock. The risky option was between $5 and $15 exclusive of the end values and had a chance of shock between 10% and 90% as indicated in a bar at the center of the screen. Finally, subject performed 243 trials of the task in three blocks (approximately ten minutes each) with as much rest as they chose in between the blocks. Subjects were told two of the trials would be selected at random to add to their payment. Decision conflict was modeled as by modulating a logistic regression of choice types (safe or risky) by normalized response times. Response times were fit to a gamma distribution and normalized as the cumulative distribution function of the gamma function at that response time. Behavioral data was then binned into tertiles of decision conflict. EEG data preprocessing and analysis was done using MNE. The EEG data was first high-pass filtered at 0.1 Hz. Then, indepen - dent components analysis was computed and components were selected to subtract based on their correlation with eyeblink and heartbeat epochs. Finally, epoched and cleaned with autoreject with a final quality check and removal of bad channels at the end. The participants had structural MRI scans that were used to transform the data to source space. Time-frequency decomposition was done using the Morlet wavelet method. Results: Peak EEG activity was observed in preliminary analysis between 0.5 and 1.0 second post-response. This is the same time period that participants would be shocked if they chose a risky option and the random chance occurred that they were to be shocked according to the probability of risk on any given trial. During trials with greater conflict, an increase in activity was observed in the left dorso-medial and dorso-lateral prefrontal cortex (dm/dlPFC) and the thalamus and cingulate.Conclusion: The areas observed to be activated in our results are consistent with previous literature. The involvement of executive function areas such as the dm- and dlPFC are consistent with the necessary areas of recruitment for choosing a difficult choice. The cingulate and thalamus are involved in alertness and attention and emotion, which is likely to accompany a decision where the outcome is being monitored closely so that the participant can learn how get more money by taking fewer shocks. Attaching emotional markers to certain values has been shown to be our method for expediating this process. Having higher resolution, more accurate biomarkers of cognitive processes relevant to psychiatric illness, can improve neurothera - peutic treatment such as providing better accuracy for Deep Brain Stimulation (DBS) targeting.193 239 Clifton Rodrigues, MBBS, Surgery - General and Gastrointestinal Colloid vs tubular carcinoma arising from IPMN: the better survival is independent of stage C. Rodrigues1, V. Morales-Oyarvide1, M. Massachusetts General Hospital, Boston, MA, USA and 2Pathology, Massachusetts General Hospital, Boston, MA, USA Introduction: Invasive intraductal papillary mucinous neoplasia (IPMN) of the pancreas is a heterogenous entity with tubular and colloid carcinoma being the two most common subtypes. Colloid carcinoma is thought to have a better survival compared to the tubular variant following resection although it remains unclear if this is due to presentation at a different tumor stage. Our aim was to compare the clinicopathological characteristics and survival after adjusting for stage between the colloid and tubular subgroups.Methods: From a prospective database of resected IPMNs, all cases with a final pathological diagnosis of an invasive IPMN with colloid or tubular histology were analyzed. We compared the clinicopathological features and evaluated progression-free survival (PFS) and overall survival (OS) using multivariate Cox regression adjusting for age, sex, pT and pN stage, tumor grade, resection margins and adjuvant therapy.Results: In total 100 patients were identified of which 36 were colloid carcinoma and 64 were tubular carcinoma. There were no differences in clinicopathological features (including TNM-stage and adjuvant therapy received) between the two groups except for significantly higher perineural (p=0.042) and vascular invasion (p=0.005) in the tubular subgroup. Median PFS for the colloid subgroup was 108.4 months vs 20.8 months in the tubular subgroup with 5-year PFS of 62.9% vs 35.6%, respectively (p=0.013). Median OS was 155.7 months in the colloid subgroup vs 35.9 months in the tubular subgroup with 5-year OS of 67.3% vs 45.8%, respectively (p=0.009). In multivariate analyses, colloid carcinoma was associated significantly longer PFS (HR 0.39, 95% CI 0.20-0.77, p=0.007) and OS (HR 0.43, 95% 0.21-0.87, p=0.020) compared with the tubular subgroup. Conclusion: Colloid carcinoma arising from IPMN has a markedly better progression-free and overall survival compared to tubular carcinoma. This is independent of tumor stage, and suggests a different underlying tumor biology between these cancer subtypes.194Clinicopathologic characteristics SD =Standard Deviation *chemotherapy and/or radiation therapy 240 Axana Rodriguez-Torres, MPH, Neurology Stroke Recurrence Risk among 80-year-old and Older Survivors of Hemorrhage Biffi2,3,4 1School of Medicine, University of California, Irvine, Irvine, CA, USA, 2Neurology, Massachusetts General Hospital, Boston, MA, USA, 3Center for Genomic Medicine, Massachusetts General Hospital, Boston, MA, USA and 4Hemorrhagic Stroke Research Program, J. Philip Kistler Stroke Research Center, Massachusetts General Hospital, Boston, MA, USA Introduction: Intracerebral Hemorrhage (ICH) is the most severe form of stroke, accounting for almost half of all stroke- related disability and mortality. Existing studies of ICH focus on outcomes among survivors age 65-80 years; survivors aged 80 years and older (i.e. oldest old) have been minimally represented in prior studies of ICH. Published evidence suggests that older ICH survivors are more likely to present with lobar ICH (associated with Cerebral Amyloid Angiopathy [CAA]), rather than with non-lobar ICH (associated with hypertensive arteriolosclerosis). As life expectancy continues to rise, this age group is likely to account for an ever-increasing proportion of ICH survivors. It is therefore imperative to collect relevant outcome information in this specific ICH survivor population, and explore potential factors contributing to outcomes in the oldest of old survivors. Disparities in secondary ICH prevention among oldest old survivors were the topic of limited investigation. Blood pressure (BP) control in particular, represents the cornerstone of secondary ICH prevention. We hypothesized that: 1) ICH survivors 80-year-old and older are at higher risk of ICH recurrence when compared to their younger counterparts, and 2) after accounting for patient- and stroke-specific characteristics, higher stroke recurrence rates among for 80-year-old and older ICH survivors are associated with higher BP. Methods: We analyzed data from survivors of primary ICH, enrolled in a single-center prospective longitudinal study at Massachusetts General Hospital (MGH). Exposures of interest include patient's characteristics (especially age < 80 years old vs. 80 years old), ICH characteristics (size, location, clinical severity at presentation), BP measurements after first ICH and medications exposures during follow-up. We captured as outcomes of interest ICH recurrence and ischemic stroke occurrence during follow-up. We performed: 1) a case-control study comparing primary ICH survivors age 80 years vs. < 80 years old (the latter group further subdivided as follows: age 55, age 56-74, age 75-79); 2) a longitudinal study ascertaining whether ICH survivors age 80 years are at higher risk for ICH recurrence, and whether such a disparity is accounted for BP measurements. Results: We followed 1464 ICH survivors (441 80 years of age and 1023 <80). In univariable analyses, ICH survivors age 80 years were more likely to be of white non-hispanic background, female gender, have lower educational level, suffer from hypertension, coronary artery disease, dementia and to require functional assistance (all p<0.05). Oldest old ICH survivors were also more likely to suffer a lobar ICH, and had lower hematoma size at presentation (both p<0.05). In multivariable analyses, lobar ICH location, white non-Hispanic background and female gender were found to be independently associated with ICH presentation at age 80 years (all p<0.05). In separate sub-group analyses for lobar and non-lobar ICH white 195non-Hispanic background and female gender remained the only factors associated with age 80 years at presentation. ICH survivors age 80 years had higher systolic BP during follow-up ( 80 years: median years: median 138, IQR 132-145, p<0.001). We present data for frequency of hypertension stages during follow-up by age category and stroke recurrence status in Figure 1. In spite of this, they were less likely to receive anti-hypertensive treatment Participants age 80 years were at higher risk for stroke recurrence (yearly rate 5.18%) compared to those age < 80 years (yearly rate 3.22%, p<0.001 for comparison). Of note, we found that among ICH survivors age 80 years those without hypertension during follow-up were not at increase stroke recurrence risk (Hazard Ratio [HR] 1.10, 95% Confidence Interval [CI] 0.68- 1.77), but those with hypertension were (HR 1.85, 95% CI 1.20-2.85).Conclusion: ICH survivors 80-year-old and older are more likely to be female, of white non-Hispanic background and presenting with lobar ICH. We found them to be at higher risk for stroke recurrence when compared to their younger counter-parts. After accounting for patient- and stroke-specific characteristics, higher stroke recurrence among 80-year-old and older survivors is accounted for by inadequate BP control. 241 Lauren Rubin, MD, Pediatrics Decreasing Emergency Department Length of Stay in a Pediatric Academic Medical Center by Expediting Provider Handoff L. Rubin1, J. Kendall1, A. Tsurutis1, Y. MGH, Boston, MA, USA, 2Practice Improvement Division, MGH, Boston, MA, USA, 3Program Coordinator, MassGeneral Hospital for Children, Boston, MA, USA and 4Quality and Safety Manager, MassGeneral Hospital for Children, Boston, MA, USA Introduction: Extended stays in the Emergency Department negatively impact patient experience and quality of care. Consequently, reducing Emergency Department length of stay (ED LOS) for admitted patients has become a hospital- wide multidisciplinary initiative. Several factors contribute to extended ED LOS including time to inpatient bed request, communication between the ED and floor teams, bed availability, and physical transport from the ED to the floor. In 2016, 75% of admitted pediatric patients at MGHfC were \"boarders\" defined as spending > 2 hours waiting for a bed. As part of the pediatric ED LOS initiative, multidisciplinary working groups were formed to address LOS from multiple angles. To address ED LOS from an inpatient perspective, time from when inpatient bed was ready to MD handoff was identified as a significant source of delay. Methods: A focus group of pediatric admitting residents identified barriers to MD handoff within 30 minutes. Based on the barriers identified, a new admission workflow was designed and implemented to prioritize and expedite MD handoff. Notable interventions included a novel paging notification system to alert both ED and inpatient admitting residents when the patient's bed was assigned, and a new admission workflow to prioritize receiving MD handoff above other inpatient tasks.) Primary outcomes included percentage of MD handoffs within 30 minutes of patient bed ready and median ED LOS time, for which monthly data was collected. Balance measures included safety report filings regarding transfers from the ED and transfers to the ICU from the inpatient floor within 24 hours of admission. Following the initial intervention, admitting residents were surveyed to identify additional barriers to accepting MD handoff within 30 minutes of bed assigned. Baseline median, percentages, and variability were determined over time through statistical process control methods. Significant differences post-implementation were determined by healthcare rules of interpretation.196Results: In the post-intervention phase, percentage of MD handoffs within 30 minutes increased from 47% to 64% (Figure 1). Simultaneously, median LOS decreased by 29 minutes or 9% (Figure 2). Surveyed admitting residents reported acute care needs of admitted patients, inability to quickly find co-admitting intern, and handoff request during morning or evening sign out as major barriers to MD pass off within 30 minutes of bed ready. Conclusion: The initiation of a new admission workflow, which included a novel paging notification system to alert both ED and inpatient admitting residents when the patient's bed was assigned was associated with a moderate increase in percentage of MD handoffs within 30 minutes (Figure 1) and continued improvement in median LOS even during winter months when inpatient capacity is highest. The post-intervention survey highlights the need for further interventions to expedite MD handoff. While median ED LOS showed improvement, this change was modest, indicating that there are likely multiple remaining barriers to decreasing Emergency Department length of stay. Additional interventions planned for the 2018-2019 academic year include increasing pre-noon discharges to create greater bed availability. Provider and family surveys have identified consultant recommendations, transportation, and prescription availability and teaching as modifiable factors which delay pre-noon discharges. A multi-disciplinary working group has been established to expedite these aspects of the discharge process. Figure 1. Percentage of MD Handoffs Within 30 Minutes of \"Bed Ready\" Figure 2. Median Length of ED Stay for Admitted Pediatric Patients 242 Claire C. Rushin, Medicine - Endocrine The Long-term Effects of Gastric Bypass on Skeletal Health Rushin1, Cambridge, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Roux-en-Y gastric bypass (RYGB) is associated with large magnitude bone loss and increased risk of fracture within the first five years after surgery; however, the long-term effects of RYGB on bone health are not well characterized beyond five years. Our study evaluated bone mineral density (BMD) in adults who underwent RYGB at least 10 years ago compared to weight-matched controls. Methods: We evaluated 17 adults who had RYGB at least 10 years ago and 17 controls matched for age, sex, race/ethnicity, and current BMI. This ongoing study anticipates a final cohort of 25 RYGB subjects with matched controls. Body composi-tion and areal BMD (aBMD) were assessed by dual energy x-ray absorptiometry (DXA) at the total hip, femoral neck, lumbar spine, and 1/3 radius. Serum calcium, vitamin D, PTH, and fasting glucose were also measured. In addition, we assessed calcium and vitamin D intake using the Food Frequency Questionnaire (FFQ) and activity levels using the Modified Activity Questionnaire (MAQ). Independent t-tests were used to compare outcomes in the surgical and control groups.Results: The RYGB and control groups were well matched for age (58 \u00b1 9 vs. 57 \u00b1 9), BMI (32 \u00b1 5 vs. 32 \u00b1 4), sex (88% female in each group), and race (13 white/non-Hispanic, 4 black/African American in each group). Average time since surgery in the RYGB group was 13.7 \u00b1 2.3 years. In preliminary analyses, patients who received RYGB had a 19.2% lower aBMD at the femoral neck (p=0.003) and 16.3% lower aBMD at the total hip (p=0.005) than controls. There were no significant differences in aBMD between the two groups at the lumbar spine or 1/3 radius. Serum 25-OH vitamin D (28 \u00b1 12 vs. 31 \u00b1 9 ng/mL; p=0.60) and PTH (73 \u00b1 42 vs. 56 \u00b1 27 pg/mL; p=0.20) were similar in the RYGB and control groups, respectively. We found no significant difference in calcium or fasting glucose levels, nor did we find differences in exercise frequency. There were no significant differences in dietary calcium or vitamin D intake. In the RYGB group, 58.8% of the subjects were regularly taking calcium supplements and 64.7% were regularly taking vitamin D supplements compared to 35.3% and 47.1% in the control group, although these differences were not statistically significant. Conclusion: This preliminary analysis suggests that individuals who have undergone RYGB more than 10 years ago have decreased aBMD at the femoral neck and total hip compared to matched non-surgical controls. These data suggest that 197negative changes in bone health after RYGB may be long lasting and long-term follow-up of skeletal health after RYGB is warranted. Additionally, post-surgical adherence to recommended calcium and vitamin D supplementation is suboptimal after RYGB. Figure 1: Femoral neck and total hip aBMD in RYGB and control groups. 243 Andrew W. Russo, Neurology Connectivity Driven Atrophy in Multiple Sclerosis A.W. Russo, K. Patel, S. Tobyne, N. Machado and E. Klawiter Neurology, Massachusetts General Hospital, Charlestown, MA, USA Introduction: Grey matter atrophy has been shown to have clinical significance in multiple sclerosis (MS). There is evidence throughout neurodegenerative disorders that atrophy can spread through anatomical pathways or target functional networks. Prior work from our lab has shown atrophied regions are disproportionately interconnected in MS. Here we investigated the influence of structural and functional connectivity on regional atrophy in MS. Methods: We collected diffusion and resting state data from 64 MS and 50 healthy control subjects on a CONNECTOM 3T scanner with maximal gradient strength of 300 mT/m. In addition, 25 MS subjects underwent a one-year follow-up scan. The cortical surface of each subject was parcellated into 294 random regions of equal size. The atrophy of each region was expressed as a residual, using an age and gender controlled general linear model derived from the healthy control dataset. The structural and functional connectomes applied to the MS group were constructed using the healthy control data. A region's atrophy exposure was defined as the sum of atrophied regions connected to that region. The atrophy of secondary and tertiary connections was weighted to reflect connectivity distance.Results: In the MS group brain, regional atrophy significantly correlated with structural atrophy exposure (r = 0.36, p = 2.88x10 -10). This correlation was improved by including secondary and tertiary connections into the definition of atrophy exposure (r = 0.39, p = 3.62x10-12). Atrophy exposure from functional connections had a small but significant correlation with regional atrophy (r = 0.15, p = .001). Over a one-year period, increased regional atrophy correlated with increased structural (r = 0.49, p=3.53x10 Conclusion: Cortical atrophy in MS is shaped by connectivity patterns. While many factors may contribute to cortical atrophy, we show the atrophy of a region is influenced by the atrophy of regions directly connected to it, as well as more global, indirect connections. This effect is stronger for structural connections than functional ones. In addition, the cortical regions most likely to increase atrophy over time are not those with the highest baseline atrophy or atrophy exposure. Increased regional atrophy is best predicted by increased atrophy exposure. While MS cortical atrophy appears diffuse and varied across subjects, connectivity can help explain its underlying pattern and potentially predict its progression. 244 Hatice D. Saatcioglu, PhD, Surgery - Pediatric Neonatal inhibition of a mesenchymal progenitor cell by MIS/AMH disrupts development and results infertility H.D. Saatcioglu1, Kano1, L. Zhang1, P. Donahoe1 and D. Pepin1 1Pediatric Surgery, Mass General Hospital, Cambridge, MA, USA, 2Obstetrics and Gynecology, Mass General Hospital, Boston, MA, USA and 3Gene Therapy Center, U Mass Medical Center, Boston, MA, USA Introduction: During the development of the male urogenital ridge, secretion of Mullerian Inhibiting Substance (MIS) by the fetal testis stimulates a specific layer, Mullerian Inhibiting Substance Receptor-2 (Misr2) positive subluminal cells, to direct the regression of the Mullerian duct. However, the developmental fate of these same Misr2+ cells during uterus development 198remains unknown. In this study, we sought to map out the fate of the female Mullerian duct during postnatal uterine develop - ment using two powerful technologies : RNA in situ (RNA scope ), and single cell sequencing. Misr2 is expressed in the subluminal mesenchyme surrounding the Mullerian duct epithelium during the early fetal urogenital ridge development of both sexes, but expression perdures only in females. We hypothesized that the subluminal Misr2+ mesenchymal cells may be progenitors of the endometrial stroma but not of the myometrium, and that the fate of these progenitors could be mapped by post-natal inhibition with MIS and single cell transcriptomics. A greater understanding of the postnatal development of the uterus and the differentiation of these Misr2+ mesenchymal stem cells may help us understand disorders of Mullerian development and idiopathic female infertilities. Methods: To characterize the dynamic Misr2 expression pattern in the developing uterus, we did RNA in situs (RNA scope ) on the mouse uterine sections at different developmental time points. To study the effect of MIS on the neonatal development and to understand the differentiation pattern of these Misr2+ uterine mesenchymal cells, we treated rat pups with adeno-associated viral vectors to deliver MIS (ligand of MISR2) continuously starting from day 1 and analyzed their uteri at various time points (n2,days 6,10,20, and 45). To further characterize the differentiation pattern of the Misr2 positive mesenchymal stem cells, we performed single cell transcriptome analysis after inDROP sequencing on control and MIS treated rat uteri cells on day 6 (n=3 for each group). These experiments were recapitulated by short-term delivery of human recombinant MIS protein purified in the laboratory. Results: Our in-situ analysis revealed that Misr2 expression is high and restricted to a subset of mesenchymal cells immedi - ately adjacent to the luminal epithelium at birth and, that expression declines in time (Fig1A). These Misr2+ mesenchymal stem cells remain susceptible to inhibition by MIS in neonatal rats, with treatment resulting in significantly smaller uteri (n3, p=0.0047, day 15), underdeveloped endometrial stromal layer (n>2, at day 6, 10, 20, and 45) and glandular defects (n>2, at day 20 and 45, but does not prevent the formation of myometrium (n>3) (Fig1B,C). As a result of these developmental defects, treated rats were completely infertile, even when treatment was restricted to just the first 6 days of development using recombinant MIS protein (daily dose: 3mg/kg; n=3, postnatal day 1 until day 6, p=0.0114) (Fig1C,D). These results confirm that MIS can inhibit normal endometrial differentiation of uterine cells postnatally, at a time when MIS production in the ovary begins. We further characterized the differentially expressed genes of the developing uteri in response to by single-cell RNA-sequencing with droplet-based microfluidics (InDrop) (Fig1E,F ). We uncovered new sets of genes that are regulated by MIS in the uterus 1) Genes uniquely present in the MIS-treated uteri expressed by an inhibited mesenchymal progenitor Misr2, Smad6) 2) Early differentiating endometrial stromal genes which are expressed by a population of cells (present in controls) that is prevented from developing under MIS treatment (Bmp7, Col3a, Vwa2). 3) Unique markers of (Misr2-) epithelial ductal cells, which are indirectly disrupted by MIS treatment (Id3 and Msx2) (Fig1E,F ). Conclusion: Single cell transcriptome analysis of the developing uterus identified novel cell types and gene signatures associ- ated with important developmental pathways, which were disrupted neonataly by exposure to MIS. Inhibition of the Misr2+ subluminal mesenchymal stem cells in the first week of life caused a failure of the endometrial stromal layers to develop, with profound consequences on the adult function of the uterus. The genes we discovered in this study provide a possible genetic basis for idiopathic female infertilities and for congenital Mullerian aplasias. Short term delivery in the neonatal periods with preservation of the ovary can have contraceptive veterinary applications as we seek novel ways to incorporate these finding to human applications. 199245 Caitlin R. Sacha, MD, OB/GYN The Effect of MGH, MA, USA and 2Pathology, MGH, Boston, MA, USA Introduction: The incidence of obesity is increasing amongst women of reproductive age in the United States, and this condition is associated with a higher risk of obstetric complications, such as preeclampsia and gestational diabetes. Prior literature has suggested that obesity increases systemic and placental pro-inflammatory cytokines. Our goal in this study is to assess the impact of overweight and obesity on placental morphology in IVF pregnancies. Methods: Placentas from live births arising from autologous IVF cycles between 2004 and 2015 were retrospectively examined and their morphology was categorized as having anatomic (e.g. cord and membrane insertion), infectious (chorio-amnionitis), inflammatory (e.g. chronic villitis of unknown deciduitis), malperfusion) logistic regression models were generated to compare placental morphology between women with normal weight (BMI < 25), overweight (BMI 25-29.9), and obesity (BMI 30), controlling for age, race, infertility diagnosis, gestational age, and number of fetuses.Results: A total of 865 placentas were available for review, including 475 placentas from women with a normal weight, 274 placentas from women with overweight, and 116 placentas from women with obesity. The average age in all groups was 35 years, and the women were predominantly Caucasian (74%). Most patients had either male factor (38%) or idiopathic (26%) infertility as their primary diagnosis. While rates of anatomic, infectious, and vascular/thrombotic placental morphology amongst women of different BMIs were similar, there was a trend towards increased inflammatory morphology with increasing BMI (aOR 1.45 for women with overweight and 1.69 for women with obesity; P-trend < 0.041; Table). Conclusion: In this large cohort of placentas arising from IVF pregnancies, increasing BMI appears to lead to higher rates of chronic villitis and plasma cell deciduitis in the placenta, which may in turn contribute to these women's increased risk for adverse antepartum and obstetric outcomes. Variation in placental morphology by BMI category *Adjusted for age, race, infertility diagnosis, gestational age, and number of fetuses. **P-trend < 0.041 246 Caitlin R. Sacha, MD, OB/GYN Advanced Maternal Age May Impact MA, USA and 2Pathology, MGH, Boston, MA, USA Introduction: Advanced maternal age (35 years and older) is associated with an increasing incidence of obstetric complica - tions, such as preeclampsia and stillbirth. The changes in placental morphology with increasing age as a potential contributor to these outcomes is not currently well understood. Our goal was to evaluate the impact of advanced maternal age on placental morphology in IVF pregnancies. Methods: A cohort of live births with placental pathology arising from autologous IVF cycles between 2009 and 2015 were retrospectively reviewed. Placental pathology was categorized using the pathology reports as: anatomic (e.g. cord and membrane insertion), infectious (chorioamnionitis), inflammatory (e.g. chronic villitis malperfusion) logistic regression models were generated to compare placental morphology between women under the age of 35 and those with advanced maternal age, controlling for race, body mass index (BMI), and infertility diagnosis. Gestational age at delivery was also controlled to take medical co-morbidities requiring earlier delivery into account.Results: A total of 839 placentas were available for review. The mean maternal age at the time of delivery was 35 years, median BMI was 23.5 kg/m 2, and most patients were Caucasian (73%). The most common IVF diagnoses were male factor (38%) and idiopathic (26%). Placentas from patients over age 35 were notable for significantly greater infectious (65 vs. 43 placentas, aOR 1.89, 95% CI 1.02-2.33) 200morphology. Abnormal anatomic (53 59 placentas) and inflammatory (24 vs. 35 placentas) morphologies occurred similarly between the under age 35 and advanced maternal age groups, respectively.Conclusion: Advanced maternal age may be associated with greater odds of infectious or vascular/thrombotic placental pathology. Understanding how placental morphology changes with age may help explain the increased incidence of obstetric complications in women over the age of 35. 247 Moshe Sade-Feldman, PhD, Cancer Center Defining cell states associated with response to checkpoint immunotherpay in melanoma M. Institute, Cambridge, MA, USA, 2Department of Medical Oncology, Dana Farber Cancer Institute, Boston, MA, USA, 3Surgical Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA, 4Brigham & Women's Hospital, Division of Rheumatology, Immunology and Allergy, Boston, MA, USA, 5Cancer Center, Massachusetts General Hospital, Boston, MA, USA, 6Pathology, Massachusetts General Hospital, Boston, MA, USA, 7Center for Immunology and Inflammatory Diseases, Massachusetts General Hospital, Boston, MA, USA and 8Department of Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: The development of antibodies that effectively block the activities of immune checkpoint proteins, including CTLA4, PD-1 or its ligand, PD-L1, has led to their approval by the FDA for treating a wide variety of cancers. These therapies are designed to overcome the inhibition of antigen-specific, effector CD8+ T lymphocytes (T-cells) by the tumor or the immune microenvironment. In melanoma, despite the high response rate (~45% for anti-PD-1, ~60% for anti-PD-1+ anti-CTLA4), most patients are refractory to therapy or acquire resistance in 10-12 months, and eventually succumb to disease. Since only a minority of patients respond to these therapies, a key question remains: why are most melanoma tumors either refractory or evolve resistance to immune CPB inhibitors? To date, several factors have been analyzed for their association with tumor growth and clinical outcome in melanoma patients, and include: I. levels of PD-L1 protein; II. load of tumor-derived neoantigens; III. defects in antigen presentation and IFN pathways; IV. abundance of partially exhausted CD8 + T-cells in responding tumors; and V. the magnitude of T-cell reinvigoration in relation to pretreatment tumor burden. While these studies have collectively contributed to the model explaining the efficacy of checkpoint therapy, their major limitations include: a) low predictive power; b) the use of pre-defined immune markers, limiting their ability to identify optimal and novel components that explain or predict clinical outcomes; and c) usage of bulk gene expression profiles, making it impossible to infer particular RNAs and functional cellular states of any specific cell type within the tumor microenvironment. Thus, identification of the key components that drive or prevent effective responses to checkpoint therapy remains the greatest unment need in the fields of cancer immunotherapy, and perhaps, medical oncology. Methods: To identify key immunological components associated with success or failure of immunotherapy, we profiled 16,291 immune cells from 48 tumor samples of melanoma patients (n=32) treated with the checkpoint inhibitors PD1 (n=37) and PD1CTLA4 (n=11), using single-cell transcriptomics. Patient response categories were defined by RECIST criteria: complete response (CR) and partial response (PR) for responders, or stable disease (SD) and progressive disease (PD) for non-responders. Profiling was performed on sorted CD45 + cells using an optimized version of the full length Smart-seq2 protocol with a median of ~1.4 million paired-end reads and a median of 2,588 genes detected per cell.Results: Initial unsupervised clustering of all 16,291 CD45 + cells identified 11 clusters. When performing the analysis by clinical outcome, we found that two clusters were significantly enriched in responder lesions while 4 clusters are enriched in non-responder lesions. Specifically, we found that both p=0.003) and G10 are enriched in non-responders. Next, we leveraged our unbiased approach to identify not only clusters but also specific markers associated with response and identified markers enriched in responder and in non-responder samples, many of which were not previously associated with clinical outcome to checkpoint immunotherapy (Figure 1). Due to the the importance of CD8 + T-cell recognition of tumor antigens in controlling tumors, we next focused our anlysis on CD8+ T cells and identified 2 main clusters CD8_G (with increased expression of genes linked to memory) and CD8_B (enriched for genes linked to cell dysfunction), both significantely p=0.005) lesions respectively. Since cells with both states coexist in each of the responder and non-responder lesions, we decided to calculate the ratio between the number of cells in these 2 clusters and observed a significant separation between responders 201(CD8_G/CD8_B>1) and non-responders (CD8_G/CD8_B<1) when looking at all samples, baseline or post samples separately. Similaer results were detected wehn looking at the expression of a single transcription factor, TCF7, in CD8+ T-cells in an independent cohort (n=43) using a different approach that could easily be applied in the clinic (Figure 2). Collec-tively, our results suggest that the ratio between T cell states is an excellent predictor of outcome for checkpoint therapy. Conclusion: Our study provides extensive unbiased data in human tumors for discovery of predictors and therapeutic targets to checkpoint immunotherapy. Indeed the results from these studies are now being used in collaboration with the Department of Pathology at MGH, to develop discovery panels that will be implemented in the clinic in the near future, and will help clinicians to optimally select patients for therapy and increase the chance of durable responses. 248 Salar Sajedi Toighoun, Ph.D., Radiology Coregistration of Fluorescence Sajedi 1Gordon Center for Medical Imaging, Massachusetts General Hospital, Boston, MA, USA and 2Radiology, Harvard Medical School, Boston, MA, USA Introduction: Histopathologic staining with hematoxylin-eosin (HE) has been used clinically for over 100 years to identify specific cells and tissues. To ensure proper identification, one or more immunomarkers are typically used, and resulting pigments are used in conjunction with HE to label cells of interest. In the conventional immunohistochemistry, the HE image is masked by the precipitating pigments. Hence, we used a new method with two independent wavelengths of near-infrared (NIR) fluorescence immunostaining on the same specimen and the same slide as conventional HE. Thus, the unstained HE can be used in conjunction with immunomarkers to clearly identify cells or molecules of interest. In this work, we use color and fluorescence images from the same sample and recognize required image transformations to coregister images and visualize the distribution of the intended biological target. We used a reference object for both modalities to measure transformation, rotation, and scaling parameters.Methods: Nikon Eclipse TE2000-U microscope equipped with two cameras on both sides: a Basler PowerPack 5.1 MP camera with 2464\u00d7 2056 image size for color imaging and ORCA-100 cooled digital camera (Hamamatsu, Japan) with 1344\u00d7 1024 image size for fluorescence Imaging, respectively. Images were obtained through a micrometer object (Olympus, Japan) and by two modalities using the same imaging parameter. The custom software Graphical User Interface (GUI) was designed to use four points of captured images from the reference object to register two images and store the registration parameters for the microscope. The software then uses the same parameters to register images from samples captured with two modalities. Results: The micrometer object was also used to measure the Image pixel size for each camera. The Image pixel size was measured to be 3.65 nm and 6.7 nm for color and fluorescence camera, respectively. The image coregistration accuracy was then calculated 3\u00d76.7 = 20.1 nm according to the pixel size and maximum pixel displacement, which was measured 3 pixels in the fluorescence image. Conclusion: Previous studies used fiducial points in the images to register different modality images from the same sample slide. In this work, we tested a new approach using a predefined shaped object and image processing algorithms to coregister florescence and color images from same sample slide. This also enables perfect coregistration of immunostaining and HE images, while alleviating the spectral overlap without altering the integrity of the original HE stained slide.202249 Amir Sajjadi, PhD, Radiology Multi-Chanel Florescence Imaging System for Intraoperative Image-Guided Surgery A. Sajjadi, H. Kang, P. Das, M. Tetrault, H. Sabet, G. El Fakhri and H. Choi Radiology, MGH, Charlestown, MA, USA Introduction: Optical imaging is getting more attention in clinical settings due to its lower cost, safety and ease of use compared to positron emission tomography (PET) or single-photon emission computed tomography (SPECT) which expose patients to various amount of radiation, especially during surgical operation. Using near-infrared window from 650 nm to 900 nm, florescence imaging is valuable because of relatively high tissue penetration and minimal autofluorescence. Combination of appropriate contrast agents and efficient imaging systems helps surgeons increasing the efficacy of surgery especially because it is invisible and can be integrated to the current surgical setting with providing simultaneous multi-channel imaging. Methods: The new multi-channel imaging system, Mesoscale Imaging System (MIS), allows real-time concurrent color imaging and two independent NIR florescence imaging channels. LED broad band light source (400-650 nm) is used for white light imaging and two diode lasers with 660 nm and 760 nm are used as excitation sources. A custom prism with internal dichroic mirrors split the light into This allows the light to be distributed to three CCD sensors in the camera (Condor3) after further filtration of NIR1 (660 \u00b1 20 nm) and NIR2 (760 \u00b1 10 nm) channels for simultaneous real-time imaging. A macro zoom lens, 0-10X (Navitar Zoom 7010) allows imaging the sample at various field of views and magnifications. Custom software is also developed to capture images and to stream all three channels on three distinguished screens along with the forth that shows all three superimposed. The software also provides real-time quantification features and analyses including auto gain, auto exposure, auto brightness/contrast/gamma (BCG), and other filters etc.Results: Experiments were performed on capillary tube phantoms with various concentrations of both 700 and 800 nm (ZW800-1) fluorophores in saline. exited with a 660 nm laser and ZW800-1 with a 760 nm laser, which were monitored under NIR1 and NIR2 channels, respectively. Further quantification and analysis were compared with our existing K-FLARE imaging system that allows separate imaging of these two fluorophores at two different channels. Real-time biodistribution of the fluorophores was performed in 25 g CD-1 mice to study the effects of illumination power distribution and imaging exposure time. The signal-to-background ratio (SBR) of the MIS was compared to the K-FLARE in terms of different exposure times and sensitivity. Conclusion: MIS provided real-time simultaneous multi-channel fluorescence imaging with better SBR compared to the K-FLARE. When used with appropriate contrast agents, it offers clinicians a powerful diagnostic and monitoring tool. Offering two spectrally distinct NIR channels, it can be utilized for monitoring tissue-specific drug delivery and tumor-to-background diagnosis in the current surgical setting. . 250 Samantha F. Sanders, BA, Medicine - Allergy/Immunology/Rheumatology Salivary Gland Disease in IgG4-Related Disease is Associated with Allergic Histories S.F. Sanders1,2, E. Della Torre3,4, C. Perugino2, J. Stone2 and Z. Wallace2 1Harvard Medical School, Boston, MA, USA, 2Rheumatology, Allergy and Immunology, Massachusetts General Hospital, Boston, MA, USA, 3Center for Cancer Research, Massachusetts General Hospital, Boston, MA, USA and 4Unit of Medicine and Clinical Immunology, School of Medicine, Universita Vita-Salute San Raffaele, Milan, Italy Introduction: The etiology of IgG4-related disease (IgG4-RD) remains unknown. The role of T-helper type 2 (Th2) cells in the pathogenesis of IgG4-RD is controversial. Given Th2 cells' involvement in allergic responses and prior IgG4-RD studies suggesting their importance in the pathogenesis, it has been hypothesized that allergic mechanisms contribute to the development of IgG4-RD. We investigated the association between allergies and IgG4-RD.Methods: The Center for IgG4-RD at Massachusetts General Hospital maintains a database of all IgG4-RD patients, including details of demographics and disease history. Allergy histories were obtained from patients via a 34-question allergist-developed survey administered at their baseline visit. Patients were considered to have a history of allergies if they reported prior symptoms or diagnosis. We included all patients who completed the allergy survey; for certain analyses, some patients were excluded because of missing data. Statistical significance was determined by Fisher's exact test or unpaired t-test, as appropriate. P values < 0.05 were considered significant.Results: Our study included 185 IgG4-RD patients from a database of 289 patients. Of the 185 patients, 140 (76%) reported any allergic symptom or diagnosis (Table 1). There was no significant difference with regard to age (P=0.1) or sex distri- bution (P=0.7) between patients with and without allergy symptoms. Skin allergies (41%), food allergies (20%), and 203anaphylaxis (8%) were less common than respiratory allergies (61%) in IgG4-RD. Patients with allergies tended to have any ear, nose, and throat (ENT) manifestations of IgG4-RD more often than those who did not have allergies (55% vs 36%, P=0.058). This trend was largely driven by a significant difference in the proportion of patients with salivary gland IgG4-RD (e.g., sialoadenitis) among those with allergies compared with those without a history of allergies (41% vs 22%, P=0.03). We found a similar difference when comparing salivary gland involvement between those with and without respiratory allergies (72% vs 28%, P=0.039).Conclusion: In a large IgG4-RD cohort, we found a significant association between IgG4-related salivary gland disease and allergic histories. More generally, we found a trend towards an association between ENT involvement and allergic histories. Of the reported allergies, respiratory allergies were most common. Respiratory allergies appear more prevalent in this cohort than the general US population: 61% of cohort patients reported a history of respiratory allergies, compared to 30% of Americans who completed a similar survey (Allergy Asthma Proc 2008; 29:600). While it is possible that shared pathogenesis is responsible for these observations, a shared upper respiratory exposure may also explain our observations given the associations between head and neck disease with allergic histories. Table 1: Characteristics of IgG4-RD relative to history of allergies * Includes air allergies, food allergies, skin reactions, and anaphylaxis Includes orbits, lacrimal glands, salivary glands, and/or other ENT manifestations ^ Includes atopic dermatitis, urticaria 251 Roberta Santiago, MD, PhD, Anesthesia, Critical Care and Pain Medicine Ventilation/Perfusion Matching in Severely Obese Patients Measured by Electrical Impedance Tomography: The Effect of Lung Recruitment Maneuvers and Mechanics-Based PEEP R. Santiago3, M. Teggia Droghi2, and L. Berra1 1Department of Anesthesia, Critical Care and Pain Medicine, Massachusetts General Hospital, Boston, MA, USA, 2School of Medicine and Surgery, University of Milan-Bicocca, Monza, Italy and 3Department of Respiratory Care, Massachusetts General Hospital, Boston, MA, USA Introduction: Almost 10% of the US population is severely obese (Body Mass Index -BMI 40 Kg/m2). Obesity per se may lead to Ventilation/Perfusion (V/Q) mismatch, particularly due to the excessive abdominal weight and related atelectasis. Every time a severely obese patient is intubated, the underlying V/Q mismatch must be considered when setting mechanical ventilation parameters. Lung recruitment maneuver (LRM) can revert the sequence atelectasis and less ventilation in posterior lung units, the most perfused areas. LRM consists in a transitory increase in airways pressure to restore the atelectatic lung units, followed by a positive end-expiratory pressure (PEEP) to maintain the opened units' best performance (compliance). Electrical impedance Tomography (EIT) can estimate V/Q matching at the bedside. (Figure 1) We hypothesized that, in severe obese patients, lung recruitment maneuver (LRM) followed by a PEEP selection based on the best dynamic respiratory system compliance (\"PEEP Titrated\") can improve V/Q mismatch. Methods: From April 2016 to July 2017, adult subjects with BMI 40 kg/m2 and requiring invasive mechanical ventilation were screened. In this cross-sectional and interventional study, EIT was used at bedside to measure V/Q matching during a standard of care PEEP setting (\"PEEP Baseline\") versus titrated PEEP (\"PEEPTitrated\"). The images generated by EIT were divided into 3 regions-of-interest (ROIs) in the supine position, namely, \"ROI 1\" anterior (non-dependent) and 204\"ROI 3\" the most posterior (dependent) lung regions. In each PEEP setting, ventilation and perfusion were measured in each ROI and matched.Results: We studied 10 adults (49\u00b115 years old), 60% females and all severely obese (BMI = 59\u00b110 Kg/m 2). PEEP levels differed significantly between the two methods (PEEPBaseline = 13\u00b1 1 cmH2O and PEEPTitrated = 21\u00b1 3 [6.5; 9.5], P< 0.0001). Ventilation was significantly redistributed from the ROI 1 to ROI 3 when comparing PEEP Baseline vs. PEEPTitrated (26\u00b16% to 17\u00b14% and 15\u00b15 to 24\u00b13%, respectively). Meanwhile, perfusion significantly increased in ROI 3 when comparing PEEPBaseline vs. PEEPTitrated (30\u00b17 to 34\u00b16%, respectively). (Figure 2) Conclusion: In severely obese patients under invasive mechanical ventilation, lung recruitment maneuvers followed by a PEEP based on best dynamic respiratory system compliance improved the match between lung ventilation and perfusion. Figure 1: On the left, EIT belt positioning (half of the belt is shown). In the middle, illustration of the innocuous electrical current injection. On the right, an example of the resulting relative image in ventilation and perfusion. Figure 2:A. Ventilation/Perfusion matching divided by Region-of-interest (ROI) and compared between the two PEEP levels. B. Ventilation and perfusion independent values, among the ROIs, PEEP Baseline vs. PEEPTitrated. Values presented as mean \u00b1 SD. *P< 0.05. 252 Jeffrey R. Savarino, BS, Pediatrics Epidemiology of Pediatric Window Falls in the Greater Boston Area J.R. of Medicine, Boston, MA, USA, 2Pediatric Critical Care Medicine, Massachusetts General Hospital, Boston, MA, USA, 3of Surgery, Division of Trauma, Emergency Services, and Surgical Critical Care, Massachusetts General Hospital, Boston, MA, USA, 4Pediatric Surgery, Massachusetts General Hospital, Boston, MA, USA and 5Harvard Medical School, Boston, MA, USA Introduction: Unintentional falls continue to be the leading cause of nonfatal injury in children. Falls from windows represent a small proportion of falls, but have contributed to injuries in almost 100,000 children over the past two decades. Despite public health campaigns, children continue to be severely injured or killed from accidental window falls in Massachusetts. We aim to investigate the demographics, risk factors, and outcomes of children presenting to a tertiary care children's hospital following an accidental window fall.Methods: The study population included patients ages 17 years and younger presenting to Massachusetts General Hospital with a diagnosis of fall from a building or structure (e-codes E880-E888), or listed diagnosis of Fall, and who on further chart review were deemed to have accidentally fallen from a window. Demographic, socioeconomic, and clinical data was extracted from the patient record. Statistical analysis was performed using one-sample t-tests and multiple linear regression using StataC.Results: 28 patients met inclusion criteria. The median age was 3.49 years (IQR = 2.55 to 4.87). 28.6% (n=8) of patients identified as Hispanic or Latino (P=0.02) while 50.0% (n=14) of patients were male of patients were White (P<0.001) 78.6% (n=22) of patients resided in rural areas (P=0.001). All recorded falls occurred between April and October. 34.6% (n=9) of patients fell from windows in single-family homes, 65.4% (n=17) fell from windows in multi-family homes or apartment buildings, (P=0.449). 52% (n=13) of falls were influenced by the presence of furniture (P=NS). 95% (n=19) patients pushed through a window screen when they fell (P<0.001). 67.9% (n=19) of patients were admitted to the PICU (P<0.001). As fall height increased, the level of care necessary to treat the patients in our cohort increased (P=0.007; 95% CI = 0.316 - 1.197). Patients with neck injuries (P=0.005; 95% CI = 0.659 - 2.193) and head injuries (P=0.026; 95% CI = 0.113 - 1.172) were also more likely to require higher level care. Conclusion: Patients identified in our cohort were young (<5 years old), had little long-term follow up within our network, and were predominately white with no disparity in gender. Almost 80% of our patients resided in rural areas and nearly 35% of falls occurred from single-family homes, in contrast with previous literature suggesting that this issue is confined predom-inately to urban areas, multi-unit houses, and children of minority race. Although most of our patients fell from windows lower than the third story, there was still a large injury burden within our cohort. The most common sites of injury were the head and face and abdomen and trunk, with the most common injuries being closed head injury, fracture, and soft tissue 205injury. For patients who followed up within our network, traumatic brain injury, mental health disorder, need for surgery, and long-term neurologic impairment were the most commonly observed outcomes. The severity of these injuries, and the high utilization of the PICU within our cohort, show the level or morbidity that can be associated with window falls from even low heights among young children. From these data, we can conclude that childhood window falls are common outside of previously identified populations and intervention should be focused on all communities. 253 Jaime Schneider, MD/PhD, Medicine - Hematology/Oncology Circulating Tumor DNA as a Tool for Predicting Response to Targeted Therapy in Gastrointestinal Malignancies J. Schneider1, A. Parikh1, M. 1Oncology, Boston, MA, and Boston, MA, USA Introduction: Patients with gastrointestinal (GI) malignancies undergoing treatment are surveyed for response or disease progression with radiographic imaging and measurement of standard tumor markers. However, there is a need for more sensitive techniques for early prediction of response to therapy. Measurement of circulating tumor DNA (ctDNA) is emerging as an important diagnostic tool that can provide real-time information about disease response during treatment. In this study, we evaluated whether dynamic monitoring of ctDNA may predict therapeutic response or resistance more effectively than standard methods in patients with GI malignancies on targeted therapies.Methods: Tumor and blood specimens were obtained from patients treated at the Massachusetts General Hospital under IRB-approved studies. Tumor biopsies were performed at the time of diagnosis and were subjected to the MGH standard molecular diagnostics panel for 104 known cancer genes. Blood samples for ctDNA analysis were collected at baseline at the start of targeted therapy, and one more more driver mutations were followed longitudinally at two week intervals starting one month after treatment. ctDNA was extracted from plasma using QIAGEN-based protocols and amplified by digital droplet PCR (ddPCR) using primers for tumor-specific point mutations. Tumor markers and CT scans were part of routine clinical care and procured using standard procedures at MGH. RECIST measurements, if available, were obtained in a blinded fashion using the Tumor Imaging Metrics Core.Results: We studied 31 patients with GI malignancies undergoing treatment with targeted therapies. Represented cancer types included colorectal (67%), gastroesophageal (20%), and biliary in ctDNA mutant allele fraction (MAF) or standard tumor markers four weeks after treatment initiation with radiographic response by RECIST. Tumor-specific circulating DNA decreased by an average of 91% in patients who went on to show partial response (PR) or stable disease (SD) by imaging compared to an average increase of 29% in patients who went on to show progressive disease (PD) (p=2.5x10 -5). All patients who ultimately achieved PR or SD had a reduction in ctDNA levels compared to baseline, making the specificity and positive predictive value 100%. In contrast, standard tumor markers (e.g. CEA, CA19-9) drawn in parallel at four weeks did not demonstrate a statistically signficant correlation with disease response (p=0.14, PR/ SD group compared to PD). Conclusion: Real-time monitoring of patient-specific tumor mutations by ctDNA analysis has the potential to predict response to targeted therapy more effectively than standard surveillance mechanisms (e.g. tumor markers). Integration of liquid biopsies into medical decision-making may allow for earlier identification of cancer progression and consequent modification of therapeutic approach. More studies are needed to determine whether incorporation of ctDNA analysis into clinical practice would impact patient outcomes. 254 Melanie Schorr, MD, Medicine - Endocrine-Neuroendocrine High Prevalence of Impaired Skeletal Integrity in Men with Anorexia Nervosa, Atypical Anorexia Nervosa and Avoidant/Restrictive Food Intake Eating Disoders, Denver, CO, USA, 2Neuroendocrine Unit, MGH, Boston, MA, USA, 3Translational and Clinical Research Center, MGH, Boston, MA, USA and 4Eating Disorders Clinical and Research Program, MGH, Boston, MA, USA Introduction: Severe bone loss is prevalent in women with anorexia nervosa (AN), but few data are available in men with AN, and no datum is available in men with atypical AN (ATYP) (AN cognitive symptoms without low weight) or avoidant/ restrictive food intake disorder (ARFID) (restrictive eating but without AN cognitive symptoms). Such data could have 206important implications for the prevention and treatment of low BMD and reduction of fracture risk in men with eating disorders (ED).Methods: We studied 103 men, 18-63y: 1) AN (n=26), 2) ARFID (n=11), 3) ATYP (n=18) and 4) healthy controls (HC) (n=48). Body composition, bone mineral density (BMD) and hip structural analysis (HSA) were assessed by DXA Results: Mean age [29 ED duration (7 \u00b1 1y) were comparable among groups. Mean BMI was similar in AN and ARFID (14.7 \u00b1 0.4 vs 15.3 \u00b1 0.4 kg/m 2), higher in ATYP (20.6 \u00b1 0.5 \u00b1 0.5 kg/m2, p<0.0005). Appendicular lean mass was lower in AN and ARFID, but not ATYP, compared to HC (p<0.0001). Vitamin D deficiency (25OH vitamin D <20 ng/mL) was present in 24 vs 0 vs 18% (p=NS), and low testosterone (T) levels (<200 ng/dL) in 65 vs 44 vs 31% of AN vs ARFID vs ATYP (p=NS). Mean BMD Z-scores at the PA spine and total hip were lower in AN and ARFID, but not ATYP, compared to HC (p<0.05). Prevalence of any BMD Z-score <-2 was higher in AN and ATYP compared to HC (66 vs 33 vs 6%, p0.01). Estimated hip strength by HSA section modulus (Z) was lower in AN, but not ARFID or ATYP, compared to HC (p<0.001). Low current BMI was positively associated with BMD Z-scores and estimated hip strength (R=0.44-0.63, p <0.0001). In stepwise regression models of the ED cohort, highest past BMI and ALM were positively, while ED duration was negatively, associated with BMD Z-scores and estimated hip strength. Men with vitamin D deficiency had lower spine and hip BMD Z-scores, as well as estimated hip strength (p<0.05), than those without vitamin D deficiency, but low T levels were not associated with low BMD.Conclusion: Men with AN are at high risk for low BMD and estimated hip strength. Men with ARFID (despite comparably low weight) and atypical AN (who are higher weight) are at lower, but still significant, risk of low BMD, but have preserved estimated hip strength. Low current BMI and long ED duration increase the risk of impaired skeletal integrity, as do low muscle mass and vitamin D deficiency\u2014complications of undernutrition observed in our cohort\u2014while a history of higher weight may be relatively protective. Low testosterone was not a determinant of low bone mass in our cohort. DXA screening should be considered in men with AN, ARFID and atypical AN. Whether differences in dietary micro/macronutrient content, psychopathology, or other disease factors in AN vs ARFID vs atypical AN contribute to differences in skeletal integrity, and whether weight recovery and/or treatment of vitamin D deficiency improve bone mass, warrants further study. 255 Alyssa Sclafani, MD, Medicine - Pulmonary Diverse Clinical Features of Interstitial Lung Disease in Patients with the Anti-SSA-52 kD Myositis-Associated Autoantibody A. Sclafani and S. Montesi Pulmonary and Critical Care Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Interstitial lung disease (ILD) is a known cause of morbidity and mortality in patients with idiopathic inflam - matory myopathies. While ILD as a complication of several myositis-specific autoantibodies, including the anti-synthetase autoantibodies, is well known, little is known about the association of ILD and many of the myositis-associated autoanti - bodies frequently seen in myositis overlap syndromes or with other connective tissue diseases. We here describe the clinical features of the anti-SSA-52 kD myositis-associated autoantibody seen in cases with ILD as the presenting feature. Methods: The clinical features of 12 patients with interstitial lung disease and a positive anti-SSA-52 kD antibody were analyzed retrospectively. For each patient demographics, presenting symptoms, concurrent rheumatologic symptoms or diagnoses, autoantibody profile, pulmonary function testing, radiographic and histopathologic patterns, treatment, and outcomes were ascertained. Results: Interstitial lung disease with an associated positive anti-SSA-52 kD antibody occurred across a wide age range (29-79 years old) in male and female smokers and non-smokers from diverse ethnic backgrounds. Patients presented hetero - geneously with progressive dyspnea and cough and in one patient with severe hypoxemic respiratory failure requiring venovenous extracorporeal membrane oxygenation. Only half of patients presented with symptoms suggestive of connective tissue disease (CTD) including Raynaud's phenomenon, arthralgias, myalgias, rash, or sicca symptoms. One patient carried a prior diagnosis of a rheumatologic disease (mixed connective tissue disease). Nearly all patients had additional positive serologies for connective tissue diseases (including positive antinuclear antibody or cytoplasmic antibody) with nearly half of patients meeting established criteria for Interstitial Pneumonia with Autoimmune Features (IPAF). Aldolase levels were elevated in almost half of patients. PFTs, when available, were mostly in a restrictive pattern with reduced DLCO. Radiographic and pathologic patterns were variable and included diffuse groundglass opacities, organizing pneumonia, nonspecific interstitial pneumonia (NSIP), usual interstitial pneumonia (UIP), or unclassifiable. Most patients were initiated on immunosuppression with corticosteroids, rituximab, or mycophenolate mofetil. Outcomes ranged from improvement in respiratory symptoms and PFTs to death from respiratory failure. Neither disease severity nor outcomes appeared to correlate with anti-SSA-52 kD antibody titer.207Conclusion: Presentations of ILD associated with a positive anti-SSA-52 kD antibody are heterogeneous and occur frequently without a prior history of rheumatologic symptoms or diagnosis of myositis. Further research is needed to assess the prognostic and treatment implications of this positive antibody. Clinicians evaluating new or acute presentations of ILD may consider testing for the anti-SSA-52 kD antibody as this finding may impact management decisions. Clinical features of 12 patients with interstitial lung disease and a positive anti-SSA-52 kD antibody Axial and coronal chest CTs of patient case #7. Presentation with hypoxemic respiratory failure requiring intubation, and significant clinical and radiographic recovery at 7 months with immunosuppression. 256 Jeehye Seo, Ph.D., Athinoula A. Martinos Center for Biomedical Imaging Relationships of sleep with resting state functional connectivity in trauma exposed individuals with and without PTSD J. Seo, K.N. Moore and E.F. Pace-Schott Psychiatry, Massachusetts General Hospital and Harvard Medical School, Charlestown, MA, USA Introduction: PTSD is characterized by hyperarousal symptoms, including sleep disturbance and exaggerated startle. Additionally, abnormal resting-state functional connectivity (rsFC) in the salience and default mode networks has been observed in PTSD. However, no research to date has investigated rsFC differences in fear-related networks between trauma-exposed individuals with and without PTSD as well as the association of rsFC measures with sleep difficulties. In trauma-exposed individuals exhibiting a wide range of PTSD symptom severity, we examined associations between sleep architecture measures and rsFC in anterior portions of salience, paralimbic, executive control and default mode networks.Methods: Individuals exposed to trauma (N=45) within the past 2 years [19 with PTSD; 13 with Clinician-Administered PTSD (CAPS) Score < 10] underwent fMRI resting-state scans at 3.0 T and a baseline ambulatory polysomnograph (PSG) on the night before their rsFC scan. Participants completed psychological interviews (including CAPS) and questionnaires (including the Hyperarousal Scale and PTSD Checklist-5). Slow wave sleep (SWS)%, REM%, and sleep efficiency (SE) were computed from baseline PSG. Connectivity maps within a fronto-limbic mask encompassing known fear- and extinction- related structures were generated for 6 six-mm sphere seeds: fear-related structures prefrontal cortex (vmPFC). Differences in rsFC with these seeds were compared between individuals with PTSD and those with a CAPS Score < 10 (trauma exposed controls or TEC) using SPM8. Additionally, among all subjects, correlation maps were generated between rsFC and the sleep measures. Mean Fisher's z-scores were extracted from each region significantly connected with each seed region, and these z-scores were regressed against sleep measures.Results: Compared to TEC, subjects with PTSD exhibited significantly decreased connectivity between left/right amygdala seeds and left hippocampus, and increased connectivity between: 1) the left amygdala seed and left orbitofrontal cortex (OFC); 2) seed and left dorsolateral prefrontal cortex (dlPFC); 3) the right AIC seed and pre-supple-mentary motor area (pre-SMA); 4) the dACC seed and SMA; 5) the vmPFC seed and right dlPFC. Across all 45 subjects, increased SWS% was associated with increased connectivity between: 1) the right amygdala seed and left pre-SMA; 2) the right AIC seed and left OFC; 3) the dACC seed and right dlPFC; and with decreased connectivity between the dACC seed and left hippocampus. Increased REM% was associated with increased connectivity between: 1) the vmPFC seed and left premotor cortex; 2) the vmPFC seed and right primary motor cortex; and with decreased connectivity between the right AIC seed and left OFC. Increased SE was associated with increased connectivity between the dACC seed and right dlPFC and between the vmPFC seed and left premotor cortex, while mean connectivity between the dACC seed and right ventrolateral prefrontal cortexdecreased with increased SE.208Conclusion: Group differences in rsFC between fear-related structures and anterior fronto-limbic regions were observed between individuals with PTSD and TEC possibly reflecting hyperarousal in those with PTSD. Fear-related regional connec-tivity values correlated with sleep measures. Greater SWS% and SE generally predicted greater rsFC between fear-related seeds and frontal regulatory regions. Greater REM% predicted greater connectivity between a fear-inhibitory region and motor areas. Moreover, the hippocampus, a region providing contextual modulation of fear and extinction responses, showed the opposite relationship with SWS% and PTSD status compared with frontal regions. These results suggest roles for fear- and fear extinction-related structures in sleep disturbances and contextual modulation of fear in PTSD. 257 Isabella I. Sereno, M.Ed., Psychiatry Relationship between perceptions of treatment goals and psychological distress in patients with advanced cancer I.I. Sereno3, General Hospital, Boston, MA, USA, 2Medicine, Massachusetts General Hospital, Boston, MA, USA, 3Psychiatric Oncology and Behavioral Sciences, Massachusetts General Hospital, Boston, MA, USA and 4Neurology, Massachusetts General Hospital, Boston, MA, USA Introduction: Several studies have demonstrated discordance between how patients perceive their goal of treatment versus how they perceive their oncologist's goal. Studies evaluating the extent and risk factors of this discordance are lacking.Methods: We conducted a cross-sectional study of 559 patients with incurable lung, gastrointestinal, breast, and brain cancers. We used the Perception of Treatment and Prognosis Questionnaire to assess patients' perceptions of both their treatment goal and their oncologist's goal and categorized responses: 1) patients who reported that both their goal and their oncologist's goal was concordant (either to cure or not to cure); and 2) patients who reported discordant perceptions of their goal versus their oncologist's goal. We assessed patients' psychological distress using the Hospital-Anxiety-and- Depression-Scale and used linear regression to assess the relationship between patients' perceptions of their treatment goal and psychological outcomes. Results: 61.7% of patients reported that both their goal and their oncologist's goal was non-curative; 19.3% reported that both their goal and their oncologist's goal was to cure their cancer; and 19.0% reported discordance between their goal and their perception of the oncologist's goal. Older age (OR=0.98, P=0.01), non-Hispanic ethnicity (OR=0.31, P=0.049), and higher education (OR=0.62, P=0.042) were associated with lower likelihood of reporting discordant goals. Patients with discordant perceptions of their goal and their oncologist's goal reported higher anxiety (B=1.56, P=0.003) compared to those who reported that both their goal and their oncologist's goal was curative. Patients who reported both their goal and the oncologist's goal was non-curative had higher depression symptoms (B=1.06, P=0.013) compared to those who reported that both their goal and the oncologists' goal was curative. Conclusion: One-fifth of patients with advanced cancer report discrepancies between their perceptions of their own and their oncologists' treatment goal which is associated with psychological distress. Tools are needed to identify patients at risk of cognitive dissonance about their prognosis. 258 Radhika Shah, PharmD, Dermatology Successful use of apixaban in dialysis patients with calciphylaxis who require anticoagulation: a retrospective analysis R. Shah1, A. Garza-Mayers1, S. Nigwekar2, D. Sykes2 and D. Kroshinsky1 1Dermatology, Massachusetts General Hospital, Livingston, NJ, USA and 2Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Calciphylaxis is a disease of dermal arteriolar calcification that results in panniculitis, skin ulceration, and necrosis. Most commonly occurs in individuals with end-stage renal disease (ESRD) on hemodialysis and is associated with a high morbidity and mortality. The use of warfarin is a risk factor in the development of calciphylaxis. Anticoagulation options are therefore limited in this patient population. Apixaban is an oral direct factor Xa inhibitor approved for the treatment of patients with deep vein thrombosis and pulmonary embolism, stroke prevention is renally excreted, there remains concern for its use in patients with advanced renal impairment. Study in the dialysis population is limited to a single study involving a small number of patients suggesting an approximate 36% increase in apixaban exposure with ESRD with no increase in maximum concentration and a limited impact of dialysis on apixaban clearance.209Methods: An electronic database search using the terms \"calciphylaxis\" and \"apixaban\" as identified in hospital discharge summaries in adult patients seen at the Massachusetts General Hospital and Brigham and Women's Hospital was conducted. 41 patients seen between January 1, 2014 and December 1,2017 were identified. Twenty patients had a diagnosis of ESRD on dialysis and started apixaban following a clinical diagnosis of calciphylaxis. This series included nine men and eleven women, ranging 45 to 83 years of age.Results: Patients were placed on anticoagulation for treatment of atrial fibrillation (17/20) and deep vein thrombosis (3/20) and antiphospholipid syndrome (1/20). Time from lesion onset to calciphylaxis diagnosis was a mean of 2.9 months (SD=4.9). Time from lesion onset to warfarin discontinuation was 4.7 months (SD=8.9). Patients were followed on apixaban for 1-118 weeks (median: 24.8 weeks). 80% (16/20) of patients had lesions on the thighs, 20% (4/20) of patients had lesions on the calves, 15% (3/20) of patients had lesions on the fingers and toes and 5% (1/20) of patients had lesions on the trunk. 50% (10/20) of patients had recurrent disease. 8/10 of these patients had calciphylaxis prior to the time of being examined in this study, at which time warfarin was not discontinued. 2/10 patients had a recurrence of calciphylaxis within 3 months of warfarin discontinuation. There was no reported DVT, pulmonary embolism, or hemodialysis fistula clot during follow up. Conclusion: The data presented here highlight the potential utility of apixaban as oral anticoagulation in patients with ESRD on dialysis and calciphylaxis 259 Radhika Shah, PharmD, Dermatology Use of Cyclosporine for the treatment of Steven Johnson Syndrome/ Toxic Epidermal Necrolysis R. Shah and D. Kroshinsky Dermatology, Massachusetts General NJ, USA Introduction: Steven Johnson Syndrome/Toxic Epidermal Necrolysis (SJS/TEN) are rare, but potentially fatal cutaneous conditions which are characterized by widespread skin and mucosal necrosis and detachment. Immune-mediated keratinocyte death is thought to the principle mechanism. Until recently, conventional therapy has been limited to the use of supportive care and IV corticosteroids. Given the likely immunologic etiology, the use of systemic immunosuppressants is now being studied for the management of these diagnoses. Methods: A retrospective medical record review of 48 patients admitted to Massachusetts General Hospital for SJS/TEN from 2010 to 2017 was conductedResults: Treatment with IV cyclosporine was assessed in 13 patients and 43 patients on IV corticosteroid therapy. There were no significant differences in age, gender or SCORTEN noted between the two treatment groups. Initial and maximum body surface area detachment were not found to be statistically different between the two groups. Time to complete reepitheliali - zation was 9.6 days for patients treated with cyclosporine and 14.1 days for patients treated with intravenous corticosteroids (P<0.0001). Mean duration of hospital stay was 17.9 days in patients treated with IV steroids and 10.5 in patients treated with cyclosporine (P=0.05). Two patients who were treated with intravenous corticosteroids developed bacteremia during their hospitalization. No patients discontinued treatment due to the medication-associated complications. Conclusion: Cyclosporine was found to be effective and well tolerated for the management of SJS/TEN. Patients treated with cyclosporine had reduced time to reepithelialization, duration of hospitalization, as well as reduced risk of bacteremia as compared to intravenous corticosteroid treatment. Patient Characteristics Comparison of outcomes between treatment groups 210260 Radhika Shah, PharmD, Dermatology Re-evaluating the Need for Routine Laboratory Monitoring in Isotretinoin Patients: A Retrospective Analysis R. Shah and D. Kroshinsky Dermatology, Massachusetts General Hospital, Livingston, NJ, USA Introduction: Acne vulgaris, a condition of pilosebaceous inflammation, affects approximately 95% of American adoles- cents. Altered keratinization of the sebaceous duct is attributed to the formation of acne. Isotretinoin, a vitamin A derivative, is a very effective treatment for acne. Routine laboratory monitoring has become part of the standard regimen for use due to concerns for the development of transaminitis, leukopenia, thrombocytopenia, and lipid abnormalities. The purpose of this study was to assess the utility of regular laboratory testing in two academic medical centers. Methods: This study is a retrospective analysis of laboratory abnormalities in patients who had been prescribed oral isotreti - noin for the treatment of acne at Massachusetts General Hospital and Brigham and Women's Hospital over the course of seven yearsResults: Laboratory abnormalities were assessed in 903 patients with a mean age of 28.6 (SD=31.8) years at the time of treatment. Therapies were aimed at achieving a 120-150mg/kg cumulative dose; patients were started on 10-20mg BID dosing with dose adjustments based on patient tolerability. There were no incidences of leukopenia, thrombocytopenia or anemia observed. Abnormal triglyceride levels after treatment onset were observed in 21.8% (N=197) of patients. In patients with elevated triglyceride levels, observation was the most common practice in 57.7% (56) followed by lowering of isotretinoin dose 38.1% (37). Elevations in AST occurred in 11.3% of patients during the course of their treatment. Elevations in ALT occurred in 3.1% of patients. Mean elevations above ULN in ALT and AST were 17 and 19 respectively. Treatment was discontinued in two patients due to triglyceride abnormalities and one patient for ALT elevation.Conclusion: The findings of this study support the thought that transient alteration of lipid and liver aminotransferases that tend to be clinically insignificant. While monitoring prior to therapy initiation may be prudent, monthly screening may be unnecessary and patients could perhaps be screened at 3 months and upon completion of the treatment course Patient Characteristics and Laboratory Values Subtype analysis of patients with elevated triglycerides and elevated liver function tests 211261 Bryant C. Shannon, MD, Emergency Dosing of Rapid Sequence Intubation Medications in the Emergency USA, 2Medicine, Beth Israel Deaconess Medical Center, Boston, MA, USA, 3Boston Medical Center, Boston, MA, USA, 4Emergency, Kaiser Permanente, Oakland, CA, USA and 5Emergency, Denver Health, Denver, CO, USA Introduction: Inappropriately dosed induction agents and neuromuscular blockade (NMB) during rapid sequence intubation (RSI) in the emergency department (ED) have been associated with hemodynamic instability, decreased first-pass intuba - tion, and inappropriate paralysis. The aim of this study was to examine variability in weight-based dosing (mg/kg) for RSI medications in the ED.Methods: Multi-center, prospective observational cohort study of patients who received RSI at 3 academic EDs. Based on the indication for intubation, patients were sub-grouped into altered mental status (AMS), respiratory failure, and trauma. We assessed for an association between the RSI agents selected and the doses for patients with BMI 30 and those <30, as well as a relationship between medications, doses, and the indication for intubation. Results: 316 patients had dosing and weight data (mean age of 55.3 \u00b1 8.6, 54.3% male, BMI 26.9\u00b1 8.1). AMS was the most common indication for intubation (42.5%) followed by respiratory failure. The majority of patients received etomidate (75.7%) and succinylcholine (58.4%). The mean dose of etomidate 0.27 \u00b1 0.1mg/kg and the mean dose of succinylcho - line was 1.7 \u00b1 0.5mg/kg. Etomidate use was associated with the lower BMI (85% v. 74%; p=.04). In regards to NMB, succinylcholine was with BMI cohort (70% 55%; p=0.008), while In comparing administered doses etomidate and succinylcholine differed between two BMI groups (P<0.001 for both). Additionally, ketamine was the only RSI drug that correlated with a particular indication for intubation (respiratory failure 14%, AMS 4%, trauma 4%; p=0.01). Conclusion: The mean doses of etomidate and succinylcholine were appropriate, regardless of BMI or indication for intuba - tion. There was a correlation between succinylcholine usage and being in the BMI 30 group, but only succinylcholine and etomidate were associated with higher doses in the <30 group. This study found no association with RSI agent selected and the indication for intubation with the exception of ketamine and respiratory failure. Additional research is needed to determine if these differences have any clinical significance. 262 Aileen Shaughnessy, BS, Neurology Gut Microbiome Assessment in People with ALS: A Pilot Study A. Shaughnessy2, K. Bjornevik3, G. Abu-Ali5, Chan6, and K. Nicholson2 1Harvard T.H. Chan School of Public Health, Boston, MA, USA, 2Neurological Clinical Research Institute, Department of Neurology, Massachusetts General Hospital, Boston, MA, USA, 3Department of Nutrition, Harvard T.H. Chan School of Public Health, Boston, MA, USA, 4Broad Institute, Cambridge, MA, USA, 5Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA, USA and 6Biostatistics Center, Massachusetts General Hospital, Boston, MA, USA Introduction: Short-chain fatty acids, and other microbiota-derived metabolites that reach blood circulation to exert potent anti-inflammatory effects, are important for maintenance of gut integrity. The adaptive immune system has emerged as a likely contributor to ALS pathogenesis. Objective: To compare the gut microbiome in people with ALS and controls, and to identify deviations in taxonomic composition in relation to ALS.Methods: We conducted a pilot case-control study at Massachusetts General Hospital examining the gut microbiome in people with ALS and age-matched controls using 16S ribosomal RNA (rRNA) gene amplicon (V4 region) and metagenomic shotgun sequencing at the Broad Institute. Interval analysis of 16S amplicon sequence data was performed for samples from 51 ALS, 57 healthy controls (HC), and 7 neurodegenerative disease control (NDC) participants. Metagenomic sequencing was subsequently performed on all available samples (68 ALS, 61 HC, and 12 NDC). Inter-individual variation in taxonomic composition was explored with unconstrained ordination of weighted UniFrac pairwise distances between 16S profiles. Significant associations between individual taxa and covariates (disease group, age, gender, race, degree of constipation, Riluzole use, G-tube use, baseline total ALSFRS-R, baseline vital capacity, disease duration, diagnostic delay, time from diagnosis to sampling) were determined using the MaAsLin multivariate association analysis pipeline. Random Forest machine learning was applied to 16S data to evaluate the potential of the gut microbiome to predict disease status. Taxonomic profiling of metagenomic reads was performed with MetaPhlAn2, and linear discriminant analysis was used to compare 212the relative abundance of bacterial species in the groups. We used the false discovery rate (FDR) approach to account for multiple comparisons.Results: In analyses based on metagenomic sequencing, the butyrate producing species Eubacterium rectale and Roseburia intestinalis were significantly lower in ALS patients compared to controls (p<0.001). Further, the total abundance of 8 dominant species with capabilities for butyrate production was significantly lower in ALS patients compared to controls (p<0.001). The results did not vary markedly with time from diagnosis to sampling, baseline total ALSFRS-R, or feeding tube status. We did not find an association between taxa and disease groups based on 16S sequencing, and no significant clustering related to any covariate was seen. Random Forest method did not yield reliable predictive markers of ALS. Conclusion: In this cross-sectional study, butyrate-producing species were lower in ALS than controls in analyses based on metagenomic sequencing, and did not correlate with disease duration or severity. We did not find any significant associations using 16S sequencing, which may suggest that higher resolution is required to adequately characterize the gut microbiome in ALS. A larger validation study with longitudinal gut microbial sampling using metagenomic sequencing is needed to verify the findings of this pilot study. 263 Angela R. Shih, Pathology Clinicopathologic and Molecular Characteristics of Poorly Differentiated Chordoma of Massachusetts General Hospital, Boston, MA, USA, 2Center for Sarcoma and Connective Tissue Oncology, Massachusetts General Hospital, Boston, MA, USA, 3Department of Radiation Oncology, Massachusetts General Hospital, Boston, MA, USA, 4Department of Orthopedic Surgery, David Geffen School of Medicine at University of California, Los Angeles, Los Angeles, CA, USA and 5Department of Orthopedic Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Chordoma is a rare malignant tumor of bone with high morbidity and mortality. Recently, aggressive pediatric poorly differentiated chordoma with SMARCB1 loss has been described. This study summarizes the clinicopathologic features of poorly differentiated chordoma with SMARCB1 loss in the largest series to date. Methods: A search of records between 1990-2017 at MGH identified 19 patients with poorly differentiated chordoma. Immunohistochemical stains were evaluated. Kaplan-Meier survival statistics and log-rank (Mantel Cox) tests compared survival with other subtypes. Molecular analysis through array comparative genomic hybridization (aCGH) was performed on three cases, and SNaPshot was performed on two cases. Results: The patients (n=19) were diagnosed at a median age of 11 years (range: 1-29). Tumors arose in the skull base and clivus The The tumors were composed of sheets of epithelioid cells with nuclear pleomorphism, abundant eosinophilic cytoplasm, and increased Tumors 100%) and brachyury (n =18/18; 100%). Patients were treated with a combination of excision, radiation therapy, and chemotherapy. Clinical follow-up reveals that six patients had local recurrence at a median of 11 (range: 3-26) months from initial diagnosis. No difference in overall survival, progres-sion free survival, local control time, and metastasis free survival was identified between poorly differentiated chordoma of the skull base and of the spine. Compared to other chordoma subtypes, poorly differentiated chordoma has a significantly decreased mean overall survival after stratification by site (p = 0.037). Array CGH of three cases showed that two cases had chromosome 22q loss, but only one of these two cases included deletion of SMARCB1. Two cases also had chromosome 9p loss, including CDKN2A at the edge of the deletion. Only one of the cases showed a complex pattern with chromosomal gains and losses involving twelve chromosomes, similar to the pattern seen in conventional chordoma. Snapshot identified an Rb1 mutation at low allelic fraction in both cases tested. Conclusion: Pediatric poorly differentiated chordoma has a distinct clinical and immunohistochemical profile, with charac - teristic SMARCB1 loss on immuonhistochemistry and decreased survival compared to conventional/chondroid chordoma. These tumors appear to have variably complex chromosomal abnormalities, occasionally with deletions involving SMARCB1 and Rb1 mutations. Based on the molecular findings in this limited number of cases, a subset may represent be a distinct type of tumor that does not appear to be genetically related to conventional chordoma. Recognition of this subtype is important because these malignancies should be treated aggressively with multimodality therapy. Further insight into the underlying genetics and biology of these tumors may suggest possible future targeted therapies.213264 Baehyun Shin, Ph.D., Center for Genomic Medicine (CGM) Novel DNA aptamers that bind to mutant huntingtin and modify its activity B. Shin1,2, Genomic Medicine, Mass General Hospital, Boston, MA, USA, 2Department of Neurology, Harvard Medical School, Boston, MA, USA, 3Division of Biology and Biological Engineering, California Institute of Technology, Pasadena, CA, USA and 4CHDI foundation, Princeton, NJ, USA Introduction: The CAG repeat expansion that elongates the polyglutamine tract in huntingtin is the root genetic cause of Huntington's Disease (HD), a debilitating neurodegenerative disorder. We found its polyglutamine tract size influences both huntingtin's structure and function, including modulating its activity in stimulating polycomb repressive complex 2 (PRC2). These observations strongly imply that small molecules that preferentially bind to mutant rather than normal huntingtin can be found and that, in doing so they may selectively influence the impact of the longer polyglutamine tract on mutant huntingtin activity.Methods: To provide an initial proof for the concept of small molecule-binding as a route to directly alter the impact of the expanded polyglutamine tract on mutant huntingtin, we have screened a library of single stranded DNA aptamers for mutant huntingtin specific binders, using highly purified human recombinant huntingtin with polyglutamine tract lengths of either 23- or 78-residues (Panel A). This biochemical strategy has yielded a specific set of aptamers that exhibit differential binding affinities to mutant and normal huntingtin. In this study, we have evaluated the aptamers' binding sites and potential ability to modulate the effects of polyglutamine tract length on huntingtin structure, PRC2-stimulating activity and therapeutic effects in HD human neural progenitor cells (hNPC). Results: We identified forty-five preferentially bind Q78-huntingtin compared to Q23-huntingtin. One of top aptamers, MS3, was chosen for further validation. First, MS3 showed the same or similar strong affinity to Q46-huntingtin, indicating our DNA-aptamer can be utilized for adult-onset CAG range (below Q50) as well as juvenile-onset CAG ragne (i.e. Q78) (Panel B). Moreover, we binds mutant proximal to lysines K2932/ K2934 in the carboxyl-terminal CTD-II domain (Panel C). When we tested a functional effect of aptamer binding, aptamer- bound mutant huntingtin abrogated the enhanced PRC2 stimulatory-activity conferred by the expanded polyglutamine tract. In a pull-down assay, the MS3-beads produced a hNPC (HD17m8) lysate, although total huntingtin levels in both cells were similar (Panel D). Furthermore, in HD hNPC, but not normal hNPC, MS3 aptamer co-localized with endogenous mutant huntingtin (Panel E) and was associated with significantly decreased PRC2 activity. Moreover, the cellular ATP content (Panel F) and cell viability in starving condition (Panel G), which are well known as HD cellular phenotypes by revealing decreased ATP level and reduced cell viability in starving condition, were significantly increased by MS3 aptamer transfection in HD hNPC specifically but not DNA oligonucleotides (GCdx). Therefore, DNA aptamers can preferentially target mutant huntingtin and modulate a gain of function endowed by the elongated polyglutamine segment. Conclusion: These mutant huntingtin binding aptamers provide novel molecular tools for delineating the effects of the HD mutation and encourage mutant huntingtin structure-based approaches to therapeutic development. We will validate the functional in vivo consequence of DNA aptamers in Hdh Q111 CAG knock-in mice, a precise genetic of the HTT CAG mutation, expressing full-length mutant HTT (Panel H) via collaboration with Dr. Wheeler in CGM. These approaches will guide us to develop promising therapeutic molecules as drug candidate for HD.214 265 Zoe Silsby, Neurosurgery Motor performance of symptomatic and asymptomatic children with Traumatic Brain Injury Z. Silsby, S. Henderson, E. Evans and A. Duhaime Massachusetts General Hospital, Boston, MA, USA Introduction: Traumatic Brain Injury (TBI) related symptoms are commonly used to monitor recovery following TBI in children. Symptom checklists are used because they are fast to administer and thought to be sensitive to subtle deficits, and thus often are used in \"return to learn\" and \"return to play\" decisions. Fatigue and dizziness are two of the most commonly reported symptoms. These complaints may correlate with physical function. Dizziness is a sign of potential vestibular dysfunction and balance impairment, whereas fatigue may be a sign of impaired endurance. The objective of this investiga - tion was to determine if children with reported fatigue or dizziness have decreased performance on objective measures of balance and endurance.Methods: Participants: Subjects were enrolled in the TRACK-TBI (Transforming Research and Clinical Knowledge in Traumatic Brain Injury) study, a multicenter observational study which collects clinical imaging, biomarkers, and outcome measures on subjects with TBI across a wide age and severity spectrum. TRACK-TBI subjects had at least a \"mild\" TBI per the American Congress of Rehabilitation Medicine definition and had acute clinical neuroimaging. Children enrolled in the TRACK-TBI study participated in 4 outcome assessments during the first year post-injury. The present study included a subset of TRACK-TBI patients, recruited at Massachusetts General Hospital between August 2016 and June 2018, during which the NIH Toolbox motor battery (NIHTB-M) was administered to assess feasibility of using this performance measure in the pediatric TBI population. The following investigation included subjects aged 8 to 16 with complete parent, child, and performance data at either 2 weeks or 6 months post-injury. Outcome Assessments: Symptom-level outcomes were obtained with the Health and Behavior Inventory (HBI), in which parents and children separately rate the frequency of TBI-related symptoms occurring over the last week. For this investigation, any endorsement of \"feels dizzy\" or \"feels like the room is spinning\" was considered positive for \"dizziness\". Endorsement of \"gets tired a lot\" or \"gets tired easily\" was positive for \"fatigue.\" Performance was assessed using the the NIHTB-M, focusing on the assessments of balance and endurance as these were expected to be associated with dizziness and fatigue respectively. Endurance is assessed via a 2-minute walk, and balance is assessed via a static standing balance test which incorporates sway data from an accelerometer. All NIHTB-Motor 215scores are reported as age-corrected standard scores with a population mean (SD) of 100 (15). Statistical Analysis: Analysis was performed using IBM SPSS Statistics for Windows, Version 25.0. Medians, interquartile ranges, and percentages were used to describe the sample. Mann-Whitney U tests were used to examine differences in NIHTB-M age-correct standard scores between symptomatic and asymptomatic children. Separate analyses were performed for parent and child reports and for each time point (2 weeks and 6 months post-injury).Results: At the 2 week timepoint, 14 children completed the NIHTB-M, with 10 subjects included in the analysis of balance and 12 subjects for endurance after exclusion of subjects with missing data. At 6 months, 16 children completed the NIHTB-M, with 14 subjects included in the analysis of balance and 15 subjects included for endurance after exclusion of subjects with missing data. At 2 weeks, children with fatigue symptoms had lower median endurance scores; differences were statistically significant at 2 weeks for the child report (p=0.036) and approached significance for parent report (p=0.073). At 6 months, no statistically significance difference was found in endurance scores between children with and without fatigue symptoms. The results of the balance/dizziness measures were less consistent. At 2 weeks, children with parent-reported dizziness had worse NIHTB-M balance scores. However, children with self-reported dizziness had better NIHTB-M median balance scores than asymptomatic children, suggesting inconsistency between parent and child reported symptoms, and between child reports and performance. At 6 months, no parents reported dizziness despite half of children reporting this symptom. Children with self-reported dizziness had lower median balance scores than those who were asymptomatic, but the difference was not statistically significant. Conclusion: The results are consistent with our expectation that in most cases children with symptoms following TBI, reported by either a parent or the child, display lower performance scores. However, at two weeks post-injury children reporting no symptoms had lower balance performance scores, raising questions about awareness of balance deficits in children. This was a preliminary analysis with a limited sample, but highlights differences between parent and child report measures, as well as the difference between questionnaire and performance measures in children following TBI. 266 Casey J. Silvernale, Medicine - Gastroenterology Lower socioeconomic status is associated with an increased prevalence of comorbid anxiety and depression among patients with irritable bowel syndrome: results from a multicenter cohort C.J. Silvernale1, B. Kuo1,2 and K. Staller1,2,3 1Gastroenterology, Massachusetts General Hospital, Boston, MA, USA, 2Center for Neurointestinal Health, Massachusetts General Hospital, Boston, MA, USA and 3Clinical and Translational Epidemiology Unit, Massachusetts General Hospital, Boston, MA, USA Introduction: Irritable Bowel Syndrome (IBS) is a functional gastrointestinal disorder (FGID) defined by recurrent abdominal pain associated with a change in frequency and/or form of stool. The prevalence of IBS in the US population is estimated at 10-20% and IBS symptoms often incur high health care costs. IBS has been associated with a wide array of physical and psychological symptoms, as well as decreased quality of life and increased utilization of health care resources. Anxiety and depression are common comorbid psychiatric disorders in IBS patients, but the population-level determinants influencing these comorbidities in IBS patients are poorly understood. Specifically, the potential relationship between socioeconomic status and psychiatric comorbidities among IBS patients has not been well studied. We sought to determine whether there was an association between comorbid affective disorders and socioeconomic status among IBS patients. Methods: We assembled a retrospective cohort of 1074 IBS patients with comorbid Generalized Anxiety Disorder (GAD) and/or Major Depressive Disorder (MDD) seen at two tertiary referral centers between 2007 and 2015. IBS patients with comorbid GAD and/or MDD were matched 3:1 by age, sex, and race to controls with IBS and no history of comorbid GAD and/or MDD. Socioeconomic status was approximated by patient zip codes; the average per capita income (per the 2015 US Census Bureau report) associated with each zip code was compared to the livable income threshold (LIT) (per the MIT Living Wage Calculator) of the county or metropolitan in which it fell. Patient LITs were divided into 5 socioeconomic groups: 0 (below the LIT), 1 (<100% above the LIT), 2 (>100% above the LIT), 3 (>200% above the LIT), and 4 (>300% above the LIT). A 5x2 chi-square test was used to assess difference in patient population distribution between the comorbid GAD and/ or MDD IBS patient cohort and the control IBS cohort. For the proportion of each patient cohort in the 5 socioeconomic groups, univariate comparisons were made using the chi-square test with Bonferroni correction for multiple testing and the average per capita income was compared using a T-test.Results: Our study included 1074 patients with IBS and comorbid GAD and/or MDD who were matched to 3713 IBS controls without comorbid GAD/MDD. The comorbid GAD/MDD IBS patient cohort was 82.1% female and 14.9% non-white, with a mean age of 55.6 years. There was no significant difference in age (P=0.85), sex (P=0.63), or race (P=0.89) between comorbid GAD/MDD IBS patients and IBS controls. There was a greater proportion of IBS patients with GAD and/or MDD in the lowest socioeconomic group compared to controls (OR=1.38, P=0.0004) (Figure 1). There was no significant 216difference between the proportion of comorbid GAD/MDD IBS patients in the highest socioeconomic group compared to controls (OR=1.15, P= 0.78) (Figure 1). The median average per capita income for comorbid GAD/MDD IBS patient cohort ($39,880.50) was also significantly lower than the control IBS patient cohort ($41,277.00) (P =0.02). There was no significant difference between the median average per capita income of GAD IBS patients and MDD IBS patients (P=0.59).Conclusion: Among IBS patients, the presence of comorbid Generalized Anxiety Disorder and/or Major Depressive Disorder is associated with lower socioeconomic status and lower average per capita income. IBS patients within the lower socioeco - nomic group are 38% more likely to develop or present with comorbid Generalized Anxiety Disorder and/or Major Depres- sive Disorder. These findings speak to a biopsychosocial model of illness, which should be considered by clinicians in the care of IBS patients. Proactive efforts directed towards diagnosing psychiatric disorders within this IBS patient subset may be a valuable means of identifying patients most likely to benefit from multidisciplinary approaches. 267 Laura C. Simko, Massachusetts, Physical Medicine and Rehabilitation Challenges to the Standardization of Trauma Data Collection: A Call for Common Data Elements for Acute and Long-Term Trauma Databases L.C. Simko1, L.A. Chen2, MA, USA, 2Shriners Hospitals for Children- Boston, Boston, MA, USA, 3University of Washington, Seattle, WA, USA, 4The Oregon Clinic, Portland, OR, USA, 5University of Michigan, Ann Arbor, MI, USA, 6Mount Sinai, Washington, DC, USA, 7Massachusetts General Hospital, Boston, MA, USA, 8UT Southwestern Medical Center, Dallas, TX, USA, 9UW Medicine Regional Burn Center, Seattle, WA, USA, 10Boston University School of Public Health, Boston, MA, USA and 11Shriners Hospitals for Children- Boston, Boston, MA, USA Introduction: Through the establishment of Common Data Elements (CDEs), existing trauma databases could be leveraged to maximize research on trauma outcomes. CDEs promote data sharing, standardization, and uniform collection which facili-tate meta-analysis and comparisons of studies, however there are no CDEs for trauma databases. The purpose of this study is to assess the extent of common data collection among five acute and longitudinal trauma databases. Methods: The data dictionaries of five trauma databases were examined to determine the extent of common data collection. Databases included two acute care databases (American Burn Association's Burn Quality Improvement Program/National Burn Repository (BQIP/NBR) and American College of Surgeons National Trauma Data Standard (NTDS)) and three longitudinal trauma databases (Burn Model System (BMS), Traumatic Brain Injury Model System (TBIMS), and Spinal Cord Injury Model System (SCIMS) National Databases). Data elements and data values were compared across the five databases. Qualitative and quantitative variations in the data were identified to highlight meaningful differences between datasets. Results: Of the thirty data elements examined, 14 (47%) were present in all five databases. Another 9 (30%) elements were present in four of the five databases. Of the 30 examined data elements, the number of elements present in each database ranged from 23 (77%) to 26 (86%). There were inconsistencies in the data values across the databases. Twelve of the fourteen data elements present in all five databases exhibited differences in possible data values. Conclusion: This study demonstrates inconsistencies in the documentation of data elements in five common trauma databases. These discrepancies are a barrier to database harmonization and to maximizing the use of these databases through linking, pooling, and comparing data.217268 Ashley Simone, Ph.D., Psychiatry Increased Risk of Reading Problems in Children with Tourette Syndrome (TS) and Comorbid Attention-deficit/ Hyperactivity Disorder A. Simone and M. Colvin Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: For pediatric TS patients, there is a high rate of comorbidity with ADHD and OCD. In ADHD, there are also high rates of reading disabilities. We questioned whether this risk is also present in TS patients. We specifically examined reading skills in a clinically-referred sample of TS pediatric patients with ADHD and OCD. Also, to determine whether any observed difficulties may be specific to TS, we also examined if greater reading difficulties could be accounted for by symptoms of inattention, hyperactivity/impulsivity, and anxiety as rated by parents. Methods: Neuropsychological data was retrospectively analyzed as part of pediatric clinical evaluations at Massachu-setts General Hospital (N=55; TS+ADHD=24, TS+ADHD+OCD=31). Individual composite z-scores were calculated for Phonological Processing, Single Word Reading, Reading Fluency, Reading Comprehension, and Spelling. The General Ability Index (GAI) was also used as a benchmark to assess specific learning difficulties in the sample. One-way sample t-tests determined whether academic skills were significantly different from this sample's mean GAI. Bonferroni correc - tion was used to correct for multiple comparisons. Correlations between BASC scores of parent-rated behavior (attention, hyperactivity/impulsivity, and anxiety) and academic achievement performance was also conducted.Results: Relative to the sample's GAI, patients had reduced Phonological Processing (t(52)=-4.80, p<0.001), Reading Fluency (t(35)=-4.88, p<0.001) and Reading Comprehension (t(35)=-6.83, p<0.001), but Single Word Reading and Spelling were intact. Relative to the normative mean GAI, TS patients continued to have difficulty with Reading Fluency (t(36) = -2.704, p = 0.01) and Reading Comprehension (t(36) = -3.723, p = 0.001, but Phonological Processing was no longer significant (t(53) = -0.701, p = 0.487). No significant correlations were found between parent-reported difficulties of anxiety, hyperactivity, or attention problems and higher order reading problems (Reading Fluency and Comprehension) at the .01 level. Conclusion: Our findings extend previous research in that TS patients appear to have greater weaknesses in integrated reading processes (reading fluency and comprehension), which do not seem to be accounted for by their attention, hyperac-tivity, and anxiety difficulties. Thus, it is important for clinicians to include reading in their assessment of children with TS, especially those with comorbidities. Future research is needed to determine whether these findings are similar in a community- based sample of individuals with TS, ADHD, and OCD, and to further investigate the relationship between academic perfor- mance and behavioral ratings. 218269 Vibha Singhal, MD, Medicine - Endocrine-Neuroendocrine Transdermal Estradiol Replacement Prevents the Reduction in Bone Formation Markers Observed with a Combined Oral Contraceptive Pill in Oligo-amenorrheic Athletes V. Singhal1,3, K.E. Ackerman1,2, Klibanski1 and M. Misra1,3 1Neuroendocrine Unit, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA, 2Division of Sports Medicine, Boston Children's Hospital and Harvard Medical School, Boston, MA, USA, 3Division of Pediatric Endocrinology, Mass General Hospital for Children and Harvard Medical School, Boston, MA, USA and 4Biostatistics Center, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA Introduction: Normal-weight oligo-amenorrheic athletes (OA) have lower bone mineral density (BMD) associated with lower levels of bone formation (P1NP) and resorption (CTX) markers compared with controls. However, data are lacking regarding the impact of estrogen administration via transdermal versus oral routes on levels of bone turnover markers, and how these changes affect BMD. Our objective was to address this knowledge gap.Methods: 121 OA, 14-25 years old, were randomized to receive (i) the 100 mcg17- estradiol transdermal patch (PATCH) with cyclic progesterone, or (ii) a 30 mcg ethinyl estradiol pill with 0.15 mg desogestrel (PILL), or no estrogen/proges - terone (NONE) for 12-months. Areal BMD (by dual energy x-ray absorptiometry) and levels insulin like growth factor-1 (IGF-1), P1NP, NTX and estradiol were assessed at baseline and 12 months. We adjusted for age, height, race and ethnicity in all analyses.Results: Randomization groups did not differ for clinical characteristics. IGF-1 and IGF-1 Z-scores, and P1NP levels decreased over 12-months in the PILL vs. PATCH groups (p= 0.048, 0.042 and 0.019 respectively) and did not differ from the changes in NONE group (p= 0.997, 0.991 and 0.842 respectively). Estradiol levels increased more in PATCH vs. both PILL and NONE groups (p = 0.014), while SHBG levels increased more in the PILL vs. PATCH (p= <0.0001) and NONE groups. Groups did not differ for changes in NTX. At the spine and femoral neck, there was an improvement in BMD Z-scores in the PATCH vs. PILL, and PATCH vs. NONE groups (data previously presented). Changes in P1NP were associated positively with changes in estradiol (=0.35, p=0.004) and IGF-1 SHBG (=-0.28, p=0.019) over 12-months. In a regression model that includes changes in estradiol, IGF-1 and SHBG levels, changes in IGF-1 remained changes in P1NP ( estimate 0.161, p= 0.016), whereas other associations were no longer significant. For changes in BMD over time, changes in estradiol were associated with changes in femoral neck and spine BMD at 12-months (r=0.27 and 0.28, p=0.024 and 0.019 respectively). Conclusion: Unlike a combined oral contraceptive pill, transdermal estradiol with cyclic progesterone does not suppress IGF-1, and increases in IGF-1 are associated with increases in bone formation markers. 270 Vibha Singhal, MD, Medicine - Endocrine-Neuroendocrine Estrogen Administration Improves Eating Disorder Psychopathology in Young Oligo-Amenorrheic Athletes F. Plessow1,2, V. Singhal1,3, C. Baskaran1,3, K. Eddy2 and M. Misra1,3 1Neuroendocrine Unit, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA, 2Eating Disorders Clinical and Research Program and Department of Psychiatry, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA and 3Division of Pediatric Endocrinology, Massachusetts General Hospital for Children and Harvard Medical School, Boston, MA, USA Introduction: Disordered eating behavior is common in conditions of functional hypothalamic amenorrhea, such as anorexia nervosa (AN) and exercise-induced amenorrhea, which are also associated with anxiety and depression. In hypoestrogenic rodents, estrogen replacement reduces anxiety-related behavior. Similarly, we have shown that estrogen replacement in adolescent girls with AN reduces anxiety and prevents the increased body dissatisfaction observed with increasing weight. However, the impact of estrogen administration on eating behavior in normal-weight young women with exercise-induced amenorrhea is unknown. We hypothesized that (1) normal-weight oligo-amenorrheic athletes (OA) would show greater disordered eating behavior and psychopathology compared to eumenorrheic athletes (EA) and non-athletes (NA), and (2) 12-months of estrogen replacement would improve those symptoms. Methods: Cross-sectional study (CSS): 109 OA, 50 EA, and 39 NA, 14-25 years old and of normal weight, completed the Eating Disorder Inventory-2 (EDI-2) and Three-Factor Eating Questionnaire (TFEQ) as self-report assessments eating behavior and psychopathology. Randomized controlled trial (RCT): OA were then randomized either the 100-mcg transdermal 17b-estradiol patch with cyclic progesterone (Patch), or a combined oral contraceptive pill with 30 mcg ethinyl estradiol and 0.15 mg desogestrel (Pill), or no estrogen (E-) for 12 months. None of the participants had a baseline diagnosis 219of active anorexia nervosa. 25, 19 and 26 young women in the Patch, Pill, and E- groups completed the EDI-2, and 19, 16, and 21 respectively completed the TFEQ at both baseline and 12-months. Data were analyzed for E+ (Patch+Pill) vs. E- groups and by routes of administration separately, controlling for age and weight change. Results: CSS: OA showed higher EDI-2 Drive for Thinness (DT) and TFEQ Cognitive Restraint of Eating scores than EA and NA (p=0.008 and 0.02 and p=0.006 and 0.02, respectively), higher EDI-2 Body Dissatisfaction (BD) scores than EA (p=0.04), and higher EDI-2 Perfectionism scores than NA (p=0.02). RCT: Over 12-months, the E+ group, compared to the E- group, showed improved trajectories for the EDI-2 p=0.046). In 3-group compari - sons (Patch, Pill, or E-), the Patch group outperformed the E- group and showed significant decreases in EDI-2 BD and TFEQ UE (p= 0.02 for both) (non-significant differences observed in the Pill vs. E- groups).Conclusion: In young OA, 12-month replacement of physiologic doses of estrogen improves trajectories of disordered eating behavior and psychopathology, emphasizing the importance of normalizing estrogen levels in this population. 271 Vibha Singhal, MD, Medicine - Endocrine-Neuroendocrine Associations of Body Composition and Bone Density, Microarchitecture and Strength Estimates in Adolescents with Obesity A. Toth1, V. Singhal1,2, F.C. Stanford3,2, K. Cooper1, M. Bredella4 and M. Misra1,2 1Neuroendocrine Unit, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA, 2Division of Pediatric Endocrinology, Massachusetts General Hospital for Children and Harvard Medical School, Boston, MA, USA, 3MGH Weight Center, Gastrointestinal Unit, Department of Medicine, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA and 4Department of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA Introduction: Previous studies in different populations (adolescent athletes, adults with obesity) have shown that excess visceral adipose tissue (VAT) is correlated with negative bone health and subcutaneous adipose tissue (SAT) is correlated with positive effects on bone health. Moreover, marrow adipose tissue (MAT) in some osteoporotic states like anorexia nervosa and type 2 diabetes mellitus is inversely associated with volumetric bone mineral density (vBMD). Data on the effects of these fat depots on bone health in adolescents with obesity are lacking.Methods: We used MRI at lumbar 4 th vertebra to assess VAT and SAT, and spectroscopy (MRS) to assess MAT at the L4 vertebra. HRpQCT was used to assess vBMD and the distal radius (non-weight-bearing bone) and tibia (weight-bearing bone) in 44 adolescents with obesity between the ages of 13-21 years who had a BMI to 35 kg/m 2. Microfinite element analysis was used to generate strength estimates.Results: We enrolled 9 males and 35 females. Mean age of the cohort was 17.89 \u00b1 2.24 years with mean BMI of 45.31 \u00b1 6.77 kg/m 2. Unlike our expectations, both at radius and tibia, no correlation of VAT, SAT or VAT/SAT with total, cortical and trabecular vBMD, microarchitecture or strength estimates were detected. At the non-weight-bearing radius, negative correlation was found between MAT and total vBMD ( = -0.39, p= volume ( = 0.-0.49, p = 0.005) and trabecular thickness ( = 0.-0.41, p = 0.02). All results were significant after controlling for age and ethnicity. No correlation was found between MAT and the cortical microarchitecture at the radius. At the weight-bearing tibia, no correlation was found between VAT, SAT and VAT/SAT and total, trabecular and cortical vBMD, microarchitecture and strength estimates. A negative correlation was observed between MAT and total vBMD ( = -0.42, p = 0.02).Conclusion: Like other osteoporotic states, MAT has negative associations with vBMD and trabecular microarchitecture particularly at the non-weight bearing site. This may provide further clues into the pathogenesis of wrist fractures which are seen at a higher frequency in adolescents with frequency as compared to normal weight population.220272 Eren Sipahi, BA, Psychiatry Unplanned pregnancy and other prenatal predictors of increased psychotic symptoms in a national cohort of 4,026 youths E. Sipahi1,2, G. Petrozzino3, Martinos Center, Charlestown, MA, USA and 3Neuroscience, Johns Hopkins University, Baltimore, MD, USA Introduction: Exposures in early development, even before birth, can affect brain health outcomes later in life. For example, pregnancy complications and reduced socioeconomic status have been associated with increased risk of psychosis later in life. There has been little research, however, into the effects of having an unplanned pregnancy and subsequent risk for psychosis. Since up to half of all pregnancies are unplanned, and psychotic spectrum symptoms in youth are now recognized as more common than previously thought, this question has wide potential relevance. Using newly released data from the Adolescent Brain Cognitive Development (ABCD) study, we compared psychosis scores from a large cohort of children ages 9-10 whose mothers reported having a planned versus unplanned pregnancy, correcting for demographic and other covariates related to pregnancy. Methods: The Adolescent Brain Cognitive Development (ABCD) Study is a nationwide, community-based, prospective study of approximately 10,000 children. This analysis uses the initial ABCD release (January, 2018) of 4,524 children ages 9-10. To assess psychosis syndromes risk, we used the self-reported Prodromal Questionnaire (PB-Q). Specifically, we used scores of how distressed the children were from experiencing the items on the PB-Q, on a scale of 0-105. Distress scores have been shown to be sensitive in predicting psychosis syndrome diagnoses from the Structured Interview for Prodromal Syndromes (SIPS). Distress score sums were grouped percentile: 0-50, 51-60, 61-70, 71-80, 81-90, and 91-99. We included covariates of interest factors related to pregnancy, including parent/guardian highest education level, presence or absence of a partner, mode of birth, prematurity at birth, pregnancy status (planned/unplanned), complications at pregnancy and birth, and use of prenatal vitamins at any point during pregnancy. Study site, race, sex, ethnicity, and age at assessment were included as nuisance covariates. Ordinal regression was performed with psychosis group as the dependent variable. A total of 4,026 subjects with complete data were ultimately included.Results: Included participants did not differ from those excluded for missing data, in either planned/unplanned pregnancy (p=.99) or psychosis scores (p=.22). Low parent/guardian education (p=.005), first-born status (p=.02), and pregnancy complications (p=.01) each independently predicted significantly higher prodromal psychosis scores, controlling for each of the other independent variables in the model. Of note, percentage of participants with unplanned pregnancy increased monotonically from 30.2% in the lowest psychosis score group (1-50 th percentile) to 46.5% in the highest psychosis score group (91-99th percentile). Maternal income, single parent household, prematurity at birth, complications at birth, and prenatal vitamin use were not significantly associ-ated with psychosis score. Conclusion: This large cohort study finds for the first time a relationship between unplanned pregnancy and increased risk of psychotic symptoms in school-aged youths. We replicated previous associations with low socioeconomic status and pregnancy complications to subsequent risk for psychosis, although the finding involving unplanned pregnancy persists after correcting for these and other contributory factors. This new association is likely multifactorial but may reflect delay or absence in protective exposures during development immediately following conception. Of note, prenatal vitamin supplement use was not a significant predictor of psychosis score, but was present at some point in the vast majority (>95%) of pregnan-cies; future data releases from ABCD will have more detailed information about the timing of prenatal vitamin exposure, which has relevance to risk for autism and other serious mental illness in childhood. The ABCD study also includes genomic and structural/functional brain imaging data for all participants, and our group is currently analyzing these data to examine the effects of unplanned pregnancy and other prenatal factors on brain development. These studies may point to mechanisms underlying associations between prenatal health and subsequent risk for severe mental illness in youth, and new potential strategies for prevention.221 273 Monica Sircar, MD, Medicine - Renal Identifying a Tentative Biomarker of Early Diabetic Nephropathy M. Sircar4, I. Rosales5, M. Selig5, D. Xu4, Z. A. and R. Thadhani4,2 1BIDMC, Medicine, Boston, MA, USA, 2Cedars-Sinai Medical Center, Los Angeles, CA, USA, 3BIDMC, Pathology, Boston, MA, USA, 4MGH, Medicine, Boston, MA, USA and 5MGH, Pathology, Boston, MA, USA Introduction: Diabetic nephropathy affects 15-25% of type 1 diabetics and 30-40% of type 2 diabetics, and is the leading cause of end stage renal disease (ESRD). The incidence of diabetes-associated ESRD has been steadily increasing for decades. Clinical signs include progressive decline in renal function, and proteinuria. There is a temporal window from the time diabetes is diagnosed to the appearance of overt kidney disease during which time the disease progresses quietly without detection. Currently, there is no way to detect early diabetic nephropathy (EDN), and mechanism(s) underlying EDN remain unknown. Here, we performed an unbiased assessment of gene-expression analysis of postmortem human kidneys to identify candidate genes that may contribute to EDN. We then selected one of the significantly increased EDN genes for further examination in postmortem human kidney and blood. Methods: Well-characterized postmortem biological samples (blood and kidney tissue) from individuals with EDN and matched nondiabetics were obtained from a commercial source. Except for obesity, there was no significant difference in any of the parameters between study groups; there were more obese individuals in the EDN group than among non-diabetic controls. Diabetes status was defined as hemoglobin A1c (HgbA1c) 6.5%. Proteinuria was identified by urine dipstick performed during the terminal hospital admission. These diagnoses were confirmed by careful review of pre- and postmortem clinical health records. There were stringent inclusion and exclusion criteria. Inclusion criteria required that donors be between 17 to 80 years of age, diabetic status confirmed as noted above, and postmortem time be within 24h from cross-clamping in the operating room (OR); the only exception with respect to HgbA1c testing was if a donor had been prescribed and/or using glycemic agent(s) including but not limited to metformin and insulin, in which case repeat A1c testing was not mandatory. Exclusion criteria included active infection, malignancy, severe glomerulosclerosis as determined in frozen OR biopsy material, history of renal replacement therapy (hemodialysis or peritoneal dialysis at any time), and any known genetic renal condition such as polycystic kidney disease, etc. Pathologic staging of diabetic tissues was performed using the 2010 Renal Pathology Society classification system for diabetic kidney disease. From each postmortem nephrectomy sample, more than 100 glomeruli were reviewed in tissue sections. Pathologic classes II and III diabetic kidney disease were considered as EDN. Total RNA was extracted from EDN and matched-control postmortem human kidneys, and mRNA were subjected to Affymetrix gene chip analysis. Differential transcriptome analyses of EDN kidneys and matched non-diabetic controls were performed and lists of significantly up-and downregulated genes were prepared. One of the most promising differentially expressed genes was probed using realtime PCR. Also, immunohistochemistry was carried out in EDN and nondiabetic kidney sections. Results: Microarray analysis showed alterations in five canonical pathways. Among them the complement pathway was the most significantly altered. One specific complement pathway gene, Complement 7 (C7), was significantly elevated in EDN kidney. Real-time PCR confirmed more than two-fold increase of C7 expression EDN kidneys compared to nondia- betic controls. Immunohistochemical staining for C7 protein showed more intense staining in EDN kidney sections than in non-diabetic kidney sections. C7 protein levels were also measured in the blood from EDN and control donors. Compared to nondiabetic blood, C7 levels were significantly higher in EDN blood.222Conclusion: Several families of genes were dysregulated in EDN kidneys, one of which was the complement system gene family. Complement 7 (C7) gene expression level was significantly increased in EDN kidneys. Compared to controls, C7 protein levels were higher in kidney as well as in blood of EDN donors. Our data show that prior to any clinical manifestations of diabetic nephropathy, such as proteinuria or changes in glomerular filtration rate, levels of C7 gene and its gene product are increased in postmortem human kidney and blood. Together, our data indicate that C7 is associated with EDN, and may be used as a molecular target for detecting and/or treating EDN. 274 Diana Smith, BA, Psychiatry Text messaging interventions to increase physical activity: A systematic review and meta-analysis D. C.M. Celano1,2 and J. 1Psychiatry, Massachusetts General Hospital, Belmont, MA, USA and 2Harvard Medical School, Cambridge, MA, USA Introduction: Physical activity is associated with improved mental and physical health, but many individuals fail to achieve recommended levels of activity. Existing physical activity interventions are time- and resource-intensive and have had limited impact on sustained behavior change. Innovative interventions that are delivered in real-time and promote activity in an ongoing manner have the potential to improve long-term physical activity. Text messaging is a promising method for delivering physical activity interventions due to the ubiquity of cellular telephones, the ability to customize messages for individuals, and the low expense of these interventions. However, the literature regarding the use of text message-delivered physical activity interventions is sparse.Methods: We performed a systematic review and meta-analysis of text message interventions to promote physical activity. A search of PubMed, PsycINFO, Scopus, and Cochrane databases from inception to January 2018 was used to identify articles, and these were supplemented with a search of ClinicalTrials.gov. Studies were eligible for inclusion in the review if they included a one-way, text messaging intervention to promote physical activity in adult samples. A subset of randomized, controlled trials that included an objective (accelerometer-based) outcome were included in a random effects meta-analysis. Results: The systematic search revealed 951 articles. Seven-hundred sixty-two were excluded based on a review of their titles and abstracts, and the full texts of 189 articles are being reviewed for inclusion. Of eight articles that included acceler - ometer-based outcomes, four found text messaging interventions to be superior to the control conditions, while the other four found no significant difference between the text messaging intervention and control conditions. Formal analyses will be presented at the time of the conference. Conclusion: Though evidence for the efficacy of text message interventions on physical activity is mixed, the results are promising and suggest that further efforts should be made to optimize these interventions for different clinical populations. 275 Isaac D. Smith, MD, Medicine - General Internal Medicine Alcohol Consumption and the Risk of Coronary Heart Disease and Mortality in Patients with Rheumatoid Arthritis I.D. Smith1, J. Wei1,2, Y. Zhang1,2, H. Choi1,2 and M.B. Bolster1,2 1Internal Medicine, Massachusetts General Hospital, Boston, MA, USA and 2Division of Rheumatology, Massachusetts General Hospital, Boston, MA, USA Introduction: Rheumatoid arthritis (RA) is associated with an increased risk of premature cardiovascular events and mortality. Prospective general population studies have shown moderate alcohol consumption is associated with a 25-40% reduced risk of all-cause mortality and coronary heart disease (CHD). Alcohol use has been discouraged in RA patients taking methotrexate (MTX) due to potential liver toxicity. However, a recent population-based study found moderate alcohol consumption (0-14 UK units or 0-8 US drinks per week) was not associated with a higher risk of hepatotoxicity. The aim of this study is to examine the effect of alcohol intake on all-cause mortality and CHD events among RA patients taking disease-modifying anti-rheumatic drugs (DMARDs), including those taking MTX. Methods: A prospective cohort study (1995-2017) was conducted using electronic medical records from The Health Improvement Network (THIN) database, representative of the general United Kingdom (UK) population. Our study popula-tion consisted of RA patients taking MTX or other DMARDs. Alcohol exposure was defined as first recorded alcohol use following RA diagnosis and divided into 5 categories: (high), 1 UK unit = 8g alcohol. We created Cox-proportional hazard models using age as the time-scale and calculated hazard ratios (HR) for the relation of alcohol consumption to all-cause mortality and CHD events, adjusting for age, sex, body mass index (BMI), smoking status, and Townsend deprivation index.223Results: Of 43214 RA (women: 70%, mean age: 60.2 years), 9102 deaths and 2013 CHD events occurred over 350612 person-years (mean follow-up = 8 years). Alcohol use was associated with a decreased risk of all-cause mortality in RA patients taking MTX, particularly with mild to moderate use. When the analysis was restricted to other DMARDs, the results did not change materially (Table). Alcohol consumption was also associated with a decreased risk of CHD events in both groups, most prominently with moderate-high to high use (Table).Conclusion: In this prospective cohort study, mild to moderate alcohol use is associated with decreased risk of all-cause mortality among RA patients taking MTX and patients taking other DMARDs. Alcohol consumption is also associated with a decreased risk of CHD events in both groups. Given these results, the prior recommendation for avoiding alcohol use in RA patients taking MTX warrants reconsideration. 276 Jordan W. Smoller, MD, ScD, Psychiatry All of Us New England: Methods for participant recruitment, enrollment, and engagement in a national, longitudinal cohort biobank study H. Hemley1, N.M. Allen1, Medicine, Partners HealthCare System, Cambridge, MA, USA, 2Boston Medical Center, Boston, MA, USA, 3Medicine, Boston Medical Center, Boston, MA, USA, 4Medicine, Brigham and Women's Hospital, Boston, MA, USA, 5Medicine, Massachusetts General Hospital, Boston, MA, USA, 6Neurology, Massachusetts General Hospital, Boston, MA, USA, 7Research Computing and Informatics, Partners HealthCare System, Boston, MA, USA and 8Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: The All of Us Research Program (AoURP) aims to recruit one million or more U.S. volunteers to participate as partners in a longitudinal cohort biobank study. AoURP is a key element of the National Institutes of Health's (NIH) Precision Medicine Initiative (PMI) to prevent disease and develop individualized treatments that account for differences in lifestyle, environment and biology through collaborations between researchers, health care providers and patients. Scientific opportunities include developing quantitative estimates of risk for a range of diseases by integrating environmental exposures and genetic factors; identifying the causes of individual variation in response to commonly used therapeutics; discovering biological markers that signal increased or decreased risk of developing common diseases; developing solutions to health disparities; use of mobile health technologies to correlate activity, physiological measures, and environmental exposures with health outcomes; empowering study participants with data and information to improve their own health; and creating a platform to enable trails of targeted therapies.Methods: Partners HealthCare System (PHS) hospitals and Boston Medical Center (BMC) are collaborating as All of Us New England (AoUNE). Funding for AoUNE began in October 2016 with alpha phase launching on September 20, 2017, beta phase on October 2, 2017, and national launch on May 6, 2018. AoUNE will enroll 93,000 participants, with a target of 46% of participants enrolling from communities underrepresented in biomedical research (UBR). Participants provide lifestyle and medical history surveys, mobile health data, and consent to link blood and urine samples to electronic health 224record (EHR) data. Recruitment processes across AoUNE hospitals begin with informed consent, wherein research assistants approach participants and have them create accounts, view videos, provide e-consent and sign EHR consent on iPads. Each participant is asked to complete a series of surveys focused on their demographic information, lifestyle habits and health status. Participants then undergo physical measurements and collection of biosamples (blood and urine). Samples are shipped to the Mayo Clinic Biobank, where they are de-identified and stored. The recruitment and enrollment processes take place in both English and Spanish. AoUNE builds trust, understanding and provide bidirectional benefit to UBR participants by prioritizing the participant voice in roles as co-leaders; soliciting feedback on which program and data sharing elements provide value; training staff continuously to ensure a culturally sensitive workforce; employing a geographic strategy of in-reach and outreach. The AoURP intends to engage and retain participants over 10 years, with additional self-assessments, opportunities to share digital health data and methods to track individual health information.Results: AoUNE is currently enrolling participants at BMC, Brigham and Women's Hospital (BWH), Massachusetts General Hospital (MGH), Brigham and Women's Faulkner Hospital (BWFH), Newton-Wellesley Hospital, and affiliated community health care centers (BWH Brookside, BWH Southern Jamaica Plain, BWFH West Roxbury, MGH Chelsea, MGH Revere). In Year 1, AoUNE has enrolled over 5,000 participants to the AoURP, with 74.4% of AoUNE participants in the program's 41.4% of participants identify race/ethnicity as American Indian/Alaska Native, Asian, Black or African American, Hispanic or Latino, Native Hawaiian or Other Pacific Islander, Multi-Ancestry, or other; 29.9% identify income as below the Federal Poverty Level; 22.8% identify age at consent as 65 years or older; 12.6% identify sexual orientation as bisexual, gay, lesbian, or \"none of these describe me\"; 10.6% identify education level as less than a high school degree; 1.4% identify geography as living in a rural zip code; and 0.04% identify as intersex, non-binary, transgender, gender identity that is different than sex at birth, or other.Conclusion: The AoURP represents the largest biomedical research study ever attempted. We have launched enrollment and engagement procedures for AoUNE, and have enrolled a highly diverse sample of more than 5,000 individuals to date. AoUNE and the larger AoURP effort will provide a platform for precision medicine research for decades to come. 277 Philip J. Spencer, MD, Surgery - Cardiac Lung Transplantation from Donors after Circulatory Death: United States and Single Center Experience P.J. Spencer1, and M.A. Villavicencio1 1Surgery, Massachusetts General Hospital, Boston, MA, USA and 2Pulmonary and Critical Care Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Lung transplants from Donors after Circulatory Death (DCD) have been scarcely used in the United States. Concerns about the warm ischemic injury, resource mal-utilization due to the uncertain timing of death, and public scrutiny may be some factors involved. Methods: Survival for recipients of a Donation after Brain Death (DBD) versus DCD was analyzed using the UNOS and our institutional database. A propensity-matching and Cox regression analysis was performed for 25 characteristics. Primary graft dysfunction metrics were compared.Results: A total of 389 out of 20,905 (2%) lung transplants were performed using DCDs in the US, and 15 out of 128 (12%) at our institution. Five and 10-year survival for DBDs was 55% and 30%, and for was 59% and 33%. Propensity-matched analysis of 311 DBD/DCD pairs did not demonstrate any difference in survival. On Cox regression, DCD was not associated with impaired survival. Male gender, Karnofsky 50, double lung transplant, and transplant year were predictors of improved survival. Age, creatinine, pulmonary fibrosis, retransplant, extracorporeal membrane oxygenation, allocation score, and donor age were predictors of worse survival. Primary graft dysfunction at time 0 was worse for recipients of DCDs (p=0.005), but equivalent at 24, 48, and 72 hours.Conclusion: DCD lung transplants remain underutilized in the United States. Nevertheless, survival is similar to DBD. Primary graft dysfunction metrics for DCDs are worse than DBDs on intensive care arrival but improved subsequently.225278 Suman Srinivasa, MD/MS, Medicine - Endocrine-Neuroendocrine Randomized Placebo Controlled Trial to Evaluate Effects of Eplerenone on Insulin Sensitivity and Inflammation among HIV-infected USA, 3Medicine/Endocrine, BWH, Boston, MA, USA and 4Neuroscience, Temple, Philadelphia, PA, USA Introduction: Metabolic dysfunction is prevalent among the HIV population. HIV-infected individuals demonstrate unique renin-angiotensin-aldosterone system (RAAS) physiology characterized by increased RAAS activation in association with visceral adiposity, insulin resistance, and inflammation. We hypothesized that a physiologically-based treatment approach targeting mineralocorticoid receptor (MR) blockade may have beneficial effects on improving insulin sensitivity and other inflammatory and metabolic indices in HIV.Methods: We conducted the first double-blinded, placebo-controlled, randomized trial using eplerenone in HIV. 46 well-treated HIV-infected individuals with increased waist circumference (WC) and abnormal glucose homeostasis by oral glucose tolerance testing were randomized to eplerenone 50mg (n=25) or placebo (n=21) daily for 6 months supplemented with lifestyle counseling. The primary endpoint was change in insulin stimulated glucose uptake normalized to insulin and lean body mass (M/I/LBM) measured by the euglycemic hyperinsulinemic clamp technique. Secondary endpoints included change in body composition and markers of inflammation. Metabolic and inflammatory parameters, including RAAS components, were evaluated under controlled sodium and posture conditions. Absolute change was determined by Student's T-test or Wilcoxon Rank Sums Test depending on normality of distribution. Results: Treatment groups (eplerenone vs. placebo) did not differ by age (49\u00b1 1 vs. 52\u00b1 1 years), male sex (60 vs. 71%), Caucasian race (60 vs. 43%), WC (111\u00b12 (all P>.05). Eplere - none did not improve insulin sensitivity compared to placebo (M/I/LBM 0.48 [-1.28,1.48] vs. 0.43 [-1.95,2.55] mg/min per a modest (-9\u00b1 vs. 26\u00b1 (-0.1[-0.3,0.1] vs. eplerenone vs. placebo. A trend towards an increase in HDL (2\u00b12 vs. -2\u00b11 mg/dL, P=.06) on eplerenone vs. placebo was seen, and after controlling for statin use, the increase in HDL became significantly different (P=.04). Changes in plasma renin greater in the eplerenone vs. placebo group demonstrating expected physiology. MR antagonism with eplerenone was well-tolerated among the HIV population with no significant changes in blood pressure (BP) or potassium.Conclusion: While eplerenone does not have adverse or beneficial effects on insulin sensitivity in HIV-infected individuals with mild abnormalities in glucose homeostasis, it may nonetheless be useful to improve cardiovascular disease risk based on inflammatory, ectopic fat and lipid lowering effects as demonstrated for the first time in this study in this population in whom RAAS activation has been shown. This study adds new safety data to the field demonstrating that this class of medication, at the dose used, is reasonably safe to use in the HIV population, the majority of whom are on multiple classes of antiretroviral therapies. Further studies of eplerenone are merited, to determine the full scope of its effects in HIV. These key hypotheses relevant to HIV will be addressed in a newly initiated RCT focused on cariovascular disease in HIV known as The MIRACLE HIV study. 279 Rachel Steinhorn, MD, Anesthesia, Critical Care and Pain Medicine A comparison of length of stay and perioperative outcomes in epidurals versus TAP catheters for cystectomy R. Steinhorn1, V. Hu2, L. Hung1, T. Anderson4 and S. Sabouri1 1Anesthesia, Critical Care and Pain Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Massachusetts Institute of Technology, Boston, MA, USA, 3Urology, Massachusetts General Hospital, Boston, MA, USA and 4Anesthesia, Stanford, Stanford, CA, USA Introduction: Epidural analgesia is frequently used to treat intraoperative and postoperative pain following open abdominal surgery. The technique has numerous drawbacks, however, especially for procedures with infraumbilical incisions that require epidural access low enough in the neuraxis to impact mobility. Transversus abdominis plane (TAP) continuous infusion catheters have been shown to reduce postoperative pain and reduce opioid requirements, but are not associated with 226the deleterious motor and hemodynamic complications that can be seen with epidurals. This study investigates the difference in length of stay and perioperative outcomes between patients receiving regional and neuraxial analgesia after cystectomy.Methods: A retrospective observational study of 124 patients who received a cystectomy under general anesthesia supple- mented by an epidural (N=68) or TAP catheters (N=56) at Massachusetts General Hospital between 3/2015 and 9/2017. Exclusion criteria included ASA 5 status, patients who received only general anesthesia, and patients who received single shot TAP blocks rather than continuous catheters. Intraoperative medication doses were corrected for length of procedure; intraop - erative and postoperative opiates were converted equivalents, and intraoperative vasopressors were converted to norepinephrine equivalent doses. The primary outcome was length of hospital stay (LOS) in days, and secondary outcomes included intraoperative opiate use, intraoperative vasopressor requirement, postoperative PCA days, and postoperative days until out of bed. Log transformed regression models controlling for age, sex, weight, ASA class, and surgeon were used to compare outcomes between groups.Results: Demographic comparison between groups revealed no significant differences (Table 1.) Patients who received TAP catheters had a significantly shorter LOS compared to patients who received epidurals by approximately two days (mean TAP=9.8 days \u00b16.5, meanEpidural=12.2 days+7.9, p=0.02), in addition to mobilizing from bed half a day earlier (meanTAP=1.5 days+0.8, meanepidural=1.2 days+0.4, p=0.01.) intraop - erative morphine equivalents per minute than patients who received regional techniques. Regional or neuraxial anesthetic technique did not correlate with need for a PCA, however (Table 2.) Conclusion: Receiving a regional versus a neuraxial anesthetic to supplement general anesthesia during a cystectomy significantly decreased LOS and time until OOB. This also likely represents a decrease in cost of hospitalization. Although intraoperative opioid requirements were decreased in patients with epidurals, they utilized opiate PCAs for a similar length of time as patients who received TAP blocks or catheters. Limitations of this study include its retrospective design, relatively small sample size, and the possibility of confounds inadequately controlled for in the statistical model. Further prospective randomized controlled study is needed to investigate differences in outcomes between patients receiving neuraxial and regional analgesia. Table 1 Patient demographicsTable 2 Perioperative outcomes between patients who received epidurals and TAP catheters 280 Sally M. Stoyell, Neurology Diffuse cortical thinning in BECTS is evident by years after diagnosis S.M. Stoyell1, Parnes1, General Hospital, Boston, MA, USA, 2Psychological Assessment Center, Massachusetts General Hospital, Boston, MA, USA, 3Martinos Center for Biological Imaging, Charlestown, MA, USA and 4Harvard Medical School, Boston, MA, USA Introduction: Benign epilepsy with centrotemporal spikes (BECTS) is a common childhood epilepsy syndrome, character - ized by a transient period of seizures that co-occurs with a dramatic period of cortical development. Children with BECTS display a stereotyped EEG pattern with spikes in the centrotemporal region. Previous studies in BECTS have shown inconsis- tent, patchy regions of increased (Kim et al. 2015, Overvliet et al. 2013), decreased (Overvliet et al. 2013) or unchanged cortical thickness (Garcia-Ramos et al. 2015) overlapping with centrotemporal regions compared to healthy controls. We hypothesized that there is a transient period of cortical thinning in the centrotemporal cortex starting at disease onset and evident later in disease. To test this, we evaluated cortical thickness in centrotemporal regions between children with BECTS and healthy controls (HC) based on disease duration, using both cross-sectional and longitudinal cohorts. We also perform post-hoc analyses to evaluate the spatial specificity of these findings.Methods: High resolution T1-weighted 3T MRI scans were collected in children with BECTS (n=24) and HC (n=19). v5.3. The centrotemporal area, global, and regional cortical thickness measures were calculated for each subject. The centrotemporal area in BECTS was defined here by the precentral, postcentral and superior temporal gyri labels from the Freesurfer Desikan-Killiany atlas. Cortical thickness differences were compared between BECTS patients based on duration of disease (\"Early\", within 2 years of first seizure; \"Late\", more than 2272 years from first seizure), and HC. Longitudinal data was evaluated in a subset of patients (n=5). Age was included as a covariate in all analyses.Results: We found a difference in cortical thickness in the centrotemporal gyri between groups adjusted between \"Early\" BECTS patients and HC was not significant (adjusted p=0.129). We found similar results when looking globally (\"Late\" p=0.017; \"Early\" p=0.18). This difference between \"Late\" BECTS and temporal, parietal, occipital, insula; MANOVA: p<0.001). Each brain region in \"Late\" BECTS compared to HC, (p< 0.05 for all tests except temporal p=0.09; one-way ANOVA, FDR corrected). Longitudinal data supports a relationship between disease duration and rate of cortical thinning in the centrotemporal region (p=0.012), where BECTS children showed a transient period of accelerated thinning corresponding to disease onset.Conclusion: We find that children with BECTS have transient, diffuse thinning of the cortex that is evident by 2 years after diagnosis. These results integrate prior inconsistent results in the literature. Future work should be done to examine the functional significance of these findings. References: Garcia-Ramos et al. 2015. Epilepsia 56: 1615-1622. Kim et Sebastian Suarez, MD, MPH, Emergency Assessment of a competency-based training to support emergency and essential surgery in Kenya when no anesthetist is available S. Suarez1, L. Ojeaburu1,2, J. and M. Omotayo1,4 1Division of Global Health and Human Rights, Department of Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 2African Institute for Health Transformation, Sagam Community Hospital, Luanda, Kenya, 3College of Surgery for East, Central, and Southern Africa, Arusha, Tanzania, United Republic of, 4Harvard Medical School, Boston, MA, USA and 5Harvard T.H. Chan School of Public Health, Boston, MA, USA Introduction: Five billion people do not have timely access to emergency and essential surgery in a timely fashion worldwide. Lack of anesthesia services, also known as the anesthesia gap, is a primary barrier. The 'Every Second Matters for Emergency and Essential Surgery - Ketamine' (ESM-Ketamine) package was designed to train non-anesthetist clinicians to support emergency and essential operations when no anaesthetist is available. The training includes ten domains within four major competencies that ESM-Ketamine providers should master. The objective of this study was to assess the clinical competency of trained ESM-Ketamine providers in support of emergency and essential surgery when no anesthetist was available.Methods: Clinical competency was defined as knowledge and performance, as portrayed by Miller's Pyramid of learning and competence. A multiple-choice examination was used to evaluate knowledge (\"knows\") before and immediately after training. Differences in the prevalence of ketamine-related adverse events between training and non-training cases were used to measure the effectiveness of performance (\"does\" and \"shows how\"). Safety data were collected prospectively using program-specific data collection cards for every operative procedure. All operative cases supported by the ESM-Ketamine package were included in the study. A paired t-test and Chi-square analysis were performed to identify differences between groups. A multinomial logistic regression was performed to account for confounding variables. Results: 22 ESM-Ketamine trainees completed the multiple-choice examinations before and after training. On average, participants' score increased by 6.5 points (total score 73% vs. 40.5%, p < 0.001) after completing the 5-day ESM-Ketamine training program. Between December 2013 and July 2018, 1,571 operative cases were supported by the ESM-Ketamine 228Package. ESM-Ketamine was used to support 256 (16.3%) surgical cases under direct supervision of an anesthetist and 1,315 (83.7%) non-training cases when no anesthetist was available. Hallucinations were treated with diazepam in 41 (16%) training cases and 142 (10.8%) non-training cases (p=0.02), hypersalivation was treated with atropine in 35 (13.7%) training cases and 112 (8.5%) non-training cases (p=0.01), brief (<30 and prolonged desaturations (>30 secs) occurred in 1 (0.4%) and 7 (0.5%) cases, respectively (p=1.0). After adjusting for ketamine dose and type of procedure, all differences and point estimates were non-significant.Conclusion: Competency-based assessments identified considerably increased knowledge on use of ketamine after comple - tion of the 5-day ESM-Ketamine training program There were no measurable differences in ESM-Ketamine provider perfor-mance and patient outcomes when comparing training cases supervised by an anesthetist versus emergency and essential operations and painful procedures when no anesthetist was available. 282 Daniela Suarez-Rebling, Emergency Behavioral and contextual barriers to optimal UBT use among skilled birth attendants in Kenya D. Suarez-Rebling1, S. Suarez1, L.F. Garg1, and T.F. Burke1,3,4 1Division of Global Health and Human Rights, Department of Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Kisumu Medical and Education Trust (KMET), Nairobi, Kenya, 3Harvard T.H. Chan School of Public Health, Boston, MA, USA and 4Harvard Medical School, Boston, MA, USA Introduction: Postpartum hemorrhage (PPH) is the leading cause of maternal death worldwide. The Every Second Matters - Uterine Balloon Tamponade (ESM-UBT) device is effective for managing uncontrolled PPH caused by atonic uterus in low-resource settings. In Kenya, the ESM-UBT package has been introduced in nearly 500 facilities and over 4,000 health - care providers have been trained. However, it has only been used in a small fraction of the cases for which it is indicated. The objective of this study was to evaluate behavioral and contextual barriers to optimal PPH management and use of the ESM-UBT package.Methods: Obstetrician-gynecologists, nurse-midwives, clinical officers, and medical officers who are involved in emergency obstetric care including PPH management underwent semi-structured interviews. Interviews were audio recorded and notes were taken independently. Recorded interviews were transcribed verbatim and a data codebook was established through an iterative process. Interview transcripts were independently coded and differences reconciled. Themes that emerged were identified.Results: Preliminary results suggested the following emerging themes: Positive perceptions that ESM-UBT devices save lives and decrease hysterectomies. Primary barriers to optimal ESM-UBT device use include fear of making a mistake, device inaccessibility, low staff skills, high staff turnover, lack of training, preference to refer or to wait for additional support before inserting UBT, and burnout. Health provider strikes and frequent failures in supply chain function were additionally described as barriers to quality PPH care.Conclusion: When providers have been trained and the necessary equipment and supplies are present, behavioral factors may be important contributors to under-utilization of ESM-UBT. This ongoing research will help guide interventions for improving optimal skilled birth attendant behavior and identify opportunities for improved overall PPH care. 283 Haoqi Sun, PhD, Neurology Brain Age Index from the Electroencephalogram of Sleep L. Hospital, Boston, MA, USA, 2Beth Israel Deaconess Medical Center, Boston, MA, USA and 3Department of Pediatrics, University of Chicago, Chicago, IL, USA Introduction: The human sleep undergoes profound changes with aging, reflected in various aspects including the electroen- cephalogram (EEG) oscillations and waveforms. Older people exhibit reduced slow waves during deep sleep, decreased sleep spindle amplitude, density and duration, and less phase coupling between slow oscillations and sleep spindles. Using these changes, the deviation of aging from normal trajectory can be quantified as \"brain age index\" (BAI). It serves as a potential biomarker related to the risk of cognitive impairment, neurological or psychiatric disease, or death. Compared to BAIs based on other modalities, BAI from sleep EEG reflects functional rather than anatomical changes, is more user-friendly compared 229to MRI and molecular biomarkers, and hence potentially facilitates repeated within-patient assessment of medications or brain stimulation.Methods: We develop an interpretable machine learning model to predict brain age based on two large sleep EEG datasets approved by Partners Institutional Review Board. First, the Massachusetts General Hospital sleep lab dataset (MGH) has 2,621 EEGs, 6 channels including frontal and C4-M1) and occipital (O1-M2 and O2-M1), and covers ages 18 to 80 years. We identify 189 patients with significant neurological or psychiatric disease using definitions from the literature. To avoid confounding of the diseases, we exclude these patients in the training set. Second, as a validation dataset, the Sleep Heart Health Study (SHH) has 3,520 EEGs, 2 channels (C3-M2 and C4-M1), and covers ages 40 to 80 years. It contains repeated EEGs from the same subject in two visits about 5 years apart, making it possible to evaluate the longitudinal reliability of the model. We extract 102 features from each 30-second epoch and average the features separately for each of the 5 sleep stages scored according to the American Academy of Sleep Medicine, yielding 510 features per EEG. A machine learning model is trained to predict the age using these features. The predicted age is called \"brain age\" (BA). The brain age index (BAI) is defined as the difference between BA and chronological age (CA). Results: (1) Prediction Performance: The model obtains a mean absolute deviation (MAD) 8.1 years and Pearson's correla - tion 0.79 (95% CI 0.77 -- 0.81) between BA and CA in healthy subjects in the MGH dataset. In comparison, the prediction performance is worse using sleep architecture features (MAD 23.5 years and Pearson's correlation 0.44, 95% CI 0.40 -- 0.48). If limited to two electrodes only, the frontal or occipital electrode pairs provide similar performance (p-value = 1), while being significantly better than the central electrodes (p-value < 0.01). Therefore, it is suggested to use frontal EEG electrodes if fewer EEG electrodes are preferred. (2) Robustness: We compute the correlation of BAI with other clinical measures of sleep, such as apnea-hypopnea index, sleep efficiency, proportion of deep sleep, and wake after sleep onset. It turns out the correlation is weak (maximum +/-0.1), indicating that BAI is relatively robust in subjects with different characteristics. (3) Longitudinal Reliability: On the SHHS dataset containing longitudinal EEGs 5 years apart, when trained on SHHS, the BA has an average of 5.5 years increase; when trained on MGH, the BA has an average of 4.3 years increase at the population level. (4) Sensitivity to Diseases: Subjects with significant neurological or psychiatric disease exhibit an average increase of BAI 5.1 years relative to healthy controls. Subjects with hypertension and diabetes show an average increase of BAI 3.6 years compared to healthy controls. (5) Interpretability: The reasons for having older or younger BAI for each individual subject can be interpreted by comparing to a model-derived age norm. The deviation of EEG-based BA from CA in individuals arises from multiple factors. Technical factors include (1) EEG artifacts not removed by preprocessing; (2) constraints of the model formulation; and (3) the first night effect. Neurophysiological factors arise from (1) night-to-night variability; (2) underlying diagnosed or undiagnosed diseases; (3) general medical health; (4) genetically-determined inter-individual differences; and (5) exposure to environmental insults. Conclusion: The brain age index can be inferred from sleep EEG. We validate BAI by assessing its prediction performance, robustness, longitudinal reliability, sensitivity to diseases and interpretability. In summary, the findings raise the prospect of using sleep EEG as a biomarker for healthy brain aging. On the other hand, there are important limitations in our study. Our dataset does not allow studying night-to-night variability of BAI. Further studies with more granular detail about individuals' medical conditions and neurological function will be required to better understand the biological significance of deviations between EEG brain age and chronological age. Figure 1. Individual cases with BA matches (diagonal) or is younger or older than CA (off-diagonal). Each panel consists of the hypnogram and corresponding spectrogram. Spectrograms are calculated as the average across the 6 EEG channels. The horizontal axis is time in hours.230284 Crystal Tan, MD, MS, Anesthesia, Critical Care and Pain Medicine Rescue Echo For Non-Cardiac Surgeries: An Interim Analysis of an Institutional Initiative C. Tan, K. Shelton and G. Staudt DACCPM, MGH, Boston, MA, USA Introduction: Intraoperative transesophageal echocardiography (TEE) is recommended for patients undergoing non-cardiac surgery who encounter life-threatening hemodynamic compromise, persistent hypotension, or persistent hypoxia. However, use of this advanced diagnostic tool may be limited by scarcity of equipment or echo-trained physicians. In May 2015, a Rescue Echo Service was created at our institution to facilitate access to TEE in non-cardiac operating rooms. We have since developed a protocol to improve the deployment of resources and to further enhance the role for intraoperative rescue echocardiography. Methods: The Rescue Echo Service was first established at our institution in May, 2015. Its initial development consisted of a Rescue Echo pager carried by a cardiac anesthesiologist, which allowed non-cardiac anesthesiologists to request a TEE in the event of intraoperative hemodynamic compromise. However, mobilizing the appropriate resources remained somewhat cumbersome and inconsistent. To improve the response time and facilitate an appropriate, expeditious exam, we developed a standardized Rescue Echo protocol. First, we expanded the Rescue Echo Service to include a team of individuals including a cardiac anesthesia attending, fellow, technician and nurse. We also created a Rescue Echo Team paging group to allow a second mode of timely, direct communication to the entire team. Team members could be alternatively alerted using a voice-controlled, wearable Vocera badge which was introduced to our operating rooms in mid-2015. Each team member was educated regarding their individual role in the group, whether it be gathering the designated Rescue Echo machine, or reporting directly to the operating room to begin assessing the situation. This has simplified the process of requesting a rescue TEE for both the requesters and the responders. In addition, we embraced the concept of cognitive aids and attached an Emergency Manual (modified for our institution from the Stanford Emergency Manual) to the designated Rescue Echo machine. We have also created a visual aid outlining an abbreviated TEE sequence to ensure a complete rescue exam. This sequence includes 5 of the 20 standard views included in the ASE/SCA Comprehensive TEE Examination and was developed to facilitate recognition of the most commonly cited cardiac abnormalities resulting in significant hemodynamic compromise (see Figure 1).Results: Since initiation of this protocol, the frequency of intraoperative rescue TEE has sharply increased each year. From May 2015 through January 2018 (33 months), we performed 183 intraoperative exams in noncardiac cases. Of these, 83 (45%) were performed on patients who were actively unstable or had potential to become unstable based on known medical history. Another 100 (55%) were performed for monitoring purposes indicated by the surgical procedure (e.g. venous air embolism monitoring for neurosurgery or cardiac or pulmonary evaluation for orthotopic liver transplant). Data were analyzed for demographics, ASA classification, surgical service, indication, findings, and interventions. Evaluative Rescue Echos were split approximately evenly between males and females and tended to occur in older patients and patients ASA class 3 and higher. The most common indications for rescue TEE were hypotension, cardiac arrest, and ST changes. A majority of exams were either normal or confirmed a known preoperative diagnosis, which is comparable to other studies of rescue echocardiography. The most commonly administered interventions in response to rescue echo findings were administration of vasopressors and volume and upgrading the level of care (see Charts 1-3). Conclusion: Intraoperative TEE can be a valuable tool to rapidly and confidently assess patients experiencing hemody-namic instability during non-cardiac surgery. In creating a Rescue Echo protocol, we hope to streamline the process from recognizing an intraoperative event to acquisition and interpretation of useful images and intervention. Future efforts will include continued tracking of Rescue Echo demographics and outcomes, clinician utilization and satisfaction surveys, and increased use of transthoracic echocardiography in cases where transesophageal echo is not indicated.231 285 Melissa M. Tanguay, B.S., Medicine - Cardiology Characterization of Differential Exercise Oxygen Uptake and Recovery Patterns in Middle-Aged Adults in the Research, Massachusetts General Hospital, Jamaica Plain, MA, USA, 2Boston University, Boston, MA, USA, 3Cardiology, Massachusetts General Hospital, Boston, MA, USA, 4Interventional Cardiology, Boston VA Healthcare System, West Roxbury, MA, USA, 5Cardiology, Boston VA Healthcare System, West Roxbury, MA, USA and 6Cardiology, Boston Children's Hospital, Boston, MA, USA Introduction: Patterns of oxygen uptake (VO2) upon initiation of exercise, in relation to workload, and during recovery, all reflect distinct aspects of how individuals adapt to acute exercise exposure. Each of these three VO2 kinetic patterns predict outcomes in referral populations of patients with established heart failure. However, none of these patterns have been characterized in a large non-referral community population and the relationship of these VO2 kinetic patterns to each other, and to traditional cardiovascular risk factors, remains uncharacterized. We hypothesized that in a community-based sample of middle aged individuals, delayed VO 2 kinetics upon initiation of exercise and lower VO2/workload slopes are associated with prolonged VO2 recovery kinetics and that these measures would correlate with cardiovascular risk factor burden. Methods: Maximal upright cycle ergometry cardiopulmonary exercise testing (CPET) was performed on 1865 Framingham Heart Study participants (age 55\u00b1 9 yrs., 54% BMI 28\u00b1 5, peak VO 2 23\u00b17 ml/kg/min). Breath-by-breath gas exchange measures were obtained during 3-minute unloaded exercise, incremental ramp exercise, and 4-minutes of recovery. Mean response time (MRT; 63% of duration to achieve steady state VO 2) and VO2/work slope starting 1-minute into incremental work were derived. VO2 recovery delay (VO2RD; time until post-exercise VO2 falls permanently below peak VO2) and T\u00bd (time for VO2 to decline by 50%, corrected for resting VO2) were measured to assess VO2 recovery kinetics. Because 232normative values for each of these variables in population studies are not established, we compared VO2 recovery kinetics among groups stratified by median MRT and VO2/W slope values. Below median MRT and above median VO2/W slope (MRT/VO2/W) and above median MRT and below median VO2/W slope (MRT/VO2/W) groups were compared. Results: Mean (\u00b1SD) values 30\u00b115s MRT, categorized the MRT/VO2/W group and 30% (n=550) in the MRT/VO2/W group. Mean (\u00b1SD) MRT and VO2/W values were 41\u00b111s and 8.2\u00b10.7ml/W, in the MRT/VO2/W group, respectively, and 17\u00b17s and 9.8\u00b10.5ml/W in the MRT/VO2/W group. Prolonged recovery kinetics were observed in the MRT/VO2/W group when compared with the MRT/VO2/W group (VO2RD (Figure). The differences in VO2 recovery kinetics persisted after adjustment for peak VO2, age, and sex (p<0.001 for both measures). In models adjusted for age and sex, MRT/VO2/W grouping remained closely aligned with cardiovascular disease risk factors and decreased cardiorespiratory fitness level (Table). Conclusion: These cross-sectional data indicate that longer VO2 mean response time and more shallow increment in VO2 relative to work during exercise are associated with delayed VO2 recovery kinetics among community-based adults. This relationship persisted after adjustment for peak VO2. Furthermore, in this initial characterization of VO2 kinetic patterns in the community, we found that VO2 increment upon initiation of exercise as well as VO2 augmentation relative to workload were closely related to risk factors for future development of cardiovascular disease. These data suggest that VO2 patterns beyond peak VO2 may help identify maladaptive responses to exercise in non-referral populations that are currently being tracked to assess their ability to independently predict future cardiovascular disease in the Framingham Heart Study. Figure. Differential oxygen uptake kinetics during unloaded and incremental ramp exercise and their relationships to delayed oxygen recovery kinetics following exercise. Displayed values for MRT, VO2/Work Slope, VO2RD and T \u00bd represent unadjusted mean values for each grouping. Table. Baseline characteristics, VO2 uptake and recovery measures, and peak exercise measures stratified by MRT/VO2/W group. Age and sex reported as unadjusted mean \u00b1 standard error or unadjusted %. All other data are reported as adjusted mean \u00b1 standard error, or adjusted %. Models for all other measures adjusted for sex and age, and recovery measures also adjusted for Peak VO2.233286 Abigail A. Testo, B.S., Psychiatry Changes in Neural Activity Following Ketamine Infusion in Individuals with Treatment Resistant Depression. A.A. Testo1,2, T. Deckersbach1,2, D. Ionescu2, and 2MGH, Boston, MA, USA Introduction: Ketamine has become a viable treatment for individuals with treatment-resistant major depressive disorder (MDD) (Zarate et. al, 2006). The mechanisms underlying the antidepressant effect of ketamine remain elusive. Reduced blood flow in the basal ganglia has been implicated in depression. (Lafer et. al, 1997). Ketamine, a glutamate receptor agonist, may affect the basal ganglia functioning through NMDA receptors located in the basal ganglia, in particular the caudate, putamen, and nucleus accumbens. Previous studies have linked increased neural activation in the caudate with anhedonia and altered regional cerebral metabolic rate (rCMRGLu) in the putamen (Murrough et al, 2015 & Lally et al, 2014). In addition, alterations in dorsal anterolateral prefrontal cortex functioning have been linked a decrease in glucose metabolism in depression. The purpose of this study is to to examine the relationship between ketamine infusion and changes in neural activation in the caudate, putamen, and nucleus accumbens as well as the dorsal prefrontal cortex using functional Magnetic Resonance Imaging (fMRI).Methods: Study participants were sixteen depressed individuals with with treatment resistant major depression stable on their medications for at least twenty-eight days. Patients were treated with Ketamine infusions at the Depression Clinical and Research Program (DCRP) at the Massachusetts General Hospital. For the neuroimaging part of the study, partici - pants completed depression severity assessments (modified Hamilton Depression Scale) and MRI scanning before and after ketamine infusion. Specifically, following the initial MRI scan participants received an open-label infusion of subanesthetic intravenous ketamine (0.5mg/kg over 40 minutes). Four hours after the ketamine infusion underwent a second MRI resting state scan. Results: We are currently analyzing the resting state MRI scans using Statistical Parametric Mapping software (SPM12). Following spatial preprocessing neural activation in the basal ganglia and prefrontal cortex before and after ketamine infusion will be compared using paired t-tests. Changes in basal ganglia and prefrontal cortex activation will be correlated with changes in cute depression severity before and after ketamine infusion. Conclusion: The results of the study will advance our current knowledge and understanding of the antideprssant effects of ketamine. Specifically, whether ketamine induced changes in acute levels of depression are mediated via changes in neural activation in the to a real-time, full dynamic range and high sensitivity quantitative optical imaging system for surgery assistance: the photon counting camera M. Tetrault1,2, H. Kang1,2, W. S. Kashiwagi1,2, G. El Fakhri1,2 and H. Choi1,2 1Radiology, Massachusetts General Hospital, Charlestown, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: Near-infrared (NIR) fluorescence imaging provides targeted dyes to help identify hard to see tissues or organs in the surgery room. This provides invaluable feedback to the medical staff and surgeons, for example where two dyes in conjunction can differentiate the thyroid and parathyroid glands. Further feedback comes from dye quantitative information, such as for evaluating changes in the lymphatic flow during lymphatic gland removal. In such a case the dye concentration is weak, and so is difficult to resolve compared to the background environment. Typical NIR cameras, based a photon integra - tion scheme, require long exposure times to resolve these low dye concentrations, impeding on responsiveness. Furthermore, long exposures cause image saturation in regions with medium to high dye concentration, limiting the upper dynamic range in the image. The user must therefore continuously choose a compromise between sensitivity, dynamic range and respon- siveness. To counter this problem, we explore in this report the concept of photon counting cameras for NIR optical imaging. The study will show that photon counting systems maintain full dynamic range and obtain better sensitivity than standard integration cameras even at low exposure times. Counting cameras thus bypasses the compromise concern in real-world surgical situations, removing a needless distraction from the operating team. Methods: Two sets of phantoms were prepared using ESNF10 (700 nm) and ZW800-1C (800 nm) NIR fluorescent dyes in glass capillary tubes. Each set had dye concentrations in logarithmic increments, from 10 nM to 10 \u00b5M, as well as a control tube with only solvent (DMSO). The phantoms were excited using 660 nm and 760 nm laser sources. A baseline quantita - tive reference was obtained using the KFlare system with 200 ms and 2000 ms exposure times. Quantitative imaging for an 234experimental photon counting system was obtained using a SPC3 demonstration camera unit from Micro Photon Devices (Italy), curtesy of Optoelectronic Components (Canada) and MIT. It was paired with a Navitar Zoom 6000 lens system and a Nikon Apo 4x microscope lens. NIR filters were manually inserted in the light collection path. The longpass filters cut off at either 720 nm or 795 nm depending on the excitation laser source. Photons were counted for 200 ms and 2000 ms intervals, and gain changed between minimum and maximum values for the KFlare where appropriate. To quantitatively compare the two cameras, images for each phantom sample were taken in a selected region of the field of view of each camera to minimize environmental and sensor divergences. The signal to background ratio was calculated for each phantom and each exposure/counting interval. Results: The signal to background ratios (SBR) with the KFlare system are presented in blue in Figures 1 and 2 for the 700 and 800 wavelengths, respectively. Saturated data points are highlighted in red. As expected, the SBR increases with exposure, and higher dye concentrations saturate at longer exposures. The SBR for the counting camera setup are presented in orange in the same figures, where in this case longer exposure time does not impact the SBR. The measurements also have no saturation phenomenon, as expected. The overall sensitivity was improved by a factor of 10 for 700 nm channel, and maintained for the 800 channel. This is particularly interesting because the counting camera has 2 and 4 times weaker detection efficiency than the KFlare for the wavelengths of interest. Overall, this means that the photon counting system can provide real-time feedback to surgeons, directly in the operating room, with high sensitivity and fast responsiveness, without the need to adjust any setting throughout the procedure. Conclusion: The experimental imaging system shows that counting cameras improve sensitivity and maintain full dynamic range at fast frame rates, opening the path to real-time, quantitative optical imaging for surgery assistance. The users no longer need to concern themselves with various camera settings and can focus on the patient. While the camera unit used in this work has poor light collection and low efficiency in the NIR region, it still provides equivalent or improved SBR over NIR optimized high-end cameras thanks to the photon counting approach. This shows that counting cameras are a very attractive solution for future surgery assistance imaging system development, for example using NIR-optimized counting camera sensors. Figure 1 - SBR for 700 nm phantoms. Figure 2 - SBR for 800 nm phantoms. Jessica Thanos, Center for Genomic Medicine (CGM) pruning for Quantitative Health, Center for Genomic Medicine and Department of Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Department of Psychiatry, Harvard Medical School, Boston, MA, USA, 3Department of Physiology and Pharmacology, Karolinska Institutet, Stockholm, Sweden and 4Chemical Biology Program, Broad Institute of Harvard and MIT, Cambridge, MA, USA Introduction: Synaptic pruning (elimination of synapses) is critical for neurodevelopment and aberrant synaptic pruning is implicated in neurodegenerative and neuropsychiatric disease. A comprehensive and scalable human cellular model is required in order to elucidate the role of aberrant synaptic pruning in human disease pathogenesis. Methods: We generated patient monocyte-derived microglia-like cells (iMGs), analyzed marker and gene expression, and characterized functional activity by direct co-culture with iPSC-derived neurons. We isolated synaptic terminals (synaptosomes) from pure neural cultures, determined synaptic labeled synaptosomes to measure engulfment of synaptic terminals in real time, and performed blocking experiments to determine whether iMG engulfment activity is complement-dependent. Samples were drawn from NeuroBank, an MGH tissue bank of more than 400 patient-derived cell lines linked to structured clinical interview, neurocognitive testing, and electronic health records. Results: iMGs resemble primary microglia (MG) in that they express MG-specific markers and genes. Immunostaining analysis shows the presence of fewer dendritic spines in neurons directly co-cultured with iMGs, suggesting that iMGs are functionally active. Synaptosomes express synaptic markers and are functionally active after freezing and storage. Real-time imaging and immunostaining of labeled synaptosomes with iMGs indicate engulfment of synaptic structures by iMGs. Conclusion: We have developed and validated a platform to model synaptic pruning in vitro using patient-derived iMGs and synaptosomes, facilitating future studies of disease phenotypes and functional high-throughput screening for drug discovery. 289 Emily L. Thorn, Neurology Focal aberrations in Chu4,2 1Neuroradiology/Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Boston University School of Medicine, Boston, MA, USA and 4Neurology, Massachusetts General Hospital, Boston, MA, USA Introduction: Epileptiform discharges are potentiated by sleep in several epilepsy syndromes, but the cause of this phenom- enon is not well understood. The thalamus is an important brain nucleus with extensive cortical connections and intimately involved in regulation of sleep physiology. Previous work has found that early thalamic injury may predispose patients to sleep-potentiated spikes (Fernandez et al. 2012, Leal et al. 2018), but the mechanism of this relationship is not known. Benign childhood epilepsy with centrotemporal spikes (BECTS) is a common pediatric epilepsy syndrome characterized by stereotyped sleep-potentiated spike activity present independently in bilateral primary sensorimotor cortices. As BECTS presents during a period of dramatic white matter development, we hypothesized that alterations in thalamocortical connec - tivity to the primary sensorimotor seizure onset zone (SOZ) would be present in children with BECTS and correlate with spike burden during sleep.Methods: 23 children with BECTS and 19 healthy controls were recruited for this study. Four subjects returned for longitu - dinal scans. Subjects underwent 3 Tesla structural and diffusion-weighted magnetic resonance imaging (2mm x 2mm x 2mm) with gradient 72 electrode sleep-deprived electroencephalographic (EEG) recordings. EEG spikes were manually marked during all available non-REM sleep epochs and summed over the time interval (range 0.002-0.625 spikes per second). Seed and target regions of interest (ROIs) were created within each hemisphere using the Desikan-Killiany atlas, with the thalamus set as a seed ROI, and SOZ cortex and non-SOZ (NSOZ) cortex as target ROIs. To infer the structural connectivity between thalamic and cortical ROI pairs, probabilistic tractography was executed using Probtrackx2 (Behrens et al. 2007) with 500 streamlines per seed voxel, 0.5 millimeter steps, and a curvature threshold of 0.2. All streamlines reaching the target ROI were summed and normalized by seed voxel count. Results for BECTS and healthy controls were plotted by age. The slope of thalamocortical connectivity versus age was computed for each group and compared between groups using nonparametric bootstrap analysis. We also compared thalamocortical connectivity with spike burden using a linear regression model, controlling for age. Results: We found a significant difference in the developmental trajectory of thalamocortical connectivity to the SOZ in BECTS cases compared to healthy controls (p=0.014), where the increase in connectivity with age observed in healthy controls was not present in BECTS children. These results did not extend to NSOZ thalamocortical connections (p=0.192). Longitudinal results support these observations, where all BECTS cases who underwent repeat imaging (n=4) showed a decrease in thalamocortical connectivity to the SOZ over the follow-up period. No relationship was found between thalam - ocortical connectivity and spike burden (p=0.840). Conclusion: These results suggest that subtle aberrations in white matter development may underlie disease progression in BECTS. Thalamocortical connectivity does not appear to directly mediate sleep spike potentiation in BECTS.236 Figure 1. Thalamocortical tractography processing pipeline. A) High resolution structural and diffusion MRIs are acquired. B) Structural MRIs are used to generate cortical and thalamic labels (top). Diffusion MRIs are used to extract diffusion parameters per voxel from 64 gradient directions (example principal directions of diffusion for 25 voxels shown in inset). C) Distribution of diffusion parameters is repeatedly sampled to infer the probability of white matter tracts between ROIs. Figure 2. Relationship between thalamo-SOZ connectivity index and age. A) Visual analysis reveals a relationship between age and CI to the SOZ among healthy controls. Solid line indicates linear regression model fit, dashed lines indicate 95% confidence intervals. B) Visual analysis reveals no relationship between age and CI to the SOZ among BECTS subjects. C) Bootstrap analysis reveals a significant difference between the slopes of the healthy controls and BECTS subjects (p=0.0123) D) Visual analysis reveals a no relationship between age and CI to the NSOZ among healthy controls or E) BECTS. Solid line indicates linear regression model fit, dashed lines indicate 95% confidence intervals. F) Bootstrap analysis reveals no difference between the slopes of NSOZ CI between the healthy controls and BECTS subjects (p=0.19). 290 Mabel Toribio, M.D., Medicine - Endocrine-Neuroendocrine Assessing Statin Effects on Cardiovascular Pathways in HIV Using a Novel Proteomics Approach: Analysis of Data From INTREPID, a Randomized Toribio1, Medicine, General Hospital, Boston, MA, USA, 2Inova Heart And Vascular Institute, Fall Church, VA, USA, 3Kowa pharmaceuticals, Montgomery, MD, USA, 4Biostatistics Center, Massachusetts General Hospital, Boston, MA, USA, 5Olink Proteomics, Watertown, MA, USA, 6AIDS Research Consortium of Atlanta, Atlanta, GA, USA and 7Internal Medicine, Icahn School of Medicine at Mount Sinai, New York, NY, USA Introduction: People with HIV (PWH) demonstrate increased cardiovascular disease (CVD), due in part to increased immune activation, inflammation, and endothelial dysfunction.Methods: In a randomized trial (INTREPID), 252 HIV-infected participants with dyslipidemia and no history of coronary artery disease were randomized (1:1) to pitavastatin 4mg vs. pravastatin 40mg for 52 weeks. Using a proteomic discovery approach, 92 proteins biomarkers were assessed using Proximity Extension Assay technology to determine the effects of statins on key atherosclerosis and CVD pathways among PWH. 225 participants had specimens available for biomarker analysis pre- and post-baseline.Results: The mean age was 49.5\u00b1 8.0 (mean\u00b1 SD), 25 mg/dL and CD4 count 620\u00b1 243 cell/mm 3. Among all participants, three proteins significantly decreased: tissue factor pathway inhibitor [Gal-4; t-statistic=3.50, FDR p-value=0.01] and insulin-like growth factor binding 2 [IGFBP-2; t-statistic=3.21, FDR p-value=0.03]. The change in TFPI was signifi-cantly different between the pitavastatin and pravastatin groups. Among all participants, change in TFPI related to the change in LDL-C (r=0.43, P<0.0001) in Lp-PLA2 (r=0.29, a proteomics approach, we demonstrated that statins led to a significant reduction in the levels of TFPI, PON3, and LDLR and an increase in Gal-4 and IGFBP-2, key proteins involved in coagulation, redox signaling, oxidative stress, and glucose metabolism. Pitavastatin led to a greater reduction in TFPI than pravastatin. These data highlight potential novel mechanisms of statin effects among PWH.237 291 Landy Torre Flores, MD, Medicine - Endocrine-Neuroendocrine Verbal Memory and Executive Function in Adolescent Girls and Young Women with Severe Obesity C. Baskaran1,2, L. Torre Flores1, F. Rickard1, Unit, MGH, Boston, MA, USA, 2Endocrinology, Boston Children's Hospital, Boston, MA, USA, 3Pediatric Endocrine Unit, MGH, Boston, MA, USA, 4Eating Disorders Clinical and Research Program, MGH, Boston, MA, USA and 5Radiology, MGH, Boston, MA, USA Introduction: There is a global increase in the prevalence of obesity in children and adolescents, especially in girls. Adoles- cence is a critical period of neurocognitive development, particularly executive function and memory. Obesity has been linked to both metabolic and neurocognitive comorbidities; however, data exploring cognitive changes in adolescents with severe obesity are limited. Our objective was to compare the cognitive indices of verbal memory and executive function in female adolescents and young adults with severe obesity vs. normal-weight controls. We also performed exploratory analysis to examine changes in cognitive indices following bariatric surgery.Methods: Thirty-four adolescent females with severe obesity (BMI kg/m 2 or BMI >35 kg/m2 with comorbidities) (OB) and 23 adolescent females of normal weight (NW) between the ages of 14 and 25 years underwent medical and neurocognitive assessment, including the Wechsler Abbreviated Scale of Intelligence (WASI) for assessing intelligence, Californian Verbal Learning Test II (CVLT II) for assessing verbal memory, and the Delis-Kaplan Executive Function System Color-Word Interference Test (D-KEFS) for assessing executive function. Eighteen subjects with severe obesity had additional follow up at 12 months with repeat neurocognitive testing and medical assessment. Ten of these had bariatric surgery after the baseline visit and 8 were followed with usual non-surgical care. Results: NW girls had a median age of 19.4 years (18.9 - 20.1) (IQR) and a BMI of 21.6 kg/m 2 (20.1 - 25.9), whereas OB had a median age of 17.6 (15.5 - 19.8) years and a median BMI of 44.9 kg/m2 (38.2 - 48.5). WASI intelligence scores, CVLT-II trial scores that assess short-term and long-term recall (verbal memory), and D-KEFS inhibition and inhibition switching scores that assess response inhibition and the cognitive flexibility aspect of executive function were significantly lower in OB vs. NW : (i) CVLT-II Trial 1: 37.5 (35.0 - 45.0) vs. 45 (40.0 - 60.0) (p<0.0001), Trial 60 - 60.0) (p=0.0002), Trials 56 - 60.0) (p<0.0001), Trial B: - 45.0) 50 - 60.0) (p=0.003), Short Delay Free 50.0) vs. 55 (48.8 - 60.0) and Long Delay Free Recall: and 55 (51.0 - 57.0) (p=0.0012). Verbal memory and executive function scores remained lower in OB even after controlling for age and baseline intelligence. In our exploratory analysis, adolescents who had bariatric surgery had a trend for improved performance on the long-term recall (0.06) and Trials 1-5 (0.09) measures of verbal memory compared with the no-treatment group. Conclusion: Female adolescents and young adults with severe obesity demonstrate suboptimal performance in tests of verbal memory and executive function compared with healthy normal-weight female adolescents and young adults. Further studies are necessary to comprehensively evaluate changes in cognitive function following bariatric surgery in female adolescents with obesity, and the determinants of such changes.238292 Alexandra Touroutoglou, PhD, Neurology Using visual assessment of atrophy to predict genetic mutation in individual patients with familial Frontotemporal Dementia A. Touroutoglou1,2, S. McGinnis1,2,3, M.J. and B. Dickerson1,2 1Neurology, Harvard Medical School/Masschusetts General Hospital, Boston, MA, USA, 2Radiology, Martinos Center for Biomedical Imaging, Charlestown, MA, USA and 3Neurology, Harvard Medical School/Brigham and Women's Hospital, Boston, MA, USA Introduction: Improving clinicians' ability to predict genetic mutations causing Frontotemporal Dementia (FTD) is useful for diagnostic evaluation and potentially referral to appropriate studies including therapeutic trials. Familial FTD cases are caused by mutations in one of the three genes: microtubule-associated protein tau (MAPT), chromosome 9 open reading frame 72 (c9orf72) and progranulin (GRN). Each of these genetic mutations is associated with a distinct pattern of neurodegener - ation. MAPT has been associated with atrophy predominantly in the temporal lobes; c9orf72 regions (Whitwell et al., 2012; Rohrer et al., 2010). The main goal of this study was to assess whether a visual assessment of atrophy (through visual inspection of MRI scans) can predict genetic mutations in FTD patients spanning different clinical diagnoses. Methods: The analysis included 15 symptomatic participants with FTD. A three-step algorithm was developed including visual rating scales of regions in the dorsolateral prefrontal cortex, lateral parietal, and anterior temporal pole. Structural MRI scans were visually rated by two raters (one experienced and one novice rater) blinded to subject diagnosis and genetic mutation. Prediction accuracy was measured with the percentage of correct predictions. Inter-rater agreement score was calculated with intraclass correlation coefficient.Results: Each rater spent 2 minutes to apply the three-step algorithm and visually rate each MRI scan. The experienced rater had 100% prediction accuracy for MAPT, 83% prediction accuracy for c9orf72 and 60% prediction accuracy for GRN. The novice rater had 75% prediction accuracy for MAPT, 50% prediction accuracy for c9orf72 and 40% prediction accuracy for GRN. Inter-rater agreement score was 0.66. Conclusion: Our findings provide preliminary evidence suggesting that simple-to-use visual scales can be useful in individual patients to predict the specific mutations in the three major genetic variants of familial FTD. 293 Jane M. Tsui, MD, Surgery - Plastic & Reconstructive Preventing Contracture in Full Thickness Austen1 Surgery, Massachusetts General Hospital, Boston, MA, USA and 2The Wellman Center for Photomedicine, Boston, MA, USA Introduction: Wound contracture is a debilitating complication resulting from excessive myofibroblast activity during wound healing. Currently, no treatment to prevent contracture exists. Photochemical Tissue Passivation(PTP) occurs when tissue is coated with photosensitive dye and exposed to visible light. PTP has been shown in other animal models to decrease fibroblast-mediated collagen contraction, decrease myofibroblast activity and strengthen tissue. We hypothesized that PTP treatment to full-thickness wounds would significantly decrease wound contracture morbidities by strengthening the wound bed with cross-linked collagen and limiting the myofibroblast response.Methods: Thirty-two C57BL/6 mice were randomized to the untreated group (n=16) or the PTP treatment group (n=16). 1x1 cm full-thickness excisional wounds were created on the dorsum of all mice. PTP wound beds were painted with 0.1% Rose Bengal solution, a photosensitive dye, and exposed to visible light at a fluence of 60J/cm 2. Wounds were serially photographed for 6-weeks to measure percent contracture. At 7, 14, 21, and 42 days post-operatively, animals were euthanized and wound skin was harvested for histological review by a dermatopathologist. Results: By the end of study, all wounds were healed. Treated wounds contracted significantly less than controls. At 7 days, control groups showed nearly 20% more contracture (67.1\u00b117.1% vs 80.3\u00b18.5%; p=0.014), and over more (27.8\u00b18.6% vs 50.3\u00b111.9%, p<0.05). At 21 days, PTP wounds were 1.05-fold less contracted (p<0.05). At 42 days, control wounds contracted to 13.6\u00b1 PTP wounds to 35.2\u00b1 and increased dermal collagen development and ingrowth, neovascularization, and development of skin appendages compared to controls.Conclusion: PTP significantly limits contracture in full-thickness excisional wounds, and may accelerate and improve wound healing.239 294 Chandler Tucker, Center for Genomic Medicine (CGM) Night eating among night shift workers: preliminary results from the SHIFT Study C. 1Center for Genomic Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Department of Integrative Physiology, University of Colorado, Boulder, Boulder, CO, USA, 3Department of Physiology, University of Murcia, Murcia, Spain and 4Division of Sleep Medicine, Harvard Medical School, Boston, MA, USA Introduction: With over 15 million adults working night shifts in the US, it has become increasing important to identify risk factors that mediate the link between night shift work and the increased risk of metabolic disorders and particularly type 2 diabetes. Mistimed food intake, i.e. food consumption during the biological night, has recently been shown to disrupt the synchrony across biological rhythms in humans, and has been linked to increased adiposity. Studies indicate that night shift workers have similar total energy intakes compared to day workers, however differences in the diurnal distribution of energy intake has not been explored extensively yet. We test the hypothesis that night shift workers have shifted caloric intakes towards later times on work days compared to day shift workers in the Shift work, Heredity, Insulin, and Food Time (SHIFT) Study (Study NCT02997319), a multicenter, large, observational study.240Methods: Here, we describe preliminary results from the first 16 night shift and 16 sex- and age- recorded food intake noting food type and quantity for 2 weeks and self-reported timing of meal intake via questionnaire. Results: Consistent with earlier observations, total energy intake and percent energy intake from carbohydrate, fat, and protein were similar between night shift and day workers (P >0.05). On work days, night shift workers consumed meals later on in the day compared to day workers, most notably dinner, the most energy dense meal of the day, and the last eating episode were consumed 4 and 7 hours later than day workers, respectively (dinner: day, 7:13pm vs. night, 11:12pm; last eating episode: day, 8:36pm vs. night, 3:23am). On non-work days, however, food intake times for all meals were similar between night shift and day workers. Conclusion: These observations support the hypothesis that night shift work may lead to mistimed food intake coinciding with the biological night, and may mediate the link between night shift work and obesity and type 2 diabetes. When completed, findings from the SHIFT Study will be able to delineate the influence of different night shift schedules on mistimed food intake exposure. 295 Sarah E. Turbett, MD, Pathology The development of a laboratory screening algorithm for Anaplasma phagocytophilum polymerase Massachusetts General Hospital, Cambridge, MA, USA, 2Medicine, Yale University, New Haven, CT, USA, 3Medicine, Johns Hopkins University, Baltimore, MD, USA and 4Pathology, University of Minnesota Medical School, Minneapolis, MN, USA Introduction: Human granulocytic anaplasmosis (HGA) is an emerging tick-borne infection caused by the bacterium Anaplasma phagocytophilum. Symptoms include fever, headache, and myalgias, and typical laboratory findings include leukopenia and thrombocytopenia. Polymerase chain reaction (PCR) is the preferred method for the diagnosis of acute infection, with a sensitivity and specificity nearing 100%. At our institution, Anaplasma PCR is overutilized with only a 3-4% positivity rate. To improve this, a phase 1 retrospective analysis was performed to determine if common complete blood count (CBC) laboratory parameters could indicate the absence of Anaplasma infection. A screening algorithm was then created and prospectively evaluated during phase 2 of the study through a mock stewardship protocol. Methods: Phase 1: Anaplasma PCR tests were included over a 3-year period (MGH IRB 2014P000623). Median CBC values were calculated using R Studio and GraphPad Prism and the significance of differences between PCR-positive and -negative cases was determined using the Mann-Whitney test. Screening criteria were calculated by a pragmatic receiver operating characteristic space analysis to generate optimal cutoff thresholds for test rejection. Phase 2: Mock stewardship was performed over a six-month period. Daily reports of the white blood cell (WBC) and platelet (PLT) counts associated with each Anaplasma PCR test were reviewed by microbiology leadership. If a CBC was not obtained by the ordering clinician, an off-line CBC was performed on the sample collected for Anaplasma PCR testing. PCR testing was \"accepted\" if the rejection criteria were not met. If the sample met laboratory criteria for \"rejection\", a committee comprised of infectious diseases specialists, microbiologists, and pathologists reviewed the case history to determine if approval should be granted based on clinical criteria. Results: Phase 1: Of the 2166 available results, 81% had an associated CBC within three days of Anaplasma PCR specimen collection. Patients with a positive Anaplasma PCR had significantly lower median WBC (4,900 p<0.0001) counts than those with a negative result (Figure 1). Combining criteria of a WBC greater than or equal to 11 K/\u00b5L and PLT count greater than or equal to 300 K/\u00b5L provided rejection criteria with 100% sensitivity and 25% specificity after excluding immunocompromised or clinically unstable patients (Figure 2). Phase 2: 663 Anaplasma PCR tests were analyzed over the 6-month period. Of those, 155 (23%) met CBC rejection criteria and were reviewed by committee (Table 1). Of those, 110 (71%) tests were mock refused and 45 (29%) were mock accepted based on clinical criteria. Of the 25 positive Anaplasma PCR tests during this time, only one (4%) met the CBC rejection criteria and committee review resulted in mock refusal. On review, the patient was completing treatment with doxycycline (treatment for Anaplasmosis) and had resolution of her symptoms, indicating limited utility of testing. None of the 45 samples that were mock accepted by clinical criteria was positive by PCR, demonstrating no additional benefit of chart review. Conclusion: In clinically stable and immunocompetent patients, a CBC-based-stewardship 11 K/\u00b5L K/\u00b5L) was sensitive in excluding Anaplasma infection. Implementation of this algorithm would improve 241Anaplasma PCR test utilization by reducing unnecessary testing by 23%. An upcoming Phase 3 study will prospectively implement stewardship of Anaplasma PCR testing using these criteria. Figure 1. Phase 1 differences in CBC laboratory values in patients with positive versus negative Anaplasma PCR test results. The shaded area represents the normal range, and the dotted horizontal line represents the threshold criteria. The open circles represent three immunocompromised or critically ill patients who tested positive for Anaplasma but had lab values above the thresholds. P values were determined with the Mann-Whitney test. Figure 2. Screening algorithm for Anaplasma PCR testing. 296 Brooks V. Udelsman, MD/MHS, Surgery Goals of Care Concordance Among Patients and Health Surrogates in the Perioperative Setting B.V. Udelsman1, N. Govea2, Z. Cooper3, A. Bader4 and M. Meyer5 1Surgery, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Surgery, Brigham and Women's Hospital, Boston, MA, USA, 4Anesthesiology, Brigham and Women's Hospital, Boston, MA, USA and 5Anesthesiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Under sedation patients temporarily surrender decision making capacity. Thus, the health decision surrogate can have an especially important role in the perioperative period. Prior studies in a general medical population and amongst patients in the intensive care unit have demonstrated poor concordance between patients and their surrogates. The aim of this study was to determine the degree of concordance in the perioperative setting and to identify areas for improvement. Methods: Prospective cohort study set in the preoperative clinic. Patients and their surrogates (dyads) who presented to the preoperative clinic were eligible for participation. Patients who presented without a surrogate were excluded. Dyads were asked about the care preferences of the patient. Answers to multiple choice questions provided by patients were compared to those of surrogates to determine concordance in seven domains: resuscitation, intubation, hemodialysis, artificial nutrition, physical disability, cognitive disability, and chronic pain. Concordance was defined as correct prediction of patient treatment preference by the surrogate. The chosen domains listed above are typically evaluated in advance care directives and we assessed them using a previously validated survey. Dyads were also surveyed on socio-demographics, self-rated quality of life, and depression using the PHQ-2. Clinical factors were obtained through chart review. Results: We identified 35 dyads, of which 32 pairs had completed the full survey. The median patient age was 68. The majority were white (90%), had graduated high-school (97%), and had an ASA score of 3. Most surrogates were either a spouse (78%) or an adult son/daughter (16%). The majority of patients (78%) and surrogates (84%) reported having prior conversations regarding the patients' goals of care. Most patients (84%) reported being \"very confident\" in their surrogates understanding of their health care preferences, while most surrogates (84%) reported similar confidence in their knowledge of the patient's preferences. Concordance ranged over the major treatment domains with a high of 81% regarding resuscitation to a low of 34% regarding artificial nutrition. Concordance tended to be better amongst dyads who reported prior conversations, but this did not reach statistical significance.Conclusion: Concordance between patients and surrogates regarding major treatment preferences is poor in the perioperative setting. This discordance could lead to serious issues in patient care including unwanted interventions and withholding desired interventions. Conversations regarding treatment preferences does not significantly effect concordance. Interventions aimed at simply encouraging discussion between patients and surrogates may not be enough to improve concordance.242Percent concordance between surrogates and patients regarding major treatment preferences. Percentages reflect missing data from incomplete surveys in some areas. 297 Nisha Udupa, BS, Medicine - Endocrine Increasing 25-hydroxyvitamin D levels over time: The Study of Women's Health Across the Nation (SWAN) D. Mitchell1, K. Ruppert2, N. Udupa1, F. Bassir1, J. Finkelstein1 and S. Burnett-Bowie1 1Endocrine, Massachusetts General Hospital, Boston, MA, USA, 2University of Pittsburgh, Pittsburgh, PA, USA, 3Brigham and Women's Hospital, Boston, MA, USA and 4University of California at Los Angeles, Los Angeles, CA, USA Introduction: The importance of vitamin D for bone health as well as its potential role in non-skeletal health has garnered much recent attention. Population-based studies investigating temporal trends in 25-hydroxyvitamin D (25OHD) have reported conflicting results. Our goal was thus to investigate changes in mean 25OHD concentration over time and predic- tors of these changes in the Study of Women's Health Across the Nation (SWAN). Methods: SWAN is a multi-site, longitudinal, community-based cohort study that enrolled women who were pre- or early perimenopausal at baseline. Participants were recruited at 7 sites; each site recruited Caucasian women, and women of additional racial/ethnic groups were recruited as follows: Black women in Boston, Chicago, Detroit, and Pittsburgh, Japanese women in Los Angeles, Hispanic women in New Jersey, and Chinese women in Oakland. We measured 25OHD in 1585 women in 1998-2000 (at age 49 \u00b13 years) and again in 2009-2011 (at age 60 \u00b13 years) by liquid chromatography-tandem mass spectrometry in a single batch. Other measures were assessed by standardized interview-administered questionnaires.Results: Over this interval, the mean 25OHD concentration increased 6.5 ng/mL (p<0.001), from 21.6\u00b19.8 to 28.1\u00b111.5 ng/mL (p<0.001)[MDMM1]. As expected, baseline mean 25OHD concentration varied by race/ethnicity of the women (14.0 ng/mL (Black women), 25.4 ng/mL (White women), 19.8 ng/mL (Chinese women), 18.3 ng/mL (Hispanic women), and 24.0 ng/mL (Japanese women) (p<0.001)). Additionally, the magnitude of increase differed between groups, ranging from 5.3 ng/mL (White women) to 8.5 ng/mL (Chinese women) (p<0.001). However, in a mixed model adjusted for age, BMI, educational attainment, household income, language use, insurance status, and season of blood draw, White and Japanese women had a significantly smaller increase in 25(OH)D compared to Black women (3.2 and 2.7 ng/dL smaller increase per 11 years, p<0.001 and p=0.005, respectively). [MDMM2] The observed increases in 25OHD did not vary by other demographic factors including country of origin, household income, language use, educational attainment, medical insurance status, or BMI category. The number of subjects reported taking a multivitamin or vitamin D supplement grew from 40.8% at the baseline visit, to 66.8% at the follow-up visit. The increase in 25OHD was higher among those either maintaining or initiating supplement use (7.7 ng/mL and 10.2 ng/mL, respectively) compared to those who never took supplements or discontinued use (1.7 ng/mL and 0.8 ng/mL, respectively) (p<0.001). Using a stringent definition of vitamin D deficiency of 25OHD 12 ng/mL, 20.4% of the cohort was deficient at baseline. Compared with the women with baseline 25(OH)D>12 ng/dL, the women with deficiency were slightly younger (48.0 vs. 48.7 years, p<0.001), had a higher BMI (31.5 vs. 27.1 kg/m2, p<0.001), were more likely to have been born in the US (p<0.001), were more likely to be Black and less likely to be White, Chinese, or Japanese, and tended to have lower household income and educational attainment. They were also less likely to report using a vitamin supplement (45.9 vs 20.7%, p<0.001). Of these women, 27.5% remained severely deficient at follow-up. Predictors of remaining deficient at follow-up included higher baseline BMI (p=0.046), race/ethnicity (p=0.001), household income (p=0.049), and lack of vitamin use (p<0.001[MDMM3]). [BS4] In a multivariable logistic model including age, educational attainment, change in BMI, study site, and Visit 12 season of blood draw, race/ethnicity and vitamin use were independent predictors of remaining severely deficient. Specifically, compared with Black women, White women were 4.7 fold less likely to remain severely deficient at Visit 12. Compared with women who never used vitamin supplements, those who either started after Visit 2 or took vitamins throughout were 4.6 and 5.9 fold less likely to remain severely deficient at Visit 12. Overall, at follow-up, 9.7% of women were deficient; deficiency was particularly pronounced among Black women at 25.4%.Conclusion: In summary, we observed an increase in average 25OHD concentration as well as a decrease in the proportion of subjects with vitamin D deficiency in this observational cohort over an approximately 11-year interval. Race/ethnicity and lack of vitamin use were major determinants of changes in 25OHD concentration and predictors of remaining vitamin D 243deficient at follow-up. All demographic subgroups had similar absolute increases in 25OHD. However, a sizeable proportion of women continued to have 25(OH)D concentrations low enough to put them at risk for skeletal complications of vitamin D deficiency. These data, if replicated in other cohorts, may prompt revision of current public health guidance and suggest that screening asymptomatic individuals for low 25(OH)D concentrations may facilitate targeted supplementation strategies to improve bone health. 298 Nneka Ufere, MD, Medicine - Gastroenterology Physicians' Perspectives on Palliative Care (PC) for Patients with End-Stage Liver Disease (ESLD): A National Survey Study N. Ufere1, J. Donlan1, L. Waldman1, J. Dienstag1, R. Chung1 and A. El-Jawahri1 1Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Medicine, Newton-Wellesley Hospital, Newton, MA, USA and 3Medicine, Brigham and Women's Hospital, Boston, MA, USA Introduction: Despite evidence demonstrating a benefit of PC in other chronic diseases, it is underutilized for patients with ESLD. We sought to examine hepatologists' and gastroenterologists' perceptions and attitudes about PC for patients with ESLD. Methods: We conducted a cross-sectional survey of physicians who provide care to patients with ESLD recruited from the American Association for the Study of Liver Diseases membership directory. Using a questionnaire adapted from prior studies, we examined physicians' attitudes about PC and perceptions of patients' reactions to PC. We identified predictors of physicians' attitudes about PC using linear regression.Results: Approximately one-third of eligible physicians (396/1236, 32%) completed the survey, of whom 237 (60%) were transplant hepatologists and 314 (79%) practiced primarily in a teaching hospital. Most physicians (86%) agreed that ESLD is a terminal condition, 95% believed that centers providing care to patients with ESLD should have PC services, and 86% trusted PC clinicians to care for their patients. Most believed that a PC clinician (88%), as opposed to a hepatologist (27%), is the best person to provide PC to patients with ESLD. However, only a minority of physicians reported collaborating frequently with inpatient (32%) or outpatient (11%) PC services. Most believed that when patients hear the term PC, they feel scared (94%) and anxious (87%) (Figure 1). A majority believed that patients would think nothing more could be done for their underlying disease (83%) or would think that their doctor had given up on them (66%) if a PC referral was suggested (Figure 2). Physicians with < 10 years in clinical practice (B=0.86, P=0.008), those who believed that ESLD is a terminal condition (B=1.09, P<0.01), and those who had a more positive perception of the quality of PC (B=0.55, P<0.001) reported more positive attitudes about PC. Conclusion: Although most physicians believe that patients with ESLD should have access to PC, they reported rarely collab- orating with PC and they had substantial concerns about patients' perceptions of PC. Interventions are needed to overcome misperceptions of PC and promote collaboration with PC clinicians for patients with ESLD. 244299 Sebastian Massachusetts General Hospital, Boston, MA, USA and 2Neurology, Yale School of Medicine, New Haven, CT, USA Introduction: Oral Anticoagulation Therapy (OAT) resumption after intracerebral hemorrhage (ICH) represents a high-stakes clinical dilemma due to increased risk of recurrent ICH, particularly after lobar ICH caused by underlying Cerebral Amyloid Angiopathy (CAA). Apolipoprotein E (APOE) 2/4 alleles are risk factors for recurrent CAA-related ICH. APOE genotype may therefore help guide decision-making regarding OAT resumption after lobar ICH. We hypothesized that APOE genotype predicts lobar ICH recurrence after OAT-ICH.Methods: We included survivors of lobar OAT-ICH, enrolled in a prospective U.S.-based single-center study conducted at Massachusetts General Hospital. We sought to determine whether APOE 2/4 genotype: 1) is associated with OAT-ICH recurrence time-to-event univariable (Log-rank) and multivariable (Cox model) analyses; 2) conferred a clinically significant improvement in OAT-ICH recurrence prediction, as defined by recursive partitioning analysis (RPA); and 3) improved clinical prediction of OAT-ICH recurrence when added to established clinical information (blood pressure during follow-up) and MRI-based CAA markers (cerebral microbleeds, white matter hyperintensities, expanded peri-vascular spaces, and cortical superficial siderosis) associated with ICH recurrence risk, by utilizing Harrell's C-statistic and Receiver Operator Characteristics (ROC) analyses.Results: We followed 110 lobar OAT-ICH survivors for a median time of 52.2 months (IQR: 33.5-61.7). We observed 26 recurrent ICH events, corresponding to an annual recurrence rate of 5.7%. APOE 2/4 variants were associated with OAT-ICH recurrence ( HR 2.14, 95% CI 1.20-3.82; 4: HR 1.50, 95% CI 1.21-3.47). These associations remained significant in multivariable analyses after adjusting for blood pressure during follow-up, anticoagulation status, antiplatelet use, cerebral microbleeds, white matter hyperintensities, expanded peri-vascular spaces, and cortical superficial siderosis. RPA revealed that APOE 2/4 conferred a significant risk for OAT-ICH recurrence. Specifically, there was a 5.9% (95% CI 3.3-8.7%) recurrence rate at 2 years and 9.5% (95% CI 4.4-14.2%) at 5 years among individuals with otherwise low recurrence risk based on neuroimaging data alone (Figure 1). Inclusion of APOE genotype in a risk model containing clinical and MRI data resulted in improved predictive ability for OAT-ICH recurrence (Harrell's C: 0.76 vs 0.68, p=0.034 and Area Under the Curve in ROC analyses for 5-year ICH recurrence 0.77 predicts lobar OAT-ICH recurrence, and can improve predictive performance when added to established clinical and neuroimaging markers associated with ICH recurrence. The utility of APOE genetic screening should be investigated in future studies of OAT-ICH. 245300 Olivia Van Benschoten, B.S., Psychiatry Development of a Multicomponent Psychological Mobile Application (App) for Patients with Acute Myeloid Leukemia (AML) O. Van Benschoten, E. Wright, V. Vacaro, J. Greer, J. Temel and A. El-Jawahri Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: Patients with a new diagnosis of AML confront a sudden and life-threatening illness, requiring an immediate disruption of their life and 4-6 week hospitalization to initiate intensive chemotherapy. During this hospitalization, they endure substantial physical symptoms and psychological distress as they struggle with the abrupt onset of the illness, uncertainty regarding their prognosis, social isolation, and loss of independence. Interventions to address the psychological distress in this population are lacking. Thus, we developed a psychological mobile app to promote effective coping and improve outcomes of patients with newly-diagnosed AML.Methods: We used a five-step app development process to create a self-administered, multi-component psychological intervention app targeting patients' needs during the 4-6-week hospitalization following a new diagnosis of AML. Patients, caregivers, oncologists, psychologists, and nurses served as stakeholders and were involved in all of the steps of to develop the app, including 1) developing a conceptual framework for the psychological intervention based upon prior literature; 2) creating a comprehensive intervention manual that incorporates medical information, psychoeducation, and psychosocial skill-building that corresponds with the illness trajectory; 3) translating the intervention manual into a mobile app storyline with substantial gamification to enhance patient engagement with intervention; 4) creating initial wireframes of the app modules; and 5) refining the app through iterative alpha and beta testing to develop the final prototype. Results: Figure 1 depicts the conceptual framework for the app which provides psychological support to enhance patient coping and self-efficacy as a means to improve outcomes such as quality of life (QOL) and mood. The app consists of five 20-minute modules which the patient will complete weekly over a five-week period. The modules include 1) supportive validation to normalize the initial shock of diagnosis, cope with the loss of independence and abrupt life disruptions, and provide reassurance; 2) psychoeducation to enhance preparedness for treatment, manage expectations, and mobilize social supports; 3) psychosocial skill-building to promote effective coping strategies and facilitate acceptance while living with uncertainty; and 4) self-care to foster positive health behaviors and enhance patients' sense of control; and 5) a summary module to review the content and reinforce the learning points from the prior modules. Two bonus modules are available for patients who wish to engage in additional cognitive-based restructuring techniques. We used gamification strategies to enhance patient engagement with the app throughout their illness course as noted in Table 1.Conclusion: We created a novel multicomponent psychological intervention app to promote effective coping and reduce psychological distress in patients newly diagnosed with AML. We utilized patients, caregivers, oncologists, psychologists, and nurses as stakeholders through an iterative app development process to ensure the relevance and acceptability of the app content and to enhance patient engagement. We are planning a pilot randomized trial to evaluate the preliminary efficacy of the app in improving outcomes for patients with AML. Table 1: Gamification to enhance patient engagement with the mobile app 246301 Neelima Vidula, MD, Cancer Center Comparison of tissue genotyping (TG) vs circulating tumor DNA (ctDNA) for selection of matched therapy and impact on clinical outcomes among patients with metastatic breast cancer (MBC) N. Vidula1, D. Juric1, Bardia1 and Medical Oncology, MGH, Boston, MA, USA and 2MGH, Boston, MA, USA Introduction: Oncogenic mutations are attractive targets for targeted therapies, but the clinical impact is unclear. We evaluated the impact of TG or ctDNA on the selection of matched therapy and clinical outcomes in MBC patients. Methods: MBC patients with TG (Next Generation Sequencing/NGS, institutional platform) or ctDNA (NGS, Guardant360) at Massachusetts General Hospital between 1/2016-12/2017 were identified. A review of records to identify tumor subtype, demographics, treatment, outcomes, and TG or ctDNA results was performed. Associations between genomic results and the selection of matched therapy targeted to an actionable mutation, progression free survival, and overall survival (OS) were determined.Results: Among 252 patients with ctDNA testing, 232 (92%) had detectable mutations. Of those 232 cases, 196 (84%) had actionable mutations and 86 patients with actionable mutations received matched therapy with agents including CDK 4/6, mTOR, SERDs, HER2 directed therapy, and DNA damaging chemotherapy. Among 118 patients with TG, 90 (76%) had detectable mutations. Of those 90 cases, 59 (66%) were actionable and 13 patients with actionable mutations received matched therapy with agents including mTOR, PI3K, or AKT inhibitors, or SERDs. There were significantly more actionable mutations detectable by ctDNA vs TG (84.5% vs 65.6%, p < 0.0005) and a significantly higher proportion of patients with ctDNA vs TG received matched therapy (46.7% vs 27.7%, p = 0.018). On multivariate Cox regression analysis adjusted for tumor subtype in ctDNA patients, OS was significantly better for patients with matched therapy vs those with unmatched therapy (HR 0.45, 95% CI 0.25-0.80, p = 0.007). Additional survival analyses will be presented at the meeting. Conclusion: ctDNA testing resulted in higher detection and greater application of matched therapy as compared with TG. Patients who had matched therapy based on ctDNA results had better OS compared to those with unmatched therapy post-ctDNA testing. These novel findings require confirmation in additional studies. 302 Julian A. Villalba, MD, Pathology Diagnostic imprecision of the most widely used nucleic acid amplification test for the detection of Chlamydia trachomatis infection in the United States J.A. Villalba, Branda, D.E. Kurant, J. Baron and S.E. Turbett Hospital, Boston, MA, USA Introduction: Chlamydia trachomatis (CT) is the most frequently reported sexually transmitted pathogen in the United States. Nucleic acid amplification testing (NAAT) is considered to be the most sensitive method for the diagnosis of this infection, and there are several commercially available, FDA-cleared assays for the detection of CT in clinical samples. The performance of these assays, however, has mainly been established through studies performed in areas with high prevalence of CT infection. Here we evaluate the precision (reproducibility) of the most widely used FDA-cleared NAAT assay for CT infection in the United States, the Aptima Combo 2 Assay\u00ae, in an urban setting with relatively low disease prevalence.Methods: We performed a retrospective analysis of test results from all samples submitted to the Clinical Microbiology Laboratory at Massachusetts General Hospital for CT NAAT from 2016 to 2017. All samples were tested using the Aptima Combo 2 Assay\u00ae on the Hologic Panther\u00ae system. Assay results (positive, negative, or equivocal) were interpreted by the Aptima assay software, using the protocol specified in the manufacturer's insert. All positive or equivocal clinical samples, and positive control samples, were retested using the same assay on a different instrument. Between-run precision was determined by calculating the coefficient of variation (CV) of the raw values (reported as relative light units or RLU). We also evaluated data from the College of American Pathologists (CAP) from proficiency testing surveys evaluating CT molecular diagnostic platforms in CLIA certified clinical laboratories all over the United States from 2016 to 2018. Results: A total of 26,429 patients were tested for CT. A total of 1,025 laboratories were tested by CAP in a three-year period, and fourteen different molecular diagnostic modalities were used for CT diagnosis. However, up to 58% (inter-annual range: 54.8-58.1%) of laboratories performed CT testing using the Hologic Aptima Combo 2 Assay\u00ae only. The prevalence of CT infection in our population was 2.4%. Clinical samples that tested positive or equivocal showed a wide distribution of both RLU values [Range: 27 - SD: 10.4 \u00b1 17.4]. Positive 247control samples also showed a wide distribution Individual samples with a mean RLU value <1000 between repeat test results showed significantly higher mean CVs [mean CV for all such samples: 38.6, IQR: 49.1], when compared to samples with a mean RLU value 1000 [mean CV for all such samples: 6.4, IQR: 6.1] (p0.0001). of the samples with a mean RLU value <1000 showed a CV 10, but only 19% of samples with a mean RLU value 1000 showed a CV 10. 11.1% of the samples with a mean RLU value <1000 had a categorical change in the interpretation of results (from positive to equivocal) due to imprecision. All samples showing a categorical change in the interpretation of results had a mean RLU value <1000 [Range: 79 - 723, mean and SD: 316.5 \u00b1 239.5]. In contrast, those that did not have a categorical change in the interpretation of results had a significantly higher mean RLU value [mean and SD: 1228.3 \u00b1 266.3] (p0.0001). All specimen types except urethral swabs showed a wide distribution of CVs. The highest mean CV value was found in anorectal specimens (mean CV: 13.4, IQR: 11).Conclusion: Nucleic acid amplification for CT infection using the Aptima Combo 2 Assay\u00ae suffers from poor precision, especially at RLU values <1000. As test interpretation is based on these values, this imprecision could result in changes in categorical interpretation (positive, negative, equivocal) in 11% of low positive cases (RLU<1000), which could affect patient care. Considering that CAP gives accreditation to most clinical microbiology laboratories in the United States, and that almost 60% of CAP inspected laboratories use this molecular assay, a significant amount of CT clinical results all over the United States could be potentially affected due to this imprecision. Reassessment of RLU cutoff is needed to address these findings. A. CV distribution in clinical samples Clinical samples that tested positive or equivocal showed a wide distribution of both RLU values and CV. B. CV distribution by Mean RLU value in clinical samples: Individual samples with a mean RLU value <1000 had significantly higher mean CVs compared to samples with a mean RLU value 1000 (p0.0001). C. Changes in categorical interpretation of results: 11.1% of samples with a mean RLU value <1000 had a categorical change in the interpretation of results (from positive to equivocal) due to imprecision. Samples with changes in categorical interpretation had a significantly lower mean RLU value than samples without a change (p 0.0001). D. CV distribution by specimen type: All specimen types except urethral swabs showed a wide distribution of CVs. 303 Mauricio A. Villavicencio, MD, MBA, Surgery - Cardiac Impact of Surgical Factors on Mortality Following Cardiac Transplantation: Is it the Reoperation or the LVAD? D.A. D'Alessandro1 1Surgery, Massachusetts General Hospital, Boston, MA, USA and 2Cardiology, Massachusetts General Hospital, Boston, MA, USA Introduction: Outcomes of cardiac transplantation in patients undergoing reoperative sternotomy are often worse than primary transplants. However, the risks imposed by a prior sternotomy, LVAD, re-transplantation, or mediastinal radiation have not been independently compared and analyzed. Methods: Using the UNOS database, a retrospective propensity matched cohort analysis was performed on 14,730 patients who received a heart transplant between 2005-2017. Of the 7,365 patients undergoing a reoperative sternotomy, 4,526 had previous surgery, 2,364 (32%) had an LVAD, and 475 (6%) had a previous transplant. Baseline characteristics were compared between groups and survival analyzed using a Cox model. Results: Compared to patients undergoing a primary transplant, patients with a history of prior sternotomy had worse long-term survival (p<0.001). There was no significant difference in survival between patients who had an LVAD and those who had a previous cardiac operation. However, all subgroups had better survival compared to patients undergoing a re-transplant (p<0.05). On multivariable analysis, prior sternotomy and radiation demonstrated an increased risk of death 248compared to primary transplants (prior cardiac surgery: HR 1.13, p=0.001; LVAD: HR 1.19, p=0.001; re-transplant: HR 1.68, p<0.001; radiation: HR 1.82, p=0.04). When excluding patients who died in the first year, there were no significant differences in overall survival between the primary transplant, prior sternotomy, LVAD, and re-transplant groups. Conclusion: Prior sternotomy and mediastinal radiation are risk factors for worse survival after cardiac transplantation, mainly due to increased early postoperative mortality. Radiation exposure confers the greatest risk of mortality in this cohort. Figure 1: Overall Post-Transplant Survival (A) Overall post-transplant survival comparing patients with a primary cardiac transplant to patients with a history of prior sternotomy. Log rank p<0.001. (B) Overall post-transplant survival stratified by type of previous intervention. 304 Ha Vo, MPH, Medicine - General Internal Medicine Engaging patient advisors to make clinical research studies more patient-centered; Lessons from five years with a patient advisory committee M. Mangla1, D. Kanady3, H. Vo1 and K. Sepucha2,1 1Division of General Internal Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA and 3Patient Advisory Committee, Massachusetts General Hospital, Boston, MA, USA Introduction: The Health Decision Sciences Center (HDSC) at MGH is dedicated to ensuring that patients are well-informed, involved and receive treatments that reflect their goals and preferences. Shared decision making (SDM) is a way to achieve better medical decisions and the HDSC supports SDM efforts by working with clinics and practices to integrate patient educational tools called decision aids (DA) and train providers in the skills required to effectively engage patients in their care. Five years ago, the HDSC created a patient advisory committee (PAC) to support research efforts to implement SDM for four common orthopedic conditions - hip and knee osteoarthritis, herniated disc, and spinal stenosis.Methods: Primary care physicians and surgeons recommended patients who had experience with these conditions for the PAC. HDSC staff interviewed candidates to determine their level of interest in research, their familiarity with research studies (prior experience with research was not required), their comfort with and ability to contribute to the group, and their availability to attend PAC meetings. Seven patients agreed to serve as patient advisors. The PAC meets quarterly at the hospital for 2 hours. The PAC advises the HDSC on aspects of the study, such as generating research questions, refining recruitment materials (i.e. surveys and cover letters) to be more patient-friendly, overcoming barriers to enrollment and promoting retention, and ensuring that the analyses and results are meaningful to patients. From feedback on how to integrate DAs into orthopedic care, to exploring how we make SDM a priority for the orthopedics department, the PAC has been vital to study progress.Results: The PAC has helped make our SDM efforts in orthopedic surgery more patient-centered. Through their involvement, we met our recruitment and enrollment targets for two large studies with response rates in excess of 70%. In addition, based on their feedback and advice, we successfully obtained funding from Patient Centered Outcomes Research Institute for a large comparative effectiveness trial. Further, we have been able to bring their ideas about improving patient care to orthopedic leadership. Some challenges of the PAC include finding a time for meetings where the physician co-investigators (who prefer before or after work hours) and PAC members (who prefer mid-day meetings) can all attend, maintaining cohesion with the PAC after turnover of members, and recruiting members from diverse backgrounds.Conclusion: Patient advisory committees provide valuable feedback and are an increasingly important component for continued success in research studies. The insights of the PAC have allowed us to modify our study procedures to ensure we make an impact in improving patients' medical decisions.249305 Ha Vo, MPH, Medicine - General Internal Medicine Increasing conversations of PROM scores in post-operative, follow-up visits in Orthopaedic Spine Surgery H. Vo1, P. Ogink2, L. Rossi3 and K. Sepucha4,1 1Division of General Internal Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Orthopaedic Spine Center, Massachusetts General Hospital, Boston, MA, USA, 3Center for Quality and Safety, Massachusetts General Hospital, Boston, MA, USA and 4Harvard Medical School, Boston, MA, USA Introduction: Patient reported outcome measures (PROMs) are increasingly collected in clinical practice. These validated questionnaires systematically capture patients' symptoms and functions and turn them into a numerical score. Pre- and post-operative PROM scores can aid clinicians in determining appropriate treatment and assessing functional status, level of pain, and impact on quality of life. The Spine Clinic at Massachusetts General Hospital has a robust collection rate - over 90% of all patients complete the PROM survey at the time of their visit. However, the spine surgeons are not consistently reviewing the PROM scores with patients and nor do they document review of PROM scores in the visit note. The purpose of this quality improvement project was to increase surgeon-patient conversations of PROM scores.Methods: The patient population were adults who were at least 6 months post-op coming in for a follow-up visit. The main outcome measure was documentation in the visit note that the surgeon reviewed the patient's PROMs during the visit. The patient visit workflow - from check-in to check-out - was mapped to identify opportunities to increase PROMs discussion among the clinic staff. The surgeons completed a survey assessing perceived barriers to them reviewing patients' PROMs during the visit. We evaluated the key barriers and developed and tested 3 interventions using the Plan-Do-Study-Act (PDSA) cycle to overcome those barriers to using PROMs. The first intervention was providing the surgeon a printed list of the PROM scores. The second intervention consisted of training surgeons to access PROMs in Epic and changing the default EPIC view to make accessing the PROMs easier. Specifically, we added Rooming tab (which displays the patient's PROMs questions and answers) and Synopsis tab (which graphically displays the patient's PROMs scores over time). The third intervention involved creating an Epic dot-phrase that pulled in a templated script stating the surgeon noted a patient's PROMs and when appropriate, reviewed them with the patient. Results: One barrier was the lack of time in the huddle (between the surgeon and Physician Assistant/Nurse Practitioner/resident/fellow prior) to discuss PROMs prior to seeing the patient. The second major barrier was inconsistency in use of the computer in consult and lack of knowledge in how to access PROM scores in EPIC. At baseline, 0% (N=36) of patients in March and April of 2018 had documentation in their visit note that the surgeon reviewed the PROMs. The first intervention, providing surgeons with a paper printout of the scores, did not result in any documentation (0/14 visits). After the second intervention, half of the visits had documentation that PROMs were reviewed (8/16). The third intervention further sustained the improvement with 12/16 visits having documentation (see Figure for weekly proportions by intervention phase).Conclusion: Through the project, discussion and documentation of PROMs increased. Although every surgeon was willing to incorporate review of PROMs into their visit, each had different challenges and none of the interventions were effective for all surgeons. Next steps include expanding to other ortho specialists and exploring ways to best utilize PROMs with new patients who are contemplating different treatment options for back pain. 250306 Chirag Vyas, MBBS, MPH, Psychiatry Relations of DNA Methylation and Molecular Aging Markers to Health and Well-being in Aging C. Vyas1,2, A. Hazra4, S. Chang1,2, W. C. Reynolds Mischoulon1, Manson4,2,3, I. De Vivo2,3 and O. Okereke1,2,3 1Psychiatry, Massachusetts General Hospital, Boston, MA, USA, 2Channing Division of Network Medicine, Brigham and Women's Hospital, Boston, MA, USA, 3Epidemiology, Harvard School of Public Health, Boston, MA, USA, 4Preventive Medicine, Brigham and Women's Hospital, Boston, MA, USA, 5Psychiatry, University of Pittsburgh School of Medicine, Pittsburgh, PA, USA and 6Psychiatry, VA Boston Healthcare System, Brockton, MA, USA Introduction: Background: Knowledge gaps exist regarding the correlations of DNA methylation age with other molecular aging markers, such as telomere length and mitochondrial DNA copy number, and the relative strength of the correlations of these markers with chronological age. Furthermore, a greater understanding is needed regarding potential associations of DNA methylation markers with psychosocial stress, behavioral and cognitive outcomes, and heath and lifestyle factors. Objective: To relate DNA methylation markers to other biological aging markers and to psychosocial, behavioral, cognitive, lifestyle and health measures.Methods: The sample included 11 psychiatric cases and 12 healthy controls with no current or lifetime history of psychiatric disorder; balanced by age and sex. Cases were diagnosed with DSM (Diagnostic and Statistical Manual)-IV-criteria psychi- atric disorders, as determined by the Mini-International Neuropsychiatric Interview (MINI). Genomic DNA was extracted from blood samples and the following were performed:genome-wide DNA methylation assay using copy number (mtCN). Exposures included: case status, depression and anxiety symptom levels, psychosocial support, subjective and objective cognitive performance and key lifestyle and health factors. Outcomes were DNA Methylation age (DNAm age), RTL, intrinsic epigenetic age acceleration(IEAA).Results: Stronger correlation with chronological age was observed for DNAm age ( =0.86;p<0.01) compared to RTL (=-0.53;p<0.01); mtCN was not correlated with age. Cases had higher mean DNAm age than controls (60.4y vs. 58.2y) but differences were not statistically significant. DNAm age was more strongly correlated with various health and behavior variables under study than RTL or mtCN e.g.correlations with DNAm correlated with psychosocial support objective were significantly related to epigenetic age acceleration, while depression and subjective cognitive score were not. We did not observe genome-wide significant differences by case status in methylation levels for any CpGs (false discovery rate adjusted p >0.05) although there were 76 CpGs that differed between cases and controls at the nominal p<10-4 level, with many at p<10-5 or p<10-6. Using DAVID and Gene Ontology(GO) database annotation tools, we evaluated these top nominal 'hits'. While only nominally significant at p<0.05, there was a suggestion of differential methylation by case status of genes involved in GO biological processes of GTPase activity (GO ID:0090630) and cell-cell adhesion (GO ID:0098609). Conclusion: 1) DNA methylation age correlated more strongly with chronological age and key psychosocial, behavioral and health variables than telomere length or mitochondrial DNA copy number. 2) Strong signals for associations with epigenetic aging were observed for psychosocial and neurobehavioral variables. 3) Possible differential methylation by psychiatric case status of genes involved in biological processes relevant in psychiatry (e.g., GTPase activity) require further exploration in larger samples.251Participant characteristics SD, Standard Deviation ; BMI, body mass index ; PHQ, Patient Health Questionnaire ; GAD, Generalized Anxiety Disorder; STIDA, Structured Telephone Interview for Dementia Assessment; DSSI, Duke Social Support Index. Scatterplots of aging markers and chronological age and correlation with different scores. 307 Lauren Waldman, BS, Psychiatry The Association between Physical Functioning, Symptom Burden, and Coping Strategies with Quality of Life (QOL) in Patients with Chronic Graft-versus-Host Disease (cGVHD) L. Waldman, L. Traeger, S. Fishman, J. Vanderklish, Y. Chen, T. Spitzer, J. Temel and A. El-Jawahri Massachusetts General Hospital, BOSTON, MA, USA Introduction: Allogeneic stem cell transplant survivors with cGVHD experience significant psychological distress and substantial impairments in their QOL. However, the impact of patients' physical functioning, symptom burden, and coping strategies on their QOL trajectory over time is currently unknown.Methods: We conducted a longitudinal study of patients with moderate to severe cGVHD recruited from a single institution. We assessed patient-reported psychological distress (Hospital Anxiety and Depression Scale), QOL (Functional Assessment of Cancer Therapy- General), physical functioning (Human Activity Profile), cGVHD symptom burden (Lee Symptom Scale), and coping (Coping Inventory for Stressful Situations) at baseline, 3 months, and 6 months. Using mixed linear effects models, we longitudinally examined the relationship between patients' QOL with their physical functioning, cGVHD symptoms, and coping strategies. Results: We enrolled 53 patients with moderate (71.7%, 38/53) or severe (28.3%, 15/53) cGVHD. The rate of clinically significant depression and anxiety symptoms at baseline were 32.1% (17/53) and 30.2% (16/33), respectively. Depression and anxiety symptoms did not change substantially over time. Patients reported impaired QOL at baseline [FACT-G: mean=70.33, SD=18.96] which did not change significantly over time [ =-0.66, SE=1.11, P=0.550]. Higher physical functioning was associated with better QOL [=0.17, SE=0.05, P=0.001] over time, while higher symptom burden was associated with worse QOL [=-0.38, SE=0.06, P<0.001] over time. The use of emotion-oriented was associated lower QOL over time [=-0.70, SE=0.14, P<0.001]. In contrast, the use of avoidance-oriented coping was associated with higher QOL [=0.38, SE=0.10, P<0.001] over time. Task-oriented coping was not significantly associated with patients' psychological distress or QOL. Conclusion: Patients with moderate or severe cGVHD report substantial psychological distress and persistently impaired QOL over time. Higher physical function and lower symptom burden are associated with improvement in patients' QOL over time. The use of certain coping strategies impacts patients' QOL trajectory. These data underscore the need for supportive care interventions to promote effective coping and enhance physical functioning in patients with cGVHD.252308 Zachary Wallace, MD, MSc, Medicine - Allergy/Immunology/Rheumatology Phenotypic Groups in IgG4-Related Disease - A Latent Class Analysis Z. Wallace1, Y. Zhang1, C. Perugino1, R. Naden2, H. of Allergy, and Immunology, MGH, Boston, MA, USA and 2McMaster University, Hamilton, ON, Canada Introduction: IgG4-related disease (IgG4-RD) is a multi-organ condition of uncertain etiology characterized by substantial morbidity if not diagnosed and treated promptly. Identifying IgG4-RD groups based on the distribution of organ involvement may facilitate diagnosis, illuminate our understanding of the pathogenesis, and guide management. We sought to identify phenotypic groups of IgG4-RD using an unbiased method.Methods: Our study patients consisted of 493 IgG4-RD cases diagnosed by 76 IgG4-RD specialists from North America, South America, Europe, and Asia. For each case, investigators reported age at disease onset and diagnosis, race/ethnicity, organ involvement, and lab results. We performed latent class analysis to identify groups with distinct patterns of organ involvement. Cases were assigned to the group in which they had the highest probability of membership. We examined the relation of selected covariates to the odds [odds ratio (OR) and 95% CI] of belonging to a group using multinomial logistic regression. To validate our results, we repeated the analysis in a separate cohort.Results: Of the 493 IgG4-RD cases, the mean age was 59.5 (\u00b1 14.0) years and 65% were male. Of the cases, 40% were Caucasian, 45% were Asian, and 12% were Hispanic. We identified four group (Table 1, Figure 1). Group 1 (\"Pancreato-Hepatobiliary\") included 31% of patients. Group 2 (\"Retroperitoneum/Aorta\") included 24% of patients. Group 3 (\"Head/Neck\") included 24% of patients. Group 4 (\"Mikulicz/Systemic\") included 22% of patients. Compared to the \"Head/Neck\" Group (Table 2), individuals in other groups were significantly more likely to be older (OR Range 1.17-1.28) and male (OR Range 9.21-11.93). Individuals in other groups were significantly much less likely to be Asian (OR Range 0.14-0.16) compared to the \"Head/Neck\" group. The \"Mikulicz/Systemic\" group included patients who tended to have quite elevated serum IgG4 levels compared to the \"Head/Neck\" group [OR 1.12 (1.02-1.22)]. We replicated our results in the second cohort.Conclusion: Using an unbiased method, we identified four phenotypic groups of IgG4-RD patients. Besides differences in organ involvement, groups were distinguished by age at diagnosis as well as race/ethnicity, gender distribution, and serum IgG4 concentrations. These groups may identify patients with IgG4-RD resulting from different risk factors or exposures and those likely to respond differently to treatment. Groups of IgG4-RD Using the 2018 Classification Criteria Validation Cohort (N=478) *Only cases with complete covariate data are included in fully-adjusted latent class analysis **Generally, a probability of group membership > 70% is considered very strong All reported proportions are probabilities conditional on latent class membership Figure 1: Organ Distribution Across Phenotypic Groups (Proportion of cohort in each group)253309 Rachel Wallwork, MD, Medicine Cigarette Smoking is a Risk Factor for IgG4-Related Disease R. Wallwork1, Z. Wallace2, Y. Zhang2, H. Choi2,3 Medicine, MGH, Boston, USA, 2Rheumatology, MGH, Boston, USA and 3Epidemiology, MGH, Boston, MA, USA Introduction: IgG4-related disease (IgG4-RD) is a fibroinflammatory condition characterized by tumefactive lesions that can occur at nearly any site, often associated with an elevated serum IgG4 concentration. Despite advances in the recognition and treatment of IgG4-RD, its etiology and pathogenesis remain unknown. Prior studies suggest that cigarette smoking may be a risk factor for IgG4-RD. We performed a case-control study to evaluate the association between cigarette smoking and the risk of IgG4-RD.Methods: All patients seen in the Center for IgG4-RD at Massachusetts General Hospital who were diagnosed with IgG4-RD between 2010 and 2018 and completed a smoking questionnaire were included in this study. Participants in the Partners HealthCare Biobank who completed a smoking questionnaire (N=30,536) were used a source of controls. For each case and control, a smoking status (never, former, current) was determined. Each case was matched to up to 5 controls based on age (5-year category) and sex. We used conditional logistic regression to compare the proportion of cases and controls with a history of cigarette smoking using odds ratios (OR) and 95% confidence intervals (CIs). Results: We identified 194 IgG4-RD cases which were matched to 970 controls (Table 1). The mean age for cases and controls was 57 years (\u00b1 13) and 57 years (\u00b1 13), respectively, and the majority of patients were male (62% and 62% in both groups). There was a greater proportion of current and former smokers among IgG4-RD cases (Current=23 with controls, IgG4-RD cases had nearly a two-fold higher odds of being current smokers (OR 1.85 [95% CI 1.11-3.07]) or former smokers (OR 1.66 [95% CI 1.16-2.39]). The association between cigarette exposure and the risk of IgG4-RD seemed to be driven by a strong association among the subgroup of patients with retroperitoneal fibrosis (RPF, 16% of cases) (OR 4.91 [95% In this case-control study of patients with IgG4-RD, current and former smoking statuses were strongly associ-ated with the risk of IgG4-RD, especially among IgG4-RD patients with RPF. Biologically, cigarette smoking is known to stimulate fibrogenesis, a key feature of IgG4-RD. While these findings require additional confirmation, cigarette smoking may be the first recognized modifiable risk factor for IgG4-RD. Further research is necessary to understand the mechanism by which smoking increases the risk of IgG4-RD. Patients with IgG4-RD may need to be counseled against smoking. Table 1: Association Between Cigarette Smoking and Risk of IgG4-RD IgG4-RD: IgG4-Releated Disease, RPF: Retroperitoneal Fibrosis 310 Emily A. Walsh, BA, Psychiatry Development of a Stepped-Care Intervention to Promote Adherence to Endocrine Therapy for Breast Boston, MA, USA and 2Harvard Medical School, Boston, MA, USA Introduction: One in eight women in the United States will be diagnosed with breast cancer in her lifetime. Most early-stage breast cancers (80%) are hormone sensitive and therefore treated with adjuvant endocrine therapy for up to 10 years to prevent breast cancer recurrence. While endocrine therapy offers a 40-50% reduction in risk of recurrence for early-stage, 254hormone receptor-positive breast cancer survivors (BCS), approximately 28-59% of survivors do not take these medications as prescribed. Several known factors influence adherence behaviors, including side effects (e.g., hot flashes, joint pain, insomnia, depression), concerns about risks from therapy (e.g., bone loss, clot, endometrial cancer), high medication cost, perceived low risk of recurrence, negative beliefs and attitudes towards endocrine therapy, and poor medication-taking self-efficacy. Additionally, approximately one third of BCS experience elevated distress (i.e., depression and anxiety), which presents a substantial barrier to adherence. Despite established literature regarding the known barriers to endocrine therapy adherence, there are few proven evidence-based and theoretically-driven interventions to modify these barriers and enhance adherence. We report the qualitative phase of a mixed-methods study to understand barriers to adherence, perceptions of endocrine therapy, and initial impressions of a patient-centered, stepped-care videoconference intervention to improve adherence, distress, and symptom management.Methods: From 11/2017-present, we are enrolling early-stage, hormone receptor-positive, female BCS taking endocrine therapy in a qualitative study with semi-structured interviews at the Massachusetts General Hospital Center for Breast Cancer. To be eligible, BCS must 1) be 21 years of age; 2) read and respond in English; 3) have completed primary treatment; 4) be within 3 months to 3 years of initiating ET; and 5) have an Eastern Cooperative Oncology Group (ECOG) performance status 0-2, indicating ambulatory and active for >50% of waking hours. We ask BCS to 1) to rate adherence to endocrine therapy in the past month on a Visual Analog Scale (VAS) and 2) answer whether they have difficulties taking their medica - tion (yes vs. no). We also ask BCS to rate the severity of their side effects (none/mild/moderate/severe). Our goal is to complete 20 interviews with BCS with low adherence (VAS 90% and/or answered yes to difficulties taking medication) and 10 interviews with BCS with high adherence (>90% and no difficulties) and moderate-severe side effects. To ensure a balanced sample, interviews are stratified based on age ( 50 vs. >50) and time on endocrine therapy (3-19 months vs. 20-36 months). Additionally, we assess level of distress with the Patient Health Questionnaire (PHQ-9) and Generalized Anxiety Disorder scale (GAD-7) to ensure representation of BCS with a range of distress. A semi-structured interview guide, developed by experts in breast oncology, psychology, survivorship, and adherence behaviors, is used to explore experiences, perceptions, and behaviors related to medication-taking and to solicit feedback about the proposed stepped-care, videocon - ference intervention. Interviews last 20-30 minutes, are completed by telephone or in-person following an outpatient breast oncology visit, and are transcribed. Research staff created a thematic framework, and two independent coders will code each interview until high reliability is achieved (Kappa > 0.80). All analyses will be conducted using NVivo 11 software.Results: Recruitment is ongoing and will be complete by August 2018. At that time, data will be coded as described above, and results, including perceptions of endocrine therapy, barriers to adherence, impressions of the intervention, and levels of distress (i.e., GAD-7 and PHQ-9), will be reported in the final presentation. To date, 194 BCS have been approached, 34 were eligible, 31 enrolled, and 26 interviewed. Of the 26 interviews, 17 were with BCS with low adherence, and 9 were with those with high adherence and moderate-severe side effects. Anecdotally, BCS were eager to share their experiences, and some express strong negative attitudes towards taking endocrine therapy medication. Many BCS endorse high levels of side effect intensity related to their medications that affect their quality of life yet report taking the medication regularly to prevent breast cancer recurrence. Consistently, BCS describe great trust in their doctors' and nurses' recommendations for endocrine therapy. Additionally, BCS express interest in the study and intervention development. They describe an acceptance of the videoconference modality and recommend that more support be offered around the time of endocrine therapy initiation.Conclusion: Given the risk for non-adherence, it is important to understand the perceptions and preferences of early-stage, hormone receptor-positive BCS taking endocrine therapy. BCS in our study are accepting of a proposed evidence-based, patient-centered intervention to provide support and address barriers to adherence at the time of therapy initiation. Findings from this qualitative interview phase will be illustrated in the final presentation and inform the development of the stepped-care, videoconference intervention to improve adherence to endocrine therapy, address symptom management, and reduce distress. We will ultimately test the feasibility and acceptability of the intervention in a randomized controlled trial and later examine whether BCS may benefit from such an intervention on both psychosocial- and treatment-related outcomes. 311 Christine Wang, Medicine Longitudinal Clinical Model for Addressing Social Determinants of Health in Student-Faculty Collaborative Clinic C. Wang1, R. J. Chu1,2 and M. Cohen1,2 1Harvard Medical School, Roxbury Crossing, MA, USA and 2Massachusetts General Hospital, Boston, MA, USA Introduction: Research demonstrates lower socioeconomic status leads to worse health outcomes, and higher ratios of social services to health care spending result in better health outcomes, implying investment in social services is associated with improved health (Bradley 2016). Recognizing the importance of social determinants of health, student clinicians at Crimson Care Collaborative at Chelsea (CCC-Chelsea) are furthering a student initiative to capture patient needs and improve referral processes for social services.255Methods: In 2016, CCC-Chelsea implemented an innovative screening form with questions in English/Spanish, categorizing resources including food pantries, ESL/GED classes, and others available for undocumented immigrants. We are reviewing our data and working with student clinicians to determine ways to improve our screening rate. We will conduct phone interviews with patients to determine the success rate of the referral process and major barriers encountered. By reviewing results and learning best practices, we plan to improve the process and promote partnerships with community organizations to assist patients.Results: Since the project's initiation, 54 patients were screened and 106 referrals were made. Preliminary results show 34% of patients had social service needs and were given an appropriate referral. Food assistance (30% of referrals), utilities and employment assistance (17%, 13.2% respectively) represented the most requested resources. These results as well as future phone interviews will help us understand patient needs. This information can also direct future partnerships with community organizations.Conclusion: By improving the referral process for this program and creating partnerships with external organizations, CCC-Chelsea's efficient use of community resources could bolster health outcomes. By improving upon our innovative social services project, we hope to create a long-standing way to integrate social determinants screening into clinical practice. And through this project, we are helping create a multidisciplinary and comprehensive healthcare model. 312 Zhidong Wang, Radiology CD117-Targeted ZW-SCF Contrast Agents for GIST Diagnosis Wang1,2, K. Bao1, H. Kang1, E. McDonald1, G. Fakhri1 and H. Choi1 1Department of Radiology, Massachusetts General Hospital and Harvard Medical School, Gordon Center for Medical Imaging, Boston, MA, USA and 2Department of General Surgery, the Second Affiliated Hospital, Xi'an Jiaotong University, Xi'an, China Introduction: Gastrointestinal stromal tumor (GIST) is the most common mesenchymal neoplasms of the gastrointestinal tract. Surgical resection is the main treatment for most GIST, but surgery appears to be difficult for metastatic or recurrent GIST patients due to the risks of positive margins. Stem cell factor (SCF) is a CD117 ligand and can specifically serve as a molecular cell surface target for the detection of GIST intraoperation when conjugated with a contrast agent. Particularly, near-infrared (NIR) fluorescent light, in the wavelength range of 700 nm to 900 nm, can provide sensitive, specific, and real-time trafficking of molecular functions with low tissue attenuation and autofluorescence. In the present study, to investi - gate the detailed mechanism of CD117 of GIST, SCF ligand human SCF buffer-exchanged with PBS, 7.8 and 20 mol equiv. of ZW800-1C NHS ester was added while stirring. The labeling ratio of ZW-SCF was calculated from the ratio of extinc-tion coefficients between the SCF (l at 280 nm, 210,000 M -1 cm-1) and ZW800-1C (l at 765 nm, 120,000 M-1cm-1), which was found to be 0.5. For GIST cell labeling with onto sterilized glass coverslips in 24-well plates (1 \u00d7 105 cells per well), 256and incubated at 37 \u00b0 for 60 min in the presence of 0.27 \u00b5M ZW-SCF. Biodistribution and targeting studies were followed 24 h post-injection of ZW-SCF in GIST-bearing xenograft tumor mice using the FLARE intraoperative imaging system.Results: ZW-SCF successfully labeled GIST cells in vitro: Strong NIR fluorescence signals of ZW-SCF was found on the surface of GIST-T1 cells (CD117 +) using the NIR fluorescence microscopy, while negligible fluorescence signals were observed in receptor-compromised GIST-5R cells (CD117-). For real-time whole body biodistribution and 10 nmol of ZW-SCF was injected intravenously into xenograft tumor mice, and their NIR fluorescence images were observed at 24 h post-injection. ZW-SCF targeted tumor site specifically with reduced nonspecific uptake in major organs including liver and spleen. Conclusion: SCF is a CD117-specific tracking and image-guided surgery. ZW-SCF is of special interest because of its targeting affinity to the GI track with long-term systemic circulation as well as reduced uptake in the reticuloendothelial system (RES) due to the zwitterionic coating. NIR fluorescence enables intraoperative imaging and image-guided surgery under the dual channel imaging system. To further confirm our preclinical research, intraoperative imaging of GIST and image-guided tumor resection using ZW-SCF will be followed in genetically engineered GIST animal models. 313 Meredith Ward, B.A., Psychiatry Linking Suicide Behaviors and Anxiety Disorders: A Family Systems Perspective J. Spandorfer1, K. Szuhany1, S. Hoeppner2, R. Lubin1, E. Bui2 and N. Simon1,2 1Psychiatry, NYU School of Medicine, New York, NY, USA and 2Psychiatry, Massachusetts General Hospital, Boston, MA, USA Introduction: Family history of suicide behaviors confers risk for mood disorders and one's own engagement in suicidal behaviors (Mann et al., 2005). However, little is known about the link between family history of suicide and anxiety disorders. Research suggests that neuroticism, a transdiagnostic factor associated with anxiety disorders, may contribute to the familial transmission of suicidal behavior (Brent & Melhem, 2008). The present study aims to fill this gap by examining the relationship between family history of suicide attempts and the presence of an anxiety disorder and comorbid depression. We hypothesized that, after adjusting for age and gender, family history of suicide would be more frequent amongst patients with anxiety disorders compared to controls, and that rates would be highest amongst those with co-occurring depression. Methods: A total of 1,005 participants (Mean age=35.4 SD=13.8 years, 52% female) reported if a first degree relative had completed or attempted suicide and underwent structured diagnostic interviews to assess for current and lifetime psychiatric disorders. Participants were either: individuals diagnosed with an anxiety disorder (Generalized Anxiety Disorder, Social Anxiety Disorder, Panic Disorder; n =685); or healthy controls with no current psychopathology at diagnostic threshold (n=320).Results: Eleven percent (n=110) of the full sample (13% [n=91] with anxiety disorders, 6% [n=19] of controls) reported family history of completed/attempted suicide. Logistic regression results indicated a significant relationship between family history of completed/attempted suicide and the presence of an anxiety disorder (b = -.89, p=.001). Contrary to hypotheses, there was no additive association of comorbid depression with family suicide behaviors above and beyond the presence of an anxiety disorder. Post-hoc analyses indicated no significant differences in the prevalence of family suicide attempts across the 3 anxiety diagnoses.Conclusion: Though limited by a cross-sectional design, our results suggest that a family history of suicidal behavior may serve as a potential cross-diagnostic risk factor for anxiety disorders with or without co-occurring depression. Although additional research is critically needed, including the prospective examination of the complex family psychiatric and neurobi- ological risk factors involved, these data support that even in the absence of depression, screening for anxiety disorders in family members of individuals who have attempted/completed suicide may be indicated. Additional analyses of transdiag - nostic psychological factors and their association with family history of suicide behaviors in adults with anxiety disorders will be presented. Future directions and implications for prevention and intervention will be discussed.257314 Rory B. Weiner, MD, Medicine - Cardiology Biologic Variability Echocardiographic Massachusetts General Hospital, Boston, MA, USA and 2School of Sport, Cardiff Metropolitan University, Cardiff, United Kingdom Introduction: Clinical echocardiography relies on longitudinal repeated measures of cardiac structure and function. To date, the within-subject biologic variability across different categories of echocardiographic parameters remains incompletely characterized. Methods: Healthy male volunteers (n = 14, age = 30\u00b1 3 yrs) were studied with transthoracic echocardiography on 5 consecutive days. Participants were imaged at identical times of day by a single sonographer using a single ultrasound machine (GE E9) in a temperature and humidity controlled laboratory. All images were interpreted according to American Society of Echocardiography guidelines by a single blinded investigator using vendor specific software (EchoPAC V112). Within-subject variability for: 1) linear, 2) area / volume, 3) tissue Doppler, and 4) spectral Doppler parameters was assessed using coefficient of variation (CV) and intraclass correlation coefficient (ICC).Results: Absolute range, CV, and ICC for different parameters within the 4 measurement categories are shown in Table 1. Pooled CV was lowest for linear parameters (3.7%), followed by area / volume (9.0%), tissue Doppler (10.1%), and spectral Doppler (11.4%), although there was significant heterogeneity within each category. Notable outliers with low variability included left ventricular mass (CV variability of echocardiographic measures of cardiac structure and function varies significantly with linear measurements being the least variable and Doppler measurements demonstrating the greatest variability. These findings need to be considered when evaluating the clinical and scientific significance of longitudinal repeated echocardio - graphic measures. 258315 Michaela K. Welch, Bachelors of Arts, Medicine - General Internal Medicine Risk factors for mammography screening failures by clinical prognostic stage at diagnosis A. McCarthy1,2, M.K. Welch1, Lehman4,2, A. Bardia5,2 and K. Armstrong6,2 1Division of General Internal Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Harvard Medical School, Boston, MA, USA, 3Magview, Inc., Burtonsville, MD, USA, 4Imaging, Massachusetts General Hospital, Boston, MA, USA, 5Hematology/Oncology, Massachusetts General Hospital, Boston, MA, USA and 6Department of Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Mammography screening failures occur when breast cancer is diagnosed after a negative mammogram before the next screening. The study objective was to examine risk factors for mammography screening failures by cancer prognosis. Methods: Women 40 years who had a negative screening mammogram at Massachusetts General Hospital from 2006-2014 were included. Women with prior breast cancer, breast implants, and non-MA residents were excluded. Screening failures were defined as breast cancers diagnosed within 1 year of negative mammogram based on linkage with the MA Cancer Registry. Prognosis was defined using new AJCC Cancer Staging Manual 8 th Edition breast cancer clinical prognostic stage groups, which incorporate grade and tumor molecular subtype. Logistic regression was used to estimate the associations of patient characteristics with screening failures, and multinomial logistic regression was used to assess associations with good (stage 0-I) versus poor prognosis (stage II-IV). Results: Among 271,080 negative mammograms, 232 screening failures occurred. The median age was 57 and most patients were white (83%). Fourteen percent reported family history of breast cancer and 44% had dense breasts. Breast density (OR=3.4 95% CI 2.5-4.6) and family history (OR=1.9 95% CI 1.4-2.6) were strongly associated with screening failure. One third of screening failures had poor prognosis. In the multinomial model, breast density and family history were associated with good prognosis; only breast density was associated with poor prognosis. Though not statistically significant, women <50 yrs had greater odds of poor prognosis (OR=1.5 95% CI 0.9-2.4).Conclusion: Most screening failures had good prognosis based on clinical prognostic staging, and risk factors differed among screening failures by tumor prognosis. Identifying women at high risk for poor prognosis screening failures would allow supplemental screening among women at highest risk of dying from breast cancer. 316 Sarah E. Wicheta, Oral and Maxillofacial Surgery Discrepencies in Interpretation of the Minor Salivary Gland Biopsy in the Diagnosis of der Groen1, W.C. Faquin2 and M. August3 1Pre-doctorate, Harvard School of Dental Medicine, Brookline, MA, USA, 2Divisions of Head and Neck Pathology and Cytology, Massachusetts General Hospital, Boston, MA, USA and 3Oral & Maxillofacial Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: The minor salivary gland biopsy (MSGB) is a major criterion for the diagnosis of Sj\u00f6gren syndrome (SS)1 and our previous review found it to be the most frequently positive of the major criteria used by the 2 classification systems in current use. However, multiple studies have outlined the difficulties of a standardized approach to MSGB interpretation.2 The purpose of this study was to answer the following clinical question: In all patients referred for MSGB to establish a diagnosis of SS, how reproducible was the focus score and subsequent assigned positive or negative diagnosis? We further sought to elucidate areas of difficulty in establishing the focus score in this cohort of patients. Methods: To answer this clinical question, we designed a retrospective study of patients referred to the Massachusetts General Hospital Department of Oral and Maxillofacial Surgery over a 5-year period for MSGB to establish a diagnosis of SS. Inclusion criteria were: complete information regarding presenting signs and symptoms, relevant laboratory data and the availability of histopathological slides of the MSGB for re-evaluation. Incomplete records and referral for reasons other than presumptive SS resulted in exclusion. Biopsies were blindly re-examined by each author utilizing strictly outlined criteria for establishing the focus score (quantitating the lymphocytic aggregates of >50 cells found within a 4mm 2 area of glandular parenchyma). These results were compared to the initial pathology reports. The new biopsy interpretations were then used to rule in/rule out a diagnosis of SS utilizing the two main classification systems. The outcome of this calculus was compared to the initial SS diagnoses rendered and used to recalculate sensitivity, specificity and positive and negative predictive values. Results: Seventy three of the 87 patients included in our initial study of the MSGB met inclusion criteria for this review. The mean age was 48.5 years (range, 19-71yrs) and 64 were women (87.6%). Upon review, 14/19 (73.6%) positive and 29/38 (76.3%) negative biopsies available for review were concordant with the initial interpretation. Sixteen biopsies initially read as \"indeterminate\" were also reviewed. 10 of 16 (62.5%) were determined to be positive and 6/16 (37.5%) were read 259as negative. Of the 33 patients with positive MSGB, 21 (63.6%) were determined to be SS positive. Of the 40 patients with negative MSGB, only one was diagnosed with SS based on other outlined criteria. Our previous study evaluating the importance of the MSGB in the diagnosis of SS and utilizing the pathology reports in the medical record yielded 80.0% sensitivity, 87.5% specificity, PPV=57.1% and NPV= 95.5%. By comparison, the current review yielded the following: 95.4% sensitivity, 78.0% specificity, PPV=63.6% and NPV=97.5%. Difficulty with focus scoring and standardization was related to disparity in tissue volume, the number of glands submitted, decisions regarding the counting of confluent foci with far greater than 50 lymphocytes, difficulty evaluating glandular parenchyma that was atrophic or fibrotic and evaluation of glands with significant periductal inflammatory changes more consistent with sialadenitis. Conclusion: The MSGB is an important major criterion in establishing a diagnosis of SS. However, the lack of reproduc-ibility in interpretation and focus scoring is problematic as demonstrated in this review and by others. Future studies will evaluate improved methods of MSGB interpretation and standardization as well as the use of immunohistochemical staining to better delineate lymphocytic aggregates and germinal centers within the biopsy. 1. Wicheta, S., Van der T., Faquin, W.M., August, M. Minor salivary gland biopsy - an important contributor to the diagnosis Sj\u00f6gren syndrome. J Oral Maxillofac Surg 75(12): 2573-2578, 2017 2. Fisher, B.A., al. Standardisation of labial salivary gland histopathology in clinical Ann Rheum Dis 76(7): 1161-1168, 2017 317 Susan R. Wilcox, M.D., Emergency Intervention to Decrease Leaving without Treatment Among Intoxicated Emergency Department Patients L. Milne1, D. Williamson1, C. Kraus1, Y. Chang3 and S.R. Wilcox1,2 1Emergency Medicine, Massachusetts General Hospital, Boston, MA, USA, 2Heart Center ICU, Massachusetts General Hospital, Boston, MA, USA and 3Medicine, Massachusetts General Hospital, Boston, MA, USA Introduction: Approximately 15% of the more than 4000 patients presenting each year to our emergency department (ED) with a chief complaint or discharge diagnosis related to alcohol were leaving without treatment (LWT). If they are not clinically sober at the time of departure, these patients are at risk for falls or other injury. Our goal was to create an intervention to decrease this rate of early departure and assess for success of the initiative. Methods: A stakeholder group identified the reasons why intoxicated patients were LWT, concluding that the primary reason patients left was there was no process in place for evaluating and caring for these patients who potentially had impaired decision-making capacity. The group created a worksheet for the triage nurse to identify and manage patients presenting with intoxication and impaired decision-making or ambulation, with protocols to keep the patient in a supervised area. We performed a before and after analysis, evaluating 12 months before and 12 months after the protocol was initiated, with the primary outcome being the rate of intoxicated patients who LWT. We also measured the recidivism rate (the rate of return to the ED within 24 hours after departure) and the ED length of stay (LOS).Results: After the intervention was initiated, the percentage of intoxicated patients who LWT decreased from 15.0% to 7.4% LWT (p<0.001). Among patients who stayed until discharge during the intervention period, the 24-hour recidivism was 9.4%, compared to 22.6% for those who LWT (p < 0.001). This difference in recidivism rates for each group was the same before and after the intervention, but fewer patients LWT after. For those patients with alcohol-related visits, the ED LOS was statistically significantly longer in the intervention phase, by a mean of 42 minutes for all patients (p< 0.001), as well as by a mean of 24 minutes for those who stayed to be dispositioned (p=0.031). Conclusion: Providing a standardized process for caring for acutely intoxicated patients leads to fewer patients leaving the ED before discharge. Patients who stay to the completion of treatment have a lower recidivism rate within 24 hours after leaving than those in the LWT category. Table 1. Clinical Data 260 Wide red arrow represents time of intervention (introduction of the worksheet). Time line to the left of the arrow represents the 12 months prior to the intervention and the time line to the right of the arrow, the 12 months after the start of the intervention. 318 Audrey Wolfe, MPH, Physical Medicine and Rehabilitation Beyond scarring: long term physical outcomes following burns A. Wolfe1, L. Simko1, L. Espinoza1, K. McMullen4, J.C. Schneider1 and C. Ryan2,3 1Physical Medicine & Rehabilitation, Spaulding Rehabilitation Hospital, Charlestown, MA, USA, 2Surgery, Massachusetts General Hospital, Boston, MA, USA, 3Surgery, Shriner's Hospitals for Children, Boston, MA, USA, 4Center on Outcome Research in Rehabilitation, University of Washington, Seattle, WA, USA, 5University of Texas Medical Branch, Galveston, TX, USA and 6University of Texas Southwestern, Dallas, TX, USA Introduction: The breadth of physical outcomes following burn injury is underexplored in the literature. The Burn Model Systems (BMS) created a review of systems questionnaire to identify persistent physical sequelae. The objective of this study was to determine the frequency of these 22 patient-reported physical outcomes up to 2 years following burn injury. Methods: Data from the BMS National Database (2015-2017) were analyzed. All individuals were 18 years of age or older at the time of data collection. A series of \"yes/no\" Review of Systems questions developed by the BMS were administered to burn survivors at 6 and 24 months after injury. Demographic and clinical characteristics of the population were determined and the Review of Systems questions were examined.Results: There were 172 subjects included in the 6 month population and 128 included in the 24 month population. The 6 month respondents were 69.8% male, had a mean (SD) age of 46.8 (15.9) years, and a mean (SD) burn size of 17.9 (18.9) percent total body surface area. At all time points, the most prevalent physical outcome was numbness, pins and needles or burning sensations in the burn scar (6 mo: 69%; 24 mo: 46%). At both the 6 and 24 month follow-up timepoints, more than a quarter of the population indicated they experienced neuropathic symptoms in the extremities, joint pain, difficulty in hot environments, cold intolerance, trouble with balance, and difficulty with memory (Table). Conclusion: Physical outcomes such as neuropathic symptoms, temperature intolerance, balance and memory impairments are commonly reported up to two years following burn injury. Further investigation is needed to better understand and address these symptoms. The 14 most prevalent outcomes at 24 months post-burn 261319 Olivia B. Wons, BS, Medicine - Endocrine-Neuroendocrine Olfactory Avoidant Restrictive Food Intake Disorder MGH, Boston, MA, USA Introduction: DSM-5 describes three different presentations of avoidant/restrictive food intake disorder (ARFID): sensory sensitivity; lack of interesting in eating or food; and fear of aversive consequences. Little is known about the etiology of ARFID, though it is possible that each presentation has a unique underlying neurobiology. Clinically, individuals with the sensory sensitivity presentation of ARFID often describe heightened sensitivity to smell, taste, and appearance of food. Methods: As part of an ongoing study on the neurobiology of ARFID and related presentations (A/R eating), we used the Sniffin' Sticks assessment to evaluate differences in olfactory performance between subjects with A/R eating (n=44) and healthy controls (n =10, ages 10 to 22, 50% male). We classified individuals with A/R eating as sensory sensitive if they scored above a predetermined cutoff score of 4 on a 5-point prototype matching assessment created for the present study (n = 24). We classified all other individuals with A/R eating as non-sensory-sensitive (n = 20), and compared both groups to healthy controls (n = 10). During the assessment 16 different Sniffin' Stick pens were presented to the participant. One at a time, the participant was asked to smell the pen and identify the scent from four different scent names presented on a card.Results: Individuals with sensory sensitive A/R eating identified significantly fewer scents correctly (M=10.21, SD=2.36) compared to both healthy controls (M=12.60, (t(51)=-2.24, p=0.0294). Conclusion: The results suggest that participants with sensory sensitive ARFID have weaker olfactory performance when identifying smells. Possible explanations are: weaker smell identification abilities contributing to the development of A/R eating, or limited experience in smelling a wide range of smells, due to their restricted diets. Future research is needed to determine if wider exposure to a variety of olfactory stimuli might facilitate symptoms resolution in the treatment of ARFID. 320 Brendan W. Wu, Oral and Maxillofacial Surgery Do Steiner or Harvold Cephalometric Analyses Better Peacock2 and L. Kaban2 1Oral and Maxillofacial Surgery, Harvard School of Dental Medicine, Boston, MA, USA and 2Oral and Maxillofacial Surgery, Massachusetts General Hospital, Boston, MA, USA Introduction: Cephalometric analyses are routinely used to aid diagnosis, plan treatment, and assess outcomes in patients with dentofacial deformities. Steiner analysis provides angular and mandible (SNB: sella-nasion-B point). Harvold analysis is less commonly used and consists of linear measurements for maxilla (MXU: condylion to anterior nasal spine) and mandible (MDU: condylion to gnathion). Few data exist comparing these analyses with clinical diagnosis and operative management. The purpose of this study is to determine which of these two analyses correlates better with diagnosis and the resultant treatment plan. We hypothesized that Harvold analysis would more closely reflect the clinical impression. Methods: This was a retrospective case study of patients undergoing orthognathic surgery at Massachusetts General Hospital from 2012 to 2016. Subjects were included with a complete initial consultation note by an attending surgeon and available initial and immediate postoperative cephalometric radiographs. Patients with craniofacial syndromes, maxillary or mandibular asymmetry, cleft lip and/or palate were excluded. Predictor variables were Steiner (SNA, SNB) and Harvold (MXU, MDU) measurements before and after surgery. Numerical values were converted to categorical indicator variables: \"hypoplasia\" if below two standard deviations (SD) of the published norm for males and females, \"neutral\" if within two SD of the mean, and \"hyperplasia\" if above two SD of the mean. Outcome variables were the maxillary and mandibular clinical impressions (hypoplasia, neutral, or hyperplasia) made at the initial clinical examination. A z-test for proportions compared the frequencies of Steiner versus Harvold matching the clinical impression. A one sample t-test compared Steiner and Harvold measurements of included subjects to the population means. Sensitivity, specificity, and positive predictive value (PPV) for Steiner and Harvold analyses were calculated with the clinical impression serving as the standard and compared with Fisher's exact test.Results: During the five-year study period, 388 patients had orthognathic surgery, and 289 were included in this study. The Steiner analysis was 30% concordant with maxillary and 52% concordant concordant both maxillary and mandibular clinical impressions (p<0.0001). Compared to the clinical impression of maxillary hypoplasia, SNA had a sensitivity of 25%, specificity of 87%, and PPV of 94%; MXU had a sensitivity of PPV of 93%. For 98%, PPV of 97%; MDU had a sensitivity of PPV of 91%. For mandibular SNB of 70%, and PPV of 23%; MDU had a sensitivity specificity of 92%, and PPV of 50%.Conclusion: Harvold linear measurements for both maxilla and mandible were more concordant with clinical impression. Neither Harvold nor Steiner analyses were highly sensitive for each diagnosis, but do provide high specificity and PPV to confirm an existing clinical impression. The results of this study indicate that the Harvold analysis be considered to augment clinical impression when planning orthognathic surgery. Third-parties should consider this analysis in their determination if criteria are met for insurance coverage. Summary of Diagnostic Testing with Cephalometric Variables 321 Ona Wu, PhD, Radiology Increased disorders of consciousness in comatose cardiac arrest patients is associated with decreased brain network complexity O. Wu1,2, Radiology, MGH, Charlestown, MA, USA, 2JP Kistler Stroke Boston, MA, USA, 3Department of Neurology, MGH, Boston, MA, USA, 4Department of Cardiac Anesthesiology and Critical Care Medicine, MGH, Boston, MA, USA, 5Department of Medicine, Cardiology Division, MGH, Boston, MA, USA, 6Department of Radiology, MGH, Boston, MA, USA, 7Spaulding Rehabilitation Hospital, Charlestown, MA, USA and 8Department of Neurology, Boston Medical Center, Boston, MA, USA Introduction: For cardiac arrest (CA) survivors initially comatose after restoration of spontaneous circulation (ROSC), the extent of brain injury and expected neurologic outcome are crucial for patient management decisions. Critical knowledge gaps persist in neuroprognostication of comatose post-CA survivors. Early prognostication remains difficult except in extreme cases: patients rapidly awakening do well, and those with minimal brain function do poorly. Most CA patients, however, fall between these extremes. Because hypoxic-ischemic injury is typically diffuse, damage to a network of brain regions is likely involved in the patient's disorder of consciousness. To quantify these complex brain network changes, we applied graph theoretical methods that are increasingly being used to understand human brain connectivity in both health and disease. Specifically, we investigated global efficiency, degree, and clustering coefficient as a function of arousal recovery (defined here as eye-opening either spontaneously or in response to stimulation).Methods: Cardiac arrest patients who remained comatose after ROSC were prospectively enrolled. Coma was defined as Glasgow Coma Scale (GCS) <=8. Five healthy controls were also enrolled. All subjects underwent 3T MRI scans. High-spatial resolution 3D T1-weighted anatomical images were acquired for registration purposes with FOV=256x256 mm 2, acquisition matrix=256x256, 176 sagittal slices (thickness 1 mm). Multiple shell diffusion imaging was acquired using 30 directions with b-values=1000 s/mm 2, and 2000 s/mm2 images. Structural probabilistic connec- tivity maps were a modification of the FSL probabilistic fiber-tracking algorithm. Structural connectivity with respect to each region-of-interest (ROI) was measured using the average probability of streamlines emanating from the source to target ROIs. Network nodes were defined using the Automated Anatomical Labels atlas. Adjacency matrices were computed with a threshold of 10%. Network topology measures were calculated using the Brain Connectivity Toolbox and compared (one-way analysis of variance, followed by post-hoc Student t-tests). To minimize potential confounding effects from early withdrawal of life sustaining therapy (WLST), we excluded subjects who had WLST before 10 days without arousal recovery.263Results: Ten patients (46\u00b126 years old, 40% male) and five controls (37\u00b119 years old, 40% male) were analyzed. Seven patients exhibited arousal recovery (AR), three did not (No AR). Four were alive at discharge, one died from brain death and five died due to WLST. GCS (median [IQR]) was 3 [3-3.75] at admission, and 6.5 [4.5-9] at the time of the research MRI, performed 5.5 [4-8.75] days after initial arrest. Global efficiency, degree, and clustering coefficient (mean\u00b1SD) among healthy controls were 0.44\u00b10.02, 12.7\u00b11.16 and and were No examples of adjancency matrices for healthy control, subject with AR and subject with no AR. Figure 2 shows box-plots of the different metrics across the 3 groups. One-way ANOVA found significant differences across the 3 groups for global efficiency (P=0.0061), degree (P=0.0015), and clustering coefficient (P=0.0059). Post-hoc analysis demonstrated the patient group had significantly lower metrics compared to healthy controls (P<0.05), suggesting loss of brain complexity. Compared to patients with AR, found No AR patients demonstrated statistically significantly lower clustering coefficients (P=0.033) and non-statistically significantly lower efficiency (P=0.15) and degree (P=0.066) values. Conclusion: Patients who failed to exhibit AR demonstrated greater disturbances in structural connectivity compared to patients who recovered arousal. These findings suggest that structural connectivity network measures may have utility in identifying patients who may achieve good outcomes, despite presenting with poor initial GCS scores. Differences in timing of MRI acquisition, potential bias from unblinding of clinical MRI sequences to treating clinicians, and small sample sizes are diagnostic limitations of our findings. Although connectivity findings were not available to the clinical team, unblinding of clinical MRI sequences to clinicians was mitigated by excluding patients with early WLST. In summary, alterations in structural connectivity measured with diffusion MRI show promise in predicting recovery and guiding patient management decisions in comatose cardiac arrest patients. Adjacency matrices for control, patient with AR and patient with no AR who died from brain death. Box plots of reduced (A) global efficiency, (B) degree, and (C) clustering coefficient with increasing disorders of consciousness. 322 Chunxia Xia, MD, PhD, Radiology Sonographic Study on Cervical Extension of Normal Thymus C. Xia, S. Chen, M. Baikpour, Q. Li, Y. Duan, C. Cui and A. Samir Radiology, MGH, Boston, MA, USA Introduction: Although thymus is centered in the superior mediastinum, it could extend into the upper neck and may be visible on thyroid ultrasound with characteristics that might overlap with that of malignant thyroid lesions, or suspected metastasis in patients with concurrent thyroid malignancy. Thus, it is sometimes misinterpreted and recommended for unnecessary fine needle aspiration (FNA), biopsy, or even surgery. This study aims to determine the prevalence of cervical extension of normal thymus on regular thyroid ultrasound exams in children and adolescents and to summarize its ultrasound features to help sonographers recognize them and to avoid unnecessary interventions. Methods: In this retrospective study, patients younger than 18 years old who underwent thyroid and lower neck ultrasound exams from January 1, 2011 to September 31, 2017, were identified and their images were reviewed on Picture Archiving and Communication System (PACS). Exclusion criteria included history of thymic diseases, unsatisfactory images in terms of quality or insufficient visualization of the lower neck. A radiologist with 12 years of experience in head and neck ultraso- nography performed the image review after receiving a focused training on the ultrasound features of the cervical extension of thymus. Cervical thymic tissue on ultrasound was identified as a well-defined focus above the sternal notch, anterior to the trachea and great vessels, and in direct continuity to the chest, without mass effect or displacement of vessels or other structures. The echogenicity of the cervical thymus was scored as following: score 1, predominantly hyperechoic compared to neck muscle (Figure 1A); score 2, mixed high and low echogenicity showing a 3, predominantly hypoechoic with an echotex - ture similar to that of the liver (Figure 1C). The following parameters were also recorded: location (left paratracheal, right paratracheal, and anterior trachea), shape (triangular, quadrilateral, lobulated, round, oval, and irregular), and the maximum anteroposterior diameter, width, and length.264Results: A total of 389 ultrasound exams were identified, from which 303 qualified cases were enrolled, including 218 (71.9%) female and 85 (28.1%) male patients, with an average age of 13.6\u00b14.3 years old. The cervical extension of thymus was found in 112 of the 303 (37.0%) cases, with thymic echogenicity scored as 1, 2 and 3 in 45 (40.2%), 32 (28.6%), and 35 (31.2%) cases, respectively. Seventy-four (66.1%) cases were predominantly localized in the left paratracheal area, 33 (29.5%) were anterior to the trachea, and 5 (4.5%) were in the right paratracheal area. Oval, quadrilateral, and triangular were the most common shapes visualized in 40 (35.7%), 32 (28.6%), and 23 (20.5%) cases, respectively, while irregular, round, and lobulated were found in 7 (6.3%), 6 (5.4%) and 4 (3.6%) cases. The 112 cases had an average anteroposterior diameter of 3.2\u00b1 1.7 cm and width of 2.3\u00b1 0.9 cm. The length was measured only in 12 cases, with a median of 2.3 cm, ranging from 1.8 to 4.1cm. Age was found to be negatively correlated with both presence of cervical thymus and the thymic score (r= -0.186, p=0.001; and r=-0.280, p<0.001). A negative correlation was also found between age and anteroposterior diameter (r=-0.191, p=0.044), but not for the other two measurements of length (p=0.489) and width (p=0.348). Conclusion: Cervical extension of thymus could be detected in as high as 37% of children and adolescents undergoing head and neck ultrasonography, with a lower detection rate and a lower anteroposterior diameter in older subjects. The echoge- nicity of the cervical thymus tends to be increased with patients aging. Figure 1 Ultrasound images (transverse sections) of cervical thymus (arrows) with different thymic scores. A. Thymic score 1 in a 14-year-old girl. Hyperechogenic thymus is visualized with a quadrilateral shape in the left paratracheal area. B. Thymic score 2 in a 16-year-old girl. Thymus tissue with mixed high and low echogenicity is visualized above the sternal notch with an irregular shape. C. Thymic score 3 in a 17-month-old baby boy. Hypoechoic thymus is visualized above right Kai Yang, Ph.D., Radiology Procedure-Specific CT Dose and Utilization Factors for CT-guided Procedures K. Yang3, S. Ganguli1, M. DeLorenzo2, H. Zheng3, X. Li1 and B. Liu1 1Radiology, Massachusetts General Hospital, Boston, MA, USA, 2University of Virginia, Charlottesville, VA, USA and 3Biostatistics Center, Massachusetts General Hospital, Boston, MA, USA Introduction: In addition to serving as a diagnostic modality, Computed Tomography (CT) is also routinely used as a guidance tool for many interventional radiology procedures, including biopsies, aspirations, drain placements, and thermal ablations. For patient protection and practice quality improvement, radiation dose associated with each CT guided intervention (CTGI) procedure should be closely monitored and analyzed. For diagnostic CT, radiation dose monitoring programs have become mandatory and national reference dose levels are being established. For interventional CT, establishing reference dose levels is challenging as the dose for each specific procedure is dependent on a wide variety of factors, including each patient size, target location, and the complexity of the specific interventional task. To the best of our knowledge, currently there is no reliable data available for CTGI procedures. Our motivation for this study is that by performing a large-scale survey of CTGI procedures and sorting the procedures into detailed clinically meaningful categories, we can achieve consis- tent and procedure-specific dose metric distributions and reference standards. Furthermore, by analyzing CT usage-related parameters, we can help create a numeric metric to evaluate and compare the CT usage of different sub-categories of CTGI procedures. Methods: This retrospective study collected dictation reports and radiation dose data from 9143 consecutive CTGI procedures performed at MGH on adult patients from 2012-2017. All procedures were sorted into four major interventional categories: ablation, aspiration, biopsy, and drainage; each of which was further divided into sub-categories. After exclusion, a total of 8213 procedures (4391 on men and 3822 on women) were divided into 21 sub-categories. Distributions of dose metrics (CTDI and DLP) and CT usage-related parameters (acquisition count and number of images) were analyzed by category with descriptive statistic outcomes, using statistic software R (ver. 3.2.4). Quantitative CT utilization factors (which measure average CT usage) for each interventional sub-category were derived using total scan length, acquisition count, and number of images.Results: Table 1 shows the detialed category list and median dose data. The global median DLP and CTDIvol from our study is 1043 mGycm and 12 mGy. The global median value of acquisition count is 9, while typical diagnostic abdomen CT scans only have two series (pre and post contrast scans). The global median value of total scan length is 842 mm and number of images is 166. Large variations of the dose metrics and utilization parameters among different sub-categories of CTGI 265procedures were observed. Using DLP as an example, the lowest and highest median DLP value ranged from 569 mGycm (B1, CT-guided 2788 mGycm (AB3, CT-guided soft tissue ablation). These results validated the rationale to perform procedure-specific dose and utilization analysis at the sub-category level for CTGI procedures. Figure 1 plots the ranked overall utilization factor for all the sub-categories. The utilization factors ranged from 0.6 (B4, soft tissue biopsy) to 3.6 (AB5, lung ablation). CT-guided abdominal drainages (D1, D2, D3, marked with arrows) can provide a good example of validation to demonstrate the clinical relevance of these derived utilization factors. For single drain (D1), double drains (D2), and three plus drains (D3), the overall utilization factor values are 0.9, 1.6, and 2.0, respectively. Conclusion: Through analysis and detailed categorization of a large number of CT-guided interventional procedures, this study provides consistent, procedure-specific dose metric distributions and quantitative CT utilization factors for CT-guided interventional procedures. CT Guided Interventional Procedure Categories Procedure specific CT utilization factors. The procedure codes were used to label the sub-category (Details in Table 1). Each box was color-coded per its major category. With the global reference utilization factor as 1 (average CT usage level across all procedure types), CT utilization factors for abdomen drainage procedures (D1 for one drain, D2 for two drains, D3 for three or more drains) are 0.9, 1.6, and 2.0, respectively (marked with arrows in the plot). This indicates that for procedures with three or more drains, CT usage is more than double of that for single-drain procedures (2.0 vs. 0.9). 324 Tae-Sung Yeum, Psychiatry Previous history of suicide attempts predict worse clinical outcome in bipolar disorder T. Yeum1, L. Sylvia1, N.E. Hall2, T. Deckersbach1 and A.A. Nierenberg1 1Department of Psychiatry, Massachusetts General Hospital, Boston, MA, USA and 2Biostatistics Center, Massachusetts General Hospital, Boston, MA, USA Introduction: Bipolar disorder is characterized by debilitating episodes of depression and mood elevation, and it has been reported to impose a great burden globally. Suicidal thoughts and behaviors occur frequently among affected individuals and are associated with worse outcomes. Researchers have focused on the association between treatment outcome and prevalence of suicide behaviors. However, there is limited evidence on the impact of previous suicide attempt history on the longitudinal course of bipolar disorder. This study investigates the effect of previous suicide attempts on the trajectory of bipolar disorder in a large, heterogeneous sample enrolled in a comparative effectiveness study.Methods: The Bipolar CHOICE (Clinical Health Outcomes Initiative in Comparative Effectiveness) study enrolled 482 participants (ages: 18 to 68 years) across 11 sites nationwide between 2010 and 2013. The follow up period was 6 months and they were assessed for CGI-BP(Clinical Global Impressions-Bipolar Version), LIFE-RIFT(Longitudinal Interval Follow-up Evaluation-Range of Impaired Functioning Tool), Q-LES-Q(Quality of Life Enjoyment and Satisfaction Questionnaire), and CHRT(Concise Health Risk Tracking). We conducted t-tests for continuous variables and chi-square tests for dichotomized variables to assess clinical and demographic differences between individuals who reported a history of suicide attempt and those who did not. We conducted mixed linear models with fixed effects for baseline suicide attempt history, time, and the attempt history-by-time interaction, to examine if baseline history of suicide attempts predicted worse outcomes over the study for individuals with bipolar disorder.Results: We found that 188 (39.09%) participants had a previous suicide attempt. The two groups showed significant differ- ence of means in overall CGI-BP scores (or global illness burden), CGI-BP for depressive symptoms, LIFE-RIFT, Q-LES-Q, and CHRT score. Previous history of suicide attempts was not associated with age, CGI-BP for manic symptoms, female gender, marital status, and comorbidity of anxiety disorder (all p's > .05). Changes in CGI-BP over 24 weeks are illustrated in Figure 1. In the linear mixed effects model, we found that there was a statistically significant group by time interaction (p < .05) such that those with a history of a suicide attempt had worse outcomes in regards to global illness burden 266(overall CGI-BP score). There was also a significant group by time interaction (p < .05) in the linear mixed effects model defining CGI-BP for depressive symptoms as a dependent variable, however in the case of manic CGI-BP the interaction was not significant. Conclusion: We found that individuals with bipolar disorder who had a previous suicide attempt experienced worse clinical outcomes, or overall illness burden, than those without such a history. These data suggest that individuals who have experienced a suicide attempt may require additional care, attention and adjunct treatments than those without a history of suicide attempt. Table 1. Baseline demographic and clinical characteristics among CHOICE participants by history of previous suicide attempts CGI-BP, Clinical Global Impressions-Bipolar Version; CHOICE, Clinical Health Outcomes Initiative in Comparative Effectiveness; CHRT, Concise Health Risk Tracking; LIFE-RIFT, Longitudinal Interval of Impaired Functioning Tool; SD, Standard Deviation; Q-LES-Q, Quality of Life Enjoyment and Satisfaction Questionnaire. Figure 1. Course of clinical improvement of (A) CGI-BP: overall, (B) CGI-BP: mania, and (C) CGI-BP: depression over 24 weeks in patients with bipolar disorder and with or without history of suicide attempts. CGI-BP, Clinical Global Impressions-Bipolar Version. 325 Amy Yule, MD, Psychiatry High Correspondence Between Child Behavior Checklist Rule Breaking Behavior Scale with Conduct Disorder in Males and Females A. Yule1, M. Fitzgerald1, Biederman1 Psychopharmacology, Massachusetts General Hospital, Boston, MA, USA and 2Psychiatry, SUNY Upstate Medical University, Syracuse, NY, USA Introduction: Considering the high morbidity associated with conduct disorder (CD), the development of easy to use methods to help identify children who may have CD could have high clinical and public health relevance. This study investi - gated the diagnostic utility of the Child Behavior Checklist (CBCL) Rule-Breaking Behavior scale to identify children of both sexes with CD.Methods: Subjects were derived from four independent datasets of children with and without attention deficit hyperac-tivity disorder and bipolar-I disorder of both sexes. Subjects were recruited from pediatric and psychiatric clinics and the community. All subjects had structured diagnostic interviews with raters blinded to subject ascertainment status. Receiver operating characteristic (ROC) curves were used to examine the ability of the CBCL Rule-Breaking Behavior scale to identify children with and without CD.Results: The sample consisted of 674 subjects (mean age of 11.7 \u00b1 3.3 years, 57% male, 94% Caucasian). Seventeen percent of participants (N=114) met structured interview criteria for the diagnosis of CD. The ROC analysis of the CBCL Rule-Breaking Behavior scale yielded an area under the curve of 0.9. A score of 60 on the CBCL Rule-Breaking Behavior scale correctly classified 82% of subjects with CD with 85% sensitivity, 81% specificity, 48% positive predictive value, 96% negative predictive value.Conclusion: The CBCL Rule-Breaking Behavior scale was an efficient tool to identify children with CD. Considering the poor prognosis associated with CD in youth of both sexes, including the risk for substance use disorders, these findings may have large clinical and public health relevance.267Sensitivity, specificity, and percent correctly classified using the conservative cut-off of >60 on the Rule- Breaking subscale of the Child Behavior Checklist to identify youth with conduct disorder in each study. Demographic characteristics of those with and without conduct disorder from the individual studies and all studies combined Not everyone has Socioeconomic Status (SES) reported. Smaller sample sizes for this measure were: Girls ADHD Study: 219, BP Disorder Controlled Study: 115; Sample size for SES for all studies combined: 629 Not everyone has race reported. Smaller sample sizes for this measure were: Girls ADHD Study: 215; BP Disorder Controlled Study: 152; Sample size for race for all studies combined: 662 326 Amanda B. Zheutlin, Ph.D., Center for Genomic Medicine (CGM) Validation of schizophrenia polygenic risk scores in 90,000 patients across three healthcare systems A.B. Zheutlin1, Massachusetts General Hospital, Boston, MA, USA, 2Department of Medicine, Vanderbilt University Medical Center, Nashville, TN, USA, 3Department of Biomedical and Translational Informatics, Geisinger Health System, Rockville, MD, USA, 4Department of Biomedical Informatics, Vanderbilt University Medical Center, Nashville, TN, USA and 5Autism and Developmental Medicine Institute, Geisinger Health System, Lewisburg, PA, USA Introduction: One of the major rate-limiting features of advancing treatment and intervention for psychiatric disorders is the lack of robust risk stratification tools. Given the substantial heritability of many psychiatric disorders, quantitative measures of genetic risk may be useful towards this end. Polygenic risk scores (PRSs), in particular, are easy and cheap to generate and can be calculated well before illness onset. However, in order to bring these instruments to the clinic, they must first be validated as predictors of clinical diagnoses given to patients in real-world clinical settings - an often much messier set of phenotypes than those used in research samples, especially across disparate healthcare systems.Methods: As part of the PsycheMERGE Consortium - a new collaboration from within the NIH-funded Electronic Medical Records and Genomics (eMERGE) network - we generated PRSs for schizophrenia using summary statistics available from the Psychiatric Genomics Consortium (PGC) and applied them to real-world clinical datasets from three large-scale, indepen- dent healthcare systems. PRS were calculated for individuals of European-American ancestry with genomic data from the Partners Healthcare Biobank (N=15,763), Vanderbilt University Medical Center biobank (BioVU) (N=18,358) and Health System MyCode dataset (N=56,926). Associations between each PRS and all medical outcomes available in patient electronic health records were assessed using univariate logistic regression in a phenome-wide association study (pheWAS). Medical outcomes were defined using 'phecodes': a hierarchical grouping of ICD-9 diagnostic codes used to reduce tens of thousands of individual codes into fewer than 2000 disease categories. Tests were corrected using a Bonferroni adjustment for the number of phecodes. Effects across sites were combined in meta-analysis using an inverse-variance random effects model.Results: Meta-analysis across healthcare systems revealed significant associations between schizophrenia PRSs and fourteen phenotypes: ten psychiatric disorders, two genitourinary, and two from other disease categories (Figure). The strongest association was with schizophrenia -- individuals in the top decile of genetic risk for schizophrenia were 2.8 times as likely to be diagnosed with the disorder than all others (and 5.1 times as likely as the bottom decile).Conclusion: We have validated schizophrenia PRSs using real-world clinical diagnoses across multiple healthcare systems. The effects were somewhat smaller than those observed in research samples (e.g., in the original PGC study, the odds of schizophrenia in the top versus bottom PRS deciles were 7.8-20), potentially due to a more representative, less \"clean\" control sample. Nonetheless, these results suggest schizophrenia PRSs could be integrated with other clinical data into risk stratification efforts in healthcare settings. Furthermore, they highlight previously unknown non-psychiatric conditions that may be common comorbidities with schizophrenia, warranting further follow-up.268 327 Pingan Zhou, MD, Radiology The Relative Contribution of Each Sonographic Feature to the Inter-Observer Variation of Thyroid Imaging, Reporting and Data Systems (TIRADS) P. Zhou Radiology Department, MGH, Yanan, China Introduction: Inter-observer variation is an important problem when using ultrasound for thyroid disease evaluation, however, there is no report about the relatively different contribution of each sonographic feature for the current American College of Radiology (ACR) TI-RADS system. This study is to explore the contribution of each sonographic feature to the inter-observer variation in TIRADS, and how one ultrasound feature may influence the others on the inter-observer variation. Methods: Retrospectively collect the patients underwent ultrasound exams in the past four months. All thyroid ultrasound exams were performed using the GE machines, and going through the same scanning protocol. Ultrasound exams were reviewed by three radiologists with more than 5 years' experience in thyroid ultrasound diagnosis. All enrolled cases are divided into training cases and final analysis cases. Based on the ACR-TIRADS, the scores were recorded at the feature level and total score level. Logistic regression analysis was performed to analyze the agreement association between ultrasound features and the TIRADS total score. Results: In the study period, 178 patients with 178 nodules were enrolled in this study (age 46.8\u00b1 10.5yrs, female 131, male 47). All patients were randomly divided into two groups, one for training (n=83), another group for final analysis (n=95). The inter-observer agreement (kappa value) in the training cases and final cases were 0.66 vs. 0.70 in the total score, 0.65 vs. 0.68 in composition, 0.64 vs. 0.69 in echogenicity, 0.75 vs. 0.78 in shape, 0.56 vs. 0.63 in margin, and 0.66 vs. 0.70 in echogenic foci. By comparing the kappa values between each feature and total TIRADS score, univariate logistic regression analysis showed the margin had the highest OR scores in both training cases (OR=4.6) and final cases (OR=3.8), which implied the margin is the most important feature to be emphasized to avoid inter-observer variation in TIRADS system. Multivariate analysis showed the shape was an independent factor for the total score agreement. Conclusion: Our results showed ultrasound features had different contributions to the inter-observer agreement. The shape was the most leading variation resources in both training cases and final cases, which should be highlighted when we use the TIRADS system. Further investigation is warranted to validate the study findings by involving more radiologists.269328 Nancy Zhu, Oral and Maxillofacial Surgery Retrospective Review of Facial Dog Bites Treated at MGH over a 10-year Period N. Zhu and M. August Oral and Maxillofacial Surgery, MGH, Boston, MA, USA Introduction: The Centers for Disease Control and Prevention estimated > 118,000 dog-bite nonfatal injuries for children < 18 years of age living in the United States in 2015, ranking dog bites among the top 5 nonfatal injuries for children aged 0-9 years. To further highlight the burden of these injuries, the Insurance information Institute reports a 93.4% increase in estimated number and cost of dog bite claims nationwide between the years of 2003 and 2017. The cost of these dog bite claims accounted for more than one third of all homeowners insurance liability claims paid out in 2014, costing in excess of $500 million. This only includes data from claims that were filed, suggesting that the actual cost may be far higher than we see. However, despite the prevalence of dog bite injuries among the pediatric population and the obvious financial burden that it places on both the victim as well as the health care system, there is no nationwide mandatory reporting or consolidation of this data. Looking at Massachusetts specifically, it has one of the best laws relating to dog attacks in the country. The focus, however, is on the legality of who is at blame when a dog attacks and under what circumstances. Even with such detailed laws in place, they do not have a mandatory reporting system that helps to document the prevalence of dog bite attacks in order to help further prevent dog bite attacks. This lack of reporting makes it difficult to obtain information from the incident and identify certain characteristics of the victims. Current literature shows that educational programs to train both the children and parents on how to behave around dogs. However, we are lacking sufficient knowledge to better target outreach efforts and patient education. Of the dog bite injuries that have been documented in the pediatric population, a disproportionate amount of approximately 80% involve the head and neck. Various studies have identified risk factors for suffering a dog bite injury such as being of school-age, smaller physical size, and short stature. Current research shows a general consensus in bedside and surgical treatment of dog bite injuries that focus on cleansing of the wound, debridement, primary repair, antibiotic therapy, immunization when needed, and wound care. However, there are relatively few studies documenting optimal treatment options that help to minimize long term consequences psychosocially. There is also a need for identification of risk factors in terms of the dog, such as breed and history of aggression. The interplay between these factors will help bring to light the relationship between types of injuries and dog breeds as well as the correlation between the type of injury with the need for follow-up and long-term psychosocial effects. This relationship is critical to understand so that we can not only better help the patient at the time of injury, but also as they move forward into their day to day lives. Our study hopes to consolidate data from the emergency department from the Massachusetts General Hospital between 2006 and 2016 in order to identify recurring characteristics in dog bite victims in order to generate a form that will help streamline the reporting process in the future. We are also are aiming to address the front end of the dog bites injuries in pediatric patients situation by using this data to help better target outreach and education programs for both parents, and children. Methods: Patients for the retrospective study were identified in the Department Oral and Maxillofacial Surgery at Massachu- setts General Hospital (MGH) operating room log using the key term 'facial dog bite'. 42 cases were identified through this method. Electronic health records of selected cases were carefully reviewed for circumstances, patient medical history, dog characteristics, legislative requirements related to the injury, standard treatment protocols pre- and post-op, location of the injury, culture data, type of injury, long-term consequences of injury, and last follow-up date. Patient medical record numbers (MRN) were de-identified and recoded for documenting data intake. Limitations: The database between 2006 and 2016 at MGH for patients that fit our criteria may be less than we expect, thus limiting the power and generalizability of our results. In addition, we are limited to the data that can be found in patient charts given the retrospective nature of this study.Results: Data collection and analysis is on-going. They will be completed by the end of August 2018. Conclusion: With our data, the general public will have an increased awareness of the prevalence of dog bite injuries in the pediatric population, which may help prevent a further increase in incidences. Furthermore, with greater comprehension of the specific factors that contribute to the prevalence of dog bites in children, precautions and policies can be better implemented. Finally, the data generated in this study will aid surgeons in treatment planning by showing which management protocols have the greatest outcomes in association with the type of case, improving treatment outcomes.270329 Yiwen Zhu, Psychiatry Using Simulations to Examine Novel Model Selection Methods in Epigenome-Wide Analyses Y. Hospital, Boston, MA, USA, 2Department of Psychiatry, Harvard Medical School, Boston, MA, USA, 3Stanley Center for Psychiatric Research, The Broad Institute of Harvard and MIT, Cambridge, MA, USA, 4MRC Integrative Epidemiology Unit, School of Social and Community Medicine, University of Bristol, Bristol, United Kingdom and 5Applied Statistics Group, University of the West of England, Bristol, United Kingdom Introduction: The availability of high-throughput genetic, epigenetic, and other types of data has spurred \"big data\" analyses that are revolutionizing our understanding of the biological underpinnings of diseases. At the same time, advancements in machine learning have brought forth exciting and sophisticated tools that are well suited for handling high dimensional data. However, there is limited understanding of how best to translate novel statistical methods into the big data setting and whether and when problems may arise. For example, little is known about whether standard multiple testing corrections applied in big data analyses are appropriate. Moreover, the relative power of novel statistical methods in the -omics data context - where effect sizes are usually small - is unknown. In this study, we summarize results from a series of simulation studies aimed at understanding the performance of a machine learning approach grounded in least angle regression (LARS; Efron et al., 2004) that was applied to epigenome-wide DNA methylation data. Previously, we used the LARS and an associated covariance test (Lockhart et al., 2014) to examine the effect of early life adversity exposure on epigenome-wide DNA methylation (DNAm) marks at age 7 by simultaneously comparing three theoretical models (Dunn et al., 2018; Smith et al., 2015). As the method had not been applied in the epigenetic setting before, we compared the Type I error rate and power of the covariance test with other post-selection inference methods under a range of different scenarios. Methods: We compared the following post-selection inference methods: (1) the covariance test (Lockhart et al., 2014), (2) an exact test based on a truncated Gaussian distribution (Tibshirani et al., 2014), (3) a new method based on a multivariate t-distribution, (4) a na\u00efve approach, where standard epigenome-wide association study (EWAS) was performed for each hypothesis, and (5) a na\u00efve Bonferroni approach, where the p-values obtained in the standard EWAS were corrected using a Bonferroni correction. The alpha value was set to 1x10 -7 (0.05/485,000), the standard epigenome-wide Bonferroni corrected threshold. To investigate the Type I error rate, the single-locus methylation values were simulated under the null first as a random normal variable ()) and later as actual DNAm values resampled from observed data. Given the epigenome-wide scope, 485,000 tests were performed. The sample size varied from 175 to 2,800. To compare the power of the five methods, we simulated data under a range of effect sizes. Similarly, the outcome was first simulated as a random normal variable with r 2 varying from 0.01 to 0.1. We additionally simulated the outcome variables as beta distributions with different parameters to cover a range of methylation status; this approach captures the underlying data generating process of DNAm microarrays more closely than normal variables (Tsai and Bell, 2015). The real exposure and outcome data were obtained from the Avon Longitudinal Study of Parents and Children (ALSPAC), a population-based birth cohort (Fraser et al., 2013; Relton et al., 2015). In the simulations, we used data capturing the exposure to sexual or physical abuse (by anyone) and constructed eight competing hypotheses. The first hypothesis was set to be the true underlying hypothesis in the power simulations.Results: Under the null hypothesis, the covariance test had inflated Type I error even when the outcome variables were normally distributed and the sample size increased. The p-values were inflated compared with other methods across the three scenarios, meaning that they were more extreme than expected. Although we were unable to perform a large number of simulations, the presence of false positive hits suggested that the family-wise error rate was above 0.05 in most cases, especially for the covariance test. The power simulations showed that the covariance test had higher power compared to other methods. The multivariate t-distribution based method had the highest coverage probabilities compared to other inference tools. Conclusion: These simulations suggested that while the covariance test had the highest statistical power compared to other methods, it did not adequately control Type I error rate when a large number of tests were performed simultaneously and the corresponding p-values had obvious inflations. The findings highlight the importance of evaluating p-values in conjunction with other evidence especially when translating novel methods into the big data setting. The study also demonstrates the usefulness of simulation studies in empirical research. As the process allows researchers to think through the assumptions of the techniques and the statistical power conditional on different sample sizes and effect sizes, we show that simulations help better assess the strength of scientific evidence in the context of clinical research.271 De p a r t m e n t InDe x MGH Page No.Anesthesia, Critical 131, 146, 163, 203, 225, 230 Athinoula A. Martinos Center 51, 96, 109, 152, 166, 168, 213, 234, 239, 267 Center for Systems Biology 71, 85, 88, 122, 135, 167, 169, 187, 191, 208, 209, 210 Emergency . . . . . 19, 24, 42, 70, 89, 92, 93, 96, 107, 110, 123, 142, 154, 160, 171, 179, 182, 185, 189, 211, 227, 228, 259 Massachusetts Eye and Ear 17, 48, 79, 97, 103, 104, 105, 106, 161, 231, 257 28, 33, 58, 62, 74, 131, 143, 147, 157, 173, 176, 194, 197, 211, 226, 228, 235, 238, 38, 71, 82, 116, 127, 181, 183, 195, 204 Performance Analysis and 20, 21, 23, 27, 30, 37, 40, 41, 42, 45, 47, 51, 53, 55, 63, 64, 68, 69, 72, 73, 75, 77, 78, 81, 85, 92, 93, 97, 98, 101, 102, 108, 111, 121, 123, 126, 129, 130, 134, 136, 142, 148, 149, 150, 151, 155, 158, 164, 170, 175, 179, 180, 186, 188, 189, 190, 192, 208, 217, 220, 222, 223, 233, 245, 250, 251, 253, 256, 265, 266, 270 Psychiatry - Benson Henry Institute for Mind Body 20, 37, 47, 65, 66, 114, 118, 137, 138, 140, 145, 174, 177, 178, 201, 202, 233, 255, 262, 263, 264, h o r In d e x Page No. Page h o r In d e x Page No. Page h o r In d e x Page No. Page h o r In d e x Page No. . 4, 47, h o r In d e x Page No. Page h o r In d e x Page No. Page h o r In d e x Page No. h o r In d e x Page No. Au t h o r In d e x Page No. Page h o r In d e x Page No. Page h o r In d e x Page No. Page h o r In d e x Page No. Page h o r In d e x Page No. Page t h o r In d e x Page No. Page h o r In d e x Page No. Page h o r In d e x Page No. Page t h o r In d e x Page No. Page "}