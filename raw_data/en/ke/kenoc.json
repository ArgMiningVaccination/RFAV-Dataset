{"title": "PDF", "author": "PDF", "url": "https://trec.nist.gov/pubs/trec31/papers/Overview_neuclir.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "Overview of the TREC 2022 NeuCLIR Track Dawn Lawrie, Sean McNamee, Douglas W. Oard, Luca Soldaini, Eugene Yang John Hopkins University Human Language Technology Center of Excellence, University of Glasgow,University of Maryland,Allen Institute for AI lawrie@jhu.edu,sean.macavaney@glasgow.ac.uk,james.mayfield@jhuapl.edu mcnamee@jhu.edu,lucas@allenai.org,oard@umd.edu,eugene.yang@jhu.edu ABSTRACT This is the first year of the TREC Neural CLIR (NeuCLIR) track, which aims to study the impact of neural approaches to cross- language information retrieval. The main task in this year's track was ad hoc ranked retrieval of Chinese, Persian, or Russian newswire documents using queries expressed in English. Topics were devel- oped using standard TREC processes, except that topics developed by an annotator for one language were assessed by a different an- notator when evaluating that topic on a different language. There were 172 total runs submitted by twelve teams. 1 INTRODUCTION Cross-language Information Retrieval (CLIR) has been studied for more than three decades, first appearing at the Text Retrieval Con- ference (TREC) in TREC-4 [ 9]. Prior to the application of deep learning, strong statistical approaches were developed that work well across many languages. Though as with most other language technologies, neural methods have led to substantial improvements in information retrieval. Several factors combined to make us feel that the time was right to press for rapid progress in CLIR: Research Community. There have been recent programs focused on CLIR such as IARPA MATERIAL1and the Johns Hopkins Human Language Technology Center of Excel- lence (HLTCOE) Summer Camp for Applied Language En- gineering (SCALE) 20212. Recent interest among natural language processing researchers in the related problems of cross-language question answering and development of multilingual embeddings have produced a new crop of re- searchers familiar with and interested in CLIR and related tasks. Algorithms. Neural advances in the state of the art in monolingual retrieval have been appearing for several years. Improvements in cross-language IR have come just in the last year or two. Data. The appearance of MS MARCO led to rapid advances in monolingual IR. Translations of MS MARCO into other languages have allowed training of CLIR systems. Addi- tional resources that could also be useful for training neu- ral CLIR models have also appeared recently, including [ 17],4WikiCLIR [ Earlier systems for experimental IR have recently been supplemented by systems such as PyTer- rier [ 18]7and Castorini8that support neural methods, and by such as Patapsco [ 8]9that are designed specifi- cally for CLIR. These systems provide a base on which to build, somewhat lowering barriers to entry, and providing a source for baselines to which progress can be compared. The NeuCLIR track was designed to take advantage of this conflu- ence of interest and resources to push the state of the art in neural CLIR forward. We expect the track to help to answer at least the following questions: What are the best neural CLIR approaches? How do the best approaches compare to the straightforward combination of machine translation and monolingual IR? How do the best neural approaches compare to the strongest statistical approaches to CLIR? Can reranking further improve retrieval effectiveness using techniques that would be impractical for full-collection retrieval? How do the resource requirements for the various approaches compare? What resources are most useful for training CLIR systems? What are the best neural multilingual information retrieval (MLIR) approaches for producing a single ranked lists con- taining documents in several languages? NeuCLIR 2022 has helped start to answer these questions. The track will continue in 2023. The NeuCLIR track maintains an official website at: https:// neuclir.github.io. 2 TASK DEFINITION We explore three tasks TREC 2022 NeuCLIR track: ad hoc CLIR, reranking CLIR, and monolingual. All three tasks use the same document collections, topics, and relevance assessments. Monolin- gual runs use topics manually translated into the language of the documents; ad hoc and reranking runs use the original English top- ics. Ad hoc runs rank documents from the entire collection, while 3https://github.com/ssun32/CLIRMatrix runs rank only the 1,000 documents that appear in the output of a NIST-provided initial run. 2.1 Ad Hoc CLIR Task The main task in the NeuCLIR track is ad hoc CLIR Systems receive a document collection in Chinese, Persian, or Russian, and a set of topics written in English. For each topic, the system must return a ranked list of 1,000 documents drawn from the entire document collection of the target language, ordered by likelihood and degree of relevance to the topic. Runs that use a human in the loop for ad hoc retrieval (or had design decisions influenced by human review of the topics) are indicated as \"manual\" runs; all others are considered \"automatic.\" 2.2 Reranking CLIR Task The reranking task provides an initial ranked list of 1,000 retrieved documents from the document collection. Each ranked list is the output of a BM25 retrieval system, which used document translation to cross the language barrier. The run ids are coe22-bm25-td-dt-* where *iszhofor Chinese, fasfor Persian, and rusfor Russian. The runs appear in bold in Tables 8, 9, and 10. These runs use the English title and descriptions for queries. Systems are then asked to rerank the documents to produce a new ordering that improves an evaluation metric. This task is suitable for teams that want to focus on second-stage scoring models, rather than on models which search an entire collection. 2.3 Monolingual Retrieval Task While monolingual retrieval is not a focus of the NeuCLIR track, monolingual runs can improve assessment pools and serve as good points of reference for cross-language runs. The monolingual re- trieval task is identical to the ad hoc task, but it uses topic files that are human translations of the English topics into a target language in a way that would be expressed by native speakers of the language. This task is suitable for teams looking to explore monolingual rank- ing in languages other than English. It is also a lower barrier to entry task for teams that are interested in the track. 3 DOCUMENTS There are three components of the 2022 NeuCLIR test collection: documents, topics, and relevance judgments. In this section we describe the documents. The document collection, NeuCLIR-1, consists of documents in three languages: Chinese, Persian, and Russian, drawn from the Common Crawl news collection.10The documents were obtained by the Common Crawl service between August 1, 2016 and July 31, 2021; most of the documents were published within this five year window. Text was extracted from each source web page using the Python utility Newspaper .11 While NIST made the documents (and topics) available for par- ticipants and will distribute them for the foreseeable future, an alternative source of the document collection can be obtained di- rectly from Common Crawl, which is the original source. A github 10https://commoncrawl.org/2016/10/news-dataset-available/ 11https://github.com/codelucas/newspaperrepository12facilitates this by providing code to access the docu- ments via their Universally Unique Identifiers (UUID). The process extracts the text from the documents and then matches the de- scriptor to ensure that both track participants and non-participants index the same documents. The collection was distributed to participants in JSONL, a list of JSON objects, one per line. Each line represents a document. Each document JSON structure consists of the following fields: id:UUID assigned by Common Crawl cc_file: raw Common Crawl document time: time of publication, or null title: article headline or title text: article body url: address of the source web page To ascertain the language of each document, its title and text were independently run through two automatic language identifica- tion tools, cld313and an in-house tool, VALID [19], a compression- based model trained on Wikipedia text. Documents for which the tools agreed on the language, or where one of the tools agreed with the language recorded in the web page metadata were included in the collection under the language of agreement; all others were removed. This is an imperfect process and some documents com- prised of text in other languages are in the collections. The extent of the language pollution is unknown; however, annotators did sometimes encounter out-of-language documents in the pools of assessed documents. These documents were always considered not relevant. While we expected this process to make errors, we had assumed that no systems would retrieve out-of-language docu- ments. This assumption proved to be false as some systems ranked documents in other languages highly. All documents with more than 24,000 characters (approximately 10 pages of text) were also removed, as very long documents create challenges in assessment. Additionally, very short documents were also removed, specifically: Chinese documents containing 75 or fewer characters, Persian doc- uments containing 100 or fewer characters, and Russian documents containing 200 or fewer characters. We observed that such docu- ments are often not genuine news articles, frequently consisting of isolated headlines or commercial advertisements. Each collection was limited in size to at most 5 million documents. After removing duplicates, the Russian collection was significantly above this threshold. Therefore, we used Scikit-learn's implemen- tation of random sampling without replacement14to downsample the collection. Final collection statistics appear in Table 1. 4 TOPICS NeuCLIR 2022 topics were developed to be traditional TREC-style information needs that are broader than CLIR question answering, which can be answered with a phrase or sentence. Topic develop- ment was done in two phases. First, assessors created a topic by writing an English description and searching for relevant docu- ments in their language. Subsequently, pools were created 14https://scikit-learn.org/stable/modules/generated/sklearn.utils.random.sample_ without_replacement.html 2Overview of the TREC Table 1: Document Collection Statistics for NeuCLIR-1 (token counts from Spacy) .Document Avg. Chars Median Chars Avg. Tokens Median Tokens Language count per Document per Document per Document per Document Chinese 3,179,209 743 613 427 356 Persian 2,232,016 2032 1427 429 300 Russian 4,627,543 1757 1198 301 204 track submissions for assessors to judge. This section focuses on topic development; Section 5 addresses relevance assessment. During topic development, assessors wrote the title, description, and narrative components of the topic and then examined thirty documents that they discovered through interactive monolingual search using a web interface to the Patapsco framework [ 8]. They recorded the number of relevant documents they discovered. Any topic where the assessor recorded more than twenty relevant docu- ments was deemed too productive for inclusion in the collection; such topics are deleterious to collection reusability because they can lead to too many relevant documents being unjudged. With the idea of adding a multilingual task to NeuCLIR 2023, we wanted topics that would exhibit relevant documents in multiple languages. Assessors therefore also judged documents for topics developed by assessors in other languages. This process developed 137 topics. A total of 89 of these were evaluated in Chinese, 69 in Persian, and 104 in Russian. All topics were examined by one of the organizers; topics that were overly ambiguous, too similar to another topic, or judged by the orga- nizers to be otherwise inappropriate were eliminated. A total of 114 topics remained; these were distributed to participants. Human translations of each topic into the three collection languages were produced, and each was subsequently vetted by a language expert to ensure translation quality. These translations were used for the monolingual task. Given that no language assessed all 137 topics, there was insuffi- cient information available after the first phase of topic development by NIST to select multilingual topics. To further evaluate topics and in particular to identify topics that would have some but not too many relevant documents, additional assessors were used to judge the topics. These additional assessors, who will be referred to as non-NIST assessors , had proficiency in the language for which they provided annotation, but generally were not native speakers of the language. The non-NIST assessors judged 63 topics in Chinese, 67 in Persian, and 68 in Russian. They used the Patapsco framework to perform monolingual retrieval using BM25; however, the inter- face for interacting with the system was different from that used by the NIST assessors. Two main differences were that non-NIST assessors provided document judgments with the interface, rather than reporting a count; and they had access to HiCAL [ 2], an active learning system, to recommend documents for assessment. While non-NIST assessors were encouraged to judge thirty documents, the actual number varied between five and sixty-two, with a me- dian of fifteen documents judged per topic. Assessors identified between zero and eighteen relevant documents with an average of 4.6 documents per topic. Because of the lack of consistency of the number of judged documents per topic, rather than using a cutoff of20 documents out of 30 as a sign of too many relevant documents, any topic with more than 65% relevant documents was deemed too productive to contribute to a reusable collection. The viability of each topic in each language was determined using both NIST assessments and non-NIST assessments. Figure 1 shows the percentage of relevant documents found for annotated topics during initial development by the two teams of assessors. A topic was viable if at least one relevant document was found by a NIST or non-NIST assessor and the topic did not appear to have too many relevant documents to support collection reusability. If there was a disagreement about the prevalence ( i.e, percentage ) of relevant documents for a topic, the NIST assessors value was used. Therefore there were cases when the NIST assessor found at least one relevant document and the additional assessment did not find any. In addition, there were topics where NIST assessors identified fewer than twenty relevant documents, but the non-NIST assessors found that more than 65% were relevant. Disagreements could come from the fact that different documents were examined and because relevance is an opinion. A priority list of topics was assembled to favor topics that ap- peared to be viable in all three languages over topics that appeared to be viable in only two languages. Each topic was assigned to a category. Figure 2 shows the distribution for all 114 topics and the distribution of the fifty topics selected for further relevance assessment. The intent was to evaluate all fifty topics in all three languages. 5 RELEVANCE JUDGMENTS Once systems were run, relevance assessment began on the chosen subset of the 114 topics submitted for each run. In the following we describe how the judgment pools were assembled and how relevance was determined. 5.1 Creating Judgment Pools Pools were created from the top-ranked documents of submitting systems. The number of documents a run contributed to a pool was based on whether the submitting team marked the run as a baseline run. Baseline runs contributed their top twenty-five documents, while non-baseline runs contributed their top fifty documents. Thus for nDCG@20 all submissions have complete judgments. 5.2 Relevance Judgment Process Assessors used a four-point scale to judge the relevance of each document. Assessors were instructed to provide their assessment as if they were gathering information to write a report on the topic. Relevance was assessed on the most valuable information found in the document; the grades Documents(b) Non-NIST Assessments Language Chinese Persian Russian Figure 1: Percentage of documents judged relevant reported by the two groups of assessors during the preliminary topic development. 379 3 10 8 4 43(a) Categorization of All T opics 35 9222(b) Categorization of T opics Selected For Assessment All 3 annotated by NIST after Non-NIST annotation Non-NIST annotators did not Farsi/Chinese bilingual topics Farsi/Russian bilingual topics Russian/Chinese bilingual topics Problematic topics Monolingual topics Figure 2: Prioritization categories for all topics and for topics selected for pooling. Very Valuable: information that would appear in the lead paragraph of a report on the topic Somewhat Valuable: information that would appear some- where in a report on the topic Not that Valuable: information that does not add new infor- mation beyond the topic description, or information that would appear only in a report footnote Not Relevant: a document without any information about the topic The qrels use a three-point graded relevance scale:15 3 points Very Valuable 1 point Somewhat Valuable 0 points Not that Valuable or Not Relevant 15An early release of the qrels had 3-2-1-0 graded relevance judgments corresponding to the 4-point scale used by assessors.5.3 Analysis of Topics During the assessment period, forty-seven Chinese topics, forty-five Persian topics, and forty-four Russian topics were judged. Within each language some topics had fewer than three relevant docu- ments, while other topics had a concerningly large percentage of relevant documents in the pools. Having topics with fewer than three relevant document can have undesirable effects on the ability to statistically distinguish systems. There are three Chinese topics, four Persian topics, and three Russian topics with fewer than three relevant documents. Thus each language has at least forty topics; such is generally thought to be the minimum number of topics necessary to fairly compare systems. Identifying topics with a large number of unidentified relevant documents is more important for future use of the collection than for track participants, since every research system had its top fifty documents judged. Scores for systems are comparable and thus systems can be ranked using them. However, given the desire to 4Overview of the TREC 2022 NeuCLIR Track Table 2: Inter-assessor confusion matrices, as raw label counts. Key: Very Valuable (VV), Somewhat Valuable (SV), Not that Valuable (NV), Not Relevant (NR). Chinese Second Label NR NV SV VVOfficialNR 7694 260 22 0 NV 165 291 71 3 SV 49 155 94 1 VV 34 99 185 20Persian Second Label NR NV SV VVOfficialNR 4996 18 7 3 NV 804 25 19 7 SV 54 8 4 3 VV 12 5 0 4Russian Second Label NR NV SV VVOfficialNR 4854 59 17 17 NV 620 53 27 71 SV 96 10 10 44 VV 44 13 13 126 create reusable collections, determining topics that likely have many unjudged relevant documents is important. One approach simply calculates the percentage of relevant documents in the pool and sets a cutoff (such as 10% prevalence) as too high to be confident that the relevant set is sufficiently identified. Using this cutoff would flag ten topics in Chinese, seven topics in Persian, and eleven topics in Russian. Figure 3 presents a closer investigation into the percentage of additional relevant documents found by non-baseline runs at var- ious pool depths (the gain curve ) by the automatic runs beyond those already found by depth 25 by some baseline run. We exclude topics that do not discover more relevant documents as the depth increases (e.g., the baseline pool contains the same set of relevant documents as any automatic pool up to depth 50). We use a knee detection heuristic [ 7] on the gain curves to identify topics that are less likely to find a sizable number of unjudged documents with a deeper pool. We calculate the maximum log slope ratio over any automatic pool depth as the indicator of curvature. Specifically, let ,be the slope of the curve from to, i.e.,()/() where is the percentage of the relevant documents found with pool depth . The maximum log slope ratio is defined as: max [1,50]log\u0012 1, ,50\u0013 . To capture the general trend of the curve instead of sharp fluctua- tions, we smooth the curve by averaging a moving window of size three. If the value is 0, the curve is a straight line without a plateau; a log ratio that is close to infinity indicates a flat line toward the end. As demonstrated in Figure 3, topics with higher prevalence tend to be continuously finding more relevant documents as the pools become larger, i.e., less likely to be completely judged, which aligns with the prior heuristics on assessing the completeness by preva- lence. Topics with moderate prevalence tend to be more complete but not guaranteed. However, the range of appropriate prevalence is subject to the language, and potentially the collection and partic- ipating systems. Nevertheless, these results these results suggest some considerations that we will bear in mind during topic curation next year. 5.4 Assessor Agreement As is the convention with TREC tracks, the official relevance assess- ments for a given topic represent a single assessor's interpretation of the topic and assessment guidelines. Nevertheless, we soughtto explore whether different assessors would have determined dif- ferent documents to be relevant for the topic and whether such user differences would affect the overall system rankings. We ex- plore the former question in this section and the latter question in Section 8.5. An additional NIST assessor labeled the relevance of the pooled documents for 28 topic-language pairs (12 topics in Chinese, 8 topics in Persian, and 8 topics in Russian), using the same assessment criteria that the topic's official assessor used. Table 2 presents confusion matrices for each language. We ob- serve that a high proportion of the differences in labels are between theNot that Valuable (NV) and the Not Relevant (NR) labels, 87% for Persian, 66% for Russian, and 41% for Chinese. These results motivate our decision to merge these labels during evaluation. Given the unbalanced nature of the relevance labels, we com- pute the Cohen's coefficient [ 5] to assess the overall agreement of the labels. We explore agreement in four settings: the original 4 relevance labels; the 3 labels used for evaluation (merging Not that Valuable andNot Relevant ); binary labels used for evaluation measures like MAP (further merging Very Valuable andSomewhat Valuable ); and a \"Fuzzy\" setting, in which adjacent labels are con- sidered matches. Table 3 presents the values for each of these settings alongside the established interpretations of their quality from Viera et al . [24] . We find that agreement improves for Persian and Chinese when the bottom two relevance labels are merged, and further improves for all languages in the binary setting. When we consider adjacent relevance scores as matches (the Fuzzy setting), we observe substantial agreement in Chinese, moderate agreement in Russian, and fair agreement in Persian. These results suggest that the Persian relevance labels may be biased towards the topic's specific assessor, while the Chinese and Russian labels potentially Table 3: Cohen's assessor agreement on a sample of rele- vance assessments, by language. values are annotated with interpretations [ 24] of (F)air, (M)oderate, and (S)ubstantial agreement (others are slight). Labels Chinese Persian Russian Overall 4 labels (M) 0.515 0.081 (F) 0.300 (F) 0.346 3 labels (M) 0.591 (S) 0.674 5Lawrie et al. 0 10 20 30 40 500.40.60.81.0% of Rel FoundChinese | prevalence=[0.0, 0.003) 0 10 20 30 40 50Persian | prevalence=[0.0, 0.003) 0 10 20 30 40 50Russian | prevalence=[0.0, 0.003) 0 10 20 30 40 500.40.60.81.0% of Rel FoundChinese | prevalence=[0.003, 0.01) 0 10 20 30 40 50Persian | prevalence=[0.003, 0.01) 0 10 20 30 40 50Russian | prevalence=[0.003, 0.01) 0 10 20 30 40 500.40.60.81.0% of Rel FoundChinese | prevalence=[0.01, 0.04) 0 10 20 30 40 50Persian | prevalence=[0.01, 0.04) 0 10 20 30 40 50Russian | prevalence=[0.01, 0.04) 0 10 20 30 40 50 Automatic Run Depth0.40.60.81.0% of Rel FoundChinese | prevalence=[0.04, 1.0) 0 10 20 30 40 50 Automatic Run DepthPersian | prevalence=[0.04, 1.0) 0 10 20 30 40 50 Automatic Run DepthRussian | prevalence=[0.04, 1.0) 0 1 2 3 4 5Maximum Log Slope Ratio Figure 3: Percentage of relevant documents found with different pooling depths on the runs. The depth for the baseline runs is set to 25. Topics are grouped by prevalence, which is the percentage of the relevant document found among all judged documents. 6Overview of the TREC 2022 NeuCLIR Track Table 4: MT Training Data Language Chinese 84,764,463 31.5 Persian 11,426,143 35.1 Russian 119,856,685 34.9 generalize better across users. Further, while there can be disagree- ment among assessors about the exact degree of relevance, such cases are generally among adjacent labels. 6 ADDITIONAL RESOURCES The track provided three types of additional resources: translated documents, translated queries, and translations of MS MARCO into the collection languages (NeuMARCO16). Links to some other types of pre-existing resources that might be useful to participants were also provided. One additional resource the track provided was machine trans- lations of the documents into English and the queries into Chinese, Persian, and Russian. These resources facilitated meaningful com- parison across systems that used machine translation to cross the language barrier. Documents were translated using a vanilla Trans- former model that was trained in-house with the Sockeye version 2 toolkit [ 2] using bitext obtained from publicly available corpora.17 The number of sentences used in training is given in Table 4, along with BLEU scores on the FLORES-101 benchmark [ 12] for each language. Query translations were obtained from Google Translate since its performance on titles was superior. While no team processed their own translations of the documents, one team produced their own translations of the queries. The track website also collected a number of multilingual and bilingual resources in the languages of the track including trans- lations of MSMARCO passages into the document languages [ 3]; HC4 - a CLIR collection built over three years of Common Crawl data in the same three languages [ 17]; and two multilingual CLIR datasets based off as 23] and WikiCLIR [22]. 7 PARTICIPANTS Including baselines, we scored 52 Chinese runs, 60 Persian runs, and 60 Russian runs. Table 5 outlines the number of runs submitted to each of the tasks: monolingual IR, ad hoc CLIR, and reranking CLIR. A total of 12 teams submitted runs for at least one language. This track had worldwide participation, with three teams from Asia, one from Europe, one from South America, and the remainder from North America. More information about participant systems is available in the teams' notebook papers. 8 RESULTS AND ANALYSIS In this section, we summarize the results and provide some analysis on topic difficulty, reusability, and the effect on system preference order of using different annotators. 16https://huggingface.co/datasets/neuclir/neumarco 17https://opus.nlpl.euTable 5: Number of runs submitted to each task Language Monolingual Ad Hoc Reranking Total Chinese 17 30 5 52 Persian 20 34 6 60 Russian 20 35 5 60 8.1 Overall Results The full results are presented in Tables 8, 9, and 10. The top-ranked systems all use a combination of title and description queries. Table 6 summarizes the effectiveness of systems categorized by the type of the model. Since huaweimtl team indicates that runs huaweimtl-{zh,fa,ru}-m-hybrid1 runs were ensembling sys- tems that includes a monolingual system (i.e., using human trans- lated queries), these three runs are marked as monolingual runs by the organizers. On average, hybrid approaches that combine dense and sparse retrieval in the system tend to provide the best nDCG@20. Both hybrid and learned-sparse (such as SPLADE [ 11]) models provide a recall at 1000 close to 80%. Note that the reranking runs tend to have a higher recall at 1000, which is based on a BM25 retrieval result with document translation, that should not be attributed to the reranking models. The variation among dense retrieval models is large, as we can observe in Figure 4. Several dense retrieval models are among the top-ranked systems while others are scattered throughout their peers. Sparse retrieval systems provide a moderate performance, which is mostly contributed by the baseline runs. The left column of Figure 4 presents the monolingual runs. De- spite not being the focus of the NeuCLIR track, they enrich the pool and provide a target for the CLIR models to compare with. The high- est performing CLIR system, in terms of nDCG@20, for Chinese and Persian outperformed the monolingual model for the corre- sponding language by about 0.015; the best Russian CLIR system achieved about the same nDCG@20 as the best Russian monolin- gual system. We defer the discussion on the pooling enrichment benefit of the monolingual runs to Section 8.4. 8.2 Topic Difficulty One of the objectives of topic development is to create a set of topics where the retrieval results are able to distinguish systems. Topics that are too easy or not having any relevant documents are not ideal. Figures 8, 9, and 10 are nDCG@20 boxplots of all the judged topics for each language. Topic 118 for Persian is an example of an easy topic where 75% of the runs achieve nDCG@20 over 0.80; in contrast, all runs score zero for topics 4 and 24 against the Chinese documents, indicating that these two topics are not likely to have any relevant document in the collection. In a practical sense, when no run has retrieved any relevant document, no relevant documents are judged during pooling, thus the topic is not usable for evaluating future systems. Topics, such as Topic 24 in Russian, with a wide range of scores, are ideal for distinguishing systems. However, topics such as 52 in Chinese and 4 in Persian can give future systems that can understand the complex semantics of the queries an advantage, and, therefore, reflect the systems' 7Lawrie et al. Table 6: Average effectiveness by the type of the CLIR runs. Chinese Persian Russian nDCG CLIR TDhybrid TDNsparse Dlearned-sparse N/Aother 4: Bar charts of submitted runs. Monolingual and CLIR runs are separated into subplots for clarity. of Dimension 1Russian(a) By Run Type nDCG@20 0.0 0.1 0.2 0.3 0.4 0.5 Run Type End-to-End CLIR Monolingual Other Query Trans Doc 2Chinese tSNE Dimension 1Persian tSNE Dimension 1Russian(b) By Model Type nDCG@20 0.0 0.1 0.2 0.3 0.4 0.5 Model Type hybrid sparse rerank dense other learned-sparse Figure 5: tSNE graphs of nDCG@20 for each submitted run. The shade of the marker indicates the overall nDCG@20 of the run. improvement. Although most systems have low nDCG scores for these topics, there are relevant documents judged thanks to pooling. Future systems that are able to retrieve these rare but relevant documents will be able to obtain a higher overall score. Systems retrieved more relevant documents for topics that are not related to any country or region, such as Topic 32 ( Peanut allergy treatment ) and 16 ( Dramatic decrease in number of bees ). Topics that are more specific to the country where the language is widely spoken tend to result in retrieval of larger numbers of relevant documents. For example, Topic 4 ( Corruption during con- struction of Vostochny Cosmodrome ) is among the easiest topics for Russian; however, there are no relevant documents in Chinese, and it is extremely challenging for Persian. Such topics with disjoint interests in different languages are not particularly problematic for evaluating CLIR but this could be an important issue in future MLIR evaluations in which the document collection contains multiple languages. 8.3 Retrieval Diversity Forming a diverse set of judgments could lead to a more reusable collection for evaluating future systems, and such judgments re- quire a diverse set of retrieval results. For each run, we form a vector with the nDCG@20 values of each topic. Thus, the size of such vectors is the number of the judged topics in the language. Figure 5 plots tSNE graphs that project the nDCG@20 vectors to two dimensions. The shade of the markers indicates the overall nDCG@20 of the run.Among different run types (Figure 5(a)), there is not a clear cluster that gathers monolingual systems, which indicates that the monolingual subtask might not provide much value for diversifying the pool. End-to-end CLIR systems (i.e., no translation involved during inference18) demonstrate two clear clusters in each language, while having a clear separation between runs that involve query translation. There are more separations among the model types. Hybrid runs cluster together in all languages, with a clear distance from the sparse runs. Several dense runs are among runs with high overall scores (darker colors), while others are among the lowest-scoring runs. This indicates that the trend appears in not only the overall scores (Figure 4) but also behavior on individual topics. Figures 11, 12, and 13 plot the retrieval similarity among all submissions, where lighter colors indicate higher similarity. For the top 100 retrieved documents (Figures 11(a), 12(a), and 13(a)), runs submitted by the same team tend to be more similar than others, which might indicate that teams are often submitting ablated runs instead of building different stacks of systems. Top- ranked systems also tend to be more similar to each other as they all put relevant documents at the top, with a more clear trend in Persian and Russian than in Chinese. Sparse runs also retrieve highly similar sets of documents in the top 100, especially in Persian and Russian, indicating that the ranking model might all be leveraging similar features. 18We inferred this from the submission metadata and considered systems marked using English queries and native documents as end-to-end CLIR runs. 9Lawrie et al. 0.0 0.1 0.2 0.3 0.4 0.5 Full Pool0.00.10.20.30.40.5Leave-Out-Unique Pool =0.9902 Chinese | nDCG@20 0.0 0.2 0.4 0.6 Full Pool0.00.10.20.30.40.50.6Leave-Out-Unique Pool =0.9968 Persian | nDCG@20 0.0 0.2 0.4 Full Pool0.00.10.20.30.40.5Leave-Out-Unique Pool =0.9936 Russian | nDCG@20 0.0 0.2 0.4 0.6 0.8 Full Pool0.00.20.40.60.8Leave-Out-Unique Pool =0.9842 Chinese | R@1000 0.0 0.2 0.4 0.6 0.8 Full Pool0.00.20.40.60.8Leave-Out-Unique Pool =0.9910 Persian | R@1000 0.0 0.2 0.4 0.6 0.8 Full Pool0.00.20.40.60.8Leave-Out-Unique Pool =0.9838 Russian | R@1000 Run Type CLIR Run Monolingual Run Figure 6: Leave-Out-Unique pool experiments. Rank correlations measured by Kendall's are marked at the corner of each graph. 0.0 0.1 0.2 0.3 0.4 0.5 Full Pool0.00.10.20.30.40.5Leave-Out-T eam-Unique Pool =0.9849 Chinese | nDCG@20 0.2 0.4 0.6 Full Pool0.00.10.20.30.40.50.6Leave-Out-T eam-Unique Pool =0.9808 Persian | nDCG@20 0.1 0.2 0.3 0.4 0.5 Full Pool0.00.10.20.30.40.5Leave-Out-T eam-Unique Pool =0.9796 Russian | nDCG@20 0.4 0.5 0.6 0.7 0.8 Full Pool0.40.50.60.70.8Leave-Out-T eam-Unique Pool =0.9573 Chinese | R@1000 0.5 0.6 0.7 0.8 0.9 Full Pool0.50.60.70.80.9Leave-Out-T eam-Unique Pool =0.9775 Persian | R@1000 0.4 0.5 0.6 0.7 0.8 0.9 Full Pool0.40.50.60.70.80.9Leave-Out-T eam-Unique Pool =0.9410 Russian | R@1000 Run Type CLIR Monolingual Run Figure 7: Leave-Out-Team-Unique pool experiments. Rank correlations measured by Kendall's are marked at the corner of each graph. 10Overview of the TREC 2022 NeuCLIR Track For the retrieved relevant documents, the trend is similar, with top-ranked runs demonstrating higher similarity. However, the light triangle in the middle is less clear, indicating that despite providing lower recall, these runs still contribute some unique relevant documents to make the pool more diverse, which leads to a more reusable collection in the future. Note that the numerator and the denominator of the similarity are the sizes of the intersection and the union of the two runs; this similarity measure would give low similarity if the recall of the two runs are too different. 8.4 Reusability To evaluate the reusability of the collection, we conduct the Leave- Out-Unique experiment [ 4,28] to test the robustness of the collec- tion. In the experiment, we leave out one run from the construction of the pool and evaluate the run with the modified qrels. This pro- cess simulates the possibility of each run being a future run that does not participate in pooling. Since the primary purpose of the evaluation is to rank systems, the differences in the actual values of the evaluation metric after modifying the qrels are negligible if the ordering of the runs remains the same. Therefore, we calculate Kendall's on the rank of the systems for quantitatively evaluating the correlation. Figure 6 demonstrate the experiment results. Each dot indicates a run where the x-axis indicates the evaluation metric on the full qrels and the y-axis is the modified version. The results illustrate that most of the runs are still on the diagonal, indicating that the collection is indeed robust and can fairly evaluate future systems that do not contribute to the pools with Kendall's close to 0.99. Since the variation between the runs submitted by a team is small, we additionally conduct a Leave-Out-Team-Unique experi- ment where we remove all submissions from the same team when evaluating a run. Such experiments are more aggressive than the Leave-Out-Unique experiments but provide a more adequate assess- ment of the reusability. Figure 7 presents the results. The correlation between the full pool and the modified pool is lower with 0.98 for nDCG@20 and 0.95 for recall at 1000. However, we argue that the correlation is still high enough for fairly evaluating future systems. 8.5 Assessor Effect on System Preference Order Given the variable agreement levels among assessors observed in Section 5.4, we explore whether the ranking of submitted systems differs when using the alternative assessments. To this end, we compare the nDCG@20, RBP, MAP, R@100, and R@1000 of the sys- tems using official judgments and the second judgments over the 28 re-assessed topic-language pairs. We measure the rank correlation of the systems using Spearman's and Kendall's statistics, and present the results in Table 7. We observe a very strong correlation for nDCG@20, RBP, and MAP ( >0.83and>0.62) and a strong correlation for R@100 and R@1000 ( >0.68and>0.50). Noting that using only 28 topics in this analysis induces a greater degree of random variation than would be the case for the full topic set, these results suggest that although assessors sometimes disagree on relevance labels, the system preference order may not change much if a different assessor provided the labels.Table 7: Correlation between systems when measured using the official assessments and the second assessor's labels. All correlations are significant TREC NeuCLIR track will continue in 2023. The CLIR task will continue, with new topics for the same collections in the same three languages. In this way, we hope to support both improved designs and improved training, since training data from this first year of the track will be available with characteristics closely approximating those of the 2023 CLIR task. We are considering three additional tasks: Multilingual Information Retrieval (MLIR). In this task, sys- tems would be asked to create a single ranked list over all three test collection languages (Chinese, Persian and Russian), and would be evaluated using the relevance judg- ments for all three languages. A similar MLIR task was evaluated in the Cross-Language Evaluation Forum (CLEF) and found to be challenging [10]. Non-English Queries (NEQ). In this task, systems would perform CLIR or MLIR using queries that are not in English. The present NeuCLIR test collection can support NEQ tasks with Chinese, Persian or Russian queries, and generation of topics in a small number of additional languages may also be possible. CLIR for Technical Documents (CLIR-TD). CLIR could be of value in many domain-specific applications, including for example law, medicine, or engineering. As a first step toward evaluating domain-specific applications of CLIR we are considering the addition of a pilot task in which the goal is to perform CLIR for technical documents in a single non-English language (either Chinese, Persian or Russian) using English queries. A promising direction is to use repositories of non English academic text, such as CNKI19or Wanfang20for Chinese, or the Russian Science 19https://www.cnki.net 20http://www.wanfangdata.com 11Lawrie et al. Citation Index21for Russian, or to select dual-language biomedical content that is indexed in PubMed. Starting three new tasks in the same year would risk dividing participating research teams in ways that would work against our goal of being able to compare results and foster progress on spe- cific tasks, however. For this reason, we want to use our planning session at TREC 2022 to discuss at least these four questions with participants: What changes to the CLIR task would further enhance the ability of participating teams to achieve their research goals? Which of the additional tasks we have listed are of greatest interest to participants (and thus most likely to attract a critical mass of participation)? Are there other new tasks that we should be considering that might be an even higher priority for NeuCLIR in 2023? Are there additional outreach opportunities that might at- tract a substantial number of new participants, and if so which new tasks might be most likely to help attract those new participants? We also plan to re-evaluate tools and methodology for reporting carbon footprint of participating systems. We note that, in 2022, only three teams reported their energy usage; many cited the lack of tools compatible with their system, while others said that keep- ing track of all stages of a track submission is too onerous and error prone (e.g., one might forget to log an experiment). Further, some ambiguity existed in how external resources should be ac- counted for; for example, should energy required to run systems that are shared with other projects in a lab be counted? Possible improvements in energy tracking include: Ask participants to measure impact only while preparing their submission: this would exclude any energy used dur- ing the training of neural models or indexing of document collections. Organizers could still use the information col- lected during a run submission to estimate the total energy usage. Explicitly formalize alternative metrics to energy report- ing: this could include compute-hours, hardware, or other information that could be used to estimate energy impact. Defer energy reporting from run submission time to note- book submission time: this would give teams more time to analyze their impact without having to sacrifice time in the weeks immediately preceding runs submission. 10 CONCLUSION CLIR at TREC is back! In this first year of NeuCLIR we have worked together to forge a research community, create new evaluation re- sources, and generate what is for the moment the world's largest collection of neural CLIR results that have been run under compa- rable conditions. Twelve participating teams (and several baseline systems) contributed a total of 172 runs, achieving strong first- year results that substantially outperformed non-neural baselines. Moreover, we might reasonably expect the 2022 NeuCLIR relevance judgments that are now available to future participants to offer 21https://elibrary.ruthe potential for continued improvement. The NeuCLIR track will continue at TREC 2023, perhaps with one or more additional tasks, so we have much to look forward to. REFERENCES [1]Kenya Abe, Kohei Shinden, and Makoto P. Kato. 2022. KASYS at the TREC 2022 NeuCLIR Track. In Proceedings of The Fifteenth Text Proceedings (TREC 2022) . Smucker, Gordon V Cormack, and Maura R Grossman. 2018. A System for Efficient High-Recall Retrieval. In The 41st International ACM SIGIR Conference on Research & mMARCO: A Multilingual Version of the MS Dataset. https: //doi.org/10.48550/ARXIV.2108.13897 [4]Chris Buckley, Darrin Dimmick, Ian Soboroff, and Ellen Voorhees. 2007. Bias and the limits of pooling for large collections. Information retrieval 10, 6 (2007), 491-508. [5] Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement 20, 1 (1960), 37-46. [6]John M. Conroy, Neil P. Molino, and Julia S. Yang. 2022. Extremely Fast Fine- Tuning for Cross Language Information Retrieval via Generalized Canonical Correlation. In Proceedings of The Fifteenth Text REtrieval Conference Proceedings (TREC 2022) . [7]Gordon V Cormack and Maura R Grossman. 2016. Engineering quality and reliability in technology-assisted review. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval . 75-84. [8] Cash Costello, Eugene Yang, Dawn Lawrie, and James Mayfield. 2022. Patapasco: A Python Framework for Cross-Language Information Retrieval Experiments. InAdvances in Information Retrieval: 44th European 10-14, 978-3-030-99739-7_33 [9] Davis and Ted E Dunning. 1995. A TREC evaluation of query translation methods for multi-lingual text retrieval. In Fourth Text Retrieval Conference , Vol. 483. [10] Nicola Ferro and Carol Peters. 2019. Information Retrieval Evaluation in a Chang- ing World: Lessons Learned from 20 Years of CLEF . Vol. 41. Springer. [11] Thibault Clinchant. 2021. SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . Association for Computing Machinery, New York, NY, USA, Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation. Transactions of the Association for putational 10 (2022), for Cross-lingual Retrieval. In Proceedings of The Fifteenth Text REtrieval Conference Proceedings (TREC 2022) . & CLIP at TREC 2022 NeuCLIR Track. In Proceedings of The Fifteenth Proceedings (TREC At TREC NeuCLIR 2022. In Proceedings of The Fifteenth Text REtrieval NeuCLIR 2022. In Proceedings of The Fifteenth REtrieval Proceedings (TREC 2022) . [17] Dawn Lawrie, and Eugene Yang. 2022. HC4: A New Suite of Test Collections for Ad Hoc CLIR. In Proceedings of the 44th European Conference on Information Retrieval (ECIR) . [18] Craig Macdonald and Nicola Tonellotto. 2020. using PyTerrier. In Proceedings of ICTIR 2020 . [19] Paul McNamee. 2016. Language and Dialect Discrimination Using Compression- Inspired Language Models. In Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial3) . The COLING 2016 Organiz- ing [20] at TREC 2022. InProceedings University of Maryland at the TREC 2022 NeuCLIR Track. In Proceedings of The 12Overview of the TREC 2022 NeuCLIR Track Translational and Knowledge-based Similarities from Relevance Rankings for Cross-Language Retrieval. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics . https://www.cl.uni-heidelberg.de/ ~riezler/publications/papers/ACL2014short.pdf [23] Shuo Sun and Kevin Duh. 2020. CLIRMatrix: A massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, Online, 4160- 4170. https://doi.org/10.18653/v1/2020.emnlp-main.340 [24] Anthony J Viera, Joanne M Garrett, et al .2005. Understanding interobserver agreement: the kappa statistic. Fam med 37, 5 (2005), 360-363.[25] Eugene Yang, Dawn James Mayfield. 2022. HLTCOE at TREC 2022 NeuCLIR Track. In Proceedings Fifteenth Text REtrieval Conference Proceedings (TREC 2022) . [26] Ge Zhang, Qiwen 2022. HNUST @ TREC NeuCLIR 2022. In Proceedings of and Jimmy Lin. 2022. Making a MIRACL: Multilingual arXiv:2210.09984 (2022). [28] Justin Zobel. 1998. How reliable are the results of large-scale information retrieval experiments?. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval . 307-314. 13Lawrie et al. Table 8: Chinese run results. Monolingual runs, which use human translations of the queries, are marked as green. Run used as the first stage retrieval for the reranking task is marked bold. * indicates manual runs. Team Run ID Man. ReR. QF QL of the 9: Persian run results. Monolingual runs, which use human translations of the queries, are marked as green. Run used as the first stage retrieval for the reranking task is marked bold. * indicates manual runs. Team Run ID Man. ReR. QF QL that the NM.unicamp wasunicamp .15Lawrie et al. Table 10: Russian run results. Monolingual runs, which use human translations of the queries, are marked as green. Run used as the first stage retrieval for the reranking task is marked bold. * indicates manual runs. Team Run ID Man. ReR. QF QL that .16Overview of the TREC 2022 NeuCLIR Track 0.0 0.2 0.4 0.6 0.8 1.0 nDCG@20(24) Wind energy in Russia(4) Corruption during construction of Vostochny Cosmodrome(26) Ukrainian Presidential Candidate Zelenskiy(52) T ourism in Beijing under the Covid-19 pandemic(48) The impact of the Three Gorges Project on Yangtze River ecosystem and relevant monitoring(25) Russian dependance on Chinese 5G equipment(19) Cases of formaldehyde related pollution in household products(80) Did Boeing hide the faults of the 737 Max aircraft(59) What foods increase irritability or anger(20) Are AIDS patients discriminated against in China(99) Saturn's hexagon(86) Iran and Turkish economical relationship over the years(7) Bicycles on trains(67) Reasons people become virtual streamers(31) Use of T eflon as potential health risk(38) The impact of the trade war between the US and China on China's export to the US(16) Dramatic decrease in number of bees(47) Meghan Markle's accusations of racism in Oprah Winfrey interview(71) Bill Gates' charity work(66) COVID-19 vaccination rate in China(36) Size of Siberian tiger population(103) Applications of artificial intelligence in agriculture(72) Playing games could effectively prevent dementia(132) First helicopter on Mars(126) Greta Thunberg's speech at UN(17) The impact of Chengdu Research Base of Giant Panda Breeding on local tourism(30) Refugee crisis divides Europe(130) Hawaii K lauea (114) Amazon labor union(65) Predict volcanic eruption(73) Internet cut off during the anti-government protests in November 2019 in Iran(58) Conservation measures for cultural relics of Mogao Caves in Dunhuang(129) Buddhas of Bamiyan(0) Iranian female athletes refugees(104) Honor killing In Iran(77) Vegetables high in calcium(62) Does Khamenei defend gasoline price hike(111) Mycorrhizal studies in agriculture(133) Whale stranding(109) Researching dead zones(39) The connection between the extraction of bitcoin cryptocurrencies by the Chinese and power outages i...(35) Drought in southwestern U.S.(18) The situation of myopia rate among K12 students in China(75) Impact of Australian bushfires on wildlife(118) Arbitrary detention of foreigners and Iranian dual nationals by Iran(96) The impact of Hong Kong protests on tourism(123) Iran produces Barekat vaccine(32) Peanut allergy treatment(5) going supernova 8: Boxplots of nDCG@20 on all Chinese runs. 17Lawrie et al. 0.0 0.2 0.4 0.6 0.8 1.0 nDCG@20(72) Playing games could effectively prevent dementia(17) The impact of Chengdu Research Base of Giant Panda Breeding on local tourism(4) Corruption during construction of Vostochny Cosmodrome(66) COVID-19 vaccination rate in China(45) Endangered animals in China(18) The situation of myopia rate among K12 students in China(67) Reasons people become virtual streamers(80) Did Boeing hide the faults of the 737 Max aircraft(52) T ourism in Beijing under the Covid-19 pandemic(59) What foods increase irritability or anger(123) Iran produces Barekat vaccine(38) The impact of the trade war between the US and China on China's export to the US(65) Predict volcanic eruption(25) Russian dependance on Chinese 5G equipment(24) Wind energy in Russia(86) Iran and Turkish economical relationship over the years(62) Does Khamenei defend gasoline price hike(111) Mycorrhizal studies in agriculture(71) Bill Gates' charity work(16) Dramatic decrease in number of bees(77) Vegetables high in calcium(133) Whale stranding(103) Applications of artificial intelligence in agriculture(36) Size of Siberian tiger population(26) Ukrainian Presidential Candidate Zelenskiy(30) Refugee crisis divides Europe(132) First helicopter on Mars(5) Danger of Betelgeuse going supernova(7) Bicycles on trains(35) Drought southwestern U.S.(126) Greta Thunberg's speech at UN(47) Meghan Markle's accusations of racism in Oprah Winfrey interview(114) Amazon labor union(31) Use of T eflon as potential health risk(99) Saturn's hexagon(129) Buddhas of Bamiyan(96) The impact of Hong Kong protests on tourism(104) Honor killing In Iran(39) The connection between the extraction of bitcoin cryptocurrencies by the Chinese and power outages i...(0) Iranian female athletes refugees(73) Internet cut off during the anti-government protests in November 2019 in Iran(109) Researching dead zones(75) Impact of Australian bushfires on wildlife(32) Peanut allergy treatment(130) Hawaii K lauea (118) Arbitrary detention of foreigners and Iranian dual nationals by Iran Figure 9: Boxplots of nDCG@20 on all Persian runs. 18Overview of the TREC 2022 NeuCLIR Track 0.0 0.2 0.4 0.6 0.8 1.0 nDCG@20(19) Cases of formaldehyde related pollution in household products(20) Are AIDS patients discriminated against in China(26) Ukrainian Presidential Candidate Zelenskiy(66) COVID-19 vaccination rate in China(80) Did Boeing hide the faults of the 737 Max aircraft(52) T ourism in Beijing under the Covid-19 pandemic(59) What foods increase irritability or anger(103) Applications of artificial intelligence in agriculture(25) Russian dependance on Chinese 5G equipment(38) The impact of the trade war between the US and China on China's export to the US(71) Bill Gates' charity work(86) Iran and Turkish economical relationship over the years(118) Arbitrary detention of foreigners and Iranian dual nationals by Iran(72) Playing games could effectively prevent dementia(65) Predict volcanic eruption(30) Refugee crisis divides Europe(73) Internet cut off during the anti-government protests in November 2019 in Iran(67) Reasons people become virtual streamers(48) The impact of the Three Gorges Project on Yangtze River ecosystem and relevant monitoring(0) Iranian female athletes refugees(132) First helicopter on Mars(133) Whale stranding(77) Vegetables high in calcium(17) The impact of Chengdu Research Base of Giant Panda Breeding on local tourism(96) The impact of Hong Kong protests on tourism(35) Drought in southwestern U.S.(75) Impact of Australian bushfires on wildlife(39) The connection between the extraction of bitcoin cryptocurrencies by the Chinese and power outages i...(45) Endangered animals in China(24) Wind energy in Russia(129) Buddhas of Bamiyan(47) Meghan Markle's accusations of racism in Oprah Winfrey interview(130) Hawaii K lauea (109) Researching dead zones(31) Use of T eflon as potential health risk(36) Size of Siberian tiger population(4) Corruption during construction of Vostochny Cosmodrome(114) Amazon labor union(5) Danger of Betelgeuse going supernova(18) The situation of myopia rate among K12 students in China(99) Saturn's hexagon(62) Does Khamenei defend gasoline price hike(16) Dramatic decrease in number of bees(7) Bicycles on trains(32) Peanut allergy treatment Figure 10: Boxplots of nDCG@20 on of Translation End-to-End CLIR Without Machine Translation Machine Query Translation Machine Document Translation Monolingual Runs With Human Query Translation Other 0.00.20.40.60.81.0(a) Chinese - Overlap of T op-100 of Translation End-to-End CLIR Without Machine Translation Machine Query Translation Machine Document Translation Monolingual Runs With Human Query Translation Other 0.00.20.40.60.81.0(b) Chinese - Overlap of Retrieved Relevant Documents (Ordered by R@1000) Figure 11: Overlap of documents retrieved by systems that participated in Chinese. Run used as the first stage retrieval for the reranking task is marked bold. * indicates manual runs.20Overview of Translation End-to-End CLIR Without Machine Translation Machine Query Translation Machine Document Translation Monolingual Runs With Human Query Translation Other 0.00.20.40.60.81.0(a) Persian - Overlap of T op-100 of Translation End-to-End CLIR Without Machine Translation Machine Query Translation Machine Document Translation Monolingual Runs With Human Query Translation Other 0.00.20.40.60.81.0(b) Persian - Overlap of Retrieved Relevant Documents (Ordered by R@1000) Figure 12: Overlap of documents retrieved by systems that participated in Persian. Run used as the first stage retrieval for the reranking task is marked bold. * indicates manual of Translation End-to-End CLIR Without Machine Translation Machine Query Translation Machine Document Translation Monolingual Runs With Human Query Translation Other 0.00.20.40.60.81.0(a) Russian - Overlap of T op-100 of Translation End-to-End CLIR Without Machine Translation Machine Query Translation Machine Document Translation Monolingual Runs With Human Query Translation Other 0.00.20.40.60.81.0(b) Russian - Overlap of Retrieved Relevant Documents (Ordered by R@1000) Figure 13: Overlap of documents retrieved by systems that participated in Russian. Run used as the first stage retrieval for the reranking task is marked bold. * "}