{"title": "PDF", "author": "PDF", "url": "https://authors.library.caltech.edu/94965/2/1612.02126.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "cleaned_text": "Kostina, Babak Hassibi Abstract \u2014Consider a control problem with a communication channel connecting the observer of a linear stochastic syst em to the controller. The goal of the controller is to minimize a quadratic cost function in the state variables and control signal, known as the linear quadratic regulator (LQR). We st udy the fundamental tradeoff between the communication rate r bits/sec and the expected cost b. We obtain a lower bound on a certain rate-cost function, which quanties the minimum di rected mutual information between the channel input and output tha t is compatible with a target LQR cost. The rate-cost function has operational signicance in multiple scenarios of interest : among others, it allows us to lower-bound the minimum communicati on rate for xed and variable length quantization, and for cont rol over noisy channels. We derive an explicit lower bound to the rate-cost function, which applies to the vector, non-Gauss ian, and partially observed systems, thereby extending and general izing an earlier explicit expression for the scalar Gaussian syst em, due to Tatikonda el al. [ 2]. The bound applies as long as the differential entropy of the system noise is not . It can be closely approached by a simple lattice quantization scheme that only quantizes the innovation, that is, the difference betw een the controller's belief about the current state and the true state. Via a separation principle between control and communicati on, similar results hold for causal lossy compression of additi ve noise Markov sources. Apart from standard dynamic programming arguments, our technical approach leverages the Shannon lo wer bound, develops new estimates for data compression with cod ing memory, and uses some recent results on high resolution vari able- length vector quantization to prove that the new converse bo unds are tight. Index Terms \u2014Linear stochastic control, LQR control, remote control, rate-distortion tradeoff, high resolution, caus al rate- distortion theory, Gauss-Markov source. I. I NTRODUCTION A. System model Consider a discrete-time stochastic linear system: Xi+1=AXi+BUi+Vi, (1) whereXiRnis the state, ViRnis the process noise, UiRmis the control action, and AandBare xed matrices of dimensions n\u00d7nandn\u00d7m, respectively. At time i, the controller observes output Giof the channel, and chooses a control action Uibased on the data it has observed up to time i. At timei, the encoder observes the output of the sensor YiRk: Yi=CXi+Wi, (2) The authors are with California Institute of Technology (e- mail: vkostina@caltech.edu ,hassibi@caltech.edu ). A part of this work was presented at the 54th Annual Allerton C onference on Communication, Control and Computing [ 1]. The work of Victoria Kostina was supported in part by the Nati onal Science Foundation (NSF) under grants CCF-1566567 and CCF- 1751356. The work of Babak Hassibi was supported in part by the NSF unde r grants CNS-0932428, CCF-1018927, CCF-1423663 and CCF-1409204, b y a grant from Qualcomm Inc., by NASA's Jet Propulsion Laboratory thr ough the President and Director's Fund, and by King Abdullah Univers ity of Science and Technology.whereCis ak\u00d7ndeterministic matrix, and WiRkis the observation noise. The encoder forms a codeword Fi, which is then passed through the channel. Like the controller, the encoder has access to the entire history of the data it has observed. See Fig. 1. We assume that system noises V1,V2...are i.i.d. zero- mean, that observation noises and independent of {Vi,Wi} i=1. We make the usual assumption that the pair (A,B)is controllable and that the pair (A,C)is observable. If the encoder observes the full system state, i .e. Yi=Xi(rather than its noise-corrupted version as in ( 2)), then we say that the system is fully observed (rather than partially observed ). SYSTEM SENSOR ENCODER CONTROLLER CHANNELGi Fi YiUi Xi Fig. 1: The distributed control system. Notation: Capital letters X,Ydenote (spatial) random vec- tors;Xt/defines(X1,...,X t)denotes temporal random vectors, or the history of vector samples up to time t;Xt i/defines(Xi,...,X t) (empty ift < i );X/defines(X1,X2,...); for i.i.d. random vectors,Xdenotes a random vector distributed the same as each of X1,X2,...;Drepresents a delay by one, i.e. DXt/defines(0,Xt1);X/definesE/bracketleftbig (XE[X])(XE[X])T/bracketrightbig denotes the covariance matrix of random vector tradeoff The efciency of a given control law at time tis measured by the linear quadratic cost balances between the deviation of the system from the desired state 0 and the control power, which are dened with respect to the 1As common in information theory, here we abuse the notation s lightly and write LQR/parenleftbig Xt,Ut1/parenrightbig to mean that LQR() is a function of the joint distribution of Xt,Ut1.2 norms induced by the matrices Q(andSt) andR. In the special caseQ=In,R=0andSt+1=In, the cost function in ( 3) is the average mean-square deviation of the system from 0, E/bracketleftBig/summationtextt i=1/badblXi/badbl2/bracketrightBig . Given a joint distribution of random vectors UtandYt, the directed mutual information is dened as [ 3] I(YtUt)/definest/summationdisplay i=1I(Yi;Ui|Ui1). (4) Directed mutual information, which captures the informati on due to causal dependence of UtonYt, and which is less than or equal to the full mutual information I(Yt;Ut), has proven useful in communication problems where causality and feedback play a role. Given a joint distribution PYtUt, it is enlightening to consider causally conditional probability kernel [ 4] PUt||Yt/definest/productdisplay i=1PUi|Ui1,Yi (5) Note thatPYtUt=PYt/bardblDUtPUt/bardblYt. In Fig. 1, the system dynamics ( 1), (2) xes the kernels PYt/bardblDUt,t= 1,2,..., while the causal channels PUt/bardblYtcomprise the encoder, the channel and the controller. The following information-theoretic quantity will play a central role in determining the operational fundamental li mits of control under communication constraints. Denition 1 (rate-cost function) .The rate-cost function tI(YtUt)(6) In this paper, we will show a simple lower bound to the rate-cost function ( 6) of the stochastic linear system ( 1), (2). Although R(b)does not have a direct operational interpretation unless the channel is probabilistically matched [ 5] to the system, it is linked to the minimum data rate required to keep the system at LQR cost b, over both noiseless and noisy channels. Namely, we will show that R(b)provides a lower bound on the minimum capacity of the channel necessary to sustain LQR cost b, valid for any encoder/controller pair. We will also show that over noiseless channels, R(b)can be closely approached by a simple variable-length lattice-ba sed quantization scheme that transmits only the innovation. C. Prior art The analysis of control under communication constraints has a rich history. The rst results on the minimum data rate required for stabilizability appeared in [ 6], [7]. These works analyze the evolution of a scalar system from a worst-case perspective. In that setting, the initial state X1is assumed to belong to a bounded set, the process noise V1,V2,... is assumed to be bounded, and the system is said to be stabilizable if there exists a (rate-constrained) control sequence such that the worst-case deviation of the system state from the target state 0is bounded: limsupt/badblXt/badbl<. In [ 6], [7], it was shown that a fully observed unstable scalar system can be kept bounded by quantized control if and only if the available data rate exceeds logAbits per sample. Tatikondaand Mitter [ 8] generalized this result to vector systems; namely, they showed that the necessary data rate to stabilize a vecto r system with bounded noise is at least r>/summationdisplay i:|i(A)|1log|i(A)|, (7) where the sum is over the unstable eigenvalues of A, i.e. those eigenvalues whose magnitude exceeds 1. Compellingly, (7) shows that only the nonstable modes of Amatter; the stable modes can be kept bounded at any arbitrarily small quantization rate (and even at zero rate if Vt0). Using a volume-based argument, Nair et al. [ 9] showed a lower bound to quantization rate in order to attain limsupt/badblXt/badbl d, thereby rening ( 7). Nair et al. [ 9] also presented an achiev- ability scheme conrming that for scalar systems, that boun d is tight. Nair and Evans [ 10] showed for systems with unbounded process and observation disturbances, Tatikonda and Mitte r's condition on the rate ( 7) continues to be necessary and sufcient in order to keep the mean-square deviation of the plant state from 0bounded, that is, in order to satisfy limsuptE/bracketleftbig /badblXt/badbl2/bracketrightbig <. Nair and Evans' converse bound [ 10] applies to xed-rate quantizers, that is, to compressors whose outputs can take one of2rvalues. Time-invariant xed-rate quantizers are unable to attain bounded cost if the noise is unbounded [ 10], regardless of their rate. The reason is that since the noise is unbounded, over time, a large magnitude noise realizatio n will inevitably be encountered, and the dynamic range of the quantizer will be exceeded by a large margin, not permitting recovery. Adaptive quantization schemes, which \"zoom out\" (i.e. stretch the quantization intervals) when the system i s far from the target and \"zoom in\" when the system is close to the target, are studied in [ 10]-[13]. Structural properties of optimal zero-delay quantizers for the compression of Marko v sources were investigated in [ 14]-[21]. In variable-rate (or length ) quantization, innite number of quantization cells. Entropy coding is applied to encode the indices of quantization cells, so that the more likely quantization cells have a shorter description and the less likely ones a longer one. Elia and Mitter [ 22] considered stabilization of a noiseless linear system controlled with a variable-length scalar quantizer, and sh owed that for a certain notion of coarseness , the coarsest quantizer has levels that law. Beyond worst-case and mean-square stabilizability, Tatikonda et al. [ 2] considered setting known as linear quadratic Gaussian (LQG) control (( 1), (2) with Gaussian disturbances and LQR cost function in ( 3)) with communication constraints and tied the minimum attainable LQG cost to the Gaussian causal rate-distortion function , introduced decades earlier by Gorbunov and Pinsker [23], which is equal to the minimal (subject to a distortion constraint) directed mutual information between the stoch astic process and its quantized representation [ 24]. Stabilizability of LQG systems under a directed mutual information constraint was studied in [ 25]. The problem of minimizing an arbitrary cost function in control of a general process under a directe d mutual information constraint was formulated in [ 26]. Control3 of general Markov processes under a mutual information constraint was studied in [ 27]. Silva et al. [ 28] elucidated the operational meaning of directed mutual information, by pointing out that it lower-bounds the rate of a quantizer embedded into a feedback loop of a control system, and by showing that the bound is approached to within 1 bit by a dithered prex-free quantizer, a compression setting in which both the compressor and the decompressor have access to a common dither - a random signal with special statistical properties. More recently, Silva et al. [ 29] computed a lower bound to the minimum quantization rate in scalar Gaussian systems with stationary disturbances and proposed a dither ed quantization scheme that performs within 1.254 bits from it . Tanaka et al. [ 30] generalized the results of [ 29] to vector systems. A connection between causal 31]. Causal rate-distortion function is challenging to evaluat e, and beyond the scalar Gauss-Markov source [ 2], [23], no closed-form expression is known for it. For stationary scal ar Gaussian processes, Derpich and Ostergaard [ 32] showed an upper bound and Silva et al. [ 29] a lower bound. For vector Gauss-Markov sources, Tanaka et al. developed a semidenit e program to compute exactly the minimum directed mutual information in quantization [ 33] and control [ 34]. D. Our contribution In this paper, we show a lower bound to R(b)of a fully observed system. We do not require the noise Vito be bounded or Gaussian. We also show that ( 7) remains necessary to keep the LQR cost bounded, even if the system noise is non- Gaussian, generalizing previously known results. Althoug h our converse lower bound holds for a general class of codes that can take full advantage of the memory of the data observed so far and that are not constrained to be linear or have any other particular structure, we show that the new bound can be closely approached within a much more narrow class of codes. Namely, a simple variable-length quantization sche me, which uses a lattice covering and which only transmits the difference between the controller's estimate about the cur rent system state and the true state, performs within a fraction o f a bit from the lower bound, with a vanishing gap as bapproaches its minimum attainable value, bmin. The scheme is a variant of a classical differential pulse-code modulation (DPCM) sch eme, in which a variable-length lattice code is used to encode the innovation process. Unlike previously proposed quantizat ion schemes with provable performance guarantees, our scheme does not use dither. Our results generalize to partially observed systems, where the encoder does not have access to Xibut only to its noise- corrupted version, Yi. For those results to hold, we require the system and observation noises to be Gaussian. Our approach is based on a new lower bound to causal rate- distortion function, termed the causal Shannon lower bound (Theorem 9in Section IIIbelow), which holds for vector Markov sources with continuous additive disturbances, as i n (1). For the scalar Gauss-Markov source, the bound coincides with a previously known expression [ 23].E. Technical approach The main idea behind our approach to show a converse (impossibility) result is to recursively lower-bound dist ortion- rate functions arising at each step. We apply the classical Shannon lower bound [ 35], which bounds the distortion-rate functionXin terms of the entropy power of X, and we use the entropy power inequality [ 36], [37] to split up the distortion- rate functions of sums of independent random variables. Sin ce Shannon's lower bound applies regardless of the distributi on of the source random variable, our technique circumvents a precise characterization of the distribution of the state a t each time instant. The technique also does not restrict the syste m noises to be Gaussian. To show that our bound can be approached at high rates, we build on the ideas from high resolution quantization theo ry. A pioneering result of Gish and Piece [ 38] states that in the limit of high resolution, a uniform scalar quantizer incurs a loss of only about1 2log22e 120.254 bits per sample. Ziv [39] showed that regardless of target distortion, the normaliz ed output entropy of a dithered scalar quantizer exceeds that o optimal vector quantizer by at most1 2log4e 120.754bits per sample. A lattice quantizer presents a natural extensio n of a scalar uniform quantizer to multiple dimensions. The advantage of lattice quantizers over uniform scalar quanti zers is that the shape of their quantization cells can be made to approach a Euclidean ball in high dimensions [ 40]. Further- more, the rate of dithered lattice quantizers conve rges to Shannon's lower bound in the limit of vanishing distortio n [41]-[43]. While the presence of a dither signal both at the encoder and the decoder greatly simplies the analysis and can improve t he quantization performance, it also complicates the enginee ring implementation. In this paper, we do not consider dithered quantization. Neither do we rely directly on the classical heuristic reasoning by Gish and Piece [ 38]. Instead, we use a non-dithered lattice quantizer followed by an entropy code r. To rigorously prove that its performance approaches our conve rse bound, we employ a recent upper bound [ 44] on the output entropy of lattice quantizers in terms of the differential e ntropy of the source, the target distortion and a smoothness parame ter of the source density. F . Paper organization In Section II, we state and discuss our main results: Sec- tion II-A focuses on the scenario where the observer sees the system state (fully observed system), Section II-B discusses a generalization to the scenario where the observer sees a noi sy measurement of the system state (partially observed system ), and Section II-C discusses the operational implications of our bounds in the settings of xed-rate quantization, varia ble- rate quantization and joint source-channel coding. In Sec- tion III, we introduce the causal lossy compression problem, and we state the causal Shannon lower bound, together with a matching achievability result. In Section IV, we discuss separation between control, estimation and communication , a structural result that allows us to disentangle the three ta sks. The proofs of the converse results are given in Section V, and the achievability schemes are presented in Section VI.4 II. M AIN RESULTS A. Fully observed system In the absence of communication constraints, the minimum LQR cost attainable in the limit of innite time is: bmin= tr(VS), (8) whereVis the covariance matrix of each the V1,V2,..., andSis the solution to the algebraic Riccati equation S=Q+AT(SM)A, (9) M/definesLT(R+BTSB)L=SB(R+BTSB)1BTS, (10) L/defines(R+BTSB)1BTS. (11) Our results quantifying the overhead over ( 8) due to com- munication constraints are expressed in terms of the entrop y power of the system and observation noises. The entropy power of an n-dimensional respect to the Lebesgue measure on Rn. The satises the following classical inequalities: N(X)(detX)1 n1 nVar[X]. (13) The rst equality in ( 99) is attained if and only if Xis Gaussian and the second if and only if Xis white. Our rst result is a lower bound on the rate-cost function. Theorem 1. Consider the fully observed linear stochastic system (1). Suppose that h(V)>. At any LQR cost b>tr(VS), the rate-cost n Theorem nontrivial if M0, which happens if rankB=nand either Q0orR0. The bound in Theorem 1continues to hold whether or not at time ithe encoder observes the previous control inputs U1,U2,...,U i1. The right-hand side of ( 14) is a decreasing function of b, which means that the controller needs to know more information about the state of the system to attain a smaller target cost. As an important special case, consider the rate - cost tradeoff where the goal is to minimize the mean-square deviation from the desired state 0. Then,Q=In,R=0, .(15) In another important V, (14) particularizes as R(b)log|detA|+n 2log/parenleftBigg 1+|detVM|1 n (btr(VS))/n/parenrightBigg .(16) 2Alllog's andexp's specifying the information units.For the scalar Gaussian system, ( 16) holds with equality. This is a consequence of known analyses [ 23], [2], [32, Th. 3] (see also Remarks 5and6in Section IIIbelow). A typical behavior of ( 15) is plotted in Fig. 3as a function of target cost b. Asbbmin, the required rate R(b) . Conversely, as b , the rate monotonically decreases approaches log|detA|. The rate-cost tradeoff provided by Theorem 1can serve as a gauge for choosing an appropriate communication rate in order to meet the control objective. F or example, in the setting of Fig. 3, decreasing the data rate below 1 nat per sample incurs a massive penalty in cost, because the bound is almost flat in that regime. On the other hand, increasing the rate from 1 to 3 nats per sample brings a lot of improvement in cost, while further increasing it beyond 3 nats results in virtually no improvement. Also plotted in Fig. 3is the output entropy of a variable-rate uniform scalar quantizer that takes advantage of the memory of the past only through the innovation, i.e. the difference between the controller's prediction of the state at time igiven the information the controller had at time i1and the true state (see Section VIfor a precise description of the quantizer). Its performance is strikingly close to the lower bound, bein g within0.5nat even at large b, despite the fact that quantizers in this class cannot attain the optimal cost exactly [ 45]. The gap further vanishes as bdecreases. The gap can be further decreased for multidimensional systems by taking advantag e of lattice quantization. These effects are formally captur ed by the achievability result we are about to present, Theorem 2. Fig. 2: The minimum quantizer entropy compatible with cost bin a fully observed system 1) with parameters n= 1,A= 2,B=Q= Denition 1the directed mutual information by the entropy of a causal quantizer, we introduce Denition 2 (entropy-cost have H(b)R(b). On the other hand, there exists a variable-length quantizer that keeps the system at cost band whose average encoded length does not exceed H(b)(see Section II-C for details). Thus, unlike R(b), the function H(b)has a direct operational interpretation. Theorem 2, presented next, holds under the assumption that the density of the noise is sufciently smooth. Specically , we adopt the following notion of a regular density. Denition 3 (Regular density, [ 46]).Letc00,c10. Differentiable probability density a random vectorXRnis called (c0,c1)-regular if3 /badblfX(x)/badbl (c1/badblx/badbl+c0)fX(x),xRn. (19) A wide class of densities satisfying smoothness condition (19) is identied in [ 46]. Gaussian, exponential, uniform, Gamma, Cauchy, are all Convo lu- tion with Gaussians produces a regular of B+Z, where/badblB/badbl ba.s., B 2. Consider linear stochastic system (1),Yi=Xi. Suppose that M0and thatVhas a regular density. Then, at any LQR cost b >tr(VS)), the entropy-cost The rst two terms in ( 20) match the rst two terms in (14). TheO1(logn)term is the penalty due to the shape of lattice quantizer cells not being exactly spherical, i.e. i t is the penalty due to the space-lling loss of the quantizer at nit en. In Section VI, we provide a precise expression for that term forn= 1,2,.... TheO2/parenleftBig (btr(VS))1 2/parenrightBig is the penalty due to the distribution of the innovation not being uniform. It becomes negligible for small btr(VS), and the speed of that convergence depends on the smoothness parameters of the noise density. Theorem 2implies that if the channel FiGiis noiseless, then there exists a quantizer with output entropy given by the right side of ( 20) that attains LQR cost b >tr(VS)), when coupled with an appropriate controller. In fact, the bo und in (20) is attainable by a simple lattice quantization scheme that only transmits the innovation of the state (a DPCM scheme). The controller computes the control action based o n the quantized data as if it was the true state (the so-called certainty equivalence control). Theorem 1gives a lower (converse) bound on the output entropy of quantizers that achieve the target cost b, with- out making any assumptions on the quantizer structure and permitting the use of the entire history of observation data . 3As usual, denotes the gradient.Theorem 2proves that the converse can be approached by a strikingly simple quantizer coupled with a standard contro ller, without common randomness (dither) at the encoder and the decoder. Furthermore, although nonuniform rate allocatio n across time is allowed by Denition 2, such freedom is not needed to achieve ( 20); the scheme that achieves ( 20) satises H(Ui|Ui1)rin the limit of large i. Although the bound in Theorem 1is tight at low b(as demonstrated by Theorem 2), ifAhas both stable and unstable eigenvalues and bis large, it is possible to improve the bound in Theorem 1by projecting out the stable modes of the system. Towards this end, consider a Jordan decomposition of A:4 A=JAJ1, (21) whereAis the Jordan form of A, andJis invertible. Without loss of generality, assume the eigenvalues of Aare ordered in decreasing magnitude order. Write Aas a direct sum of its Jordan blocks, A=A1...A\u00afs. For somessuch that 1 s\u00afsn, let= dim(A1)+...+dim(As)be the dimension of the column space of the rst sJordan blocks. Consider the orthogonal projection matrix onto that space, T , where is a 0-1-valued n\u00d7matrix given by: /defines/bracketleftbiggI 0/bracketrightbigg The improvement of Theorem 1can now be formulated. Theorem 3. Consider the fully observed linear stochastic system (1),Yi=Xi. Suppose that h(V)>. Letbe a diagonal matrix such that M/definesJTMJ/{ollowsequal, whereJis dened in (21). At any LQR cost b (25) If=n, thena=|detA|, and the bound in ( 23) reduces to (14). On the other hand, taking to be the number of unstable eigenvalues in ( 23), one can conclude R(b)/summationdisplay i:|i(A)|1log|i(A)|. (26) If the dimensionality of control is less than that of the sys- , the bounds in the dependence onb. bound in Theorem 4below is a decreasing function ofb, even ifm<n . Theorem 4. Consider the fully observed linear stochastic system (1),Yi=Xi. Suppose that h(V)>. of Ahas been previously applied in the context of control under communication constraints in e.g. [ 8], to Theorem 1. We conclude Section II-A with a few technical remarks. Remark 1.Since the running the weaker result of Nair and Evans [10], who showed the necessity of ( 26) to keepsuptE/bracketleftbig /badblXt/badbl2/bracketrightbig bounded. Note also that the approach of Nair and Evans [ 10] applies only to xed-rate quantization, while our approach encompasses both xed- and variable-rate quantization, as well as control over noisy channels and non-Gaussian system disturbances. Remark 2.Tatikonda et al. [ 2, (17)] proposed to apply the classical reverse waterlling, known to achieve the noncau sal Gaussian rate-distortion function, at each step i, to compute the causal Gaussian rate-distortion function for the seque nce of vectors {Xi} i=1. Unfortunately, reverse waterlling is only suboptimal, as can be veried numerically by comparing [ 2, (17)] to the semidenite program of Tanaka et al. [ 33] (Fig. 3). The reason reverse waterlling does not apply is that i(t) in [2, (15)] depend on the distortion threshold, an effect not present in the computation of the classical non-causal rate - distortion function. Remark 3.The semidenite program (SDP) of Tanaka et al. [33] provides an exact numerical solution to the Gaussian causal rate-distortion-function, while our results in The orems 1,3,4provide analytical lower bounds, which hold beyond Gaussian noise. Fig. 3presents a numerical comparison be- tween our lower bound in Theorem 3and the exact calculation ofR(d), for a randomly generated 3-dimensional system. For the example in Fig. 3, our lower bound is within 0.14bits from the optimum. While always tight in low-cost regime, in medium-cost regime it will become looser if the spread of the eigenvalues of Ais large. 10 20 30 40 50 60 70 801.522.533.54 b PSfrag replacements R(b), bitsSDP [ 34] bound (Theorem 3) dC R(d) V(d,) Fig. 3: Our lower bound in Theorem 3, the exact rate-cost function computed using the SDP method [ 34], and the reverse R=I;BandVwere The reverse watelling solution is evidently strictly suboptimal.B. Partially observed system Consider now the scenario in which the encoder sees a noisy observation of the system state and forms a codeword to transmit to the controller using its present and past nois y observations. If the system and observation noises are join tly Gaussian, our results in Section II-A generalize readily. In the absence of communication constraints, the minimum cost decomposes into two terms, the cost due to the noise in th e system in ( 8), and the cost due to the noise in the observation of the state: bmin= tr(VS)+tr/parenleftbig ATMA/parenrightbig , (30) estimation error XtE/bracketleftbig Xt|Yt,Ut1/bracketrightbig in the limit t . The celebrated separation principle of estimation and control states that the minimum cost in ( 30) can be attained by separately estimating the value of Xiusing the noisy observations Yi, and by applying the optimal control to the estimate as if it was the true state of the system. If system and observation noises ar e Gaussian, the optimal estimator admits a particularly eleg ant implementation via the Kalman lter. Then, is given by =PK/parenleftbig CPCT+W/parenrightbig KT, (31) whereWis the covariance matrix of algebraic Riccati equation rate-cost function is bounded in terms of the steady state covariance matrix of the innovation in encoder's state estimate: N/definesK/parenleftbig CPCT+W/parenrightbig KT=AAT+V. (34) Theorem 5. Consider the partially observed linear stochastic system (1),(2). Suppose further that X1,VandWare all Gaussian. At any target LQR cost b > b min, the rate-cost function is bounded below as R(b)log|detA|+n 2log/parenleftBigg nontrivial if both M0 andN0. The necessary condition for that is rankB= rankC=n. In the fully observed system, C=K=I, P=N=V,W=0, and ( 35) reduces to ( 16). As we will see in Section V, the key to proving Theorem 5is to represent the evolution of the current encoder's estimate o f the state in terms of its previous best estimate and an independe nt Gaussian, as carried out by the Kalman lter recursion. It is for this technique to apply that we require the noises to be Gaussian.5As for the achievable scheme, as in the fully observed case, the observer quantizes the innovation, only now the innovation is the difference between the observer's estimate and the controller's estimate. The scheme operate s as follows. The observer sees Yi, recursively computes its estimate of the state Xi, computes the innovation, quantizes the innovation. The controller receives the quantized valu e 5Recall that Theorems 1,2,3make no such restrictions.7 of the innovation, recursively computes its own estimate of the stateXi, and forms the control action that is a function of controller's state estimate only. Accordingly, the task s of state estimation, quantization and control are separated. The momentous insight afforded by Theorem 6below is that in the high rate regime, this simple scheme performs provably clos e toR(b), the best rate-cost tradeoff theoretically attainable. Theorem 6. Consider the partially observed linear stochastic system (1),(2). Suppose that M0,N0, and thatX1,V andWare all Gaussian. At any LQR cost b > b min, the entropy-cost function is follows. 7. Consider the observed linear stochastic system (1),(2). Suppose that (A,B)is controllable and (A,C) is observable. Suppose further that X1,VandWare all Gaussian. At any LQR cost b>bmin, the 8. Consider the partially linear stochastic system (1),(2). Suppose that (A,B)is controllable, (A,C)is observable, mkn. further that X1,V andWare all Gaussian. At any LQR cost b > b min, the rate-cost function is bounded from below as 8reduces to Theorem 5. C. Operational implications So far we have formally dened the rate-cost / entropy- cost functions as the limiting solutions of a minimal direct ed mutual information / entropy subject to an LQR cost constrai nt (Denition 1/ Denition 2) and presented lower and upper bounds to those functions. In this section we discuss the operational implications of the results in Section II-A and Section II-B in several communication scenarios.1) Control over a noisy channel Consider rst a general scenario in which the channel in Fig. 1is a dynamical channel dened by causal kernels {PGt/bardblFt} t=1. For the class of directed information stable channels, the feedback capacity of the channel is [ 47], [48] C= liminf tsup PFt/bardblDGt1 tI(FtGt), (41) If past channel outputs are not available at the encoder, the n thesupis over allPFt, andI(FtGt) =I(Ft;Gt). The following result, the proof of which is deferred until Section IV, implies that the converse results in Theorems 1,3, 5and7present lower bounds on the capacity of the channel necessary to attain b. Proposition 1. A necessary condition for stabilizing the sys- tem in (1),(2)at LQR cost bis, R(b)C. (42) One remarkable special case when equality in ( 42) is attained is control of a scalar Gaussian system over a scalar memoryless AWGN channel [ 2], [49]-[51]. In that case, the channel is probabilistically matched to the data to be trans - mitted [ 5], no coding beyond simple amplication is needed, and linearly transmitting the innovation is optimal [ 49]. In practice, such matching rarely occurs, and intelligent joi nt source-channel coding techniques can lead to a signicant p er- formance improvement. One such technique that approaches (42) in a particular scenario is discussed in [ 51]. In general, how closely the bound in ( 42) can be approached over noisy channels remains an open problem. 2) Control under xed-rate quantization If the channel connecting the encoder to the controller is a noiseless bit pipe that accepts a xed number of rbits per channel use (so that C=r), bothR(b)andH(b)are lower bounds on the minimum quantization rate rrequired to attain costb: R(b)H(b)r. (43) Therefore, the converse results in Theorems 1,3,4,5,7and 8give sharp lower bounds on the minimum size of a xed- rate quantizer compatible with LQR cost b. The achievability results in Theorems 2and 6are insufcient to establish the existence of a xed-rate quantizer of a rate approaching R(b). While attempting to nd a time-invariant xed-rate quantizer operating at any nite cost is futile [ 10], determining whether there exists an adaptive xed-rate quantization sc heme approaching the converses in Theorems intriguing open question. 3) Control under variable-length quantization Assume now that the channel connecting the encoder to the controller is a noiseless channel that accepts an average of rbits per sample. That is, the channel input alphabet is the set of all binary strings, {,0,1,00,01,...}, and the encoding functionPFt/bardblYtmust be such that 1 tt/summationdisplay i=1E[(Fi)]r, (44) where(Fi)denotes the length of the binary string Fi.8 The minimum encoded average length L(X)in lossless compression of object Xis bounded as [ 52], [53] L(X)H(X) (45) L(X)+log2(L(X)+1)+log2e. (46) Note that ( 45) states that the optimum compressed length is below the entropy. This is a consequence of lifting the prex condition: without prex constraints one can compress at an average rate slightly below the entropy [ 52], [54]. The operational rate-cost function length quantization, Rvar(b), is dened as the limsup of the inmum of r's such that LQR cost band ( 44) are achievable in the limit of large t. It follows from ( 45) that Rvar(b)H(b), (47) which implies the existence of a variable-length quantizer whose average rate does not exceed the expressions in The- orems 2and6. Likewise, by ( 46) and Jensen's inequality, H(b)Rvar(b)+log2(Rvar(b)+1)+log2e, (48) which leads via ( 18) to the lower bound Rvar(b)1(R(b)), where1(\u00b7)is the functional inverse of (x) =x+ log2(x+ 1) + log2e. Thus, Theorems 1,3,4,5,7and 8 provide lower bounds to Rvar(b). Consequently, our converse and achievability results in Section II-A and Section II-B characterize the operational rate-cost tradeoff for contr ol with variable-length quantizers. Remark 4.The minimum average compressed length among all prex-free lossless compressors, L p(X), satisesH(X) L p(X)H(X)+1.Therefore, the minimum average rate of a prex-free quantizer compatible with cost bis bounded as R(b)Rvar,p(b)H(b) + 1.Accordingly, the theorems in Section II-A and Section II-B also characterize the rate-cost tradeoff control with variable-length prex -free quantizers. III. C AUSAL SHANNON LOWER BOUND In this section, we consider causal compression of a discret e- time random process, S= (S1,S2,...), under Understanding causal compression bears great independent interest and, due to separation between quantization and control, it is also vital to understanding quantized control, as explained in Section IVbelow. Causally conditioned directed information and functions) The causal rate- and entropy-distortion functions under th eweighted MSE in (49)with side information Zcausally available at side information, we write RS(d)/ HS(d)for the causal rate- / entropy-distortion functions. DPCM encoder , upon observing the current source sample Si, computes the state innovation Sirecursively using Si/definesSiSi|i1, (54) whereSi|i1/definesE/bracketleftBig Si|Si1/bracketrightBig is the a priori (predicted) state estimate at the decoder given previous decoder's 2below implies that the causal rate- and entropy-distortion functions are attained in the class of D PCM (letTi=Si|i1in Proposition 2). No independence among samples of either the innovation process {Si}or its encoded version{Si}is required for this to hold. Proposition 2. Let the stochastic process {Ti}be adapted to the ltration generated by {Zi}. Then, RS/bardblZ(d) =RS+T/bardblZ(d), (56) HS/bardblZ(d) =HS+T/bardblZ(d). (57) Proof. Since mutual information, entropy and distortion mea- sure ( 49) are all invariant to shifts and Tiis a common knowl- edge to both encoder Proposition 2implies further that the rate-distortion trade- offs for controlled and uncontrolled processes are the same . Indeed, consider the Markov process Si+1=ASi+Vi, (58) obtained by letting Ui0,i= 1,2,... in (1). The uncontrolled process ( 58) and the controlled one ( 1) are related throughXi=Si+/summationtexti1 j=1Ai1jBUj. Provided that the encoder and the decoder both have access to past controls, 2withTi=/summationtexti1 j=1Ai1jBUjyields RS(d) =RX/bardblDU(d), (59) HS(d) =HX/bardblDU(d), (60) whereDUsignies causal availability of past controls. The choice of these controls does not affect the rate-distortio n tradeoffs in ( 59) and ( 60). Furthermore, using Si|i1=ASi1, Xi|i1=AXi1+BUi1, it is easy to show that both processes ( 58) and ( 1) have the same innovation process:9 Xi=Si. Thus the same DPCM scheme can be used to encode both. The following bound is a major result of this paper. Theorem 9. For the Markov process (58)and a sequence of weight matrices such that lim i(detWi)1 n=w>0, (61) causal 2log/parenleftbigg a2+wN(V) d/n/parenrightbigg where a/defines|detA|1 n. (63) The proof of Theorem 9uses the classical Shannon lower bound, together with a dynamic programming argument. The- orem 9can thereby be viewed as an extension of Shannon's lower bound to the causal compression setting. Remark 5.For scalar Gauss-Markov sources, equality holds in (62) as long as the expression under the logis1, recovering the known result 6.In of the long-term average (Ces` aro mean) con- straint in ( 49), [2], [23] considered a more stringent constraint in which the average distortion is bounded at each time insta nt i. In the innite time horizon, for Gauss-Markov sources, bot h formulations are equivalent, as optimal rates and distorti ons settle down to their steady states [ 32, Th. 3], [ 55]. For the scalar case this equivalence also follows by comparing ( 62) (obtained with a Ces` aro mean constraint) and ( 64) (obtained in [2], [23] with a pointwise constaint). Theorem 10, stated next, shows that the converse in The- orem 9can be approached by a DPCM quantization scheme with uniform rate and distortion allocations (in the limit o f innite time horizon). This implies that nonuniform rate an d distortion allocations permitted by ( 49) and ( 50)-(51) cannot yield signicant performance gains. Theorem 10. For the Markov process (58)such thatVhas a regular density and a sequence of weight matrices such that (61)holds, the causal that limiE/bracketleftBig (SiSi)TWi(SiSi)/bracketrightBig dand limtH(Si|Si1)the right side of (65). We conclude Section IIIwith two extensions of Theorem 9. If the dynamic range of the eigenvalues of Ais large, the bound in Theorem 11below, obtained by projecting out a subset of smaller eigenvalues of A, can provide an improvement over Theorem 9at medium to large d. Recall the denition of matricesJ,in (21), (22), respectively.Theorem 11. Consider the uncontrolled process (58)and the distortion in (49). Consider matrices Visuch that iVi/parenrightbig/parenrightbig1 m>0. ( 26). If Wiis singular, or if Vidoes not have a density with respect to the Lebesgue measure on Rn, the bounds in Theorem 9 and 11reduce tonlogaandnloga, respectively, losing the dependence on d. The bound in Theorem 12below is a decreasing function of d, even if Wiis singular, or if Viis supported on a subspace of Rn. Theorem 12. Consider the uncontrolled process (58)and the distortion in (49). Assume that the weight matrices Wisatisfy limiWi=LTL, whereLis anm\u00d7nmatrix,mn. Suppose further random vector with covariance matrix V, wherekm, andKiaren\u00d7kmatrices such that limiKi=K. The 9. IV. C ONTROL ,ESTIMATION AND COMMUNICATION SEPARATED An early quantization-control separation result for Gauss ian systems was proposed by Fischer [ 56]. Tatikonda et al. [ 2] considered control of fully observed system over a noisy channel and showed that certainty equivalence control is optimal if and only if control has no dual effect , that is to say the present control cannot affect the future state uncertai nty. Here, we observe that as long as both the encoder and the controller have access to past controls, separated design f or control over noisy channels is optimal, both for fully obser ved systems and for Gaussian partially observed systems. Recall the last-step weight matrix Stin (3), and let Si,1 it1be the =LT i(R+BTSi+1B)Li, (73) Li/defines(R+BTSi+1B)1BTSi+1. The optimal control of partially observed system in ( 1), (2) in the scenario of Fig. 1, for a xed dynamical channel {PGi|Gi1,Fi} i=1can be obtained as follows:10 state estimate . (75) encoder signal U i= LiAXito channel codeword Fi; (iii) Having received the channel output Gi, the decoder computesUiand applies it to the system. The encoder {PFi|Yi,Ui1}and the controller {PUi|Gi,Ui1} are designed to minimize the LQR cost, written as follows. Theorem 13. The LQR cost in the scenario of Fig. 1for the partially system (1),(2)separates as, LQR/parenleftbig Xt,Ut1/parenrightbig =t1/summationdisplay i=0ci+t1/summationdisplay i=1ei+t1/summationdisplay i=1di, (76) where the control, estimation and communication costs are respectively given by ci/definestr(VSi+1), ( 80), completing the squares and using thatVi1is and independent of Xi1,Ui1, obtain bt=b1+t1/summationdisplay i=1E/bracketleftbig VT iSi+1Vi/bracketrightbig +t1/summationdisplay i=1qi, (83) which is equivalent to ( 76) in the fully observed scenario, i.e. whenXi=Xi. To show the more general case, observe rst that for random vectorsX,YandXforming a Markov chain qi intoqi=ei+di, and thereby rewrite ( 83) as ( 76). Using Theorem 13, we write the minimum over all admissible control sequences. Equality in ( 85) is not attained in general; two important scenarios when is it achieved are fully observed systems and for Gaussian partially observed systems. In the fully obser ved case, the estimation terms eidisappear, and in the Gaussianpartially observed case, it is well known that those terms do not depend on the choice of controls. Indeed, if ViandWiare both Gaussian, then the optimal estimator ( 75) can be implemented via a linear recursion given by the Kalman lter. At time i, having observed Yi, the Kalman lter forms an estimate of the system state using Yiand the prior estimate Xi1as follows (e.g. [ 57]): Xi=AXi1+BUi1+KiYi, (86) where Yi/definesYiCAXi1CBUi1 (87) y is the innovation , Gaussian and independent of Xi1,Ui1 andYi1, and the recursion: Ki/definesPi|i1CT/parenleftbig CPi|i1CT+W/parenrightbig1, (88) Pi+1|i=A(IKiC)Pi|i1AT+V,P1|0/definesX1.(89) The covariances of the innovation and the estimation error a re given by, respectively, Cov(Yi) =CPi|i1CT+W, (90) Cov(XiXi) = (IKiC)Pi|i1, (91) and the estimation is thus given by ei= tr/parenleftbig/parenleftbig IKiC)Pi|i1/parenrightbig ATMiA/parenrightbig . (92) An immediate corollary to Theorem 13is the following. Corollary 1. In both fully observed systems and Gaussian partially observed systems, the rate-cost and the entropy- cost functions and independent of the control sequence Uand are given by, respectively, R(b) =RX/bardblDU(bbmin), (93) H(b) =HX/bardblDU(bbmin), (94) wherebminis the minimum cost attainable without commu- nication constraints in (30), and causal rate- and entropy- distortion functions are evaluated with weight matrices Wi=ATMiA. (95) Proof. Since equality in ( 85) holds, we need to minimize/summationtextt1 i=1disubject to either directed information or entropy con- straint. Once we argue that the minimal achievable distorti ons can be equivalently written as di=E/bracketleftBig whereXiis the of Xi, andUi=LiAXi, we will immediately obtain ( 93), (94). But this follows via the same arguments as in the proof of Proposition 6below, using data processing for directed information [ 58, Lemma 4.8.1] in lieu of that for mutual information. Via Corollary 1, we can show the converse for control over noisy channels in Proposition 1using a converse for track- ing over noisy channels. Tracking S1,S2,... over a causal feedback channel PGt/bardblFtgives rise to a joint distribution of the form PStPFt/bardblSt,DGtPGt/bardblFtPSt/bardblGt, wherePFt/bardblSt,DGt andPSt/bardblGtrepresent encoder and decoder mappings, and the goal is to minimize the distortion between StandSt. A necessary condition for the existence of an encoder/decode r pair achieving distortion din the limit of innite time horizon is [58, Th. 5.3.2], RS(d)C. (97)11 Proposition by plugging ( in ( 97). V. C ONVERSE THEOREMS :TOOLS AND PROOFS We start by introducing a few denitions and tools, some classical, some novel, that form the basis of our technique. Conditional entropy power is entropy of X. Proposition 3. ForXRn, nN(X|U)Var[X|U], (99) with equality if and only if X=U+S, whereSis Gaussian. Proof. The unconditional case is a well-known maximum entropy result (e.g. [ 59, Example 12.2.8]). This implies that for each realization of u,nN(X|U=u)Var[X|U=u]. Taking expectation with respect to Uof both sides and using strict convexity of x/mastoexp(x), we obtain ( 99) together with condition for equality. An essential component of our analysis, the conditional entropy power inequality (EPI), follows from the unconditi onal EPI (Conditional EPI) .IfX YgivenU, then N(X+Y|U)N(X|U)+N(Y|U). (100) In causal data compression, the quantized data at current step creates the side information for the data to be compress ed at the next step. The following bound to the conditional entropy power minimized over side information will be vital in proving our converse theorems. Proposition 4. ForXRn, inf PU|X:I(X;U)rh(X|U) h(X)r, (102) which is equivalent to ( 101). square, the entropy power scales as N(LX|U) =|detL|2 nN(X|U). (103) The next proposition generalizes the scaling property ( 103) to the case where the multiplying matrix is not square.6 Proposition 5. LetXRnbe a random vector with covariance X0, letmn, and let Lbe anm\u00d7n matrix det/parenleftbig LXLT/parenrightbig detX/parenrightBigg1 m (N(X))n m (104) 6Proposition 5is stated for the unconditional case for simplicity only; naturally, its conditional version also holds.Equality holds in (104)ifm=nor ifXis Gaussian. Proof. Without loss of generality, assume D(\u00b7/badbl\u00b7)as h(X) =1 follows by substituting ( 107) into ( 105) and applying (12). The (single-shot) distortion-rate function with respect t o the weighted mean-square distortion is dened as follows. Denition 5 (conditional distortion-rate function) .LetX Rnbe a random vector, and M/{ollowsequal0be ann\u00d7nmatrix. The distortion-rate function under the weighted MSE with si de information Uat both the encoder and the decoder is Dr,M(X|U)/definesinf PX|XU: I(X;X|U)rE/bracketleftBig (XX)TM(XX)/bracketrightBig .(108) If no side information is available, i.e. U0, we denote the corresponding unconditional distortion-rate functio n by Dr,M(X). The distortion-rate function under MSE distortion corresponds to M=I, and we simply denote Dr(X|U)/definesDr,I(X|U). (109) The next proposition equates the distortion-rate function s under weighted and non-weighed MSE. Proposition 6. LetXRnbe a random vector, and let Lbe anm\u00d7nmatrix. The following equality holds. Dr(LX|U) =Dr,LTL(X|U). (110) Proof. We show the unconditional version of ( 110); the con- ditional one is analogous. We Dr(LX)/definesinf X:I(LX;X)rE/bracketleftBig To show in (112), letbe the orthogonal projection matrix onto the column space of L. We /badblx/badbl /badblx/badbl andLx=Lxto claim E/bracketleftBig for mu- tual information to claim I(LX;X)I(LX;X). Likewise, holds in ( 113) by data processing. To show that holds in (112), we note that the optimization problem in ( 112) is obtained by restricting the domain of minimization in ( 111) toXIm(L)7. To show that holds in ( 113), we note that the optimization problem in ( 112) is obtained by restricting the domain of minimization in ( 113) toXIm(L)satisfying 7The image of a linear transformation described by matrix Lis the span of its column vectors.12 the Markov chain condition XLXX, since for such X, I(X;X) =I(LX;X) =I(LX;LX). Remark 7.We may always assume that Xhas uncorrelated components when computing distortion-rate functions. Ind eed, letLbe the orthogonal transformation that diagonalizes the covariance matrix of X. SinceLTL=I, by Proposition 6the MSE distortion-rate functions of XandLXcoincide. The following tool will be instrumental in our analysis. Theorem 15 (Conditional Shannon lower bound) .The condi- tional distortion-rate function is bounded below X=U+S, whereS N(0,2I). Proof. Theorem 15is a conditional version of Shannon's lower bound [ 35]. Using Propositions 3and4, the distortion-rate func- tion of a white Gaussian vector with the same differential entropy as the original vector. Although beyond Gaussian X, Shannon's lower bound is rarely attained with equality [ 60], it is approached at high rates [ 61]. The tightness of Shannon's lower bound at high rates is key to arguing that the bound in Theorem 1can in fact be approached. For convenience, we record the following result, which is an immediate corollary to Proposition 4. Proposition 7. LetXRnbe a random vector. The following inequality holds: min URn:I(X;U)sDr(X|U)Dr+s(X). (119) Remark 8.IfXis Gaussian, then min URn:I(X;U)sDr(X|U) =Dr+s(X), (120) and the minimum is attained by a Gaussian U. For non- GaussianX,holds in ( 120). We are now equipped to prove our converse theorems. Proof of Theorem 9.For any causal kernel PSt/bardblStinduced by a code, denote the per-stage information rates ri/definesI(Si;Si|Si1). UsingriI(Si;Si|Si1), Shannon's (Theorem 15), Proposition n, ( 127) is by the conditional EPI (Theorem 14), (128) is due to ( 119). Note that ( 126)-(129) holds for an arbitrary encoded sequence S1,...,Si1, including the optimal one. Rewriting ( 129) such that for allit, wiw. Assumption ( 61) ensures that t<. The distortion constraint in ( 52) and the bound ( 123) imply w t/summationtextt i=tdi1 t/summationtextt i=1widid. (133) In particular, ( 133) implies that dtdt w, which together witha2dt1+nN(V)nmin{N(S1),N(V)}means that the rst term in ( 132) normalized by tis bounded below by a quantity that vanishes as t . Since the function x/mastolog/parenleftBig a2+nN(V) x/parenrightBig is convex and decreasing, by Jensen's inequality and ( 133) the sum in ( ( 134) bytand taking taking a limt followed by a lim0, we obtain ( 62). Proof of Theorem 11.We start by making two observations. First, putting S i/definesJ1SiandV i/definesJ1Vi, we may write S i+1=AS i+V rate-distortion functions of SandSsatisfy RS,{Wi}(d) =RS ,{JTWiJ}(d), (136) where we indicated the weight matrices in the subscript. Second, if 0/ecedesequalT/ecedesequalI, andcommutes with L, then TLTL/ecedesequalLTL. (137) Due to ( 136) we may focus on evaluating the rate-distortion function for S. Since/parenleftbig T /parenrightbig2/ecedesequalT /ecedesequalIandT commutes with Li, we may apply ( 137) and Theorem 15to obtain E/bracketleftBig iS i)TT JTWiJT (S iS i)/bracketrightBig (139) w id i, (140) where d i/definesDri(T S i|Si1), i= 1,2,...,t. (142) Using 142), 14and Proposition 7, we establish d i=Dri(T i1+T i1|Si1) (143) Dri(T AS exp(2ri/n) (147) The rest of the proof follows that of Theorem 9. Proof of Theorem 12.It is easy to see (along the lines of (131)) that if WiLTL, we may put WiLTLwithout affecting the (causal) rate-distortion function. Similar ly, if Vi=KiV iandKiK, we may put Vi=KV i. We will therefore focus on bounding the distortion with weight matr ix LTLand withVi=KV i. Adopting the convention S1V0, we rewrite ( 58) as, Si=i1/summationdisplay j=0Aij1Vj (148) Fixing causal reproduction vector Si, for0ji1, random variable Vj/definesVjE/bracketleftBig Vj|Si/bracketrightBig =VjE/bracketleftBig independent of Sj. Note that different Vj's are uncorrelated. Indeed, to verify and ( 150); (152) leverages Proposi- tion6to minimize each term of the sum over PSi j|Vj1subject to the constraint I(Vj1;Si j)i/summationdisplay =jr, (155) rate, as dened before in ( 121); (153) is due to Proposition 5. To verify that constraint ( 121) implies (155), we apply the independence of Vj1andSj1and the chain rule of mutual information to =I(Vj1;Si i+1andd iare tied in a recursive relationship 129): d i+1= exp(2ri+1/n)/parenleftBig a2d i+N(V)k m/parenrightBig . (157) The rest of the proof follows along the lines of ( 130)-(134). Proof of Theorem 1.IfAis rank-decient, the right side of (14) is, and there is nothing to prove. Assume detA/ne}ationslash= 0. Further, the case rankB< n impliesdetM= 0 and is covered by Theorem 3. Assume rankB=n. According to Corollary 1, Theorem 9and ( 59), it (95). SinceS>0(e.g. Proof Theorem 3.According (95). Consider rst the case rankB=n. Letibe a diagonal matrix such that 0/ecedesequali/ecedesequalJTMiJ. The weight matrices JTATMiAJ=ATM iA/{ollowsequalATiA, and thus the assumption Theorem 11is with Vi=1 2 iA, and ( 23) follows. IfrankB<n,Mis singular, and the bound in ( 23) reduces to simply R(b)loga. (158) To show ( 158), x some>0. Without loss of generality, we Bas follows: B/defines/bracketleftbig BB/bracketrightbig , (159) where(nm)\u00d7nmatrixBis chosen so that the of BspanRn. We also augment the m\u00d7mmatrixRin (3): R/defines/bracketleftbigg R 0 0 Inm/bracketrightbigg . (160) Consider the augmented system parameterized by : Xi+1=AXi+BUi+Vi, (161) where control inputs Uiaren-dimensional. The augmented system in ( 161) achieves the same or smaller quadratic cost as the system in ( 1), because we can always let Ui=/bracketleftbig Ui0/bracketrightbigT to equalize the costs. Therefore, R(b)sup >0R(b), (162) whereR(b)denotes the rate-cost function for the system in (161) with parameter in (159), (160). In particular, since (158) holds for the augmented system it must also hold for the original system. Proof of Theorem 4.According to Proposition 6, the causal rate-distortion function of the uncontrolled process {Si}with weight matrices Wi=ATMiAis to that of assume rankB= rankC=n. The more general case is considered in Theorem 7. According to ( 86) and Corollary 1. Proof of Theorem 7.The proof is similar to that of Theorem 3 uses Theorem 11to lower-bound RX/bardblDU(d). Proof of Theorem 8.The proof is of Theorem 4, Theorem 12to lower-bound RX/bardblDU(d). VI. A CHIEVABILITY THEOREMS :TOOLS this section, we will prove Theorems 2and6. Our achievability scheme employs lattice quantization. A latticeCinRnis a discrete set of points that is closed under reflection and addition. The nearest-neighbor quantizer is the mapping qC:Rn/masto C dened by qC(x)/definesargmin cC/badblxc/badbl. (164) Covering efciency of Cis measured by C/defines/parenleftbiggBC VC/parenrightbigg1 n , (165) whereVCthe volume of the V oronoi cells of lattice C: VC/definesVol({xRn:qC(x) =c}), (166) where arbitrary c C, andBCis the volume of a ball whose radius is equal to that of the V oronoi cells of C. The radius ofBCis called covering radius of lattice C. By denition, C1, and the closer Cis to1the more sphere-like the V oronoi cells of Care and the better lattice Cis for covering. Proof of Theorem 10.The proof analyses a DPCM scheme. First, we describe how the codebook is generated, then we describe the operation of the encoder and the decoder, and then we proceed to the analysis of the scheme. Codebook design. To maximize covering efciency, we use the best known n-dimensional lattice quantizer q=qCnscaled so that its covering radius is d. Encoder. Upon observing Si, the encoder computes the state innovation Sirecursively using the formula Si/definesSiASi1, (167) whereSiis the decoder's state estimate at time i(putS0/defines0). The encoder transmits the index of Qi/definesq(W1 2 iSi). (168) Decoder. The decoder recovers the lattice cell identied by the encoder, and forms its state estimate as Si=ASi1+Si, (169) Si/definesW1 2 iQi. (170) Analysis. The distortion at i=1H(Qi), it sufces to bound the unconditional entropy of Qi. First, we establish that W1 2 iSihas a regular density. Using the assumption that Vihas a(c0,c1)-regular density, it's easy to see that ic0,w1 to (171),/parenleftBig SiSi/parenrightBigT ATWiA/parenleftBig SiSi/parenrightBig ai/definessup z/ne}ationslash=0zTATWiAz zTWiz. (173) From ( 167) and ( 58), Si=A(Si1Si1)+Vi1, (174) and it follows via [ 46, Prop. 3] that W1 2 iSihas(w1 i(c0+ a1 W1 2 iSi/bracketrightBig aid+vi, (175) where we denoted for brevity vi/definestr(VWi), (176) Now, [ 44, Th. 8] implies that the entropy of Qisatises: To estimate the entropy power of W1 2 iSi, we use ( 172) and (174) to bound the Wasserstein distance between W1 2 iSiand W1 2 iVi1, so that [ 46, 1] bound ( 179) to bound limt1 t/summationtextt i=1H(Qi), we obtain the statement of Theo- Proof of Theorem 2.Due to Corollary 1and ( 60), it sufces to bound the entropy-distortion function of the process ( 58). Such a bound is provided in Theorem 10. Proof of Theorem 6.Due to Corollary 1, it sufces to bound the conditional entropy-distortion function of the Kalman lter estimates process in ( 86). Such a bound follows from ( 60) and Theorem 10. VII. C ONCLUSION We studied the fundamental tradeoff between the communi- cation requirements and the attainable quadratic cost in fu lly and partially observed linear stochastic control systems. We introduced the rate-cost function in Denition 1, and showed sharp lower bounds to it in Theorems 1,3,4(fully observed system) and Theorems 5,7,8(partially observed system). The results in Theorem 2(fully observed system) and Theorem 6(partially observed system) show that the converse can be approached, in the high rate / low cost regime , by a simple variable-rate lattice-based scheme in which onl y the quantized value of the innovation is transmitted. Via th e separation principle, the same conclusions hold for causal compression of Markov sources: a converse, which may be viewed as a causal counterpart of Shannon's lower bound, is stated in Theorem 9, and a matching achievability in Theorem 10. Extending the analysis of the partially observed case to non - Gaussian noises would be of interest. It also remains an open question whether the converse bound in Theorem 1can be approached by xed-rate quantization, or over noisy channe ls. Finally, it would be interesting to see whether using non-la ttice quantizers can help to narrow down the gap in Fig. 3. VIII. A CKNOWLEDGEMENT The authors acknowledge many stimulating discussions with Dr. Anatoly Khina and his helpful comments on the earlier versions of the manuscript. The authors are also grateful to Ayush Pandey, who generated the plot in Fig. 3. REFERENCES [1] V . Kostina and Conference on Communication, Con trol Computing , Monticello, IL, Oct. 2016, pp. Sahai, and r control over a communication channel,\" IEEE Transactions on Automatic Control , vol. 49, no. 9, pp. 1549-1561, 2004. [3] information for channels with feed back,\" Ph.D. dissertation, ETH Zurich, 1998. [5] M. Gastpar, B. Rimoldi, and M. Vetterli, \"To o code: lossy source-channel communication revisited,\" IEEE Transactions on Information Theory vol. 49, no. 5, pp. 1147-1158, May 2003. [6] J. Baillieul, \"Feedback designs device arrays with com- munication channel bandwidth constraints,\" in ARO Workshop on Smart Structures, Pennsylvania State Univ , 1999, pp. 16-18. [7] W. S. Brockett, \"Systems with nite communi cation back,\" IEEE Transactions on Automatic no. 5, pp. 1049-1053, 1999. [8] n constraints,\" IEEE Transactions on , vol. 49, no. 7, pp. 1056-1068, 2004.[9] B. G. N. Nair, F. Fagnani, S. Zampieri, and R. J. Evans, \"Fe edback control under data rate constraints: An overview,\" Proceedings of the IEEE , vol. 95, no. 1, pp. 108-137, 2007. [10] G. N. Nair and R. J. Evans, \"Stabilizability of stochast ic linear systems with nite feedback data rates,\" SIAM Journal on Control and Optimiza- tion, vol. 43, no. 2, pp. 413-436, 2004. [11] W. Brockett and feedback sta bilization of linear systems,\" IEEE transactions on Automatic Control , vol. 45, pp. 1279-1289, 2000. [12] S. Y\u00a8 uksel, \"Stochastic stabilization linear systems with xed- rate limited feedback,\" IEEE Transactions on Automatic Control , vol. 55, 2847-2853, IEEE Transactions on Automatic , vol. 59, no. 6, pp. 1612-1617, 2014. [14] H. Witsenhausen, \"On the structure of real-time sour ce coders,\" The Bell System Technical Journal , vol. 58, no. 6, pp. 1437-1451, IEEE , vol. 28, no. 2, pp. 167-186, 1982. [16] J. prob- lems,\" IEEE Transactions , 29, 6, pp. 814-820, 1983. [17] V S. quential vector quantization of Markov sources,\" SIAM journal on control and optimization , vol. 40, no. 1, pp. 135-148, 2001. [18] and in noisy communication,\" IEEE Transactions on Information Theory , vol. 52, no. 9, pp. 4017-4035, Information Theory , vol. 60, no. 10, pp. 5975-5991, 2014. [20] R. G. Wood, T. Linder, and S. Y\u00a8 uksel, \"Optimality of Wal rand-Varaiya type policies and for zero delay codi ng of Markov sources,\" in Proceedings 2015 IEEE International Symposium on Infor- mation Theory , Hong Kong, June 2015. [21] uksel, T. Basar, and S. P. Meyn, causal qua ntization of Markov sources with distortion constraints,\" in Information Theory and Applications Workshop, 2008 , 2008, pp. 26-30. [22] N. Elia and S. linear syste ms with limited information,\" IEEE transactions on Automatic Control , vol. 46, no. 9, pp. 1384-1400, 2001. and [24] C. D. Charalambous, P. A. Stavrou, and N. U. Ahmed, \"Nona nticipative rate distortion function and relations to ltering theory, \"IEEE Transac- tions on Automatic Control , vol. no. 4, pp. 937-952, 2014. [25] C. Charalambous and A. Farhadi, \"LQG optimality and s eparation principle for general discrete time partially observed sto chastic systems over nite capacity communication channels,\" Automatica , vol. 44, no. 12, pp. 3181-3188, 2008. C. D. Charalambous, C. K. Kourtellaris, and C. Hadjicos tis, \"Optimal encoder and control strategies in stochastic control subje ct to rate constraints for channels with memory and feedback,\" in 2011 50th IEEE Conference on Decision and Control and European Control Con ference , Dec 2011, pp. 4522-4527. ally inattentive control of Markov processes,\" SIAM Journal on Control and Optimiza- tion, vol. 54, no. 2, pp. 987-1016, 2016. [28] E. I. Silva, M. S. Derpich, and J. Ostergaard, \"A framewo rk for subject to data-rate constra ints,\" IEEE Transactions on Automatic Control , vol. 56, no. 8, pp. 1886-1899, 2011. [29] E. M. Encina, \"A ch aracterization of the minimal average data rate that guarantees a given clos ed-loop performance level,\" IEEE Transactions on Automatic Control , 2016. [30] T. Tanaka, K. H. Johansson, T. Oechtering, H. Sandberg, and M. Skoglund, \"Rate of codes in LQG control system s,\" 2016 IEEE International Symposium on Informat ion Theory , Barcelona, Spain, July 2016, pp. 2399-2403. [31] P. A. J. \u00d8stergaard, C. and M. Derpich, \"An upper bound to zero-delay rate distortion via kalman ering for vector gaussian in stationa ry sources,\" IEEE16 Theory 58, no. 5, pp. 3131-3152, May 2012. [33] T. Tanaka, K.-K. K. Kim, P. A. Parrilo, and S. K. Mitter, \" Semidenite programming approach to Gaussian sequential rate-distort ion trade-offs,\" IEEE Transactions on 62, no. 4, pp. 1896-1910, 2017. [34] T. Tanaka, P. M. Esfahani, and K. Mitter, \"LQG control with minimum directed information: Semidenite programming IEEE Transactions on 2017. [35] C. E. Shannon, \"Coding theorems for a source wi th a delity criterion,\" IRE Int. Conv. Rec. , vol. 7, no. 1, pp. 142-163, Mar. 1959, reprinted with changes in Information and Decision Processes , R. E. Machol, Ed. New York: McGraw-Hill, J. , vol. 27, pp. 379-423, July October 1948. A. J. Stam, \"Some inequalities satised by the quantiti es of information of Fisher and Shannon,\" Information and Control , vol. 2, no. 2, pp. 101-112, 1959. zing,\" IEEE Transactions Information no. 5, pp. 676-683, 1968. [39] J. Ziv, \"On universal quantization,\" IEEE Transactions on Information Theory , vol. 31, no. 3, pp. 344-347, 1985. 4, pp. 373-380, Jul. 1979. [42] R. Zamir quantizers,\" IEEE Information Theory , vol. 38, no. 2, pp. 428-436, Mar. 1992. [43] T. T. quantization,\" IEEE Transactions on Information Theory , vol. 40, no. pp. 575-579, Mar. 1994. [44] V . Kostina, low distortion and nite block- length,\" IEEE Transactions on Information Theory , vol. 63, 7, pp. 4268-4285, July 2017. [45] M. Fu, \"Lack quantized line ar quadratic Gaussian control,\" IEEE Transactions on Automatic , 9, pp. 2385-2390, 2012. [46] Y . Polyanskiy and Y . Wu, \"Wasserstein continuity of ent ropy and outer bounds for interference channels,\" IEEE Transactions on Information Theory , vol. 62, no. 7, pp. 3992-4002, July 2016. [47] Y .-H. Kim, \"A coding a class of stationary ch annels with feedback,\" IEEE Transactions on Information Theory , 54, no. 4, pp. 1488-1499, 2008. [48] S. Tatikonda and of channels wi th feedback,\" IEEE Transactions on Information Theory , vol. 55, no. 1, pp. 323-349, Jan 2009. [49] R. Bansal and T. Bas \u00b8ar, \"Simultaneous design of measur ement control strategies for stochastic systems with feedback,\" , vol. 25, no. 5, 679-694, 1989. J. S. Freudenberg, R. H. V . Solo, \"Stabil ization disturbance attenuation over a Gaussian communication cha nnel,\" IEEE Transactions on Automatic Control , vol. 55, no. 3, pp. 795-799, 2010. [51] A. G. Hassibi, Multi-rate AWGN An analog joint source-channel coding perspective,\" in 55th IEEE Conference on Decision and Control . Las Vegas, NV , Dec. 2016. [52] A. Wyner, \"An upper bound on the entropy series,\" Information and Control , vol. 20, no. 2, pp. 176-181, Mar. 1972. [53] N. Alon and A. Orlitsky, \"A lower bound on the expected le ngth of one-to-one codes,\" IEEE Transactions on Information Theory , vol. 40, no. 5, pp. 1670-1672, Sep. 1994. [54] W. Szpankowski and expected Information Theory vol. 57, no. 7, pp. 4017-4025, July 2011. [55] T. Tanaka, \"Semidenite representation of sequential rate-distortion function for stationary Gauss-Markov processes,\" in Proceedings 2015 IEEE Conference on Control Applications (CCA) , Sep. Control 27, no. 4, pp. 996-998, Aug 1982. [57] D. P. Bertsekas, Dynamic programming and optimal control . Athena Scientic Belmont, MA, 1995, vol. 1. M.I.T., 2000.[59] T. M. Cover and J. A. Thomas, Elements of information theory , 2nd ed. John Wiley & Sons, 2012. [60] A. Gerrish and P. Schultheiss, \"Information rates of no n-Gaussian processes,\" IEEE Transactions on Information Theory , vol. 10, no. 4, pp. 265-271, Oct. 1964. [61] Y . N. Linkov, \"Evaluation of -entropy of random variables for small ,\" Problems of Information Transmission , vol. 1, no. 2, pp. 18-26, 1965. PSfrag bits (Theorem 3) dC R(d) V(d,)Victoria Kostina joined Caltech as an Assistant Professor of Electrical Engineering in the fall of 2014. She holds a Bachelor's degree from Moscow institute of Physics and Technology (2004), where she was afliated with the Institute for Information Transmission Problems of the Russian Academy of Sciences, a Master's degree from University of Ottawa (2006), and a PhD from Princeton University (2013). She received the Natural Sciences and Engi- neering Research Council of Canada postgraduate scholarship (2009-2012), the Princeton Electrical Engineering Best Dissertation Award (2013), the Simons-Be rkeley research fellowship (2015) and the NSF CAREER award (2017). Kostina' s research spans information theory, coding, control and communicati ons. PSfrag replacements R(b), bits SDP [ 34] (Theorem 3) dC R(d) V(d,)Babak Hassibi was born in Tehran, Iran, in 1967. He received the B.S. degree from the University of Tehran in 1989, and the M.S. and Ph.D. degrees from Stanford University in 1993 and 1996, respectively, all in electrical engineering. He has been with the California Institute of Tech- nology since January 2001, where he is currently the Mose and Lilian S. Bohn Professor of Electrical Engineering. From 2013-2016 he was the Gordon M. Binder/Amgen Professor of Electrical Engineering and from 2008-2015 he was Executive Ofcer of Electrical Engineering, as well as Associate Director of In formation Science and Technology. From October 1996 to October 1998 he was a res earch associate at the Information Systems Laboratory, Stanford University, and from November 1998 to December 2000 he was a Member of the Tech nical Staff in the Mathematical Sciences Research Center at Bell L aboratories, Murray Hill, NJ. He has also held short-term appointments at Ricoh California Research Center, the Indian Institute of Science, and Linko ping University, Sweden. His research interests include communications and information theory, control and network science, and signal processing and machine learning. He is the coauthor of the books (both with A.H. Saye d and T. Kailath) Indenite Quadratic H2and HTheories (New York: SIAM, 1999) and Linear Estimation (Englewood Cliffs, NJ: Prentice Hall, 2000). He is a recipie nt of an Alborz Foundation Fellowship, the 1999 O. Hugo Schuck best paper aw ard of the American Automatic Control Council (with H. Hindi and S.P. B oyd), the 2002 National ScienceFoundation Career Award, the 2002 Oka wa Foundation Research Grant for Information and Telecommunications, th e 2003 David and Lucille Packard Fellowship for Science and Engineering, th e 2003 Presidential Early Career Award for Scientists and Engineers (PECASE), a nd the 2009 Al-Marai Award for Innovative Research in Communications, and was a participant in the 2004 National Academy of Engineering \"Fr ontiers in Engineering\"program. He has been a Guest Editor for the IEEE Transactions on Inform ation Theory special issue on \"space-time transmission, recepti on, coding and signal processing\" was an Associate Editor for Communications of t he IEEE Trans- actions on Information Theory during 2004-2006, and is curr ently an Editor for the Journal \"Foundations and Trends in Information and C ommunication\" and for the IEEE Transactions on Network Science and Enginee ring. He is an IEEE Information Theory Society Distinguished Lecturer for 2016-2017.17 APPENDIX This appendix summarizes the tools used in the proofs of Section VI. The rst is a tool to bound the difference between the differential entropies of two random vectors whose distributions are close to each other. Proposition 8 ([46, Prop. 1]) .LetXandYbe random vectors with nite second moments. If the density of Xis(c0, c1)- XandY: W(X,Y)/definesinf/parenleftbig E/bracketleftbig /badblXY/badbl2/bracketrightbig/parenrightbig1 2, (182) where the inmum is over all joint distributions PXYwhose marginals are PXandPY. The next result helps us establish that the random vectors encountered at each step of the control system operation hav e regular densities. Proposition 9 ( [46, Prop. 3]) .If the density of Zis(c0, c1)- B Z,/badblB/badbl ba.s., then that of B+Zis (c0+c1b, c1)-regular. The next result gives an upper bound to the output entropy of lattice quantizers. Theorem 16 (Corollary to [ 44, Th. 8]) .Suppose that fXis (c0,c1)-regular. There exists a lattice 2+1/parenrightBig , the Gamma function. The leading term in ( 184) is Shannon's lower bound (the functional inverse of ( 115)). The contribution of the remaining terms becomes negligible if nis large and dis small. Indeed, by Stirling's approximation, as n , n=1 2logn+O(1). (186) On the other hand, Rogers [ 62, Theorem 5.9] showed that for eachn3, there exists Cnwith covering efciency nlogCnlog2 2e(logn+loglogn+c), (187) wherecis a constant. Therefore, the terms nlogCnandn are logarithmic in n, so in high dimension their contribution becomes negligible compared to the rst term in ( 184).In low dimension, the contribution of these terms can be computed as follows. The thinnest lattice covering is known in dimensions 1 to 23 is V oronoi's principal lattice of the r st type [ 63](A n), which nis proven to be the thinnest lattice covering possible in dimensions n= 1,2,...,5. ForA n-based lattice quantizer, we can compute in ( 184) as logAn+n n=1 2log2e(n+2) 12(n+1)11 n. (189) APPENDIX REFERENCES [62] C. A. Rogers, Packing and covering . Cambridge University Press, 1964, no. 54. [63] J. H. Conway and N. J. A. Sloane, Sphere packings, lattices and groups . Springer Science & Business Media, New York, 2013, vol. 290. "}