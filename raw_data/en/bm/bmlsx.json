{"title": "October 2015 - DC's Improbable Science", "author": null, "url": null, "hostname": null, "description": null, "sitename": "DC's Improbable Science", "date": "2015-10-24", "cleaned_text": "Monthly Archives: October 2015 Every day one sees politicians on TV assuring us that nuclear deterrence works because there no nuclear weapon has been exploded in anger since 1945. They clearly have no understanding of statistics. With a few plausible assumptions, we can easily calculate that the time until the next bomb explodes could be as little as 20 years. Be scared, very scared. The first assumption is that bombs go off at random intervals. Since we have had only one so far (counting Hiroshima and Nagasaki as a single event), this can't be verified. But given the large number of small influences that control when a bomb explodes (whether in war or by accident), it is the natural assumption to make. The assumption is given some credence by the observation that the [intervals between wars](http://www.americanscientist.org/issues/id.3269,y.2002,no.1,content.true,page.99999,css.print/issue.aspx) are random [ [download pdf](http://www.dcscience.net/Statistics-of-Deadly-Quarrels-American-Scientist.pdf)]. If the intervals between bombs are random, that implies that the distribution of the length of the intervals is exponential in shape, The nature of this distribution has already been explained in an earlier post about the random lengths of time for which a patient stays in an intensive care unit. If you haven't come across an exponential distribution before, please [look at that post](http://www.dcscience.net/2009/10/26/queueing-for-beds-andrei-markov-and-why-i-still-love-the-nhs/#calcs) before moving on. All that we know is that 70 years have elapsed since the last bomb. so the interval until the next one must be greater than 70 years. The probability that a random interval is longer than 70 years can be found from the cumulative form of the exponential distribution. If we denote the true mean interval between bombs as $\\mu$ then the probability that an intervals is longer than 70 years is \\[ \\text{Prob}\\left( \\text{interval > 70}\\right)=\\exp{\\left(\\frac{-70}{\\mu_\\mathrm{lo}}\\right)} \\] We can get a lower 95% confidence limit (call it $\\mu_\\mathrm{lo}$) for the mean interval between bombs by the argument used in [Lecture on Biostatistics](http://www.dcscience.net/Lectures_on_biostatistics-ocr4.pdf), section 7.8 (page 108). If we imagine that $\\mu_\\mathrm{lo}$ were the true mean, we want it to be such that there is a 2.5% chance that we observe an interval that is greater than 70 years. That is, we want to solve \\[ \\exp{\\left(\\frac{-70}{\\mu_\\mathrm{lo}}\\right)} = 0.025\\] That's easily solved by taking natural logs of both sides, giving \\[ \\mu_\\mathrm{lo} = \\frac{-70}{\\ln{\\left(0.025\\right)}}= 19.0\\text{ years}\\] A similar argument leads to an upper confidence limit, $\\mu_\\mathrm{hi}$, for the mean interval between bombs, by solving \\[ = 0.975\\] so \\[ \\mu_\\mathrm{hi} = \\frac{-70}{\\ln{\\left(0.975\\right)}}= 2765\\text{ years}\\] If the worst case were true, and the mean interval between bombs was 19 years. then the distribution of the time to the next bomb would have an exponential probability density function, $f(t)$, | | \\[ f(t) = \\frac{1}{19} \\exp{\\left(\\frac{-70}{19}\\right)} \\] There would be a 50% chance that the waiting time until the next bomb would be less than the median of this distribution, =19 ln(0.5) = 13.2 years. | | In summary, the observation that there has been no explosion for 70 years implies that the mean time until the next explosion lies (with 95% confidence) between 19 years and 2765 years. If it were 19 years, there would be a 50% chance that the waiting time to the next bomb could be less than 13.2 years. Thus there is no reason at all to think that nuclear deterrence works well enough to protect the world from incineration. Another approach My statistical colleague, the ace probabilist [Alan Hawkes](http://www.onemol.org.uk/?page_id=175), suggested a slightly different approach to the problem, via likelihood. The likelihood of a particular value of the interval between bombs is defined as the probability of making the observation(s), given a particular value of $\\mu$. In this case, there is one observation, that the interval between bombs is more than 70 years. The likelihood, $L\\left(\\mu\\right)$, of any specified value of $\\mu$ is thus \\[L\\left(\\mu\\right)=\\text{Prob}\\left( \\text{interval > 70 | }\\mu\\right) = \\exp{\\left(\\frac{-70}{\\mu}\\right)} \\] | | If we plot this function (graph on right) shows that it increases with $\\mu$ continuously, so the maximum likelihood estimate of $\\mu$ is infinity. An infinite wait until the next bomb is perfect deterrence. But again we need confidence limits for this. Since the upper limit is infinite, the appropriate thing to calculate is a one-sided lower 95% confidence limit. This is found by solving \\[ \\exp{\\left(\\frac{-70}{\\mu_\\mathrm{lo}}\\right)} 0.05\\] which gives \\[ \\mu_\\mathrm{lo} years}\\] Summary The first approach gives 95% confidence limits for the average time until we get incinerated as 19 years to 2765 years. The second approach gives the lower limit as 23.4 years. There is no important difference between the two methods of calculation. This shows that the bland assurances of politicians that \"nuclear deterrence works\" is not justified. It is not the purpose of this post to predict when the next bomb will explode, but rather to point out that the available information tells us very little about that question. This seems important to me because it contradicts directly the frequent assurances that deterrence works. The only consolation is that, since I'm now 79, it's unlikely that I'll live long enough to see the conflagration. Anyone younger than me would be advised to get off their backsides and do something about it, before you are destroyed by innumerate politicians. Postscript While talking about politicians and war it seems relevant to reproduce Peter Kennard's powerful image of the Iraq war. and with that, to quote the comment made by Tony Blair's aide, Lance Price It's a bit like [my feeling about priests](http://www.dcscience.net/2011/11/05/science-philosophy-and-religion-which-best-offers-us-the-tools-to-understand-the-world-around-us/#12stations) doing the twelve stations of the cross. Politicians and priests masturbating at the expense of kids getting slaughtered (at a safe distance, of course). Follow-up [Chalkdust](http://chalkdustmagazine.com/) is a magazine published by students of maths from UCL Mathematics department. Judging by its [first issue](http://issuu.com/chalkdust/docs/main2/1?e=1), it's an excellent vehicle for popularisation of maths. I have a piece in the second issue You can view the whole [second issue on line](http://issuu.com/chalkdust/docs/chalkdust-issue-02-full), or download a [pdf of the whole issue](http://www.dcscience.net/chalkdust-issue-02.pdf). Or a [pdf of my bit](http://www.dcscience.net/Colquhoun-2015-chalkdust.pdf) only: On the Perils of P values. The piece started out as another exposition of the interpretation of P values, but the whole of the first part turned into an explanation of the principles of randomisation tests. It beats me why anybody still does a Student's t test. The idea of randomisation tests is very old. They are as powerful as t tests when the assumptions of the latter are fulfilled but a lot better when the assumptions are wrong (in the jargon, they are uniformly-most-powerful tests). Not only that, but you need no mathematics to do a randomisation test, whereas you need a good deal of mathematics to follow [Student's 1908 paper](http://www.dcscience.net/Student-t-1908.pdf). And the randomisation test makes transparently clear that random allocation of treatments is a basic and essential assumption that's necessary for the the validity of any test of statistical significance. I made a short video that explains the principles behind the randomisation tests, to go with the printed article (a bit of animation always helps). When I first came across the principals of randomisation tests, i was entranced by the simplicity of the idea. Chapters 6 - 9 of [my old textbook](http://www.dcscience.net/Lectures_on_biostatistics-ocr4.pdf) were written to popularise them. You can find much more detail there. In fact it's only towards the end that I reiterate the idea that P values don't answer the question that experimenters want to ask, namely:- if I claim I have made a discovery because P is small, what's the chance that I'll be wrong? If you want the full story on that, [read my paper.](http://rsos.royalsocietypublishing.org/content/1/3/140216) The story it tells is not very original, but it still isn't known to most experimenters (because most statisticians still don't teach it on elementary courses). The paper must have struck a chord because it's had over 80,000 full text views and more than 10,000 pdf downloads. It reached an altmetric score of 975 (since when it has been mysteriously declining). That's gratifying, but it is also a condemnation of the use of metrics. The paper is not original and it's quite simple, yet it's had far more \"impact\" than anything to do with my real work. If you want simpler versions than the full paper, try this blog ( [part 1](http://www.dcscience.net/2014/03/10/on-the-hazards-of-significance-testing-part-1-screening/) and [part 2](http://www.dcscience.net/2014/03/24/on-the-hazards-of-significance-testing-part-2-the-false-discovery-rate-or-how-not-to-make-a-fool-of-yourself-with-p-values/)), or the Youtube video about misinterpretation of P values. The R code for doing 2-sample randomisation tests You can download a [ pdf file that describes the two R scripts](http://www.dcscience.net/R-script-for-ran-tests.pdf). There are two different R programs. One re-samples randomly a specified number of times (the default is 100,000 times, but you can do any number). [Download two_sample_rantest.R](http://www.dcscience.net/two_sample_rantest.R) The other uses every possible sample -in the case of the two samples of 10 observations,it gives the distribution for all 184,756 ways of selecting 10 observations from 20. [Download 2-sample-rantest-exact.R](http://www.dcscience.net/2-sample-rantest-exact.R) The launch party Today the people who organise Chalkdust magazine held a party in the mathematics department at UCL. The editorial director is a graduate student in maths, [Rafael Prieto Curiel](https://www.ucl.ac.uk/news/students/032015/032015-27032015-seven-questions-with-rafael-prieto-curiel). He was, at one time in the Mexican police force (he said he'd suffered more crime in London than in Mexico City). He, and the [rest of the team](https://www.ucl.ac.uk/news/students/032015/032015-27032015-seven-questions-with-rafael-prieto-curiel), are deeply impressive. They've done a terrific job. Support them. The party cakes Rafael Prieto doing the introduction Rafael Prieto doing the introduction Rafael Prieto and me I got the T shirt Decoding the T shirt The top line is \"I\" because that's the usual symbol for the square root of -1. | | The second line is one of many equations that describe a heart shape. It can be plotted by calculating a matrix of values of the left hand side for a range of values of x and y. Then plot the contour for a values x and y for which the left hand side is equal to 1. | | Follow-up 5 November 2015 The Mann-Whitney test I was stimulated to write this follow-up because yesterday I was asked by a friend to comment on the fact that five different tests all gave identical P values, P = 0.0079. The [paper in question](/Science2015Sewald.pdf) was in Science magazine (see Fig. 1), so it [ wouldn't surprise me if the statistics were done badly](http://www.dcscience.net/2014/11/02/two-more-cases-of-hype-in-glamour-journals-magnets-cocoa-and-memory/), but in this case there is an innocent explanation. The Chalkdust article, and the video, are about randomisation tests done using the original observed numbers, so look at them before reading on. There is a more detailed explanation in Chapter 9 of [Lectures on Biostatistics](http://www.dcscience.net/Lectures_on_biostatistics-ocr4.pdf). Before it became feasible to do this sort of test, there was a simpler, and less efficient, version in which the observations were ranked in ascending order, and the observed values were replaced by their ranks. This was known as the Mann Whitney test. It had the virtue that because all the 'observations' were now integers, the number of possible results of resampling was limited so it was possible to construct tables to allow one to get a rough P value. Of course, replacing observations by their ranks throws away some information, and now that we have computers there is no need to use a Mann-Whitney test ever. But that's what was used in this paper. In the paper (Fig 1) comparisons are made between two groups (assumed to be independent) with 5 observations in each group. The 10 observations are just the ranks, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. To do the randomisation test we select 5 of these numbers at random for sample A, and the other 5 are sample B. (Of course this supposes that the treatments were applied randomly in the real experiment, which is unlikely to be true.) In fact there are only 10!/(5!.5!) = 252 possible ways to select a sample of 5 from 10, so it's easy to list all of them. In the case where there is no overlap between the groups, one group will contain the smallest observations (ranks 1, 2, 3, 4, 5, and the other group will contain the highest observations, ranks 6, 7, 8, 9, 10. In this case, the sum of the 'observations' in group A is 15, and the sum for group B is 40.These add to the sum of the first 10 integers, 10.(10+1)/2 = 55. The mean (which corresponds to a difference between means of zero) is 55/2 = 27.5. There are two ways of getting an allocation as extreme as this (first group low, as above, or second group low, the other tail of the distribution). The two tailed P value is therefore 2/252 = 0.0079. This will be the result whenever the two groups don't overlap, regardless of the numerical values of the observations. It's the smallest P value the test can produce with 5 observations in each group. The whole randomisation distribution looks like this In this case, the abscissa is the sum of the ranks in sample A, rather than the difference between means for the two groups (the latter is easily calculated from the former). The red line shows the observed value, 15. There is only one way to get a total of 15 for group A: it must contain the lowest 5 ranks (group A = 1, 2, 3, 4, 5). There is also only one way to get a total of 16 (group A = 1, 2, 3, 4, 6),and there are two ways of getting a total of 17 (group A = 1, 2, 3, 4, 7, or 1, 2, 3, 5, 6), But there are 20 different ways of getting a sum of 27 or 28 (which straddle the mean, 27.5). The printout (.txt file) from the R program that was used to generate the distribution is as follows. | | Randomisation test: exact calculation all possible samples INPUTS: exact calculation: all possible samples OUTPUTS Result of t test Some problems. Figure 1 alone shows 16 two-sample comparisons, but no correction for multiple comparisons seems to have been made. A crude Bonferroni correction would require replacement of a P = 0.05 threshold with P = 0.05/16 = 0.003. None of the 5 tests that gave P = 0.0079 reaches this level (of course the whole i [dea of a threshold level is absurd anyway](http://rsos.royalsocietypublishing.org/content/1/3/140216#sec-13)). Furthermore, even a single test that gave P = 0.0079 would be expected to have a [false positive rate "}